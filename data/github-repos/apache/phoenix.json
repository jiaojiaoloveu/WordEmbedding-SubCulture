{"_default": {"1": {"vincentpoon": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/195f82bae2835a6bfc9481205bad7e8f21944471", "message": "PHOENIX-4531 Delete on a table with a global mutable index can issue client-side deletes against the index"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/edc9d12dc056640526e388d7ccb1a6e2c6d3c51c", "message": "Immutable table SINGLE_CELL_ARRAY_WITH_OFFSETS values starting with separator byte return null in query results"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/b2d5b4d75d4698981b291fecfac3efa3fb6e2649", "message": "PHOENIX-4373 Local index variable length key can have trailing nulls while upserting"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/e50b357a0f8d9c6e8e2e803881e4cc197559b822", "message": "PHOENIX-4269 IndexScrutinyToolIT is flapping"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/53910f9f88cd47f51f68691042306c89118b6ab3", "message": "PHOENIX-4242 Fix Indexer post-compact hook logging of NPE and TableNotFound"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/adda7f10b648fa1d739a1ee739780fa1643faa81", "message": "Amend PHOENIX-4039 Increase default number of RPC retries for our index rebuild task\n\nSigned-off-by: Andrew Purtell <apurtell@apache.org>"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/7a83b8a1c4c9cefb6027b9aa2020af1dd18b599a", "message": "PHOENIX-3948 Enable shorter time outs for server-side index writes\n\nSigned-off-by: gjacoby <gjacoby@apache.org>"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/269", "title": "PHOENIX-2460 Implement scrutiny command to validate whether or not an\u2026", "body": "\u2026 index is in sync with the data table", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/247", "title": "PHOENIX-3825 Mutable Index rebuild does not write an index version fo\u2026", "body": "\u2026r each data row version", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/244", "title": "PHOENIX-3824 Mutable Index partial rebuild adds more than one index r\u2026", "body": "\u2026ow for updated data row", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/243", "title": "PHOENIX-3806 IndexUpdateManager spending a lot of time sorting mutati\u2026", "body": "\u2026ons on Index rebuild", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "joshelser": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/5fb3f7fa86bb9496f91f1f861f000c0089157340", "message": "PHOENIX-4542 Use .sha256 and .sha512"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/0aa1cfb4e14f52c9770d5da5bae7f4868152ca7e", "message": "PHOENIX-4541 Fix apache-rat-check failures"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/2136b002c37db478ffea11233f9ebb80276d2594", "message": "PHOENIX-4466 Do not relocate hadoop code (addendum)\n\nTurns out relocating hadoop-common (most obviously) breaks\nsome security-related classes in hadoop-common around Kerberos logins."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/93306e9e28a0f13cbac87055c30fb9a781ae3345", "message": "PHOENIX-4510 Fix performance.py issue in not finding tests jar (Artem Ervits)\n\nSigned-off-by: Josh Elser <elserj@apache.org>"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/0584d3cb2c2e65c56381f33d93360e64cf79f993", "message": "PHOENIX-4509 Fix performance.py usage text (Artem Ervits)\n\nSigned-off-by: Josh Elser <elserj@apache.org>"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/34693843abe4490b54fbd30512bf7d98d0f59c0d", "message": "PHOENIX-4466 Relocate Avatica and hadoop-common in thin-client jar (Toshihiro Suzuki)\n\nWhen using the thin-client in Spark, we encounter problems in that Spark\nis placing its own version of avatica on the classpath as well. We can\nrelocate most of Avatica (all but the protobuf generated messages as\ntheir classnames are required to be 'org.apache.calcite.avatica.proto'\npresently) and hadoop-common to avoid future problems."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/5cb02da74c15b0ae7c0fb4c880d60a2d1b6d18aa", "message": "PHOENIX-4449 Bundle a copy of Argparse-1.4.0 for installations that need it"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/4a1f0df6143ba705a48b5051aee52dab158afe8d", "message": "PHOENIX-4351 Add i18n-util to bin LICENSE file and to dependencyManagement"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/3f28cc0ad609a6afd6a70a32aa598a5bb5e66dd8", "message": "PHOENIX-4261 Remove unused netty-all dependency"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/052490e09f2271eaa84dc9ab123a62a87123a498", "message": "PHOENIX-4189 Introduce a class that wraps the Map of primary key data"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/0311e4f56fa10a6ea3eb3e0dce8a9d6fa70a48f8", "message": "PHOENIX-4207 Improve zombie-test detection in test-patch.sh"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/d9ac3f109b86d70a3ec8f5e82175f03343be10d0", "message": "PHOENIX-4191 Categorize uncategorized integration tests\n\nUncategorized tests results in Maven not running them."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/4ee35057c6a63c347f959361338b517d4f5b38c4", "message": "PHOENIX-4188 Disable inline-DTDs in Pherf XML records"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/16e0511ff3e65d2463ab4481b9dd9a42cdf18461", "message": "PHOENIX-4107 Add a toggle for PQS registration for the load balancer"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/f8bd40e9fdd57b3af5ed8dc4f08357b22b78b479", "message": "PHOENIX-4088 Clean up SQLExceptionCode (Csaba Skrabak)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/27170bd78dc59c999d94259f14fffd93ae5e89e8", "message": "PHOENIX-3769 Use length from correct byte array in PhoenixIndexBuilder (Sneha Kanekar)\n\nThis was found via failing tests on ppc64."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/87976eb6f9892ab8c20f38db5e1cef3934ec2b89", "message": "PHOENIX-4004 Remove unnecessary allocations in server-side mutable secondary-index path"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/ca1105630dab43a8629e2efa0171d914e0140b3e", "message": "PHOENIX-4042 Add hadoop metrics2-based Indexer coproc metrics"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/b7b571b7db0c58ff488e435d5a3cf6c45a41fe86", "message": "PHOENIX-4011 Use more reasonable branch names for Phoenix and Hadoop versions"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/164001422", "body": "Gabriel's patch is the right way to fix this.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/164001422/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/phoenix/comments/18831881", "body": "> UserGroupInformation.getCurrentUser() will not be thread safe.\n\n```\n  @InterfaceAudience.Public\n  @InterfaceStability.Evolving\n  public synchronized\n  static UserGroupInformation getCurrentUser() throws IOException {\n    AccessControlContext context = AccessController.getContext();\n    Subject subject = Subject.getSubject(context);\n    if (subject == null || subject.getPrincipals(User.class).isEmpty()) {\n      return getLoginUser();\n    } else {\n      return new UserGroupInformation(subject);\n    }\n  }\n```\n\nAm I missing something, @dbahir?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18831881/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "karanmehta93": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/e3faa954952fbe6f9ea5a9e792a5d275d9193a53", "message": "PHOENIX-4528 PhoenixAccessController checks permissions only at table level when creating views"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/c075a17879309fe4def5a621951d72929fb10c3a", "message": "PHOENIX-4424 Allow users to create \"DEFAULT\" and \"HBASE\" Schema (Uppercase Schema Names)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/88038a2dacb7aa1a90015163d4d75d04793e4e11", "message": "PHOENIX-672 Add GRANT and REVOKE commands using HBase AccessController"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/d46d4e564d94077b637ad8f34d8785a4ac3486a5", "message": "PHOENIX-4389 Flapping tests SystemTablePermissionsIT and MigrateSystemTablesToSystemNamespaceIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/82a4dd8f78b38c0da6b019ccdead6879b87c6f26", "message": "PHOENIX-3757 System mutex table not being created in SYSTEM namespace when namespace mapping is enabled"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/289", "title": "PHOENIX-4528 PhoenixAccessController checks permissions only at table\u2026", "body": "\u2026 level when creating views\r\n\r\n@ankitsinghal @twdsilva Please review.\r\n\r\n@ankitsinghal Please suggest new tests that can be added to verify this patch. The test that I added only verifies that create views would succeed. The change that I have made is generic, however it will be good to add tests that cover scenarios that include creation or dropping of index tables.", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/284", "title": "PHOENIX-4424 Allow users to create DEFAULT and HBASE Schema (Uppercas\u2026", "body": "\u2026e Schema Names)\r\n\r\n@twdsilva Please review.", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lhofhansl": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/2d655cdc723c1d69ed9ff556438727e378ac1ed5", "message": "PHOENIX-4076 Move master branch up to HBase 1.4.0. (Andrew Purtell and Lars Hofhansl)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/c216b667a8da568f768c0d26f46fa1a9c0994a04", "message": "PHOENIX-4360 Prevent System.Catalog from splitting."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/44c00345b946cbdd9fc8205f8121cd60a065c0fe", "message": "PHOENIX-4165 Do not wait no new memory chunk can be allocated."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jtaylor-sfdc": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/27d6582827b9306e66d3bfd430c6186ac165fb08", "message": "PHOENIX-4523 phoenix.schema.isNamespaceMappingEnabled problem (Karan Mehta)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/83adf0d1a6e75c97f8ed696340c7af167834f7e9", "message": "PHOENIX-4514 A incorrect key object is used in SequenceManager#validateSequences (Chia-Ping Tsai)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/5f733b38978c7caaab3428fa5680eab614f56cb2", "message": "PHOENIX-4522 Fail to remove the schema from client-side cache (Chia-Ping Tsai)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/2759727e444ef8ae7475d25979be84bfe89895f5", "message": "PHOENIX-4487 Missing SYSTEM.MUTEX table upgrading from 4.7 to 4.13"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/1636f6182ae89e9c4b97c877aa919a6edac5bbc2", "message": "PHOENIX-4488 Cache config parameters for MetaDataEndPointImpl during initialization"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/d6e61af807f7a4e605c61217bac556ffe00ea237", "message": "PHOENIX-4415 Ignore CURRENT_SCN property if set in Pig Storer"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/ca1e17b75ced1e618869ad2c2ada019fcc336c02", "message": "PHOENIX-4387 DefaultColumnValueIT failing in non-US build environments (Pedro Boado)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/03fc3314219f364bf91dd09921e51ecf94a41aff", "message": "PHOENIX-4384 Phoenix server jar doesn't include icu4j jars (Shehzaad Nakhoda)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/79eff5f89adb2c05024272203eebf0504f82ee3d", "message": "PHOENIX-4349 Update version to 4.13.0"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/ee4355791acf3f31568fcd8c43367947d25a1386", "message": "PHOENIX-4237 Allow sorting on (Java) collation keys for non-English locales (Shehzaad Nakhoda)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/1e48eabe4cbf72ce71fb0dbdd6053a9600133ee4", "message": "PHOENIX-4348 Point deletes do not work when there are immutable indexes with only row key columns"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/dc9c2fa8f92548a67a58ea495eed1011b5294fa5", "message": "PHOENIX-4335 System catalog snapshot created each time a new connection is created"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/54a8f27304908f2259109eb230a2d3f261c2270a", "message": "PHOENIX-4290 Full table scan performed for DELETE with table having immutable indexes"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/a7af29f9e90308d5a2805cc3eabf4e607fbe3cb2", "message": "Revert \"PHOENIX-4322 DESC primary key column with variable length does not work in SkipScanFilter\"\n\nThis reverts commit b0220fa7522fd7e1848ad428a47121b205dec504."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/6b24e0d5869839f861f3b7069e865e71d1fc61c6", "message": "Revert \"PHOENIX-4322 DESC primary key column with variable length does not work in SkipScanFilter (fix test failures)\"\n\nThis reverts commit 45a9c275dbbf9206264236c690f40c309d97da3c."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/0c38f493ca4e35eefa2297f62cbe56cca47bb81d", "message": "PHOENIX-4329 Test IndexScrutinyTool while table is taking writes (Vincent Poon)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/438ac5676e8e8f0a69875d9b91acaf5c8ac6201c", "message": "PHOENIX-4277 Treat delete markers consistently with puts for point-in-time scans"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/7cdcb2313b08d2eaeb775f0c989642f8d416cfb6", "message": "PHOENIX-4310 Remove unnecessary casts in UngroupedAggregateRegionObserverIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/dc432b9754d028d0d36652c69b25e0fdb735b3fa", "message": "PHOENIX-4280 Delete doesn't work when immutable indexes are in building state"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/c594038a1bee96b92edd9c788969e3e2dbd6db5a", "message": "PHOENIX-4295 Fix argument order for StatsCollectorIT derived classes"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/ccc44386e05fd7191cd498c8e35b4bb96df2eef7", "message": "PHOENIX-4294 Allow scalar function to declare that it's not thread safe"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/ebfff4e61b377310be105e3a668ab4d8ad3bb87f", "message": "Updating pom version to 4.13.0-SNAPSHOT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/0f4528e3dd08ffab6385f0c0f48fbd3df991978a", "message": "Add script for creating an RC from the Mac"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/f0bc4cdb5bbf96b316c78cc816400b04f63e911b", "message": "PHOENIX-4274 Test case for Hint query for index on view does not use include"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/8e91c62bdde4998896418453d260401224a21f74", "message": "PHOENIX-4275 Use unique table names for ViewIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/bd21ed3d680ff742666943bb72baf7968c8b5b09", "message": "PHOENIX-4239 Fix flapping test in PartialIndexRebuilderIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/62defe43212f83c2f8152af162554ea49046bd62", "message": "PHOENIX-4238 MR IndexScrutinyTool break with salted tables and indexes on views"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/c1ef11289afa370d65088b28a80931844eab08b3", "message": "PHOENIX-4258 Breakup ScanQueryIT into several integration tests so as not to create too many tables in one test"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/f1b4578193b900f3440af681e222d18307de28ce", "message": "PHOENIX-4258 Breakup ScanQueryIT into several integration tests so as not to create too many tables in one test"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/c4b478dc11898483f11c5880d85b2d03cca427d3", "message": "PHOENIX-4239 Ignore flapping PartialIndexRebuilderIT tests until fix is ready"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/f00380ef79795a675cfa5b50082147a273ac26f9", "message": "PHOENIX-4214 Scans which write should not block region split or close (Vincent Poon)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/f4e51143a35bdd48f025b925c9988f98de92a4c3", "message": "Revert \"PHOENIX-4214 Scans which write should not block region split or close (Vincent Poon)\"\n\nThis reverts commit 5d9572736a991f19121477a0822d4b8bf26b4c69."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/e9593529fac63d24629044c6e221f7369b08cf0e", "message": "PHOENIX-4258 Breakup ScanQueryIT into several integration tests so as not to create too many tables in one test"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/02f159165f3b3fa4ca0981745e190e6733f5501d", "message": "PHOENIX-4257 Breakup GroupByIT,MutableQueryIT, and QueryIT into several integration tests so as not to create too many tables in one test"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/e257550629031a281678e9f1bd32e0faf08e61a5", "message": "PHOENIX-4257 Breakup GroupByIT into several integration tests so as not to create too many tables in one test"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/3d91cf7c78b3b40c9ea63cf7d9057ca84846b0de", "message": "PHOENIX-4256 Breakup ClientTimeArithmeticQueryIT into several integration tests so as not to create too many tables in one test"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/d76a4f4e515af02e1c6f9a0dcb7b5e4318b4fbfb", "message": "PHOENIX-4248 Breakup IndexExpressionIT into several integration tests so as not to create too many tables in one test (addendum)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/29acb7786a7cbc49dd277e3448ef90db9ac35110", "message": "PHOENIX-4253 Make doSetup final to prevent errand overrides"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/34c14c1bdb11d589469ae90027583d8b6f9cf8cf", "message": "PHOENIX-4248 IndexExpressionIT into several integration tests so as not to create too many tables in one test (addendum)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/cabd3c0b4ab1f81ee2676ddfd6b932f39b97ca70", "message": "PHOENIX-4252 Remove BaseClientManagedTimeIT as it's no longer used"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/dd3b7b6c04acfc53a1d5cdd5e866a59ae71557c3", "message": "PHOENIX-4251 Breakup ProductMetricsIT into several integration tests so as not to create too many tables in one test"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/eafb58bec2e35e0a999e40b2831b504c05295328", "message": "PHOENIX-4250 Breakup AlterTableIT into several integration tests so as not to create too many tables in one test"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/3593ec8bc0042388f119652dc8177efc05f39d80", "message": "PHOENIX-4249 Decrease unique name allocation for SequenceIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/3d9adc6f3f298ab5077b8fddf9d17aa4e4a11e56", "message": "PHOENIX-4248 Breakup IndexExpressionIT into several integration tests so as not to create too many tables in one test"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/834133a194a75c9231f9773b0bd89fd752dcf5dc", "message": "PHOENIX-4246 Breakup join related tests into several integration tests so as not to create too many tables in one test"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/ee20a8c9ba8c7b491937eaf93a889010daebba98", "message": "PHOENIX-4246 Breakup join related tests into several integration tests so as not to create too many tables in one test"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/fd7776407737cf6cc5cb95eb8c5b9380af4b560c", "message": "PHOENIX-4245 Breakup IndexIT into several integration tests so as not to create too many tables in one test"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/527f786ac4b4883677fa77d0e31c71ea7d518872", "message": "PHOENIX-4244 Breakup ArrayIT into several integration tests so as not to create too many tables in one test"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/06f58d56c9e2b3badb5e3f6bd5092f03f58f00a8", "message": "PHOENIX-4096 Disallow DML operations on connections with CURRENT_SCN set"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/5d9572736a991f19121477a0822d4b8bf26b4c69", "message": "PHOENIX-4214 Scans which write should not block region split or close (Vincent Poon)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/36879392c5eff5d427e9c110b54d8484bcb6606e", "message": "PHOENIX-3815 Only disable indexes on which write failures occurred (Vincent Poon)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/94601de5f5f966fb8bcd1a069409bee460bf2400", "message": "PHOENIX-4233 IndexScrutiny test tool does not work for salted and shared index tables"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/d13a2e5b27db8d22344442a9fc9890a37052f0f9", "message": "PHOENIX-4230 Write index updates in postBatchMutateIndispensably for transactional tables"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/944bed73585a5ff826997895c2da43720b229d8a", "message": "PHOENIX-4178 Detect failed index write while rebuilder is running with index staying active"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/0a1b33aa2eeda721aadb2369f0aa2d612fb4a24c", "message": "PHOENIX-4166 Use Tephra HBase 1.3 compat module"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/815ce00a382e43dd3f9a72b36107a3582f3f730f", "message": "PHOENIX-4212 Addendum to remove unused member variables for DerivedTableIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/bcdf292dab26b12f210f8758b414b36bc7f4a7c4", "message": "PHOENIX-4221 Disallow DML operations on connections with CURRENT_SCN set - VariableLengthPKIT (Ethan Wang)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/3ed60237e9685a6cab8a4bf9b8f71ba015fb710c", "message": "PHOENIX-4220 Upper bound not being used in partial index rebuilder"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/bcb755d1de3e6968a12671c46efa34fe2f96a518", "message": "PHOENIX-4212 Disallow DML operations on connections with CURRENT_SCN set - DerivedTableIT (Ethan Wang)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/e47e78477802940148b6457021a6362cefb002e6", "message": "PHOENIX-4189 Introduce a class that wraps the Map of primary key data (addendum)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/e455df310de0633daf8ddf5aeae985981db3c06f", "message": "PHOENIX-4199 Modify SequenceIT.java to not use CurrentSCN"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/711b5bfad0ea6bb9402ef6834b76fb292991969f", "message": "PHOENIX-4209 Disallow DML operations on connections with CURRENT_SCN set - DistinctCountIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/6c3bcb6a94b48496d02830747ddd6fcb0fa833af", "message": "PHOENIX-4209 Disallow DML operations on connections with CURRENT_SCN set (Ethan Wang)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/26f45f0fafe7bdefec23a5b1ade7c30722c0b93d", "message": "PHOENIX-4211 Change tephra dependency in phoenix-kafka to test dependency like other poms"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/8ee8f092a67ef278bef1656afadb488c6f6f47f9", "message": "PHOENIX-4208 Modify tests to not use CurrentSCN"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/4c2674b1ddc679d897933fd266273bf8c50db21c", "message": "PHOENIX-4208 Modify tests to not use CurrentSCN"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/26a7b9e8b051c270ae8003332ed2ebb4bb91d972", "message": "PHOENIX-4180 Modify tests to generate unique table names and not use CURRENT_SCN (Rahul Shrivastava)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/9e5cd3ffe20779be9b1f3a8a5d28a0b05d664236", "message": "PHOENIX-4190 Salted local index failure is causing region server to abort"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/3392fa6fc2b8b0eb5ce683fb8bad4a421110b817", "message": "PHOENIX-4182 DatabaseMetaData.getTables throws exception when getting SEQUENCE information"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/63b71caadaeb7bebf66c20fb3a1ddc7c3adbff13", "message": "PHOENIX-4192 Remove unnecessary cast in TestPropertyPolicy"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/6775dc5c489bd002186e8b5375dd89b666836067", "message": "PHOENIX-4179 Use max timestamp of projected cells for cell timestamp returned to client"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/64b808971698880980d06f17b0924e6e22d95e12", "message": "PHOENIX-3953 Clear INDEX_DISABLED_TIMESTAMP and disable index on compaction (addendum 2)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/8c90a0bf4269aa83bbed26afa3437e2d444c68bb", "message": "PHOENIX-4162 Disallow transition from DISABLE to INACTIVE when INDEX_DISABLE_TIMESTAMP is zero"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/a1c75a9ec31eb646d7f4e0eb2363e8dc6d465103", "message": "PHOENIX-3953 Clear INDEX_DISABLED_TIMESTAMP and disable index on compaction (addendum)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/3f58452f4995490df81b74bb40dcc4e4d2a7329e", "message": "Revert \"PHOENIX-3815 Only disable indexes on which write failures occurred (Vincent Poon)\"\n\nThis reverts commit 7cf7b3abb10fc6a1ace0cb77615f054a8a6378f7."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/7cf7b3abb10fc6a1ace0cb77615f054a8a6378f7", "message": "PHOENIX-3815 Only disable indexes on which write failures occurred (Vincent Poon)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/6e6c0a390b7139c5ab2541d27871dbfb9c08c059", "message": "PHOENIX-418 Include APPROX_COUNT_DISTINCT in server jar for support of approximate COUNT DISTINCT (Ethan Wang)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/c83cec8903fd558777fb4dcd86a8bd67838f46e4", "message": "PHOENIX-4128 CNF org/apache/commons/math3/exception/OutOfRangeException (Ethan Wang)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/fc659488361c91b569f15a26dcbab5cbb24c276b", "message": "PHOENIX-2460 Implement scrutiny command to validate whether\n or not an index is in sync with the data table (Vincent Poon)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/d6381afc3af976ccdbb874d4458ea17b1e8a1d32", "message": "PHOENIX-418 Support approximate COUNT DISTINCT (Ethan Wang)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/435441ea8ba336e1967b03cf84f1868c5ef14790", "message": "PHOENIX-3953 Clear INDEX_DISABLED_TIMESTAMP and disable index on compaction"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/7d3368b62e91aa572cbb5c1e7083336d3f0c6538", "message": "PHOENIX-4124 Two new tests without Apache licenses"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/8b7a836a5e5502c1c217c6e3b6ab7298b980cb81", "message": "PHOENIX-4122 Prevent IllegalMonitorStateException from occurring when releasing row locks"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ss77892": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/3035fb11b8523c68b70e55d9a0fd1646eb6d15cf", "message": "PHOENIX-4525 Integer overflow in GroupBy execution"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/90c7241611667e3cd3689ce6a72762c6315231ef", "message": "PHOENIX-4456 queryserver script doesn't perform as expected."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/ee728a4d19c004ad456b24cd228fb2351362472d", "message": "PHOENIX-4439 QueryServer pid file name doesn't comply the usual schema we are using in hadoop ecosystem"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/aaa41a33d025ad6daa832fe8b42fc235e7154648", "message": "PHOENIX-3112 Partial row scan not handled correctly"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/033a2fc2a91052a6db94da55d87c173d4dbdabab", "message": "PHOENIX-4224 Automatic resending cache for HashJoin doesn't work when cache has expired on server side"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/764eb8f13237a89961e259aca366e45b38b76ef1", "message": "PHOENIX-4225 Using Google cache may lead to lock up on RS side."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/839be97e9ef2d23d3e9713313d4a93521bc74028", "message": "PHOENIX-4068 Atomic Upsert salted table with error(java.lang.NullPointerException)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/cec7e1cf8794e7ec0ee5c8be9a32e33cd211ec3b", "message": "PHOENIX-3406 CSV BulkLoad MR job incorrectly handle ROW_TIMESTAMP"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/9c458fa3d3ecdeb17de5b717c26cfdea1608c358", "message": "PHOENIX-3960 PhoenixStorageHandler for Hive doesn't work from Spark jobs"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/165", "title": "PHOENIX-2743 HivePhoenixHandler for big-big join with predicate push \u2026", "body": "Rebased on the current master as a single patch.\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/159", "title": "PHOENIX-2535 Create shaded clients (thin + thick)", "body": "Shaded client + PHOENIX-2267\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/150", "title": "PHOENIX-331 impementation", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "maryannxue": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/b3854d2c16fa102e087ecdfba6f366ba4d174dc6", "message": "PHOENIX-4508 Order-by not optimized in sort-merge-join on salted tables"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/412329a7415302831954891285d291055328c28b", "message": "PHOENIX-4437 Make QueryPlan.getEstimatedBytesToScan() independent of getExplainPlan() and pull optimize() out of getExplainPlan()"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/071fbce499c5303ffdcd4bbe25c1cda788aced0c", "message": "PHOENIX-3050 Handle DESC columns in child/parent join optimization"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/d77c237b560900671c3a9c58f6f2398342655e8a", "message": "PHOENIX-4322 DESC primary key column with variable length does not work in SkipScanFilter"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/6807dacce7d063e14f06bc57888e7d2a5f78863a", "message": "PHOENIX-4288 Indexes not used when ordering by primary key"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/45a9c275dbbf9206264236c690f40c309d97da3c", "message": "PHOENIX-4322 DESC primary key column with variable length does not work in SkipScanFilter (fix test failures)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/b0220fa7522fd7e1848ad428a47121b205dec504", "message": "PHOENIX-4322 DESC primary key column with variable length does not work in SkipScanFilter"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/281", "title": "PHOENIX-4288 Indexes not used when ordering by primary key", "body": "1. Add class Cost.\r\n2. Add method getCost() in QueryPlan.\r\n3. Let QueryOptimizer choose the best plan based on Cost; meanwhile if stats are not available the QueryOptimizer will keep the original behavior.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/278", "title": "PHOENIX-4322 DESC primary key column with variable length does not work in SkipScanFilter", "body": "Changes:\r\nAvoid adding an extra trailing separator to the key", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/228", "title": "PHOENIX-3355 Register Phoenix built-in functions as Calcite functions", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "twdsilva": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/f7142879f33cae236e0530a8ed4eeaad1542d66a", "message": "PHOENIX-4473 Exception when Adding new columns to base table and view diverge (addendum)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/ae21f87338de5c80efd8e89b256415faad0e00a3", "message": "PHOENIX-4473 Exception when Adding new columns to base table and view diverge (Ankit Singhal)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/9355a4d262d31d8d65e1467bcc351bb99760e11d", "message": "PHOENIX-4468 Looking up a parent index table of a child view from a different client fails (addendum)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/c935f57f5e7afb42ff8b62a4712ad7d8ffed17cc", "message": "PHOENIX-4468 Looking up a parent index table of a child view from a different client fails"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/9d8be0e9214ba3680a81c399c5da316c1b91c99b", "message": "PHOENIX-4460 High GC / RS shutdown when we use select query with IN clause using 4.10 phoenix client on 4.13 phoenix server"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/25359a95f1a8c5aa6e852e750fd6da6e10249387", "message": "PHOENIX-4386 Calculate the estimatedSize of MutationState using Map<TableRef, Map<ImmutableBytesPtr,RowMutationState>> mutations (addendum)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/355ee522c1d4ff07cf9fbb0a9a01e43e3f702730", "message": "Revert \"PHOENIX-4386 Calculate the estimatedSize of MutationState using Map<TableRef, Map<ImmutableBytesPtr,RowMutationState>> mutations (addendum)\"\n\nThis reverts commit 4e0c0a33ed8b401f7785dde8979041dd5ab9a1f4."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/4e0c0a33ed8b401f7785dde8979041dd5ab9a1f4", "message": "PHOENIX-4386 Calculate the estimatedSize of MutationState using Map<TableRef, Map<ImmutableBytesPtr,RowMutationState>> mutations (addendum)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/4944081ec093204a859d8c9ba57606e4e59bd0fb", "message": "PHOENIX-4386 Calculate the estimatedSize of MutationState using Map<TableRef, Map<ImmutableBytesPtr,RowMutationState>> mutations"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/ef3bce18fe7373b66136d933cc364001dff2c3f8", "message": "PHOENIX-4381 Calculate the estimatedSize of MutationState incrementally"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/2053905683409225ffdc1c0ae4fc6c759604a80d", "message": "PHOENIX-4379 Upgrade code to create CHILD links should only create the links for views and not for indexes"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/8f9356a2bdd6ba603158899eba38750c85e8e574", "message": "PHOENIX-3460 Namespace separator : should not be allowed in table or schema name"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/fe13b257e5dfe29581b1c3265d79596f194954cd", "message": "PHOENIX-4292 Filters on Tables and Views with composite PK of VARCHAR fields with sort direction DESC do not work (addendum)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/0461fe855aaed27e23cb5d17e8be022a82626162", "message": "Filters on Tables and Views with composite PK of VARCHAR fields with sort direction DESC do not work"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/024f407f25208e396908b2d783166e0fbf22c1dc", "message": "PHOENIX-3556 Remove usage of com.google.common.collect.Iterators.emptyIterator()"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/d1df8893f9d57a49e1272e201c17c8bc56683be2", "message": "PHOENIX-4259 Split up IndexExtendedIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/22f336d52003d7a6368552b6f3b8454b296ad1e1", "message": "PHOENIX-4187 Use server timestamp for ROW_TIMESTAMP column when value is not specified (addendum)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/45079c4ea41b78ef71efe7d27c5de8026415c5c3", "message": "PHOENIX-4187 Use server timestamp for ROW_TIMESTAMP column when value is not specified"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/3fb104ee56518ca066672e199bff1ed4f4bd9a7e", "message": "PHOENIX-4218 Remove usage of current scn from UserDefinedFunctionsIT.testUDFsWithLatestTimestamp"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/7e45e9ea1eee5c71cb14ed1eb7d54c05dc91b46a", "message": "PHOENIX-4186 Modify NativeHBaseTypesIT to not use CurrentSCN"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/534fa05dad66cc8aedfcbc6622bfbbd8ed714b73", "message": "PHOENIX-4022 Add PhoenixMetricsLog interface that can be used to log metrics for queries and mutations"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/e05b7cdd3286d7021d5836ecf7446a1c2aed0b9c", "message": "PHOENIX-3978 Expose mutation failures in our metrics"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/f5f32e3b8d111b47c3f5c83f4dfff4e4e7af6863", "message": "PHOENIX-3986 UngroupedAggregateRegionObserver.commitBatch() should set the index metadata as an attribute on every mutation"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/84c825066ac44515b7c3f445bdef8828153d48ff", "message": "PHOENIX-3734 Refactor Phoenix to use TAL instead of direct calls to Tephra (addendum)"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/73304997", "body": "@JamesRTaylor  \nClosing this pull request as it has been committed to master and 4.0\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/73304997/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/115383349", "body": "@petercdc  Can you please rebase this pull request with the json branch? @AakashPradeep had already implemented the json data type.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/115383349/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/124247112", "body": "@ictwanglei  Can you please rebase and upload new patch, this one does not apply cleanly to the json branch?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/124247112/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/124781109", "body": "Nice work @petercdc  Can you please use this JIRA for the pull request https://issues.apache.org/jira/browse/PHOENIX-2144 ? \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/124781109/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/125361795", "body": "@petercdc can you create a JIRA user, so you can be assigned to the issue?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/125361795/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128525943", "body": "@petercdc  Thanks for the updated pull request. @JamesRTaylor  @AakashPradeep Do you have any additional feedback? \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128525943/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/156588650", "body": "Created new pull PR\nhttps://github.com/apache/phoenix/pull/127\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/156588650/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/158247567", "body": "For the last few files that aren't showing up in the diff view , you can view the diffs here\nhttps://patch-diff.githubusercontent.com/raw/apache/phoenix/pull/130.patch\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/158247567/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/159137696", "body": "@samarthjain  @JamesRTaylor \nThanks for the review, I will make the remaining changes on commit.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/159137696/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/phoenix/comments/20281656", "body": "After the rebase the following assert in AlterTableIT.testAddingColumnsToTablesAndViewsWithEncodedColumns was failing.\r\n\r\n```\r\nassertNull(\"A view should always have the column qualifier counter as null\", view.getEncodedCQCounter().getNextQualifier(DEFAULT_COLUMN_FAMILY));\r\n```\r\n\r\n This was because after the rebase when you add a column or update a table property, the new PTable is sent from the server to the client and this is added to the client cache. Previously we just used to modify the PTable on the client side. \r\n\r\nThe createFromProto code to deserialize the PTable was setting the encodedColumnQualifierCounter to  new EncodedCQCounter() (even if the table is a view) which caused the assert to fail.", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/20281656/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22988143", "body": "Is this the correct way to get the index column byte size for an expression?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22988143/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22988189", "body": "Is it ok to serialize/deserialize indexedExpressions this way or will this break compatiblilty?   \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22988189/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22988222", "body": "I am not sure how to estimate the size of indexedExpressions\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22988222/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22988250", "body": "Is it ok to set the maxLength and scale to null?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22988250/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23117945", "body": "If I don't have the CAST, the indexed would not be used. This is because date1 and date2 and NULLABLE and since DATE is fixedwidth IndexUtil.getIndexColumnDataType converts the data type to Decimal .\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23117945/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23118093", "body": "When I parse expressions in IndexMaintainer (line 331), I first use SQLParser.parseCondition to create a parse node and then create an ExpressionCompiler and create the expression.\n At this point, I don't have access to a connection so I set it to null. The connection was only being used to get the tenant id. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23118093/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23265883", "body": "I added a getEstimatedByteSize() to Expression and implemented this method in the subclasses to get the estimated byte size()\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23265883/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23266265", "body": "I added this so that we don't need to specify a CAST in order to use the index (similar to the check in IndexStatementRewriter)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23266265/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23267987", "body": "In IndexColumnExpressionCompiler, all the visitEnter methods check if the ParseNode matches an expression in the index, if there is a match  it returns false for visitEnter. Then in the visitLeave I call convertAndVisitParseNode which creates a ColumnParseNode for the column that contains the matching expression and then calls the visit method on the column parse node. \n\nInstead of this, should I only check if there is a match in the visitLeave ?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23267987/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23268273", "body": "I changed this to use expression.toString().\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23268273/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23268496", "body": "indexedExpressions contain both regular columns and expressions. For regular columns the expression will be a ColumnExpression. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23268496/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23270696", "body": "I added indexedColumnTypes back and used the sign of the length of emptyKeyValueCFPtr to differentiate between old and new clients. \nIndexMaintainer has changed quite a lot, If an old client is talking to a server that has been upgraded, will I have to maintain the old behavior? \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23270696/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275462", "body": "I modified the code to use expression.getMaxLength() and expression.getScale()\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275462/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275475", "body": "I modified the code to use expression.getMaxLength() and expression.getScale().\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275475/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275488", "body": "I modified it to set the expression string correctly.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275510", "body": "I modified the code to set the expression string correctly.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275510/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275567", "body": "I move this outside so that this bit is set correctly for expressions as well. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275567/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275593", "body": "I fixed this so that we the sort order is used correctly for expressions. I will add a test for this as well.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275593/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275602", "body": "I made both these changes (determinism and aggregate check). I will add a test for these.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275602/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23276893", "body": "I created a map from expression string to expression and just look up the expression in the visitor methods.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23276893/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23663726", "body": "I have to create a KeyValueColumnExpression for every column reference in indexedColumns, but this requires a PColumn \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23663726/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23663821", "body": "This is needed so that expressions that refer to case senstive columns work correctly.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23663821/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23663994", "body": "used to keep track of whether a column family is case sensitive\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23663994/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23664424", "body": "I have removed this check.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23664424/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23664456", "body": "I have removed the null check.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23664456/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23717459", "body": "I added 3 date columns to INDEX_DATA_TABLE and MUTABLE_INDEX_DATA_TABLE, so I just added default values for these columns.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23717459/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23717484", "body": "Its not required, I will remove it.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23717484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23717555", "body": "When I create the phoenix connection I use\n\n PhoenixConnection connection = QueryUtil.getConnection(env.getConfiguration()).unwrap(PhoenixConnection.class);\n\nwhich throws ClassNotFoundException\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23717555/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23718870", "body": "If I don't initialize indexedColumnTypes here, I would need to initialize it in the constructor , so I would have to duplicate the loop over expressions and calling KeyValueExpressionVisitor in the constructor.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23718870/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23719273", "body": "I need numPkColumnsAdded to initialize the pkColumns list later . \nThe boolean isAddingPKColumn can be derived from numPkColumnsAdded, so I removed it, I can add it back using the boolean makes it more readable\n\n// create PK column list that includes the newly created columns\n                        List<PColumn> pkColumns = Lists.newArrayListWithExpectedSize(table.getPKColumns().size()+numPkColumnsAdded);\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23719273/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "samarthjain": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/72bc8190272480fae17475398ef492a3071a3a44", "message": "PHOENIX-4397 Incorrect query results when with stats are disabled on a salted table"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/895d067974639cd2205b14940e4e46864b4e2060", "message": "PHOENIX-4287 Add null check for parent name"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/7d2205d0c9854f61e667a4939eeed645de518f45", "message": "PHOENIX-4287 Make indexes inherit use stats property from their parent table or view"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/61684c4431d16deff53adfbb91ea76c13642df61", "message": "PHOENIX-4332 Indexes should inherit guide post width of the base data table"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/87b6628674042df285c775e8b0d8259d3c0780a0", "message": "PHOENIX-4343 In CREATE TABLE allow setting guide post width only on base data tables"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/82364f6b3083d309f2035f1fd6d132a77ecef71a", "message": "PHOENIX-4287 Addendum to correctly set useStatsForParallelization property"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/a39633169b75ceef340782379dd6c3c51d960142", "message": "PHOENIX-4333 Test to demonstrate partial stats information for tenant views"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/97fe4f8aa24d2b0cdf9d1418252b4d69cfb6e7a1", "message": "PHOENIX-4287 Incorrect aggregate query results when stats are disable for parallelization"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/60a9b099eccaf328fd796b93176d8ac665fe039c", "message": "PHOENIX-4289 UPDATE STATISTICS command does not collect stats for local indexes"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/68b5db54d6dd79d521cf29dd84cef8e9b362cbf4", "message": "PHOENIX-4273 Don't run MutableIndexSplitIT tests till fixed"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/54e2bbf83ba3493dc70eb3d68965453475da2f73", "message": "PHOENIX-4272 Dial down number of threads and repeats in ConcurrentMutationsIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/119f86e0c29ed6331df35028d37f6964393f122b", "message": "PHOENIX-4265 NPE when ROW_TIMESTAMP is SQL timestamp column"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/cfe4c9356d0cb9ef16fc297ea1192d35b95862fd", "message": "PHOENIX-4260 Breakup NotQueryIT into multiple test classes"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/7d22ca582665facda4d075bc27b64f7c0f8b5821", "message": "PHOENIX-4255 Addendum"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/86dae1ed7bc3101aa9fe3f780e31b02cacad02e2", "message": "PHOENIX-4255 Breakup StatsCollectorIT into several integration tests to limit number of tables created"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/a4ce14badb00eb51c34059c602e33e1a889877e5", "message": "PHOENIX-4243 Using rowtimestamp column with RVC causes an exception sometimes"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/84dc1d44ad8cdee9e89bf0a63c411669dc3ec2fd", "message": "PHOENIX-4007 Addendum to add null check when closing statsWriter"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/6d8357e9029e639de952d76493203e161e237adb", "message": "PHOENIX-4007 Surface time at which byte/row estimate information was computed in explain plan output"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/41e137199a9e42c080475c96653be6e9fc451b99", "message": "PHOENIX-4202 Clear client side cache in ConnectionQueryServicesImpl#clearCache()"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/b53de204104bbd0262ed8fd7e0341f147698f1f1", "message": "PHOENIX-4201 Addendum to clear cache in deleteMetadata"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/790e8d4d2b84a0e234609b2d8420e3ff398f0dee", "message": "PHOENIX-4201 Remove usage of SCN from QueryDatabaseMetadataIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/1d4025a57daedeebb64ef735b1055ebc9c6b2131", "message": "PHOENIX-4196 Remove SCN usage from ToNumberFunctionIT, TruncateFunctionIT and UpdateCacheAcrossDifferentClientsIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/0e79023088bc19b6cbdb022149c3f80285a5d25c", "message": "PHOENIX-4184 Reduce flappiness of FlappingAlterTableIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/814276d4b4b08be0681f1c402cfb3cc35f01fa0a", "message": "PHOENIX-4177 Convert TopNIT to extend ParallelStatsDisabledIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/28aebd6af3b635c98c8f1782295ea6c85167d659", "message": "PHOENIX-4171 Creating immutable index is timing out intermittently"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/134424ebd44f730344ff5da93a6ec3f734d77d4b", "message": "PHOENIX-4170 Remove rebuildIndexOnFailure param from MutableIndexFailureIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/64658fe5a64e7089f5208ece25769bf644f96846", "message": "Revert \"PHOENIX-4170 Remove rebuildIndexOnFailure param from MutableIndexFailureIT\"\n\nThis reverts commit dd5642ff55cbc829765114d0be051cb48081e4a6."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/dd5642ff55cbc829765114d0be051cb48081e4a6", "message": "PHOENIX-4170 Remove rebuildIndexOnFailure param from MutableIndexFailureIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/cdfb08bd8862693b4826c4714259a63737dd2c3f", "message": "PHOENIX-4151 Addendum to fix test failure"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/a3bb174bc82743aa2ac7b59b9d1978a788a4d19f", "message": "PHOENIX-4151 Tests extending BaseQueryIT are flapping"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/27ef19f8ce5b956deb795a45ba04ca865fef7ad9", "message": "PHOENIX-4141 Fix flapping TableSnapshotReadsMapReduceIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/6e9ce8742e73eccb0d7678ae8f09be9e76b9be98", "message": "PHOENIX-4156 Fix flapping MutableIndexFailureIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/f03c43bdd2abb18f3a1edbc717340b52b529fb48", "message": "PHOENIX-4155 Convert CreateTableIT to extend ParallelStatsDisabledIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/21630d85f06d12db4b75776eb708814a0c360ec5", "message": "Revert \"PHOENIX-4141 Fix flapping TableSnapshotReadsMapReduceIT\"\n\nThis reverts commit 378b56c4ad71f9e10887adec92618300285f6d2d."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/38915feef5f590a882cdc45c7e880943ab099603", "message": "Revert \"PHOENIX-4141 Addendum to fix test failure\"\n\nThis reverts commit 7d8b8430212fae117ac09faf6b7c22bf673e9073."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/0ace91f715e5136404774f98d1441ba639618248", "message": "Revert \"PHOENIX-4141 Fix flapping TableSnapshotReadsMapReduceIT\"\n\nThis reverts commit 9fb318bd8f03911bc7cb333a0ec974a685156fb7."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/01dbd123bd70fec5391531f833ba9b73e1178f2b", "message": "PHOENIX-4110 Shutdown mini cluster when number of tables grows beyond a threshold"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/a8f119cb696b012dc1338f1abe82c8fbc9cfea4e", "message": "PHOENIX-4152 Don't swallow or wrap exception in BaseQueryIT constructor"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/9fb318bd8f03911bc7cb333a0ec974a685156fb7", "message": "PHOENIX-4141 Fix flapping TableSnapshotReadsMapReduceIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/81c025b37109059cc7d00d1bb558b8e794b60723", "message": "PHOENIX-4153 Temporarily disable java doc warning check in QA builds"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/1aabbfa0d04261d1cebba845deaebf4c4048ae62", "message": "PHOENIX-4143 ConcurrentMutationsIT flaps"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/c2e85f2131669c381e61cc3d6982ab66e4ed63b9", "message": "PHOENIX-4131 UngroupedAggregateRegionObserver.preClose() and doPostScannerOpen() can deadlock"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/7d8b8430212fae117ac09faf6b7c22bf673e9073", "message": "PHOENIX-4141 Addendum to fix test failure"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/e6c1f01c5f7d5c9996017714efe90202a95355b2", "message": "Revert \"PHOENIX-4140 Disable HiveTezIT and HiveMapReduceIT since they don't work most of the times\""}, {"url": "https://api.github.com/repos/apache/phoenix/commits/378b56c4ad71f9e10887adec92618300285f6d2d", "message": "PHOENIX-4141 Fix flapping TableSnapshotReadsMapReduceIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/4ca7a0791841ba504ac3daac9fc6e8fec0c148e6", "message": "PHOENIX-4140 Disable HiveTezIT and HiveMapReduceIT since they don't work most of the times"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/437402d4850bebcd769858b756c1e08abf544b00", "message": "PHOENIX-4132 TestIndexWriter hangs on 1.8 JRE"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/a5fb51948225ee7ee9ece76d233f4a150006e4a9", "message": "PHOENIX-4126 Reset FAIL_WRITE flag in MutableIndexFailureIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/dd008f4fe4e67836a6fb362843c9618e4e518182", "message": "PHOENIX-4118 Disable tests in ImmutableIndexIT till PHOENIX-2582 is fixed"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/608160e61c3951287bdb27f08d9463b82862c362", "message": "PHOENIX-4103 Convert ColumnProjectionOptimizationIT to extend ParallelStatsDisabledIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/7348da45b42d3fc02b81f7bed357730fb8ddd0dd", "message": "PHOENIX-4102 Convert ArrayIT to extend BaseUniqueNamesOwnClusterIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/c92ddc451f93f34144be9aeda7cb9cedece450b3", "message": "PHOENIX-4098 BaseUniqueNamesOwnClusterIT doesn't need to tear down minicluster"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/bff368d493c41cc9b71dfd70b3261ff54de6938c", "message": "PHOENIX-4041 CoprocessorHConnectionTableFactory should not open a new HConnection when shutting down"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/d87931c02bcfabfb3442b11afdaf9614ed9d17de", "message": "PHOENIX-4039 Increase default number of RPC retries for our index rebuild task"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/269e8c47f0093912a8568817dd1347945cc22429", "message": "PHOENIX-4038 Get rid of IndexHandlerIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/40e438edcd797d4803f5cc1c993bd421592fa9e0", "message": "PHOENIX-4032 Make MutableIndexFailureIT more resilient"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/48341ae3fcc645aa7f559ae98606c522c563268d", "message": "PHOENIX-4027 Addendum - move testRebuildIndexConnectionProperties to its own class. Fix typo for number of rpc retries"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/18ea6edc00029e7e900ad95562fa73da0e5ccf51", "message": "PHOENIX-3994 Addendum - set index rpc controller factory for transactional indexer"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/bf83b8dd65d8c0110456d352bed96fa18676886a", "message": "PHOENIX-4030 Decrease TEAR_DOWN_THRESHOLD in ParallelRunListener to prevent OOM in tests"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/d541d6f2875a590580e8ccf05f26795083b06658", "message": "PHOENIX-4027 Mark index as disabled during partial rebuild after configurable amount of time"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/3477977f3589e5458013715652953a6aa1697814", "message": "PHOENIX-4024 Renew lease thread names should be unique across various ConnectionQueryServices instances"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/ce52c37f0b22c35a98d50193ed5647e44eb27e0b", "message": "PHOENIX-3947 Increase scan time out for partial index rebuild and retry only once"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/8a34de7a4c74584f5fb3a7ab9aaa64fd6269717b", "message": "PHOENIX-3994 Index RPC priority still depends on the controller factory property in hbase-site.xml"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/64b9a6c5d5aacb13cd2a0fd65589435f7acd155a", "message": "Revert \"PHOENIX-4016 Comment out handler count overrides and reduce number of forked JVMs in tests\"\n\nThis reverts commit 06c10dc391220da8e54f0191bef1c9b8b702f651."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/06c10dc391220da8e54f0191bef1c9b8b702f651", "message": "PHOENIX-4016 Comment out handler count overrides and reduce number of forked JVMs in tests"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/caeaec8355023ce93656f56ad6dfa77d5dbc3517", "message": "PHOENIX-4015 Change UpsertSelectIT to extend ParallelStatsDisabledIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/4aac7fce629109d653384abcad38d48e15abb079", "message": "Revert \"PHOENIX-3994 Index RPC priority still depends on the controller factory property in hbase-site.xml\""}, {"url": "https://api.github.com/repos/apache/phoenix/commits/c9bc3a7e5e380b0f6225091429976c8d705d15d1", "message": "PHOENIX-3994 Index RPC priority still depends on the controller factory property in hbase-site.xml"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/a752cd14851f1b1730c0ea3043a6dd0017fac5fc", "message": "PHOENIX-4000 Increase zookeeper session timeout in tests to prevent region server aborts"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/77509422266adfc190892bd57faf8b81b072519e", "message": "PHOENIX-3938 Don't throw IOException out of coprocessor hooks"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/78348649", "body": "This is fantastic @codymarcel  and @mujtabachohan. Just a few concurrency issues to be ironed out, otherwise looking good!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/78348649/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/87022884", "body": "@gabrielreid - does this look good for commit now? We are planning on cutting the RC for 4.3.1 pretty soon. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/87022884/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/87109980", "body": "Tests ran fine for me locally so I went ahead and committed the patch.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/87109980/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/93872039", "body": "+1, LGTM. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/93872039/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/96133889", "body": "Almost there Rajesh. Just some minor nits and questions, otherwise LGTM.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/96133889/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/158249482", "body": "The check in TableRef.equals() isn't complete. It should be something like this:\n\nif (((table.getName() == null && other.table.getName() != null)\n||(table.getName() != null && other.table.getName() == null) \n|| !table.getName().getString().equals(other.table.getName().getString()))) \nreturn false;\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/158249482/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/159119469", "body": "+1. This is fantastic work, Thomas! Other than a few minor nits and comments, this looks great!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/159119469/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/phoenix/comments/20281443", "body": "hey Thomas, can you tell me more about the test failure that this fixes. My local test run was successful. Did it fail after the rebase? If so, which test/s? ", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/20281443/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15389352", "body": "Looks like writer can be final. Also, is this class meant to be thread safe? If yes, maybe annotate with @ThreadSafe? \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15389352/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "aertoria": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/1c3387d0eb2fcec2423dda029aa65ca66f547416", "message": "PHOENIX-3837 Feature enabling to set property on an index with Alter statement\n\nSigned-off-by: aertoria <castives@gmail.com>"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/bab06d688acde9801ae171420f94871b8e78684f", "message": "PHOENIX-4283 fix a coearceByte issue which causes nested group by big int incorrect\n\nSigned-off-by: aertoria <castives@gmail.com>"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/2ad5d4b48c16743b3f3968a858f9da19c14070fa", "message": "PHOENIX-4150 nit: adding license and reformating comments\n\nSigned-off-by: Thomas D'Silva <tdsilva@apache.org>"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/481440d08a409de4067619d3a7cd333eeef6e717", "message": "PHOENIX-4150 Adding property policy for user defined properties\n\nSigned-off-by: Thomas D'Silva <tdsilva@apache.org>"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/74762df7a5649592ad81ca4ad72425acc2b0a823", "message": "PHOENIX-4080 Amending error message when phoenix client version is not match with servers\n\nSigned-off-by: Geoffrey Jacoby <gjacoby@apache.org>"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/287", "title": "PHOENIX-4370 Surface hbase metrics from perconnection to global metrics", "body": "PHOENIX-4370 Surface hbase metrics from perconnection to global metrics\r\n\r\nOpening this p.r. for the connivence of discussion", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ChinmaySKulkarni": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/ddd3ef28d36f963b295cef74db372f13040ea22a", "message": "PHOENIX-4361: Remove redundant argument in separateAndValidateProperties in CQSI\n\nSigned-off-by: aertoria <castives@gmail.com>"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/282", "title": "PHOENIX-4361: Remove redundant argument in separateAndValidateProperties in CQSI", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gjacoby126": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/1d8a6bc3a6a277d9e3201066b753fa9fd7018545", "message": "PHOENIX-4342 - Surface QueryPlan in MutationPlan"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/ff80555537ef103f73258115fc766bbe89c430a0", "message": "PHOENIX-4229 - Parent-Child linking rows in System.Catalog break tenant view replication"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/bf3c13b6d169e90e06c54a338a5931163a32d743", "message": "PHOENIX-4066 - Unused variable in MutationState.send()"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ankitsinghal": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/217867c78108b29d991794726c01c1eefb49b828", "message": "PHOENIX-4198 Remove the need for users to have access to the Phoenix SYSTEM tables to create tables"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/7865a59b0bbc4ea5e8ab775b920f5f608115707b", "message": "PHOENIX-4010 Hash Join cache may not be send to all regionservers when we have stale HBase meta cache"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/b13413614fef3cdb87233fd1543081e7198d685f", "message": "PHOENIX-4083 Incorrect conditional when including OFFSET into explain plan (addendum)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/5c7822df3b5a2aaf7a6b3ab6059ad8bda2900139", "message": "PHOENIX-4083 Incorrect conditional when including OFFSET into explain plan (JC)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/b0109feb92fdd9e19bb6f70412d0c476ec60d3d4", "message": "PHOENIX-3944 ReadOnlyTableException occurs when we map Phoenix view to an existing HBase table with Namespace Mapping enabled (Toshihiro Suzuki)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/268", "title": "PHOENIX-4010 Hash Join cache may not be send to all regionservers whe\u2026", "body": "\u2026n we have stale HBase meta cache", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/211", "title": "PHOENIX-3254 IndexId Sequence is incremented even if index exists alr\u2026", "body": "\u2026eady\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/210", "title": "PHOENIX-2890 Extend IndexTool to allow incremental index rebuilds", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/167", "title": "PHOENIX-2862 Do client server compatibility checks before upgrading system tables", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/172089737", "body": "Thanks @JamesRTaylor  for the review.\nI have made the changes you have suggested above except this one.\nbyte[] currentGuidePostBytes = SchemaUtil.copyKeyIfNecessary(currentGuidePost);\n\nAs PrefixByteDecoder updates the previous buffer only whenever maxLength is passed as a part of optimization.\n\npublic ImmutableBytesWritable decode(DataInput in) throws IOException {\n        int prefixLen = WritableUtils.readVInt(in);\n        int suffixLen = WritableUtils.readVInt(in);\n        int length = prefixLen + suffixLen;\n        byte[] b;\n        if (maxLength == -1) { // Allocate new byte array each time\n            b = new byte[length];\n            System.arraycopy(previous.get(), previous.getOffset(), b, 0, prefixLen);\n        } else { // Reuse same buffer each time\n            b = previous.get();\n        }\n        in.readFully(b, prefixLen, suffixLen);\n        previous.set(b, 0, length);\n        return previous;\n    }\n\nso I need to copy bytes even if the length of the ImmutableByteWritable is equal to byte[] contained in it.\n\nWhat do you think about this?\n\nAnd also I have fixed the failure test cases as well. there was logical operator problem while incrementing \nguidePosts till the start key.\n\nI have run the complete test suite now and will confirm you once it is completed.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/172089737/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/172366554", "body": "Made the changes in GuidePostInfoBuilder as suggested and attach a wip patch for stats upgrade from server side on ticket. \nPlease review and confirm if I'm on the right track.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/172366554/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/phoenix/comments/22672204", "body": "Do we really need to catch exception here. It can be thrown from the caller right.\r\n@Override\r\n      public Tuple next() throws SQLException {", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/22672204/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/22672267", "body": "can't we use the same statement . something like this?\r\nFETCH p=NEXT|p=PRIOR (a=NUMBER)? (ROW|ROWS)? FROM c=cursor_name {ret = factory.fetch(c,p!=null, a == null ? 1 :  Integer.parseInt( a.getText() )); }", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/22672267/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/22672273", "body": "read config in constructor ", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/22672273/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/22672590", "body": "please remove sys.out ", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/22672590/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/22674761", "body": "Add a test case when first call itself is prior", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/22674761/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/22674824", "body": "please update all the test cases with smaller result cache size (like 2)", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/22674824/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/22674920", "body": "add a similar test case with FETCH NEXT 1000 (more than no. of rows in table) and confirm FETCH PRIOR", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/22674920/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/22676177", "body": "can't we use double linked list instead of two stacks to avoid data copy and for better management.\r\nCheck and use appropriate implementation of java.util.ListIterator", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/22676177/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/22676426", "body": "There is a duplication of code, please move them to methods.\r\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/22676426/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/22676567", "body": "code looks brittle here.  can you please refactor it and move this cache logic in different iterator", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/22676567/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/22676582", "body": "can you please add a comment for each conditional path.", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/22676582/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "JamesRTaylor": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/2a81efeb71f00c60f59ca7a5f912b57318c24f68", "message": "PHOENIX-4257 Breakup GroupByIT,MutableQueryIT, and QueryIT into several integration tests so as not to create too many tables in one test"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/ccf98c2201584d1551964c12ae30fb90f91deac5", "message": "PHOENIX-4257 Breakup GroupByIT,MutableQueryIT, and QueryIT into several integration tests so as not to create too many tables in one test"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/176f541ceb36c74ecdb88d113132a4ff2e44a86b", "message": "PHOENIX-4239 Fix flapping test in PartialIndexRebuilderIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/4969794bf0c91305d01c8f016abe95039642d46e", "message": "PHOENIX-4138 Create a hard limit on number of indexes per table (Churro Morales)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/df5b0d213887ef79b83d1e4dbf15dce8d128272f", "message": "PHOENIX-4212 Disallow DML operations on connections with CURRENT_SCN set - DerivedTableIT (Ethan Wang)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/415c6e3523bc42f82178c163b891fc5eb22f6a96", "message": "PHOENIX-4212 Disallow DML operations on connections with CURRENT_SCN set - DerivedTableIT (Ethan Wang)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/586cdab9f82661a40fa12be881b6adfed034b09c", "message": "PHOENIX-4185 Rewrite tests to disable DDL and DML for PercentileIT.java and ProductMetricsIT.java (Ethan Wang)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/2e5986a76aa171a62b342a46cc984a00a3a36746", "message": "PHOENIX-4169 Explicitly cap timeout for index disable RPC on compaction (Vincent Poon)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/33b12c789e5c24cbf4d711006e4b8c41e9393a78", "message": "PHOENIX-4205 Modify OutOfOrderMutationsIT to not use CURRENT_SCN"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/b46cbd375e3d2ee9a11644825c13937572c027cd", "message": "PHOENIX-4175 Convert tests using CURRENT_SCN to not use it when possible"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/6c5bc3bba7732357bf3fc4ab39e7fda10e97539e", "message": "PHOENIX-4173 Ensure that the rebuild fails if an index that transitions back to disabled while rebuilding"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/fd893ef47be0780ab8b7c5991426092fd504b322", "message": "PHOENIX-4114 Derive ConcurrentMutationsIT from ParallelStatsDisabledIT"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/813c1c8aa0f44f0b38ef5bb4c9ed418f693b5c0d", "message": "PHOENIX-4109 Ensure mutations are processed in batches with same time stamp during partial rebuild"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/a0f47c2bec568ebcfca6be2fe8e0fd3af9c01a60", "message": "PHOENIX-4106 Replace System.currentTimeMillis() calls with EnvironmentEdgeManager.currentTimeMillis() calls"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/1dc900884896867d6e641d1c9470eacac133cf91", "message": "PHOENIX-4105 Fix tests broken due to PHOENIX-4089 (addendum 1)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/afcebdcae5270e91eefc60eed1da2095d12c4580", "message": "PHOENIX-4105 Fix tests broken due to PHOENIX-4089 (addendum)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/5c1b1b55ee6ac9010f2e4ee0f9d31ba42795dd31", "message": "PHOENIX-4105 Fix tests broken due to PHOENIX-4089"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/203a57f48c3145011fc4fcd6ab214ed204ac0101", "message": "PHOENIX-4099 Do not write table data again when replaying mutations for partial index rebuild"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/b3473022cae55d2f73f961372ee1826de31e1382", "message": "PHOENIX-4099 Do not write table data again when replaying mutations for partial index rebuild (addendum)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/649b737a81243adc43b508a90addc9a2962c6bc1", "message": "PHOENIX-4095 Prevent index from getting out of sync with data table during partial rebuild"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/ce6b891fd658f6593845d1155509d0f8a599336f", "message": "PHOENIX-4089 Prevent index from getting out of sync with data table under high concurrency"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/b2c8e34f1bed9dc2e68a719c3ec5470c5dc595bf", "message": "PHOENIX-4070 Delete row should mask upserts at same timestamp (addendum for typos)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/19de020b66706493ae98eaa45282504a559f2f2e", "message": "PHOENIX-3525 Cap automatic index rebuilding to inactive timestamp (addendum)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/29b76635944c717a5226e001abd85c045287344a", "message": "PHOENIX-4074 Race condition in LazyValueGetter (Samarth Jain)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/30224d34b72f52f896c2d1b94c3f675c3c9eef32", "message": "PHOENIX-3525 Cap automatic index rebuilding to inactive timestamp"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/5320304f2a013570e9c671b741658d5c8e187890", "message": "PHOENIX-4073 Disallow operations on a closed Connection"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/d762c6a83e8260297fd8d6afcb51bc1c49dce23f", "message": "PHOENIX-4072 Prevent NPE for PreparedStatement.setObject of null"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/1e74895ad83dfe1ada90897f95fb5c93e2cc8eee", "message": "PHOENIX-4071 PDataType.compareTo(Object,Object,PDataType) does not handle null values"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/78f35a65d0cc55014c51da70c847969a2ea3c5a6", "message": "PHOENIX-4070 Delete row should mask upserts at same timestamp"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/0bd43f536a13609b35629308c415aebc800d6799", "message": "PHOENIX-4059 Index maintenance incorrect when indexed column updated from null to null with STORE_NULLS=true"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/e494fe9fa744ec2948cf75b007e92fef0b1ba829", "message": "PHOENIX-4057 Do not issue index updates for out of order mutation"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/5e33dc12bc088bd0008d89f0a5cd7d5c368efa25", "message": "PHOENIX-153 Implement TABLESAMPLE clause (Ethan Wang)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/54d9e1c36c46e7c50c29def08cf866599c7a4e45", "message": "PHOENIX-4053 Lock row exclusively when necessary for mutable secondary indexing"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/ba0a67f827c40a3ce2974970e49e1760c164161a", "message": "PHOENIX-4028 Provide option to not throw index write failure back to client"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/7220592ff43235431c6d2909094e5e832665a73d", "message": "PHOENIX-4028 Provide option to not throw index write failure back to client"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48488374", "body": "Thanks for the pull, @chrajeshbabu. Nice work! Looks like you need to rebase as there are merge conflicts currently.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48488374/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48496677", "body": "Excellent job, @chrajeshbabu! A few minor issues, but overall this is great! Looking forward to seeing how it performs!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48496677/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48825943", "body": "Related to the revert of the ScanRanges change, you'll need to make this change to prevent the SkipRangeParallelIteratorRegionSplitter from being used (as you always need to scan all regions for a local index). Cleanest might be to just implement a simple ParallelIteratorRegionSplitter for use when a local index is used that just returns all regions:\n\n```\npublic class ParallelIteratorRegionSplitterFactory {\n\n    public static ParallelIteratorRegionSplitter getSplitter(StatementContext context, TableRef table, HintNode hintNode) throws SQLException {\n        if (!isLocalIndex && context.getScanRanges().useSkipScanFilter()) {\n            return SkipRangeParallelIteratorRegionSplitter.getInstance(context, table, hintNode);\n        }\n        return DefaultParallelIteratorRegionSplitter.getInstance(context, table, hintNode);\n    }\n}\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48825943/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48839791", "body": "Sounds good, @chrajeshbabu. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48839791/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48853884", "body": "Just a few very minor items. Just add the check to disable creating local indexes on a table with immutable rows and then let's check this in. Great work, @chrajeshbabu!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48853884/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48887938", "body": "@ramkrish86 - would you have time to check this in to master? We can add this minor check in a follow up commit. @chrajeshbabu - will an alternate patch be required for the 4.0 branch? 4.0 should match master, but it seems to be ever so slightly different.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48887938/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48941536", "body": "@ramkrish86 - just add a .patch to the url for the pull request and you'll have the patch file to apply against the normal/updatable repo. The github repo is read-only for everyone.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48941536/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/49374984", "body": "Nice work, @jfernandosf. Would you mind attaching a .patch file to the JIRA for 3.0, 4.0, and master (if they're different)?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/49374984/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/50102170", "body": "The code looks very clean, @jyates. Would love to get a demo and understand the overall flow and any limitations a bit better. What's the typical way a Phoenix user will interact with this to get their metrics? Got any time tomorrow?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/50102170/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/50422772", "body": "Looking very good, @jeffreyz88.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/50422772/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/53378107", "body": "Nice start, @ramkrish86. I went through your pull and gave you some feedback. Please let me know if you have comments/questions. Thanks - this is going to be a big improvement!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/53378107/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/53386020", "body": "One thing I didn't see is how you're handling multiple column families. Is that still left to be done? Think a bit on when you think is the best time to \"merge\" the guideposts. Maybe when you read the stats table in MetaDataEndPointImpl you can execute this logic and then the PTableStats that get passed into PTable are already the \"merged\" ones? Probably don't want to do this in ParallelIterators, as you'd do it again and again for every query execution.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/53386020/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/53618543", "body": "Looking close, @ramkrish86. Nice work. I think you need to just code up the consumption of the guideposts in ParallelIterators (see my comment there), add the logic for a \"min\" elapsed time for stats collection, and then do some basic local perf testing to see the impact. Then we can follow up with the issue that handles views correctly (just passing through the start/stop row through your coprocessor call) and add some more tests.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/53618543/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54002950", "body": "@ramkrish86 - just wanted to make sure you're not waiting on me for anything. I think this will be the \"big\" feature for 3.2/4.2 release. I think we should shoot for a monthly release schedule (like HBase does). If you can make the above changes and do 3-5M row local perf test, I think we'll be just about ready to pull this in. Thoughts?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54002950/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54014952", "body": "Sounds good, Ram. Thanks for all your effort on this one. Looking forward to see the perf numbers.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54014952/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54078575", "body": "Yes, the DefaultParallelIteratorsRegionSplitterIT should be modified to test the new behavior.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54078575/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54418091", "body": "-> Test cases are not running and all the test cases are failing at the deleteTable sequence.\nPost a stack trace - not sure offhand why this would be. \n-> The checking of the timestamp stored in the table and avoiding concurrent update stats does not work. I am checking on it.\n-> The clearing of the metadata cache should happen in a lock? If so then we need to see every where (even while reading from the cache) we should introduce a new lock.\nWe shouldn't need a new lock. What's wrong with the existing lock mechanism?\n-> Estimated byte size() is yet to be added to Ptable\nThis should be easy to add - just have your PTableStats implement a getEstimatedSize() method and estimate based on the size of the guidepost array. \n-> Multi CF is not yet tested.\nNo problem - you can do this after your initial perf testing.\n\nBut this would help in getting the review done on the latest changes.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54418091/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54506497", "body": "Our tests seem to be passing in our Jenkins builds - what do you think might be different in your env? FYI, this code(i.e. using HTablePool) is to work around an HBase bug that Jeffrey fixed for 0.98.6.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54506497/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54509066", "body": "No, update should not be necessary. My point was more that it's something particular to your local changes or env. It's likely that the connection to a HTable from a region server is failing due to a local config issue in your env. For now, you can change that to a region.getScannet() to get yourself unblocked.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54509066/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54513280", "body": "We'll need to circle back to this, though, as we can't check-in like that. The original code might work now with Jesse's check in this morning.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54513280/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55229101", "body": "Looking very good, @ramkrish86. I've made a pass through the whole thing and given you some feedback. Most important is getting the logic in ParallelIterators correct. Love the perf numbers.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55229101/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55236911", "body": "One additional thing to do is beef up the tests for this. We'll want to have lower level tests that validate the guidepost values (you can just set the number of bytes to traverse for each guidepost to a really small value). Also, we should have tests for no column families, one column family, and multiple column families involved in a query (both when the default/empty column family is referenced and when it's not).\n\nThen some end-to-end tests where there's enough data in a single region where you'd have more than one guidepost. Again, you can set the number of bytes to traverse for each guidepost to a really small value - might be good as the default for testing - to simulate multiple guideposts per region (we already have many tests that work across multiple regions, but might need to have a few that add a bit more data)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55236911/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55311741", "body": "Multiple CF tests: MultiCfQueryExecIT, one CF tests: QueryIT, no CF tests: KeyOnlyIT. Queries across multiple regions: Any time a table is pre-split. For example, see BaseQueryIT, when we initialize ATABLE, we pass in getDefaultSplits(tenantId) which forces the rows to be split across multiple regions.\n\n```\n@Before\npublic void initTable() throws Exception {\n     ts = nextTimestamp();\n    initATableValues(tenantId, getDefaultSplits(tenantId), date=new Date(System.currentTimeMillis()), ts);\n    if (indexDDL != null && indexDDL.length() > 0) {\n        Properties props = PropertiesUtil.deepCopy(TEST_PROPERTIES);\n        props.setProperty(PhoenixRuntime.CURRENT_SCN_ATTRIB, Long.toString(ts));\n        Connection conn = DriverManager.getConnection(getUrl(), props);\n        conn.createStatement().execute(indexDDL);\n    }\n}\n```\n\nAny table initialized in the static block of BaseTest may be split into multiple regions at initialization time. You'll get some decent coverage if you set the \"number of bytes before adding a guidepost\" parameter very low, so that basically after a single row you'll get a guidepost. We might be ok starting with that as long as you add specific test cases that test the lifecycle of these guideposts: adding the expected number for a given table with data across multiple regions, adding data to the table, re-analyzing removes old guideposts and adds new ones, a split cleans up the old ones and adds new ones, multi-tenant table with an analyze against it (i.e. testing a \"partial\" analyze). Then maybe some lower level ones that test your changes to DefaultParallelIteratorRegionSplitter.getSplits() and the merge bisect logic there.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55311741/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55555910", "body": "No, don't use the intersect method - it's not what you want. You need to just run through both the region ranges and the guideposts together, adding ranges (always inclusive at the start and exclusive at the end which is the default) as you go. Both the guideposts and the region boundaries are sorted. You'll have an algorithm like this:\n\n```\ncurrentPoint = currentRegion.startRange;\nwhile (more guideposts or regions) {\n    while (currentGuidepost <= currentRegion.endRange) {\n        add(new range from currentPoint to currentGuidepost);\n        currentPoint = currentGuidepost;\n        increment to next guidepost;\n    }\n    add(new range from currentPoint to currentRegion.endRange);\n    increment to next region;\n    currentPoint = currentRegion.startRange;\n}\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55555910/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55556203", "body": "Other more detailed advice. Simplify this and don't catch all exceptions.\n\n```\n    try {\n        if (table.getColumnFamilies().isEmpty() || scan.getFamilyMap().isEmpty()) {\n            // For sure we can get the defaultCF from the table\n            gps = table.getTableStats().getGuidePosts().get(defaultCF);\n        } else {\n            if (scan.getFamilyMap().containsKey(defaultCF)) { // Favor using default CF if it's used in scan\n                gps = table.getColumnFamily(defaultCF).getGuidePosts();\n            } else { // Otherwise, just use first CF in use by scan\n                gps = table.getColumnFamily(scan.getFamilyMap().keySet().iterator().next()).getGuidePosts();\n            }\n        }\n    } catch (ColumnFamilyNotFoundException cfne) { // Don't catch all exceptions here\n        logger.error(\"Error while getting guideposts for the cf \" + Bytes.toString(defaultCF));\n    }\n\n    if (gps == null) {\n        return regions;\n    }\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55556203/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55556372", "body": "Then, store the guidepost already as a List<byte[]> on PTable and PColumnFamily, as you don't want to get hit with the deserialization cost again and again. There should be _no_ PTable.getTableStats(), instead there should be a List<byte[]> getTableStats() on both PTable and PColumnFamily. You can just use the list here without doing any translation.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55556372/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55665720", "body": "Please see previous comments on the PTable interface, first try/catch block simplifications and the algorithm. Why are you using an N^2 algorithm? The guideposts are sorted.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55665720/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55811836", "body": "Don't we collect guideposts for all CFs? Why wouldn't the guideposts for which ever CF is referenced be used?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55811836/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55944456", "body": "Wow, this is looking good, @ramkrish86! Couple more comments and tweaks are needed. Then just a bit more testing. We need to make sure the guideposts are being delete/updated correctly in their lifecycle.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55944456/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55979859", "body": "Had an offline conversation with Jesse and he had a good idea. Can we change the property name by dropping the \"trace\" part of it and have this instead? I don't think having \"custom\" in there adds anything either. How about just phoenix.annotation? Then you can use these for tracing and logging both.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55979859/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/56146006", "body": "+1 after fixing the above very minor nits. Great work, @ramkrish86 !\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/56146006/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/67112893", "body": "Nice work, @dhacker1341. Some minor comments, but if all the tests pass, I'll commit after that.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/67112893/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/70047865", "body": "Wow, @twdsilva. This is fantastic work! In terms of testing, we should test this in the context of view index, salted tables, local indexes, immutable indexes. Another dimension is the kind of expressions used in the functional index. Let's figure out a good way to do this generally, so we get good test coverage without too much copy/paste.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/70047865/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/70776305", "body": "Thanks for the updates. It's looking very good! I'm not sure which ones you've implemented or not, though. Would you mind responding to my comments to let me know one way or the other?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/70776305/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/72729899", "body": "Looks great, @twdsilva. +1 if all unit tests pass.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/72729899/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/78206018", "body": "Fantastic work, Cody and Mujtaba. Would be good if you reviewed this too, @mujtabachohan. As a follow on check-in:\n- Change your test that rely on a cluster to use our BaseTest class instead (so it uses the mini cluster instead of relying on a cluster).\n- For these tests, put them under src/it (for integration tests), as these are the longer running tests that run on maven verify.\n- Add a markdown page to our website so folks know how to use it.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/78206018/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/78853991", "body": "@shuxiong - you're doing a great job so far. Just revert SignFunction back to your original version, keep the PWholeNumber and PRealNumber classes you created but make signum polymorphic, and throw an exception in the case of NaN for double (though I don't think it's possible for that to occur in Phoenix), and you'll be done with this one and ready to move on to other bigger issues. Appreciate your work here.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/78853991/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/79185104", "body": "Couple of very minor items, but otherwise we're good to go. Excellent work, @shuxiong. Nice first contribution.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/79185104/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/83363422", "body": "Somewhere in Indexer surrounding the point at which a batch of mutations is send over the wire, maybe in doPostWithExceptions, we should call IndexQosCompat.addIndexQosForTable(tableName, batchUUID) which would create a key out of tableName together with batchUUID and add it to the concurrent map. Then, when the batch has been sent, it'd call IndexQosCompat.removeIndexQosForTable(tableName, batchUUID), constructing the same key to remove it from the map. The IndexQosRpcControllerFactory would do a get on the concurrent map using only a key with only the tableName (that matches any batchUUID) in order to know it should use the special queue. That way, the map still gets cleaned up, but works when many different callers are writing index updates at the same time (as the tableName + batch UUID would always be unique). This also avoids the corner case of an index being dropped and subsequently a table being created with the same name. The current Config based approach would find that table name for the table and use the high priority queue.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/83363422/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/83909591", "body": "This looks very good. Do the tests pass?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/83909591/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/86683996", "body": "@tzolkincz - we're about to cut a 4.3.1 RC, so if you need this in, we'll need it today. Otherwise, 4.4.0 may be your next release vehicle. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/86683996/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/86829091", "body": "@gabrielreid - if this looks good to you, please commit to 4.3 and the other branches. Thanks for the contributions and reviews!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/86829091/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/87145157", "body": "Good stuff, @jmahonin. Thanks so much for the contribution. Any Spark experts out there that can review this? @apurtell ? @mravi ?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/87145157/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/87145235", "body": "@SagarSharma - have you read the step-by-step guide on how to write a built in function? http://phoenix-hbase.blogspot.com/2013/04/how-to-add-your-own-built-in-function.html. Looks like you declaring your POWER function to take a VARCHAR as an argument? \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/87145235/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88239723", "body": "Thanks for the contribution, @michfr . LGTM. I'll get this committed to 4.x and master soon.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88239723/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/93892269", "body": "@twdsilva  - would you mind reviewing this?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/93892269/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/95414642", "body": "Very nice, @AakashPradeep! @twdsilva - would you mind reviewing too? Also, looks like it needs to be rebased.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/95414642/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/95652863", "body": "@AakashPradeep - this pull request is associated with the wrong JIRA, as it seems to be adding comments to PHOENIX-174. Can you close this pull and open two new ones? One for PHOENIX-628 and then one for PHOENIX-1710 (for the new built-in functions)? Make sure to use the JIRA in the commit message so it get associated correctly.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/95652863/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/100526094", "body": "This is looking good, @nmaillard. Still some copy/paste code that we should aim to cleanup. Also, take care that no exceptions are swallowed. Wrap checked exceptions in a SQLException if it's declared in the  method and otherwise wrap in a RuntimeException and throw it. Also, make sure to remove or convert all System.out.println calls to logging instead.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/100526094/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/100526103", "body": "@mravi  - would you mind giving this a review?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/100526103/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/102078845", "body": "@ddraj - I left feedback and it was never responded to or implemented.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/102078845/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/104059599", "body": "Actually, change it to rhs.getDataType().isCoercibleTo(PVarChar.INSTANCE). That's more or less as you've described, but done in a way that doesn't hard code type checks. Our PDataType does not necessarily map 1:1 with SQL types, so down the road we might add a PUTF16EncodedType which would still map to a VARCHAR is SQL. This type of check makes a change like this more manageable.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/104059599/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/104078110", "body": "I'd make JSON castable to VARCHAR instead of coercible. That way you can do an explicit CAST, but it would not be auto-coerced.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/104078110/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/120245488", "body": "Looks very good, @siddhimehta. Thanks for the quick turnaround. Couple of minor nits and then I'll commit it on your behalf.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/120245488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128456464", "body": "Thanks so much for the contribution, @JeffreyLyonsD2L. Would you mind helping to review and commit this, @elilevine? I think the main thing to check is that there aren't any places in the code that Jeffrey missed updating.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128456464/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128863633", "body": "This feature would allow the tenant ID to be represented through more types than just a VARCHAR. Some users may have tenants identified by an int or a long instead of a string. The tenant ID would still be passed as the stringified value, but the underlying table could use an int or a long as the column type. As you mentioned, the system tables (SYSTEM.CATALOG and SYSTEM.SEQUENCE) would need to continue to use the string representation. Every type can be converted to a string, so this should be ok (pending testing).\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128863633/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128869235", "body": "The system tables need to be able to store the tenant ID regardless of its type in the data table, so it wouldn't make sense to type the system tables (i.e. the _all_ approach isn't viable IMHO). If a table already exists with a different typed leading column than CHAR/VARCHAR, then it would not be possible to map a multi-tenant Phoenix table to it. This just provides a little bit more flexibility. I'll ping the original filer of the JIRA and see if he has further comment on his use case.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128869235/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/131611818", "body": "Thanks for the update, Nishani. This is definitely an improvement. The primary information missing is the query itself. Without this, the user doesn't know what they're looking at. The query should be the description of the root span. I'd recommend displaying this at the top of the page in the blue box instead of the \"data retrieved\" text. It's important enough to always see this information rather than making it a tooltip. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/131611818/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/133897978", "body": "Thanks for the pull. One feature that Phoenix (and other RDBMS) has is that if a column identifier is double quoted, then it's treated as case sensitive. Otherwise, it's upper cased as you've mentioned. Would be good if we could accommodate this too in the phoenix-spark integration. If we can't do this, then we may need to keep it as case sensitive as otherwise we can't support the case sensitive use case.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/133897978/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/133910608", "body": "Yes, I think that would work. We'd need unit tests around this too, please. The function we go through to normalize identifier/column references is SchemaUtil.normalizeIdentifier(String identifier). Here's a SQL example (which would work fine):\n{code}\nCREATE TABLE \"t\" (id VARCHAR PRIMARY KEY, \"v\" VARCHAR);\nUPSERT INTO \"t\" (ID, \"v\") VALUES ('a','b');\nSELECT ID, \"v\" FROM \"t\";\nSELECT id, \"v\" FROM \"t\";\n{code}\n\nWould you have some cycles to review this pull, @jmahonin?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/133910608/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/145189319", "body": "Thanks for the patch, @d9liang  -  I reviewed and left you some minor feedback. Overall, it looks very good. One other thing to check (and add tests for) is other code paths that insert data: namely UPSERT SELECT (on client-side and server-side) and CSV bulk load (which goes through a similar code path as UPSERT VALUES, so you might already be covered there, not sure). This would ensure that we're consistently truncating fixed width types in all cases.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/145189319/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/145719742", "body": "+1 to the patch. Thanks for the contribution, @chiastic-security. I've pushed it to master and will push to other branches soon.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/145719742/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/148763881", "body": "Thanks for the pull request, @navis. Would you mind including a unit test as well? \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/148763881/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/150397877", "body": "Thanks for the pull request, @navis. A couple of minor comments, but overall it looks great. FYI, our PDataType class is stateless (it was an enum originally), so we currently access maxLength/precision and scale through the PDatum interface (from which PColumn and Expression are derived). Now that PDataType is no longer an enum, it might be nice to allow instantiation with maxLength and scale provided at construction time. Please file a JIRA.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/150397877/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/156610104", "body": "Looking good, @twdsilva. I've commented on three or four \"real\" issues, one followup issue, and a few misc nits. Great job!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/156610104/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/157971369", "body": "I can't find PhoenixTransactionalIndexer. Did your latest pull forget to add new files? Or maybe I'm just missing it? FYI, I remember there are a few bits of commented out code that should be removed from that file. Everything else looks good to me - except for a few minor comments above.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/157971369/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/158572388", "body": "FYI, @twdsilva broke this up into two separate pulls because GitHub wasn't show the diff for all files. The tests are over in https://github.com/apache/phoenix/pull/132. Both this pull and that one will be committed together - this is just to make the review easier.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/158572388/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/160713601", "body": "Thanks, @gliptak. Patch looks fine, but please amend your commit message by prefixing with PHOENIX-2465 so that it gets tied to the JIRA, like this: \"PHOENIX-2465 Upgrade ANTLR version to correct generation error with Java 8\"\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/160713601/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/160852616", "body": "Couple of other comments:\n- You'll need to add code during installation of 4.7.0 (in ConnectionQueryServicesImpl.init()) that takes care of removing old coprocessors, adding new coprocessors, disabling local indexes and potentially automatically kicking off a new index build\n- We'll likely want to add a check that disallows column family names prefixed with \"C#\" to prevent inadvertently treating these as local index shadow column families. If we can get rid of the checks that use this prefix, then we might not need to do this.\n- Also, there's an edge case of an existing column family using the \"C#\" prefix. Unlikely, but not sure how we'd want to handle this. Maybe error on an attempt to create a local index on such a table?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/160852616/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/161471331", "body": "Awesome, @ndimiduk - I love it. Couple of questions and some nits:\n- For JSON, is only single level supported? If not, how is nested JSON handled?\n- Looks like lists are converted into ARRAYs which is nice. What happens if the array elements have different data types? I guess you'd just log an error and ignore that row?\n- Can you make sure the indenting conforms to the 4 space convention?\n- Also, please make sure imports don't use \\* and that their order is correct (see dev/phoenix.importorder and dev/eclipse_prefs_phoenix.epf).\n\n@gabrielreid - got a few spare cycles for a review?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/161471331/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/162146972", "body": "This seems like a good idea. @mravi - would you have a few spare cycles to review?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/162146972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/168795121", "body": "Thanks for the patch, @freakyzoidberg. @samarthjain - would you mind reviewing and committing? It looks good to me. Probably worth making a quick pass through the code for calls to SortOrder.invert() and make sure we're setting the offset as is done in this patch.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/168795121/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/172037773", "body": "Thanks for the patch, @ankitsinghal. The main remaining issues are around not treating an ImmutableBytesWritable the same as a byte[]. Maybe easiest for now to have BaseResultIterators.getParallelScans() declare currentKey as an ImmutableBytesWritable, but keep\n\n```\n// Do this as infrequently as possible to prevent a copy of the backing byte array\nbyte[] currentKeyBytes = SchemaUtil.copyKeyIfNecessary(currentKey);\nbyte[] currentGuidePostBytes = SchemaUtil.copyKeyIfNecessary(currentGuidePost);\n\nScan newScan = scanRanges.intersectScan(scan, currentKeyBytes, currentGuidePostBytes, keyOffset, false);\nscans = addNewScan(parallelScans, scans, newScan, currentGuidePostBytes, false, regionLocation);\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/172037773/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/172148769", "body": "Looks good functionally now, @ankitsinghal. Just needs a little bit of cleanup. Nice work!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/172148769/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/172279531", "body": "Looking good. One more pass, and I think we'll be good to go. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/172279531/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/phoenix/comments/15361268", "body": "This should pass through the same arguments as below:\n    ResultIterator scanner = new TableResultIterator(mutationState, tableRef, scan, scanMetrics);\n\nThe mutationState  was cloned here because the one in context may change. Otherwise, we end up not passing through the transaction. We have a test, but it's ignored and we're waiting for a Tephra fix to enable it. I'll modify this and confirm it fixes the issue (as I'm testing with a Tephra snapshot).\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/15361268/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/16795354", "body": "There were no symptoms, but only a warning in the logs. In this case, Phoenix is purely tracking memory usage, but wasn't issuing a close under some circumstances on the client and thus not freeing memory on the server (until a GC occurred). I don't think it's related to what you're seeing. Are you seeing this in our 4.7.0 release and if so can you reliably reproduce it? If you wouldn't mind filing a JIRA with the steps necessary to reproduce the issue, that be much appreciated. We have a lot of regression tests in place, but it's always possible something slipped through the cracks.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/16795354/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/16855409", "body": "When a group by is being performed, unless the group by key is made up of the leading PK columns of a table or index, Phoenix holds in memory each distinct group by key and the partial aggregation. This will spill to disk based on phoenix.groupby.maxCacheSize, but performance will drop in this case. There may be other in memory representations we can use to reduce the memory footprint. I've filed PHOENIX-2800 for some potential future work. Let me know if you'd be interested in contributing that.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/16855409/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218273", "body": "We need to update the index state in the reducer as we can't depend on the client remaining up for the duration of the index build (which could take 24 hours or more potentially).\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218273/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218366", "body": "Why did the row count change here for this test? The default for auto commit is already false. For transactional workloads, we should see the two rows even though they're not committed. However, we should be using the same connection (conn2 instead of conn when we do the count( \\* ) . Does the test pass if you make that change?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218366/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218447", "body": "For the non direct API branch, the reducer does something different, so we can't take the same approach. I'd leave it as-is and just do the simple test change.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218447/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218466", "body": "Ah, I missed that comment. We should change that comment as it doesn't really make sense.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218466/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218742", "body": "@twdsilva - would you mind taking a look at this?\n\nIn theory, the call to setUpRealDriver() in the setup test method should end up calling this to setup the txn client:\n\n```\n    if (clientProps.getBoolean(QueryServices.TRANSACTIONS_ENABLED, QueryServicesOptions.DEFAULT_TRANSACTIONS_ENABLED)) {\n        setupTxManager();\n    }\n```\n\nI wonder if it's somehow mistakenly using the test driver? This is somewhat convoluted, unfortunately.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218742/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218769", "body": "Would you mind committing your latest changes, @SsnL?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218769/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713140", "body": "Should this be CREATE LOCAL INDEX?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713140/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713329", "body": "Minor nit: get rid of create.getProps().get(\"\") check in outer if and add if (list != null) check\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713329/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713390", "body": "Isn't this change already in master (hopefully)?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713572", "body": "Add comment here explaining this please: Rather than not use a local index when a column not contained by it is referenced, we join back to the data table in our coprocessor since this is a relatively cheap operation given that we know the join is local.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713572/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713646", "body": "Remove this TODO comment, as it's done.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713646/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713759", "body": "Comment?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713759/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713917", "body": "Comment: build map from dataColumn to what will be it's position in single KeyValue value bytes returned from the coprocessor that joins from the index row back to the data row.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713917/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14714086", "body": "This shouldn't be necessary. The local index rows will be ordered correctly within each region.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14714086/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14714225", "body": "Comment on if/when table is different than context.getResolver().getTables().get(0).getTable()\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14714225/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14714621", "body": "This code needs to be in the scanOrdered code block as well. How about factoring this outside of the call to scanOrdered/scanUnordered and passing the necessary info through both calls?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14714621/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14714692", "body": "Here too - add to scanOrdered code too.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14714692/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14715264", "body": "Also, if it's the same, can you use context.getCurrentTable().getTable() instead as it's a bit more readable.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14715264/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14717103", "body": "There's a bit more you need to do to handle ORDER BY correctly. It'd be for the case in which a data column was referenced in the ORDER BY while the index table is being used to satisfy the query. Not here, but in [OrderedResultIterator](https://github.com/apache/phoenix/blob/master/phoenix-core/src/main/java/org/apache/phoenix/iterate/OrderedResultIterator.java). In getResultIterator, in the beginning of the for loop, you'll need to wrap the result in the same way if there are dataColumns (i.e. call IndexUtil.wrapResultUsingOffset)\n\n```\n        for (Tuple result = delegate.next(); result != null; result = delegate.next()) {\n            int pos = 0;\n            ImmutableBytesWritable[] sortKeys = new ImmutableBytesWritable[numSortKeys];\n            for (Expression expression : expressions) {\n                final ImmutableBytesWritable sortKey = new ImmutableBytesWritable();\n                boolean evaluated = expression.evaluate(result, sortKey);\n                // set the sort key that failed to get evaluated with null\n                sortKeys[pos++] = evaluated && sortKey.getLength() > 0 ? sortKey : null;\n            }\n            queueEntries.add(new ResultEntry(sortKeys, result));\n        }\n```\n\nWithout this, the table data column expressions that aren't in the local index will fail to evaluate. You should add a test for this too.\n\nI think the cleanest way to handle this is by wrapping the ResultIterator passed into the OrderedResultIterator. Just create a new ResultIterator that delegates to the original one and does the wrapping necessary. Then you won't need to change OrderedResultIterator at all.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14717103/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14717444", "body": "Actually, nix this, as the innerScanner you pass to OrderedResultIterator already does all of this. You likely already have a test for the ORDER BY case I mentioned, but if not, please add one.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14717444/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14717528", "body": "comment please\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14717528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14718183", "body": "Remove this comment, as this has already been done.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14718183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14718590", "body": "This TODO should be implemented. Just implement a new method on ColumnRef for cloning:\n\n```\npublic ColumnRef cloneAtTimestamp(long timestamp) {\n    return new ColumnRef(this, timestamp);\n}\n```\n\nOveride this on LocalIndexDataColumnRef by adding this:\n\n```\n@Overide\npublic LocalIndexDataColumnRef cloneAtTimestamp(long timestamp) {\n    return new LocalIndexDataColumnRef(this, timestamp);\n}\n```\n\nMake the ColumnRef(ColumnRef columnRef, long timeStamp) constructor protected and replace existing calls to it with the calls to the new clone method. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14718590/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14762643", "body": "One thing that's necessary, though, to maintain rows in row key order is to modify ScanPlan.java:118 to do a merge sort instead of a concat:\n\n```\n        if ((isSalted || isLocalIndex) &&\n                (context.getConnection().getQueryServices().getProps().getBoolean(\n                        QueryServices.ROW_KEY_ORDER_SALTED_TABLE_ATTRIB,\n                        QueryServicesOptions.DEFAULT_ROW_KEY_ORDER_SALTED_TABLE) ||\n                 orderBy == OrderBy.FWD_ROW_KEY_ORDER_BY ||\n                 orderBy == OrderBy.REV_ROW_KEY_ORDER_BY)) { // ORDER BY was optimized out b/c query is in row key order\n            scanner = new MergeSortRowKeyResultIterator(iterators, SaltingUtil.NUM_SALTING_BYTES, orderBy == OrderBy.REV_ROW_KEY_ORDER_BY);\n        } else {\n            scanner = new ConcatResultIterator(iterators);\n        }\n```\n\nLocal indexes are similar to salted tables in that the parallel scans will all be within a region, ordered correctly. As long as we do a merge sort across the results of these scans, the rows will be ordered correctly.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14762643/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14852114", "body": "This code looks familiar. If it matches original (I think from QueryIT), can you move it into the base test class instead of copy/paste?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14852114/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14852129", "body": "Looks like the if and the else branch match. If that's expected/correct, can you get rid of the if statement?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14852129/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14854253", "body": "Excellent. Would you mind updating this pull request? I think it's ready to get committed, no?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14854253/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14855032", "body": "This shouldn't be necessary and will cause the skip scan not to be used (which will be horrible for point queries). It should work fine to do the skip scan for each region (much better than doing a full region scan for every region).\n\nCan you remove it and add a test that does a point lookup with multiple values (for example, a IN clause with multiple values for an indexed column)?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14855032/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14859660", "body": "Just move the check for the index being a local index outside of the loop (and skip that index in that case). We should never match based on the column name. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14859660/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14859843", "body": "Thinking about this more, it should probably just be disallowed to create a local index on a table with immutable rows. I'm not sure if there's a need and I suspect we haven't tested this combination. We can always add it down-the-road.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14859843/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14859858", "body": "it's should be its\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14859858/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14860075", "body": "Add check here for if (dataTable.isImmutableRows() && statement.getIndexType() == IndexType.LOCAL) and just throw a SQLException with a new error code that local indexes aren't allowed on tables with immutable rows.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14860075/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14860092", "body": "Remove this comment block as it's not relevant any longer.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14860092/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14860115", "body": "Not sure I understand this comment. This code applies to joins, I believe. Can you confirm/elaborate? Is the TODO still relavent?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14860115/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14860588", "body": "Add this at the end of the enum\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14860588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14860610", "body": "Please add TODO to add getSplitsPerRegion method to ParallelIteratorRegionSplitter so we can move this special case to your new implementation for local indexes.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14860610/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14867224", "body": "On second thought, why do we need this to be different for local indexes? Seems like we could run parallel scans over part of each region just like we do with other scans, no?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14867224/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14873050", "body": "In a follow up commit, we should look to commonize the above salting logic with this logic for local indexes. It's doing the same thing, it's just adding one byte for the salt byte while the local indexing is adding the region start key.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14873050/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14899221", "body": "Let's discuss in a separate JIRA. It's not a big deal.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14899221/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14899559", "body": "That's what the splitsPerRegion variable and subsequent logic in ParallelIterators does - it creates additional split points within the range so that multiple scans get run over a single region. We'd want to prefix each of these with the same start region key. I'll open a separate JIRA for this too. It's not a big deal - most of the time the parallelization slots would be used up by having to do a scan in each region anyway.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14899559/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15383971", "body": "Ideally, if possible, you'd want to build this UPSERT statement once, call conn.prepareStatement(upsert) on it, and then here just bind the bind variables and execute it.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15383971/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15384076", "body": "Should/does this test querying the metrics table?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15384076/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15499821", "body": "Why this change?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15499821/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15499838", "body": "Please revert - I've already made this change.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15499838/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15500164", "body": "Use the static constants you defined for these already in QueryServicesOptions\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15500164/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15500187", "body": "Can you please follow the 4 space convention that's used everywhere else?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15500187/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15500517", "body": "Shouldn't this match what's done below instead? Otherwise, when we do the plan.iterator().next() below, it'll probably get overridden.\n\n```\nplan.getContext().setScanTimeRange(new TimeRange(lowerBoundTimeStamp, Long.MAX_VALUE))\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15500517/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15500727", "body": "I think its better if this block of code lived in buildPartialIndexFromTimeStamp, as it's not related to building the index. Also, would you mind adding a comment about why you're setting the index from DISABLED to INACTIVE? Is it so that incremental maintenance will start again and data rows that are updated while the partial rebuild is occurring will get added to the index (i.e. you won't miss any)?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15500727/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15500850", "body": "Rather than pass through the lowerBoundTimeStamp and set it on the StatementContext, a better place would be on TableRef. That's where the upperBoundTimeStamp lives. If you stored it there, you could establish it on the TableRef being passed through already which would be simpler. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15500850/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15501061", "body": "I'm torn on this one. I think this is probably easiest, as I wouldn't want to require two jars. Is there any value to having a phoenix jar with only phoenix stuff in it? I suppose not. If we go this route, we should get rid of the minimal-phoenix-client.jar, as this jar will match that one.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15501061/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15501164", "body": "Add _ATTRIB to follow convention. Maybe INDEX_PARTIAL_REBUILD_ATTRIB?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15501164/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15501192", "body": "INDEX_PARTIAL_REBUILD_INTERVAL_ATTRIB?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15501192/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15501223", "body": "INDEX_PARTIAL_REBUILD_OVERLAP_ATTRIB\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15501223/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15501287", "body": "Probably a better home would be MetaDataUtil as this doesn't have anything to do with MetaDataClient\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15501287/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15509748", "body": "Are there more tests? How about testing these scenarios:\n- updates coming into a table with a disable index (to make sure no updates are lost)\n- partially updating the index table\n- partially updating a local index table\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15509748/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16694581", "body": "We're trying to get away from ClientManagedTimeTests. Can you derive from HBaseManagedTimeTest and use the corresponding annotation instead?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16694581/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16694608", "body": "No need for your own driver or HBaseTestingUtility - you should be able to do everything you need at the Phoenix level.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16694608/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16694641", "body": "Is this function really worth sharing?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16694641/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16694672", "body": "I don't think we need to support both - let's just stick with ANALYZE.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16694672/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16694781", "body": "We wouldn't want to use a global read lock here - at most we want to have a lock with a name based on the table name.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16694781/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16694959", "body": "I forgot to mention this before, but it's important that we always use the guide posts here to determine the set of work that will be done. We'll get rid of the hint for not splitting up work. I'm not super keen on falling back to using Bytes.split(), but not sure what a better alternative would be. I suppose when a table is initially created this will be the case for a while. We should at a minimum log a warning if we fallback to this. Any ideas here?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16694959/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16695090", "body": "Rather than have a separate cache for this, I'd rather piggyback on the existing cache. How about if we just read the stats anytime we re-load the PTable? Then an updateStats would just need to invalidate the cache entry for the table. Next time the table is accessed, the stats table would be re-read. This should be ok, as a table is only invalidated when its schema changes or an index is added/removed\n\nOne side note: we'll want to make sure that the PStats info has a getEstimatedByteSize() and that PTable takes this into account for it's getEstimatedByteSize() so that this is accounted for in the size of each cached PTable.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16695090/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16695134", "body": "I don't think we'll need this at all if we just re-read the stats when we re-load the table.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16695134/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16695198", "body": "Change back to 15mins? If you want a different one for test purposes, see QueryServicesTestOptions.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16695198/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16695226", "body": "Throw Phoenix TableNotFoundException here instead\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16695226/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16695690", "body": "How about something like this:\n- If there's a single region, we don't try to divide it\n- When a table splits, we initiate an update stats call on it - can we capture that in a coprocessor? If not, perhaps we can infer that it happened based on the current stats versus the number of regions.\n- We have the concept of a \"minimum time to recalc stats\" as a separate config and store the time when the stats were last calculated in the stats table. This will prevent too many analyze stats from being called (i.e. it wouldn't be dangerous to invoke an analyze stats here on the client, even if many clients do it, as the ones after the first one would be a noop).\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16695690/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16695740", "body": "Left over from testing?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16695740/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16695765", "body": "Just don't pass bindCount here and have a default method that returns 0. I can't think of a case where there'd be bind variables for this statement.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16695765/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16695860", "body": "Create an ConnectionQueryServices.updateStats(String tenantId, String schemaName, String tableName) method and move this code to ConnectionQueryServicesImpl. The idea is that all HBase code is encapsulated in that class. You can make the one in ConnectionlessQueryServicesImpl be a noop. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16695860/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696129", "body": "PTable should be immutable, so the stats should only be passed through the constructor.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696129/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696186", "body": "Looks somewhat expensive to calculate. PTableStats should be immutable, so instead calculate this in a static method and set during construction.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696186/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696289", "body": "Include both schemaNameBytes and tableNameBytes here.\nAlso, add a bytes startRow and bytes stopRow so we can do a \"partial\" status update. This will be especially useful to enable a tenant-specific table to update only its own stats.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696289/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696378", "body": "Should be a builder.steTableNameBytes too, as schemaName and tableName are separate - see other calls on MetaDataEndpointImpl.\n\nWe should also add a startRow/stopRow so we can do a partial update of stats. See note in protobuf def of stats request.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696378/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696389", "body": "Is all this new code, or did it get moved?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696389/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696446", "body": "Keep PTableStats as immutable.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696446/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696487", "body": "Pass through startRow/stopRow at a minimum.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696487/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696586", "body": "These classes seem like overkill given what we're collecting. Can this be simplified?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696586/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696763", "body": "The schemaName and tableName are derived from the HTable name which you can get from the RegionCoprocessorEnvironment. We have util methods in SchemaUtil that'll give you back the schemaName and tableName given the byte[] or String of a full HTable name. Is this still an issue?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696763/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696921", "body": "This is a tricky, but kind of interesting case. If there's a tenantId, then that means that the user has asked to analyze stats for a tenant-specific table. In that case, you'd want to get the physical table name from PTable to pass through the endpoint call, but we'd want an option of passing a startRow/stopRow to only analyze _part_ of a table. It's ok if you want to TODO this, but please file a JIRA for this (and still always get the physical table name).\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696921/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696980", "body": "See other examples here of how to get the PTable given the tenantId, schemaName, and tableName. You'd want to look it up and then use the PTable.getPhysicalName() to get the actual HBase table. See below for how to handle if there's a tenantId, but it would form a start/stop key that should be passed through the update stats method (it's fine to TODO this for now).\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16696980/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16698996", "body": "One other caveat: since it's quite possible that the table won't change (i.e. won't have any columns or indexes added/removed) so will remain in the cache too long, we should modify the cache for a max TTL for a cache entry. This can be based on this update frequency for stats. That way, a table, regardless of if it's been changed, will age out of the cache and be re-loaded along with any new stats. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16698996/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699150", "body": "Also, we won't use targetConcurrency and maxConcurrency. Instead we'll add all guideposts for the regions here (they'll get pruned later when we apply/intersect them with the scan start/stop row). This will help in a bunch of ways by dividing the query up into smaller equal sized scans - we won't get lease expiration exceptions from the server b/c the client will be specifying smaller chunks. We'll also get better fairness between queries, b/c no one query will dominate a thread for too long a time. It also simplifies things quite a bit.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699150/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699284", "body": "You can get at stats in a number of ways: query the system.stats directly or use the PTableStats off of PTable (which is easy to get to by doing an unwrap to access the cache directly from ConnectionQueryServices)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699284/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699418", "body": "Yes, the latter, but with the above caveat (to age-out a cache entry - it's an option on the guava-based Cache): The invalidation is done only when the schema changes or index is removed/added which also means there is no need for periodic scanning of stats table.\n\nThis will be much simpler and we only have to manage a single cache. Perf should be fine - it's just an extra Scan and if it was done recently will be in the block cache anyway.\n\nMake sure you update the size of PTableImpl by taking into account the size of a PTableStatsImpl so the cache is sized correctly.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699418/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699479", "body": "See above caveat for aging out the PTable from the cache. Also, the manual call to ANALYZE will invalidate the table after the stats have been updated, so the next select would get the new stats.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699479/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699532", "body": "Ok, I'll leave it up to you. That was just my first impression, but I'm fine with it if you think they add value without adding too much complexity.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699532/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699691", "body": "I wouldn't assume that only the size changing means it's different. Just compare the hash code in the equals check and add a hashCode member variable that gets set at construction time.\n\nAlso, I'm not sure we need a Map - take a look and see if you can get away with just a List<byte[]> where they're ordered. See my comments in ParallelIterators - I believe well basically just use all of them, all the time.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699691/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16720268", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16720268/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16720739", "body": "It's fine to collect it per region. I'm talking about the _usage_ of these guideposts. Instead of driving this per region in this loop, we can just get back _all_ guideposts for a table here and return them. No need to break this down per region. Thus, I don't think PTable needs to keep a map. All it needs is all the guideposts for a table. I don't think the logic to get them to \"round-robin\" per region server is worth preserving. We can just randomly shuffle the guideposts and we'll get the same effect. This routine will become very simple.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16720739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16720778", "body": "We won't need this anymore.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16720778/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16720836", "body": "The only caveat here is that for local indexes we shouldn't ever fall back on Bytes.split(), but if we get rid of that here, we'll need no special case for this.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16720836/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16720928", "body": "Ah, right - never mind - thought it was somehow related to ExecutableAddColumnStatement.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16720928/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16726785", "body": "Actually, I don't think we'll even need minKey/maxKey at this point. Just the guideposts.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16726785/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16729698", "body": "I'd rather just keep only what we need, especially for PTable. Keep the server-side code that figures out what the min and max are, but we won't need to calculate it for now.\n\nI think once the guidepost work gets wrapped up, we can step back and do some design on what additional stats we'll need (to guide other optimization decisions) and how to surface these.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16729698/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16730944", "body": "Thanks, Ram. Yep, that's my fault - I thought we'd need these min/max keys, but turns out we don't. Just want to keep things as simple as we can.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16730944/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16793270", "body": "Yes, correct - add a last_analyze_start_date or something like that in your system.stats table. Then in your coprocessor, before kicking off the stats collection, check the current date against this date and have it be a NOOP if less than the min time (based on config parameter).\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16793270/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16794301", "body": "Here's what I think we should do here:\n- Store guideposts per column family. It's probably easiest if the PK is of the following form:\n  <cf varchar not null><guidepost varbinary null>. I'm not sure there's any value in using a VARBINARY ARRAY. We should just make sure that we can delete the old guideposts and add the new ones easily.\n- Here, you'd still want to loop through the regions as above, but you want to get all guideposts for the column families involved in the query. Let's take the simple case where there's only one. In that case, you'd intersect all the region boundaries with the guideposts - this will be a bit easier if the guideposts are sorted already. The set of intersections will be what gets returned here.\n- For the multi-column family case, I think we want to do the same processing as above per column family and then we'll coalesce any overlapping ranges.\n- We have the intersect and coalesce methods you'll need in our KeyRange class, so the code should be relatively small\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16794301/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16794419", "body": "That's fine, but just make sure you always get the physical name from PTable so this won't break if an attempt is made to analyze a view.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16794419/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16794520", "body": "Ok, whatever you think works best here.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16794520/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16796319", "body": "Might be better to do this check from the Phoenix client _before_ you invoke the coprocessor. Also, I think you'd only want to update the last_analyze_start_date when the entire table is analyzed (as opposed to just part of it, for example when a split occurs - we should kick off an analyze for the two regions that split, or when a view is analyzed which would just re-analyze a part of the table)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16796319/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16797475", "body": "That sounds fine, but then switch region name after cf name so we can get the stats more easily. I think a good pk would be: table name + cf name + region name. Then use the stats name as a regular key value column with the correct type (i.e. VARBINARY ARRAY for guideposts). Probably cleaner to store the stats on the PColumnFamily instead of PTable too. You can still pass then through the PTableImpl constructor, but might as well break them up by CF and pass through the constituent parts to the PColumnFamilyImpl constructor. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16797475/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16937281", "body": "One minor issue is the case where we have no column family. It's possible to not declare any key value column in a CREATE TABLE statement. In that case, the PTable will no column families. In that case, I'd suppose we could just store the guideposts on the PTable instead of the PColumnFamily.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16937281/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16962143", "body": "Good point. The best way to ensure other clients will re-read the metadata from the server is to update a timestamp on a Cell in the row. Jeffrey made a change like this to \"force\" clients to do this by doing a Put of the empty key value. See MetaDataEndPointImpl.updateIndexState() line 1538:\n\n```\n                if(dataTableKey != null) {\n                    // make a copy of tableMetadata\n                    tableMetadata = new ArrayList<Mutation>(tableMetadata);\n                    // insert an empty KV to trigger time stamp update on data table row\n                    Put p = new Put(dataTableKey);\n                    p.add(TABLE_FAMILY_BYTES, QueryConstants.EMPTY_COLUMN_BYTES, timeStamp, ByteUtil.EMPTY_BYTE_ARRAY);\n                    tableMetadata.add(p);\n                }\n```\n\nHow about having your invalidate method do that instead? In this case, the client makes a getTable() call in MetaDataClient.updateCache() and the new PTable is sent across (which would include the stats).\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16962143/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16971394", "body": "Use a select query - we try to use Phoenix as much as possible.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16971394/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16996015", "body": "The table build by buildTable() will have a timeStamp of MAX(timeStamp) over all its cells.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16996015/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16998265", "body": "Also, your new invalidateTable() call should get the current table and then set the empty key value cell timestamp to currentTable.getTimeStamp() + 1. This should be done while the lock is in place.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16998265/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17061906", "body": "+1 to storing guideposts in sorted order (hopefully that's a byproduct of\nthe way you process/add them.\n\nNo need to adjust the guidepost - treat them similar to region boundaries.\nThe first range should start from an empty byte array and go to the first\nguidepost, next one from first to next guidepost, and last one from last\nguidepost to an empty byte array.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17061906/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17064233", "body": "Make sure that your KeyRange is inclusive on lower and exclusive on upper\npart of range.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17064233/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17032875", "body": "Please include a new unit test - perhaps in QueryIT or feel free to create your own - that demonstrates a need for the fix.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17032875/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17405762", "body": "Please make sure you have the latest, as there were some recent changes here (and I don't think your change will affect the code here at all)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17405762/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17406801", "body": "PTable will need this as well for the case where there are no column families.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17406801/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17619890", "body": "Add to estimatedSize here based on List<byte[]> values (not PTableStats map, as it shouldn't be stored anywhere).\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17619890/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17684909", "body": "Remove any System.out.prinln() before checking in please. This would be a good place to assert for the expected guidepost values, though.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17684909/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17709795", "body": "This looks like it'll pretty much still work to build the PTableStats\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17709795/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17748555", "body": "+1. Much cleaner!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17748555/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17757212", "body": "SYSTEM.SEQUENCE -> SYSTEM.STATS in comment\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17757212/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17757630", "body": "Ok - please remove then.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17757630/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17771836", "body": "Nit - update this comment as per above.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17771836/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17771927", "body": "Add estimatedSize += SizedUtil.sizeOfArrayList(guidePostsSize);\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17771927/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17771980", "body": "Remove these two lines for estimateSize and replace with estimatedSize += family.getEstimatedSize();\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17771980/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17772001", "body": "Actually, keep those two lines, but don't forget to add the estimatedSize += family.getEstimatedSize();\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17772001/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17772216", "body": "If this class no longer exists, please remove it from the pull. Otherwise, remove the System.out.println()\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17772216/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17578973", "body": "How about adding a custom annotation for known properties like TENANT_ID and CURRENT_SCN?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17578973/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/21877974", "body": "By convention, we use test in the test name. How about something like testMinMaxForFixedLength()?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/21877974/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/21878001", "body": "Minor nit, but maybe have a final local variable for getAggregatorExpression() since we call it a number of times.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/21878001/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/21878021", "body": "Minor nit: formatting looks a bit wonky. We have eclipse prefs and import order you can import in phoenix/dev.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/21878021/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22992211", "body": "Do you need to do this CAST here in order for the index to be used? Or if you just had date1+1 and date2+1 would it still be used?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22992211/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22992467", "body": "When is connection null?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22992467/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22992519", "body": "This is like a copy constructor, so instead of null for the new arg, you should likely use column.getExpressionStr().\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22992519/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22992701", "body": "Build a Map<Expression,Expression> in the visitor constructor that maps from \"compiled expressionStr\" to \"column expression in index for that expression\", and then here just look for a given expression. You should already have the expression on the way out of the vistorLeave methods. It's unlikely you need to specialize any visitorEnter methods.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22992701/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22992799", "body": "Having to specialize every visitorLeave is a bit painful. One possibility would be to make the wrapGroupByExpression method protected, override it, and do a super.wrapGroupByExpression() call (and perhaps rename it). I believe that method is called from every visitorLeave method.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22992799/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22992815", "body": "Same comment here about replacing null with sourceColumn.getExpressionStr().\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22992815/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22993089", "body": "Move this if outside, as it'll apply for functional indexes as well. In the grammar, when you declare an indexed column as DESC, then this will be true. Setting this bit let's us know that we need to invert the bits after evaluating the expression.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22993089/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22993159", "body": "Yes, this looks right.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22993159/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22993260", "body": "We should be able to invert a function expression (any expression in the ON list for an index definition)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22993260/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22993421", "body": "This will likely be a b/w compat issue (as old clients wouldn't serialize this. We may need to do something somewhat hacky, like negate an already serialized value as a flag that this index maintainer may contain expressions.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22993421/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22993446", "body": "There's not a great way to do this. Maybe just have a constant size and multiply by the number of functional expressions.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22993446/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22993543", "body": "Use expression.getMaxLength() and expression.getScale() instead.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22993543/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22993796", "body": "I think here you'll want to check if expressionCompiler.isAggregate() and throw here if that's the case. It wouldn't be valid to use aggregate functions in a functional index.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22993796/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22993837", "body": "Tell me more about this. Perhaps use the expression.toString() as the name?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22993837/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22997007", "body": "Another check you'll want is if expression.getDeterminism() != ALWAYS then throw. That'll rule out NEXT VALUE FOR and CURRENT_DATE() which don't really make sense (and can't be evaluated on the server side anyway).\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22997007/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22997129", "body": "Also, the removal of the indexedColumnsTypes will be a problem. You'll need to not do that plus do your new serialization based on the hacky way I mentioned (we should protobuf this at the next major release to give us more flexibility here).\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22997129/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22997186", "body": "How does this interop with having a mix of pure data columns in the ON clause with functional expressions? The order is significant as far as I remember, so I think you'll want to have both in the same list.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/22997186/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23121956", "body": "Ah, yes, that's an issue we'll need to address. You'll want to translate the functional expression using the same method we use for the index statement rewritter. Then it should use it without the cast being necessary.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23121956/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23123213", "body": "Actually re-writing the functional index expressions before the visitor traversal by ExpressionCompiler will simplify things quite a bit. The tricky bit is managing what happens when a column cannot be resolved. If there are functional indexes, you don't want to throw and stop processing, as you may find a functional index that matches still. However, if you don't find a functional index, you need to throw the ColumnNotFoundException (as local indexes tolerate this and collect them, while global ones don't). A slight tweak of this may be in order for local indexes to just collect columns that cannot be resolved (instead of catching the ColumnNotFoundException).\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23123213/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275041", "body": "That's just one of the conversions we do. I'd rather not duplicate the logic here. I think if we just use the IndexStatementRewriter to rewrite the functional expression, we can compare apples-to-apples and just check for equality on the way out of the visitLeave methods. Can't seem to find my earlier comment, but also, don't duplicate all the visitEnter/visitLeave methods. Instead, make the ExpressionCompiler.wrapGroupByExpression() method public and override it here (rename it to something more general like transformExpression too).\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275041/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275183", "body": "Yes, I think it's better to only check for a match in the visitLeave (see comment about not having to specialize all visitLeave methods, but relying on overriding a wrapGroupByExpression method instead).\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275243", "body": "Yes, same behavior for an old client talking to new server. Will this be difficult?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275243/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275302", "body": "Does it help if you treat them the same, as a regular column is an expression if you parse it?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23275302/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23278372", "body": "I meant to create a map from <Functional Expression> -> <Index Column Expression> where you use this to look for occurrences of the functional expression to replace with a reference to the column expression in the index table. You'd do a get into the map from the ExpressionCompiler.wrapGroupByExpression() which is called on each visitLeave method. If you find an entry, you use the ColumnExpression into the index table, otherwise, you use the original expression. This is the expression you provide as an argument to super.wrapGroupByExpression(). Let's talk in person tomorrow if this doesn't make sense.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23278372/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23278380", "body": "The rest of this class will no longer be necessary.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23278380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23278448", "body": "A nice-to-have would be to not need this catch. If col.getExpressionStr() by default could evaluate to what is calculated above, that would be ideal.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23278448/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23278539", "body": "Not crazy about having to do this null check as it may cascade to a lot of places. You may be able to create a PhoenixConnection from IndexMaintainer if need be.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23278539/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23279803", "body": "The IndexMaintainer is sent from the client to the server - it's like a compiled index definition. You'll just need to make sure that when you deserialize the data, you can detect that it's from an old client and you can read it correctly to establish the correct state. Remember, you'll know in this case that there are no functional indexes.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23279803/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23663940", "body": "You can go through this constructor that only requires a PDatum (which you can define inline):\n\n```\npublic KeyValueColumnExpression(PDatum column, byte[] cf, byte[] cq) {\n    super(column);\n    this.cf = cf;\n    this.cq = cq;\n}\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23663940/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23666966", "body": "Why is this? That seems strange.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23666966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23667242", "body": "Why these changes?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23667242/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23667329", "body": "Is this throw still req'd?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23667329/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23667480", "body": "What throws a ClassNotFoundException?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23667480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23667592", "body": "Remove\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23667592/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23667770", "body": "Will indexColumn.getExpressionStr() be null for a non functional index? If so, instead of the instanceof check for ColumnExpression below, can we do that here based on the null check?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23667770/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23668047", "body": "Don't we still read and write indexedColumnTypes? Seems like we wouldn't need this.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23668047/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23668433", "body": "We should be able to store the expressionStr for a functional index in a case sensitive manner. You'll get this for free when you parse and compile the functional expression. If a referenced column is not surrounded with double quotes, then it'll be upper cased, otherwise it won't. Then the equality check will always be a straight equals check.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23668433/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23668462", "body": "Remove if not needed\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23668462/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23668528", "body": "Why is this needed?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23668528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23668579", "body": "Remove TODO\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23668579/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23668616", "body": "Is this change necessary?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23668616/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23668722", "body": "Probably easier is to always treat column references in functional expressions as case sensitive. If they had double quotes originally, they'd have been left as-is, and if they didn't, they'd be upper-cased.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23668722/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23669302", "body": "I think you can likely do this by adding a boolean to the ParseNodeFactory constructor (or deriving your own) that determines if column, table, schema references are case sensitive. You'll use this option/subclass for your SQLParser when you parse the expressionStr. Just specialize the following methods:\n\n```\npublic ColumnParseNode column(TableName tableName, String name, String alias) {\n    return new ColumnParseNode(tableName,name,alias);\n}\n\npublic TableName table(String schemaName, String tableName) {\n    return TableName.createNormalized(schemaName,tableName);\n}\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23669302/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23669771", "body": "Why is the string replace necessary? Note that by going from parseNode -> expression -> string, you'll have case sensitive column, table, and schema references\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23669771/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23669823", "body": "You should normalize the parseNode too. This puts constants on the RHS as expected among other things. You can do this like this:\n\n```\nparseNode = StatementNormalizer.normalize(parseNode, resolver);\n```\n\nA good test is to use a constant on the RHS and it'll be moved to the LHS (like in a CASE WHEN statement).\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23669823/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23669864", "body": "Minor: indenting looks off here.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23669864/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23670061", "body": "It'd be nice to get rid of this special logic (you know how I love instanceof checks :-) ). One way would be to have your own ExpressionCompiler below where you specialize the resolveColumn call. You know you've got only a single column reference if visitor.isTopLevel() is true in the resolveColumn call.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23670061/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23670722", "body": "I think this may not be worth adding, as the more methods we have here, the higher the burden of adding new expressions. The only place we use this is for the IndexMaintainer to serialize it's bytes (true?), but we'll likely move toward Protobufs in the next major release. Could we have a crude estimate instead? Maybe based on total number of expressions in the tree (which we could track in ExpressionCompiler in this method (by just have a totalNodeCount member variable, incrementing it here by one, and resetting it to zero in reset()):\n\n```\n @Override\npublic void addElement(List<Expression> l, Expression element) {\n    nodeCount--;\n    l.add(element);\n}\n```\n\nThoughts? \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23670722/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23673555", "body": "Or a better way is to change unusedPkColumns to be a Set<Expression>. These can be created directly through\n\n```\nList<PColumn> pkColumns = table.getPKColumns();\nList<Expression> unusedPkColumns = new LinkedHashSet(pkColumns.size());\nfor (int i = 0; i < pkColumns; i++) {\n    unusedPkColumns.add(new RowKeyColumnExpression(pkColumns.get(i), \n        new RowKeyValueAccessor(pkColumns, i));\n}\n```\n\nThen, here you can get rid of the special case, and instead after compilation, do the following:\n\n```\nif (expression.isStateless()) { // Will be true for any constant (including a view constant)\n    continue;\n}\nunusedPkColumns.remove(expression);\n```\n\nWe don't need to include an expression that evaluates to a constant in the index. You can test this by adding a function index on something like 1 + 2.\n\nIn this way, you can remove the if statement completely.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23673555/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23673742", "body": "Also, this is where you'd want a SQLParser that makes all column, table, and schema references as case sensitive (i.e. for ColumnParseNode). You could just have a parseCaseSensitiveExpression static method.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23673742/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23673808", "body": "Here too, create/use SQLParser.parseCaseSensitiveExpression()\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23673808/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23674358", "body": "Instead of comparing type and value for equals, do this:\n\n```\nreturn type.isComparableTo(other.type) && type.compareTo(value, other.value, other.type) == 0;\n```\n\nThis will ensure that a CHAR type of 'AAA' will equal a VARCHAR type of 'AAA' and a DECIMAL type with a value of 5.0 will equal an INTEGER type with a value of 5.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23674358/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23708841", "body": "Actually, thinking about this a bit more, you'll want to force schema, table, and column names to be surrounded by double quotes when the expression.toString() is done. This will prevent any parse errors for any strange names and ensure case sensitivity which is what you want. The column.toString() is based off of the displayName passed into ColumnRef. The displayName is actually gotten from the TableRef which is created in the ColumnResolver. So... you'll need to add an option in the ColumnResolver (in FromCompiler) that indicates that you want column references surrounded by double quotes (i.e. case sensitive). This info can be passed through the TableRef constructor and applied in the getDisplayName() method.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23708841/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23710801", "body": "If the null check idea above won't work, instead of the instanceof check below, can you track in the ExpressionCompiler if you've compiled an expression versus only a column that maps to the PK column in the data table? You could specialize the resolveColumn method of your ExpressionCompiler and check visitor.isTopLevel() and SchemaUtil.isPKColumn(IndexUtil.getDataColumn(dataTable, indexColumn.getName().getString()))\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/23710801/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "aaraujo": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/5a21734f10a90fa8de0dce390aedc8edeb52b26c", "message": "PHOENIX-4168 Pluggable Remote User Extraction\n\nAdds factory for creating RemoteUserExtractor instances. The factory\ncan be overridden using ServiceLoader. The default factory creates\nPhoenixRemoteUserExtractor instances.\n\nSigned-off-by: Josh Elser <elserj@apache.org>"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/263", "title": "PHOENIX-3817 Verify replication tool", "body": "MapReduce tool that compares data accross tables. SQL conditions may be\r\nused to optionally compare only a subset of the tables.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jmahonin": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/ad52201e07670d342ef33c5e8bd2ee595fe559cc", "message": "PHOENIX-4159 phoenix-spark tests are failing\n\nFix usage of underlying JUnit 'TemporaryFolder' in phoenix-spark\ntests. Need to disable parallel execution until JUnit 4.13 is\nreleased (https://github.com/junit-team/junit4/issues/1223)"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88176830", "body": "Thanks for the feedback @mravi , point comments below:\n\n1: Right, I'll try get that sorted out. The original phoenix-spark library would not work with 1.7 for some reason, but that may no longer be the case.\n\n2: Good catch. I think IntelliJ did something a little funny here on me, that file was supposed to be in the main hierarchy.\n\n3 / 4: It's my first kick at extending Spark (and Phoenix for that matter), but the naming scheme and file separation was modelled off of DataStax' Spark-Cassandra connector, which I figured is as good a model as any:\nhttps://github.com/datastax/spark-cassandra-connector/tree/master/spark-cassandra-connector/src/main/scala/com/datastax/spark/connector\n\nIn theory, doing it that way means a user can have just one import to get all the nice implicit definitions:\n`import org.apache.phoenix.spark._`\n\n5: I've never had much luck with getting the Scala integration working well on any IDE, I just run 'mvn test' from the CLI.\n\nRe: Good to haves\n1. I totally agree, but I don't think I can afford the cycles at the moment. My hope was that by modelling after the spark-cassandra-connector, it would be relatively painless to add for either a third party, or myself in the hopefully not-too-distant-future.\n2. Great idea, I hadn't actually seen that usage with Spark SQL yet. We're still using the RDD API internally. On a quick glance it looks fairly straight-forward to implement.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88176830/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88179901", "body": "@mravi Great. I can get 1 and 2 taken care of today.\n\nAre new commits on this PR fine, or would you prefer a new PR with a rebased commit?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88179901/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88226194", "body": "JDK 1.7 and the ProductRDDFunctions package location have been fixed up.\n\nI tried to make some headway on getting the unit tests to run in the IDE. If you're seeing the same errors I am, it may be because you're using OS X. \n\nThere's a \"Netty transport\" bind error, which can be solved by adding \"SPARK_LOCAL_IP=127.0.0.1\" to the environment variables. However, after's fixed that I end up with HBase minicluster bind errors. There seem to be instructions here to fix those, which should also solve the Netty issue above, but it hasn't worked for me at all:\nhttp://stackoverflow.com/questions/18717681/hbasetestingutility-could-not-start-my-mini-cluster\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88226194/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88285575", "body": "Can confirm the memory settings needed adjustment on 7u76 on Linux.\n\nSpecial thanks to @robdaemon who had an excellent library to work with, and a pre-emptive thanks to @dacort for the copyrights!\n\nMy preference would be to put the RelationProvider work under a new ticket. It's separate functionality, and although I've made a bit of headway there, testing against the current SparkSQLContext API is yielding some bizarre results. \n\nIt's fairly likely I'm doing something wrong, but that's a brand new API for Spark. Most of the methods have @DeveloperAPI annotations (read: unstable), and a number of fixes are slated for Spark SQL in 1.3.1, so I expect a bit of churn in that area for the time being.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88285575/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88569122", "body": "I was able to spend a bit more time on the RelationProvider work. The DDL for custom providers doesn't work through the 'sql()' method on SparkSQLContext, perhaps by design or perhaps due to a bug, but the 'load()' method does work to create DataFrames using arbitrary data sources.\n\nI'm still not entirely familiar with their API, and a lot of it is still very new and probably subject to churn, but I tried to base the it on existing examples in the Spark repo, such as the JDBC and Parquet data sources.\n\nI've got a new commit on a side branch here:\nhttps://github.com/FileTrek/phoenix/commit/16f4540ef0889fc6534c91b8638c16001114ba1a\n\nIf you're all OK with those changes going in on this PR, I can push them up here. Otherwise, I'll stash them aside for a new ticket.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88569122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/89020951", "body": "@mravi Sounds good. FYI, I was able to run the PhoenixRDDTests in IntelliJ on Linux with no modifications using the 'ScalaTest' configuration.\n\nIn summary, I think these are the follow-up tasks that need tickets:\n- Support parallel execution in phoenix-spark unit tests\n- Add support for Spark data source to phoenix-spark module\n- Add Java API for phoenix-spark\n\nI can put those into JIRA if needed.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/89020951/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/91239667", "body": "Merged in 'master' to update with new integration tests\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/91239667/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/92343659", "body": "Thanks for the review @mravi \n\nThat HBaseConfiguration.create() step is a great idea, I'll make that change ASAP.\n\nRe: naming scheme, I'd attempted to follow Cassandra-Spark connector, since there's not yet too much available for reference code, but also the feature sets would be relatively closely aligned:\nhttps://github.com/datastax/spark-cassandra-connector/tree/master/spark-cassandra-connector/src/main/scala/com/datastax/spark/connector\n\nAlthough I'm not completely married to the idea, both Datastax (Cassandra) and Databricks (Spark) seem to follow a _Functions.scala scheme, where _ is the class to which implicit helper parameters are being attached. In this case, the new 'ProductRDDFunctions' applies the implicit helper function 'saveToPhoenix' to objects of type RDD[Product], or an RDD of tuples.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/92343659/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/92903008", "body": "Re: step c, I would lean towards not including either the spark or scala library JARs. They are provided by the Spark runtime itself, so I'm not sure it makes sense to bundle them within the phoenix assembly JAR. Does that make sense to you?\n\nref:\nhttps://spark.apache.org/docs/latest/submitting-applications.html\nhttps://github.com/sbt/sbt-assembly#-provided-configuration\n\nAfter those, I think the only other runtime dependency not already in the all-common-jars file is one for snappy-java, which I'm not sure is explicitly needed anymore that we're part of a multi-module build. I will double-check.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/92903008/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/92981336", "body": "I misread your review at first and had thought you asked to update all-common-dependencies, rather than all-common JARS.\n\na), b) and c) are all addressed in the latest commit here, and final patchfiles are attached to the JIRA ticket.\n\nThanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/92981336/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/120925695", "body": "Thanks for the patch. I'll look into getting this in ASAP.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/120925695/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/120999971", "body": "@tianyi Could you try out the revised patch on the ticket [1], and let me know if it works for you?\n\n[1] https://issues.apache.org/jira/browse/PHOENIX-2112\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/120999971/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/121215183", "body": "@tianyi Fantastic. Your patch has already been committed here, so don't worry about adjusting this PR:\nhttps://github.com/apache/phoenix/commit/70c542d8e7b8bdcea58e2ef8cbf76143ec5ae66c\n\nThanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/121215183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/121979444", "body": "Yup, we've got it in for the next release. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/121979444/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/133939400", "body": "Sure thing, on a quick glance on mobile this looks good, but I'll try spend some time with it tomorrow.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/133939400/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/134629729", "body": "@randerzander Not sure if you saw, but I've got a new version of your patch up at https://issues.apache.org/jira/browse/PHOENIX-2196\n\nIf you could take a quick look at let me know if that works for you I'll get it in ASAP.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/134629729/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/150211117", "body": "This looks great @navis \n\nThe Spark portion looks fine. I'll leave the updates to ColumnInfo for @ravimagham @JamesRTaylor et. al. to review\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/150211117/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/151491230", "body": "How's this look, @JamesRTaylor / @ravimagham ?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/151491230/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/160761094", "body": "Hi @gliptak \n\nI tried to run the tests with the patch applied, and got a number of failures. I think that 'beforeAll' and 'afterAll' aren't actually getting invoked in with the JUnitSuite for some reason.\n\nI tried adding the '@BeforeClass' and '@AfterClass' annotations, but JUnit warns that the methods should be static. Sadly, I think the only way to make that work is with a companion object (e.g. 'object PhoenixSparkIT' which contains the beforeAll and afterAll definitions).\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/160761094/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/160831941", "body": "Definitely closer, it does run now, but it takes about 4x longer to run than before. The 'doSetup' and 'doTeardown' used to be run only at the very beginning and at the very end, and now they're being run before every test due to the '@Before' and '@After' annotations.\n\nI wonder if it's possible to do something like the 'ScalaClassRuleTest' in [1] and either specify a '@BeforeAll' or an '@ClassRule' in a companion object and move the 'doSetup' and 'doTeardown' there?\n\n[1] http://randomallsorts.blogspot.ca/2012/11/junit-411-whats-new-rules.html\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/160831941/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/161313636", "body": "Got this merged in here:\nhttps://github.com/apache/phoenix/commit/b8faae52c6bee91393678e74de09ab8a215da856\n\nFeel free to close this PR. Thanks again!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/161313636/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/161512711", "body": "Thanks @gliptak for looking into this.\n\nI'll spend a bit of time seeing if I can work something out here. I think the main goal is just to try be more compatible with JUnit annotations in inherited classes, but I do like having these run as fast as possible as well.\n\nI don't think it's necessarily a major problem if we do nothing here either - the issue only became apparent after merging in transaction support, a pretty large infrastructure addition, which only took a few lines to work around with the existing tests.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/161512711/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/162212491", "body": "This is a good idea. There are actually a few implementations of DBWritable in the codebase:\n- PhoenixRecordWritable\n- PhoenixPigDBWritable\n- PhoenixIndexDBWritable\n\nThe code seems very similar across implementations, so we definitely should have one base implementation that works well across the board and can be simply extended, if need be. Having it in 'phoenix-core' is a bonus too.\n\nI'd like to see the phoenix-spark 'PhoenixRecordWritable' renamed to maybe 'PhoenixSparkRecordWritable' and have it trimmed down to implement this new class, but maybe that's better suited to a follow-up PR.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/162212491/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/162538013", "body": "Unless it's already on your radar @ndimiduk , I can take a stab some time this week at updating the scala version to extend the new Java version.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/162538013/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "kliewkliew": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/0a3ef6c1b5f2013ce6b3e5158e0417294ab7d92b", "message": "PHOENIX-2314 Cannot prepare parameterized statement with a 'like' predicate"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/238", "title": "PHOENIX-3690 Support declaring default values in Phoenix-Calcite: add\u2026", "body": "\u2026endum to support new interface after CALCITE-1702", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rahulsIOT": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/c104a4dd3184cc6db5c18f3aa4d562122c3df699", "message": "PHOENIX-3654 Load Balancer for Phoenix Query Server\n\nCloses apache/phoenix#236\n\nSigned-off-by: Josh Elser <elserj@apache.org>"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/242", "title": "PQS metrics -  https://issues.apache.org/jira/browse/PHOENIX-3655 ", "body": "Phoenix Query Server metrics\r\nhttps://issues.apache.org/jira/browse/PHOENIX-3655\r\n\r\nThe following properties is added along with the defaults\r\n1. \"phoenix.query.server.metrics\" - default is true ( means pqs will collect metrics by default )\r\n2. \"phoenix.query.server.metrics.report.interval.ms\" - default is 30 sec ( every 30 sec, the global metrics will be output to sink )\r\n3. \"phoenix.query.server.metrics.type.of.sink\" - default is file  ( other option is slf4j)\r\n4. \"phoenix.query.server.filesink.filename\" - Filename if using fileSink (default is  /tmp/pqsSinkFile)\r\n\r\nif user setup \"phoenix.query.server.metrics\"  in hbase-site.xml to true, we add the following properties to JDBC connection (QueryServices.COLLECT_REQUEST_LEVEL_METRICS=  true). The starts the metrics collect at request level. \r\n\r\nThere are two types of metrics \r\n1. Global - This metrics is collect every interval and pushed to sink ( File or Slf4j). Apart from this, it is also exposed in JMX via Mbeans.\r\n2. Request - These metrics is collected at statement and connection close level. Once collected, it is pushed to the sink. These metrics are not exposed via JMX as metrics are for specific table requests.  For requests level metrics, we have file and Slf4j ( LOG) as types of sink. So, the metrics will be pushed to a specified file or log file .\r\n\r\n\r\nFor details on Phoenix Metrics - refer - https://phoenix.apache.org/metrics.html\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bijugs": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/671b10c6e8878ae381b4c8260c3e895fd97441e7", "message": "PHOENIX-3954 Update error message to be more user friendly\n\nSigned-off-by: gjacoby <gjacoby@apache.org>"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/258", "title": "PHOENIX-3921 Change the condition checking in ScanUtil#isReversed", "body": "The current logic will return ``isReversed`` as ``true`` whether the ``BaseScannerRegionObserver.REVERSE_SCAN`` attribute is set to ``PDataType.TRUE_BYTES`` or ``PDataType.FALSE_BYTES``. The PR is to change it to return ``true`` only if  ``BaseScannerRegionObserver.REVERSE_SCAN`` attribute is set to ``PDataType.TRUE_BYTES``.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/249", "title": "PHOENIX-3878 Add license headers missed in PHOENIX-3572", "body": "Add Apache license headers missed in ``PHOENIX-3572``.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pboado": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/286", "title": "Sync 4.x-HBase-1.2 to master", "body": "This PR syncs 4.x-HBase-1.2 to master branch", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xsq0718": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/280", "title": "indextool inedxTable is not an index table for dataTable", "body": "Phoenix;phoenix-4.8.0-cdh5.8.0\r\nHbase;1.2.0\r\nCreate phoenixTable ;\r\nCREATE Table \"everAp\"(pk VARCHAR PRIMARY KEY,\"ba\".\"ap\" varchar,\"ba\".\"ft\" varchar,\"ba\".\"et\" varchar,\"ba\".\"n\" varchar);\r\n\r\nCreate Index;\r\ncreate local index EVERAP_INDEX_AP on \"everAp\"(\"ba\".\"ap\") async;\r\nUse indexTool\uff1b\r\n./hbase org.apache.phoenix.mapreduce.index.IndexTool -dt \\\"\\\"everAp\\\"\\\" -it EVERAP_INDEX_AP -op hdfs:/hbase/data/default/everApIndc\r\n\r\n\r\n/cloudera/parcels/CDH-5.8.2-1.cdh5.8.2.p0.3/lib/hbase/bin/../lib/native/Linux-amd64-64\r\n17/11/02 15:08:09 INFO zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp\r\n17/11/02 15:08:09 INFO zookeeper.ZooKeeper: Client environment:java.compiler=<NA>\r\n17/11/02 15:08:09 INFO zookeeper.ZooKeeper: Client environment:os.name=Linux\r\n17/11/02 15:08:09 INFO zookeeper.ZooKeeper: Client environment:os.arch=amd64\r\n17/11/02 15:08:09 INFO zookeeper.ZooKeeper: Client environment:os.version=2.6.32-504.el6.x86_64\r\n17/11/02 15:08:09 INFO zookeeper.ZooKeeper: Client environment:user.name=root\r\n17/11/02 15:08:09 INFO zookeeper.ZooKeeper: Client environment:user.home=/root\r\n17/11/02 15:08:09 INFO zookeeper.ZooKeeper: Client environment:user.dir=/opt/cloudera/parcels/CDH-5.8.2-1.cdh5.8.2.p0.3/bin\r\n17/11/02 15:08:09 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=slave1:2181,slave2:2181,master:2181 sessionTimeout=60000 watcher=hconnection-0x4470f8a60x0, quorum=slave1:2181,slave2:2181,master:2181, baseZNode=/hbase\r\n17/11/02 15:08:09 INFO zookeeper.ClientCnxn: Opening socket connection to server master/192.168.0.250:2181. Will not attempt to authenticate using SASL (unknown error)\r\n17/11/02 15:08:09 INFO zookeeper.ClientCnxn: Socket connection established, initiating session, client: /192.168.0.250:53140, server: master/192.168.0.250:2181\r\n17/11/02 15:08:09 INFO zookeeper.ClientCnxn: Session establishment complete on server master/192.168.0.250:2181, sessionid = 0x35f518ca651786a, negotiated timeout = 60000\r\n17/11/02 15:08:10 INFO metrics.Metrics: Initializing metrics system: phoenix\r\n17/11/02 15:08:10 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-phoenix.properties,hadoop-metrics2.properties\r\n17/11/02 15:08:10 INFO impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\r\n17/11/02 15:08:10 INFO impl.MetricsSystemImpl: phoenix metrics system started\r\n17/11/02 15:08:11 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available\r\n17/11/02 15:08:12 ERROR index.IndexTool: An exception occurred while performing the indexing job: IllegalArgumentException:  EVERAP_INDEX_AP is not an index table for everAp  at:\r\njava.lang.IllegalArgumentException:  EVERAP_INDEX_AP is not an index table for everAp \r\n\tat org.apache.phoenix.mapreduce.index.IndexTool.run(IndexTool.java:190)\r\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)\r\n\tat org.apache.phoenix.mapreduce.index.IndexTool.main(IndexTool.java:394)\r\n\r\nYou have mail in /var/spool/mail/root\r\n\r\n**help!!!**", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shehzaadn": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/275", "title": "PHOENIX-4237: Add function to calculate Java collation keys", "body": "Here we implement a generalized solution for calculating Java collation keys by creating Java collators based on a user locale. These collation keys can then be used in an ORDER BY clause to sort strings in a natural-language-appropriate way. We add a new Phoenix function COLLKEY. In general usage for this function will be:\r\n\r\nselect name from my_table order by COLLKEY(name, 'zh_TW')\r\n\r\nWe use artifacts from the ICU4J project and recently open-sourced grammaticus project (by Maven dependency). We were forced to include some code from ICU4J because some jars produced by that project aren't published in Maven. We also include code from Salesforce that has been licensed for open-source release but not yet published as artifacts in maven.\r\n\r\nThere are three commits that split the changes into three logical pieces:\r\n\r\n1) f8cb121: Add the external source code described above\r\n2) fdbb5e0: Make changes needed to the Phoenix license due to the above (and fix to what seems to be an existing bug) \r\n3) 98cfc10: The actual function implementation of COLLKEY - new code that uses the code introduced above and newly introduced dependencies via maven.\r\n\r\nThanks in advance to the Phoenix community for your feedback on this.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "daiamo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/274", "title": "4.8 h base 1.2 cdh5.8", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "TheRealHaui": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/273", "title": "Add equals verification test", "body": "Adds an equals verification Unit Test.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/272", "title": "avoid-equals-null-problem", "body": "Avoids NullPointerException in any case.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rhshriva": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/271", "title": "PHOENIX-4180 - Modify tests to generate unique table names and not us\u2026", "body": "The following tests was changed. \r\n\r\n./phoenix-core/src/it/java/org/apache/phoenix/end2end/ArrayIT.java\r\n./phoenix-core/src/it/java/org/apache/phoenix/end2end/ClientTimeArithmeticQueryIT.java\r\n./phoenix-core/src/it/java/org/apache/phoenix/end2end/ColumnProjectionOptimizationIT.java\r\n./phoenix-core/src/it/java/org/apache/phoenix/end2end/ConcurrentMutationsIT.java\r\n./phoenix-core/src/it/java/org/apache/phoenix/end2end/CursorWithRowValueConstructorIT.java\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "szq80140": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/270", "title": " `DEFAULT_TIME_ZONE_ID`  should take value from `TimeZone.getDefault().getID()` instead of constant `GMT`", "body": " `DEFAULT_TIME_ZONE_ID`  should take value from `TimeZone.getDefault().getID()` instead of constant `GMT`, so that all `DATE`  and  `TIMESTAMP`  column values are formatted  using timezone configured in JVM.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kaisenc": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/267", "title": "4.10 h base 1.2", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "asdf2014": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/264", "title": "PHOENIX-3961 Should delete `tableOutputPath` in `completebulkload`", "body": "For solving the situation that too many `.tmp` files in /tmp directory when processing a huge `bulkload` job, we should consider deleting `tableOutputPath` in `completebulkload`.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "akshita-malhotra": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/260", "title": "PHOENIX-3812: Use HBase snapshots in async index building M/R job", "body": "- Index tool creates a snapshot and uses it as a configuration parameter to run index M/R job using HBase snapshot.\r\n- Add option to configure use of snapshots in IndexTool", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/256", "title": "PHOENIX-3477 patch for 4.x-HBase-1.1", "body": "PHOENIX-3477 patch for 4.x-HBase-1.1", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/255", "title": "PHOENIX-3744 for 4.x-HBase-0.98", "body": "PHOENIX-3744 patch for 4.x-HBase-0.98 branch", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/239", "title": "PHOENIX-3744: Support snapshot scanners for MR-based Non-aggregate queries", "body": "- Parallel Scan grouper is extended to differentiate the functionality for getting region boundaries\r\n\r\n- Added integration test, compares the snapshot read result with the result from select query by setting CurrentScn value.\r\n\r\n- the configuration parameter is the snapshot name key, if set do a snapshot read\r\n\r\n- Used an existing PhoenixIndexDBWritable class for the purpose of testing, will add a new one as I will add more tests.\r\n\r\n- ExpressionProjector functionality is extended for snapshots as the keyvalue format returned from TableSnapshotScanner is different from ClientScanner and therefore not properly interrupted by Phoenix thereby returning null in case of projected columns.\r\nFor the same table, following shows the different format of the keyvalues:\r\n\r\n1. ClientScanner:\r\nkeyvalues={AAPL/_v:\\x00\\x00\\x00\\x01/1493061452132/Put/vlen=7/seqid=0/value=\u0004SSDD\u0000\u0001}\r\n\r\n2. TableSnapshotScanner:\r\nkeyvalues={AAPL/0:\\x00\\x00\\x00\\x00/1493061673859/Put/vlen=1/seqid=4/value=x, \r\nAAPL/0:\\x80\\x0B/1493061673859/Put/vlen=4/seqid=4/value=SSDD}\r\n\r\n@JamesRTaylor @lhofhansl \r\n\r\nTo DO:\r\nAdd more integration tests to cover different scenarios such as where clause etc\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dizzy2": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/259", "title": "fixed setting of jetty resource base directory in phoenix-tracing-web\u2026", "body": "fixed obtaining of resource base for embedded jetty in tracing web appp - original implementation of obtaining webapp root from ProtectionDomain is not reliable and doesn't work on some JVMs. New implementation gets the correct webapp location from class loader.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ssanthanam-sfdc": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/254", "title": "PHOENIX-3903 Generate empty javadoc jar to comply with Maven central", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/253", "title": "PHOENIX-3903 Generate empty javadoc jar to comply with Maven central", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/252", "title": "PHOENIX-3903 Generate empty javadoc jar to comply with Maven central", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/251", "title": "Generate empty javadoc jar to comply with Maven Central", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/250", "title": "Generate empty javadoc jar to comply with Maven Central", "body": "Im guessing I need to add same change to the following repositories \r\n\r\n4.10-HBase-1.1, 4.10-HBase-1.2, 4.x-HBase-0.98, 4.x-HBase-1.1, 4.x-HBase-1.2 ?", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "churrodog": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/248", "title": "Phoenix 3534", "body": "I'll squash these down after a review ", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/158", "title": "PHOENIX-2822 - Tests that extend BaseHBaseManagedTimeIT are very slow", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "anirudha": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/245", "title": "Harden Phoenix - distributed big link list and atomicity tests over phoenix", "body": "Adds:\r\n-a new testing utility for test with multiple threads.\r\n-a new test driver to execute tests against an existing cluster.\r\n-two new basic tests from the HBase testing framework.\r\n\r\nPlease enter the commit message for your changes. Lines starting\r\n\r\nthis is still work in progress..\r\nGabriel Jimenez\r\nMassachusetts Institute of Technology\r\nBloomberg LP (intern) \r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "apurtell": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/240", "title": "PHOENIX-3808 Implement chaos tests using HBase's hbase-it facility", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88248990", "body": "I'm not a Spark expert @JamesRTaylor  . I skimmed the latest. Allowing builds with JDK 1.7 would have been the big change I'd have recommended, and it's already been done.\n\nI checked out this PR and ran a build, which completed. I was able to run the unit tests of the new module from the Maven command line, on Linux FWIW:\n\n```\n$ mvn -DskipTests clean install\n$ mvn test -rf :phoenix-spark \n[...]\n- Can create valid SQL\n- Can convert Phoenix schema\n- Can create schema RDD and execute query\n- Can create schema RDD and execute query on case sensitive table (no config)\n- Can create schema RDD and execute constrained query\n- Using a predicate referring to a non-existent column should fail\n- Can create schema RDD with predicate that will never match\n- Can create schema RDD with complex predicate\n- Can query an array table\n- Can read a table as an RDD\n- Can save to phoenix table\n- Can save Java and Joda dates to Phoenix (no config)\n- Not specifying a zkUrl or a config quorum URL should fail\nRun completed in 1 minute, 12 seconds.\nTotal number of tests run: 13\nSuites: completed 2, aborted 0\nTests: succeeded 13, failed 0, canceled 0, ignored 0, pending 0\nAll tests passed.\n```\n\nWith 7u75 I run out of PermGen running PhoenixRDDTest, but fixed that:\n\n```\ndiff --git a/phoenix-spark/pom.xml b/phoenix-spark/pom.xml\n index 5c0c754..21baa16 100644\n--- a/phoenix-spark/pom.xml\n+++ b/phoenix-spark/pom.xml\n@@ -503,6 +503,7 @@\n             <configuration>\n               <parallel>true</parallel>\n               <tagsToExclude>Integration-Test</tagsToExclude>\n+              <argLine>-Xmx3g -XX:MaxPermSize=512m -XX:ReservedCodeCacheSize=512m\n             </configuration>\n           </execution>\n           <execution>\n```\n\nThe unit tests are not robust against parallel execution with other HBase or Phoenix test suite invocations on the same host, but this can be fixed with a followup issue with port assignment from random ranges with retries using new port candidates on bind errors. \n\nLGTM for a commit to trunk with some minor follow-ups. \n\n> Extend the org.apache.spark.sql.sources.RelationProvider and have PhoenixDatasource.\n\nMaybe we should split this work up. The integration as-is is directly useful on its own. The SparkSQL integration nice-to-have can be additional work on a new JIRA / PR?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88248990/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "1emo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/237", "title": "4.9 h base 1.2", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "comnetwork": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/235", "title": "PHOENIX-3745", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/194", "title": "PHOENIX-3199", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gabrielreid": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/231", "title": "PHOENIX-3471 Add query plan matching system", "body": "Add a generic system for parsing and matching Calcite query\r\nplans using Hamcrest matchers. The general intention is to make\r\nmatching of query plans less brittle and somewhat easier to write\r\nthan simply matching the full text of the query plan.", "author_association": "MEMBER"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/57537014", "body": "I'm definitely a fan of the idea of moving the stuff for reading and writing to a general mapreduce package! \n\nLooks like there is still quite a bit of javadoc fixing up to do in there as well (there are a number of class descriptions that just say something like \"Describe your class here\". I'll go through and add a few comments on a few things I think might be worthwhile to consider doing differently as well.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/57537014/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/82582761", "body": "This code looks pretty good (with the exception of a few code-formatting issues). I think that the test coverage should be extended a bit though, with the following things being added:\n- tests using both sort orders (ascending and descending)\n- a test where a the input value is null\n- a test where the search string isn't found to verify the return value in that situation\n- a test where the function is used in a filter (as this ensures that the function is serialized/deserialized)\n\nSpecifically around the serialization/deserialization, there will be a no-args constructor needed, as well as an init method to initialize the search string. See the implementation of RegexpSplitFunction for an example of how this is done. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/82582761/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/82584873", "body": "A couple of additional test cases which would also be good:\n- testing with strings that contains multi-byte characters\n- testing with empty (but not null) strings\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/82584873/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/83401354", "body": "@naveenmadhire the added test cases look good, but it looks like your most-recent commit also removed the integration test.\n\nThe integration test should include a test like before (basically checking that the function works in a real use case), as well as checking that it works when used as a filter, something like:\n\n```\nselect * from table t where instr(t.value, 'test') > 5\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/83401354/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/85666137", "body": "The tests you've added look good. It looks like the reason that you're getting issues with the DESC tests is because you're returning the SortOrder of the underlying column in the getSortOrder method. Just removing the getSortOrder method completely should resolve this.\n\nIt looks like the filter tests also won't work. This will be because the function instance isn't correctly being deserialized/initialized on the server side. Adding in the following method will ensure that the initialization happens correctly:\n\n```\n@Override\npublic void readFields(DataInput input) throws IOException {\n    super.readFields(input);\n    init();\n}\n```\n\nI also took a closer look at the actual testing of the string searching functionality. It looks to me like this could be a lot more simple, although maybe I'm missing something. However, wouldn't it work to just do the following in the evaluate method:\n\n```\n@Override\npublic boolean evaluate(Tuple tuple, ImmutableBytesWritable ptr) {\n\n    if (!getStringExpression().evaluate(tuple, ptr)) {\n        return false;\n    }\n\n    String sourceStr = (String) PVarchar.INSTANCE.toObject(ptr, getStringExpression().getSortOrder());\n\n    int position = sourceStr.indexOf(strToSearch);\n    ptr.set(PInteger.INSTANCE.toBytes(position));\n    return true;\n}\n```\n\nCould you see if that works ok for all test cases, etc? It looks like the code in the `searchString` method is doing a lot of unnecessary array copying otherwise.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/85666137/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/85670735", "body": "Patch looks good, but I've got the one remark/question that I put on the commit. Can you take a look at that and let me know what you think?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/85670735/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/85894438", "body": "I don't think it's the same issue in TIMEZONE_OFFSET because the timezone cache isn't static, so (as far as I know) it will only be accessed by a single thread. That being said, I think it makes sense to make the same change there as well, if you're up for doing that.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/85894438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/85937432", "body": "Making it more abstract and not having duplication of code definitely sounds like the way to go, if you're up for doing that it would be great.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/85937432/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/86879632", "body": "@tzolkincz I just tried running this on a Phoenix install, and there seems to be an issue with the code now if I use the CONVERT_TZ function over multiple rows.\n\nI'm able to replicate it in ConvertTimezoneFunctionIT with the following test:\n\n```\n@Test\npublic void testConvertMultipleRecords() throws Exception {\n    Connection conn = DriverManager.getConnection(getUrl());\n    String ddl = \"CREATE TABLE IF NOT EXISTS TIMEZONE_OFFSET_TEST (k1 INTEGER NOT NULL, dates DATE CONSTRAINT pk PRIMARY KEY (k1))\";\n    Statement stmt = conn.createStatement();\n    stmt.execute(ddl);\n    stmt.execute( \"UPSERT INTO TIMEZONE_OFFSET_TEST (k1, dates) VALUES (1, TO_DATE('2014-03-01 00:00:00'))\");\n    stmt.execute( \"UPSERT INTO TIMEZONE_OFFSET_TEST (k1, dates) VALUES (2, TO_DATE('2014-03-01 00:00:00'))\");\n    conn.commit();\n\n    ResultSet rs = stmt.executeQuery(\n            \"SELECT k1, dates, CONVERT_TZ(dates, 'UTC', 'America/Adak') FROM TIMEZONE_OFFSET_TEST\");\n\n    assertTrue(rs.next());\n    assertEquals(1393596000000L, rs.getDate(3).getTime()); //Fri, 28 Feb 2014 14:00:00\n    assertTrue(rs.next());\n    assertEquals(1393596000000L, rs.getDate(3).getTime()); //Fri, 28 Feb 2014 14:00:00\n    assertFalse(rs.next());\n}\n```\n\nRunning that test results in the following:\n\n```\njava.sql.SQLException: ERROR 201 (22000): Illegal data. Unknown timezone \ufffd\u0000\u0001Dx\ufffdg\u0000Adak\n    at org.apache.phoenix.exception.SQLExceptionCode$Factory$1.newException(SQLExceptionCode.java:362)\n    at org.apache.phoenix.exception.SQLExceptionInfo.buildException(SQLExceptionInfo.java:133)\n    at org.apache.phoenix.schema.IllegalDataException.<init>(IllegalDataException.java:38)\n    at org.apache.phoenix.cache.JodaTimezoneCache.getInstance(JodaTimezoneCache.java:48)\n    at org.apache.phoenix.cache.JodaTimezoneCache.getInstance(JodaTimezoneCache.java:60)\n    at org.apache.phoenix.expression.function.ConvertTimezoneFunction.evaluate(ConvertTimezoneFunction.java:70)\n    at org.apache.phoenix.compile.ExpressionProjector.getValue(ExpressionProjector.java:69)\n    at org.apache.phoenix.jdbc.PhoenixResultSet.getDate(PhoenixResultSet.java:356)\n    at org.apache.phoenix.end2end.ConvertTimezoneFunctionIT.testConvertMultipleRecords(ConvertTimezoneFunctionIT.java:80)\n```\n\nCould you look into what's going wrong?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/86879632/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88218650", "body": "Looks good, and I think that the test cases cover what needs to be covered. \n\nCould you rebase the patch on the master branch, and squash the commits into a single commit? If you're not familiar with how to do this, just let me know and I'll do it when I commit the code.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88218650/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/94018951", "body": "Looks good, except that this breaks sqlline.py for Windows. Could you take a look at how psql.py uses the same approach but also uses subprocess.list2cmdline to allow running it on Windows, and try to use the same approach here?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/94018951/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/94307344", "body": "This has been pushed to master and 4.x branch. Thanks @abisek! \n\n@JamesRTaylor can you close this PR, or give me the required rights to do so?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/94307344/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/98099314", "body": "I'm not really in favor of this change, as it would break the default working of the loader, and because it actually _is_ possible to specify a tab (or other escape-based character) as the separator, although the way of doing this is not well known.\n\nYou can supply a tab character on the command line in the current loader in two ways:\n1. after typing the \"-d\" and a leading single quote, type Ctrl+v and then hit the tab button, and a closing single quote. This provides a tab literal on the command line\n2. enter the -d parameter as -d $'\\t' \n\nThis general issue (and both of these solutions) are not Phoenix-specific, they're just the way in which you provide escape character values on the command line.\n\nI'm going to update the documentation on http://phoenix.apache.org/bulk_dataload.html to include this information.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/98099314/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/139287029", "body": "I think that this is an interesting idea, but did you see my comment on [PHOENIX-2238](https://issues.apache.org/jira/browse/PHOENIX-2238)? There is support built in to the shell which allows you to supply non-printable characters.\n\nI can see how this might be more user-friendly for some, while being confusing for others (who are already familiar with the standard method of providing non-printable characters on the command line). \n\nAnother concern that I have is that providing these kinds of literals that start with backslashes also require some understanding about the rules of escaping when supplying command line parameters, so this kind of makes me think that it might be better to go completely for expecting users to know how to work with the shell properly.\n\nWhat do you think?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/139287029/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/162239436", "body": "Very cool, looks good! I noted a few small nits in the commit, but apart from that it all looks good to me.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/162239436/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "z-york": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/230", "title": "PHOENIX-3603 Fix compilation errors against hbase 1.3.0 release", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "iqrahadian": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/227", "title": "4.8 h base 1.2", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nico-pappagianis": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/226", "title": "Pass tenantId parameter to PhoenixRDD when reading tables", "body": "- Added corresponding tests\r\n- Cleaned up some test code", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/221", "title": "PHOENIX-3427 fix saveToRdd for tenant-specific connections", "body": "@JamesRTaylor @jmahonin \r\n\r\nHere is the fix for writing to tenant-specific objects via RDDs.\r\n\r\nI added an optional tenantId argument to saveToPhoenix. Let me know if there's another way we'd like to approach this (setting it in the conf, perhaps).\r\n\r\nThanks again for all of the help!", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kalyanhadooptraining": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/225", "title": "V4.7.0 h base 1.1", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "danieldingzju": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/209", "title": "open configuration to improve performance and maintainance", "body": "as mentioned in https://phoenix.apache.org/tuning.html.\n\nI could not set the configuration of phoenix, so I'v tired to add the code to solve this problem.\n\nthen I can just set the phoenix like this:\n\n![image](https://cloud.githubusercontent.com/assets/1768553/18822243/319ff8ee-83e0-11e6-8997-2faba1403260.png)\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "AyolaJayamaha": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/202", "title": "PHOENIX-3193 Tracing UI cleanup ", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/195", "title": "Zipkin", "body": "Flipping zipkin chart on vertical axis would be done on the zipkin codebase.\nThe zipkin codebase does not support date/time in milliseconds. \n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/193", "title": "Improvements to Phoenix Web App", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/112", "title": "PHOENIX-2187: Adding front-end of Tracing Web App", "body": "This will include the followingTracing visualization features.\n- [x] List - lists the traces with their attributes\n- [x] Trace Count - Chart view over the trace description\n- [x] Dependency Tree - tree view of trace ids\n- [x] Timeline - timeline of trace ids\n- [x] Trace Distribution - Distribution chart of hosts of traces\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/111", "title": "PHOENIX-2186 : Back end services", "body": "This will include the following components.\n- [x] Main class\n- [x] Pom file\n  - [x] Adding as module for root pom\n- [x] Launch script\n- [x] Backend trace service API\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/103", "title": "PHOENIX-1118 : Tracing Web Application", "body": "# Tracing Web App\n- [x] Web app create from jetty \n  - [x]  Web App creation / build\n  - [x] Pom file mapping to root pom\n  - [x] Starting script in bin file\n- [x] Adding angularjs and google chart\n- [x] Feature list for trace web application\n  - [x] Listing trace in web UI (table view)\n  - [x] Timeline by trace Id\n  - [x] Dependency Tree for trace\n  - [x] Trace distribution over the host\n  - [x] Trace Count by trace trypes\n- [x] Communicating from Java to Javascript\n  - [x] Remove sample data\n- [x] Adding test\n  - [x] Adding test specs for to test JS\n  - [x] Adding test for build\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/96", "title": "Adding tracingweb app", "body": "## Adding tracing web app\n\nThere is no way of visualizing Phoenix tracing information. Adding simple charting to visualize the tracing information is done by this pull request.\n### Feature List\n\nWebapp to visualize tracing information with features to search, expand, filter by sql statements etc.\n### Task List\n- [ ] Web app create from jetty\n  - [x] Web App creation / build\n  - [x] Pom file mapping to root pom\n  - [ ] Starting script in bin file\n- [x] Adding angularjs\n- [x] Adding charts and timeline\n  - [x] Adding Google charts\n  - [x]  Adding d3 and NVD3 Chart\n- [ ]  Communicating from Java to Javascript\n  - [ ] Remove sample data\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/95", "title": "Adding tracingweb app", "body": "# Tracing Web App\n- [x] Web app create from jetty \n  - [x]  Web App creation / build\n  - [ ] Pom file mapping to root pom\n  - [ ] Starting script in bin file\n- [x] Adding angularjs\n- [x] Adding charts and timeline\n  - [x] Adding Google charts\n  - [x] Adding d3 and NVD3 Chart \n- [ ] Communicating from Java to Javascript\n  - [ ] Remove sample data\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/122967254", "body": "### Merge milestone 3.1 PR.\n1. You can build \"TacingWebApp\" by \n   `mvn clean install`\n2. Start the TracingWebApp \n   `java -jar target/phoenix-tracing-webapp-4.5.0-SNAPSHOT-runnable.jar`\n3. View the Content - http://localhost:8890/webapp/#\n\n### Note\n\nYou can set the port of the trace app by -Dphoenix.traceserver.http.port={portNo}\n\neg: `-Dphoenix.traceserver.http.port=8890 server will start in 8890`\n\n![screenshot from 2015-07-20 23 14 13](https://cloud.githubusercontent.com/assets/1260234/8782934/552a7cb2-2f36-11e5-8616-e6f70e0dd160.png)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/122967254/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/125292795", "body": "Building trace web app from root.\n![screenshot from 2015-07-27 23 38 15](https://cloud.githubusercontent.com/assets/1260234/8913865/4a1b0ca0-34b9-11e5-9d64-db9e6faf5d35.png)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/125292795/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/125697098", "body": "You can start the web app from the command given below.\n`java -jar target/phoenix-tracing-webapp-4.4.1-HBase-0.98-SNAPSHOT-runnable.jar` \nYou can change the web app port from \n`-Dphoenix.traceserver.http.port=8887`\n\nA view similar to screenshot will be seen from the Phoenix tracelist.\n`http://localhost:8864/webapp/#/list`\n\nEnable tracing before you attempt above steps.\n\n![screenshot from 2015-07-28 23 04 20](https://cloud.githubusercontent.com/assets/1260234/8939252/f0948f8a-3580-11e5-9e42-6906304d19d0.png)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/125697098/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/125707558", "body": "### Starting Trace WebApp\n\n`python traceserver.py start`\nTrace web app will start in [http://localhost:8864/webapp/](http://localhost:8864/webapp/)\n\n### Stop Trace-WebApp\n\n`python traceserver.py stop`\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/125707558/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/127286516", "body": "Test Specs are written in jasmine and jasmine-maven-plugin is used to integrate test to build \n\n![screenshot from 2015-08-03 21 20 51](https://cloud.githubusercontent.com/assets/1260234/9041505/f29c9742-3a25-11e5-872d-321bf6417c69.png)\n\nTimeline works with Phoenix actual dataset.\n\n![screenshot from 2015-08-03 12 05 07](https://cloud.githubusercontent.com/assets/1260234/9041568/44560ee2-3a26-11e5-9644-ca6b5cf07feb.png)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/127286516/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128098489", "body": "Phoenix binding for Trace Count Chart \n![screenshot from 2015-08-05 23 42 32](https://cloud.githubusercontent.com/assets/1260234/9093943/c8643f2c-3bcc-11e5-9aba-caf3f97ac920.png)\n\nTrace Distribution\n![screenshot from 2015-08-05 23 40 29](https://cloud.githubusercontent.com/assets/1260234/9093942/c6c2850c-3bcc-11e5-9e80-485f6b6f91c8.png)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128098489/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/129188195", "body": "Attaching dependency tree view\n![screenshot from 2015-08-09 18 39 35](https://cloud.githubusercontent.com/assets/1260234/9155058/fc1985b2-3ec8-11e5-87ad-6aa4ac9f3d3d.png)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/129188195/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/129547016", "body": "![screenshot from 2015-08-10 19 37 43](https://cloud.githubusercontent.com/assets/1260234/9178646/daae8d8a-3fb5-11e5-89d0-8e3559f7ee50.png)\nTrace count supported with different chart types - pie,bar,column,area,line\n\n![screenshot from 2015-08-10 19 37 56](https://cloud.githubusercontent.com/assets/1260234/9178726/08028106-3fb6-11e5-9698-8851d671ba7e.png)\nItems are listed with button linked to dependency chart.\n\n![screenshot from 2015-08-10 19 38 13](https://cloud.githubusercontent.com/assets/1260234/9178749/2b2a0d7a-3fb6-11e5-902f-d5fb86436733.png)\nDependency chart with tooltip\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/129547016/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/131591980", "body": "Dependency tree with text and tooltip\n\n![screenshot_from_2015-08-16_161355](https://cloud.githubusercontent.com/assets/1260234/9294520/277049ae-446d-11e5-8b27-5cf8092ee269.png)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/131591980/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/131679366", "body": "Hi, James\n\nI will try to added description of the root span for state notification box.\n\nThanks\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/131679366/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/131859265", "body": "@JamesRTaylor : Here is new look of dependency tree with SQL query \n\n![screenshot from 2015-08-17 20 44 37](https://cloud.githubusercontent.com/assets/1260234/9308402/943f1efc-4521-11e5-8723-1266d1ee699b.png)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/131859265/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/133053181", "body": "Build Apache Phoenix from root\nmvn clean install\n\nStart the backend services\n`./bin/traceserver.py start`\n\nStop the backend services\n`./bin/traceserver.py stop`\n\n![screenshot from 2015-08-20 20 13 55](https://cloud.githubusercontent.com/assets/1260234/9387559/c5783b5c-477e-11e5-917a-82b980babf6d.png)\n\nThe services can be viewed by accessing the following location\n\nhttp://localhost:8864/trace?action=get\n![screenshot from 2015-08-20 20 13 59](https://cloud.githubusercontent.com/assets/1260234/9387582/ec41f80e-477e-11e5-848b-ca1e05b7cd86.png)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/133053181/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/133054413", "body": "Build phoenix-tracing-webapp by\n`mvn package`\nIt will run jasmine JS Test Specs with testing JS files in Tracing Web app\n\n![screenshot from 2015-08-20 21 05 31](https://cloud.githubusercontent.com/assets/1260234/9387657/3bf6401c-477f-11e5-9d8e-78af949f6723.png)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/133054413/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/138435673", "body": "@ndimiduk   & @jtaylor-sfdc  @JamesRTaylor can either of you please take a look at this PR?\n\nThanks.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/138435673/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "prakul": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/197", "title": "PHOENIX-3201 Implement DAYOFWEEK and DAYOFYEAR built-in functions", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/190", "title": "PHOENIX-3036 : Modify phoenix IT tests to extend BaseHBaseManagedTimeTableReuseIT", "body": "\u2026\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xiaopeng-liao": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/196", "title": "[PHOENIX-2648] Add dynamic column support for spark integration", "body": "It supports both RDD and Dataframe read /write, \n# Things needed consideration\n\nWhen loading from Dataframe, there is a need to convert from catalyst data type to Phoenix type, ex. \nStringType to Varchar, Array<Integer> to INTEGER_ARRAY,. etc. The code is under phoenix-spark/src/main/scala/org.apache.phoenix.spark.DataFrameFunctions.scala\n# Usages\n- **RDD**\n\n**Save**\n\n```\nval dataSet = List((1L, \"1\", 1, 1), (2L, \"2\", 2, 2), (3L, \"3\", 3, 3))\nsc\n  .parallelize(dataSet)\n  .saveToPhoenix(\n    \"OUTPUT_TEST_TABLE\",\n    Seq(\"ID\", \"COL1\", \"COL2\", \"COL4<INTEGER\"),\n    hbaseConfiguration\n)\n```\n\n**Read**\n\n```\n    val columnNames = Seq(\"ID\", \"COL1\", \"COL2\", \"COL5<INTEGER\")\n    // Load the results back\n    val loaded = sc.phoenixTableAsRDD(\n      \"OUTPUT_TEST_TABLE\",columnNames,\n      conf = hbaseConfiguration\n    )\n```\n- **Dataframe**\n\n**Save**\nIt will get data types from Dataframe and convert to Phoenix supported types\n\n```\nval dataSet = List((1L, \"1\", 1, 1,\"2\"), (2L, \"2\", 2, 2,\"3\"), (3L, \"3\", 3, 3,\"4\"))\nsc\n  .parallelize(dataSet).toDF(\"ID\",\"COL1\",\"COL2\",\"COL6\",\"COL7\")\n  .saveToPhoenix(\"OUTPUT_TEST_TABLE\",zkUrl = Some(quorumAddress))\n```\n\n**Read**\n\n```\nval df1 = sqlContext.phoenixTableAsDataFrame(\"OUTPUT_TEST_TABLE\", Array(\"ID\", \n    \"COL1\",\"COL6<INTEGER\", \"COL7<VARCHAR\"), conf = hbaseConfiguration)\n```\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "btbytes": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/186", "title": "cleanup sqlline*.py using python idioms.", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "RCheungIT": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/184", "title": "PHOENIX-2405 ", "body": "https://issues.apache.org/jira/browse/PHOENIX-2405\n\nHi @maryannxue, I guess this time it may be closer to what you described. \nI think the threshold in DeferredResultIterator should be different from the threshold in DeferredByteBufferSegmentQueue, but I don't know where to get it.\nAlso, I don't find a good way to get rid of the offset in Iterator.\nWould you mind giving me any suggestions?\n\nThanks\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/175", "title": "PHOENIX-2405 Not for merge, just request a review to check whether I'm on the right way", "body": "https://issues.apache.org/jira/browse/PHOENIX-2405\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Boruch28": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/177", "title": "Support DISTINCT for SUM", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/164", "title": "Handling the situation when hashCode == Integer.MIN_VALUE", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/163", "title": "Removed unused code. Fix Boolean.equals(String).", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/162", "title": "Fix possible NPE.", "body": "Add verify for null in override method equals.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/160", "title": " Change concat Strings in foreach to StringBuilder.", "body": " Change != to equals for Strings.\n Remove unnecessary == true.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "KTW81": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/176", "title": "4.7.0-HBase", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "opsun": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/174", "title": "PHOENIX-2952 fix array_length return nagetive value", "body": "https://issues.apache.org/jira/browse/PHOENIX-2952\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Pranavan135": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/173", "title": "Milli time changed to nano time - Do not merge", "body": "This PR needs some HTrace Level changes. I made those changes. We can merge the PR when HTrace is updated with my PR. \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "co2y": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/169", "title": "Update BUILDING", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chrajeshbabu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/168", "title": "PHOENIX-1734 Local index improvements(Rajeshbabu)", "body": "This is the patch for new implementation of local index where we store local index data in the separate column families in the same table than different table. \n\nThis patch needs https://issues.apache.org/jira/browse/HBASE-15600 to be committed to HBase to write the local index updates directly to the region and make it transactional with data table updates. Phoenix uses MiniBatchOperationInProgress#addOperationsFromCP API introduced at HBASE-15600.\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/156", "title": "PHOENIX-2628 Ensure split when iterating through results handled corr\u2026", "body": "The patch fixes issues with splits and merges while scanning local indexes.  \n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/135", "title": "PHOENIX-1734 Local index improvements", "body": "Patch supports storing local indexing data in the same data table.\n1) Removed code used HBase internals in balancer, split and merge.\n2) Create index create column families suffix with L#  for data column families.\n3) Changes in read and write path to use column families prefixed with L# for local indexes.\n4) Done changes to tests.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48798867", "body": "Thanks for review @JamesRTaylor. I have resolved the conflicts  and handled all the comments locally. I will submit it once I verify OrderedResultIterator scenario.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48798867/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48839233", "body": "bq. Cleanest might be to just implement a simple ParallelIteratorRegionSplitter for use when a local index is used that just returns all regions:\nI will add new ParallelIteratorRegionSplitter for local index and remove the unnecessary changes in SkipRangeParallelIteratorRegionSplitter/DefaultParallelIteratorRegionSplitter. \n\nThen I will submit another pull request. \n\nThanks @JamesRTaylor \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48839233/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48945719", "body": "bq. Just add the check to disable creating local indexes on a table with immutable rows and then let's check this in. \nChanged pull request to disallow local index on immutable rows and added some test cases.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48945719/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48949296", "body": "Resolved the conflicts after PHOENIX-1002 also. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48949296/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/96145601", "body": "Thanks Samarth for reviews. Will update patch addressing the comments.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/96145601/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/96276745", "body": "Thanks @JamesRTaylor  @samarthjain \nI have addressed the review comments and added to pull request. If it's ok I will commit this tomorrow morning IST and work on subtasks.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/96276745/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/96969719", "body": "It's committed. Hence closing.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/96969719/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/160454376", "body": "Thanks for review @JamesRTaylor. Please find my answers to the comments.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/160454376/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834683", "body": "Somehow missed this. Corrected.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834867", "body": "corrected.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834867/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834896", "body": "Yes James. This change already there in master branch\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834896/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834905", "body": "Added the comment.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834905/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834954", "body": "removed.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834954/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14835021", "body": "comment added.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14835021/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14835033", "body": "done.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14835033/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14835161", "body": "Thanks for pointing this James. Yes merge sort is fine. Done the changes locally.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14835161/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850777", "body": "Moved the changes outside of scanOrdered/scanUnordered and passing through necessary info through calls.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850777/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850781", "body": "done.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850781/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850789", "body": "This I will verify James.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850789/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850792", "body": "done.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850792/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850794", "body": "Already done in master.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850794/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850795", "body": "done.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850795/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14853783", "body": "bq. There's a bit more you need to do to handle ORDER BY correctly. It'd be for the case in which a data column was referenced in the ORDER BY while the index table is being used to satisfy the query.\nThis is working fine James. I have added test case. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14853783/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14853806", "body": "This is the actual code. Earlier by mistake I missed to add LOCAL in if branch so if else branches are same. \n            if (localIndex) {\n                ddl = \"CREATE LOCAL INDEX \" + INDEX_TABLE_NAME + \" ON \" + DATA_TABLE_FULL_NAME + \" (date_col)\";\n            } else {\n                ddl = \"CREATE INDEX \" + INDEX_TABLE_NAME + \" ON \" + DATA_TABLE_FULL_NAME + \" (date_col)\";\n            }\n\nNow I have corrected it.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14853806/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14858124", "body": "Even with the change skip scan will be used James. The change is required because the key ranges generated by compiler won't be in the local index regions key range because local index rows have prefixed region start key extra. Without the change mostly no region will be selected for scanning.\n\nQueryIT#testSimpleInListStatement is the test case verifies the same. \nHere is the explain query result.\n\nCLIENT PARALLEL 4-WAY SKIP SCAN ON 2 KEYS OVER _LOCAL_IDX_ATABLE [-32768,2] - [-32768,4]\n    SERVER FILTER BY FIRST KEY ONLY AND ORGANIZATION_ID = '00D300000000XHP'\nCLIENT MERGE SORT\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14858124/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14897250", "body": "Done.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14897250/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14897277", "body": "Removed.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14897277/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14897862", "body": "This change is not required in the master branch James. In local-index branch join queries have failed with ColumnNotFoundException with local indexes when join query has columns other than index. So I have added the check. \nCurrently join back from data table is not supported in join queries. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14897862/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14897879", "body": "done.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14897879/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14898287", "body": "bq. why do we need this to be different for local indexes? Seems like we could run parallel scans over part of each region just like we do with other scans, no?\nIndex table rows have region start key as prefix so even if we split region key ranges into multiple and scan parallelly,only first scanner gives the results and remaining all the scanners just return nothing. So number of splits of local index region setting to 1.\nif we want to start multiple scanners for local index into multiple parts then we need to split the scan ranges into multiple.  \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14898287/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14898383", "body": "bq. getSplitsPerRegion method to ParallelIteratorRegionSplitter so we can move this special case to your new implementation for local indexes.\nCurrent patch added getSplitsPerRegion method to ParallelIteratorRegionSplitter and returning one in LocalIndexParallelIteratorRegionSplitter  and removed the changes in other splitter.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14898383/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14898689", "body": "one main difference is there between the two methods James. in addSaltByte if keyrange upper or lower bound is unbound we don't set salting byte. in case of local index if both start or end keys are unbound then only we don't prefix the bytes. So not able to commonize both.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14898689/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ankurjain86": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/161", "title": "PHOENIX-2834 Support Phoenix over CDH 5.7.0", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gkanade": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/157", "title": "merging latest avatica", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "aaronst": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/152", "title": "PHOENIX-2757 Phoenix Can't Coerce String to Boolean", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zeeshanasghar": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/149", "title": "Utility classes should not have public constructors", "body": "This pull request is focused on resolving occurrences of rule: \nsquid:S1118 - Utility classes should not have public constructors.\n\nYou can find more information about the issue here:\nhttp://dev.eclipse.org/sonar/rules/show/squid:S1118\n\nPlease let me know if you have any questions.\n\nZeeshan\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "freakyzoidberg": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/146", "title": "PHOENIX-2364 Fix Reverse timestamp row key", "body": "When inverting the nanosecond part of a Timestamp key the offset should be reset.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ndimiduk": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/145", "title": "PHOENIX-2517 Bulk load tools should support multiple input files", "body": "", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/142", "title": "PHOENIX-2492 Expose PhoenixRecordWritable outside of spark", "body": "This is a simple translation of phoenix-spark's `PhoenixRecordWritable` to java, and has been helpful for my needs. Probably the next step is to rip out the scala version and replumb to use the single implementation everywhere.\n", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/141", "title": "PHOENIX-2481 JSON bulkload tool", "body": "", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/29", "title": "PHOENIX-1514 Break up PDataType Enum", "body": "", "author_association": "MEMBER"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/118404084", "body": "Yeah, this looks much better.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/118404084/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/132789228", "body": "I haven't done a phoenix release, but this looks like what I had in mind. Thanks for fixing the missing headers while you're in there -- these should be caught by the apache:rat plugin; i'm not sure why they're not.\n\n@JamesRTaylor and @chrajeshbabu have done phoenix releases recently. Does this look good by you guys?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/132789228/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/161410084", "body": "Add a tool for loading json documents (one record per line) and refactor code in common between JSON and CSV loading.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/161410084/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/162033375", "body": "> For JSON, is only single level supported? If not, how is nested JSON handled?\n\nNo special handling is provided. A nested json object will be read as Map and presented to the upsert statement as such. Since Phoenix doesn't support maps, an error would result.\n\n> Looks like lists are converted into ARRAYs which is nice. What happens if the array elements have different data types? I guess you'd just log an error and ignore that row?\n\nIndeed. Or fail the job, if `--ignore-errors` is not specified.\n\n> Can you make sure the indenting conforms to the 4 space convention?\n\nPardon me, @JamesRTaylor. Done.\n\n> Also, please make sure imports don't use \\* and that their order is correct\n\nDone.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/162033375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/162108877", "body": "Okay. Went through all javadocs. Also rebased onto master.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/162108877/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/162270732", "body": "Sounds like we have a game-plan to proceed. I'm happy to pursue the work of consolidation, though I'm concerned about touching (and breaking) so many means of interaction. We'll see how testing goes.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/162270732/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/162668915", "body": "Please help yourself @jmahonin. I've been able to move forward with my own project via the class in this PR. I should be getting into some spark stuff later this week, which is the only reason this might come up again on my end.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/162668915/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "navis": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/125", "title": "PHOENIX-2349 SortOrderExpressionTest.toChar() is failing in different locale", "body": "For example in ko.kr,\n\n```\nTests run: 24, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.025 sec <<< FAILURE! - in org.apache.phoenix.expression.SortOrderExpressionTest\ntoChar(org.apache.phoenix.expression.SortOrderExpressionTest)  Time elapsed: 0.007 sec  <<< FAILURE!\norg.junit.ComparisonFailure: expected:<12/11/01 12:00 [AM]> but was:<12/11/01 12:00 [\uc624\uc804]>\n    at org.junit.Assert.assertEquals(Assert.java:115)\n    at org.apache.phoenix.expression.SortOrderExpressionTest.evaluateAndAssertResult(SortOrderExpressionTest.java:322)\n    at org.apache.phoenix.expression.SortOrderExpressionTest.evaluateAndAssertResult(SortOrderExpressionTest.java:312)\n    at org.apache.phoenix.expression.SortOrderExpressionTest.toChar(SortOrderExpressionTest.java:149)\n```\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/124", "title": "PHOENIX-2288 Phoenix-Spark: PDecimal precision and scale aren't carried through to Spark DataFrame", "body": "from jira description\n\n> When loading a Spark dataframe from a Phoenix table with a 'DECIMAL' type, the underlying precision and scale aren't carried forward to Spark.\n> \n> The Spark catalyst schema converter should load these from the underlying column. These appear to be exposed in the ResultSetMetaData, but if there was a way to expose these somehow through ColumnInfo, it would be cleaner.\n> \n> I'm not sure if Pig has the same issues or not, but I suspect it may.\n\nIt seemed enough just for current usage in spark-interagation. But in long term, PDataType should contain meta information like maxLength or precision, etc.\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/149186165", "body": "Made two version of patches (a69735b and a20e123). \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/149186165/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "jstanier": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/123", "title": "Update performance.py", "body": "Use .py extension as this is a python script.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "blacklp": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/122", "title": "Add missing test annotation + remove unnecessary assignment", "body": "Without the Test annotation the test was not being executed.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chiastic-security": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/120", "title": "PHOENIX-2257 Fix broken integration test for query over derived GROUP\u2026", "body": "\u2026 BY table (testDerivedTableWithGroupBy()), which worked on Java 7 but not on Java 8.\n\nThe test was retrieving two rows from a query with no ORDER BY clause, and assuming that they would come back in a specific order. This happened to match the order that Java 7 returned them in, but the order has changed in Java 8.\n\nTest fixed by not assuming any particular order.\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/145763579", "body": "Thanks. Did this get pushed? The PR is still coming up as open in github.\n\nMaybe accepting it doesn't automatically close it? That would be weird, though.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/145763579/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "bpanneton": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/119", "title": "Fix for performance.py not using JAVA_HOME", "body": "", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/145112736", "body": "Yea, that would be better. I'll fix it up.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/145112736/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/145114015", "body": "Patch should now follow how queryserver.py gets the JAVA_HOME dir\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/145114015/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/145143153", "body": "Fixed the java that I missed.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/145143153/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/146227005", "body": "Updated to have sqlline, sqlline-thing and psql in there.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/146227005/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "d9liang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/117", "title": "Phoenix 572 Truncate CHAR with max length on upsert", "body": "This transaction truncate string to the CHAR max length value on upsert. A test case is added and passed. Please review.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mujtabachohan": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/115", "title": "PHOENIX-2182", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "randerzander": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/114", "title": "PHOENIX-2196 - auto capitalize field names in DF->Phoenix table save method", "body": "", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/133893731", "body": "As an example, reading a CSV file where headers aren't capitalized, the below is necessary before calling df.save:\n\nvar df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"DROPMALFORMED\", \"true\").load(input)\nval columns = df.columns.map(x => x.toUpperCase)\ndf = df.toDF(columns:_*)\n\nIt would be nice for end users if the .save implementation handled this detail\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/133893731/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/133904250", "body": "If I understand correctly, this change should work-\n\nAny fieldnames with starting and ending quotes are untouched. If they aren't quoted, then they're auto-capitalized.\n\nDoes this work better?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/133904250/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "julianhyde": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/113", "title": "PHOENIX-1706 Create skeleton for parsing DDL", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jfernandosf": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/105", "title": "PHOENIX-1791. Adding ability for Pherf Scenarios to write to a multi-tenant view.", "body": "Small change to Pherf to enable Pherf to allow Scenario declarations to specify multi-tenant views for the tableName attribute and then load data into the tenant specific view.  This change is  to add the tenantId attribute to the scenario XML. If a view is specified in the tableName and a tenantId attribute is specified Pherf will use a tenant specific connection to write to the view.\n\nIn the code we pass this tenantId when we get a connection. If it is not specified null is returned and therefore Pherf gets a standard connection when working with that Scenario. If it is specified then Pherf gets a multi-tenant connection for that scenario and then a multi-tenant view can be specified as value for the tableName attribute for the scenario.\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/4", "title": "PHOENIX-1096 Fix for concurreny bug caused by a cached SequenceExpression using the same byte[] buffer in multiple threads", "body": "This is a fix for the Upsert...Select issue we ran into when using Sequences in the SELECT portion of the statement. We were generating mutations with identical row keys because multiple rows were being assigned the same sequence number. I tracked this down to the fact that we cache SequenceExpression and have a member field byte[] where we store the sequence after reading it. This buffer was being written to by multiple threads and so there was no guarantee a thread would see the sequence it read and based on timing multiple threads would read the same sequence when generating the row key for the mutation. \n\nMoving the buffer to a local variable addresses this issue.\n\nI have also included a test that Samarth and I created to repro the issue that is now passing.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15088076", "body": "In order to reliably repro this issue we needed a higher level of concurrency that the defaults and there we need to override the default thread pool size to 64 so we can have 64 salt buckets.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15088076/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ictwanglei": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/102", "title": "PHOENIX-1661 Implement built-in functions for JSON", "body": "Add some  built-in functions for JSON : JsonArrayElementsFunction.java JsonArrayLengthFunction.java JsonEachFunction.java JsonObjectKeysFunction.java JsonPopulateRecordFunction.java JsonPopulateRecordSetFunction.java. ArrayToJsonFunction.java\n\nAdd end2end test files.\n\nModify file PhoenixJson.java to which I added some helper methods.\n\n@JamesRTaylor @twdsilva @AakashPradeep\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/117210029", "body": "@AakashPradeep  sorry for the missing of comments and other coding mistakes and thanks for your patient guidance. I have modified the code according to your suggestion.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/117210029/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "petercdc": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/101", "title": "PHOENIX-2144 Implement JSON operators", "body": "1.Adding following JSON operators:\n&nbsp;&nbsp;JSON get data Operator : \"->\" \"->>\" \"#>\" \"#>>\"\n&nbsp;&nbsp;JSON Boolean Operactor : \"?\" \"?|\" \"?&\" \"<@\" \"@>\"\n\n2.add a unit test file: JSONOperationT.java\n\n3.Adding functions for some JSON operators in PhoenixJson.java \n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/125942611", "body": "Hello, @twdsilva .Below link is our JIRA account\nhttps://issues.apache.org/jira/secure/ViewProfile.jspa?name=petercdc\nThank you.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/125942611/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "siddhimehta": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/98", "title": "PHOENIX-2098 - Udf that given a number bulk reserves sequences.", "body": "", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/120243946", "body": "@JamesRTaylor  Pull request for PHOENIX-2098\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/120243946/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "lalinsky": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/94", "title": "PHOENIX-2081 Fix BIGINT range on the Datatypes docs page", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/phoenix/comments/12842382", "body": "This removed phoenix-server\\* jar files from the package. Was that intentional?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/12842382/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "codymarcel": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/89", "title": "Mixed R/W workload support", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/41", "title": "working additions of pherf to phoenix as a project module", "body": "\"mvn clean package\" builds everything correctly.\nAll tests pass.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/78597830", "body": "I think I have addressed any outstanding concerns. Let me know if there is anything else.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/78597830/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/112984238", "body": "@mujtabachohan @twdsilva \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/112984238/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/112984696", "body": "@samarthjain\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/112984696/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "naveenmadhire": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/86", "title": "PHOENIX-1984", "body": "Changes to INSTR string function to have 1 based return type.\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/80", "title": "PHOENIX-1883 Repeat Function", "body": "Repeat Function Implementation. Initial Changes.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/82285730", "body": "Adding some more changes related to Test.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/82285730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/82606330", "body": "Thanks Gabriel. I will work on adding some more test cases to this and changes mentioned.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/82606330/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/82716115", "body": "@gabrielreid I added all the changes which you mentioned. Also I created testcases for all types of combinations, including multibytes and empty strings. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/82716115/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/83573791", "body": "I am having some configuration issues when running a end-2-end test cases. I will figure out today and run that.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/83573791/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/84472511", "body": "@gabrielreid I wrote the required test cases in InstrFunctionIT.java. All ASC test cases are working fine. But for DESC I am getting inverted value as output. For expected value 5 the select statement is giving -6. I confirmed that INSTR function is returning the correct value both for DESC and ASC however the output of select is returning different values for DESC. Is there anything else which I need to handle for DESC scenarios. \nInterestingly, all the testcases of InstrFunctionTest are working fine.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/84472511/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/87907541", "body": "@gabrielreid I didn't realize we have pre defined method already available in stringUtil. I've modified the code accordingly and all the test cases are working fine.\n\nDo you see any other test cases need to be added? \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/87907541/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88311044", "body": "@gabrielreid I tried to do this, but messed it up and created a new Pull request 61. I am closing this request.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88311044/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88311224", "body": "@gabrielreid This is the new pull request which I had to create for the 1712 changes as I messed it while doing a rebase of the pull request.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88311224/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/96502348", "body": "Implemented in Master. Hence Closing.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/96502348/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "prashantkommireddi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/85", "title": "PHOENIX-1981 : PhoenixHBase Load and Store Funcs should handle all Pig data types", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/84", "title": "PHOENIX-1965 : Upgrade Pig to version 0.13", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "AakashPradeep": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/81", "title": "PHOENIX-1938 Like operator should throw proper exception when it is used with data type other then Varcahr and Char", "body": "LIKE operator should be supported only for Varchar and Char and it should throw SQLException with SQLExceptionCode.TYPE_NOT_SUPPORTED_FOR_OPERATOR for any other data type. \n\nPostgres specification : http://www.postgresql.org/docs/8.3/static/functions-matching.html\nOracle specification: http://docs.oracle.com/cd/B12037_01/server.101/b10759/conditions016.htm\n\n@JamesRTaylor  @twdsilva  Can you please review this.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/76", "title": "PHOENIX-628 Support native JSON data type", "body": "This pull request has following changes:\n1. Renames\n    a)  PJsonDataType to PJson\n    b)  PJsonDataTypeTest to PJsonTest \n    c) PhoenixJsonE2ETest to PhoenixJsonIT\n2. Added new SqlException Code INVALID_JSON_DATA\n3. Some changes in PhoenixJson\n4. Removed overridden method from PJson for which default implementation is fine.\n       a) coerceBytes()\n       b) isCoercibleTo().\n\n@JamesRTaylor and @twdsilva  please review the changelist.  \n\nNote : Some of the classes have space formatting problem, I am trying to fix my eclipse for that. Hopefully it will be resolved in next pull request. \n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/95399238", "body": "@JamesRTaylor  Can you please review Phoenix Json.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/95399238/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/96815390", "body": "@twdsilva  @JamesRTaylor  I have made changes as per the comments. Please let me know if there is anything more.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/96815390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/101011634", "body": "@JamesRTaylor and @twdsilva  can you please review it. It is required for Json. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/101011634/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/104047363", "body": "As per the SQL specification, LIKE only works with String data type (http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt search for \"Like predicate\") . \nIf it has been set to PVarchar.isCoercible() that means anything which can be coerced to Varchar but is not varchar for example VarBinary and JSON, can also be used with \"Like\". \n\nIMHO Like should only work with Varhcar and char datatype and throw illegal data type error with other datatype arguments.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/104047363/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/104074791", "body": "Agreed, but then for all data type like JSON which is coercible to Varchar or Char but do not want to support \"LIKE\" will have to add an if statement.\n\nDo you think thats ok ?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/104074791/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dumindux": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/79", "title": "PHOENIX-1875 ARRAY_PREPEND function implemented ", "body": "PR for the ARRAY_PREPEND function with tests.\nHere an issue with the ARRAY_APPEND function is also fixed(I have found it while doing array prepend. sorry about that. I haven't added ptr offset while obtaining array element offsets with getOffset())\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nmaillard": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/74", "title": "Phoenix 331- Phoenix-Hive initial commit ", "body": "", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/100843474", "body": "Have made the changes per the first feedback, let me know if there is anything else\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/100843474/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/102079613", "body": "thanks everyone, sorry I have had little connectivity the last couple of days. I will adress new feedback right away\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/102079613/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "cmarcel": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/72", "title": "Phoenix 1728", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cbrcrash": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/71", "title": "[PHOENIX-1682] removed normalization of table name from PhoenixRuntime.getTable()", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "7shurik": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/70", "title": "JIRA Phoenix 1872", "body": "I'm fixing https://issues.apache.org/jira/browse/PHOENIX-1872. The fix is to use PrimitiveBooleanArray of boolean[] instead of PhoenixArray of Boolean[]. \n\nI considered using BitSet and not sure we'd get the benefit vs. using boolean[]. Also the code is much easier to maintain.\n\nI've ran and fixed unit tests and appropriate integration tests as well.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/68", "title": "Phoenix 1759", "body": "This is a fix for https://issues.apache.org/jira/browse/PHOENIX-1759. There's a set of unit tests as well as an integration test.\n\nMain code change is adding a PhoenixArray.toString() implementation and toString on each primitive array type. There're also a couple of fixes for toStringLiteral not to barf on nulls.\n\nPlease let me know if you see any problems.\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/93865916", "body": "@samarthjain @JamesRTaylor are you still reviewing? I've uploaded the version with suggested changes, including a single implementation of PhoenixArray.toString().\nSerhiy Bilousov on the JIRA said he might review over the weekend as well.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/93865916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/96093375", "body": "I'm fixing the Apache license header in PrimitiveIntPhoenixArrayToStringTest.java.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/96093375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "rangent": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/67", "title": "[PHOENIX-1814] Exponential notation parsing and tests", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "StormAll": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/62", "title": "PHOENIX-1798 change Collections.empytList() to Lists.newArrayList()", "body": "This change related to PHOENIX-1798.\nIf the select SQL query a lot of data, the issue will occur.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "michfr": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/60", "title": "Additional date query tests.", "body": "Update previous pull request to have the new tests use a class derived from BaseHBaseManagedTimeIT.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/57", "title": "Update2", "body": "for merge only\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/75690857", "body": "Here are some additional date tests.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/75690857/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dhacker1341": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/52", "title": "PHOENIX-1744 unsigned long cast to timestamp", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/33", "title": "PHOENIX-1469 4.2 patch fix binary columns for indexing", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/32", "title": "PHOENIX-1469 ", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/31", "title": "PHOENIX-1362 fix min/max function on fixed length data types", "body": "Sorry for the second pull request - still figuring out git.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "karel1980": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/51", "title": "PHOENIX-1755 Improve error logging if csv line has insufficient fields", "body": "Fix for PHOENIX-1755\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/83621999", "body": "Wrong ticket number! Should be PHOENIX-1755, will create new PR\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/83621999/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/83625295", "body": "Note to self:\nhttp://41.media.tumblr.com/17776053d6c9891d256eff90819dde0e/tumblr_mtheifr5pk1s52saoo1_500.png\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/83625295/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "shuxiong": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/46", "title": "Draft for PHOENIX-1287 subtask, ByteBasedLikeExpression", "body": "Hi @JamesRTaylor ,\n\nThis is a draft for\n1. Abstraction for Regex Engine, byte-based one(JONI) and string-based one(j.u.regex)\n2. ByteBasedLikeExpression and StringBasedLikeExpression based on 1.\n\nComment if you think there is anything to be improved.\nThen I will work on other expressions(RegexpReplaceFunction, RegexpSplitFunction, RegexpSubstrFunction) next.\n\nThanks.\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/78433212", "body": "Hi @JamesRTaylor ,\nI update the code, adding two more types, PWholeNumber and PRealNumber. and rename the function from getSign to signum.\n\nThanks.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/78433212/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/78868573", "body": "Hi @JamesRTaylor  , I already revert the changes and combine all commits into a single one.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/78868573/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/83941154", "body": "All tests for LikeExpression are passed.\n\nI update the code and have a draft for RegexpReplaceFunction. But I can't find any tests or end-to-end test for ReplaceFunction. I will write some testcases later. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/83941154/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "tzolkincz": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/43", "title": "PHOENIX-1722 Speedup CONVERT_TZ function", "body": "Using Joda Time lib instead of java.util.TimeZone. This would speedup this function more than 3 times. I've also updated version of Joda Time to 2.7 cause of some timezone bugfixes and speedups.\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/42", "title": "PHOENIX-1722 Speedup CONVERT_TZ function", "body": "Using Joda Time lib instead of java.util.TimeZone. This would speedup this function more than 3 times. I've also updated version of Joda Time to 2.7 cause of some timezone bugfixes and speedups.\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/27", "title": "PHOENIX-1343 Array insert unique function", "body": "Hi,\nI've created ARRAY_INSERT_UNIQUE function for add elements to array which are unique. Code is executed at region coprocessor hence there is no network roundtrip and consistency. In fact consistency is enforced by row lock.\nExample query: \n\n```\nUPSERT INTO ARRAY_UPSERT_UNIQUE_TABLE (k1, arr) VALUES (1, ARRAY_INSERT_UNIQUE(ARRAY[1, 2, 5]))\n```\n\nFunction accepts any array data type, but can't modify array at the end of row key.\nI've implemented some logic for run coprocessors for data modificatioin. I'd be glad to hear some feedback.\n\nHow it works:\nThere is UpsertFunctionExpression expression that defines interface for other server side upsert functions. Any of child function implements getAttributes method. These attributes (plus info about cf and qualifier, sort order and data type) are attached to put as attributes.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/59688390", "body": "integrated\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/59688390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/59688393", "body": "integrated\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/59688393/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/59688396", "body": "integrated\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/59688396/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/84464732", "body": "@gabrielreid I made changes as you suggested.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/84464732/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/85680197", "body": "Thank you for review @gabrielreid, i've solved that.\nI realized [TIMEZONE_OFFSET](https://github.com/apache/phoenix/blob/master/phoenix-core/src/main/java/org/apache/phoenix/expression/function/TimezoneOffsetFunction.java#L48) have similar issue. I think we should rather extract timezone cache mechanism and make it as part of org.apache.phoenix.cache. Or are there some other use-cases of (instance) cache so we could implement it more generally?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/85680197/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/85911379", "body": "Yes, it should be thread safe but we could consider to use same lib for timezones and same caching mechanism.  So if you are for duplication of [caching](https://github.com/apache/phoenix/pull/42/files#diff-83862191fb126a769d5fd74be3534d90R98) code, I'll do that. It is small fraction of code, but I'd rather make it more abstract. However it's not major question here.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/85911379/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/86740884", "body": "Thanks for notification.\nI'm done with my changes. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/86740884/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/86913995", "body": "I'm on it, it looks strange.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/86913995/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/86985696", "body": "yes my bad. Fixed\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/86985696/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "ayingshu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/39", "title": "Phoenix-1580 union all impl", "body": "Implementation using wrapped TupleProjectionPlan. My own implementation of association RowProjector with ResultIterator was commented out in code. \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "elilevine": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/37", "title": "Surface partial saves in CommitExcepiton (PHOENIX-900)", "body": "https://issues.apache.org/jira/browse/PHOENIX-900\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/21", "title": "Added custom annotations to logs in query-related functionality in client- and server-side code for PHOENIX-1198", "body": "@JamesRTaylor, take a look when you get a chance. Shouldn't be anything controversial. Thanks!\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55315763", "body": "@jyates Can you take a look? Thanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55315763/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55981263", "body": "I like it. Good call.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55981263/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/56245922", "body": "Merged into master and 4.0 branches.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/56245922/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/56899160", "body": "Merged now.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/56899160/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/76333924", "body": "Pushed to main and 4.0 branches. Closing this PR.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/76333924/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128459657", "body": "Thanks for the ping, @JamesRTaylor. Haven't seen this PR until now. I'll review and commit today or tomorrow if everything looks good.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128459657/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128830531", "body": "Good stuff, @JeffreyLyonsD2L. Added a few questions and comments to the PR.  \n\nThere is one more issue that it not tackled here that we need to at least discuss IMHO. Phoenix's metadata tables, such as SYSTEM.CATALOG lead with TENANT_ID VARCHAR column (take a look in QueryConstants.java). This works well in a world where all multi-tenant tables can only have VARCHAR tenant id columns. Your changes introduce the possibility that different multi-tenant tables will have different types of tenant id columns.\n\nWhat's the right solution here? The correct thing is probably changing TENANT_ID column type to BINARY in system tables, as well as adding a TENANT_ID_TYPE column to allow tenant ids to be converted correctly. But that will make upgrades for existing clusters a pain. Leave them as VARCHAR?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128830531/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128840703", "body": "The TENANT_ID column in system tables is used to quickly filter metadata belonging to a tenant. Agree we could probably get away with leaving it VARCHAR because that is how tenant ids are passed to connections anyway.\n\nNow that got me thinking... if we extending that logic further, why then do we need to implement support for other data types for tenant id columns if tenant ids are always passed in as strings to connections? Why not keep them all as VARCHAR/CHAR, like they are now?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128840703/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128866480", "body": "Yeah, I understand that the feature allows TENANT_ID on data tables to be a different type. What I'm not sure about is how useful this feature would be in real life if internally we are treating tenant ids as strings in connections and system tables anyway. Why not just do the same for data tables, as well (which is how it is now implemented)? As you mentioned, every type can be converted to a string and users can do easily do that for data tables. \n\nIf we relax type constrains for data tables but not for metadata tables/connections we are making the implementation of multi-tenancy in phoenix internally inconsistent. Should be all or nothing IMHO.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128866480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/129580603", "body": "Everything looks good in terms of code. Excellent work, @JeffreyLyonsD2L. The only thing to figure out now is whether or not we should add the feature at all. I agree that changing the schema of SYSTEM.\\* tables is off the table. So do we allow the tenant id column in data tables to be of different type than in metadata tables? It does not sit well with me and makes multi-tenancy seem inconsistent IMHO, as I mentioned above, since tenant id is always passed in as string to connections and is stored as such in system tables.\n\n@JeffreyLyonsD2L, maybe you can shine some light on our use-case. You mentioned that not having to do data conversion is a plus. In what way? Are you starting with existing data and don't want to do data conversion?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/129580603/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/129690118", "body": "Thanks for the details, @JeffreyLyonsD2L. I have to say I'm still torn on whether or not this Jira makes much sense. What if I told you that your concern #2 is already addressed? Currently tenant id columns can be either VARCHAR or fixed-length CHAR. Does that make it easier for you to use multi-tenancy in Phoenix?\n\nRegarding your points #1 and 3, you are forced to do the conversion once per Connection when you set the TenantId parameter either way. Not sure what kind of conversion you mean beyond that. Normally when you read/write via a tenant-specific connection you only set TenantId on the Connection and don't need to actually write it or read it. Are you working with this data over non-tenant-specific connections?\n\n---\n\nI went though your code again. Everything looks good. The only thing missing now is:\n1. Tests that create tenant views with non-string tenant id columns and test data upsertion/retrieval.\n2. Same as above but negative. Test what happens when conversion fails.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/129690118/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/129690323", "body": "Closing this PR. This has been checked in by @twdsilva. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/129690323/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/130002784", "body": "Makes sense, @JeffreyLyonsD2L. The existing data and integrations with other components interacting with the HBase tables do make the feature worth it IMHO. I'll commit after you add those tests. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/130002784/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/130400117", "body": "Good stuff, @JeffreyLyonsD2L! One last thing: can you update this pull request so it merges with Phoenix master cleanly (there are some conflicts now)? Also, would be great to see a PR for one of the 4.x branches if this patch does not merge cleanly there. You don't need to do it for each 4.x branch, one is fine. I'll take care of the rest. Again, thank you for your contributions.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/130400117/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/130783527", "body": "This PR is now in master and can be closed.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/130783527/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/130848164", "body": "Pushed this to 4.x-HBase-0.98 but the patch does not apply cleanly to 4.x-HBase-1.0. @JeffreyLyonsD2L, mind doing a final PR for the 4.x-HBase-1.0 branch? Hopefully the last hurdle. :-)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/130848164/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/131231511", "body": "This is now in and can be closed.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/131231511/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/131231573", "body": "Thanks for the new PR, @JeffreyLyonsD2L. It's now in and this PR can be closed.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/131231573/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17579564", "body": "Good call. Will add.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17579564/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17644266", "body": "Contracts are worthless if they are not enforceable. ;o)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17644266/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17644279", "body": "don't follow\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17644279/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17644317", "body": "Generics gets screwed up if emptyMap() is used in the parameter. I'll see what I can do to make this cleaner.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17644317/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17644459", "body": "Tracing-only changes are in PR #13. This PR was supposed to be only for logging-related code but I screwed it up because I pulled in my tracing changes used in #13 because for logging I am using the same way to pass annotations in. Tried to make that clear in this PR's description but clearly failed. Sorry about that. Maybe take a look at #13 that has tracing-only changes?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17644459/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/18074893", "body": "New attribute for Scans to pass custom annotations to coprocessors.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/18074893/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/18074909", "body": "Removed unused variable.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/18074909/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/18074918", "body": "Added a bunch of toString() methods for objects that might be logged.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/18074918/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "jacobtardieu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/35", "title": "PHOENIX-1601 Fix performance script for binary distribution of phoenix", "body": "Change the PHOENIX_TEST_JAR_PATTERN since it was also matching flume tests jars.\n\nThe performance.py script wasn't working anymore in the bin distribution of phoenix\nbecause the flume test jar was matching the test jar pattern for phoenix core test jar.\nThis new pattern fixes this issue.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mravi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/23", "title": "PHOENIX-MR", "body": "Hi Gabriel,\n   This is a new pull request for MR support in Phoenix. Can you please review it.\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/22", "title": "Drop for Phoenix-MR.", "body": "TODO:\n   A sample MR job.\n  Junit tests for the MR support code.\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/19", "title": "Phoenix-Pig issue patches for master", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/57540992", "body": "Thanks Gabriel, Will do the necessary changes and do another drop of code soon. Also, a sample example for MR is at https://gist.github.com/mravi/501fa00b942764eb0dca . \n  Keeping it for documentation purpose.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/57540992/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88157968", "body": "Nice work @jmahonin .  Couple of minor changes\n1.  JDK version in the pom.xml is 1.8. You would need to downgrade to 1.7\n2.  For the ProductRDDFunctions.scala file, I notice a mismatch in the package declaration.\n3. Would be ideal if we could have a scala file say PhoenixSparkContext that merges the functionality you have written in ProductRDDFunctions and  SparkContextFunctions .\n4. Renaming SparkSqlContextFunctions to just PhoenixSparkSqlContext to make easier for end users.\n5. The build goes through fine but when I try to run the PhoenixRDDTest from a ScalaIDE , I keep getting errors. It could be more of a IDE thing which i will fix and get back on tests results. \n\nGood to haves\n1. A Java friendly version of the PhoenixSparkContext and PhoenixSparkSqlContext classes for a easier adoption for java folks ( like me :) )\n2. Extend the org.apache.spark.sql.sources.RelationProvider  and have PhoenixDatasource. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88157968/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88178852", "body": "@jmahonin  I can join forces with you on the Good to haves \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88178852/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88928590", "body": "@jmahonin  Let's have the changes for registering as a data source in a separate JIRA. The changes to this commit LGTM.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88928590/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/89733786", "body": "@jmahonin Thanks for the nice work !! I have pushed these changes to 4.X and master branch. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/89733786/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/90094412", "body": "Thanks @jmahonin  for the quick turnaround on this.\n I will review this today and get back.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/90094412/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/91166341", "body": "Good work @jmahonin . I have merged the files onto master and 4.x-HBase-0.98 branches. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/91166341/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/92292084", "body": "@jmahonin \n   One minor change.  Can you please replace the following line in PhoenixRDD.scala and ProductRDDFunctions. This will ensure we load hbase-site.xml so all configuration parameters set for HBase / Phoenix can be applied.\n\n---\n\nval config = new Configuration(conf)  . \nto\nval config = HBaseConfiguration.create(conf)  \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/92292084/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/92298667", "body": "@jmahonin \n      Would it be good to have ProductRDDFunctions renamed to  PhoenixRDDFunctions? I believe you had a good reason to name it as that in the first instance. Thoughts?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/92298667/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/92636784", "body": "Thanks @jmahonin  for making the necessary changes .  Below are the final set of tasks that need to be done before we publish documentation.\n\na) In phoenix/pom.xml , Add the dependency of phoenix-spark module under <dependencyManagement>.  \nhttps://github.com/apache/phoenix/blob/master/pom.xml#L427\nb) In phoenix/phoenix-assembly/pom.xml , Add the dependency of phoenix-spark module .    https://github.com/apache/phoenix/blob/master/phoenix-assembly/pom.xml#L148\nc) In phoenix/phoenx-assembly/build/all-common-jars.xml , register spark artifacts.\n   https://github.com/apache/phoenix/blob/master/phoenix-assembly/src/build/components/all-common-jars.xml#L73\nd) Generating HTML  from the README . \n\nIs it possible for you to create patches for both the master the 4.4.X HBase 0.98 branch and attach to the JIRA? I can take care of the d) task and push to SVN . \n\nThanks!!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/92636784/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/95799980", "body": "Thanks a lot for the patch @nmaillard . Couple of additional minor fixes \n1) Couple of classes have your name as the author. Please remove it.\n2) Certain classes have lot of commented code . \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/95799980/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/162263899", "body": "+1 for a single implementation to be used across all modules. Off late, I had to create a new one for Sqoop support  :( . \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/162263899/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "jyates": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/20", "title": "Removing hadoop-compat modules + reflection", "body": "There was some reflection and wrapping done in the metrics/tracing tools\nto support working with Hadoop1/2 (though hadoop1 support was never completed).\nRemoving this extra code now that we don't want to support hadoop1 anymore\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/5", "title": "Implement HTrace based tracing", "body": "Small issue in that not everyone serializes htrace annotations, but that's an\nopen question of the right way to do that anyways\n\nAdding tracing to:\n- MutationState\n- query plan tracing\n- iterators\n\nMetrics writing is generalized to support eventual hadoop1 implementation.\nAlso, supporting test-skipping with a custom Hadoop1 test runner + annotation\n\nDefault builds to hadoop2, rather than hadoop1 (particularly as hadoop1 is\nnow a second-class citizen).\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55315928", "body": "Not until after we sort out this prod bug at SFDC. Maybe next week?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55315928/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55827917", "body": "This would be a lot nicer if you didn't mix the logging changes with the tracing changes - the changelist would be much more succinct and to the point. Otherwise, seems like the right approach, though it could use some cleanup (docs, readability, etc)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55827917/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/83699353", "body": "At first blush, it seems like we need to a way to age off index tables or the set of tables could grow out of control, especially if people are creating and deleting a lot of index tables with only minimal writes. On the other hand, even if we had 1 megabyte of table names, that would be likely ~ 50,000 table names (20bytes each). So not really a big problem with cleanup, but James' point above:\n\n>  corner case of an index being dropped and subsequently a table being created with the same name\n\nIs still a concern.\n\nOtherwise, lgtm\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/83699353/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/106560323", "body": "Couple of minor comments. otherwise, lgtm\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/106560323/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15407718", "body": "Considered that, but the number of tags and annotations are variable, so you can't just build a single statement - you would have to build a separate one for the tags/annotations each time, so you might as well just do them all at once\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15407718/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15407764", "body": "yes, it does, via the TraceReader\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15407764/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15407823", "body": "Its just a test class, no too worried about that kind of stuff. Not assumed to be thread safe, though it probably is.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/15407823/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17636321", "body": "could use Collections.emptyMap() here\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17636321/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17636396", "body": "This seems a strange way to handle it, when used in practice. Its not entirely clear that all the log elements should be annotated. In fact, it would be nice, at least in this initial pass, to separate the logging from the tracing in terms of the per-connection annotations.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17636396/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17636409", "body": "nit: unneccessary change\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17636409/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17636458", "body": "nit: docs\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17636458/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17636489", "body": "i think if you mark something as not null, you don't need to do the null check - kind of the deal with programming by contract, ya?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17636489/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17636519", "body": "customAnnotations seems the wrong name. Maybe something like \"addConnectionAnnotations\"?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17636519/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17636539", "body": "nit: flip the logic and save the indent for readabililty\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17636539/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "SeonghwanMoon": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/16", "title": "Phoenix-1216 fix code", "body": "Phoenix-1216 fix code\nChange spooling directory\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ramkrish86": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/12", "title": "Phoenix-180", "body": "A fresh pull request with all the changes for guideposts. \nTested in a single cluster for different combinations. \n\"SELECT count(*) FROM  %s WHERE host='CS' AND domain='Apple.com' AND FEATURE >= 'Dashboard' AND FEATURE <= 'Report';\" % (table)) \nWith 5000000 rows\n# Withoutpatch\n## COUNT(1)\n\n```\n555169\n```\n\nTime: 2.264 sec(s)\n# Withpatch\n##   COUNT(1)\n\n```\n555169\n```\n\nTime: 0.813 sec(s)\n# Withoutpatch\n##   COUNT(1)\n\n   1002201\nTime: 1.889 sec(s)\n# Withpatch\n##   COUNT(1)\n\n   1002201\nTime: 1.075 sec(s)\n\nAm sure we would get better results if we are able to tune the guidePosts width so that instead of fetching in the range of 1000s inside a region we can fetch 10ks of result in every chunk.  Also should work with bigger tables and more rows.\nThe tests were performed with 256*1024 and 1024 \\* 1024 as the guidepostsWidth size. \nThe main thing you can observe in the above case is that the query targetted to single region is parallelized and distributed to smaller chunks and so the work done by every thread is less. \n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/8", "title": "Phoenix 180", "body": "First revision for review.  There are some open points here.  Still no test case added to consume the stats.  Will do the performance test once the system is available with me.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48926040", "body": "Just committing it JAmes.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48926040/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48927901", "body": "I think I don't have write access to do this merge. \n     Only those with write access to this repository can merge pull requests. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48927901/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48991061", "body": "bq. just add a .patch to the url for the pull request and you'll have the patch file to apply against the normal/updatable repo\nThat's nice.  I tried it, it works. :)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48991061/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/53409242", "body": "```\nOne thing I didn't see is how you're handling multiple column families. Is that still left to be done?\n```\n\nNo not yet in this patch.  I initially created a patch where there was some sort of grouping that happened per family. Later dropped it.\n\n```\n Think a bit on when you think is the best time to \"merge\" the guideposts. Maybe when you read the stats table in MetaDataEndPointImpl you can execute this logic and then the PTableStats that get passed into PTable are already the \"merged\" ones?\n```\n\nMerge in the sense- after the guideposts are collected per family? You mean this \"merge\"\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/53409242/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54014739", "body": "@JamesRTaylor - Thanks for the review.  I will do my level best to complete all the comments.  Some of them have now become design changes because now we will be collecting based on CF rather than region name. So the guide posts will be a map with key as CF and byte[](representing the guideposts). It was a long weekend here and so could not take this up.  Will post an updated patch ASAP may be in a day or two.  After that will do performance testing. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54014739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54037058", "body": "DefaultParallelIteratorsRegionSplitterIT - how about these test cases.  We need the new behaviour or we need to update the test cases to get the required result?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54037058/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54406335", "body": "```\njava.lang.IllegalArgumentException: KeyValue size too large\n    at org.apache.hadoop.hbase.client.HTable.validatePut(HTable.java:1312)\n    at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:941)\n    at org.apache.hadoop.hbase.client.HTable.put(HTable.java:908)\n    at org.apache.hadoop.hbase.coprocessor.CoprocessorHost$Environment$HTableWrapper.put(CoprocessorHost.java:444)\n    at org.apache.phoenix.schema.stat.StatisticsTable.updateStats(StatisticsTable.java:126)\n    at org.apache.phoenix.schema.stat.StatisticsScanner.close(StatisticsScanner.java:90)\n    at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:87)\n    at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)\n    at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)\n    at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1480)\n    at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:744)\n```\n\nWhat could be ideal value for those guidePost collection - Like after how many bytes could we collect the guideposts?  And for the performance.py what could be the best value based on its distribution?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54406335/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54417314", "body": "Why is the pull request referring to the old branch ?\n\n```\ncommit b4c0cb1df9fc43d76c43de052f5f8007b51b4909\n```\n\nAuthor: Ramkrishna ramkrishna.s.vasudevan@intel.com\nDate:   Thu Sep 4 12:14:13 2014 +0530\n\n```\nPhoenix-180\n```\n\ncommit 3abb90bb0fa0721a333f919f4d0c734cf51028fc\nAuthor: Rajeshbabu Chintaguntla rajeshbabu.chintaguntla@huawei.com\nDate:   Wed Sep 3 18:16:49 2014 +0800\n\n```\nPHOENIX-1139 Failed to disable local index when index update fails(addendum)\n```\n\ncommit 845888bcb58a6fe5975de4cfabbe9266407de9dc\nAuthor: Rajeshbabu Chintaguntla rajeshbabu@apache.org\n\nThis is what gets displayed in my local log history.  And my branch is Phoenix-180_1\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54417314/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54418748", "body": "Pls use this https://github.com/apache/phoenix/pull/11\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54418748/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54434740", "body": "```\nHTablePool pool = new HTablePool (env.getConfiguration(),1);\n    try {\n        HTableInterface hTable = pool.getTable(PhoenixDatabaseMetaData.SYSTEM_CATALOG_NAME);\n        try {\n            ResultScanner scanner = hTable.getScanner(scan);\n            try {\n                Result result = scanner.next();\n                return result != null;\n            }\n            finally {\n                scanner.close();\n            }\n        } finally {\n            hTable.close();\n        }\n    } finally {\n        pool.close();\n    }\n```\n\nAll the testcases are hanging in this place when we do htable.getscanner(). This code is in dropTable when we check for hasViews().\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54434740/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54507302", "body": "Do I need to update to 0.98.6 then? It definitely hangs for every test case.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54507302/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54511440", "body": "Ok. now it works after changing it to region.scanner() and further corresponding changes. Thanks James.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54511440/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54811713", "body": "Pls check the latest commit. It has all the updated code changes. One thing to note is that when run in a cluster the update query in the StatisticsTable fails while committing the upsert query saying that table not found.(SYSTEM.STATS) but that does not happen in test cases.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54811713/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54813424", "body": "BTW where can we see the phoenix logs or how to enable them?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54813424/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55113987", "body": "Moved over to https://github.com/apache/phoenix/pull/12. Hence closing.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55113987/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55114041", "body": "Closing this as the pull moved over to https://github.com/apache/phoenix/pull/12.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55114041/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55217482", "body": "I think one change would be required in the split hooks.  Let me see it once again and confirm it.  If needed will make the changes and push it here.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55217482/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55257152", "body": "Pls point me to testcases that has no CF, one CF and multiple CFs.  Also where there are queries across multiple regions.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55257152/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55550080", "body": "```\n    protected List<KeyRange> genKeyRanges(List<HRegionLocation> regions) {\n    if (regions.isEmpty()) { return Collections.emptyList(); }\n    Scan scan = context.getScan();\n    PTable table = this.tableRef.getTable();\n    byte[] defaultCF = SchemaUtil.getEmptyColumnFamily(table);\n    List<byte[]> gps = null;\n    try {\n        if (table.getColumnFamilies().isEmpty()) {\n            // For sure we can get the defaultCF from the table\n            gps = table.getTableStats().getGuidePosts().get(defaultCF);\n        } else {\n            if (scan.getFamilyMap().size() > 0) {\n                if (scan.getFamilyMap().containsKey(defaultCF)) { // Favor using default CF if it's used in scan\n                    gps = table.getColumnFamily(defaultCF).getGuidePosts();\n                } else { // Otherwise, just use first CF in use by scan\n                    gps = table.getColumnFamily(scan.getFamilyMap().keySet().iterator().next()).getGuidePosts();\n                }\n            } else {\n                gps = table.getColumnFamily(defaultCF).getGuidePosts();\n            }\n        }\n    } catch (Exception cfne) {\n        logger.error(\"Error while getting guideposts for the cf \" + Bytes.toString(defaultCF));\n    }\n\n    List<KeyRange> regionStartEndKey = Lists.newArrayListWithExpectedSize(regions.size());\n    for (HRegionLocation region : regions) {\n        regionStartEndKey.add(KeyRange.getKeyRange(region.getRegionInfo().getStartKey(), region.getRegionInfo()\n                .getEndKey()));\n    }\n    List<KeyRange> guidePosts = Lists.newArrayListWithCapacity(regions.size());\n    List<byte[]> guidePostsBytes = Lists.newArrayListWithCapacity(regions.size());\n    // Only one cf to be used here\n    if (gps != null) {\n        // the guide posts will arrive in sorted order here as we are focusing on only one cf\n        for (byte[] guidePost : gps) {\n            PhoenixArray array = (PhoenixArray)PDataType.VARBINARY_ARRAY.toObject(guidePost);\n            if (array != null && array.getDimensions() != 0) {\n                for (int j = 0; j < array.getDimensions(); j++) {\n                    guidePostsBytes.add(array.toBytes(j));\n                }\n            }\n        }\n    }\n    int size = guidePostsBytes.size();\n    if (size > 0) {\n        if (size > 1) {\n            guidePosts.add(KeyRange.getKeyRange(HConstants.EMPTY_BYTE_ARRAY, guidePostsBytes.get(0)));\n            for (int i = 0; i < size - 2; i++) {\n                guidePosts.add(KeyRange.getKeyRange(guidePostsBytes.get(i), (guidePostsBytes.get(i + 1))));\n            }\n            guidePosts.add(KeyRange.getKeyRange(guidePostsBytes.get(size - 2), (guidePostsBytes.get(size - 1))));\n            guidePosts.add(KeyRange.getKeyRange(guidePostsBytes.get(size - 1), (HConstants.EMPTY_BYTE_ARRAY)));\n        } else {\n            byte[] gp = guidePostsBytes.get(0);\n            guidePosts.add(KeyRange.getKeyRange(HConstants.EMPTY_BYTE_ARRAY, gp));\n            guidePosts.add(KeyRange.getKeyRange(gp, HConstants.EMPTY_BYTE_ARRAY));\n        }\n\n    }\n    if (guidePosts.size() > 0) {\n        List<KeyRange> intersect = KeyRange.intersect(guidePosts, regionStartEndKey);\n        return intersect;\n    } else {\n        return regionStartEndKey;\n    }\n}\n```\n\nDo you think this will work out?  Here trying to form keyranges from all the guideposts and intersecting with the region boundaries?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55550080/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55633717", "body": "All things are fine. But after a split not sure what is happening.  The thread that is in postSplit just hangs.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55633717/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55643833", "body": "```\nprotected List<KeyRange> genKeyRanges(List<HRegionLocation> regions) {\n    if (regions.isEmpty()) { return Collections.emptyList(); }\n    Scan scan = context.getScan();\n    PTable table = this.tableRef.getTable();\n    byte[] defaultCF = SchemaUtil.getEmptyColumnFamily(table);\n    List<byte[]> gps = null;\n    try {\n        if (table.getColumnFamilies().isEmpty()) {\n            // For sure we can get the defaultCF from the table\n            gps = table.getTableStats().getGuidePosts().get(defaultCF);\n        } else {\n            if (scan.getFamilyMap().size() > 0) {\n                if (scan.getFamilyMap().containsKey(defaultCF)) { // Favor using default CF if it's used in scan\n                    gps = table.getColumnFamily(defaultCF).getGuidePosts();\n                } else { // Otherwise, just use first CF in use by scan\n                    gps = table.getColumnFamily(scan.getFamilyMap().keySet().iterator().next()).getGuidePosts();\n                }\n            } else {\n                gps = table.getColumnFamily(defaultCF).getGuidePosts();\n            }\n        }\n    } catch (Exception cfne) {\n        logger.error(\"Error while getting guideposts for the cf \" + Bytes.toString(defaultCF));\n    }\n    List<KeyRange> guidePosts = Lists.newArrayListWithCapacity(regions.size());\n    List<KeyRange> regionStartEndKey = Lists.newArrayListWithExpectedSize(regions.size());\n    for (HRegionLocation region : regions) {\n        regionStartEndKey.add(KeyRange.getKeyRange(region.getRegionInfo().getStartKey(), region\n                .getRegionInfo().getEndKey()));\n    }\n    if (gps != null) {\n        byte[] startKey = regions.get(0).getRegionInfo().getStartKey();\n        int regionSize = regions.size();\n        int regionIndex = 0;\n        int guideIndex = 0;\n        int gpsSize = gps.size();\n        while ((regionIndex <= regionSize - 1) && (guideIndex <= gpsSize - 1)) {\n            byte[] guidePost = gps.get(guideIndex);\n            PhoenixArray array = (PhoenixArray)PDataType.VARBINARY_ARRAY.toObject(guidePost);\n            byte[] regionEndKey = regions.get(regionIndex).getRegionInfo().getEndKey();\n            if (array != null && array.getDimensions() != 0) {\n                boolean intersects = false ;\n                for (int j = 0; j < array.getDimensions(); j++) {\n                    byte[] currentGuidePost = array.toBytes(j);\n                    if (Bytes.compareTo(currentGuidePost, regionEndKey) <= 0) {\n                        KeyRange keyRange = KeyRange.getKeyRange(startKey, currentGuidePost);\n                        // Contains check may be too coslty\n                        if(keyRange != KeyRange.EMPTY_RANGE) {\n                            guidePosts.add(keyRange);\n                        }\n                        startKey = currentGuidePost;\n                        if (!intersects) {\n                            guideIndex++;\n                            intersects = true;\n                        }\n                    }\n                }\n            }\n            guidePosts.add(KeyRange.getKeyRange(startKey, regionEndKey));\n            regionIndex++;\n            if (regionIndex <= regionSize - 1) {\n                startKey = regions.get(regionIndex).getRegionInfo().getStartKey();\n            }\n        }\n        if (guidePosts.size() > 0) {\n            List<KeyRange> intersect = KeyRange.intersect(guidePosts, regionStartEndKey);\n            return intersect;\n        } else {\n            return regionStartEndKey;\n        }\n    } else {\n        return regionStartEndKey;\n    }\n```\n\nPls see the last intersect. I will do this for now and ensure that we are able to get the splits correctly.  Without that as split code does not work correctly - because from the split hook not able to do any updations on the split table.  So we could better target in another JIRA.  \nThe last intersect would ensure that though we get new guideposts based on the new regions and also the the other guide posts also matches with other region's end key we get overlapping Keyranges. So finally to resolve that we could intersect it with the region's start and end key.  @JamesRTaylor  What do you think?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55643833/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55737370", "body": "Updated with all the comments.  The ColumnNotFoundException is needed because in case of MultiCF and there is an alter table query then the table does not have default CF and the scan also has the default CF in the scan map.  So it is always better to catch that exception to just let through.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55737370/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55789826", "body": "One thing that I saw in the TenantSpecificTablesDMLIT though we collect the guide posts for the default family, when the query actually happens we get some CF 0.USER. So we are not able to use any of the guideposts.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/55789826/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16698592", "body": "Ok\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16698592/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16698610", "body": "Ok.. I added for doing some testing from my side if the entries are added to the stats table. Will remove them.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16698610/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16698623", "body": "May be not. will try removing it. Test cases side I did some alterations just to see if things are working fine.  I will refactor them.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16698623/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16698634", "body": "Ok.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16698634/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16698733", "body": "Ok.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16698733/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16698962", "body": "```\nThen an updateStats would just need to invalidate the cache entry for the table. Next time the table is accessed, the stats    table would be re-read.\nHow about if we just read the stats anytime we re-load the PTable\n```\n\nSo you mean when ever getTable() is called collect the udpateStats in a seperate thread.  If the stats was really changed then we would be returning the new stats that was updated (after invalidation). Or you mean that when ever we reload the table and populate the cache we do a scan of the STATS table and get the updated stats info in the main thread only.  The invalidation is done only when the schema changes or index is removed/added which also means there is no need for periodic scanning of stats table. Am i right here? \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16698962/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699028", "body": "I think all this periodic checking of stats table itself wont't be needed is what you mean? So my doubt is this suppose I have some select queries coming periodically to a table and there are updations also happening to that table, if we are able to collect the stats periodically, for the newer select queries we would be able to use the udpated information right? Only if we collect the stats during re-load it would mean that if there is no schema change or index addition/deletion the updated stats info may not be used. For me I think periodic updation of the stats would be a better choice.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699028/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699103", "body": "ok\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699103/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699116", "body": "Ok.. will use that QueryServicesTestOptions\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699116/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699137", "body": "Ok\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699137/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699156", "body": "I will add a TODO and file a JIRA for now.  Once the chore part is done I will take up that JIRA.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699156/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699208", "body": "Ok\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699208/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699329", "body": "StatisticsScanner is something we may need if we want to collect the stats as part of major compaction.\nStatisticsWriter may be can be simplified and merged. similarly with StatisticsValue, can be made an inner class (POJO like class)\nStatisticsTracker i think it would be needed so that we could extend that and impl that in the observers.\nStatisticsUtils are just util methods to add some utitilites to extract the table name, region name from the STAT table's row key.\nStatisticsTable is a wrapper on the STATS HTable.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699329/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699399", "body": "I think it may be possible. Let me see.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699399/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699454", "body": "Ya I too felt it was expensive.  But is that ok if we just depend on the size of the collected stats list?  If you see here once there is a change in size of the stats we just consider that something has changed.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16699454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16704394", "body": "In one of the other comments in the JIRA, it was decided to do based on timer task. Hence did that way. Ok so for the cache the maxTTL will be the frequency at which the stat updation should happen, which is 15 mins. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16704394/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16704699", "body": "As there is no need for checking if there is any change in the stats we need not do any comparison of the stats too.  This would mean that it would be easy to make PTableStats immutable too. And also the stats table can be read only for that table only - not in case of a the updator thread that would scan the whole table.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16704699/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16705274", "body": "You mean the empty line or the \n    @Override\n     public UpdateStatisticsStatement updateStatistics(NamedTableNode table) {\n          return new ExecutableUpdateStatisticsStatement(table, 0);\n        }\nThis is just an implementation in the ParseNodeFactory\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16705274/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16708853", "body": "```\nInstead we'll add all guideposts for the regions here\n```\n\nAll the stats that we collect should be region based right? Or you mean collect the guideposts for the table and then use that information? But I doubt when we do the scan in the endpoint we would anyway collecting it per region.  The reason is if we try collecting it per region I fear if we would need to set the per region stats on the PTableStats object? Not making it immutable? \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16708853/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16726689", "body": "Ok.. So guidePosts can be for the table but the minkey and maxkey is per region right? Will work based on the feedback.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16726689/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16728896", "body": "```\nActually, I don't think we'll even need minKey/maxKey at this point. Just the guideposts\n```\n\nOkie.  I will keep a map for the min key and max key at least in the impl level (incase we need to use it later )and a list for the guide posts then.  So the API will return a map for the min key and max key and list for the guide posts.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16728896/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16730042", "body": "Ok. My thinking was as we are not collecting any other stats in the stats table other than min, max keys and the guide posts it made sense to design PTableStats to support these basics. So for now will make it immutable with guideposts only\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16730042/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16764530", "body": "Done this. Introduce a new method in MEtaDataService.proto that invalidates the cache for a specific table key.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16764530/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16765083", "body": "Done\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16765083/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16765176", "body": "Ok\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16765176/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16765207", "body": "Done\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16765207/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16765332", "body": "Done\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16765332/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16765696", "body": "Changed this.  Let me check if it is woking fine. If so then no problem. Can remove the TODO.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16765696/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16766530", "body": "Removed StatisticsWriter and StatisticsValue( later we may need this like a simple POJO) when we need some more stats to be added.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16766530/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16767859", "body": "```\nWe have the concept of a \"minimum time to recalc stats\" as a separate config and store the time when the stats were last calculated in the stats table.\n```\n\nSo storing this in the stats table means per table again? So we will read the stats table and seeing the timestamp we will not further read the stats table and will become a NOOP ? Am i right?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16767859/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16795912", "body": "So inside the CP we actually do a scan of the table's region for which update stats is issued. So before that create an HTable instance to talk to the system.stats table and read the column where the last analyzed time is stored for that table and then proceed with the scan of the region? I am bit skeptical here of talking with the stats table inside the endpoint call for the table's region.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16795912/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16796576", "body": "ACtually in the current schema I have added CF in the primary key followed by the stats cols (includes min and max key and the guide posts.) I can collect the information per CF.  When we try collecting the guideposts it would be any way per region and inside that we could group per CF and write the same to the stats table.  The stats table wil have table name not null, region name, cf name followed by min, max and guide posts?\nBy using VARBINARY ARRAY the entire set of guide posts collected for that region can be combined as one entry. \nSo if we are adding guide posts per CF then the PTableStats also will accept a map with key as CF and then the guide posts? Sorry for the asking more questions just making myself clear.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16796576/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16830455", "body": "Ok..This can be done. The only thing is multiple select queries would reach the stats table.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16830455/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16943000", "body": "Ok.. So in that case the stats wil be associated with the table directly. For now I will first finish the case so that the PTable has a PColumnFamily and the stats (guidePosts) are part of this PcolumnFamily.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16943000/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16961974", "body": "Though we invalidate the meta data cache and again do update cache so that we get the latest stats updated to the PTableImpl or PColumnFamily but how to clear it from the TAbleRef? Still it holds the older reference.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16961974/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16971170", "body": "That's right. It is the best way to do it. One more thing is inorder to retrive the latest time when the stats got updated so that based on that we don issue concurrent update stats query, should we do that in MetaDataClient with a select query or should be a normal get from the HTable? As a measure of good practice am asking this.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16971170/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16972273", "body": "Even after updating the cell. I ended up in the same problem :(. I think may be because of the test case that is clientmanaged. (DefaultParallelIteratorsRegionSplitterIT)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16972273/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16975421", "body": "So this put that we make should have a timestamp (the current ts) added with it or leave it to the server to add it.\n\n```\n// Query for the latest table first, since it's not cached\n        table = buildTable(key, cacheKey, region, HConstants.LATEST_TIMESTAMP);\n        if (table != null && table.getTimeStamp() < clientTimeStamp) {\n            return table;\n        }\n        // Otherwise, query for an older version of the table - it won't be cached\n        return buildTable(key, cacheKey, region, clientTimeStamp);\n```\n\nSuppose we add a cell (empty cell) so that we can udpdate the cache, still the above code will not allow to do that because before the first 'if' we try to use HConstants.LATEST_TIMESTAMP and inside buildTable() we were were updating the metaDataCache for the table with the timestamp of the cell that we created. (fine till here) but the if conditon would fail because table.getTimeStamp = the timestamp of the emtpy kv and the clienttimestamp = 130 (for eg). So it goes into the second build table. There the passed timeSTamp = 130. So we create a PTable instance but with its timestamp as 130 and we don't udpate the metadatacache. As this table (with ts = 130) is returned out \n\n```\n            if (table.getTimeStamp() != tableTimeStamp) {\n            builder.setTable(PTableImpl.toProto(table));\n        }\n```\n\nbefore building the PTable from the protos we check for the existing time stamp and the table's timestamp both of the matches and we end up in no created the table from the proto.  This internally means that for the FromCompiler the table ref is not updated with the latest. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16975421/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16975454", "body": "Am i missing something here? This happens for even HBaseManaged test cases. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16975454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16977664", "body": "```\n            if (table != null && table.getTimeStamp() < clientTimeStamp) {\n            return table;\n        }\n```\n\nShould the above condition be != in MetaDataEndpointImpl in doGetTable after we build the table that is from 0 to LATEST_TIMESTAMP?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/16977664/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17045111", "body": "```\n this will be a bit easier if the guideposts are sorted already\n```\n\nIdeally the guideposts should be sorted because we are inserting the guideposts with the following rowkey - tablename, fam and regioname - so considering there is only one fam those sets of guideposts would be sorted i feel. \nAnother is, if the scan has to retrieve the row that is part of the guidepost itself then the splits that we form from the guideposts, will actually needs to be incremented by +1 (last byte) so that the upperrange of the key range could be used as the stop row and will cover the exact row that is as part of the guide posts.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17045111/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17063004", "body": "@JamesRTaylor - So for a single guide post it will be from empty byte array to the guide post and from that guidepost to the last empty byte array.  Now when I intersect this with the available regions then it becomes different and thus not able to use that split. I will check once again and get back on that.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17063004/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17067622", "body": "MAy be the inclusive and exclusive thing was the reason.  Now it works.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17067622/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17067688", "body": "Oh you have suggested the same James :). Refreshing the git page.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17067688/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17411122", "body": "As I am removing the statsManager this clear will move out of the try block and hence the alignment changes.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17411122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17707949", "body": "Actually this class no longer exists.  All the assertions are moved to the DefaultParallelIteratorsIt and other test cases.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17707949/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17716029", "body": "Got an NPE here. The guideposts and min max key maps are shared. So they should be synchronized.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17716029/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17716267", "body": "I mean suppose an ANALYZE table is happening and parallely a major compaction happens we are not restricting this scenario then there is a chance that the guide post maps that is per region would get cleared/updated could be the problem.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17716267/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17772645", "body": "Am not sure why this keeps coming.  I don have this in my IDE also. How to remove it?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17772645/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17772753", "body": "This is already there in the above code where we form PColumnFamily estimatedSize += family.getEstimatedSize();\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17772753/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "cancylin": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/9", "title": "PHOENIX-1225", "body": "Evaluation of OR between columns in PK and not in PK incorrect. \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17034179", "body": "I download the DBTools from http://www.dbtools.com.br/EN/queryit/.\nTest the query in postgresql.\n![psqloutput](https://cloud.githubusercontent.com/assets/8636229/4130494/715d628c-3338-11e4-99c1-e2a077820415.png)\n\nAfter modify the WhereOptiMizer.java.\nPhoenix outputs the same result.\n             ID       NAME    ADDRESS       SBIN      MONEY     NUMBER INDATE         PASSWD \n\n---\n\n```\n     1    Tester1    HSINCHU                   0.0        1.1 2012-12-23 10:00:00 6655447788 \n     2    Tester2                              0.0        2.1 2012-10-25 14:00:00   11223344 \n     3    Tester3                              0.0        2.0 2012-12-25 10:09:53 6654321897 \n     6 LikeTesterEnd                              0.0        7.1 2012-12-25 11:00:00 9876543210 \n     8    Tester3     Taipei                   0.0        1.0 2011-12-25 10:00:00       5678 \n    25    Tester1   HSINCHUU                   0.0        1.1 2012-12-23 10:00:00 6655447788 \n    26  Tester100 HSINCHUUXX                 12.03      100.1 2010-10-10 10:10:10       pass \n```\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17034179/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "jeffreyz88": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/6", "title": "Phoenix-1112: Atomically rebuild index partially when index update fails", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sunshihua": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/61742017", "body": "Please ignore this PR. It has been posted to the public repo unintentional.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/61742017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "robdaemon": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88267669", "body": "So happy to see this go in. I'll push on my former colleagues at Simply Measured about the copyrights!!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88267669/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dacort": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88267935", "body": "Consider me poked! Will take a look at this and follow up tomorrow by the latest.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88267935/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88298339", "body": "@jmahonin Thanks for this PR!\n\nYou're good to go with respect to copyrights. Thanks for putting those in the appropriate place. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/88298339/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "abisek": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/94068262", "body": "@gabrielreid - good catch! I updated the code to handle the shell quoting in a platform-specific manner. \n\nI verified the changes on Linux. I'm going to try it on Windows by end of the day today.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/94068262/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/94106504", "body": "Verified on Windows as well:\n\n```\nC:\\Phoenix\\phoenix\\bin>python sqlline.py asreeniva-wsl2.internal.salesforce.com:2181:/hbase;TenantId=t1\nSetting property: [isolation, TRANSACTION_READ_COMMITTED]\nissuing: !connect jdbc:phoenix:asreeniva-wsl2.internal.salesforce.com:2181:/hbase;TenantId=t1 none none org.apache.phoenix.jdbc.PhoenixDriver\nConnecting to jdbc:phoenix:asreeniva-wsl2.internal.salesforce.com:2181:/hbase;TenantId=t1\n...\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/94106504/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "SagarSharma": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/96727939", "body": "sorry sir for late reply, busy due to exams . No sir i didn't read that . Thank you for the link .\nI'll try to make it correct ASAP.......\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/96727939/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "twinklestart": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/98599371", "body": "ok,I have tried  this two ways to load data ,it works well,thanks,I will close this pull request\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/98599371/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ddraj": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/101949365", "body": "Ping @JamesRTaylor  @mravi .. Any more inputs for @nmaillard ? Thanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/101949365/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "fiserro": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/104680357", "body": "Unfortunately it's still wrong\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/104680357/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "tianyi": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/121160165", "body": "@jmahonin , your patch is the same with the changes in our internal version.\n\nIt works well under Spark 1.4.0\n\nShould I add the `spark.version` changes in `pom.xml` in this PR? \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/121160165/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/121978717", "body": "Should I close this PR?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/121978717/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "JeffreyLyonsD2L": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128498647", "body": "Added the changes requested, and removed the redundant padding.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128498647/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128835914", "body": "Thanks for all the feedback @elilevine and @samarthjain! I'm east coast, so I'm just heading out of the office now, but I should be able to get the changes up sometime tomorrow.\n\nThe above question kind of goes outside my realm of interaction with Phoenix so I'm not sure if my handle on it is correct. Is the point of the TenantId in a table like System.Catalog to tie specific schema to a tenant? If so I would lean toward leaving as VARCHAR since any TenantId has to be at least convertible to a VARCHAR since it comes down the connection as one, though I agree the your 'correct' solution seems like the most complete way forward.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/128835914/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/129218019", "body": "I can probably provide a little more information about our use case once I meet back with my team on Monday, but I can definitely say that James' point about being able to map existing tables with the concept of a tenantId without having to convert all of the data would be a big plus for us.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/129218019/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/129597825", "body": "Hey @elilevine, thanks and I appreciate the review, I've been learning a lot :)\n\nIn terms of our use case, we have a pre-existing schema that we don't want to modify. For one, it would be a significant refactor to some of our other code in order to handle converting our existing TenantIds to strings. \n\nThe other issue is that we have a lot of traffic that is interacting with the cluster directly through HBase and not through Phoenix. For this reason we like having the TenantIds as fixed length, so that the rowkeys are easy and logical to parse.\n\nIt also seems reasonable to me that if the TenantId is naturally a number instead of a name that it could be represented with one, instead of forcing a conversion to string when we store it.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/129597825/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/129874512", "body": "For points #1 and #3, I think I really mean converting the data we already have. We have a lot of data in our system with our current schema and would have to upgrade the entire schema and most of our historical data to fit the new standards, which for us would be a non-trivial process.\n\nWe are also writing to the underlying tables through our own library, not through Phoenix, and that system is set up to have TenantId as an UNSIGNED_INT. This code is pretty ingrained in our system, and would require a massive refactor and significant restructure of how our cluster works to switch over to using something like fixed-length CHAR arrays.\n\nAs for the tests, I'll get them up as soon as possible.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/129874512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/130012385", "body": "Thanks @elilevine, it has been a very interesting discussion for me to see :)\n\nWith regards to the tests you mentioned earlier, I've added them to the TenantIdTypeIT. Although in looking over the review, I saw I missed one of your previous comments regarding testing tenant-specific indexes and sequences, so I'll add some of those this afternoon.\n\nI'm fairly new to committing to open source, so is there anything you need from me organizationally or git-wise once the final code changes are in (something like going through and resolving the conflicts, or rebasing)?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/130012385/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/130064362", "body": "Added the last few tests.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/130064362/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/130350426", "body": "Alright @elilevine, final revisions to the tests are up. As I mentioned in my earlier post, I'm not quite sure where my work ends, but I'll be available to fix/help with anything needed for the next few days.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/130350426/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/130432243", "body": "You're very welcome, I've found the process streamlined and would be encouraged to contribute again! I resolved the conflicts on this branch, and created another PR for the 4.x-HBase-0.98 branch (#108 for reference)\n\nI'll be available fore help if there is anything else needed.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/130432243/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "gliptak": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/132392617", "body": "@ndimiduk Would you like to see additional changes? Thanks\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/132392617/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/159116060", "body": "https://github.com/apache/phoenix/pull/110\n@ndimiduk @JamesRTaylor and @chrajeshbabu Please review. Thanks\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/159116060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/160742249", "body": "@JamesRTaylor I updated the commit message\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/160742249/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/160784699", "body": "@jmahonin Could you run the tests locally (my local setup is not quite working ...)? Thanks\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/160784699/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/160817033", "body": "Now it runs locally.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/160817033/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/161137156", "body": "I measured 2x longer (with the Scalatest throwing warnings/errors during tear down ...). I will try to add the companion object.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/161137156/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/161151768", "body": "Scala \"static\" methods on the companion object are not recognized as such by the `@BeforeClass` and `@AfterClass` annotations.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/161151768/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/161358663", "body": "Another possibility is to duplicate `BaseHBaseManagedTimeIT`/`BaseTest` functionality in `Scala`/`FunSuite` ...\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/161358663/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "chu11": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/145111243", "body": "Note: This patch is for https://issues.apache.org/jira/browse/PHOENIX-2303\n\nMy only comment is should this patch look at hbase-env (similar to queryserver.py)?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/145111243/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/145141680", "body": "There's another call to 'java' later on in performance.py that also needs to be adjusted.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/145141680/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/145145800", "body": "LGTM\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/145145800/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "chuxi": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/172724839", "body": "the test. \n\n![screen shot 2016-01-19 at 11 05 41 am](https://cloud.githubusercontent.com/assets/5573798/12408593/ee4365f8-be9c-11e5-93f7-256476021cbd.png)\n\n<img width=\"493\" alt=\"screen shot 2016-01-19 at 11 08 23 am\" src=\"https://cloud.githubusercontent.com/assets/5573798/12408604/01141ae2-be9d-11e5-9e52-7876f734dcfa.png\">\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/172724839/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "djh4230": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/phoenix/comments/16779652", "body": "hi James,\nWe are using Phoenix for a while. We found the aggression operation ,like filter,group by,order by etc,cost too much cache of server side recently. We suspected  if there is a memory leak. But today i  found you have fixed a memory leak bug.  Could  you please describe the memory leak appearance in your case?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/16779652/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/16795183", "body": "hi, james\nI am using Phoenix for a while. Recently i found  it cost too much memcache in server side when i do aggression operation like group by etc. I suspect that there is a memory leak in server side. But i am not very sure. And i find you have fixed a bug about server side memory cache. Could you please describe the appreance in your case? \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/16795183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/16853391", "body": "yes, I saw this in 4.7.0 release.\n\n In my case, i want to get data from a table with 60 million data, about 70G. There is no index.\nselect max(PK),\"req_id_card\",\"req_name_card\",max(\"req_time\") as \"req_time\" from \"channellog\"(\"req_id_card\" varchar,\"req_name_card\" varchar,\"res_result\" varchar)  where \"resourceId\"='185' and \"channelId\"='15' and \"res_result\"='1' and to_date(\"req_time\",'yyyy-MM-dd HH:mm:ss')>=to_date('2016-03-24 14:40:10','yyyy-MM-dd HH:mm:ss')  group by \"req_id_card\" ,\"req_name_card\"\n\nIt will cost 5-6G cache in server side. I don't known if it's normal. And also this happed in join operation.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/16853391/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "SsnL": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/phoenix/comments/18218383", "body": "Yes it passes. \n\nPreviously, the two new rows are inserted through `conn`, not the new `conn2`, which contradicts the original comments. That is the reason I made the change.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218383/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218395", "body": "This makes sense. Should we do that without direct API (i.e. when using `PhoenixIndexImportMapper`) as well?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218395/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218417", "body": "I think it makes more sense to add a new query test for `conn2` with expected row count 4.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218417/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218478", "body": "Agreed\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218478/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218510", "body": "I see. \n\nThe tests failed when direct API and transactional are both on. The error was due to `TransactionContext` having a `null` `txClient`. I wasn't sure how to trace and fix that. However, these tests passes once I changed to use \n`IndexToolUtil.updateIndexState(connection, qDataTable, indexTable, PIndexState.ACTIVE);` from client instead of what was in the reducer.\n\nDo you have any insights on what might be wrong here, James?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218510/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18219844", "body": "Will do when I finish them.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18219844/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "dbahir": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/phoenix/comments/18831582", "body": "UserGroupInformation.getCurrentUser() will not be thread safe.\n\nThe file below is a patch that addresses that and other issues\n\n[phoenix.txt](https://github.com/apache/phoenix/files/445294/phoenix.txt)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18831582/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18834022", "body": "My bad regarding the synchronization, you are correct.\n\nDid you get to look at the comment regarding the user login ?\n\nCan you allow another to login with a different principal? Would that cause a security issue?\nIf we create one driver(One) with user A and then create another driver(Two) with user B the info in the UGI now is that of user B. So there can be a situation where driver One will be using credentials of user B.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18834022/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "dispalt": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17644319", "body": "``` java\nif (span == null)\n  return;\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/17644319/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}}}}