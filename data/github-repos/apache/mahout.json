{"_default": {"1": {"rawkintrevo": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/mahout/commits/7dff35bc3c61c3e0b95e8e59406742be15203a69", "message": "WEBSITE-NOJIRA Direct download on homepage button"}, {"url": "https://api.github.com/repos/apache/mahout/commits/84f389140c3781ace210ad9fc977f0d464666644", "message": "WEBSITE-NOJIRA Add mahout-version"}, {"url": "https://api.github.com/repos/apache/mahout/commits/71ce4bf599f7abe8508724d124fa4554153055c6", "message": "WEBSITE-NOJIRA Added Clustering to Navbar"}, {"url": "https://api.github.com/repos/apache/mahout/commits/5c51c60180b05f5ea70ef7b86daf62c7e250ef08", "message": "WEBSITE-NOJIRA Added mahout-version variable"}, {"url": "https://api.github.com/repos/apache/mahout/commits/36da4cf3d4982e246191a3ec7eac2495a25f8a86", "message": "WEBSITE-NOJIRA Added Fn for news"}, {"url": "https://api.github.com/repos/apache/mahout/commits/d53d444374c874a89abd87042a4bdd6a57915258", "message": "WEBSITE-NOJIRA Fix Mahout Overview Link"}, {"url": "https://api.github.com/repos/apache/mahout/commits/4c1d12e39801ca7e2fa1170190d77e150ff11d66", "message": "WEBSITE-NOJIRA Fix Download link, etc."}, {"url": "https://api.github.com/repos/apache/mahout/commits/b9895898d799ae913ad2b37e10b4ae75467b5e37", "message": "EMPTY commit to kick re-establish asf-pages mirror"}, {"url": "https://api.github.com/repos/apache/mahout/commits/6141be782e271748c9839de1e2b837eabb25033c", "message": "WEBSITE-NOJIRA Fix API Docs Link"}, {"url": "https://api.github.com/repos/apache/mahout/commits/1c528a194ebad97f24934685e7bfd9872ebe3019", "message": "WEBSITE-NOJIRA Various website fixes"}, {"url": "https://api.github.com/repos/apache/mahout/commits/ca8b4935c4014397ee4ffc7cd7964dce0979cdf6", "message": "doap_Mahout.rdf"}, {"url": "https://api.github.com/repos/apache/mahout/commits/5a07b51b4f85ea54ee234bd8e2bc320f49f8968c", "message": "WEBSITE removed themes- no longer used- updated how-to-update-website.md"}, {"url": "https://api.github.com/repos/apache/mahout/commits/fc36bef157dc47d9572c4cfd460067e1916910a0", "message": "MAHOUT-1981 Update build_site.sh to not commit entire website"}, {"url": "https://api.github.com/repos/apache/mahout/commits/03918713563bf6aac55299d7239c4e042795faf0", "message": "MAHOUT-1981 Missing Gemfile Added"}, {"url": "https://api.github.com/repos/apache/mahout/commits/9beddd3108ae453091a21a881c6693016f5f2012", "message": "MAHOUT-1981 Merged site updates, fixed navbars, Mathjax"}, {"url": "https://api.github.com/repos/apache/mahout/commits/4f2108c576daaa3198671568eaa619266e787b1a", "message": "[WEBSITE] Small change to make sure publishing"}, {"url": "https://api.github.com/repos/apache/mahout/commits/8a7bd9156880f3f12509c64cda62ac389cbb4f7d", "message": "[WEBSITE] Updating Front Site Config"}, {"url": "https://api.github.com/repos/apache/mahout/commits/9fe4aadc731ffecf3996992b51b8044c880b0e94", "message": "[WEBSITE] Download Javadocs if not present"}, {"url": "https://api.github.com/repos/apache/mahout/commits/1e6584d78e8dc83c04fd1c29a8b01d7e63b6005d", "message": "[WEBSITE] Issues with build_site.sh"}, {"url": "https://api.github.com/repos/apache/mahout/commits/697eae12c513f15e97a18b8a4e143fdc53064b7f", "message": "[WEBSITE] Deleted frontsite"}, {"url": "https://api.github.com/repos/apache/mahout/commits/2483037dc65fd69a508759f241e62709ea780af3", "message": "testing for infra"}, {"url": "https://api.github.com/repos/apache/mahout/commits/52154565410a7ebb9f9cdfe13ad9f039c1f8abb7", "message": "sync asf-site to github with empty"}, {"url": "https://api.github.com/repos/apache/mahout/commits/c80e00f894779b9c937eea280b3b7c39d07ed610", "message": "[WEBSITE] Jekyll build path command updates"}, {"url": "https://api.github.com/repos/apache/mahout/commits/2613883f2f6af05e69cdcc5945a456480747566e", "message": "[WEBSITE] Update how-to-update-website instructions"}, {"url": "https://api.github.com/repos/apache/mahout/commits/f0a246a341cdf66e1ff727a469ed268c00b7ebfe", "message": "[WEBSITE] Build_site.sh lost executable again"}, {"url": "https://api.github.com/repos/apache/mahout/commits/2872a3082549731314e4426774978da86698aa3a", "message": "[WEBSITE] Build_site.sh lost executable again"}, {"url": "https://api.github.com/repos/apache/mahout/commits/bb33d9a5a09a8b77c0853c41cd85228cecc40675", "message": "[WEBSITE] Build_site.sh issues"}, {"url": "https://api.github.com/repos/apache/mahout/commits/d5014fc543343b08a0c876c2b76d09f86092e86c", "message": "[WEBSITE] Build-site.sh executable"}, {"url": "https://api.github.com/repos/apache/mahout/commits/fe77fc19fc0c4d0c05c55a30a473acc71e30f1de", "message": "[WEBSITE] Move BuildingMahout.md"}, {"url": "https://api.github.com/repos/apache/mahout/commits/e591012439c04e98d669ef9732fde865a9ef76fa", "message": "[NO-JIRA][WEBSITE] Emergency surgery 1, jeckyll not building"}, {"url": "https://api.github.com/repos/apache/mahout/commits/b213213a302caa91fc4b5a64247a4a1862244b38", "message": "[NO-JIRA][WEBSITE] Make build_site.sh executable"}, {"url": "https://api.github.com/repos/apache/mahout/commits/54f01bce24a946b2f93f6606f6c970e62c3c57ee", "message": "large commits (website) can break mirror, trying to re-sync"}, {"url": "https://api.github.com/repos/apache/mahout/commits/5f3133c337ca89893b0accb2e7edae9a4e756166", "message": "MAHOUT-1984 Establishing New Build Procedures for Website"}, {"url": "https://api.github.com/repos/apache/mahout/commits/defbbd20f78c7b9e0bcc3a81d3d79d76be32cf23", "message": "NOJIRA Fix LastFM CCO Row Cardinality Bug closes apache/mahout#351"}, {"url": "https://api.github.com/repos/apache/mahout/commits/1d198100aa02005e8e5e0319ca2a08113bf58468", "message": "closes apache/mahout#338"}, {"url": "https://api.github.com/repos/apache/mahout/commits/a128b768e62a229be845855ecee235240efec8f7", "message": "closes apache/mahout#236"}, {"url": "https://api.github.com/repos/apache/mahout/commits/d9b32f308804859b382e855d5a323a62ba7153e1", "message": "MAHOUT-1999 Multi-artifact build closes apache/mahout#350"}, {"url": "https://api.github.com/repos/apache/mahout/commits/be254cdf3d7d24826e52a0f3db66902e485fad43", "message": "closes apache/mahout#337"}, {"url": "https://api.github.com/repos/apache/mahout/commits/b99e71c32c9fece5acf4485e7f3073d50240336a", "message": "MAHOUT-2012 Fix hardcoded dataset in canopy clustering"}, {"url": "https://api.github.com/repos/apache/mahout/commits/d108041d92faa0327339e478305222022b858329", "message": "still broken, but profile order matters"}, {"url": "https://api.github.com/repos/apache/mahout/commits/aedb0243715058c5e32a7026b7350e221b3fc788", "message": "throwing poo at the wall"}, {"url": "https://api.github.com/repos/apache/mahout/commits/cbdec6e059a07f2f33a3e67a5f3ce1febf963651", "message": "distribution was looking for jar named with classifier"}, {"url": "https://api.github.com/repos/apache/mahout/commits/2670242f0d065bf2deb521f337d7a2d8b705b0b2", "message": "moved distribution module to end"}, {"url": "https://api.github.com/repos/apache/mahout/commits/0f82e09527150753901ea0deeb2fcec8d45de781", "message": "moved integration module to end"}, {"url": "https://api.github.com/repos/apache/mahout/commits/50f605a0d5c484a5b0d4028fb09d15aba57a290a", "message": "add scala version to spark profiles or math-scala doesn't build"}, {"url": "https://api.github.com/repos/apache/mahout/commits/e2dde2272fe5956faaf1da2b34bc9119d7f33a5d", "message": "h2o dependency reduced"}, {"url": "https://api.github.com/repos/apache/mahout/commits/ecf642f7de1fd56435282e7db6734e6abe359d7b", "message": "fix antrun copy issue on h2o"}, {"url": "https://api.github.com/repos/apache/mahout/commits/89484d8791bbeba6ccd42271de67d76541529100", "message": "added tests"}, {"url": "https://api.github.com/repos/apache/mahout/commits/9f83099af8c9e788da00cdb422f9e97ac8492961", "message": "failing on h2o, but works otherwise"}, {"url": "https://api.github.com/repos/apache/mahout/commits/b3c6708bab882e2637a854b1a00ec21dbe268b2d", "message": "better cleaning"}, {"url": "https://api.github.com/repos/apache/mahout/commits/9a4f9d36298259274dac94665eda7ce9e482cdbd", "message": "viennacl and viennacl-omp Pall-scala"}, {"url": "https://api.github.com/repos/apache/mahout/commits/72dfcee8c15594c7d3cc9daee13de0752451a48b", "message": "rebased off of master fresh"}, {"url": "https://api.github.com/repos/apache/mahout/commits/e3530b24132b39ee7ada7d5ec9a9edf388984fb1", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/4149843c13c8eb5c54deaad3fd77a22968c5f966", "message": "[maven-release-plugin] prepare release mahout-0.13.1"}, {"url": "https://api.github.com/repos/apache/mahout/commits/47a9362255fed1e1f8a21c1bf93549b308987d52", "message": "Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/mahout"}, {"url": "https://api.github.com/repos/apache/mahout/commits/8c14b309cb4319cb304b68328bc646a3ec48f55d", "message": "MAHOUT-1999 Automate Multi-artifact release"}, {"url": "https://api.github.com/repos/apache/mahout/commits/74766e309cb371059ce73fe2f7a585d6757662a4", "message": "MAHOUT-1998 Naming Spark Artifacts closes apache/mahout#330"}, {"url": "https://api.github.com/repos/apache/mahout/commits/ef5820f22f6a3e7acfdcde278bc28e8c8f0c41d1", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/c806504cd9b85705659a27c12116b9ef04a22dad", "message": "[maven-release-plugin] prepare release mahout-0.13.1-rc1"}, {"url": "https://api.github.com/repos/apache/mahout/commits/d264c1b6a3dd5451ff002f86bd241098a4b6f5f6", "message": "[maven-release-plugin] rollback the release of mahout-0.13.1"}, {"url": "https://api.github.com/repos/apache/mahout/commits/f04fa9afe83ce9e147909c9956b62f05f6257aba", "message": "[maven-release-plugin] prepare release mahout-0.13.1"}, {"url": "https://api.github.com/repos/apache/mahout/commits/d3fb367b31cfe7932e2c1c5607838158b8df0e73", "message": "[maven-release-plugin] rollback the release of mahout-0.13.1"}, {"url": "https://api.github.com/repos/apache/mahout/commits/3f8d181ca519982c95e7d2f83f8d0701748406f2", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/6081793d063381c3e6c7ef121ee66660ec789ce6", "message": "[maven-release-plugin] prepare release mahout-0.13.1"}, {"url": "https://api.github.com/repos/apache/mahout/commits/182ef0229842054ba1bbad4352791d493546ade1", "message": "closes apache/mahout#324"}, {"url": "https://api.github.com/repos/apache/mahout/commits/b41e140cc01a58a233eb91e632ed5e1780607406", "message": "MAHOUT-1969 Add Spark Profiles closes apache/mahout#329"}, {"url": "https://api.github.com/repos/apache/mahout/commits/fed8c472084b976965b59cdc3129d20e7fde7735", "message": "closes apache/mahout#179"}, {"url": "https://api.github.com/repos/apache/mahout/commits/c17bee3c2705495b638d81ae2ad374bf7494c3f3", "message": "MAHOUT-1988 Make Native Solvers Scala 2.11 Complient closes apache/mahout#326"}, {"url": "https://api.github.com/repos/apache/mahout/commits/c29496cb11372baddbb76acdee51530347525645", "message": "MAHOUT-1976 Canopy Clustering closes apache/mahout#314"}, {"url": "https://api.github.com/repos/apache/mahout/commits/9b4eabb9cfc27eb4f8d062b33114e32a590dc791", "message": "closes apache/mahout#295"}, {"url": "https://api.github.com/repos/apache/mahout/commits/2f2ca3c987745c51fcaa10d056dbd9d6503a9fd6", "message": "closes apache/mahout#299"}, {"url": "https://api.github.com/repos/apache/mahout/commits/7c0babd79bf352c98d46a4d7dab38d11ef9b6669", "message": "WEBSITE Emergency Patches for go-live closes apache/mahout#317"}, {"url": "https://api.github.com/repos/apache/mahout/commits/534c2f8d9a5788db0d671d9623109791dcd579c8", "message": "WEBSITE Emergency Patches for go-live"}, {"url": "https://api.github.com/repos/apache/mahout/commits/b45178e2832cc30bace75ec29be8dfaa943444c2", "message": "WEBSITE Spelling and grammer in Contrib Tutorial"}, {"url": "https://api.github.com/repos/apache/mahout/commits/be2ad1c75b756ca7a797596ab70356308360a509", "message": "WEBSITE Spelling and grammer in Contrib Tutorial"}, {"url": "https://api.github.com/repos/apache/mahout/commits/d5072a082bf702d5a6f275dd058f7fefa5cb2a5d", "message": "MAHOUT-1980 Tutorial for Contributing Algos closes apache/mahout#316"}, {"url": "https://api.github.com/repos/apache/mahout/commits/f71020f41107fd4ac621c9814313e5e2cd0b014b", "message": "MAHOUT-1979 Remove references to develop branch"}, {"url": "https://api.github.com/repos/apache/mahout/commits/9c69c28af0494875d7e9417b82a11b79a063c146", "message": "Merge branch 'master' into website"}, {"url": "https://api.github.com/repos/apache/mahout/commits/f463a5c4cc832932663afe5331cc5ad379abc599", "message": "WEBSITE added dmitriys blog"}, {"url": "https://api.github.com/repos/apache/mahout/commits/3c53a6dcfb0cc8e25d7da4b62e8ed056a7629b98", "message": "WEBSITE final cleanup before merge to master"}, {"url": "https://api.github.com/repos/apache/mahout/commits/c4feca039d93cfa10c074d732511ec3dfa86b69e", "message": "WEBSITE Added Overview to docs"}, {"url": "https://api.github.com/repos/apache/mahout/commits/fc433408f4f2c9b6065598fa27fe7f9e1a3533a0", "message": "WEBSITE Added serial correlation docs"}, {"url": "https://api.github.com/repos/apache/mahout/commits/6e5359d59ac02f9df18176ce465de6afe7b2078f", "message": "WEBSITE Added Deprecated to All MR Titles"}, {"url": "https://api.github.com/repos/apache/mahout/commits/a84294211d9f426e7bc8709f6f876ce23b21ba6d", "message": "WEBSITE refactored Mr and regular navbars into one"}, {"url": "https://api.github.com/repos/apache/mahout/commits/38cc9e2706e07612cac9b400aac19001253e2fee", "message": "MAHOUT-1975 Add GitHub PR Template closes apache/mahout#313"}, {"url": "https://api.github.com/repos/apache/mahout/commits/bd360a2b34432f01cfa53bb7854c5f49854b73ca", "message": "WEBSITE Clenup Tutorials in top navbar"}, {"url": "https://api.github.com/repos/apache/mahout/commits/6f135ee6deeb8f70f33bde17c2353dd2db9dedb1", "message": "WEBSITE Reccomender Docs need attention"}, {"url": "https://api.github.com/repos/apache/mahout/commits/0b38f5167297b3654f1f5fa5e79ce6e8d086ae53", "message": "WEBSITE Adding Docs"}, {"url": "https://api.github.com/repos/apache/mahout/commits/c8bdf2efc66d9dbba07d0c50d8369fb721581149", "message": "NO-JIRA Add ASFv2 License"}, {"url": "https://api.github.com/repos/apache/mahout/commits/516e3fb9ab340a2e641770e32d6511c8c7b365b6", "message": "WEBSITE Ported MR-Clustering Tutorials and Algos"}, {"url": "https://api.github.com/repos/apache/mahout/commits/b582dc5291512ace348540491db8671d7d41c0a2", "message": "WEBSITE Added LastFM tutorial and screenshots"}, {"url": "https://api.github.com/repos/apache/mahout/commits/747d94b1da72c7e3b909087b8f04e03d760df346", "message": "WEBSITE Added Eigenfaces Demo"}, {"url": "https://api.github.com/repos/apache/mahout/commits/2346425bdb4c11133bda8759f106c7637007d702", "message": "WEBSITE Updated All Side Navbars"}, {"url": "https://api.github.com/repos/apache/mahout/commits/9759e024ee93bbcbef142ab15b92e71207096798", "message": "WEBSITE added sidenav-bars for algos and tutorials pages"}, {"url": "https://api.github.com/repos/apache/mahout/commits/c81fc8b72b2fa36469b839423fddab13feba869d", "message": "WEBSITE Porting Old Website"}, {"url": "https://api.github.com/repos/apache/mahout/commits/9c0314528b4fec247ec92654bfcc1471a880e578", "message": "WEBSITE Triage of Old Site Migration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/3a724debcff6c26765a77300d79b28b32b7e3398", "message": "Cleaning up css and nav menus, ready to start porting pages"}, {"url": "https://api.github.com/repos/apache/mahout/commits/0e718ec99681319487c2b7eade17bb58beb1ff3d", "message": "WEBSITE rename old-site"}, {"url": "https://api.github.com/repos/apache/mahout/commits/ac56b5512356ed4a2c590808ee0f68bb512afa10", "message": "WEBSITE Ported old website"}, {"url": "https://api.github.com/repos/apache/mahout/commits/a04a32fd12430d3460690ef057e98ecf628bc198", "message": "Merge branch 'website' of https://git-wip-us.apache.org/repos/asf/mahout into website\n\n# Conflicts:\n#\twebsite/README.md\n#\twebsite/_includes/themes/mahout2/navbar_docs.html\n#\twebsite/_includes/themes/mahout2/page.html\n#\twebsite/developers/how-to-update-the-website.md\n#\twebsite/index.md"}, {"url": "https://api.github.com/repos/apache/mahout/commits/ae8e3bbe62acb5473ea59e78e15982ad465b9334", "message": "MAHOUT-1953 Delete Jars in top directory on mvn clean closes apache/mahout#312"}, {"url": "https://api.github.com/repos/apache/mahout/commits/e2549b78cd17ecdaeaec5ca67bad2474a7cb2d60", "message": "Refactored to docs and front sub sites"}, {"url": "https://api.github.com/repos/apache/mahout/commits/506212ba1b991b8681b35b73abe89537643158c4", "message": "resolved conflicts"}, {"url": "https://api.github.com/repos/apache/mahout/commits/7fcf655aa45d49a549a537f4449d2431883ce5b6", "message": "ported developers and community md, significantly altered theme"}, {"url": "https://api.github.com/repos/apache/mahout/commits/00ef278cd05d11120faf6305b773564b9b0ef671", "message": "[WEBSITE] Ported several old docs"}, {"url": "https://api.github.com/repos/apache/mahout/commits/3958af3df16fd7d8546a8d7eaba2e3c692644f70", "message": "merged with master"}, {"url": "https://api.github.com/repos/apache/mahout/commits/c89e3126c4d85fcdfcdee657b87fd48b6671ba38", "message": "updated readme and build instructions"}, {"url": "https://api.github.com/repos/apache/mahout/commits/a98bd9db539416484c89cf49d7433179d5bc9a43", "message": "Ported tutorials, mahout-samsara, distributed-bindings sections"}, {"url": "https://api.github.com/repos/apache/mahout/commits/0b2d3a15f0adfd5b847dce8e6e33c82797a1f906", "message": "Added Stubs for Precanned Algos and reccomenders"}, {"url": "https://api.github.com/repos/apache/mahout/commits/cd830b69bbc31addbf2375d92fcd70ce09c6543e", "message": "Ported Distributed Linear Algebra Pages"}, {"url": "https://api.github.com/repos/apache/mahout/commits/660036ebba42b739162ec4456ca4da4cea64af2a", "message": "remove permalinks where not appropriate"}, {"url": "https://api.github.com/repos/apache/mahout/commits/8f879548a3e310b960b6b555bd8def40cf60130f", "message": "[WEBSITE] Added Theme"}, {"url": "https://api.github.com/repos/apache/mahout/commits/64108c94d9918f031bb17b78403db3866eaf5dc6", "message": "Added themes and example theme"}, {"url": "https://api.github.com/repos/apache/mahout/commits/4c05501b6c10074149b93500beae9fa5f0d421ca", "message": "updated readme.md with layout info"}, {"url": "https://api.github.com/repos/apache/mahout/commits/bcdad32ac3d250d7c42195d9bfa5e48fcbd4fe1f", "message": "[WEBSITE] Reset build instructions"}, {"url": "https://api.github.com/repos/apache/mahout/commits/a60c79e7a2f32aa3e3c78d184e4219360f24869e", "message": "[WEBSITE] Added Jekyll Bootstrapper etc"}, {"url": "https://api.github.com/repos/apache/mahout/commits/08e02602e947ff945b9bd73ab5f0b45863df3e53", "message": "MAHOUT-1971 Aggregate Transpose Bug closes apache/mahout#307"}, {"url": "https://api.github.com/repos/apache/mahout/commits/4d059f85f7341130193a74d18a951fe3110f884b", "message": "MAHOUT-1970 Add Spark Pseudoclusters in TravisCI closes  apache/mahout#306"}, {"url": "https://api.github.com/repos/apache/mahout/commits/eae79da4c2642a195a63462f8781e56600c1295e", "message": "MAHOUT-1965 Multiple Spark Builds in TravisCI closes  apache/mahout#303"}, {"url": "https://api.github.com/repos/apache/mahout/commits/ca24f0c44931aaf6ea57ef97384e12e39ccc561d", "message": "MAHOUT-1950 Fix block unread error in shell closes apache/mahout#291"}, {"url": "https://api.github.com/repos/apache/mahout/commits/5bccac1193a071d9406283969ff395990a8495c5", "message": "Add Trevor Grant (rawkintrevo) to Signing Keys"}, {"url": "https://api.github.com/repos/apache/mahout/commits/4e0106ae5edafbef7b1411d66bc5f9e9bf44045a", "message": "MAHOUT-1926 Fix p value calc closes apache/mahout#288"}, {"url": "https://api.github.com/repos/apache/mahout/commits/d848625df7144ef14dd90300adcf253f08aa28e4", "message": "[MAHOUT-1924][MAHOUT-1925] Add DW and Unit Tests to CochraneOrcutt closes apache/mahout#287"}, {"url": "https://api.github.com/repos/apache/mahout/commits/ba37a8aa20d0969d2db5d810ecd6d68f7ea57978", "message": "MAHOUT-1924 Add Unit Test for DurbinWatson Test closes apache/mahout#282"}, {"url": "https://api.github.com/repos/apache/mahout/commits/5afdc68e0a25e9f66a0d707a7f76d46d9603b614", "message": "MAHOUT-1894 Add Support for Spark 2.x closes apache/mahout#271"}, {"url": "https://api.github.com/repos/apache/mahout/commits/52aceaf68cc198f6d5c3fb77ac16e21abce6e6dc", "message": "MAHOUT-1931 Add Test for MeanCenter closes apache/mahout#281"}, {"url": "https://api.github.com/repos/apache/mahout/commits/a70a8733c6db0e5dcf02384f8dd474469c42e7c5", "message": "MAHOUT-1930 Add Test for Standard Scaler closes apache/mahout#280"}, {"url": "https://api.github.com/repos/apache/mahout/commits/60bb751926524b62be52f9b4c9d1c70d735a0afc", "message": "MAHOUT-1936 fix AsFactor allReduce block closes apache/mahout#278"}, {"url": "https://api.github.com/repos/apache/mahout/commits/f4631528b6d017d974ed46d191651e1f7edbe6a7", "message": "MAHOUT-1935 fix double calculation of XtX closes apache/mahout#277"}, {"url": "https://api.github.com/repos/apache/mahout/commits/7a3617b33eb397b0d2f44e3a1456f266196c1282", "message": "MAHOUT-1923 Propagate cacheHint in distributed qr decomp closes apache/mahout#276"}, {"url": "https://api.github.com/repos/apache/mahout/commits/771339c82c36929d5b331215fbe792626de373af", "message": "MAHOUT-1922 Propagate cacheHint in dspca closes apache/mahout#275"}, {"url": "https://api.github.com/repos/apache/mahout/commits/fa9aeacccc11e50a166d290521030305d51fa787", "message": "MAHOUT-1924 Propagate cacheHint in dssvd closes apache/mahout#274"}, {"url": "https://api.github.com/repos/apache/mahout/commits/b67034e22b04124c3ec449ef5e6f3cb0b7224e0b", "message": "MAHOUT-1920 Add OpenMP to Dependency Reduced Spark Jar closes apache/mahout#270"}, {"url": "https://api.github.com/repos/apache/mahout/commits/9a31923eae3727d9d91bd2c2ed8df12a616a577e", "message": "MAHOUT-1856 Add Framework for Models, Fitters, and Tests closes apache/mahout#246"}, {"url": "https://api.github.com/repos/apache/mahout/commits/84e90ed23327355f92abeb4aede8f3a9ee5b5867", "message": "MAHOUT-1895 Add convenience methods for converting Vectors to Scala Types closes apache/mahout#262"}, {"url": "https://api.github.com/repos/apache/mahout/commits/3e2d63c10b66a2e24ae65a575a7c2215f72e61cb", "message": "closes apache/mahout#263"}, {"url": "https://api.github.com/repos/apache/mahout/commits/b3b72cb658ed6ad59a092eb554b06898163a1375", "message": "MAHOUT-1896: Add convenience methods for interacting with SparkML closes apache/mahout-263"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/mahout/pulls/347", "title": "[WIP][NO-JIRA] Mahout Cylons Demo Donation", "body": "### Purpose of PR:\r\nThe so-called \"Cylons demo\" was well received at multiple talks in early Fall 2017, and there has been interest in continuing work on the project.  The Cylon demo showcases Mahout in the following ways:\r\n- A more robust use of the \"Eigenfaces Demo\" (on Apache Spark)\r\n- How Mahout \"precanned\" algos can be chopped up and used in Flink Streaming Applications\r\n- How Mahout facilitates the so-called \"Lambda style\" machine learning paradigm by training an offline\r\nmodel in Apache Spark and utilizing it in Apache Flink with Apache Solr as the \"model server\" as well as a so called \"Kappa\" style by continuously training and applying a Canopy Function.\r\n\r\nAs this was originally a demo- it is somewhat 'dirty' to say the least. Much work is to be done to fully integrate this as a nice clean demo- especially with respect to documentation.  \r\n\r\nNote that a working drone is NOT required to run this demo as one can tie into any RSTP video feed and there are many public ones available (as well as creating one with many webcams). \r\n\r\nOpening this as a WIP PR that others may contribute and help me get it ready for merging into the trunk. \r\n\r\n### Important ToDos\r\nPlease mark each with an \"x\"\r\n- [ ] A JIRA ticket exists (if not, please create this first)[https://issues.apache.org/jira/browse/ZEPPELIN/]\r\n- [ ] Title of PR is \"MAHOUT-XXXX Brief Description of Changes\" where XXXX is the JIRA number.\r\n- [ ] Created unit tests where appropriate\r\n- [ ] Added licenses correct on newly added files\r\n- [ ] Assigned JIRA to self\r\n- [ ] Added documentation in scala docs/java docs, and to website\r\n- [ ] Successfully built and ran all unit tests, verified that all tests pass locally.\r\n\r\nIf all of these things aren't complete, but you still feel it is\r\nappropriate to open a PR, please add [WIP] after MAHOUT-XXXX before the\r\ndescriptions- e.g. \"MAHOUT-XXXX [WIP] Description of Change\"\r\n\r\nDoes this change break earlier versions?\r\n\r\nIs this the beginning of a larger project for which a feature branch should be made?\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "andrewmusselman": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/mahout/commits/b887ab3653590c854f2b118162114f1133b9534d", "message": "Merge branch 'master' into mahout-1981"}, {"url": "https://api.github.com/repos/apache/mahout/commits/9a2055e7b7bd1788b0b7523367772650ffdb6b53", "message": "NOJIRA: Shrinking the home page splash area vertically, moving ASF logo to the top."}, {"url": "https://api.github.com/repos/apache/mahout/commits/2be27b3784412beb907374026cd5e5c787a55eec", "message": "NOJIRA: Updating how to release page."}, {"url": "https://api.github.com/repos/apache/mahout/commits/a2b60567ac55f38f942f9fabd3981c1f8b784723", "message": "Removing develop and website branches from CI"}, {"url": "https://api.github.com/repos/apache/mahout/commits/a8a719efaedd453059241b15fbbd99a9e9ceb4c8", "message": "Merge branch 'master' into develop"}, {"url": "https://api.github.com/repos/apache/mahout/commits/2a8139dcfb3ad523aeaced225c162ceabde7dcff", "message": "Merge branch 'develop' into website"}, {"url": "https://api.github.com/repos/apache/mahout/commits/c397ef7f7b104a56980d928466022ad2b59ac297", "message": "NOJIRA: Adding website branch to travis"}, {"url": "https://api.github.com/repos/apache/mahout/commits/e16bcc3d152eb6cf9f512f75bbf5e009b00fe992", "message": "MAHOUT-1933: Migrate website from CMS to Jekyll closes apache/mahout#304"}, {"url": "https://api.github.com/repos/apache/mahout/commits/a5c7cec9fbe24112c71cef5bc428b342a4a78ac3", "message": "Merge branch 'website' into MAHOUT-1933"}, {"url": "https://api.github.com/repos/apache/mahout/commits/f2f8cba4279b86b3a902fcac27e6f5db1d1e49ba", "message": "Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/mahout"}, {"url": "https://api.github.com/repos/apache/mahout/commits/db5a1d5ce5a6e4a3999cf2b943e29e9b64bbed5d", "message": "NOJIRA: Adding new develop branch to travis"}, {"url": "https://api.github.com/repos/apache/mahout/commits/2e5093ea269b2d0e6a57e16db24f261bbf39b1ce", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/cca62c5a600703b99a6291440193dd233103d4e4", "message": "[maven-release-plugin] prepare release mahout-0.13.0"}, {"url": "https://api.github.com/repos/apache/mahout/commits/31b611f6f4a773e944fabf827d9ff747bc9624dc", "message": "Removing extra vienna blocks in bin.xml"}, {"url": "https://api.github.com/repos/apache/mahout/commits/eef02000ad748d5a28f418c2da5fcff877de542f", "message": "Merge branch 'master' of github.com:apache/mahout"}, {"url": "https://api.github.com/repos/apache/mahout/commits/98ca93ed31ea9d84f925ef89886ecd52584da6c7", "message": "Rolling back"}, {"url": "https://api.github.com/repos/apache/mahout/commits/13019f7e4e0a192ffd9dae430fa83a9d6cff84c7", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/2333d08acc1fe476775b4ffa471cf6a1a8a10a43", "message": "[maven-release-plugin] prepare release mahout-0.13.0"}, {"url": "https://api.github.com/repos/apache/mahout/commits/059226746276a263f85d5c81d8173123ce81d45d", "message": "Adding project.version back for viennacl-omp module in viennacl profile."}, {"url": "https://api.github.com/repos/apache/mahout/commits/54ba6c706d36d0e0c100dce05804486efb8a5cc0", "message": "Removing versions from viennacl profiles"}, {"url": "https://api.github.com/repos/apache/mahout/commits/a69fa9d4d7e442975882bec6ead383474c4ef003", "message": "Rolling back"}, {"url": "https://api.github.com/repos/apache/mahout/commits/e2d0e14a3490904d74ab2dbac16ee955c3bc09d4", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/c408c74ba0e96e738dfe1ab62190014030c88ebd", "message": "[maven-release-plugin] prepare release mahout-0.13.0"}, {"url": "https://api.github.com/repos/apache/mahout/commits/b77f89f5da98142eb021e0c5ab3fbdccd5a8bcb9", "message": "Rollback"}, {"url": "https://api.github.com/repos/apache/mahout/commits/6b2e312599c2ae285d682dced977eb82db9c2d4c", "message": "Rollback"}, {"url": "https://api.github.com/repos/apache/mahout/commits/2d41ae3daddd77265c86a231d4f9a2ddadab4b5c", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/aa63105983037819b70fabc84fa576cc8c761634", "message": "[maven-release-plugin] prepare release mahout-0.13.0"}, {"url": "https://api.github.com/repos/apache/mahout/commits/267b34edd472b0329e0d5e8a9122d799779bdb08", "message": "NOJIRA: using project.version attribute in distribution pom"}, {"url": "https://api.github.com/repos/apache/mahout/commits/683990bfdeabb4ce6e0de00844d598204bc30cd8", "message": "Rollback"}, {"url": "https://api.github.com/repos/apache/mahout/commits/75b8cbba3e1364d9d1d8af3c91e5dd8bc00a005e", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/7b71b3160e105e7cd7ceac8b50028b5c2a945689", "message": "[maven-release-plugin] prepare release mahout-0.13.0"}, {"url": "https://api.github.com/repos/apache/mahout/commits/700c270ea554ef33e31a74874199a1be13ee4ae0", "message": "Rollback"}, {"url": "https://api.github.com/repos/apache/mahout/commits/999851e2a805e4c706dba8718c00e0458e2446d2", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/c0f58903b7aa29442814e1f7080858bb6ab9e96e", "message": "[maven-release-plugin] prepare release mahout-0.13.0"}, {"url": "https://api.github.com/repos/apache/mahout/commits/3b36ba28288ab08900fad23bfdf8a55057f63658", "message": "Typo in readme"}, {"url": "https://api.github.com/repos/apache/mahout/commits/28c1d20e21dcb97235a0c4dd5a0e22bf1783f6cc", "message": "MAHOUT-1960: fix flipped sign in example closes apache/mahout#298"}, {"url": "https://api.github.com/repos/apache/mahout/commits/717b1939a8e9d5d66a8746d03a178f4987e8399b", "message": "Rolling back RC."}, {"url": "https://api.github.com/repos/apache/mahout/commits/7a7930e856d0119b3650a34e7007cdfc96bb35a0", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/bb781665d9055121a8fc33257a47f5cbad4ed538", "message": "[maven-release-plugin] prepare release mahout-0.13.0"}, {"url": "https://api.github.com/repos/apache/mahout/commits/d7cdbb009015475c91f8ada27b9834c1473bc91b", "message": "NOJIRA: Editing spacing in readme"}, {"url": "https://api.github.com/repos/apache/mahout/commits/1199e51179bf858ccc3c809b332a0cc9cd950adb", "message": "Merge branch 'master' into MAHOUT-1956"}, {"url": "https://api.github.com/repos/apache/mahout/commits/2367fde821d5f6b052038322a60b0146ac3b3a08", "message": "Rolling back release"}, {"url": "https://api.github.com/repos/apache/mahout/commits/d337d994263b9c268e8131b14dda989d0dd211b8", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/0ba81db1234e7603fa0ae4a9fd0fb0881a0b523c", "message": "[maven-release-plugin] prepare release mahout-0.13.0"}, {"url": "https://api.github.com/repos/apache/mahout/commits/1f3156309f797fe7021383e534ffe5c2a94079f8", "message": "Rolling back release"}, {"url": "https://api.github.com/repos/apache/mahout/commits/6b5ae4d443465cd720e86d51b1bd2d377c0c4929", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/d18fcead78568f18615e2c1e43d50b41ef444df1", "message": "[maven-release-plugin] prepare release mahout-0.13.0"}, {"url": "https://api.github.com/repos/apache/mahout/commits/66bfc6dcf84701eda4f4bbfd326cf1325f44dc2b", "message": "Rolling back release"}, {"url": "https://api.github.com/repos/apache/mahout/commits/07c56f2a9ea1deb98885198d80cd6359d6b4e5a5", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/2418038df19060c6efffff5f93909210ea68e699", "message": "[maven-release-plugin] prepare release mahout-0.13.0"}, {"url": "https://api.github.com/repos/apache/mahout/commits/d9bc8c5241252fc7a9b8415d38e4896eb8db8bc7", "message": "Replacing vienna-omp versions in distribution/pom.xml"}, {"url": "https://api.github.com/repos/apache/mahout/commits/e42e26c5d93e42cb340d744325af219f3d4354f9", "message": "Reverting to 0.13.0-SNAPSHOT"}, {"url": "https://api.github.com/repos/apache/mahout/commits/08248ada165e8c3ebc96e4917129486490eb05f2", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/a6b007b65d7d90fac2f88ba857551d2c5872277b", "message": "[maven-release-plugin] prepare release mahout-0.13.0"}, {"url": "https://api.github.com/repos/apache/mahout/commits/857c96d2cd5a120791f5303ad0fdf45f48d2d5dc", "message": "Rolling back RC, changing some logging"}, {"url": "https://api.github.com/repos/apache/mahout/commits/74d2f886116ed5284e67751a62598fd9dc3ba6b7", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/67e0ccf7a0841481a8b3dcb3b814a356dbccce0b", "message": "[maven-release-plugin] prepare release mahout-0.13.0"}, {"url": "https://api.github.com/repos/apache/mahout/commits/f4f3bfa31ddcd9cffdccd5bc7c60cb22d41ba48e", "message": "Reverting to 0.13.0-SNAPSHOT manually"}, {"url": "https://api.github.com/repos/apache/mahout/commits/a8ff16f2788f584b806721a479edb16ea9810897", "message": "Reverting to 0.13.0 manually"}, {"url": "https://api.github.com/repos/apache/mahout/commits/7d0e6225cbc8a6f186af40a94098fbf91e9f81df", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/dc7b6f493df093aa30e418224950c467d02725ab", "message": "[maven-release-plugin] prepare release mahout-0.13.0"}, {"url": "https://api.github.com/repos/apache/mahout/commits/3a8a64b6a9e8159855d1f4fcfc681cae00489a17", "message": "[maven-release-plugin] rollback the release of mahout-0.13.0"}, {"url": "https://api.github.com/repos/apache/mahout/commits/8bc43395cff7a0060eaf33f6b282a673b4e35051", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/565af6c11ca7b97b3a0b317c09cd964978ec8b6e", "message": "[maven-release-plugin] prepare release mahout-0.13.0"}, {"url": "https://api.github.com/repos/apache/mahout/commits/e1cdfa75448b501c49c46767f866fc8e263c2030", "message": "[maven-release-plugin] rollback the release of mahout-0.13.0"}, {"url": "https://api.github.com/repos/apache/mahout/commits/a89500f33ff35f08f9d15e73eccf6ae0e1c69f81", "message": "[maven-release-plugin] prepare release mahout-0.13.0"}, {"url": "https://api.github.com/repos/apache/mahout/commits/58244008986b7ab35c1bd5c87bd6065c2e61bdec", "message": "[maven-release-plugin] rollback the release of mahout-0.13.0"}, {"url": "https://api.github.com/repos/apache/mahout/commits/aa95461e567f9981a6b886856d49b46acd366e9c", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/3f2e1512f597ae416c265284d6b7817e9adfad5c", "message": "[maven-release-plugin] prepare release mahout-0.13.0"}, {"url": "https://api.github.com/repos/apache/mahout/commits/c842ddf556f6efe12942d2f44778091b2567d145", "message": "NOJIRA: Language in readme"}, {"url": "https://api.github.com/repos/apache/mahout/commits/9ed9b4af4153fd3fed42d928c505d05335b29b92", "message": "MAHOUT-1913: Clean Up of VCL bindings closes apache/mahout#290"}, {"url": "https://api.github.com/repos/apache/mahout/commits/7883ebc26ea40e62d8c73797e15a4ba801ae6348", "message": "NOJIRA: Removing flink module from the default build; MAHOUT-1947 is for putting in a flink-specific profile."}, {"url": "https://api.github.com/repos/apache/mahout/commits/b8f9d26db736268ec1b268cebaec1bed2e860523", "message": "MAHOUT-1903: Fix VCL vector %*% vector implementation"}, {"url": "https://api.github.com/repos/apache/mahout/commits/d36db1d2d775d9b6ade59a451d4746f39b1a354a", "message": "Adding signing key to KEYS"}, {"url": "https://api.github.com/repos/apache/mahout/commits/a2cbd15709e8f5789711d171afc5206fa6ec63ed", "message": "MAHOUT-1910: Remove .zip archive from build"}, {"url": "https://api.github.com/repos/apache/mahout/commits/3351b75b3fbae173a2919c8c61268707ed193ec8", "message": "Closes apache/mahout#62"}, {"url": "https://api.github.com/repos/apache/mahout/commits/edaddde9422a142ce07570b74278b42effd1853c", "message": "Merge branch 'master' of github.com:andrewmusselman/mahout"}, {"url": "https://api.github.com/repos/apache/mahout/commits/25d0ac866edd4ff496950835746fce2519d379bf", "message": "Closes apache/mahout#231 *Won't fix*"}, {"url": "https://api.github.com/repos/apache/mahout/commits/b391c76502a5294284fe761de86f952fd91434a5", "message": "Merge branch 'master' of https://github.com/apache/mahout"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13259038", "body": "Would be nice to have a workflow where the changelog changes along with the bug though; I'd be okay dealing with conflicts on this file unless things get really crazy.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13259038/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32252910", "body": "Empty method?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32252910/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32255098", "body": "Trailing backtick\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32255098/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32257171", "body": "Oh I see this is just moved out of line 51\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32257171/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32257679", "body": "Why does this return a Matrix whereas the previous one returns DrmLike[K], and is there a default number of samples in the previous one?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32257679/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32258149", "body": "=> instead of \u21d2\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32258149/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32260540", "body": "Apostrophe here and line 203\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32260540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32260805", "body": "Extra whitespace after several => operators\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32260805/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32264003", "body": "This looks like a pass-through to DistributedEngine; am I reading it right or missing something?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32264003/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32264061", "body": "Same with these; I don't understand the package.scala construct though.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32264061/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32277244", "body": "Will this be confusing if someone tries to call cache() with NONE set and it's not cached?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32277244/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32280831", "body": "This is a bit tough to read but could be I'm just not accustomed to the idioms, including non-bracketed if-else blocks\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32280831/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32281497", "body": "Do we have a plan to improve/remove?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32281497/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32282381", "body": "Remove comment\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32282381/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "jackcgai": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/mahout/commits/6188fa578acdfb275119a4f85ba128fc0506b4f8", "message": "MAHOUT-2007 Fix wikipedia xml dump url closes apache/mahout#339"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pferrel": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/mahout/commits/800a9ed6d7e015aa82b9eb7624bb441b71a8f397", "message": "MAHOUT-2019 SparkRow Matrix Speedup and fixing change to scala 2.11 made by build script"}, {"url": "https://api.github.com/repos/apache/mahout/commits/f8f8f127231781bca0b981c26e2387bfad3958c2", "message": "MAHOUT-1950 fixes CLI dirvers missing classes, need to make sure this doesn't break something else"}, {"url": "https://api.github.com/repos/apache/mahout/commits/00a2883ec69b0807a5486c61dfcc7ef27f35ddc6", "message": "NOJIRA fixed the Kryo unable to serialze void problem"}, {"url": "https://api.github.com/repos/apache/mahout/commits/a511d6f45e88a133c16469fefb8087fa2dab7d94", "message": "fixing a scoring problem"}, {"url": "https://api.github.com/repos/apache/mahout/commits/1f5e36f249aabc68495ec15f64f5ed6754d9f1e2", "message": "MAHOUT-1883 closes no PR, adds dataset filtering for minimal needed to do cross-occurrence"}, {"url": "https://api.github.com/repos/apache/mahout/commits/c9ee7282d0c695a6eb76dda9590a5386309c758a", "message": "Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/mahout"}, {"url": "https://api.github.com/repos/apache/mahout/commits/220c47493d30c0928e116af9210ab1786068ab13", "message": "added tests for new CCO data filtering to the minimum subset needed"}, {"url": "https://api.github.com/repos/apache/mahout/commits/b5fe4aab22e7867ae057a6cdb1610cfa17555311", "message": "MAHOUT-1853: Add new thresholds and partitioning methods to SimilarityAnalysis"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/mahout/issues/comments/45234064", "body": "I could use a little advice here. The epinions and movielens tests in the examples folder. Should they be put into the build?\n\nPros: good example data.\nCons: the reading and writing are not HDFS, they use java i/o and so only work locally. It is easy to change the Spark context to use a cluster but the data still has to be local. These tests would be easier to maintain if they were attached to the ItemSimilarityDriver, which will handle cluster storage and execution and will be maintained better.\n\nI'd rather move them out into an ItemSimilarityDriver examples folder and will do this if no one objects. They will not be build tests, obviously, since they take too long.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45234064/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45234551", "body": "yes, but downloading is described in the comments\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45234551/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45235053", "body": "I guess I'm suggesting that examples like these might be good in the right place. Not as build tests but as usage examples. As long as they use only supported code (read/write for instance)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45235053/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45238498", "body": "You a Ren and Stimpy fan or is it just the way you feel sometimes?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45238498/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45241940", "body": "Hah, that's me looking at my own code\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45241940/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45252176", "body": "Please do not commit this to the master!\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45252176/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45413683", "body": "In general the problem is the one stated in the top description. If I need to create a new DrmLike+RDD please advise.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45413683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45421615", "body": "I think I found the answer. \n\n```\n  val drmInteractions = drmWrap[Int](indexedInteractions, numRows, numColumns)\n```\n\nThis creates a CheckpointedDrm, with an rdd and DrmLike[Int] trait interface. Seems to work even!\n\nLikely due to my Scala ignorance I couldn't find a scaladoc for the helpers in the package object. I did find a reference to drmWrap in the PDF but couldn't find a scaladoc. Are scaladocs just a wip or did I miss it somewhere? \n\nFor anyone reading this, look at the helper functions in the package.scala files peppered throughout. Some are Spark specific and some just Scala helpers, be sure to lookup the use of Scala package objects. \n\nAnyway, block is removed, more to do before merging.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45421615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45560733", "body": "According to the instructions I merge from my branch anyway. I can close it right? The instruction for closing without merging?\n\nI assume you got my mail about finding the blocker now there are some questions about the cooccurrence algo itself.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45560733/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45626614", "body": "Go ahead and hit the butto. Still have a bit more to do here.\n\nOn Jun 9, 2014, at 6:47 PM, Dmitriy Lyubimov notifications@github.com wrote:\n\nyou can close -- but since i originated the PR, it is easier for me (I have \naccess to the \"close\" button on it while everyone else would have to use \n\"close apache/mahout#8\" commit to do the same.) \n\nOn Mon, Jun 9, 2014 at 5:20 PM, Pat Ferrel notifications@github.com wrote: \n\n> According to the instructions I merge from my branch anyway. I can close \n> it right? The instruction for closing without merging? \n> \n> I assume you got my mail about finding the blocker now there are some \n> questions about the cooccurrence algo itself. \n> \n> \u2014 \n> Reply to this email directly or view it on GitHub \n> https://github.com/apache/mahout/pull/8#issuecomment-45560733. \n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45626614/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45666189", "body": "yes, passes most tests but still not ready\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45666189/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45825150", "body": "OK, passes all tests, I added colCounts to SparkEngine and MatrixOps so Dmitriy will probably want to take a look. \n\nI think this is ready to merge.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45825150/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45912084", "body": "Awaiting Sebastian's take on the naming of 'colCounts' to better fit R-Like Semantics\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45912084/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45917234", "body": "I already fixed the header.\n\nI agree with Ted, kinda what functional programming is for. The reason I didn't use the Java aggregate is because it wasn't distributed. Still probably beyond this ticket. I'll refactor if a Scala journeyman wants to provide a general mechanism. I'm still on training wheels.\n\nThis still needs to be tested in a distributed Spark+HDFS environment and MAHOUT-1561 will make testing easy. I'd be happy to merge this and move on, which will have the side effect of testing larger datasets and clusters.\n\nIf Someone wants to test this now on a Spark+HDFS cluster, please do!\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45917234/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45923020", "body": "numNonZeroElementsPerColumn? vs colSums?\n\nOK\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45923020/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46035704", "body": "Accounting for possible negative values in matrix of drm columns.\n\ndrm case was a simple fix but in core Functions.java was modified to include a \"notEqual(value)\" function. There may be some other way to do this but it is a trivial function and now rather obvious. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46035704/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46203596", "body": "restarting work on this, closing PR until ready for review\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46203596/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46598676", "body": "Docs written describing this here: https://github.com/pferrel/harness/wiki\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46598676/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46598913", "body": "BTW untested on a cluster. Still trying to get mine back working.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46598913/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46756833", "body": "ok. well I'll assume the naming conventions are ok the Scala is reasonable and start the polish and tests. There are a lot of combinations of options. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46756833/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47129279", "body": "This is getting close to merge time. There are several test cases, which exercise most of the code in the PR. It is building with tests on the current master.\n\nMissing but needs to be done before merge:\n- a modified mahout script that runs this. \n\nSignificantly missing, but planned for another PR:\n- two, or more input streams for cross-cooccurrence. This version will do a cross-cooccurrence calc by filtering one input tuple stream into two matrices. This allows testing and may be a common use case but should not be the only option for this use.\n- uses HashBiMaps from Guava for _all_ ID management, even when the IDs are Mahout ordinals. Also all four ID indexes are created even though in this case the External Row/User IDs are never used. An optimization would calculate only the dictionaries needed.\n- HashBiMaps are created once and broadcast to the rest of the jobs. These are not based on rdds and so we may want to do something about these in the future. Haven't thought much about this so suggestions are welcome.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47129279/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47303150", "body": "This sounds like what happened to me.\n\nIt looks like you are using two local with one remote repo each, correct? The instructions Dmitriy and I wrote describe one local and with two remotes.  \n\nYou have to merge your branch into the master with:\n\n```\ngit pull origin mahout-xxxx # pulls from your github branch\ngit checkout master # move to the master branch\ngit pull apache master # just to make sure you have the latest\ngit merge --squash mahout-xxxx # merge your branch with the master AND squash\ngit push apache master # push the now merged master to apache's git\n```\n\nThe last push goes to apache's git repo, otherwise it will not get mirrored to github. Your's was not pushed according to the history on github, which mirrors what was pushed to the apache git repo.\n\nPing me on gtalk: pat@occamsmachete.com if I'm online I'll try to help\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47303150/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47361158", "body": "I see, then what you did is equivalent afaik. But you don't need the pull of a URL since you have that same URL set to 'origin'. But I guess you knew that. The PR is only a place to talk about a branch. you don't merge a PR you merge the branch it refers to.\n\nYou can check your history without pushing to github (though that is a nice visual way to look at it) just do a `git log --date-order --graph` on any branch before a push. So you could do the `git merge --squash mahout-xxxx` then look at mater's history before pushing anywhere. In my case I saw the same as you and so until that is figured out it shouldn't stop us from pushing code IMO. It is a small inconvenience to see where you merged the master with your branch as long as last commit comments are intact describing why the branch is being merged and pushed and it contains the comment that closes the PR. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47361158/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47365291", "body": "As to rebasing:\n\nInstead of doing what you and I did, merging the master every so often with the branch, you can rebase to the new master each time instead. This mainly affects commit history and might be the way to go, given that squash doesn't do exactly what we thought in all cases. \n\nThere is a good writeup on this [here](http://mettadore.com/analysis/the-ever-deployable-github-workflow/) that describes a version of what github does internally. The basic idea is (true of Mahout also) that the master is always deployable and the branch history should not be interwoven into the master when merging a branch.\n\nYou are right to be careful about pushing changes to apache. The policy makes rewriting that history very difficult if not impossible. A little clutter is probably OK but if rebasing causes the right history to get written it will be better in the long run.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47365291/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47375744", "body": "- pom edited\n- assembly/job.xml added\n- mahout script edited, no mahout.cmd but will ask someone with a windows machine to update it.\n- updated MahoutKryoRegistrator to add HashBiMap\n\nRuns on clustered HDFS + Spark, still using Hadoop 1.2.1 but after some more testing and hopefully some review of the stuff in this comment, it's about ready to merge.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47375744/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47697509", "body": "Doing final tests before push, will close MAHOUT-1541, MAHOUT-1568, and MAHOUT-1569\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47697509/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47711165", "body": "And the tests fail. Legacy itemsimilarity is not producing the same results on the same files. Looks like a problem in CooccurrenceAnalysis.cooccurrence. The tests for that class have the wrong values and so pass incorrectly.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47711165/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47960471", "body": "The discrepancy between Spark and mrlegacy is in the final output value from the legacy code\n\n```\nreturn 1.0 - 1.0 / (1.0 + logLikelihood);\n```\n\nThis produces the same results for spark and mrlegacy so i'll go with it but would love an explanation. BTW this has to go into CooccurrrenceAnalysis.cooccurrence it is not put in the LLR calculation.\n\nheh, maps values to the range of 0..1\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47960471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48749896", "body": "Are the scalatests implemented in the Spark module that covers math-scala code implemented here somewhere? I'd vote against merge untils those are in all in place and passing.\n\nThe cf stuff has a rather major bug the I'm working on so I wouldn't move this into math-scala just yet, although it would make an interesting speed comparison once completed. The cf changes will require DSL additions that will be under separate review. Don't have a pr number yet.\n\nAlso I may have missed it but there should be clear instructions for how to build this and run it.  This is like a heart transplant. Before you release the patient make sure all systems are working correctly, the DSL is not the whole body. There should at least be some end-to-end pipelines in examples that anyone can run from a local installation.\n\nBeyond these details I have a bigger issue with merging this. Now every time the DSL is changed it may break things in h20 specific code. It already does in cf for instance but I've signed up to fix those fro spark. No committer has signed up to fix code in both Spark and H2O. IMO this is untenable. \n\nTo solve this the entire data prep pipeline must be virtualized to run on either engine so the tests for things like CF and ItemSimilarity (and the multitude of others to come) pass and are engine independent. As it stands any DSL change that breaks the build will have to rely on a contributor's fix. Even if one of you guys was made a committer we will have this problem where a needed change breaks one or the other engine specific code. Unless 99% of the entire pipeline is engine neutral the build will be unmaintainable.\n\nCrudely speaking this means doing away with all references to a SparkContext and any use of it. So it's not just a matter of reproducing the spark module but reducing the need for one. Making it so small that breakages in one or the other engines code will be infrequent. \n\nI raised this red flag long ago but in the heat of other issues it seemed minor, but I don't think it can be ignored anymore.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48749896/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48754615", "body": "The test issue is with the tests in the spark module that actually test stuff in the math-scala module. Remember our discussion about splitting impl from test for cf? There are several things that cannot be tested without the engine in place.\n\nI will be vocal about objecting to TBD for pipelines. The build will be unmaintainable unless the spark module is reduced to trivial and tiny bits. Any change to the DSL could break things I do not know how to fix and really don't want to sign up for--namely h2o specific TBD stuff.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48754615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48761123", "body": "Exactly, thanks. I see you've done the same for CF also great.\n\nBut this illustrates the problem. I need to change 50% of the tests in CF cooccurrence because they were not catching a bug. Now the tests live in two places h2o and spark. And unless I change the tests in both places the build will break. The files look virtually identical except for the imports, which is good. If that's true, I wonder if we could we use a Scala macro to keep the code all in one file? We might be able to take the same code and produce two artifacts that are both run at build time. That would reduce the load on devs for this kind of thing. \n\nHowever currently almost all IO code is spark specific. You must have re-implemented drm.writeDrm for h2o.  Until this is **not** a re-implementation but is engine neutral we are going to have a growing problem. I am the only person currently working in spark specific land and only Dmitriy and Sebastian are writing for V2. When other committers get past the Scala barrier and start committing similar stuff they will immediately face this. \n\nBTW I am very interested in seeing how h2o ItemSimilarityDriver compares to the spark version. IMO this is the kind of motivation we have to see. If you implemented the driver or the reader/writers we could compare speed on h2o and spark. we have a large enough dataset to make it interesting.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48761123/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48767998", "body": "So you don't see how changing the drm API or storage format will now break code in two places written for two different engines? If I make the change to drm I can fix spark breakage but not h2o. This bit of code is extremely stable and super simple for spark so may be a bad example but new code will not be so stable just the opposite. For each new IO operation (SparkContext dependent)  or engine tuning (SparkConf dependent) we will grow the problem. The core will become untouchable or breakage will happen in places one engineer will not be able to fix.\n\nThis is a real issue, I need to change code in math-scala today, already have but it isn't pushed. Who knows what that will break in h2o implementations? I will be changing cooccurrence tests, so have to make them in two places. Maybe I can do that but when they diverge further than this example I won't be able to.\n\nYou guys need to address these issues as if you were supporting two engines for all Mahout code or you will never see what Mahout committers problems will be.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48767998/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49362805", "body": "BTW I also moved CF into math-scala, leaving tests in spark. There may be a way to move those into math-scala with the new engine neutral test stuff, I'll look into that before I merge. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49362805/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49684400", "body": "Call me the loyal opposition. I'd rather merge math with h2o than h2o with Mahout but will bow to the majority and I count the vote at 2 to one (me).\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49684400/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49738832", "body": "If you are asking me if there is likelihood of significant additions to the DSL or core operations that will require \"symmetric\" implementations in two engines, the answer is yes. Look at Ted's wishlist. To get cooccurrence data prep working has brought up two issues and this is one of the simplest algos.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49738832/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49791289", "body": "Don't overreact here. Dmitriy asked a question so I answered it. I have no intention of further debate on this. I wouldn't block this if I could. It would take a lot more committers making a fuss to do that and I don't see it. I'll be happy to live with the majority view and try to constructively keep the project on track.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49791289/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50664882", "body": "So to be clear, this will require 1.7 on all machines from now on? Not just build and running h2o? Is there a profile that removes this from the build requirements?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50664882/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50668509", "body": "cool, BTW I read Ted and Suneel as +1 in the email thread\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50668509/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51114808", "body": "Success on a cluster with cross-cooccurrence using epinions ratings and trust data.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51114808/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51408987", "body": "Sorry was off the internet during a move (curse you giant nameless cable company!)\n\nAnyway these tests are substantially changed in https://github.com/apache/mahout/pull/36 but I haven't been able to get the new build until now, will check and push 36 first.\n\nAs to building and tearing down contexts I'm not helping things. For each driver test DistributedSparkSuite in the beforeEach creates a context so I use that to start the test. Then the driver I am using needs to start a context so for every time I call a driver I precede it with the \"afterEach\" call to shut down the context. Then call the driver, then call \"beforeEach\" to restore the test context. I also had to tell the driver in a special invisible option not to load Mahout jars with a \"--dontAddMahoutJars\". So the context is being built 3 times for every test. but that hasn't changed, it's always been that way.\n\nWe could reuse a single context per test but it would require disabling some stuff in the driver along the lines of what I had to do with \"--dontAddMahoutJars\". Since I've already had to do this I don't think it would be a big deal to disable a little more. I'll look at it once 36 is pushed.\n\nIs there any reason to build the context more than once per suite? Seems like if I disable the context things in the driver we could run all tests in a single context, right?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51408987/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51410370", "body": "OK so DistributedSparkSuite moved the create context into the beforeAll?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51410370/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51413783", "body": "Do you want to push this with the \"ignore\"s and I'll fix them to use the new DistributedSparkSuite as it gets into the master?\n\nBTW any reason we aren't doing Scala 2.11 since we are upping to Java 7 and Spark 1? \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51413783/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51417124", "body": "Ok, I just pushed the new tests, maybe they work. Don't laugh it could happen.\n\nThere are likely to be problems with my calling afterEach and beforeEach since their meaning has changed. Fixing this will require mods to the driver too I expect and it'll probably be easier for me to do it.\n\nIf you are almost ready with this I'll upgrade to Spark 1.0.1 and grab your branch.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51417124/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51514939", "body": "made changes to use the test context in the driver and tests seem to complete correctly up to the point they try to read the output file, which does contain the correct results.\n\n```\n    val indicatorLines = mahoutCtx.textFile(OutPath + \"/indicator-matrix/part-00000\")\n```\n\nThe part file is created in the driver using `rdd.saveAsTextFile(dest)`. It seems like something was getting done before by shutting down the context, maybe I need to close the output file(s) (not sure how to do that since it's created inside the saveAsTextFile call)?\n\n```\njava.lang.NullPointerException\n    at org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:1215)\n    at org.apache.spark.SparkContext.defaultMinPartitions(SparkContext.scala:1222)\n    at org.apache.spark.SparkContext.textFile$default$2(SparkContext.scala:456)\n    at org.apache.mahout.drivers.ItemSimilarityDriverSuite$$anonfun$4.apply$mcV$sp(ItemSimilarityDriverSuite.scala:303)\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51514939/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51517989", "body": "I can't push them back to you so they are here\nhttps://github.com/pferrel/mahout/tree/spark-1.0.x\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51517989/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51524833", "body": "Wait, I found the problem. I'm closing the context at the end of the driver. I'll fix it.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51524833/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51525161", "body": "ok, past the failure. Now I have to do some test cleanup. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51525161/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51541649", "body": "OK, pushed it back to you. The test pass. All drivers and tests share a single context and man are they fast now. Still using DistributedSparkSuite.\n\nBTW the tmp dir really needs to be deleted beforeAll and afterEach for convenience not afterAll so I changed that. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51541649/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53754239", "body": "Heads up, this changes the classname for CooccurrenceAnalysis to SimilarityAnalysis. There don't seem to be any tests in h2o so I don't think is should break anything. The spark tests have been updated naturally.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53754239/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53958930", "body": "yep, didn't auto close with the commit message, closing now.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53958930/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/mahout/comments/6657027", "body": "Negative interactions?\n\nI guess It is possible input, I\u2019ll change that.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/6657027/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/6657099", "body": "BTW where are the tests for MatrixOps?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/6657099/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/6663830", "body": "nevemind, rather obvious.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/6663830/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/6664314", "body": "Oops, fixed. Added to the test cases.\n\nOn Jun 12, 2014, at 8:22 PM, Dmitriy Lyubimov notifications@github.com wrote:\n\nI still dont get it. This implementation counts positive elements, not nonzeroes.\n\n\u2014\nReply to this email directly or view it on GitHub.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/6664314/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/6666770", "body": "I agree and said that in several places.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/6666770/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/8585771", "body": "Building for hadoop 1.2.1 I get the following compile error:\n\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project mahout-integration: Compilation failure: Compilation failure:\n[ERROR] /Users/pat/mahout/integration/src/main/java/org/apache/mahout/utils/ConcatenateVectorsJob.java:[116,73] cannot find symbol\n[ERROR] symbol  : method file(org.apache.hadoop.fs.Path)\n[ERROR] location: class org.apache.hadoop.io.SequenceFile.Reader\n[ERROR] /Users/pat/mahout/integration/src/main/java/org/apache/mahout/utils/ConcatenateVectorsJob.java:[116,16] internal error; cannot instantiate org.apache.hadoop.io.SequenceFile.Reader.<init> at org.apache.hadoop.io.SequenceFile.Reader to ()\n\nIs this hadoop 2 dependant? \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/8585771/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/8585996", "body": "Yeah, that's what I thought. I went back to Gokhan's commit and all it well.\n\nIs there some reason you can't use the old API for that line? Isn't the main point the precondition? \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/8585996/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/10684389", "body": "huh? I thought the next version was 0.10.1?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/10684389/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/10927175", "body": "Are these required for all scala modules? Can this be put in spark-shell pom instead?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/10927175/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13520938", "body": "It will walk a dir tree finding file by pattern match, not implemented yet and AFAIK not implemented in HDFS which will look in a single dir for pattern matched file names. I'll look to see if Sean did something like this already.\n\nI'm not satisfied with the packaging yet so I agree. If you know of something that does this be happy to take it out.\n\nYes, I will double check that everything has the license statement.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13520938/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13520946", "body": "This recursive search is very common in log file organization and a goal of this is to allow direct reading of log files in production environments.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13520946/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13706588", "body": "Yeah, figured that was ugly and should be > 0 but since iterating nonZero it does work. I'll change this.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13706588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13706737", "body": "yes, I don't see what they did here. it looks correct in the editor. I get this occasionally and haven't figured it out. Often with diffs that you are merging. I end up merging white space that looks identical.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13706737/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13706797", "body": "OK, well thanks for reviewing!\n\nI avoided the numNonZero from Java because it is an upper limit and didn't want to confuse this with that. This is the actual count. But nonZeroCounts is fine if you prefer. It does seem more descriptive.\n\nComment on non-negative below\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13706797/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13708030", "body": "err actually nonZeroCounts doesn't work because we need to indicate column. Java verbose style might be numNonZeroColElements. I await a better suggestion. This is a vector like colSums and the Java getNumNonZeroElements is an Int and isn't reliable?\n\nThe distributed check will never catch a 0 since we iterate nonZero but it is wrong and I'll fix. The inCore? I assume you mean the MatrixOps version. I'll comment there.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13708030/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13708276", "body": "Are you saying this doesn't count non-zero elements?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13708276/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13709105", "body": "Sorry, I never pay much attention to that. What's the limit?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13709105/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13714291", "body": "colCounts or whatever we call it is just as efficient, is distributed and tells the reader what is the important value. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13714291/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13714354", "body": "got it, I'll remove the comment since we do rely on it in the code\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13714354/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13771632", "body": "This is creating a Vector of non-zero counts per column, just like colSums is summing the column's values. The function is simply needed in the aggregateColumns. If you are suggesting another way to do this you'll have to be more explicit.\n\n#17 has nothing to do with that afaict. It is about finding non-zero elements in a Vector.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13771632/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13771652", "body": "Taken from \"equal\" in the same file. Changed one character. But I'll note the point.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13771652/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13771890", "body": "We may want to check for illegal values at some place in the pipeline. This is here so I don't forget. At present a negative value is legal. If we make it illegal I want this to fail.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13771890/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13781567", "body": "Nice. I didn't look deep enough to see that f is the column vector. I'll change that.\n\nWhile we are at it, I now know about A'A (which is the slim calc?) that doesn't really compute A'. If you do similar for two different matrices:`B.t %*% A` does B.t ever get checkpointed?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13781567/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13781773", "body": "Whenever I modify a mature file that someone else has created, my general rule is to stay with the style of the collective authors. Here I agree that the 1.0, 0.0 is better I'm hesitant to change it here when 1, and 0 are used throughout the file and I don't want to change it everywhere. There is probably more chance of me messing something up accidentally than actually fixing something if I change the whole file. If this seem wrong let me know but in past jobs we did this to avoid constant thrash over minor style disagreements.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13781773/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13782871", "body": "used Sebastian's getNumNonZeroElements here. Is your issue with Dmitriy's suggestion? This is only for in core matrices, the code used for drms is the stuff in SparkEngine, which accumulates using the nonZero iterator on row Vectors.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13782871/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14001263", "body": "Needs to be taken out before merge. Was forced for testing, not production.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14001263/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14001316", "body": "Yeah, I agree, the files need to be rearranged but I wanted to get an eye on the design in case there was some deeper need to refactor.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14001316/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14001554", "body": "Ok, seems like I remember these had to be passed in when I wrote that, my mistake,\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14001554/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14001642", "body": "This file is from Sebastian, it will be updated to use the driver and kept in examples. Was originally from a patch for MAHOUT-1464\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14001642/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14325027", "body": "I hear you but maybe a compromise is best in this case. The Reader and Writer  traits are core but trivial. I put them in their own file. The TextDelimited stuff is much more complext but have some one liner syntactic sugar to make them easier to use so I put these in another file--all related to read/write of text delimited files. Gives some better logical grouping, which can be missing from one class one file in Java. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14325027/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14325090", "body": "OK, so create a class that contains an instance of MapLike[String, Any] then use the above constructors and supply a new implicit conversion in the companion Object? I've seen this pattern but wasn't sure why to use it. I'll give it a try here but if you could say why this is good practice i'd appreciate it.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14325090/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15150759", "body": "This supports an immutable CheckpointedDrm. It will create a new CheckpointedDrm with row cardinality that leaves some rows with no representation in the underlying rdd. The tests that use this seem to work but it may be an accident of the test data and this may not be a good implementation.\n\nThe question is if a CheckpointedDrm, needs to be backed by an rdd with n => {} for all zero valued rows or if the absence of anything in the rdd, in other words a gap in the row key sequence, is equivalent to inserting an n = {}.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15150759/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15184775", "body": "I see no fundamental reason for these not to work but it may not be part of the DRM contract. So maybe I'll make a feature request Jira to support this. \n\nIn the meantime rbind will not solve this because A will have missing rows at the end but B may have them throughout--let alone some future C. So I think reading in all data into one drm with one row and column id space then chopping into two or more drms based on column ranges should give us empty rows where they are needed (I certainly hope so or I'm in trouble). Will have to keep track of which column ids go in which slice but that's doable. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15184775/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15198911", "body": "you are looking at old code and this is not meant to be merged. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15198911/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16151394", "body": "It's been a long time since I used java.io. \"Try\" is bad form if you just drop them all so I added a check and got rid of all tries.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16151394/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/21950639", "body": "No idea. I merge the master before any push. Your changes should be in with the merge, right? I could remove this and push it but would rather someone else do it so we can see if it happens again on my next merge.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/21950639/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/22326191", "body": "This is as many as seem safe. Lots inside mrlegacy that could be excluded but its all in the same artifact so leaving in unless someone knows how to exclude particular partial packages. \n\nWon't change the code to trim things from the classpath in this commit but I suspect the dependencies.jar may be all that is needed for spark-shell and drivers.\n\n@andrewpalumbo there's little chance this will mess up your drivers so I may push this after some more testing on my side.\n\nOh, and the dependencies.jar is half the size of the mrlegacy job.jar\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/22326191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/22337057", "body": "@dlyubimov open to that argument but maybe you can explain. This allows us to add dependencies by putting them in any dependent module pom but without dealing with the assembly xml. Doing it with includes mean we have to remember to add to the module pom as well as this assembly every time. Also the excludes will only be things in the environment, I would think change seldom, even with new versions.\n\nThe reverse argument is also true. If we add new parts of spark or scala we'll have to add them to the excludes since they are already in the environment. Not sure what those would be but maybe you have some examples.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/22337057/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/23890923", "body": "This would seem to imply some tests are nesting contexts. If so it's a Mahout bug. \n\nSpark 1.2 is probably the next thing to target since it's been out for awhile and is in the distros. It also has the fix for missing anon functions in the artifacts so it should remove the need to build Spark from source.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/23890923/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24956028", "body": "yes, the only comment guidelines are in the scaladoc section of the style guide so I follow them everywhere except a few one liners.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24956028/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24956118", "body": "an extra space didn't seem worth the commit history. The original substantive change was a missing \">\" typo if you care.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24956118/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24956351", "body": "The function is an Arity-0 method. Perhaps I misread: http://docs.scala-lang.org/style/method-invocation.html\n\nArity-0\nScala allows the omission of parentheses on methods of arity-0 (no arguments):\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24956351/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24956479", "body": "yep, trying to shorten lines.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24956479/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24956598", "body": "The larger issue is probably that infix is used and that is because the referenced examples for this code use infix as Scala style dictates. Unless we want to make some decision about that I have no problem with mixing infix and dot styling. These are more of a DSL than a dot method call if that make any difference.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24956598/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25003562", "body": "Yeah, they (Scala style) say to not put it in if the description says what is returned. Debatable here.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25003562/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25003633", "body": "thought I got those, thanks\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25003633/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25003804", "body": "You should have been a lawyer, I'll fix\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25003804/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25006833", "body": "Wouldn't that be \n\n```\nblah blah blah ... {\n  x =>\n    do something\n}\n```\n\nAt least that's what my editor wants to do. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25006833/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25014869", "body": "No idea. I don't remember doing anything with that, I'll look.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25014869/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25017996", "body": "Must be a setting because that was from Idea\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25017996/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25992085", "body": "You could add a default param to the function, like:\n\ndef parseIOOptions(numInputs: Int = 1, noOutput: Boolean = false)\n\nThen a conditional around the requied(). This will work fine for everyone else with no change.\n\nI wouldn't just remove it because it may be the only required param.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25992085/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25992327", "body": "or just don't call that helper and put your own output option in your driver. I made these helpers to keep some options standardized but no hard rule.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25992327/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/29524322", "body": "Ok, this works without any kryo.register. I've done it both ways. \n\nNote that there is no separate serializer class, the class extends Serializable but doesn't implement anything special and so should just use the Seq[(K,V)] for one map. The inverse is calculated after the broadcast deserialization so it is never on the wire.\n\nIf the rest looks ok I'll remove this line but still not sure why it is bad. Spark recommends using kryo by registering classes. Doesn't removing this mean using default serialization instead of Kryo? Isn't this just using kryo compression and more compact serialization since I wrote no serialization code?\n\nI agree with everything you said so not sure what I'm doing wrong but I'll remove the line.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/29524322/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/29429384", "body": "Seems like a Seq will do for most collections of String so only one append method, I'll change and test. Not sure if there are significant performance issues. The use of .view is very subtle, not sure if it has any effect.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/29429384/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/30535229", "body": "Took out BiDictionary from Kryo per @dlyubimov Not completely sure why this is bad except _any_ mods to Spark/Kryo setup can bite you later. This means that serialization of the dictionaries is not as fast as it could be but make Mahout easier to integrate with applications as a library, though not much because you still have to make sure this method get called before a context is created.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/30535229/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/31936283", "body": "minimally dfsWrite should probably use a non-deprecated write function. This requires a factory for the vector used in drms, is this VectorWritable?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/31936283/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32275250", "body": "There seems to be a way to use reflection to get the Writable factory from the RDD by setting it to null. not sure exactly how to do that here. If anyone wants to take a shot see the last reference, an example in SparkContext.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32275250/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33508872", "body": "So are you suggesting that we deprecate dfsWrite? Meaning leave it as is with a deprecated Spark function for now and replace it wherever it's called? Then when Spark removes the deprecated function we remove dfsWrite?\n\nCan we handle your code with an implicit conversion that will get called before dfsWrite?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33508872/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33614228", "body": "Ok, need to rebase against your branch. \n\nIf I can get this to build against the current master/0.11.0-SNAPSHOT I'd be inclined to push it so someone speak up if there is an objection. Once again, it will disable the shell build, for now.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33614228/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "andrewpalumbo": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/mahout/commits/fe4cb1a0919eebd8262ad8f0ff142a53de9eba42", "message": "MAHOUT-2018: missing dash delimiter in mahout-spark module pom.xml. closes apache/mahout#341"}, {"url": "https://api.github.com/repos/apache/mahout/commits/e56378cf7191fefa56eab6c17266a956d01f709e", "message": "(NOJIRA) Manual rollback to 0.13.1-SNAPSHOT + cosmetic fixes"}, {"url": "https://api.github.com/repos/apache/mahout/commits/36c15d8a633f6836b35578e9915c96f4ca71ac55", "message": "MAHOUT-1994: clean viennacl jars on 'mvn clean -Pviennacl -Phadoop2'. closes apache/mahout#328"}, {"url": "https://api.github.com/repos/apache/mahout/commits/3d0c271df2713f125a435f30f399a6865f87636b", "message": "[noJira] flip signs in SparseSparseDrmTimere.mscala example"}, {"url": "https://api.github.com/repos/apache/mahout/commits/9eb68e5319ad6f7692779e566206a159f98be8b9", "message": "[MAHOUT-1956] Update README.md to include build instructions and an example for GPU/GPU/JVM"}, {"url": "https://api.github.com/repos/apache/mahout/commits/049619464145feb44805877cea337a4d84179083", "message": "[MAHOUT-1955] ViennaCL jars are not being picked up by the shell startup script. closes apache/mahout#294"}, {"url": "https://api.github.com/repos/apache/mahout/commits/b32b6d9731481f00ea3b96e3129b9bc96e34eff5", "message": "[MAHOUT-1939] Remove fastutils shaded jar.  Not being picked up by CLI or shell. closes apache/mahout#293"}, {"url": "https://api.github.com/repos/apache/mahout/commits/eb278dc3d3789e3e945af77d48551aecc15ab33b", "message": "MAHOUT-1919: Flink Module breaks the build regularly this closes apache/mahout#289"}, {"url": "https://api.github.com/repos/apache/mahout/commits/4d1464ef4d7a1d278c828d29a906e8f431a2fcb5", "message": "MAHOUT-1939: Shade fastutil jar conflicictng with CDH Spark #285"}, {"url": "https://api.github.com/repos/apache/mahout/commits/51b1ab99a86311078aa205f32ad178acdb824fc9", "message": "MAHOUT-1938: When building on linux, haswell properties are not working. closes apache/mahout#284"}, {"url": "https://api.github.com/repos/apache/mahout/commits/0f4b3d97cfc651a4bc052515b260648cd0093ac2", "message": "[MAHOUT-1903][MAHOUT-1907] VCL Vector memory copy fix. closes apache/mahout#286"}, {"url": "https://api.github.com/repos/apache/mahout/commits/1eacd7f9d6e34b681e8f789ce83b06a45dc92d55", "message": "Add code signing key to KEYS"}, {"url": "https://api.github.com/repos/apache/mahout/commits/bd0579d4c69e4c38955e82f12d7fcef5e537fdb5", "message": "Closing: Deprecated MapReduce.  Thanks for the contribution. this closes apache/mahout#256"}, {"url": "https://api.github.com/repos/apache/mahout/commits/189dfe577236c009c3f66b611728395108c69801", "message": "this closes apache/mahout#239"}, {"url": "https://api.github.com/repos/apache/mahout/commits/67f3a65cf6b68a2c6ac34130ab16d1a118006f45", "message": "MAHOUT-1912: MAHOUT-1912: CLI driver tests not working with vienniacl. cloase apache/mahout#283"}, {"url": "https://api.github.com/repos/apache/mahout/commits/22c5e86480c124c95b2a4698e23a0d98cd7a474b", "message": "MAHOUT-1934: OpenMP jars aren't being picked up add in dependency-reduced.jar to put javacpp jars on executor classpath. Hack for now. closes apache/mahout#273"}, {"url": "https://api.github.com/repos/apache/mahout/commits/fa89753707d3ad8eba4f4328e164b832d4f699b6", "message": "closes apache/mahout#272"}, {"url": "https://api.github.com/repos/apache/mahout/commits/991583abf6534a0a7118bfea3af60bf3170397d3", "message": "[MAHOUT-1934],[MAHOUT-1911]: Clean up getMahoutContext() and surroundnig functions"}, {"url": "https://api.github.com/repos/apache/mahout/commits/f8596b8668ca5104a28efd54d0ceaab51a619c89", "message": "MAHOUT-1855: Small Fix to Root Pom- (fix broken build)"}, {"url": "https://api.github.com/repos/apache/mahout/commits/034790cce40fcee2b7a875c482345d35f7c0fa8d", "message": "MAHOUT-1885: Inital commit of VCL bindings. closes apache/mahout#269 closes apache/mahout#261"}, {"url": "https://api.github.com/repos/apache/mahout/commits/19182a81dcb76a3b7df55aad1d8d1086e2df7c0f", "message": "Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/mahout"}, {"url": "https://api.github.com/repos/apache/mahout/commits/8e0e8b5572e0d24c1930ed60fec6d02693b41575", "message": "MAHOUT-1906: Ensure customJars are added to the MahoutContext under certain conditions for spark 1.6+"}, {"url": "https://api.github.com/repos/apache/mahout/commits/f4a71d084958f2e1865efc8ac8115cd51e1e57d9", "message": "MAHOUT-1837: fix bug in drm.blockify(): use SparseRowMatrix by default to test for density. closes apache/mahout#252"}, {"url": "https://api.github.com/repos/apache/mahout/commits/727e5be85c0326d9c009d9cdc361fe47ffa201ad", "message": "MAHOUT-1837: fix incorrect <= threshold to > threshold to indicate a dense matrix. Refactored name to densityAnalysis(...). closes apache/mahout#244"}, {"url": "https://api.github.com/repos/apache/mahout/commits/d9940489d2f849d36af396d603f6170ab560e505", "message": "MAHOUT-1837: Sparse/Dense Matrix analysis for Matrix Multiplication. closes apache/mahout#228"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/mahout/pulls/349", "title": "[WIP]MAHOUT-2027: spark-ec2 launch scripts with ViennaCL/JCuda installation", "body": "### Purpose of PR:\r\nPlease give a short description of what this PR is for.\r\n\r\n\r\n### Important ToDos\r\nPlease mark each with an \"x\"\r\n- [ x] A JIRA ticket exists (if not, please create this first)[https://issues.apache.org/jira/browse/ZEPPELIN/]\r\n- [ x] Title of PR is \"MAHOUT-XXXX Brief Description of Changes\" where XXXX is the JIRA number.\r\n- [ ] Created unit tests where appropriate\r\n- [x ] Added licenses correct on newly added files\r\n- [x ] Assigned JIRA to self\r\n- [ ] Added documentation in scala docs/java docs, and to website\r\n- [ ] Successfully built and ran all unit tests, verified that all tests pass locally.\r\n\r\nIf all of these things aren't complete, but you still feel it is\r\nappropriate to open a PR, please add [WIP] after MAHOUT-XXXX before the\r\ndescriptions- e.g. \"MAHOUT-XXXX [WIP] Description of Change\"\r\n\r\nDoes this change break earlier versions?\r\nno - exampe\r\nIs this the beginning of a larger project for which a feature branch should be made?\r\nno", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/318", "title": "[WIP]MAHOUT-1974 (dense cuda multiplication)", "body": "### Purpose of PR:\r\nPlease give a short description of what this PR is for.\r\n\r\n\r\n### Important ToDos\r\nPlease mark each with an \"x\"\r\n- [x] A JIRA ticket exists (if not, please create this first)[https://issues.apache.org/jira/browse/MAHOUT/]\r\n- [x] Title of PR is \"MAHOUT-XXXX Brief Description of Changes\" where XXXX is the JIRA number.\r\n- [x] Created unit tests where appropriate\r\n- [ ] Added licenses correct on newly added files\r\n- [ ] Assigned JIRA to self\r\n- [ ] Added documentation in scala docs/java docs, and to website\r\n- [ ] Successfully built and ran all unit tests, verified that all tests pass locally.\r\n\r\nIf all of these things aren't complete, but you still feel it is\r\nappropriate to open a PR, please add [WIP] after MAHOUT-XXXX before the\r\ndescriptions- e.g. \"MAHOUT-XXXX [WIP] Description of Change\"\r\n\r\nDoes this change break earlier versions?\r\n\r\nIs this the beginning of a larger project for which a feature branch should be made?\r\n", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/129", "title": "MAHOUT-1706: remove dependency jars from /lib in the binary distribution", "body": "The mahout distribution currently is shipping ~56 MB of dependecy jars in the /lib directory of the distribution.   These are only added to the classpath by /bin/mahout in the binary distribution. It seems that we can remove them from the distribution. (we need to get the size of the distribution down)\n\nAny input is appreciated. \n", "author_association": "MEMBER"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/mahout/issues/comments/47295528", "body": "I must have done something wrong it doesn't look like my master ever merged the MAHOUT-1536 branch.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47295528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47301902", "body": "Maybe because i changed a capital and then changed it back so there were no actual files changed the pull from my remote branch to my local master never executed the pull --squash?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47301902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47355092", "body": "Thanks alot Pat for looking at this.  I didn't want to push to apache master because i looked at the log and saw all of those old commits so i pushed to my origin master. \n\nI thought that I was working with the two remote one local model that you were talking about. But i was trying to kind of fake committing a pull request (from my MAHOUT-1536 branch) so i guess it really doesn't make sense. \n\nI tried again just now, and and saw that it was squashing the commits from the MAHOUT-1536 branch.   \n\nI still have all of my commit history on my master though- I guess I have to rebase my master?    \n\nI have to catch a flight now so I'll have to figure this out later. Thanks again.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47355092/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47984557", "body": "Interesting writeup on rebasing.. thx. I'm going to try rebasing next week when i get a finally get some time.    Closing this up for now to not clutter up the PRs.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47984557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49396941", "body": "One of their first questions was where to move it (out of the o.a.m.sparkbindings.drm package).  My suggestion, for now, was a new package in the spark module: o.a.m.classification.(naivebayes).   mrlegacy keeps classification algo packages in o.a.m.classifier.  If we wanted to keep consistent with naming (stick with classifiers v. classification) would this cause conflicts?  \n\nThe next step would be to create some scalatests for this.  This should be relatively easy since most of the rest of mrlegacy NB has is either MR independent or has sequential options. So it should be somewhat straightforward to reproduce (some of) the mrlegacy tests. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49396941/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49397174", "body": "Also, the MAHOUT-1493a patch was written by me to illustrate how the NaiveBayesModel constructor  needed to be called- so don't hold any style points against them on this :). \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49397174/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49813616", "body": "Hi @avati, is there something Java 1.7 specific in the dependencies here? I'm getting a test failure in the h2o module:\n\nDiscovery starting.\n**\\* RUN ABORTED ***\n  java.lang.UnsupportedClassVersionError: water/MRTask : Unsupported major.minor version 51.0\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49813616/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49817944", "body": "Willdo.  I've been running 1.6 on this machine because I think that's what we're officially stuck at. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49817944/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50572257", "body": "All tests are passing for me now running Java 1.7\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50572257/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50802726", "body": "There's a link to sun coding conventions on the how to contribute page:\n\nhttp://mahout.apache.org/developers/how-to-contribute.html\n\nFrom the \"conventions\" link under \"making changes\" #4.\n\nSent from my Verizon Wireless 4G LTE smartphone\n\n<div>-------- Original message --------</div><div>From: Anand Avati notifications@github.com </div><div>Date:07/31/2014  2:40 PM  (GMT-05:00) </div><div>To: apache/mahout mahout@noreply.github.com </div><div>Cc: Andrew Palumbo ap.dev@outlook.com </div><div>Subject: Re: [mahout] MAHOUT-1500 H20  (#21) </div><div>\n</div>Where can I find the Sun coding style? Is it this - http://www.oracle.com/technetwork/java/codeconventions-150003.pdf ?\n\n\u2014\nReply to this email directly or view it on GitHub.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50802726/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51099050", "body": "Hello, could you please add in a check for the $MAHOUT_LOCAL environment variable so that the script can run off of both local and HDFS file systems?\n\nSent from my Verizon Wireless 4G LTE smartphone\n\n<div>-------- Original message --------</div><div>From: roengram <notifications@github.com> </div><div>Date:08/04/2014  2:45 AM  (GMT-05:00) </div><div>To: apache/mahout <mahout@noreply.github.com> </div><div>Subject: [mahout] MAHOUT-1594 (#38) </div><div>\n</div>\n\nDetail: https://issues.apache.org/jira/browse/MAHOUT-1594\n\nFactorization example doesn&#39;t work correctly with Hadoop version: 2.4.0.2.1.1.0-385. The reason is that the example uses local for working directories. I&#39;ve changed all references to local dirs to HDFS ones, change Linux shell commands to hadoop equivalents.\nYou can merge this Pull Request by running:\n\n  git pull https://github.com/roengram/mahout MAHOUT.1594\n\nOr you can view, comment on it, or merge it online at:\n\n  https://github.com/apache/mahout/pull/38\n\n-- Commit Summary --\n- Use HDFS instead of local dir\n\n-- File Changes --\n\n```\nM examples/bin/factorize-movielens-1M.sh (14)\n```\n\n-- Patch Links --\n\nhttps://github.com/apache/mahout/pull/38.patch\nhttps://github.com/apache/mahout/pull/38.diff\n\n---\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/apache/mahout/pull/38\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51099050/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51405333", "body": "Absolutely!  I was just getting ready to do this and write some tests. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51405333/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51405454", "body": "Am hoping to see how the (short) Spark and H2O tests compare \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51405454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51407423", "body": "Yes-- and as a reminder  this code is  a compilation of  patches that were writen before the abastraction away from spark (not by myself).  I've not looked at it too closely, and just put it up for comment and feedback for the Berlin TU students.   I've been away almost the entire summer and had planned on doing some work on this to get back up to speed and to try out the H2o bindings.  \n\nSo please- yes any comments are welcome. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51407423/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51414355", "body": "@dlyubimov thanks for all the comments I'm going to try to get the changes in shortly.  \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51414355/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51436303", "body": "Hi @roengram for your MAHOUT_LOCAL environment variable check you'll want to use something like this:\n\n```\nif [ \"$HADOOP_HOME\" != \"\" ] && [ \"$MAHOUT_LOCAL\" == \"\" ] ; then\n        # do your hadoop filesystem work here\n        $HADOOP dfs -rmr ${WORK_DIR}/dir1\n        $HADOOP dfs -rmr ${WORK_DIR}/dir2\n        $HADOOP dfs -put ${WORK_DIR}/dir1 ${WORK_DIR}/dir1\n fi\n```\n\nYou can look at the other examples in $MAHOUT_HOME/examples/bin to get a better idea.  \n\nI did try to run this in local mode with no success\n\nAndy\n\nWe also need to ensure that all Hadoop commands are compatible with Hadoop 1.2.1\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51436303/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51662654", "body": "I made most of the changes from Dmitriy's comments.  I've done some (hackish) work here just to get this in the right package, compiling and and testing.  Changed the Array[DrmLike] to DrmLike for the sparse feature input (for now).  This is very basic and assumes that each row correspnds to a unique label.  The only real engine backed DRM work right now is done in:\n\n```\nval weightsPerFeature = observationsPerLabel.colSums\n```\n\nI've added a Spark test suite with a test for a skeleton NB model. Tests pass here on Spark.   I've also added an h2o test suite on my MAHOUT-1500nb branch with relativly minimal effort (had to make a few dependency changes to the h20/pom.xml). h2o tests pass.\n\nObviously there is still a lot of work to do here and it won't be ready to merge anytime soon, so I'll leave this PR open for a little while in case anybody's interested and then close it until i have some more work done on it as to not clog up the PR page. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51662654/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51664409", "body": "Ugg- sorry about that- my local repo went out of wack when I was merging something else.  Thanks. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51664409/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51671230", "body": "OK.. rebased on master.  Not sure exactly what happened there.  everything should be up to date. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51671230/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51671783", "body": "Thanks for the reviews!\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51671783/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51672766", "body": "It still needs alot of work before its really a functioning Classifier. At the moment it assumes that each instance is a uniqe class- It probably needs to go back to accepting an array as Sebastian originally had it.  It definitly needs some more tests.  But i'm fine with merging this in as a base now and working on it from there.  It definitly will make it easier to work with!   And shouldn't interfere with anything else.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51672766/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/52261287", "body": "As far as I can tell, this is just waiting to be merged.  The vote has passed in favor of merging.  I've been looking at this a bit over the past week.  I wrote some simple tests for Naive Bayes from M-1493 on top of this and found that it integrated very easily (as far as writing tests in Math-Scala and then extending them in h2o and spark test suites).\n\nI'm not familiar with the inner workings of h2o and am new to scala and the DSL, but the code looks good to me. From what I can see there are a couple of more (very minor) style points that i've noticed (see above comments).  And a couple of updates that need to be made to get this working against the current master.\n\nMy issue has been with the Java 1.7 h2o-core artifact.  I've brought it up a couple of times, and it seems that its not as much of a problem as I'd originally thought.  I am still a little concerned with tests will fail for someone running 1.6. Is there a way to get a 1.6 artifact in here?   Please someone let me know if I'm being overly cautious here.  \n\nLong story short- looks good to me:  +1 from me on merging if we can get that artifact issue solved (or if it is really a non-issue). \n\nLooking back at the email archive over the past few months, I do share many of the concerns that have been brought up,  Especially regarding documentation of spark/h20 supported algorithms, and think that we need to get that up quickly.\n\nSomeone with a better working knowledge of h2o and scala/DSL may want to assign this and review it further and merge. If its just a question simply of needing someone to assign this to and merging it, I can do it. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/52261287/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53359715", "body": "Looks good- All my tests pass here with 1.6 and 1.7.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53359715/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53455542", "body": "@avati - it looks like some of your changes from Dmitriy's style reviews were lost in this branch could you please update those?     \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53455542/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53492639", "body": "Tests pass in distributed mode for me.  Could someone please double check the h2o/pom.xml for me?  I'm not sure if there's anything that needs to be added to not break the nightly build.    \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53492639/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53761366", "body": "That shouldn't break anything in h2o\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53761366/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53788269", "body": "Looks good anand, thanks.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53788269/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/54626604", "body": "@avati thanks! \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/54626604/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/54668342", "body": "@avati - Don't worry about it- I've already made some changes- i'll just take it out.  Thx.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/54668342/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/54692047", "body": "Not at all- I was just closing it up to keep the PR list clean  while i did some work on it (was hoping to soon).  Should just merge it as we discussed and then work from there?  Was going to do a little more work on it but I'm not sure how long that will take.  \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/54692047/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/54693386", "body": "cool- thx. I plan on focusing on this for a while now (when I can).  I have to look at it closely again.  \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/54693386/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55501933", "body": "This is still very much \"java in scala\", and needs alot of work (not even sure if what im referring to as the \"Combiner\" even works .  But conceptually training should work through to NaiveBayesModel on any engine.  NaiveBayesModel is an MRLegacy object.  \n\nMy thinking is that extractLabelsAndAggregateObservations can be overridden and optimized for any engine in its specific module.   \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55501933/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55551628", "body": "thanks for lookling at it Dmitriy.  I'm sure the patch can be written more elegantly.  There is some information here from the spark Mailing list about what is causing the problem:\n\nhttp://apache-spark-user-list.1001560.n3.nabble.com/Spark-SequenceFile-Java-API-Repeat-Key-Values-td353.html\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55551628/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55631586", "body": "(2) and (3) seem best to me,  I should be able to look more closely at it  tonight to try to get a better understing of what it entails.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55631586/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56401176", "body": "Still a work in progress, (and still in need of some cleanup). The latest commits now solve the original key object reuse problem by method (2) - reading key type in from the SequenceFile  Headers and then matching on it:\n\n```\nmahout> val drmTFIDF= drmFromHDFS( path = \"/tmp/mahout-work-andy/20news-test-vectors/part-r-00000\")\n14/09/22 11:20:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\ndrmTFIDF: org.apache.mahout.math.drm.CheckpointedDrm[_] = org.apache.mahout.sparkbindings.drm.CheckpointedDrmSpark@adf7236 \nmahout> val rowLabels=drmTFIDF.getRowLabelBindings\nrowLabels: java.util.Map[String,Integer] = {/soc.religion.christian/21427=6141, /comp.graphics/38427=422, /comp.sys.ibm.pc.hardware/60526=1281, /misc.forsale/76295=2495, /soc.religion.christian/21332=6103, /sci.med/59045=5265, /sci.electronics/54343=5096, /comp.sys.ibm.pc.hardware/60928=1404, /rec.sport.hockey/54173=4205, /rec.motorcycles/104596=3282, /rec.autos/103326=2968, /talk.politics.misc/179110=7333, /comp.windows.x/66966=1944, /rec.autos/103707=3053, /comp.windows.x/67474=2146, /rec.sport.baseball/105011=3850, /talk.religion.misc/83812=7424, /comp.graphics/38707=522, /comp.graphics/38597=484, /sci.electronics/54317=5083, /rec.motorcycles/104708=3322, /rec.sport.hockey/53627=3994, /comp.sys.mac.hardware/51633=1601, /sci.crypt/16088=4686, /sci.electronics/53714=4840, /rec.sport.ho...\nmahout> rowLabels.size\nres15: Int = 7598\n```\n\nWhich is what I am expecting.  \n\nTwo problems that I am still having:\n\n(1) Its not yet solving the problem of setting the DrmLike[_] ClassTag yet.\n\n```\nmahout> def getKeyClassTag[K: ClassTag](drm:DrmLike[K]) = implicitly[ClassTag[K]]\n\nmahout> getKeyClassTag(drmTFIDF)\nres13: scala.reflect.ClassTag[_] = Object \n```\n\n I believe that this is just because I'm not setting it correctly due to my limited scala abilities.\n\n~~(2) DRM DFS i/o (local) is failing.  I believe that this may  downside to integrating HDFS I/O code into the spark module. I'm not positive I'm setting the configuration correctly inside of drmFromHDFS(...).  I have no problem reading in the files from within the spark-shell, but the spark `DRM DFS i/o (local)` test is failing with:~~\n\n```\nDRM DFS i/o (local) *** FAILED ***\njava.io.FileNotFoundException: /home/andy/sandbox/mahout/spark/tmp/UploadedDRM (Is a directory)\n```\n\n~~I believe may be because SequenceFile.readHeader(...) is trying to read from HDFS and the test is writing locally.  I will continue to look into this.~~  \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56401176/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56408133", "body": "ok great- thanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56408133/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56410489", "body": "sorry- had a typo when trying to pull\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56410489/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56411169", "body": "yeah- sorry had your name spelled wrong....  thx\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56411169/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56415515", "body": "Ahh- great!:\n\n```\nmahout> drmTFIDF.checkpoint(CacheHint.NONE).keyClassTag \nres3: scala.reflect.ClassTag[_] = java.lang.String\n```\n\n(after applying #53 )\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56415515/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/mahout/comments/10927850", "body": "I can't remember at the moment if it was only the spark-shell module that needed them or if others did as well. \u00a0I believe it had something to do with the spark-1.2 upgrade, part of which made it into master. \u00a0\n\nPutting them in the shell pom may work. \u00a0\n\nSent from my Verizon Wireless 4G LTE smartphone\n\n<div>-------- Original message --------</div><div>From: Pat Ferrel notifications@github.com </div><div>Date:04/27/2015  5:36 PM  (GMT-05:00) </div><div>To: apache/mahout mahout@noreply.github.com </div><div>Cc: Andrew Palumbo ap.dev@outlook.com </div><div>Subject: Re: [mahout] added scala dependencies in math-scala to fix spark-shell (71165a5) </div><div>\n</div>Are these required for all scala modules? Can this be put in spark-shell pom instead?\n\n\u2014\nReply to this email directly or view it on GitHub.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/10927850/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/comments/16715391", "body": "LGTM\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/16715391/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/comments/18009403", "body": "yep looks like.. I can fix it.  I was also thinking the name should be refactored to `densityAnalysis` so that `true` = `isDense`\\- seems more intuituive.  what do you think? Does sparsityAnalysis have any specific meaning outside of here?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/18009403/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/comments/18009913", "body": "yes that would make sense.  I've done the Refactoring and the fix, and have a jira open to add densitAnalysis() in other places.  am just leaving for the day, so will push the open PR now and make note of this in the current jira or create a new one soon. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/18009913/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/comments/18010076", "body": "We could do a rolling density analysis of each vector and convert all to dense vectors after a certain threshold is met then do a shallow Copy into a  DenseMatrix I'm not sure if that is worth the overhead. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/18010076/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/comments/18010101", "body": "no- that doesn't make sense..\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/18010101/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/comments/18778712", "body": "thank you for reporting this, @AddictedCS.  This commit was actually a bugfix for #228,  and as you point out was not released. in 0.12.2.  The `densityAnalysis()` method is actually  intended to keep memory usage down by using `DenseMatrix`s in any cases where data is estimated to be > 25% non-zero.\n\nThe exception here is being thrown because when blockifying a matrix we use `DenseMatrix` as the default, and then run `densityAnalysis()` on that matrix.       \n\nI suppose a good fix might be to default to a `SparseMatrix` block when blockifying a DRM, check the density of the block,  and in a the case that it is not sparse, create a `DenseMatrix` and copy the data into it.\n\nCould you give a little more information about your dataset i.e. full dimensions, as well as the memory settings that you've provided during execution.  \n\nThanks again for reporting this.      \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/18778712/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/comments/18781772", "body": "Re-opened MAHOUT-1837. Will get a fix out soon.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/18781772/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/comments/18801276", "body": "Maybe we should consider adding in a user definable global property for a density threshold?   \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/18801276/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/comments/20306139", "body": "@smarthi read this too quickly, earlier,  I think that you may need to do something like \r\n\r\n``` val dBlock = new DenseMatrix(block, true)```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/20306139/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/comments/21219678", "body": "@pferrel This does not look like my code at all.. I can't remember writing..", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/21219678/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/comments/21220461", "body": "@pat- I am pretty sure that I would not write `Assert.assertEquals(...)`\r\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/21220461/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": [{"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15920526", "body": "The  MAHOUT_HOME variable will very likly be set either \"true\", or unset. a simple check for it being set will do rather than checking that it is set to an integer.  \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15920526/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274064", "body": "if statment needs braces\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274064/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274105", "body": "for loop needs braces\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274105/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274130", "body": "if statement needs braces\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274130/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274188", "body": "if statement needs braces... a few more occurences i believe as well in other files \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274188/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274291", "body": "for loop needs braces ... here and in a few other places\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274291/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274324", "body": "indentation after catch\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274324/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274365", "body": "indentation after catch here and a few more as well\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274365/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274418", "body": "need to update this to new mahout-math-scala artifact name\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274418/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274443", "body": "need to update to new mahout-math-scala artifact name\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274443/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274540", "body": "need to update this to:\n\n```\noverride protected def beforeAll(configMap: ConfigMap) {\nsuper.beforeAll(configMap)\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16274540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16739580", "body": "Possibly one more commit missing?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16739580/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/17184390", "body": "@avati - is there any need to store the masterURL?  i dont see any usage of it.  \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/17184390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/18001536", "body": "I'm not sure here- we need to remove one of the functions them but I think we should be using val2key here not key2val correct?\n\n``` scala\n     key2val(v: AnyRef) => v.asInstanceOf[IntWritable].get\n     val2key(x: Any) => new Integer(x.asInstanceOf[IntWritable].get)\n```\n\n  so later when we map to the RDD:\n\n``` scala\nval drmRdd = rdd.map { t => val2keyFunc(t._1) -> t._2.get()}\n```\n\nthey will be of form `[Integer][Vector]`rather than `[IntWritable][Vector]` \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/18001536/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/18002168", "body": "Yeah- i left it commented for now because there's some work to be done in h2o also.  Introducing the field to the `CheckpointedDrm` trait at math-scala required an implementation in h2o.  So after adding \n\n``` scala\n  /** Explicit extraction of key class Tag   */\n  def keyClassTag: ClassTag[K] = implicitly[ClassTag[K]]\n```\n\n to `CheckpointedDrmH20.scala`\n\nThe H2o DFS i/o tests are failing at \n\n``` scala\n   // Make sure keys are correctly identified as ints\n    drmB.checkpoint(CacheHint.NONE).keyClassTag shouldBe ClassTag.Int\n```\n\n  it looks like we need to do some similar class matching in h2o- should be relatively simple because `H20Hdfs.java` uses a SequenceFile reader and already checks the Key Class.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/18002168/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16022971", "body": "Yeah I was having alot of trouble with the imports. I'm new to scala. I'd tried your import suggestions, but was having trouble with them.  I was getting errors when I was using:\n\n```\nimport RLikeDrmOps._\n```\n\nI put the package in:\n\n```\norg.apache.mahout.classifier.naivebayes\n```\n\nso it is a level down from `org.apache.mahout.math` so from what I was thinking it needed the `drm.RLikeDrmOps._`\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16022971/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16022978", "body": "NaiveBayesModel is a java class in mr-legacy\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16022978/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/21430349", "body": "Hey anand,  getting an error here. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/21430349/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/21431045", "body": "Thanks again for looking at this, Anand - I'm getting the following with patch applied and the h2o tests enabled.  Appreciate it! \n\n```\n- NB Aggregator *** FAILED ***\n  java.lang.IllegalArgumentException: Not a String\n  at water.fvec.Chunk.set_impl(Chunk.java:189)\n  at water.fvec.Chunk.set0(Chunk.java:158)\n  at water.fvec.Chunk.set(Chunk.java:105)\n  at org.apache.mahout.h2obindings.H2OHelper.drmFromMatrix(H2OHelper.java:334)\n  at org.apache.mahout.h2obindings.H2OEngine$.drmParallelizeWithRowLabels(H2OEngine.scala:83)\n  at org.apache.mahout.math.drm.package$.drmParallelizeWithRowLabels(package.scala:67)\n  at org.apache.mahout.classifier.naivebayes.NBTestBase$$anonfun$2.apply$mcV$sp(NBTestBase.scala:90)\n  at org.apache.mahout.classifier.naivebayes.NBTestBase$$anonfun$2.apply(NBTestBase.scala:69)\n  at org.apache.mahout.classifier.naivebayes.NBTestBase$$anonfun$2.apply(NBTestBase.scala:69)\n  at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)\n  ...\n- Model DFS Serialization *** FAILED ***\n  java.lang.IllegalArgumentException: Not a String\n  at water.fvec.Chunk.set_impl(Chunk.java:189)\n  at water.fvec.Chunk.set0(Chunk.java:158)\n  at water.fvec.Chunk.set(Chunk.java:105)\n  at org.apache.mahout.h2obindings.H2OHelper.drmFromMatrix(H2OHelper.java:334)\n  at org.apache.mahout.h2obindings.H2OEngine$.drmParallelizeWithRowLabels(H2OEngine.scala:83)\n  at org.apache.mahout.math.drm.package$.drmParallelizeWithRowLabels(package.scala:67)\n  at org.apache.mahout.classifier.naivebayes.NBModel.dfsWrite(NBModel.scala:144)\n  at org.apache.mahout.classifier.naivebayes.NBTestBase$$anonfun$3.apply$mcV$sp(NBTestBase.scala:142)\n  at org.apache.mahout.classifier.naivebayes.NBTestBase$$anonfun$3.apply(NBTestBase.scala:117)\n  at org.apache.mahout.classifier.naivebayes.NBTestBase$$anonfun$3.apply(NBTestBase.scala:117)\n  ...\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/21431045/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/22009635", "body": "looks like the exception is being thrown here:\n\n``` java\nwater.fvec.Chunk.set_impl(Chunk.java:189)\n\nboolean set_impl (int idx, String str) { throw new IllegalArgumentException(\"Not a String\"); }\n```\n\n~~I've been looking through some of the more recent code and it seems that the `Vec.Writer.set( long i, String str)` signature has been phased out.~~\n\nSorry- was looking in the wrong place- `h2o` rather than `h2o-dev`\n\nWe should probably address this as a separate issue since we have to update to a newer h2o-core artifact, and there may be some other issues in there too.  \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/22009635/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/22010435", "body": "Cool thanks.    \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/22010435/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/22327470", "body": "Sounds good - push whenever u think is good- I don't think there will be any problems... \u00a0 the only difference that I can think of from the basic structure of the \u00a0item-similarity driver is that the NB driver makea calls to a SparkNB object from the spark module which overrides the math-scala implementation and calls spark aggregateByKey(...). So as long as that's available and math-scala is available there shouldn't be any issues.\u00a0\n\nSent from my Verizon Wireless 4G LTE smartphone\n\n<div>-------- Original message --------</div><div>From: Pat Ferrel notifications@github.com </div><div>Date:12/29/2014  3:03 PM  (GMT-05:00) </div><div>To: apache/mahout mahout@noreply.github.com </div><div>Cc: Andrew Palumbo ap.dev@outlook.com </div><div>Subject: Re: [mahout] MAHOUT-1636 (#69) </div><div>\n</div>In spark/src/main/assembly/dependencies.xml:\n\n> ```\n>      <exclude>org.apache.hadoop:hadoop-core</exclude>\n> ```\n> -        <exclude>org.apache.spark:spark-core_${scala.major}</exclude>\n> -        <exclude>org.scala-lang:scala-library</exclude>\n> -        <exclude>jackson-core-asl</exclude>\n> -        <exclude>jackson-mapper-asl</exclude>\n> -        <exclude>xstream</exclude>\n> -        <exclude>lucene-core</exclude>\n> -        <exclude>lucene-analyzers-common</exclude>\n>      </excludes>\n>    </dependencySet>\n>   This is as many as seem safe. Lots inside mrlegacy that could be excluded but its all in the same artifact so leaving in unless someone knows how to exclude particular partial packages.\n\nWon't change the code to trim things from the classpath in this commit but I suspect the dependencies.jar may be all that is needed for spark-shell and drivers.\n\n@andrewpalumbo there's little chance this will mess up your drivers so I may push this after some more testing on my side.\n\n\u2014\nReply to this email directly or view it on GitHub.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/22327470/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24955709", "body": "An accidental replacement of \"indicator\" with \"similarity\" here?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24955709/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24955978", "body": "Was this `@return` meant to be removed? \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24955978/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25026753", "body": "I see... makes sense.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25026753/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25989376", "body": "@pferrel would changing this from a required option break anything for you?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25989376/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25990201", "body": "thx.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25990201/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27781100", "body": "@dlyubimov, @pferrel  -does adding `.par(auto = true)` here make sense?  Otherwise there are no explicit partitioning instructions for the input data. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27781100/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27768929", "body": "Can we work with the small dataset to save time for now, while we're testing examples, and change it before Back we ship?  The `spark-document-classifier.mscala` script https://github.com/apache/mahout/blob/master/examples/bin/spark-document-classifier.mscala needs the \"partial larger 256M\" dataset to train on in order to classify the sample text correctly.     \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27768929/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/28839428", "body": "If there are no objections to a max 10x20 matrix to display, or anything else, i will commit this to master and the 0.10.1 branch.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/28839428/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/29562662", "body": "there's a conflict here.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/29562662/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32072341", "body": "I think this line can come out for now.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32072341/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32072191", "body": "yeah- this definitely can wait..  I'll re-synch it after #135 is pushed and we can keep it here as a patch for people that want to use 1.3 on the 0.10.x branch.  \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32072191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33214708", "body": "Should this be: `val s = sBcast: Vector` ?\n (rather than                `tBcast: Vector`)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33214708/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33216490", "body": "oops.. I had a typo in the above comment (edited now)..  \n\n`val s = tBcast: Vector` should be:\n`val s = sBcast: Vector`\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33216490/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33221710", "body": "Yeah- definite copy/paste typo.  The h2o/spark tests catch it.  After changing it, the tests run fine.  \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33221710/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33415160", "body": "It seems that the solution could be to map the DrmRdd keys and values to their respective Writables before calling `.saveAsSequenceFile(path)`.    We can use the k2wFunc for the Key, and the value will always be a Vector for Drms so simply map to a VectorWritable.     This is essentially what is done internally in `.saveAsSequenceFile` for standard Writables.\n\n```\n    // convert RDD to Writables\n    rddInput.toDrmRdd().map(x => (k2wFunc(x._1), v2w(x._2)) )\n                       .saveAsSequenceFile(path)\n```\n\nHowever the above gives a runtime exception since the Writables are not Java Serializable: `org.apache.spark.SparkException: Task not serializable`.  Any thoughts on how to do this correctly? \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33415160/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33499832", "body": "Ok, so we can eliminate the runtime error by mapping each key/value to the appropriate `Writable` (not from within a function):\n\n``````\nif (ktag.runtimeClass == classOf[Int]) {\n      rddInput.toDrmRdd()\n\n        .map( x =>(new IntWritable(x._1.asInstanceOf[Int]), new VectorWritable(x._2))).saveAsSequenceFile(path)\n\n    } else if (ktag.runtimeClass == classOf[String]){\n      rddInput.toDrmRdd()\n\n        .map( x =>(new Text(x._1.asInstanceOf[String]), new VectorWritable(x._2))).saveAsSequenceFile(path)\n\n    } else if (ktag.runtimeClass == classOf[Long]) {\n      rddInput.toDrmRdd()\n\n        .map( x =>(new LongWritable(x._1.asInstanceOf[Long]), new VectorWritable(x._2))).saveAsSequenceFile(path)\n\n    } else throw new IllegalArgumentException(\"Do not know how to convert class tag %s to Writable.\".format(ktag))```\n``````\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33499832/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33514280", "body": "That is actually using the non-deprecated `.saveAsSequenceFile(path)`  I'm just suggesting that we could skip all of the implicit conversions and we explicitly map the RDD to Writables ourselves.  Then call `.saveAsSequenceFile(path)` on the RDD of eg. `[IntWritable, VectorWritable]`. This is actually what Spark does in `.saveAsSequenceFile(path)` : https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala#L97\n\nif either a Key or a Value is not a `Writable`, it converts one or the other or both to a Writable using eg.: `self.map(x => (anyToWritable(x._1), anyToWritable(x._2)))`\n\nand then calls `.saveAsHadoopFile(...)` on the Mapped RDD.  \n\nIf it detects that both are Writables though as would be the case if we mapped them explicitly, it simply calls `.saveAsHadoopFile(...)`.  So By mapping them ourselves in `.dfsWrite(...)` we shouldn't incur any additional overhead.\n\nActually we may just be able to call `.saveAsHadoopFile(...)` directly on a mapped => Writable RDD from `.dfsWrite(...)`.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33514280/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33523447", "body": "implemented here (passes all tests):\n\nhttps://github.com/andrewpalumbo/mahout/blob/MAHOUT-1653-jun/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrmSpark.scala#L157\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33523447/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33617050", "body": "+1 (no objection that is..)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33617050/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33975950", "body": "the problem is that the clean up method that we were using is not part of the Spark REPL Developer API.  So the method that we were using to close the SparkContext:\n\n```\n// from our old 1.2 compatible MahoutSparkILoop\ndef sparkCleanUp() {\n      echoToShell(\"Stopping Spark context.\")\n      _interp.beQuietDuring {\n        _interp.interpret(\"sdc.stop()\")\n      }\n   }\n```\n\nis not available to us to override anymore.  So upon exiting the shell, the private `SparkIloop.sparkCleanUp()` is called and looks to call `sc.stop()`: \n\n```\n// from current 1.3+ SparkILoop\nprivate def sparkCleanUp(){\n    echo(\"Stopping spark context.\")\n    intp.beQuietDuring {\n      command(\"sc.stop()\")\n    }\n  }\n```\n\nand giving an error,  leaving our `sdc` context open.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33975950/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33979712", "body": "> comes to that, we could potentially initialize sc with spark context and sdc with mahout context. perhaps that would be ok and create compatibility throughout.\n\nAre you thinking something like:\n\n```\n@transient val sc = {\n           val _sc = org.apache.spark.repl.Main.interp.createSparkContext()\n           println(\"Spark context available as sc.\")\n           _sc\n         }\n@transient implicit val sdc: org.apache.mahout.math.drm.DistributedContext =\n            new org.apache.mahout.sparkbindings.SparkDistributedContext(sc)\n\n        echoToShell(\"Mahout distributed context is available as \\\"implicit val sdc\\\".\")\n```\n\nI'm not sure where we'd be able to call `sdc.close()`  but I'm not sure that that would be necessary anyway since that method in turn simply calls `sc.stop()` which is handled by `SparkILoop.sparkCleanUp()`...  so it might be a little messy but not too bad.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33979712/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33981278", "body": "Actually, now that I'm looking at it again without worrying about the possibility of backwards compatibility <1.3 we should also  be creating a SqlContext in that `initalizeSpark()` call:\n\nFrom SparkILoop.initializeSpark():\n\n```\n      command(\"\"\"\n         @transient val sqlContext = {\n           val _sqlContext = org.apache.spark.repl.Main.interp.createSQLContext()\n           println(\"SQL context available as sqlContext.\")\n           _sqlContext\n         }\n        \"\"\")\n      command(\"import org.apache.spark.SparkContext._\")\n      command(\"import sqlContext.implicits._\")\n      command(\"import sqlContext.sql\")\n      command(\"import org.apache.spark.sql.functions._\")\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33981278/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33999528", "body": "Ok gotcha.. thanks. I guess it also really doesn't matter where the spark context is stopped either.    \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33999528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/34004807", "body": "Reset the mahout distributed context to `implicit val sdc`.  Set `val sc` to the SparkContext in `sdc`.  Created a SqlContext as is done in the overridden method. \n\nThe SqlContext is untested.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/34004807/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/34061617", "body": "Ok- I see what you are saying..  I don't think that we can access that field From the interpreter though.  ie:\n\n```\nval sdc = mahoutSparkContext(...)  // or something like this...\n_interp.interepret(\"@transient implicit val sdc = this.sdc\")\n```\n\nwill give an error like:\n\n```\nerror: value sdc is not a member of iwC$iwC$iwC$iwC.....\n```\n\nI believe since the line is converted to an object by the interpreter not of type MahoutSparkILoop. \n\nWhat we are currently doing is actually following the Spark REPL blueprint:\n\n// Create a SparkContext in SparkILoop\nhttps://github.com/apache/spark/blob/branch-1.3/repl/scala-2.10/src/main/scala/org/apache/spark/repl/SparkILoop.scala#L1005\n\n// Create an other SparkContext in the Intrepter:\nhttps://github.com/apache/spark/blob/branch-1.3/repl/scala-2.10/src/main/scala/org/apache/spark/repl/SparkILoopInit.scala#L121\n\nLet me know if you think there's a better way around it.\n\noops.. this should have been in a line note..\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/34061617/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/34067056", "body": "```\nvar sdc: SparkDistributedContext = _\n\noverride def createSparkContext(): SparkContext = {\n[...]\n   sdc = mahoutSparkContext(\n      masterUrl = master,\n      appName = \"Mahout Spark Shell\",\n      customJars = jars,\n      sparkConf = conf\n    )\n\n    _interp.sparkContext = sdc\n[...]\n}\n\n_interp.interpret(\"\"\"\n      @transient implicit val sdc =\n         org.apache.spark.repl.Main.interp.asInstanceOf[org.apache.mahout.sparkbindings.shell.MahoutSparkILoop].sdc\n         \"\"\")\n```\n\ngives a NPE:\n\n```\njava.lang.NullPointerException\n    at $iwC$$iwC$$iwC$$iwC.<init>(<console>:12)\n    at $iwC$$iwC$$iwC.<init>(<console>:17)\n    at $iwC$$iwC.<init>(<console>:19)\n    at $iwC.<init>(<console>:21)\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/34067056/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/34091459", "body": "uggh.. of course-- i was doing it backwards.  the SparkContext (along with the SparkDistributedContext) have to be created by a call to `createSparkContext()` before we can access sdc (hence the above NPE).\n\n```\n   _interp.interpret(\"\"\"\n         @transient val sc = {\n           val _sc = org.apache.spark.repl.Main.interp.createSparkContext()\n           println(\"Spark context available as sc.\")\n           _sc\n         }\n              \"\"\")\n\n      _interp.interpret(\"\"\"\n\n         @transient implicit val sdc: org.apache.mahout.sparkbindings.SparkDistributedContext =\n            org.apache.spark.repl.Main.interp.asInstanceOf[org.apache.mahout.sparkbindings.shell.MahoutSparkILoop].sdc\n\n                        \"\"\")\n\n      echoToShell(\"Mahout distributed context is available as \\\"implicit val sdc\\\".\")\n```\n\nworks fine.  Had it in my head that  2 SparkContexts were being created rather than 2 MahoutSparkContextes...\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/34091459/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/34195169", "body": "@dlyubimov I've made the necessary changes clean up the the redundant creation of a SparkDistributedContext.  Thanks alot for the input.  \n\nYou mentioned is that we could declare the SparkContext as `implicit val sc = ...`. Is there a reason that it should be implicit?\n\nI've left it as `val sc =...` for now since that is the way Spark declares is in this method.     Thx.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/34195169/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/34217755", "body": "Thank you.  I'll leave it as is.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/34217755/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/43186854", "body": "need to fix this to point to correct dir if this is gonna be used.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/43186854/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "tbfly": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/mahout/commits/5a3b4f509cfca5ab7ce5fe85a4fd006e89e460c8", "message": "NOJIRA - run-item-sim.sh patch closes apache/mahout#325"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mikekap": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/mahout/commits/a799ead59aa77cab0596f72bbe40872b59367e35", "message": "MAHOUT-1795 Create Scala 2.11 Profiles apache/mahout#179"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kinow": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/mahout/commits/31833fa6827729155934be77bdde7c546ebd47f3", "message": "WEBSITE Typos and grammar closes apache/mahout#320"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dustinvanstee": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/mahout/commits/cf5a33d565f6602f6624bc4d34c62c048379140f", "message": "MAHOUT-1962 Add Ftest closes apache/mahout#300"}, {"url": "https://api.github.com/repos/apache/mahout/commits/54ef150e8f31ce2292ea02902674d86c9c35ffd7", "message": "Initial MVP for the website using Jekyll"}, {"url": "https://api.github.com/repos/apache/mahout/commits/724b9070cbde160d237f58a5a178125f00dcfc84", "message": "Merge remote-tracking branch 'apache/master'"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/mahout/pulls/335", "title": "MAHOUT-2000 [WIP] Add maven profile for Spark 2.2", "body": "### Purpose of PR: Add spark 2.2 into travisCI framework and add a maven spark 2.2 profile.\r\n\r\n\r\n### Important ToDos\r\nPlease mark each with an \"x\"\r\n- [x] A JIRA ticket exists (if not, please create this first)[https://issues.apache.org/jira/browse/ZEPPELIN/]\r\n- [ x] Title of PR is \"MAHOUT-XXXX Brief Description of Changes\" where XXXX is the JIRA number.\r\n- [ ] Created unit tests where appropriate\r\n- [ ] Added licenses correct on newly added files\r\n- [x ] Assigned JIRA to self\r\n- [ ] Added documentation in scala docs/java docs, and to website\r\n- [ ] Successfully built and ran all unit tests, verified that all tests pass locally.\r\n\r\nIf all of these things aren't complete, but you still feel it is\r\nappropriate to open a PR, please add [WIP] after MAHOUT-XXXX before the\r\ndescriptions- e.g. \"MAHOUT-XXXX [WIP] Description of Change\"\r\n\r\nDoes this change break earlier versions?\r\n\r\nIs this the beginning of a larger project for which a feature branch should be made?\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "smarthi": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/mahout/commits/84d43790f97988d034b33c55f76ce966949c3168", "message": "Add viennacl jars to binary distro, this closes apache/mahout#301"}, {"url": "https://api.github.com/repos/apache/mahout/commits/b289f4657fb8fe16f60f806ef2b2dee86f666e74", "message": "MAHOUT-1900: Add a getter to DenseMatrix for the double[][] values field, this closes apache/mahout#267"}, {"url": "https://api.github.com/repos/apache/mahout/commits/1e081287671b186915148bedc05aae4384230c8f", "message": "MAHOUT-1901: Remove h2o from Binary Release Build, this closes apache/mahout#266"}, {"url": "https://api.github.com/repos/apache/mahout/commits/8a0a074898c345b73b17a058301acb01352db12b", "message": "MAHOUT-1875: Use faster shallowCopy for dense matices in blockify drm\\/package.blockify(), this closes apache/mahout#264"}, {"url": "https://api.github.com/repos/apache/mahout/commits/8c614a1661f58b42e84acf2b65de9e11703db7c2", "message": "NoJira: Bump up Flink and Slf4j versions"}, {"url": "https://api.github.com/repos/apache/mahout/commits/292b718a633efae5ff1b47772b9dd7bf9f1ca6da", "message": "MAHOUT-1888: Performance Bug with Mahout Vector Serialization, this closes apache/mahout#260"}, {"url": "https://api.github.com/repos/apache/mahout/commits/20fdf9b9fb25dcae9e8b83242d777c373605576f", "message": "No Jira: Update DOAP to the most recent release"}, {"url": "https://api.github.com/repos/apache/mahout/commits/e87567af05884506a37e8751df0fa7f41a9d94d0", "message": "Add Apache license header to READMEmd, this closes apache/mahout#259"}, {"url": "https://api.github.com/repos/apache/mahout/commits/2f6399f3fd36f5074c2f4892407ed9d4f889fa07", "message": "MAHOUT-1881:flink-config.yaml is not copied to /conf in Binary Distro, this closes apache/mahout#255"}, {"url": "https://api.github.com/repos/apache/mahout/commits/e73fdb8694e80e1e95a1213097434749726fd8af", "message": "MAHOUT-1865: Remove Hadoop1 Profile, this closes apache/mahout#253"}, {"url": "https://api.github.com/repos/apache/mahout/commits/c5934c2f73cc0b07ca686f76330e085893283390", "message": "this closes apache/mahout#254"}, {"url": "https://api.github.com/repos/apache/mahout/commits/a5e820c95c4f420d8f44dfc827fb2128c9f0750d", "message": "MAHOUT-1880: Remove H2O bindings from the Release binaries"}, {"url": "https://api.github.com/repos/apache/mahout/commits/eb70eb820cbccb5d82750cd2f6541444bbb2e0dd", "message": "Add Codecoverage maven plugin"}, {"url": "https://api.github.com/repos/apache/mahout/commits/476972d2e3c4b297ac66e711c121ce80fc621d50", "message": "added maven code coverage plugin, this closes apache/mahout#250"}, {"url": "https://api.github.com/repos/apache/mahout/commits/4d0cd66a6269eb02fceaabdb11d70fd38d433474", "message": "MAHOUT-1876: Upgrade lucene to 5.5.2 and fix compilation failures, this closes apache/mahout#248"}, {"url": "https://api.github.com/repos/apache/mahout/commits/33c1eab1133d8f286fb3fd18a105789e7ebee44a", "message": "MAHOUT-1877: Fix for Flink 1.1.0 and hadoop 2 compatibility, fixed by Flink 1.1.1"}, {"url": "https://api.github.com/repos/apache/mahout/commits/0ecb6685c2bb8fd2e8857967954d616a59cc2389", "message": "MAHOUT-1877: Switch to Flink 1.1.0, this closes apache/mahout#249"}, {"url": "https://api.github.com/repos/apache/mahout/commits/45ee3cf097f35735d49b6861c7b9fe04a3d7b562", "message": "Added Coveralls badge"}, {"url": "https://api.github.com/repos/apache/mahout/commits/cfe52f2fa178e0ac4bbb7b761551af88fd658ed0", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/b8ce2493d365dd72a426ebba48310995fe22468c", "message": "[maven-release-plugin] prepare release mahout-0.12.2"}, {"url": "https://api.github.com/repos/apache/mahout/commits/1a2b69384c9c3401b51c0127ff6d063518290d76", "message": "Rolling back Mahout 0.12.2 Release candidate, thanks github connectivity issues"}, {"url": "https://api.github.com/repos/apache/mahout/commits/627900ac8d5ac7f17361bfa4dafcdef0682664ea", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/26bd91275eff3aefb5200025409750aa0f250e59", "message": "[maven-release-plugin] prepare release mahout-0.12.2"}, {"url": "https://api.github.com/repos/apache/mahout/commits/a7212d8a417b9d549cd5b5a7be13bc8831d5e9e0", "message": "[maven-release-plugin] rollback the release of mahout-0.12.2"}, {"url": "https://api.github.com/repos/apache/mahout/commits/8028859c46a7f65503ea5bb0682957d69355d83f", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/cbdac4f0b38e4afe87a9307cd0dd2a8fb0b1d3da", "message": "[maven-release-plugin] prepare release mahout-0.12.2"}, {"url": "https://api.github.com/repos/apache/mahout/commits/fc0697224e6f9e120b7ef237b6e8fe447c39b22d", "message": "Rolling back Mahout 0.12.2 Release candidate, thanks github connectivity issues"}, {"url": "https://api.github.com/repos/apache/mahout/commits/b50582f2ac39b4f2f2e09670428448466f6257fb", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/ae5fc27584e478da52d16f85ef7d634a3726aec5", "message": "[maven-release-plugin] prepare release mahout-0.12.2"}, {"url": "https://api.github.com/repos/apache/mahout/commits/1141a8d3cb47f6c1e3f4a44d9397e2aba7ab43a1", "message": "[maven-release-plugin] rollback the release of mahout-0.12.2"}, {"url": "https://api.github.com/repos/apache/mahout/commits/2803873c35b0ddc9c92c86d295edee60f52d8af1", "message": "[maven-release-plugin] prepare for next development iteration"}, {"url": "https://api.github.com/repos/apache/mahout/commits/5c7491544eb69f60d4800a7a1544d0f725439197", "message": "[maven-release-plugin] prepare release mahout-0.12.2"}, {"url": "https://api.github.com/repos/apache/mahout/commits/f81a909d0f5fdb10e7100b808e01ef4bf2096394", "message": "[maven-release-plugin] rollback the release of mahout-0.12.2"}, {"url": "https://api.github.com/repos/apache/mahout/commits/fe8e39acb1335e2caf24748424aa85c4085c57af", "message": "Rolling back Mahout 0.12.2 Release candidate, thanks github connectivity issues"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/mahout/comments/8585978", "body": "It's using hadoop 2 api and is not backward compatible hadoop 1.x. I can revert that back this weekend \n\nSent from my iPhone\n\n> On Nov 15, 2014, at 9:41 AM, Pat Ferrel notifications@github.com wrote:\n> \n> Building for hadoop 1.2.1 I get the following compile error:\n> \n> [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project mahout-integration: Compilation failure: Compilation failure:\n> [ERROR] /Users/pat/mahout/integration/src/main/java/org/apache/mahout/utils/ConcatenateVectorsJob.java:[116,73] cannot find symbol\n> [ERROR] symbol : method file(org.apache.hadoop.fs.Path)\n> [ERROR] location: class org.apache.hadoop.io.SequenceFile.Reader\n> [ERROR] /Users/pat/mahout/integration/src/main/java/org/apache/mahout/utils/ConcatenateVectorsJob.java:[116,16] internal error; cannot instantiate org.apache.hadoop.io.SequenceFile.Reader. at org.apache.hadoop.io.SequenceFile.Reader to ()\n> \n> Is this hadoop 2 dependant?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/8585978/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/comments/10708415", "body": "Change that to 'mahout-samsara'\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/10708415/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/comments/10724487", "body": "@sslavic  Revert the change and lets mark this Jira as 'Won't Fix'.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/10724487/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": [{"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27347138", "body": "this line can be moved on top of the if block, need not be repeated twice then \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27347138/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27347142", "body": "move this to top of if block, see previous comment\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27347142/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27347144", "body": "replace by new ArrayList<>()\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27347144/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27347130", "body": "Replace Lists.newArrayList() with new ArrayList<>(). Since we are now on Java 7. Also replace all similar lines in this code.  Same with Maps.newHashMap() to be replaced by new HashMap<>()\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27347130/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775196", "body": "Desist from using Guava collections if need be now that we r on Java 7.  This import can be dispensed with and replaced with java.util.ArrayList\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775196/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775274", "body": "u should commit with the same message as previous one, that way the same PR\nis updated\n\nOn Sun, Apr 5, 2015 at 2:35 AM, Anand Avati notifications@github.com\nwrote:\n\n> In\n> mr/src/test/java/org/apache/mahout/classifier/df/tools/VisualizerTest.java\n> https://github.com/apache/mahout/pull/105#discussion_r27775259:\n> \n> > @@ -17,9 +17,7 @@\n> > \n> >  package org.apache.mahout.classifier.df.tools;\n> > \n> > -import java.util.List;\n> > \n> > ## -import java.util.Random;\n> > \n> > +import com.google.common.collect.Lists;\n> \n> Not sure why this PR still says 2 commits, there should be 3 now. Can you\n> take the third commit manually?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/105/files#r27775259.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775274/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775370", "body": "U should be able to get rid of Closeables too, and replace that with Java 7 try - resources;  don't worry about that I'll take care of it. JFyi.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775370/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775419", "body": "Go ahead and do it if u like, I have a big change coming up which fixes\nsome of that stuff.\n\nOn Sun, Apr 5, 2015 at 3:12 AM, Anand Avati notifications@github.com\nwrote:\n\n> In\n> mr/src/main/java/org/apache/mahout/cf/taste/impl/similarity/precompute/MultithreadedBatchItemSimilarities.java\n> https://github.com/apache/mahout/pull/106#discussion_r27775411:\n> \n> > @@ -26,7 +26,7 @@\n> >  import java.util.concurrent.TimeUnit;\n> >  import java.util.concurrent.atomic.AtomicInteger;\n> > \n> > -import com.google.common.collect.Lists;\n> > +import java.util.ArrayList;\n> >  import com.google.common.io.Closeables;\n> \n> Argh, I blindly replaced only collections! I could have removed Closeables\n> as well, would be trivial. I'm assuming you will be doing it, based on your\n> previous comment.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/106/files#r27775411.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775419/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/31836245", "body": "See an 'final implicit' further down in a DQR.scala, should this be 'final implicit' too ?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/31836245/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/36580607", "body": "change this to be JDK 1.7 compliant, the project's on 1.7 since 0.10.0\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/36580607/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/42918937", "body": "mistake it should be eof, will fix that\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/42918937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/42921823", "body": "In the flink branch, I have it as [1.7, ). Keep it that way, the upper bound is open\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/42921823/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/36376711", "body": "Update this to 0.11.1-SNAPSHOT, since we will be releasing 0.11.0 this week\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/36376711/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/42565606", "body": "there is a Flink-scala-shell in the flink project. It should be available\nwith the flink project binaries.\n\nOn Tue, Oct 20, 2015 at 6:01 PM, Dmitriy Lyubimov notifications@github.com\nwrote:\n\n> In pom.xml\n> https://github.com/apache/mahout/pull/137#discussion_r42560808:\n> \n> > @@ -121,6 +121,8 @@\n> >      <scala.compat.version>2.10</scala.compat.version>\n> >      <scala.version>2.10.4</scala.version>\n> >      <spark.version>1.3.1</spark.version>\n> > -    <!-- TODO: Remove snapshot dependency when Flink 0.9.1 is released -->\n> > -    <flink.version>0.9-SNAPSHOT</flink.version>\n> \n> @alexey https://github.com/alexey : BTW is there such a thing as Flink\n> shell?\n> \u2026 <#15087459e636606c_>\n> On Tue, Oct 20, 2015 at 12:54 AM, Henry Saputra notifications@github.com\n> wrote: In pom.xml <#137 (comment)\n> https://github.com/apache/mahout/pull/137#discussion_r42465218>: > @@\n> -121,6 +121,8 @@ > <scala.compat.version>2.10</scala.compat.version> >\n> <scala.version>2.10.4</scala.version> >\n> <spark.version>1.3.1</spark.version> > + <!-- TODO: Remove snapshot\n> dependency when Flink 0.9.1 is released --> > +\n> <flink.version>0.9-SNAPSHOT</flink.version> Flink 0.9.1 is out so we could\n> remove the SNAPSHOT label I suppose. \u2014 Reply to this email directly or view\n> it on GitHub https://github.com/apache/mahout/pull/137/files#r42465218.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/137/files#r42560808.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/42565606/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/43394890", "body": "Given that this is a Spark upgrade release + Bug fixes, lets keep the release at 0.11.1-Snapshot\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/43394890/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "kunalcsc630": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/mahout/pulls/354", "title": "Fixed grammatical error", "body": "### Purpose of PR:\r\nPlease give a short description of what this PR is for.\r\n\r\n\r\n### Important ToDos\r\nPlease mark each with an \"x\"\r\n- [ ] A JIRA ticket exists (if not, please create this first)[https://issues.apache.org/jira/browse/ZEPPELIN/]\r\n- [ ] Title of PR is \"MAHOUT-XXXX Brief Description of Changes\" where XXXX is the JIRA number.\r\n- [ ] Created unit tests where appropriate\r\n- [ ] Added licenses correct on newly added files\r\n- [ ] Assigned JIRA to self\r\n- [ ] Added documentation in scala docs/java docs, and to website\r\n- [ ] Successfully built and ran all unit tests, verified that all tests pass locally.\r\n\r\nIf all of these things aren't complete, but you still feel it is\r\nappropriate to open a PR, please add [WIP] after MAHOUT-XXXX before the\r\ndescriptions- e.g. \"MAHOUT-XXXX [WIP] Description of Change\"\r\n\r\nDoes this change break earlier versions?\r\n\r\nIs this the beginning of a larger project for which a feature branch should be made?\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "davidtmiller": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/mahout/pulls/352", "title": "Mahout 1981 - Front End Design Update", "body": "### Purpose of PR:\r\nPlease give a short description of what this PR is for.\r\n\r\n\r\n### Important ToDos\r\nPlease mark each with an \"x\"\r\n- [ ] A JIRA ticket exists (if not, please create this first)[https://issues.apache.org/jira/browse/ZEPPELIN/]\r\n- [ ] Title of PR is \"MAHOUT-XXXX Brief Description of Changes\" where XXXX is the JIRA number.\r\n- [ ] Created unit tests where appropriate\r\n- [ ] Added licenses correct on newly added files\r\n- [ ] Assigned JIRA to self\r\n- [ ] Added documentation in scala docs/java docs, and to website\r\n- [ ] Successfully built and ran all unit tests, verified that all tests pass locally.\r\n\r\nIf all of these things aren't complete, but you still feel it is\r\nappropriate to open a PR, please add [WIP] after MAHOUT-XXXX before the\r\ndescriptions- e.g. \"MAHOUT-XXXX [WIP] Description of Change\"\r\n\r\nDoes this change break earlier versions?\r\n\r\nIs this the beginning of a larger project for which a feature branch should be made?\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "BruceKuiLiu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/mahout/pulls/346", "title": "Consider returning a zero length array rather than null.", "body": "It is often a better design to return a length zero array rather than a null reference to indicate that there are no results (i.e., an empty list of results).\r\nThis way, no explicit check for null is needed by clients of the method.\r\nOn the other hand, using null to indicate \"there is no answer to this question\" is probably appropriate.\r\nhttp://findbugs.sourceforge.net/bugDescriptions.html#PZLA_PREFER_ZERO_LENGTH_ARRAYS\r\n\r\n\r\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/345", "title": "Remove the redundant null check statements of a non-null value.", "body": "This IfStatement is a redundant check of a known non-null otherObj because the null check of otherObj is contained in the previous instanceof check.\r\nhttp://findbugs.sourceforge.net/bugDescriptions.html#RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE\r\n\r\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/344", "title": "Add an IfStatement to check the return value of resultFile.delete().", "body": "This statement returns a value that is not checked.\r\nThe return value should be checked since it can indicate an unusual or unexpected function execution.\r\nThe statement returns false if the file could not be successfully deleted (rather than throwing an Exception).\r\nIf the result was not checked, developers would not notice if the statement signals an unexpected behavior by returning an atypical return value.\r\nhttp://findbugs.sourceforge.net/bugDescriptions.html#RV_RETURN_VALUE_IGNORED_BAD_PRACTICE\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "holdenk": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/mahout/pulls/340", "title": "MAHOUT-2015 [WIP]: Expose Mahout's OLS algorithm in the Spark ML API", "body": "### Purpose of PR:\r\nExpose Mahout's OLS algorithm in the Spark ML API\r\n\r\n### Important ToDos\r\nPlease mark each with an \"x\"\r\n- [ X] A JIRA ticket exists (if not, please create this first)[https://issues.apache.org/jira/browse/ZEPPELIN/]\r\n- [ X ] Title of PR is \"MAHOUT-XXXX Brief Description of Changes\" where XXXX is the JIRA number.\r\n- [ X ] Created unit tests where appropriate\r\n- [ X ] Added licenses correct on newly added files\r\n- [ X ] Assigned JIRA to self\r\n- [ ] Added documentation in scala docs/java docs, and to website\r\n- [ ] Successfully built and ran all unit tests, verified that all tests pass locally.\r\n\r\nIf all of these things aren't complete, but you still feel it is\r\nappropriate to open a PR, please add [WIP] after MAHOUT-XXXX before the\r\ndescriptions- e.g. \"MAHOUT-XXXX [WIP] Description of Change\"\r\n\r\nDoes this change break earlier versions?\r\n\r\nNo\r\n\r\nIs this the beginning of a larger project for which a feature branch should be made?\r\n\r\nNo", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "AdityaAS": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/mahout/pulls/334", "title": "[WIP] MAHOUT-1991 - Add DBSCAN", "body": "DBSCAN - Google Summer of Code '17 - Apache Software Foundation (Apache Mahout)\r\n\r\nGoals: Implement DBSCAN algorithm\r\n\r\nWork done:\r\n- Coded Sequential and Distributed versions of DBSCAN\r\n- Added docs and unit tests\r\nWork left to do:\r\n- Improve distributed DBSCAN to make it more accurate\r\n- Include cluster quality index calculation\r\n\r\nAdded the sequential version of the algorithm. Docs and Unit Tests where appropriate.\r\nWill be adding rtree module and the approximate distributed algorithm soon.\r\n\r\nLink to a short report describing the whole project can be found [here](https://docs.google.com/document/d/11sLJkwAftR2-Fmglju8c7dGTf1i3trkbEV2Zr9OlilU/edit?usp=sharing).\r\n\r\n@rawkintrevo do help me out.\r\n\r\n### Important ToDos\r\nPlease mark each with an \"x\"\r\n- [x] A JIRA ticket exists (if not, please create this first)[https://issues.apache.org/jira/browse/ZEPPELIN/]\r\n- [x] Title of PR is \"MAHOUT-XXXX Brief Description of Changes\" where XXXX is the JIRA number.\r\n- [x] Created unit tests where appropriate\r\n- [x] Added licenses correct on newly added files\r\n- [x] Assigned JIRA to self\r\n- [x] Added documentation in scala docs/java docs, and to website\r\n- [x] Successfully built and ran all unit tests, verified that all tests pass locally.\r\n\r\nIf all of these things aren't complete, but you still feel it is\r\nappropriate to open a PR, please add [WIP] after MAHOUT-XXXX before the\r\ndescriptions- e.g. \"MAHOUT-XXXX [WIP] Description of Change\"\r\n\r\nDoes this change break earlier versions?\r\n\r\nIs this the beginning of a larger project for which a feature branch should be made?\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nsakharnykh": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/mahout/pulls/310", "title": "MAHOUT-1974 CUDA support", "body": "Initial PR for CUDA bindings support through JCuda", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "skanjila": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/mahout/pulls/279", "title": "Mahout 1904", "body": "Added a helper function to pass fail the method, first cut of this as an iteration to be reviewed.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/245", "title": "Mahout 1869", "body": "Added the ability to dump output to csv file\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/mahout/issues/comments/45238065", "body": "Ok I'll be doing a more substantial commit that might help us see how the APIs around a DataFrame\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45238065/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "BertrandDechoux": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/mahout/pulls/265", "title": "Add optional manual configuration of AtA's number of partitions.", "body": "```scala\r\n// Determine how many partitions the new matrix would need approximately. We base that on\r\n// geometry only, but it may eventually not be that adequate. Indeed, A'A tends to be much more\r\n// dense in reality than the source.\r\n```\r\nAllow override when estimation is not adequate because AtA is indeed more dense...", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "MaineC": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/mahout/pulls/240", "title": "Add one year mahout blog post draft.", "body": "The outline as offered by @smarthi with a bit of boiler plate. Needs some more love and content in the individual sections.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "manognavemulapati": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/mahout/pulls/181", "title": "An alternative scaling method for Baum Welch for HMM.", "body": "This implements an alternative scaling method (called rescaling) for Baum Welch algorithm for unsupervised HMM training. The new scaling method partially addresses Mahout-627. The existing scaling method  based on log scaling is not numerically stable when tried with the Mapreduce version of Baum Welch proposed for Mahout-627. With the rescaling method, I am able to successfully run the Mapreduce version of Baum Welch on long training sequences. Further details and references are given in the following writeup.\nImplementation of an Alternative Scaling for Baum-Welch Algorithm for Hidden Markov Model (HMM) in Apache Mahout -- http://manognavemulapati.github.io/mahout/HMMScaling.pdf.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "michellemay": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/mahout/pulls/174", "title": "MAHOUT-1786: Make classes implements Serializable for Spark 1.5+", "body": "Add some \"implements Serializable\" for Apache Spark 1.5+\nThere might be other classes that would benefit from the same modification.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sugaE": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/mahout/pulls/131", "title": "Fix bug: Add commit after executeUpdate", "body": "It will not write database without commit().\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ghost": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/mahout/pulls/70", "title": "Non-negative Matrix Factorization and Probabilistic Matrix Factorization", "body": "Non-negative Matrix Factorization, using classical multiplicative update rule.\nProbabilistic Matrix Factorization, using stochastic gradient descent\n\nUse Movielens dataset  to test, get reasonable result. \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mihaipitu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/mahout/pulls/35", "title": "Sparse Linear Methods (SLIM) Recommender with two optimization techniques", "body": "The SLIM algorithm generates efficient recommendations and its performance is shown in the original paper (http://glaros.dtc.umn.edu/gkhome/fetch/papers/SLIM2011icdm.pdf). The study demonstrates that SLIM outperforms traditional algorithms (such as itemkNN, userkNN, SVD or other Matrix Factorization approaches) on various data-sets in terms of time efficiency and recommendation quality.\r\nSLIM's optimization problem can be solved using Least Square optimization (designed for explicit feedback datasets) and Bayesian Personalized Ranking (designed for implicit feedback datasets).", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dlyubimov": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/mahout/issues/comments/44071171", "body": "ok i think this is more or less it for now, untill we do more API tweaking to fit \"multiple sink\" model of Stratosphere. But at least all currently working api is fully abstracted and moved out to math-scala module with 0 Spark (or Hadoop) dependencies. Please feel free to dig in and point out how i am being stupid :) \n\n:8ball: \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/44071171/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/44353426", "body": "Please include jira #.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/44353426/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/44496355", "body": "looks benign to me.\n\nNeed review from folks that were working on Hadoop 2 integration, to make sure this is in line with the rest of the effort. MAHOUT-1565\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/44496355/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/44713857", "body": "A slight nuisance here is that A' cannot be checkpointed. This is because the keys of A, in the most general case, are not `Int`s.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/44713857/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/44714458", "body": "perhaps a better test is needed that introduces some random jitter into the input.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/44714458/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45017407", "body": "Want to commit it now. 90% is bug fixes and refactoring. Added zip-optimization for identically distributed elementwise operators, etc.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45017407/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45156215", "body": "FunctionalView matrix introduced general concept of dense/sparse matrix. surprisingly, abstract matrix has none. So MatrixWritable just tries to look at a row view to figure this out. Perhaps the isDense functionality should be added to AbstractMatrix in the first place?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45156215/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45167847", "body": "do we need help committing this? Are we committing this?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45167847/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45236939", "body": "Ok, this is still is too little substance for review, and did not seem to generate much new ideas either. Plus i am probably will not be spending much time on it any time soon.\n\nWithdrawing request for the time being \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45236939/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45237262", "body": "I Like Pat's github avatar :+1: \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45237262/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45239528", "body": "i don't even know that cartoon, just thought is was funny. Yeah, thinking of it, it is how i used to feel most of the time looking at my colleagues' code at work \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45239528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45252464", "body": "this is work in progress. We get it :)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45252464/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45401402", "body": "also added (pretty naive) auto parallelism adjustments\n\n```\n(A+B) auto_||\n```\n\nwhich looks at spark.default.parallelism (P), assumes better parallelism is acheved if partitions are bumped up to round-ups of 0.95 \\* P or 1.80 \\* (P) whichever is closer. if current partition set is already greater (in cardinality) than rounded-up 1.80 \\* (P), all is left as is.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45401402/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45553612", "body": "i will commit it soon then\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45553612/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45557670", "body": "Pat, so, we are not going to use this for merging into merging, i take it? I will close it, you can keep working on your other requests.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45557670/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45565428", "body": "you can close -- but since i originated the PR, it is easier for me (I have\naccess to the \"close\" button on it while everyone else would have to use\n\"close apache/mahout#8\" commit to do the same.)\n\nOn Mon, Jun 9, 2014 at 5:20 PM, Pat Ferrel notifications@github.com wrote:\n\n> According to the instructions I merge from my branch anyway. I can close\n> it right? The instruction for closing without merging?\n> \n> I assume you got my mail about finding the blocker now there are some\n> questions about the cooccurrence algo itself.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/8#issuecomment-45560733.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45565428/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45645033", "body": "I'd rather hack finalization than create another variable.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45645033/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45646307", "body": "i assume this is current PR for MAHOUT-1464?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45646307/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45653266", "body": "Thanks Anand. Do you have jira # for changelog? Or, i guess, i can merge it without a changelog if you don't care.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45653266/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45654741", "body": "1529 is closed now. besides, it doesn't have anything to do with shell.\n\nit's fine this is a small change, i'll merge it without issue\n\nOn Tue, Jun 10, 2014 at 11:38 AM, Anand Avati notifications@github.com\nwrote:\n\n> I assumed this is part of MAHOUT-1529 itself (which renamed @sc\n> https://github.com/sc to @sdc https://github.com/sdc). Let me\n> resubmit with MAHOUT-1529 in the commit message?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/15#issuecomment-45653931.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45654741/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45784922", "body": "So what does performance comparison look like?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45784922/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45785235", "body": "Could somebody tell me if this is a good or bad idea, or just ugly DSL syntax.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45785235/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45831409", "body": "These are explicit and implicit tools that allow to address weak scalability and that Sebastian absolutely correctly complained about lack of thereof\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45831409/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45915940", "body": "fix header to say MAHOUT-1464, then hit close and reopen, it will restart the echo.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45915940/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46044957", "body": "I guess i am a bit conflicted. \n\nOn one hand, if abstraction contract says one thing, concrete implementation definitely must not do something else -- even if it means inefficiency. (If approximation is desired, the contract must be very explicit about it: \n\n```\n/**\n * Return the number of non zero elements in the vector.\n *\n * @return an int\n */\nint getNumNonZeroElements();\n```\n\nActually i've been using this for some time and have always been this is what it does -- reports the non-zero elements. In that sense, i am wholeheartedly support this patch since i firmly believe it is a better situation to be in compared to current state of things. (nothing can trump declared abstract contract -- if there's faster less than accurate alternative, just create another perhaps optional contract that explicitly says so). but you can't have a contract that says \"number of non-zero elements\" and return say 50 where true value of non-zero elements is 10. \n\nOn the other hand, it is very painfully enticing to implement that as an internal counter rather than a computation -- by controlling all assigment flows. Which might be origanizationally difficult. The analogy here is that say hash sets always know their cardinality without having to compute it.\n\nSo, i'd say, if count tracking is difficult (or viewed as more expensive option since it obviously adds load to modifications), then i'd say commit this.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46044957/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46057614", "body": "Bringing up a second PR from same branch. You really need just to rebase the changes over current master. This may not merge well.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46057614/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46356129", "body": "Ok, so are we ready to commit this?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46356129/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46358010", "body": "@Sebastian, you wanted this, can you take a look oif this serves the purpose? \n\n@tdunning, spark allows finer splits (minSplits parameter) on hdfs loading and on intermediate products. In our case, this patch adds minSplits parameter to fromHDFS() method to allow to specify finer-than-iniitial HDFS load parallelism explicitly. \n\nIn addition to that, parallelism can also be re-adjusted explicitly or automatically using the operators i mentioned. \n\nAutomatic parallellism adjustment uses current parallelism and spark cluster default parallelisms as guidelines (i.e. it assumes tasks are already of somewhat reasonable size, per above).\n\ni.g. loading matrix from hdfs \n\n```\nval a = drmFromHDFS(path, minsplits=100) \n```\n\nto make sure number of partitions (i.e. map tasks) is at least 100 on matrix load.\n\nThis mainly has to do with the fact that algebraic flops often grow asymptotically faster than the input size, so in some cases default cluster size is the best guideline in terms of load balancing.\n\n(in fact, since in Spark tasks are super cheap to run, i think it is ok to split 400%, 500% of default parallelism to achieve more even load). \n\nOptimizer computes (\"predicts\") parallelism of shuffles based on existing parallelism of products, but after a long chain of operations this predictions may deteriorate (or just not work well for whatever reason). in this case we may readjust it explicitly at any checkpoint of expression: \n\n```\n val drmAtA = (drmA.t %*% drmA) min_|| 100\n```\n\nwill make sure that this product will be shuffled into at least 100 partitions. Analogously, \n\n```\nval drmAtA = (drmA.t %*% drmA) exact_|| 100\n```\n\nwill make sure that this product will be shuffled into exactly 100 partitions. \n\nReadjusting parallelism also requires implicit optimizer checkpoint, e.g. the latter would be equivalent to \n\n```\nval drmAtA = (drmA.t %*% drmA).checkpoint(NONE) exact_|| 100\n```\n\nRepartitioning does not necessarily invoke shuffle task (e.g. if a partition can be just split into two, it can be run as a map-only thing). \n\nThis patch also does not contain optimizations causing potentially changing parallelism of any previous operations in the physical pipeline. This is something left todo, something to think about.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46358010/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46383849", "body": "It is a method. In scala, everything is a method and everything is an operator. There's no distinction. \n\nThe only question  is whether having `exact_||` is too ugly. I want them to be pithy and expressive,  10 letters max. I was entertaining hijacking `||` operator but it needs modifiers such as exact, min and auto. Hence. Suggestions how to spell it out exactly are welcome.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46383849/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46384178", "body": "actually, sorry, i take it back. In Scala every operator is also a method syntactically, but not necessarily the other way around.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46384178/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46384722", "body": "e.g. one can write things like `A.t.%*%(A).exact_||(100)`\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46384722/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46387080", "body": "This is now ready for algorithmical review. I've updated \"implementation working notes\" in the attached pdf https://issues.apache.org/jira/secure/attachment/12650938/distributed-als-with-confidence.pdf\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46387080/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46398143", "body": "Ted, are you ready to help with a concrete alternative? This is a very\nsmall issue compared to even the patch, lets build a list of alternatives\nand vote. But lets get it done\n\nMy additional variants\n\nminSplits,...\nminPar, exactPar, autoPar (consitent with scala's collection.par())\n\nTo give something to vote down for Ted\n\n> =|| :=||\n> :||=\n\nNot ok with me\n\nminParts\nminParallelism\nminPartitions\nrepartition\nreshuffle\nand other do-something kind\n\nYour variants--?\nOn Jun 17, 2014 9:59 PM, \"Ted Dunning\" notifications@github.com wrote:\n\n> Yes.\n> \n> But I was talking about the gratuitous use of non-alpha characters.\n> Excessive use of operator overloading is also a bit of a problem.\n> \n> Just because you can doesn't mean you should.\n> \n> Sent from my iPhone\n> \n> > On Jun 17, 2014, at 17:58, Dmitriy Lyubimov notifications@github.com\n> > wrote:\n> > \n> > e.g. one can write things like A.t.%*%(A).exact_||(100)\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/13#issuecomment-46396097.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46398143/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46455493", "body": "Another variation \n\n```\ndrmA.par(min=100)\ndrmA.par(exact=100)\n```\n\nthat would be roughly consistent with parallel collections in Scala.\n\nthe minSplits(100), exactSplits(100) is a naming consistent with Spark i guess.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46455493/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46489240", "body": "Thanks.\n\n+1 on par(...) too. Great, i will engineer the fix and commit then.\n\nOn Wed, Jun 18, 2014 at 1:20 PM, Sebastian notifications@github.com wrote:\n\n> I dont like the _|| because it looks like a mathematical operation and is\n> not intuitive IMHO. I prefer something that looks like an annotation to the\n> code and is human readable. So I vote for the .par() syntax\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/13#issuecomment-46488559.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46489240/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46510934", "body": "Sorry Ted. i was a bit in a hurry with this. \n\nIn my defense i can only state that I already asked you for concrete spelling suggestions before you just answered that  I shouldn't use operators. So i assumed that was all i could get in terms of concrete spellings and this decision satisfied your criteria.\n\nBesides, we got two +2 on par(). (which is consistent with Scala collections).\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46510934/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46512881", "body": "Could i get a \"nay\" or \"yay\" on this please?\n\n(\"Nays\" must be accompanied with an alternative per Apache. )\n\nthanks.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46512881/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46606053", "body": "General note: a lot of style problems. \nCode lines not to exceed 120 charactes (i think i saw some suspiciously long). \n\nDefinitely lack of comments. \n\nFYI Spark comment style is the following: every comment starts with a captial letter and is formatted to cut off at 100th character. And they are very draconian about it. And that's what i followed here as well. \n\nWell the 100th character is questionable since it cannot be auto formatted in IDEA, but I do believe comments do need some justification applied on the right. \n\nfor closure stacks, i'd suggest the following comment /etc style\n\n```\nval b = A\n\n   // I want to map\n  .map { tuple => \n    ....\n  }\n\n  // I want to filter \n  .filter { tuple => \n   .... \n  }\n```\n\nSo it would be useful to state in plain words  what closure is to accomplish, since they are functional units, i.e. function-grade citizens, and as such, imo deserve some explanation.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46606053/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48766569", "body": "Look at #28. Just spent 30 mins doing quick refactoring, should help you with test independence. Every  engine should run some common asserts which are included in the `*SuiteBase` traits\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48766569/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48769459", "body": "@pferrel  (in case you are talking to me) sorry don't have time to read the whole discussion. if you can point me to concrete places in the code  what you think is needed to be done and why, i may be able to try to figure it. But as for as h20 issue, independent tests have nothing really new that @avati hasn't already done (except he cut-and-pasted them, and now he needs just to remove all cut-and-paste and just pull in a trait form math-scala).\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48769459/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48950240", "body": "@avati I take it you are saying it is useful  for #21 . So i will merge it then. I guess i'd like to make sure it is not overly annoyingly screwing something for somebody else (@pferrel?) first; but oh well. if it does, i can always issue a revert.\n\nre: rebasing: i am not a big fun of rebasing as it would screw further pulls for anyone who dared to copy (and especially, work off)  your public branch. if you just merge, imo it would be totally ok -- but it's up to you.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48950240/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49657713", "body": "sent a PR against your branch with edits. If you merge please, my commit will appear in this PR. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49657713/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49668725", "body": "I am (obviously) ok with this commit. Perhaps it is worth to see if there are more bugs or pitfals with this, i obviously did not do a due diligence. Maybe add a few more tests doing stuff with non-int keys.\n\nit also (obviously) does not define behavior for cases of duplicate non-int keys, but i guess we can iron out these corner cases as we go if that's indeed a problem. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49668725/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49670252", "body": "Agree with @avati, without looking at much else, NB definitely doesn't belong in `package org.apache.mahout.sparkbindings.drm.classification`. A better root perhaps is simply `o.a.m.classification`, even if it clashes with some legacy (java) packages ?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49670252/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49670601", "body": "looks fine to me.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49670601/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49672673", "body": "Can't make sense of this PR. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49672673/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49679569", "body": "Could the author please either provide info what it is about or close this request if it was done by mistake? thanks.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49679569/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49680647", "body": "will it merge with rbind() code?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49680647/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49681458", "body": "```\nrbind() is not yet added, I wasn't even sure if the DRM api would be accepted before I\nimplemented for H2O. I plan to submit a separate PR for rbind().\n```\n\nok. contingent on this promise, +1 on merging.\n\ngiven magnitude of this review, i suggest 2 more votes/reviewers. Additional non-binding reviews/sign-offs from 0xdata members are also IMO desirable.\n\nAnd IMO we need to resolve whatever concerns Pat may have with this PR. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49681458/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49682101", "body": "I would like @pferrel to sign off here \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49682101/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49685224", "body": "On Mon, Jul 21, 2014 at 5:31 PM, Pat Ferrel notifications@github.com\nwrote:\n\n> Call me the loyal opposition. I'd rather merge math with h2o than h2o with\n> Mahout but will bow to the majority and I count the vote at 2 to one (me).\n> \n> is it +0? http://www.apache.org/foundation/voting.html\n\nThere is a subtle danger that introducing new DRMLike operations will then\nrequire H20 symmetric implementation. So if there's a lot still expected,\ni'd say -0 is validated. It is important that _you_ tell us that, because\nas it stands you are the only one working on a method at the moment. (well\ni do some internally as well, but my additions are strictly minor, i don't\nneed anything earth shattering).\n\n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/21#issuecomment-49684400.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49685224/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49802148", "body": "Lazy evaluation.\ni.e. if element-wise scalar execution is not put into physical plan, then fix will never be evaluated.\nSimilarly probably could be fixed in other conditions.\nAlso should survive \"masking\" stuff (such as mapBlock() or other unary operators in between source rdd and elementwise scalar).\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49802148/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49803586", "body": "@avati Anand, i would like to try and squeeze #33 ahead of this. \n\n#33 changes DrmLike by adding a new lazy evaluation to plans (canHaveMissingRows) to track potentially missing row condition thruout DAGs if it ever was (lazily) detected in the original sources.\n\nit also fixes A+1 case on spark side. Spark side is fairly agnostic of other engines, it really up to them if they would allow missing implied rows or not. Spark engine chooses to allow that and perform lazy evaluation whenever required.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49803586/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49807387", "body": "Also fixes `A + B` with missing rows. \n\nWhat else there was?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49807387/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49808110", "body": "@avati no i don't think so. This is similar to quick summaries of nrow, ncol and these need to be known before RDD chain is constructed (e.g. at op rewriting stage, perhaps). \n\nIt may be viewed as an architectural problem as we don't very explicitly define separation between physical operators and logical (or, rather, every logical operator is also physical, although inverse is false). So DAG plans should have private[mahout] collection of properties that help logical rewrites. \n\nMissing rows should be pertinent to other engines as well as we ask them to support DRM over HDFS ( drmLoadFromHDFS method), and in persistent form DRM may have missing implied rows regardless of the engine. The engine, subsequently, may choose to fix it eagerly or lazily -- but it doesn't change the fact that DRMs in Mahout historically may have missing implied rows, as coming out from vectorizers, there's no agreement to the contrary AFAICT.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49808110/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49809300", "body": "the point is there has to be diagnostics propagating thru DAG to help rewrites based on initial quick summaries of the checkpoints. \n\nE.g. in initial version there was also a non-zero element estimate that (I hoped) would help optimizer to take certain rewrite decisions. I removed it as it was too much for the first step. But if cost optimizations ever become a larger part of this, operators need to be able to carry optimizer specific information . \n\nRDDInput stuff on the other hand is merely an \"either/or\" rdd (either blockifed or row-wise). As such it is the sole final product of the optimizer. Result can't carry helper information pertinent to building result tree, I feel that would be a bad optimizer architecture.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49809300/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49809684", "body": "Like i said, abstraction for optimizer-related information might be better (and perhaps the one that can carry engine-specific parts), but these attribute bags must be on the dag itself.\n\nI also see that there's a big chance this will be a factor for Flink integration as well.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49809684/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49810810", "body": "Hm. CBind test over implied rows works for me. Can't seem to reproduce.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49810810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49813626", "body": "yeah these codes (`a ew b` fix and CBind post-cogrouping) are doing identical thing although just slightly differently. Not sure which one would be more efficient at this point. Probably neither. :)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49813626/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49813755", "body": "some of the byte code classes are of higher version than the JRE you are\nrunning on. I suppose, yes, there are some 1.7 specific dependency jars\nthere\n\nOn Tue, Jul 22, 2014 at 4:07 PM, Andrew Palumbo notifications@github.com\nwrote:\n\n> Hi @avati https://github.com/avati, is there something Java 1.7\n> specific in the dependencies here? I'm getting a test failure in the h2o\n> module:\n> \n> Discovery starting.\n> **\\* RUN ABORTED ***\n> java.lang.UnsupportedClassVersionError: water/MRTask : Unsupported\n> major.minor version 51.0\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/21#issuecomment-49813616.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49813755/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50554340", "body": "As i indicated, i am waiting on 2 more votes.  We had what i have no choice\nbut interpret as +0 from Pat, and +1 from me.\n\nOn Tue, Jul 29, 2014 at 4:24 PM, Anand Avati notifications@github.com\nwrote:\n\n> @dlyubimov https://github.com/dlyubimov Thanks for merging #30\n> https://github.com/apache/mahout/pull/30. I have now added Rbind\n> operator and refreshed the PR. All tests are passing. Let me know if this\n> is sufficient for merge.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/21#issuecomment-50553289.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50554340/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50654169", "body": "(1) No review from most vocal backers? \n\n(2) m-1500 is unassigned. Whoever wishes to commit this issue, please take over m-1500 and continue. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50654169/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50797941", "body": "there used to be a guide on old website... but i guess it got axed. it\nbasically said it is Sun style + 120 character line width constraint. I\nthink there also were templates for Eclipse somewhere.\n\nThere's also checkstyle maven plugin tuned up to report those (i remember\npeople were making me to run it and make sure every one of those went away)\nbut my take the checkstyle currently is tuned very aggressively. My minimum\nhygiene thing is line width, indentations, naming convention and operator\nspacing (out of which, IDE can take care of everything). With Sean's\ndeparture, out standards on style are nowhere where they used to be.\n\nin scala, style is still evolving, but I pushed a few things in particular.\nBaseline is to follow the Spark code style (and btw they are very strict\nabout it; e..g they insist that i write comments which are starting with\ncapital letter and aligned at 100th character on the right, which IDE\ncannot do automatically). We also were discussing closure styles elsewhere.\n\nOn Thu, Jul 31, 2014 at 11:04 AM, Anand Avati notifications@github.com\nwrote:\n\n> Is there a code style doc used by mahout? I dont use idea (just emacs)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/21#issuecomment-50795818.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50797941/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51274951", "body": "itemsimilarity driver stuff is failing on this. \n\nItemSimilarityDriverSuite:\n113754 [ScalaTest-main-running-ItemSimilarityDriverSuite] DEBUG org.apache.mahout.sparkbindings.blas.AtA$  - Applying slim A'A.\n114171 [ScalaTest-main-running-ItemSimilarityDriverSuite] DEBUG org.apache.mahout.sparkbindings.blas.AtB$  - A and B for A'B are not identically partitioned, performing inner join.\n- ItemSimilarityDriver, non-full-spec CSV **\\* FAILED ***\n  Set(iphone    galaxy:1.7260924347106847,iphone:1.7260924347106847,ipad:0.6795961471815897,nexus:0.6795961471815897, surface   surface:4.498681156950466, nexus        iphone:1.7260924347106847,ipad:0.6795961471815897,surface:0.6795961471815897,nexus:0.6795961471815897,galaxy:1.7260924347106847, ipad   galaxy:1.7260924347106847,iphone:1.7260924347106847,ipad:0.6795961471815897,nexus:0.6795961471815897, galaxy    galaxy:1.7260924347106847,iphone:1.7260924347106847,ipad:0.6795961471815897,nexus:0.6795961471815897) did not equal Set(nexus   nexus:0.6795961471815897,iphone:1.7260924347106847,ipad:0.6795961471815897,surface:0.6795961471815897,galaxy:1.7260924347106847, ipad   nexus:0.6795961471815897,iphone:1.7260924347106847,ipad:0.6795961471815897,galaxy:1.7260924347106847, surface   surface:4.498681156950466, iphone       nexus:0.6795961471815897,iphone:1.7260924347106847,ipad:0.6795961471815897,galaxy:1.7260924347106847, galaxy    nexus:0.6795961471815897,iphone:1.7260924347106847,ipad:0.6795961471815897,galaxy:1.7260924347106847) (ItemSimilarityDriverSuite.scala:142)  \n\nthe rest seems to pass\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51274951/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51276205", "body": "also, tests run much slower although cpu remains unsaturated. Something about setting up and tearing down local spark context ???\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51276205/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51389335", "body": "@pferrel perhaps you could look at ItemSimilaritySuite, it doesn't work on spark 1.0 here? I disabled the tests for now since they are failing.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51389335/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51404288", "body": "So, can we move the package please?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51404288/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51406631", "body": "@andrewpalumbo can you perhaps comment on the code itself so we see what you are talking about?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51406631/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51409540", "body": "On Wed, Aug 6, 2014 at 3:55 PM, Pat Ferrel notifications@github.com wrote:\n\n> Sorry was off the internet during a move (curse you giant nameless cable\n> company!)\n> \n> Anyway these tests are substantially changed in #36\n> https://github.com/apache/mahout/pull/36 but I haven't been able to get\n> the new build until now, will check and push 36 first.\n> \n> As to building and tearing down contexts I'm not helping things. For each\n> driver test DistributedSparkSuite in the beforeEach creates a context so I\n> use that to start the test. Then the driver I am using needs to start a\n> context so for every time I call a driver I precede it with the \"afterEach\"\n> call to shut down the context. Then call the driver, then call \"beforeEach\"\n> to restore the test context. I also had to tell the driver in a special\n> invisible option not to load Mahout jars with a \"--dontAddMahoutJars\". So\n> the context is being built 3 times for every test. but that hasn't changed,\n> it's always been that way.\n> \n> We could reuse a single context per test but it would require disabling\n> some stuff in the driver along the lines of what I had to do with\n> \"--dontAddMahoutJars\". Since I've already had to do this I don't think it\n> would be a big deal to disable a little more. I'll look at it once 36 is\n> pushed.\n> \n> Is there any reason to build the context more than once per suite?\n> \n> Usually, there's not and that's exactly what this branch is moving towards\n> (note: this PR is not against master but to  to a side branch called\n> `spark-1.0.x`).\n> Also that's what they seem to have done in Spark 1.0 as well.\n\nThere are sometimes (in my other projects) a need to create a custom\ncontext but not in Mahout codebase.\n\n> Seems like if I disable the context things in the driver we could run all\n> tests in a single context, right?\n> \n> Right. This branch has already switched to doing that. All algebra tests\n> seem to be fine but these tests are failing now. not sure why. seems\n> functional to me.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/40#issuecomment-51408987.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51409540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51410605", "body": "> OK so DistributedSparkSuite moved the create context into the beforeAll?\n\non this branch, yes.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51410605/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51415419", "body": "On Wed, Aug 6, 2014 at 4:56 PM, Pat Ferrel notifications@github.com wrote:\n\n> Do you want to push this with the \"ignore\"s and I'll fix them to use the\n> new DistributedSparkSuite as it gets into the master?\n\nNo i probably don't want ot merge it with non-working tests. As usual, i\ncan add you as collaborator in my account (if i have not yet done so) so\nyou could push directly to my source branch of this (so it reflects in the\nPR instantaniously) or you can PR against my spark 1.0.x branch, or you can\njust send me a regular git patch with email, whichever works.\n\n> BTW any reason we aren't doing Scala 2.11 since we are upping to Java 7\n> and Spark 1?\n\nThe reason Scala is fixed where it is fixed is because it is paired to\nSpark's version of Scala. Migration between major versions of Scala is a\nbig deal, for Spark and otherwise. Stuff will not work. Minor version of\nScala should be generally portable.\n\n>  \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/40#issuecomment-51413783.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51415419/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51415624", "body": "sure. there're tons of stuff in progress but we can only use released\nartifact as dependencies.\n\nOn Wed, Aug 6, 2014 at 5:19 PM, Anand Avati notifications@github.com\nwrote:\n\n> Scala 2.11 port of Spark is in progress [\n> https://issues.apache.org/jira/browse/SPARK-1812]\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/40#issuecomment-51415479.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51415624/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51416019", "body": "alternatively, you can also just give me a verbal hint what i need to fix,\nand i can try to patch to the best of my ability.\n\nOn Wed, Aug 6, 2014 at 5:18 PM, Dmitriy Lyubimov dlieu.7@gmail.com wrote:\n\n> On Wed, Aug 6, 2014 at 4:56 PM, Pat Ferrel notifications@github.com\n> wrote:\n> \n> > Do you want to push this with the \"ignore\"s and I'll fix them to use the\n> > new DistributedSparkSuite as it gets into the master?\n> \n> No i probably don't want ot merge it with non-working tests. As usual, i\n> can add you as collaborator in my account (if i have not yet done so) so\n> you could push directly to my source branch of this (so it reflects in the\n> PR instantaniously) or you can PR against my spark 1.0.x branch, or you can\n> just send me a regular git patch with email, whichever works.\n> \n> > BTW any reason we aren't doing Scala 2.11 since we are upping to Java 7\n> > and Spark 1?\n> \n> The reason Scala is fixed where it is fixed is because it is paired to\n> Spark's version of Scala. Migration between major versions of Scala is a\n> big deal, for Spark and otherwise. Stuff will not work. Minor version of\n> Scala should be generally portable.\n> \n> >  \u2014\n> > Reply to this email directly or view it on GitHub\n> > https://github.com/apache/mahout/pull/40#issuecomment-51413783.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51416019/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51515920", "body": "@pferrel where are the changes?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51515920/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51518645", "body": "assuming you are on your local branch named spark-1.0.x with your commit on\ntop of mine current head, please execute\n     push git@github.com:dlyubimov/mahout spark-1.0.x\n\nthis should go thru\n\nOn Thu, Aug 7, 2014 at 12:13 PM, Pat Ferrel notifications@github.com\nwrote:\n\n> I can't push them back to you so they are here\n> https://github.com/pferrel/mahout/tree/spark-1.0.x\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/40#issuecomment-51517989.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51518645/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51518705", "body": "oh ok, never mind\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51518705/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51523570", "body": "ok i guess like you said tests are still failing\n\nOn Thu, Aug 7, 2014 at 12:18 PM, Dmitriy Lyubimov dlieu.7@gmail.com wrote:\n\n> assuming you are on your local branch named spark-1.0.x with your commit\n> on top of mine current head, please execute\n>      push git@github.com:dlyubimov/mahout spark-1.0.x\n> \n> this should go thru\n> \n> On Thu, Aug 7, 2014 at 12:13 PM, Pat Ferrel notifications@github.com\n> wrote:\n> \n> > I can't push them back to you so they are here\n> > https://github.com/pferrel/mahout/tree/spark-1.0.x\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub\n> > https://github.com/apache/mahout/pull/40#issuecomment-51517989.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51523570/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51525329", "body": "@pferrel: So one problem with those tests is that they are creating 2 spark\nsessions. 1 session is created by tests and another session is created by\ndriver.\n\nSpark is very strict with this:\n\n(1) Spark is not reentrant w.r.t. session creation are non-reentrant (not\njust thread-unsafe) -- meaning you can only safely have at most 1 session\nat a time in a jvm.\n(2) Spark session itself is reentrant -- meaning multiple threads may\ninvoke asynchronous computational actions on the same session.\n\nThis may not always manifest, but in the end it always will (ask me how i\nknow :)\n\nso the problem with those tests is that they probably must not be featuring\nDistributedSparkSuite but rather just MahoutSuite. Or alternatively you may\npass an already existing mahoutContext to the driver code for reuse. But\nyou must ensure the above constraint. The effects will range dramatically\nif not (from mislabeled rdd partitions in the block manager to   lockups\nand internal race conditions)\n\nOn Thu, Aug 7, 2014 at 12:59 PM, Dmitriy Lyubimov dlieu.7@gmail.com wrote:\n\n> ok i guess like you said tests are still failing\n> \n> On Thu, Aug 7, 2014 at 12:18 PM, Dmitriy Lyubimov dlieu.7@gmail.com\n> wrote:\n> \n> > assuming you are on your local branch named spark-1.0.x with your commit\n> > on top of mine current head, please execute\n> >      push git@github.com:dlyubimov/mahout spark-1.0.x\n> > \n> > this should go thru\n> > \n> > On Thu, Aug 7, 2014 at 12:13 PM, Pat Ferrel notifications@github.com\n> > wrote:\n> > \n> > > I can't push them back to you so they are here\n> > > https://github.com/pferrel/mahout/tree/spark-1.0.x\n> > > \n> > > \u2014\n> > > Reply to this email directly or view it on GitHub\n> > > https://github.com/apache/mahout/pull/40#issuecomment-51517989.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51525329/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51526214", "body": "Btw your handling of temporary directory is quite to the point, you may quite possibly make it part of MahoutSuite. Then i have one other place that may use it. Also see similar code in MahoutTest java class for JUnit -- we could just use that i guess.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51526214/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51642800", "body": "excellent. seems to be working for me. \n\nlet me squash it and merge to apache/mahout spark_1.0.x. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51642800/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51663705", "body": "I think the patch went out of sync with current master pretty badly. may require merge, optionally with `-Xtheirs` if conflicts become too ugly.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51663705/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51671505", "body": "Looks much -much cleaner, thank you Andrew!.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51671505/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51671975", "body": "ok i suppose since the test passes we can merge that in now, and tweak any problems later, in order not to have to handle any more conflicts...\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51671975/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51995734", "body": "ship it\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51995734/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53745490", "body": "> may still have the commit bit for ASF git, but can't merge the pull request myself\n> Thanks, Sean. \n\nYes, you can merge. A bit exploded beyond what's needed IMO but still useful [1]. Also, it works best if master is first merged to the PR branch and conflicts, if any, resolved there, so when you `--squash` stuff to master, you don't have to worry about conflicts on top of everything else. Hope this helps.\n\n[1] http://mahout.apache.org/developers/github.html\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53745490/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53748236", "body": "I meant the PR doc. Strictly in my opinion, since original versions  stuff\nhas been added that was not strictly to the point and makes read longer and\ntherefore harder than it needs be.\n\nOn Thu, Aug 28, 2014 at 9:14 AM, Sean Owen notifications@github.com wrote:\n\n> Ah right, should have RTFM. Thanks! When you say \"beyond what's needed\"\n> were you commenting on the PR, or on the docs? Just checking whether you\n> meant you wanted to discuss the change more.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/46#issuecomment-53746682.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53748236/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53942318", "body": "ok i thought we already committed that? sorry lost track of stuff here\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53942318/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53942389", "body": "i am all for it, but i think it would be nice to do rule and cost-based decision tree for matrix operands first (something perhaps similar to what has been done to vector-vector aggregate() and assign())\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53942389/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53942759", "body": "`\u001b[32m- dssvd - the naive-est - q=0\u001b[0m\n1 [Executor task launch worker-1] ERROR org.apache.spark.executor.Executor  - Exception in task ID 52\njava.io.FileNotFoundException: http://67.195.81.155:38091/broadcast_4\n    at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1457)\n    at org.apache.spark.broadcast.HttpBroadcast$.read(HttpBroadcast.scala:196)\n    at org.apache.spark.broadcast.HttpBroadcast.readObject(HttpBroadcast.scala:89)\n    at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)`\n\nnot sure what may be causing this.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53942759/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/54686571", "body": "is this abandoned?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/54686571/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/54692207", "body": "No, do what you think is right... just was checking.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/54692207/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55169199", "body": "Anand,\n\nRobin Anil was the principal architect of vector-vector binary op\nperformance. I need to dig in email archives (deep, heh, ) but there was a\nnice TDD on google doc on their approach.\n\nfor most part, this was relying on 2 estimates: (1) interrogating vectors\non asymptotic cost of random element access vs. sequential non-zero element access;\nand (2) interrogating operation whether it is plus-like or *-like (key\ndifference between elementwise + and elementwise \\* is that one needs\niterating on zip of either-non-zeros, and the other only needs to iterate\non zip of both-non-zeros only.\n\nI will try to dig for that doc (i was reviewing it several times in the\npast) if it is still available.\n\nMy understandign (and criticism) of this solution is that it only accounts\nfor asymptotic cost rather than actual cost. In reality computing O(n) cost\nis probably fairly naive, as you probably would rather make an estimate\nalong the lines A + B*O(n) and learning A and B via a calibration routine.\nNot sure how much pure asymptotical cost assumption is really detrimental\nin reality.\n\nIn terms of matrices i was thinking important factors to interrogate are\nvector/hash backing; and in case of vector backing, the vector orientation\n(column-wise, row-wise). That should be enough to choose algos from, at\nleast those written by Ted. Obviously view stuff such as transposed view\nshould flip those as needed.\n\nIf one of operands are dense JBlas or JCuda, i guess it also needs to\nassess the cost of converting the other operand into native JBlas and JCuda\n(perhaps using calibration learning, too).\n\nOn Wed, Sep 10, 2014 at 12:09 PM, Anand Avati notifications@github.com\nwrote:\n\n> @dlyubimov https://github.com/dlyubimov - I am digging into the project\n> history to understand more about the rule/cost framework and what has been\n> done for aggregate/assign for vector-vector. Any pointers for more context\n> in this direction appreciated.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/44#issuecomment-55166855.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55169199/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55169986", "body": "Aha. here is the stuff \nhttps://issues.apache.org/jira/browse/MAHOUT-1202\nhttps://docs.google.com/document/d/1g1PjUuvjyh2LBdq2_rKLIcUiDbeOORA1sCJiSsz-JVU/edit#heading=h.koi571fvwha3\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55169986/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55172550", "body": "yes it is\n\nOn Wed, Sep 10, 2014 at 12:31 PM, Anand Avati notifications@github.com\nwrote:\n\n> Is this the document -\n> \n> https://docs.google.com/document/d/1g1PjUuvjyh2LBdq2_rKLIcUiDbeOORA1sCJiSsz-JVU/edit#\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/44#issuecomment-55169860.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55172550/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55179455", "body": "Another thing about vector-vector operations is that it would be nice to restructure it so that vector-vector cost assessment and algorithm selection happens only once for a batch of similar vector-vector operations, which is definitely the case for vector-backed matrix structures\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55179455/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55198016", "body": "that's probably true. they may make cost a function of number of non-zeros in each particular vector.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55198016/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55550711", "body": "@andrewpalumbo I have to disagree with this patch. drmFromHDFS actually does exactly what it is supposed to. Here is the test script (assuming you are running in local mode you should see mappers outputs directly to your console and see distinct keys there.: \n\n```\nval drmA = drmParallelizeEmpty(30, 40)\n\n val drmB = drmA.mapBlock() { case (keys, block) =>\n    keys.map { key => s\"key-${key}\"} -> block\n }\n\n\n // in local mode we can see printouts to console so we\n // can check if the keys are actually there as strings\n\n drmB.mapBlock() { case (keys,block) =>\n   keys.map(println)\n   keys -> block\n }.collect\n\n // save\n drmB.writeDRM(\"B-with-test-keys\")\n\n // load back\n val drmC = drmFromHDFS(\"B-with-test-keys\")\n\n // check the keys are still text there\n drmB.mapBlock() { case (keys,block) =>\n   keys.map(println)\n   keys -> block\n }.collect\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55550711/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55551176", "body": "Oops. the script has error. sorry i do see some duplicates in drm C\n\nval inCoreC = drmC.mapBlock() { case (keys,block) =>\n  keys.map(println)\n  keys -> block\n}.collect\n\nkey-2\nkey-2\nkey-2\nkey-17\nkey-17\nkey-17\nkey-29\nkey-29\nkey-29\nkey-5\nkey-5\nkey-5\nkey-8\nkey-8\nkey-8\nkey-20\nkey-20\nkey-20\nkey-23\nkey-23\nkey-23\nkey-14\nkey-14\nkey-14\nkey-11\nkey-11\nkey-11\nkey-26\nkey-26\nkey-26\n\nI did check row bindings on drmB and they are correct, but after save-reload cycle they are no more correct. Which means collect() is not the reason, it is either save or drmFromHdfs. I still think the patch is kludgy and there has to be a simpler way to fix this though. Let me consider this for a moment. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55551176/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55552838", "body": "yeah i dont think cloning is needed, since we unwrapping the actual object (string, int etc). So by the time it is cached, it is a collection of ints or stings but not writables. So what Matei is saying there, should not already be applicable.\n\nMore likely, i was thinking, spark is creating a strict collection when it fuses two maps, so more likely fix would be simply removing extra map, something along the lines \n\n```\n val rdd = sc.sequenceFile(path, classOf[Writable], classOf[VectorWritable], minPartitions = parMin)\n\n... \n\n  val drmRdd = rdd.map { t => key2valFunc(t._1) -> t._2.get}\n...\n```\n\n(so there's no forced map fusion). But it doesn't seem to work in my tests either.  Weird.\n\nOk i think i want to stop spending my neurons on this for now and  to have a look at it tomorrow again. \n\nBut i don't think we should clone anything. we just need to transform away from writable before attempting any caches.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55552838/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55552882", "body": "Actually i am  starting to think that caching to \"memory only\" is also a mistake here. If people want to cache immediately after load, they can do it with drm.checkpoint(), i am not sure why it should be trying to cache it right away. i'd remove that line as well.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55552882/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55553012", "body": "Also it puzzles me that the test `DRM DFS i/o (local)` in `DrmLikeSuiteBase` should have been catching this, or modified to catch this. but it wasn't catching it, most likely because by default matrices parallelize into 10 partitions, so in this case any partition would have at most 1 row, at which point Writable problem of course will not manifest. \n\nThat's what i'd start with, modify this test to reproduce this.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55553012/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55554028", "body": "ok yes i know why it is. Spark doesn't of course read the header file and set proper class tag evidence on the loaded rdd. hence, none of the conversions happen and drm is loaded as simply DrmLike[Writable]. which is of course not what should ideally happen since information about key type is already avalable in the sequence file header. \n\nThis would require some thinking since correct conversion away from writable to concrete key type does need to happen. \n\nbummer.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55554028/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55627685", "body": "Well, there are three solutions to the problem I see. One is easy, another is pretty easy but perhaps more expensive operationally.\n\nFirst, let's clarify what is going on there. \n\nsince the key type is having a ClassTag as context bound: DrmLike type is defined as DrmLike[K:ClassTag], it means that returning CheckpointedDrm[_](which is subtype) from drmFromHDFS in fact implicitly means returning two things: CheckpointedDrm and evidence:ContextTag[K]. The evidence is essentially a run time class information (including all generics hierarchies possibly attached to it).\n\nThinking is, since key class name is stored in the sequence file headers, it makes sense to expect for this routine to read out the header and infer ContextTag[K] on its own. As comment indicates there, it actually expected Spark to do that. But simple examination of sequenceFile() reveals that Spark actually does not do that. Instead, it is just propagates the evidence it already got when this method is called, and doesn't read any headers (or anything for that matter) until actual execution action occurs. As a result, intended conversion from writable to its wrapped type (String, Int or Long) does not really happen as it stands; instead, Writable is just propagated up the food chain, and we of course already know that writable instances are reused and would not make it thru strict Scala collections. \n\nSo, based on that, there are three ways to address that (that i see). \n\n(1) One, simple way, is to escape this problem the same way Spark has escaped it: namely, require actual key type evidence from the user. That would require changing method signature to \n\n```\ndef drmFromHDFS[K:ClassTag] (path: String, parMin:Int = 0)(implicit sc: DistributedContext): CheckpointedDrm[K] \n```\n\nThen you'd have to implicitly supply evidence by doing something like this:\n\n```\nval drmA:DrmLike[String] = drmFromHdfs(...)\n```\n\nPerhaps a bit better variation would be \n\n```\n val drmA = drmFromHdfs(...)(implicitly[ClassTag[String]])\n```\n\nI honestly don't like this too much. \n\nFirst, you have to remember to explicitly give it a type like above instead of implicit type inference which is expected in Scala \n\n```\n val drmA=...\n```\n\nAnother reason i don't like it is that you potentially are supplying a redundant information (since this information is already available form sequence file headers). Not only that, you _have_ to be right, or your delayed execution will fail in the tasks (so it is late error catching, which is fairly hairy for most users, so i'd expect a wall of questions of the type \"why my tasks fail\" for this very basic operation. \n\nSo, not so user-friendly (although those who know will cope). \n\n(2) Another way to fix it is to keep current method contract, and fill in this functionality that I believe is missing in Spark, namely, examine input headers and load in key class type and thus infer proper ClassTag[_]. \n\nThere practically no downsides to this, except for one. Up until now we pretty much avoided direct dependencies on any Hadoop libraries. Spark goes to great length (mostly, reflection and pluggable Strategies based on Hadoop version) in order to retain compatibility with various HDFS flavors and versions. I just don't want to join this game. Very much so.\n\n(3) Another way is similar to (2) in the sense that we keep the signature and idea of filling the gap of inferring ClassTag[_] ourselves, but we do it by examining first Writeable key instance in the dataset. \n\nThis is even better than (2) since it keeps us from dealing with HDFS apis directly. But obvious downside is that we probably now will have to load the entire partition in spark instead of just reading sequence file header. So much for delayed action and memory allocation. \n\n---\n\nmy personal preference is probably going with (2) while exploring (3) in terms of how bad this practice actually may get. Of course (1) is also an option since it is the easiest to implement.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55627685/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55641649", "body": "@andrewpalumbo  ok -- thank you for taking initiative on this! much appreciated, this seems like the most  cruel bug to me that ever happened to date on the spark stuff. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55641649/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56404290", "body": "on (1), it doesn't work because it takes classTag from the method bound,\nnot from actual evidence in the class.\n\nin order for this to work, i suggest to add\n\ndef keyClassTag:ClassTag[K]\n\nto CheckpointedDrm trait and implement it in concrete checkpoined\nimplementations as simply `implicitly[ClassTag[K]]`. Unfortunately you\ncannot implement it in a trait (like inside DrmLike or CheckpointedDRM)\nbecause as it stands, traits do not support access to concrete class\nevidence (as our workaround demonstrates, it is theoretically possible to\nsupport it thru virtual query to implementation, but as it stands, scala is\nnot really there).\n\non (2), you need to ask to load not the directory, but rather any partition file\ninside that directory. Obviously you need to require that source directory\ncontains at least on partion file with a good sequence file header.\n\nAlso keep in mind that SequenceFile api changed A LOT between hadoop 2 and\n1 and spark works with both, but naive (non-reflection) implementation can\nonly work with whatever currently declared as Mahout dependency. This is\nwhy i am saying implementing it with full cross-version hadoop\ncompatibility the way Spark does is extremely hairy.\n\nOn Mon, Sep 22, 2014 at 9:36 AM, Andrew Palumbo notifications@github.com\nwrote:\n\n> Still a work in progress, (and still in need of some cleanup). The latest\n> commits now solve the original key object reuse problem by method (2) -\n> reading key type in from the SequenceFile Headers and then matching on it:\n> \n> mahout> val drmTFIDF= drmFromHDFS( path = \"/tmp/mahout-work-andy/20news-test-vectors/part-r-00000\")\n> 14/09/22 11:20:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n> drmTFIDF: org.apache.mahout.math.drm.CheckpointedDrm[_] = org.apache.mahout.sparkbindings.drm.CheckpointedDrmSpark@adf7236\n> mahout> val rowLabels=drmTFIDF.getRowLabelBindings\n> rowLabels: java.util.Map[String,Integer] = {/soc.religion.christian/21427=6141, /comp.graphics/38427=422, /comp.sys.ibm.pc.hardware/60526=1281, /misc.forsale/76295=2495, /soc.religion.christian/21332=6103, /sci.med/59045=5265, /sci.electronics/54343=5096, /comp.sys.ibm.pc.hardware/60928=1404, /rec.sport.hockey/54173=4205, /rec.motorcycles/104596=3282, /rec.autos/103326=2968, /talk.politics.misc/179110=7333, /comp.windows.x/66966=1944, /rec.autos/103707=3053, /comp.windows.x/67474=2146, /rec.sport.baseball/105011=3850, /talk.religion.misc/83812=7424, /comp.graphics/38707=522, /comp.graphics/38597=484, /sci.electronics/54317=5083, /rec.motorcycles/104708=3322, /rec.sport.hockey/53627=3994, /comp.sys.mac.hardware/51633=1601, /sci.crypt/16088=4686, /sci.electronics/53714=4840, /rec.sport.ho...\n> mahout> rowLabels.size\n> res15: Int = 7598\n> \n> Which is what I am expecting.\n> \n> Two problems that I am still having:\n> \n> (1) Its not yet solving the problem of setting the DrmLike[_] ClassTag yet.\n> \n> mahout> def getKeyClassTag[K: ClassTag](drm:DrmLike[K]) = implicitly[ClassTag[K]]\n> \n> mahout> getKeyClassTag(drmTFIDF)\n> res13: scala.reflect.ClassTag[_] = Object\n> \n> I believe that this is just because I'm not setting it correctly due to my\n> limited scala abilities.\n> \n> (2) DRM DFS i/o (local) is failing. I believe that this may downside to\n> integrating HDFS I/O code into the spark module. I'm not positive I'm\n> setting the configuration correctly inside of drmFromHDFS(...). I have no\n> problem reading in the files from within the spark-shell, but the spark DRM\n> DFS i/o (local) test is failing with:\n> \n> DRM DFS i/o (local) **\\* FAILED ***\n> java.io.FileNotFoundException: /home/andy/sandbox/mahout/spark/tmp/UploadedDRM (Is a directory)\n> \n> I believe may be because SequenceFile.readHeader(...) is trying to read\n> from HDFS and the test is writing locally. I will continue to look into\n> this.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/52#issuecomment-56401176.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56404290/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56407286", "body": "@andrewpalumbo see #53 for added test claim of correct key class tag. (warning, #53 disables h20 compilation for the time being). you can merge it in your #52 work branch for the test to verify the solution correctly.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56407286/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56407640", "body": "Obviously, #53 assert inside the i/o test fails right now  with \n\n```\n org.apache.hadoop.io.Writable was not equal to Int\n```\n\nbecause it doesn't deduce correct key type.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56407640/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56408611", "body": "I think I made a mistake in this sense by not defining DrmLike or at least CheckpointedDrm as an abstract class instead of a trait. That way, classtag evidence would've been accessible to abstract class. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56408611/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56410501", "body": "`git pull git@github.com:dlyubimov/mahout MAHOUT-1615-drmFromHdfs`?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56410501/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56410749", "body": "or i guess `git pull https://github.com/dlyubimov/mahout MAHOUT-1615-drmFromHdfs`?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/56410749/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/mahout/comments/6656953", "body": "I still dont get it. This implementation counts positive elements, not nonzeroes.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/6656953/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/6666688", "body": "Considerations of negative interactions of co-occurence analysis are irrelevant since you are implementing contract that has nothing to do with any particular algorithm you may be using it for.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/6666688/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/10723951", "body": "who-a?...\n\nOn Tue, Apr 14, 2015 at 1:55 AM, Stevo Slavi\u0107 notifications@github.com\nwrote:\n\n> directory names of submodules do not match artifactId, e.g. they do not\n> have mahout prefix\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/commit/f7b69fabf1253b5e735e269c9410459d91816cdd#commitcomment-10708902\n> .\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/10723951/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/10724331", "body": "I am not in favor of renaming artifacts in general, and in this particular case as well. \n\nIn general, because renaming artifacts create incredible operational and legal headaches on the scale you can't even begin to imagine, in certain places :)\n\nin particular, because If \"samsara\" is to refer to computing environment, (let's say algebra environment in particular), then its code is not contained in any single module. instead, it's dispersed around.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/10724331/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/18009341", "body": "I think there's a bug in this line. If we return true for dense outcome of the analysis, the comparison should be reversed to `>`  in this line.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/18009341/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/18009547", "body": "uhm, yes, density makes more sense.\n\nOn Fri, Jun 24, 2016 at 1:45 PM, Andrew Palumbo notifications@github.com\nwrote:\n\n> yep looks like.. I can fix it. I was also thinking the name should be\n> refactored to densityAnalysis so that true = isDense- seems more\n> intuituive. what do you think? Does sparsityAnalysis have any specific\n> meaning outside of here?\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/apache/mahout/commit/d9940489d2f849d36af396d603f6170ab560e505#commitcomment-18009403,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAf7_5MT7tIGk-w7SYJYCoXuNOvpQxiuks5qPEHggaJpZM4I-G2e\n> .\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/18009547/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/18009583", "body": "I also think it should be more thorough to use shallow initialization, especially in the case when input is already coming as dense and is using dense vectors. right now it looks like this would be copying it. \n\ne.g.:\n\n```\n    val block = new SparseRowMatrix(vectors.size, blockncol, vectors, true, true)\n\n    // Test the density of the data. If the matrix does not meet the\n    // requirements for density, convert the Vectors to a sparse Matrix.\n    val resBlock = if (sparsityAnalysis(block)) {\n      val shallow = vectors.forall(_.isInstanceOf[DenseVector])\n      if (shallow) {\n        // I don't like it but at this point this is the only way to avoid copying with DenseMatrix\n        // initialized by DenseVectors:\n        val vclass = classOf[DenseVector]\n        val valAttr = vclass.getField(\"values\")\n        // Use shallow initialization over backing array of Doubles.\n        new DenseMatrix(vectors.map(v \u21d2 valAttr.get(v).asInstanceOf[Array[Double]]),true)\n      } else {\n        dense(vectors)\n      }\n\n    } else {\n\n      // Sparse matrix: we already created it as a sparse matrix wrapper. There may be a path\n      // to improvement in case the payload comes in as dense vectors, but analysis says they\n      // are really sparse so shallow wrapper doesn't make much sense and we would need to\n      // copy the data into truly sparse vectors in order to truly save memory here. TODO\n      block\n    }\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/18009583/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/18009657", "body": "although... reflection to extract values[] doesn't work. i think this need a patch on dense matrix that accepts array of vectors and `shallowIfPossible` flag that would do shallow init if incoming vectors are DenseVectors.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/18009657/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/18009705", "body": "i guess we really need to patch dense(vecs) implementation to implement shallow initialization where possible. Or have it as \n\n```\ndef dense[R](rows:R*)(shallowIfPossible:Boolean=true) \n```\n\n?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/18009705/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/18009875", "body": "no.. this form doesn't work either.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/18009875/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/18780257", "body": "oh yeah i had to patch it over myself. this was bad. The fix (i think) was\nto go over a _collection_ of vectors that are read and formed there anyway\nand then run DA over them (using shallow-initialized row sparse matrix)\n\nOn Thu, Aug 25, 2016 at 9:12 AM, Andrew Palumbo notifications@github.com\nwrote:\n\n> thank you for reporting this, @AddictedCS https://github.com/AddictedCS.\n> This commit was actually a bugfix for #228\n> https://github.com/apache/mahout/pull/228, and as you point out was not\n> released. in 0.12.2. The densityAnalysis() method is actually intended to\n> keep memory usage down by using DenseMatrixs in any cases where data is\n> estimated to be > 25% non-zero.\n> \n> The exception here is being thrown because when blockifying a matrix we\n> use DenseMatrix as the default, and then run densityAnalysis() on that\n> matrix.\n> \n> I suppose a good fix might be to default to a SparseMatrix block when\n> blockifying a DRM, check the density of the block, and in a the case that\n> it is not sparse, create a DenseMatrix and copy the data into it.\n> \n> Could you give a little more information about your dataset i.e. full\n> dimensions, as well as the memory settings that you've provided during\n> execution.\n> \n> Thanks again for reporting this.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/apache/mahout/commit/727e5be85c0326d9c009d9cdc361fe47ffa201ad#commitcomment-18778712,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAf7_3c2a0iGE-4anmqtIatViaQoOuwjks5qjb7YgaJpZM4Jsxix\n> .\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/18780257/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/18798505", "body": "short answer: it probably should be lower but we don't know exactly how\nmuch lower -- this is a pretty wild guess. We can't test for exact optimum\nbecause of the variability of data and pipelines and lack of good\nquantitative guess in the solvers as it stands per below.\n\nlonger answer:\n(a) higher than one does not save much memory to care much but create\nsignificant dangers outlined below\n(b) sparse solvers are not that much faster -- but likely even slower\n(c) product of solvers will likely turn dense if higher than but we don't\nspare much effort to anticipate that before we solve (e.g., multiplication\nproduct will be no sparser than outer product of the densiest column of LHS\ntimes densiest row of the RHS.  we can assume that with big data density\nvariance among those will be significant and so the actual numbers for such\nwill be >>mean(density) tested (25%). Since like i said we don't anticipate\nproduct density, we probably would be better off keeping this safely low\n(actually if it were up to me, i probably would not sparsify products that\ndon't pass 20% test)\n\nOn Fri, Aug 26, 2016 at 11:48 AM, Sebastian notifications@github.com\nwrote:\n\n> Just a question out of curiosity, why have a default threshold of 25% ?\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/apache/mahout/commit/727e5be85c0326d9c009d9cdc361fe47ffa201ad#commitcomment-18797338,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAf7_6njxIKEO2jazJ0CnsDYxqsmkY_nks5qjzUEgaJpZM4Jsxix\n> .\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/18798505/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/18798722", "body": "(c) product of solvers will likely turn dense if higher than but we don't\n\n> spare much effort to anticipate that before we solve (e.g., multiplication\n> product will be no sparser than outer product of the densiest column of LHS\n> times densiest row of the RHS.\n> \n>  sorry -- this should read -- densiest outer product. Still the idea is\n> things tend to become somewhat densier in the output of the solver, and\n> we'd rather not risk of dense product via sparse solver, since it will\n> usually make the sparse solver to run much slower than a dense solver in\n> its stead.\n\nI guess we can benchmark it with multiplication of random sparse matrices\nto see where the speed balance strikes, but even then it would be pretty\nflawed since we can expect significant variability of non-randomness in\ndata as well as cost-per-iteration variability among  algorithms.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/18798722/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/comments/18801468", "body": "I am dubious this will be ever called for. This could only make sense for a\nparticular dataset and particular pipeline and even then optimum woudl\nchange from operation to operation. I don't believe this is a priority for\nnow. I believe most datasets are either much densier or much sparser than\nour threshold, there's rarely something in between (and if it is, it'd be\nserved quite well by the dense framework).\n\nOn Fri, Aug 26, 2016 at 7:56 PM, Andrew Palumbo notifications@github.com\nwrote:\n\n> Maybe we should consider adding in a user definable global property for a\n> density threshold?\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/apache/mahout/commit/727e5be85c0326d9c009d9cdc361fe47ffa201ad#commitcomment-18801276,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAf7_y0_wQ9ap7te7ZKAN84vHEk9vfSfks5qj6dggaJpZM4Jsxix\n> .\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/18801468/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13257657", "body": "I think it'd be better to add this when (after) you will be doing a squash pull. Otherwise you'd be merging this file with other changes. This file is guaranteed to change by other commits every time. Although most likely this conflict will be handled automatically by git. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13257657/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13518008", "body": "I don't think we need to bring any dfs utils into scala. First, why is it not covered by Hadoop \"glob\"s or tons of hdfs helpers that Sean had created? Second, i think it needs to be shared by all platforms hence it probably needs to go where this common stuff resides in java. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13518008/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13518011", "body": "i guess license is coming.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13518011/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13688078", "body": "is that where the major problem was? is that because assignment of sequential vector to sequential vector is that slow? or this is an assignment of random vector to a sequential vector? (sequential to sequential actually should be ok methinks). \n\nAnyway I don't see any immediate problems, and the quality of your work usually doesn't require any deep scrutiny, so i'd say ship it. Actually the sooner the better, because i am very close to actually give it all a good spanking\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13688078/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13687695", "body": "we don't use vector.set and vector.get in scala. we use dsl which would look simply  \n\n```\nacc(elem.index) += 1\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13687695/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13687718", "body": "No need to re align. IDEA scala plugin does a good job donig style indentation. Are you using IDEA?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13687718/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13687795", "body": "There are few doubts there. \n(1) DSL follows R. Which R function this mimics? If none, can we get away with doing it on java side? ( i guess we can't get around it if you want to do it for DRMs). \n(2) the name IMO is misleading given description. nonZeroCounts()?\n(3) The implementation (both in-core and distributed, BTW) seem to contradict the description. It looks like implementation actually counts number of _non-negative_ elements rather than what description portrays.\n(4) I would like Sebastian's feedback on this too, since he is both the primary co-occurrence author and understands R-like semantics idea very deeply.  If there's a better semantics (like I suspect it should be), he should know that.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13687795/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13688228", "body": "a line too long? (style?)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13688228/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13711381", "body": "it looks like, to me. don't have time to look in depth. but distributed code definitely counts non-negatives with explicit inline conditional >0\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13711381/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13711414", "body": "it is very easy to tweak tests though to check if in doubt\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13711414/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13770844", "body": "Since this returns double, correct style is to say 1.0 or 0.0 not 1 or 0 on java side regardless of implicit conversion\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13770844/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13770924", "body": "style: spacing?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13770924/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13771023", "body": "Hm. isn't that is made obsolete with Sebastian's PR #17 ?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13771023/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13771395", "body": "not sure if coocurrence test changes like that are necessary\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13771395/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13772128", "body": "just relaying some historical discussion in Mahout. It was known to create a bug in my own Mahout commit once. Which had sparked the discussion about constant formatting.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13772128/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13775208", "body": "i mean, shouldn't this specialized version be more effective than an aggregate: \n\n```\ndef apply(f: Vector): Double = f.getnumNonZeroElements().toDouble\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13775208/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13987402", "body": "if no custom jars are used, parameter does not have to be there. Scala default parameters are pretty useful alternative to tons of overloaded functions.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13987402/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13987451", "body": "again, no need to specify customJars if no jars are added.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13987451/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13987539", "body": "num partitions here -- is it a \"magic: number or you know it is enough for this particular dataset and dataset cannot change?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13987539/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13987730", "body": "Here and elsewhere. If compound closure, parenthesis could be ommitted \n\n```\n interactions.map { \n      case (rowID, columnID) =>\n         val rowIndex = rowIDDictionary_bcast.value.get(rowID).get\n```\n\n...\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13987730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13987777", "body": "here and elsewhere: style spacing\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13987777/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13987854", "body": "Also, it is a matter of convention, but I usually prefer to put public traits and classes in their own file. One may argue \"old habits die hard\", but i still think it is the right thing to do for publicly visible things.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13987854/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13988052", "body": "Hm. i'd still rather delegate than extend, \n\nThen you can either expose parameters as a public value; or, assuming you just want to be able to write schema.any-MapLike-method, you can provide an implicit conversion from Schema to MapLike.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13988052/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14617005", "body": "Not sure why you are saying this. If I am not misinterpreting what you are saying, neither theoretical nor practical estimate supports this. Added an assignment benchmark with and without openhash intermediary. here are the results of running it: \n\n```\nTesting started at 12:26 PM ...\nAverage assignment seqSparse2seqSparse time: 29.673 ms\nAverage assignment seqSparse2seqSparse via Random Access Sparse time: 406.510 ms\n```\n\nThis of course assumes that we, for the most part, are having SeqSparse vectors, not RandomAccessSparse ones as payload (which we always are, unless somebody explicitly messes it up with a `mapBlock`.) \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14617005/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15187205", "body": "style: (1) we don't use ( in closures, (2) we try to use -> here, i.e recommended style to reduce amount of punctuation: \n\n```\n// Adjust RHS keys\nval bAdjusted = b.map { case (key, vec) => (key + n1) -> vec }\n\n// Dump a and b together \n....\n```\n\nalso definitely needs more comments on closures\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15187205/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15187303", "body": "There really should be only two types of routines:  (1) Int (2) everything else. \n\nLong does not have ordering semantics (unlike int) because ordering algebraically is only important for transpositions, and since vector.set(i) only accepts an Int, there is no special significance for Long keys thru-out (i.e. long labels are bearing no special meaning compared to, say, 128 bit hashes). \n\nOther than that, it makes sense. One thing I am not sure if union() is the best operation here, maybe there's a special opertaion that implies just dumping all partitions together without doing reshuffling, need to check with Spark API.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15187303/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15187912", "body": "i keep a nagging feeling that there got to be a better way than writing something like `implicitly[ClassTag[Int]])`. The fact that it is implicitly inferrable without special context means there's a public `val` defined somewhere for it, so perhaps a better style is to refer to that val explicitly than ask the compiler to find it for us for something that is overtly general type.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15187912/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15188151", "body": "Generally, if one can use blockified representation, one probably should rather than try to de-blockify operands (i.e. `toDrmRdd()` should be last choice here). Or better yet, since we are just dropping all partitions together, ideally, we can just look up current representation and use blocks if it is already blockified, and use rows if it is not yet blockified (i.e. handle both cases separately).\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15188151/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15188440", "body": "hm. i wonder if we can just insert mapBlock() operator at logical layer to handle key mutations here on one of the operands and have same semantics (concatenate partitions) at the physical layer. \n\nGenerally, if we can handle a problem at logical level, we always should try to do that, since it minimizes the engine specific problem. \n\nSo the engine should probably have a physical operator of vertical concatenation only, regardless of the key type.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15188440/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15198464", "body": "-1 on this diff. \n\ni am not sure what problem it is solving but there has got to be a different way to solve it. Matlab/R semantics is deemed sufficient to solve algebraic problems historically and they did not have a need for this. So shouldn't we. \n\nif nothing else, ultimately one can always exit to RDD level and re-format RDD content to whatever liking.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15198464/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15899701", "body": "variable doesn't work here?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15899701/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15900661", "body": "ok, makes sense. Checked maven for spark -- it has the same thing there.\n\nOn Wed, Aug 6, 2014 at 1:08 PM, Anand Avati notifications@github.com\nwrote:\n\n> In math-scala/pom.xml:\n> \n> > @@ -27,7 +27,7 @@\n> >      <relativePath>../pom.xml</relativePath>\n> >    </parent>\n> > -  <artifactId>mahout-math-scala</artifactId>\n> > -  <artifactId>mahout-math-scala_2.10</artifactId>\n> \n> Unfortunately variables do not work. See\n> https://github.com/apache/spark/pull/996#discussion_r15679675 for similar\n> discussion.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/39/files#r15900010.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15900661/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16143599", "body": "it would seem to me tests for directory would be unnecessary. \n\nalso recursion would probably be less circumlocutory if it were considered as deleting (any) resource rather than a directory, i.e.  something along the lines \n\n```\ndef quietDelete(file:File) {\n    for (nested <- file.listFiles) quietDelete(nested)\n    try { file.delete }\n}\n```\n\nI am also not sure if try { } is actually necessary as this stuff will probably not produce an exception but rather returns false in case of failure. I don't remember already. Also i guess listFiles() may produce `null` under some circumstances, also not sure\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16143599/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13929215", "body": "I think if all these tests are passing, this would be an incredibly cool step forward in this issue..\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13929215/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15203732", "body": "hm. i thought this was not part of distributed decompositions suite and has been moved out to math-scala?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15203732/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15654236", "body": "Well one fundamental thing that i surely missed is that this module's physical operators are written in java wheres all tests and apis are in scala. So the maven module is fragmented between java and scala code. Something i was intentionally triying to avoid (either module is 100% java, or 100% scala).\n\nI suppose it is not going to stop it from committing now, but it just shows how superficial my initial review was.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15654236/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15654554", "body": "Here and elsewhere. Operator spacing style. please use autoformatting features in idea.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15654554/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15654625", "body": "Mahout doesn't use underline prefixes for class attributes. We follow standard Sun style conventions as far as java code is concerned.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15654625/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15654715", "body": "spacing\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15654715/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15654963", "body": "empty body should not be specified with {}\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15654963/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15655025", "body": "remove extra line\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15655025/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15655056", "body": "remove extra line please (probably in prototype of this test, too)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15655056/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15655171", "body": "This -SNAPSHOT dependency should be fixed ASAP. \n\nFirst, we cannot even release with snapshot dependencies. Second, the artifact commit level is undefined with snapshots, may cause problems for people trying to compile later and having different commit levels of this artifact in the cache. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15655171/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15657066", "body": "here and elsewhere: lower camel case for variables is standard in Sun  style conventions, i.e. `drmA`, not `DrmA`\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15657066/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/17954398", "body": "we want of course to fix it. unfortunately i am still unable to compile h20. I read somewhere that the error i am getting means there's a broken jar in classpath, but i am still getting this error even after i cleaned out my entire local maven repo.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/17954398/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/17954424", "body": "this can be removed entirely. use key2val instead.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/17954424/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/18014902", "body": "perhaps naming is to blame. key2val was meant to be the transformation from\nfile key (i.e. writable) to actual non-reused type such as Int etc. val2key\nwas meant to be inverse (and in the non-edited code it is), but it is not\nused in context of this method and therefore should be omited. i.e it\nshould be simply\n\n```\n   val (key2val, exactTag) = .... match ...\n```\n\nOn Wed, Sep 24, 2014 at 2:23 PM, Andrew Palumbo notifications@github.com\nwrote:\n\n> In spark/src/main/scala/org/apache/mahout/sparkbindings/SparkEngine.scala:\n> \n> > ```\n> >        (v: AnyRef) => v.asInstanceOf[LongWritable].get,\n> > ```\n> > -          (x: Any) => new LongWritable(x.asInstanceOf[Int]),\n> > -          (x: Any) => new LongWritable(x.asInstanceOf[LongWritable].get),\n> \n> I'm not sure here- we need to remove one of the functions them but I think\n> we should be using val2key here not key2val correct?\n> \n> ```\n>  key2val(v: AnyRef) => v.asInstanceOf[IntWritable].get\n>  val2key(x: Any) => new Integer(x.asInstanceOf[IntWritable].get)\n> ```\n> \n> so later when we map to the RDD:\n> \n> val drmRdd = rdd.map { t => val2keyFunc(t._1) -> t._2.get()}\n> \n> they will be of form [Integer][Vector]rather than [IntWritable][Vector]\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/52/files#r18001536.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/18014902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/18014938", "body": "i mean, this should just be \n\n```\n val (key2valFunc, unwrappedKeyTag) = keyTag match {....\n```\n\nand `key2valFunc` should eventually be used for transformation.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/18014938/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/18014952", "body": "While we at this, we probably should use cacheHint 'NONE' here. Spark automatically disables HadoopRDD's caching anyway.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/18014952/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/18015155", "body": ".. and since we know that Writables are not be useable since they are reused, we probably should block this case completely out with an error?\n\nThere's another piece of information to consider. Spark itself definex implicit conversions from some well-known Writables to their payload types. Perhaps we should support everything that's there; and maybe even figure a way to automatically apply everything that Spark exports, without even doing cases. I tried to figure how to do that (i remember that) but still haven't figured. it may not be possible; at least, i remember i haven't figured how that might be done. But we are at least 1.5 years past that moment, so perhaps we could revisist this from a fresh perspective. It would require eyeballing Spark's implicit Writable conversions again.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/18015155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/18108922", "body": "Can we please squirrel away lines 136-143 and all direct hadoop imports into a separate function, separate util helper object please? I am still very wary of direct hadoop dependencies and i predict this will not work on _all_ CDH /public hadoop releases.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/18108922/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/18109074", "body": "I think we don't need a key tag for writable. We only need key tag for value type (i.e. ClassTag.Int, etc.). This is not used anywhere. I suspect just matching on the class should be o.k.\n\n I mean, initially I thought Spark would give it correctly via rdd, but it didn't so we don't need to match on a classtag.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/18109074/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/18432276", "body": "yes this looks right now\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/18432276/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/21948535", "body": "why does this keep getting added (even with comment \"we don't use it????) I keep cleaning this out and it appears again. If it is not being used why it is here? It adds tons of time to compilation to build this jar and clogs artifact repo \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/21948535/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/21948590", "body": "Even worse, even hadoop now doesn't support job jars??? (not that this could be used for mapreduce anyway)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/21948590/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15908429", "body": "Imports look engine-independent. Should not be Spark-coupled. \nImports probably should include implicit operations per documentation doc (which is why the code does weird stuff like `new MatrixOps(m)`).\n\nThe \"standard\" recommended way to do imports for engine -independent code\n\n```\n import org.apache.mahout.math._\n import scalabindings._\n import RLikeOps._\n import drm._\n import RLikeDrmOps._\n```\n\nif java collections are used (e.g. something like `for (row <- matrix) { ... }`) then it also would need \n\n```\nimport collection._\nimport JavaConversions._\n```\n\nto enable all implicits.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15908429/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15908475", "body": "if imports are properly done, this should just be \n\n```\n val weightsPerLabelAndFeature = dense(observationsPerLabel.map(_.colSums))\n```\n\nNote that this is not Spark-dependent code. the `map` here is Scala collection map.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15908475/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15908489", "body": "Similarly this should be just \n\n```\n val weightsPerFeature = weightsPerLabelAndFeature.colSums\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15908489/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15908508", "body": "Same thing here. no need for `new MatrixOps`...  here and elsewhere \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15908508/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15908555", "body": "in Mahout Scala, this slicing should look \n\n```\n... weightsPerLabelAndFeature(labelIndex, ::)\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15908555/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15908947", "body": "Mahout convention is to write these as `1.0` rather than a float.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15908947/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16019061", "body": "er...not sure why changelog shows these changes, please sync to latest master so no changes until squash-commit please.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16019061/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16019136", "body": "weird. this should be just \n\n```\n import drm._\n import RLikeDrmOps._\n```\n\n(since math._ is already imported) (wrong import order?)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16019136/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16019231", "body": "hm. why any changes to Co-oc analysis? i think this needs merging with master with -Xtheirs\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16019231/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16022926", "body": "please remove `{ }` for classes with no body\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16022926/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16022935", "body": "what is  using mr-legacy here?\n\ni guess the `ComplementaryThetaTrainer`?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16022935/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16023021", "body": "ok\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16023021/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/22326490", "body": "instead of filtering what is _exluded_ (opt-out) i'd rather determine\nwhat's the minimum opt-in (in the assembly plugin source file). That's\ncommon practice, excludes are tedious, and, most importantly, don't tell\nyou a bit what exactly you are ending up with\n\nOn Mon, Dec 29, 2014 at 12:03 PM, Pat Ferrel notifications@github.com\nwrote:\n\n> In spark/src/main/assembly/dependencies.xml\n> https://github.com/apache/mahout/pull/69#discussion-diff-22326191:\n> \n> > ```\n> >      <exclude>org.apache.hadoop:hadoop-core</exclude>\n> > ```\n> > -        <exclude>org.apache.spark:spark-core_${scala.major}</exclude>\n> > -        <exclude>org.scala-lang:scala-library</exclude>\n> > -        <exclude>jackson-core-asl</exclude>\n> > -        <exclude>jackson-mapper-asl</exclude>\n> > -        <exclude>xstream</exclude>\n> > -        <exclude>lucene-core</exclude>\n> > -        <exclude>lucene-analyzers-common</exclude>\n> >      </excludes>\n> >    </dependencySet>\n> \n> This is as many as seem safe. Lots inside mrlegacy that could be excluded\n> but its all in the same artifact so leaving in unless someone knows how to\n> exclude particular partial packages.\n> \n> Won't change the code to trim things from the classpath in this commit but\n> I suspect the dependencies.jar may be all that is needed for spark-shell\n> and drivers.\n> \n> @andrewpalumbo https://github.com/andrewpalumbo there's little chance\n> this will mess up your drivers so I may push this after some more testing\n> on my side.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/69/files#r22326191.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/22326490/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/23556922", "body": "I guess in case of H20 this will break with NPE... Perhaps we can make it an Option[...] and gracefully report capability is not supported by engine if it happens to be None. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/23556922/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/23577359", "body": "merge artifact\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/23577359/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/23578645", "body": "Why is this property important for tests? they won't run otherwise?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/23578645/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/23638378", "body": "@hamnis, could you please explain what is significance of this parameter -- \"allowMultipleContexts\" in 1.2.0 so that tests won't run?\n\nAs far as i remember, current head is using only one context (per suite).so next suite should run another context, but by that time the previous one should already be gone. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/23638378/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/23655178", "body": "Hm... I think i want to understand this more.\n\nI suspect the right thing to do is to understand why how our test setup is\nflawed for 1.2  and fix context instantiation rather than put this property\nin.\n\nCould we please delay this until we move on to 1.2 at which point.\n\nOtherwise i suppose i don't see a reason not to bump to 1.1.1 at this point.\n\nOn Tue, Jan 27, 2015 at 3:00 PM, Erlend Hamnaberg notifications@github.com\nwrote:\n\n> In\n> spark/src/test/scala/org/apache/mahout/sparkbindings/test/DistributedSparkSuite.scala\n> https://github.com/apache/mahout/pull/71#discussion_r23651449:\n> \n> > @@ -39,6 +39,7 @@ trait DistributedSparkSuite extends DistributedMahoutSuite with LoggerConfigurat\n> >            .set(\"spark.kryoserializer.buffer.mb\", \"15\")\n> >            .set(\"spark.akka.frameSize\", \"30\")\n> >            .set(\"spark.default.parallelism\", \"10\")\n> > -          .set(\"spark.driver.allowMultipleContexts\", \"true\")\n> \n> This was done since the tests failed with an error message similar to:\n> \"Please enable this property to make tests work\". Currently using my phone,\n> so can't check exact error message. To reproduce: set spark version to\n> 1.2.0 and run tests.\n> \u2026 <#14b2d9fb32e77c9c_>\n> On 27 Jan 2015 21:09, \"Dmitriy Lyubimov\" notifications@github.com\n> wrote: In\n> spark/src/test/scala/org/apache/mahout/sparkbindings/test/DistributedSparkSuite.scala\n> https://github.com/apache/mahout/pull/71#discussion_r23638378: > @@\n> -39,6 +39,7 @@ trait DistributedSparkSuite extends DistributedMahoutSuite\n> with LoggerConfigurat > .set(\"spark.kryoserializer.buffer.mb\", \"15\") >\n> .set(\"spark.akka.frameSize\", \"30\") > .set(\"spark.default.parallelism\",\n> \"10\") > + .set(\"spark.driver.allowMultipleContexts\", \"true\") @hamnis\n> https://github.com/hamnis https://github.com/hamnis, could you please\n> explain what is significance of this parameter -- \"allowMultipleContexts\"\n> in 1.2.0 so that tests won't run? As far as i remember, current head is\n> using only one context (per suite).so next suite should run another\n> context, but by that time the previous one should already be gone. \u2014 Reply\n> to this email directly or view it on GitHub <\n> https://github.com/apache/mahout/pull/71/files#r23638378>.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/71/files#r23651449.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/23655178/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24697038", "body": "why is this removed?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24697038/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24697097", "body": "oh. this is why we kept growing unused contexts in tests? \n\nif so, good catch. thank you.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24697097/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24701880", "body": "Hm. indeed. ok.\n\nNormally we import both and i'd say drm._ is the place for it. RLikeDrmOps\ndeals with R dialect specifically so i'd say if i had to remove it, i'd\nremove it from RLikeDrmOps (assuming all tests pass).\n\nOn Fri, Feb 13, 2015 at 2:05 PM, Erlend Hamnaberg notifications@github.com\nwrote:\n\n> In math-scala/src/main/scala/org/apache/mahout/math/drm/package.scala\n> https://github.com/apache/mahout/pull/74#discussion_r24700771:\n> \n> > @@ -80,9 +80,6 @@ package object drm {\n> >    /*\\* Just throw all engine operations into context as well. */\n> >    implicit def ctx2engine(ctx: DistributedContext): DistributedEngine = ctx.engine\n> > -  implicit def drm2drmCpOps[K: ClassTag](drm: CheckpointedDrm[K]): CheckpointedOps[K] =\n> \n> it already exits here:\n> https://github.com/apache/mahout/blob/master/math-scala/src/main/scala/org/apache/mahout/math/drm/RLikeDrmOps.scala#L137.\n> if you upgrade to 2.11 this would crash. and both are imported into scope.\n> Why this does not crash on 2.10 i have no idea.\n> Not sure which one that should be removed, but I chose this one.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/74/files#r24700771.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24701880/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24701966", "body": "BTW Erlend thank you for looking into this. Much appreciated!\n\nOn Fri, Feb 13, 2015 at 2:20 PM, Dmitriy Lyubimov dlieu.7@gmail.com wrote:\n\n> Hm. indeed. ok.\n> \n> Normally we import both and i'd say drm._ is the place for it. RLikeDrmOps\n> deals with R dialect specifically so i'd say if i had to remove it, i'd\n> remove it from RLikeDrmOps (assuming all tests pass).\n> \n> On Fri, Feb 13, 2015 at 2:05 PM, Erlend Hamnaberg <\n> notifications@github.com> wrote:\n> \n> > In math-scala/src/main/scala/org/apache/mahout/math/drm/package.scala\n> > https://github.com/apache/mahout/pull/74#discussion_r24700771:\n> > \n> > > @@ -80,9 +80,6 @@ package object drm {\n> > >    /*\\* Just throw all engine operations into context as well. */\n> > >    implicit def ctx2engine(ctx: DistributedContext): DistributedEngine = ctx.engine\n> > > -  implicit def drm2drmCpOps[K: ClassTag](drm: CheckpointedDrm[K]): CheckpointedOps[K] =\n> > \n> > it already exits here:\n> > https://github.com/apache/mahout/blob/master/math-scala/src/main/scala/org/apache/mahout/math/drm/RLikeDrmOps.scala#L137.\n> > if you upgrade to 2.11 this would crash. and both are imported into scope.\n> > Why this does not crash on 2.10 i have no idea.\n> > Not sure which one that should be removed, but I chose this one.\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub\n> > https://github.com/apache/mahout/pull/74/files#r24700771.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24701966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24937438", "body": "use of canonical relative paths is IMO preferred.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24937438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24937543", "body": "i thought this is scaladoc style comment. I thought inline comment is just // or /\\* */I may be wrong.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24937543/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24937600", "body": "i believe this change introduces wrong style. the prior version was the correct style.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24937600/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24937867", "body": "no-parenthesis is a proprety style in Scala. E.g. all java getters by default are styled without (). I am guessing start() is really a routine here, not a property. The convention is that a thing without () must not change object state or have any other side effects.  The setters (optionally) then are encoded using `def start_=(value)` form.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24937867/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24938238", "body": "we probably need to choose between scala-style scaladoc multiline \n\n```\n  /** Multiline Comment\n   */\n```\n\nand Spark (java) -style multiline \n\n```\n  /**\n   * multiline comment\n   */\n```\n\nI generally follow spark convention (i.e. scaladoc style for single line and the latter variation for multilines) but in this patch i see mix of both styles even in the same file.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24938238/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24938559", "body": "for multiline closures i generally prefer to see a line feed after argument line \n\n```\n... { x => \n    do something \n}\n```\n\nstyle as opposed to \n\n```\n... {  \n    x => do something \n}\n```\n\nI guess either is fine but in this patch i see both styles. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24938559/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24956259", "body": "well the point is this is not scaladoc comment at all. As in no scaladoc is generated here. This is an inline comment. therefore it should not be marked with /**.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24956259/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24956652", "body": "quoting from your reference:\n`However, this syntax should *only* be used when the method in question has\nno side-effects (purely-functional). In other words, it would be acceptable\nto omit parentheses when calling queue.size, but not when calling println().\n`\n\nKind of what i am saying. No side effects rule is violated (and, frankly,\nit is violated by anything but a computed property).\n\nOn Wed, Feb 18, 2015 at 4:33 PM, Pat Ferrel notifications@github.com\nwrote:\n\n> In\n> spark/src/main/scala/org/apache/mahout/drivers/RowSimilarityDriver.scala\n> https://github.com/apache/mahout/pull/76#discussion_r24956351:\n> \n> > @@ -135,7 +133,7 @@ object RowSimilarityDriver extends MahoutSparkDriver {\n> >    }\n> > \n> >    override def process: Unit = {\n> > -    start()\n> > -    start\n> \n> The function is an Arity-0 method. Perhaps I misread:\n> http://docs.scala-lang.org/style/method-invocation.html\n> \n> Arity-0\n> Scala allows the omission of parentheses on methods of arity-0 (no\n> arguments):\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/76/files#r24956351.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24956652/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24956846", "body": "if we don't have enough space we still can do a new line after closure params, right?  i.e. \n\n```\nblah blah blah .... {\n  x =>\n  do something else \n}\n```\n\nit's just looks inconsistent with that other closure style pattern you use elsewhere.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24956846/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25010890", "body": "this is Mahout's position on this with javadoc style as well afaik. Not to leave empty @@tags if they are already obvious.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25010890/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25010972", "body": "i think either is fine. (fwiw idea scala plugin, which is the only scala plugin out there that actually works, does not add extra indentation inside curly brackets regardless).\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25010972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25011283", "body": "imo this is no small convention. at least i found it incredibly useful since they expand syntax of an attribute here that significant way. Note also attribute-esque nature of the convention since they also augmented 0-arity call with an assignemt \"magic\" syntax def name_=(value) which in code will just be an attribute assignment semantics such as `name = value`.\n\nSo this is not random.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25011283/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25014295", "body": "i still don't understand why canonical path is replaced with non-canonical path here.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25014295/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25989764", "body": "i think maybe it is worth to import scala.math._ and use abs() directly..\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/25989764/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27678773", "body": "Defaults should be overwritten from MAHOUT_OPTS, right? but it doesn't look like it checks if it is already set?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27678773/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/29522655", "body": "-1 So i told before i don't see a need for this. These structures only exist in the memory. HashMap, TreeMap, (Bi)Dictionary etc.etc. on the wire they all serialize as Seq[(K,V)]. \n\nSo by that logic if i start using tree types in my mappers, i need to add a specific serialization support for it. No, i don't. Just because i want to build a treeSet in memory, doesn't mean i need to add extra support for it in serialization. \n\nI have a pretty strong opinion that we must not have any specific type in kryo serialization but matrices and vectors (because they use less than trivial logic there, they are not sequences of stuff). But any collection type of standard stuff should be happily serializable as a standard wire-friendly collection (i.e. sequence of standard stuff). \n\nThe only tricky thing might be that we don't want to materialize sequences of AnyVal stuff before rebuilding them into indexed collections; so this latter issue may need a bit of experimenting, but this is a technical issue, not architectural.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/29522655/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/31838736", "body": "yes it should, in fact.  thanks. although it is more about private than final; private values cannot be neighter changed nor overriden anyway. I am a bit surprised Scala allows \"private final\" in the first place.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/31838736/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/31938822", "body": "taken care of\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/31938822/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32253651", "body": "I am not sure how h20 does it but it looks like broadcast is just serialized to the backend with all requests, not really broadcasted. if so, there are no associated resources so method needs to do nothing. Anyhow it's something for Anand @avati  to look at.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32253651/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32255611", "body": "Noted for round of style fixes.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32255611/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32261115", "body": "I am using the unicode chars now (change of style that happened to me a few months ago) but it has not gone over the entire codebase. We probably may want to standardize on this (later) but for now i just keep style consistent with the rest of the file. If i change the style, i change the style in the whole file.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32261115/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32261427", "body": "I needed approximate uniform hypergeometric sampler quick. I have distributed version as well but it is not the code i am allowed to commit. Spark only supports k-sampler that can fit in memory, unfortunately. I thought of keeping it uniform but it wouldn't make much sense since implementation would have to re-distribute it again and the way i used it i collect it back again so it would be double-trip for nothing but just keeping the API uniform... we can discuss it in a separate issue if you want.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32261427/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32277536", "body": "NONE means yes, it's not cached.\n\nThe material effect of setting \"NONE\" cache level in spark, as far as i\nunderstand, is exactly as not setting anything at all; the difference is\nonce you set it to anything, you cannot change it. So this means if i call\n\"NONE\" it would lock me to that choice with spark (which did not make much\nsense to me) otherwise it doesn't matter.\n\nOn Thu, Jun 11, 2015 at 3:56 PM, Andrew Musselman notifications@github.com\nwrote:\n\n> In\n> spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrmSpark.scala\n> https://github.com/apache/mahout/pull/135#discussion_r32277244:\n> \n> > @@ -78,8 +78,8 @@ class CheckpointedDrmSpark[K: ClassTag](\n> >    }\n> > \n> >    def cache() = {\n> > -    if (!cached) {\n> > -      rdd.persist(_cacheStorageLevel)\n> > -    if (!cached && _cacheStorageLevel != StorageLevel.NONE) {\n> \n> Will this be confusing if someone tries to call cache() with NONE set and\n> it's not cached?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/135/files#r32277244.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32277536/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32278958", "body": "Any public api, if it is not a matrix method,  is package-level set of functions. This is to follow R conventions where most of the functions are not obviously connected to any object. I.e. we just write something like \n\nimport o.a.m.math.drm._\n....\n\nval sample = drmSampleRows(...)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32278958/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32279040", "body": "also, i think engine variable is not visible to user, only to samsara functions. so there is no 2 ways for user to invoke that.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32279040/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32281350", "body": "this is scala style yes. they decided to do away with inline ?: (bincond)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32281350/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32282538", "body": "it's replaced. remove is the plan. experiments show it's decisively losing\nin current spark implementations. May change though with future spark\nversions. one never can be sure.\n\nOn Thu, Jun 11, 2015 at 5:09 PM, Andrew Musselman notifications@github.com\nwrote:\n\n> In spark/src/main/scala/org/apache/mahout/sparkbindings/blas/AtB.scala\n> https://github.com/apache/mahout/pull/135#discussion_r32281497:\n> \n> > ```\n> > */\n> > ```\n> > -  def atb_nograph[A: ClassTag](\n> > -      operator: OpAtB[A],\n> > -      srcA: DrmRddInput[A],\n> > -      srcB: DrmRddInput[A],\n> > -      zippable:Boolean = false\n> > -      ): DrmRddInput[Int] = {\n> > -  @deprecated(\"slow, will remove\", since = \"0.10.2\")\n> \n> Do we have a plan to improve/remove?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/135/files#r32281497.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32282538/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/31955348", "body": "ok stop-gap but perhaps we can do it on top of #135 ..\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/31955348/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32784126", "body": "I am sorry. but this is wrong.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32784126/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33203413", "body": "Other than it is valid Scala style? no. \n\nbtw every tool i have shows it correctly for me, including less, web pages, latex/lyx, etc. etc. Use intellij. really.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33203413/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33218018", "body": "Of course it should. Typo when copied? Strange that tests ran fine... it obviously would be a wrong result at least and index oob at most\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33218018/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33974540", "body": "What's the rationale behind this rename? \n\nI'd rather keep it as is. \n\nMy reasoning is, ok spark shells have 'sc' reserved for spark context, so we can't use the same variable name just to avoid clashes between mahout-enabled scripts and just spark-enabled scripts that potentially may expect spark context to be named \"sc\" (not likely since it is implicit, but still).\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33974540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33974666", "body": "comes to that, we could potentially initialize sc with spark context and sdc with mahout context. perhaps that would be ok and create compatibility throughout.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33974666/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33983540", "body": "yes.\n\nExcept we are already calling \"createMahoutContext\" during createSparkContext, so we just need to stash its output as Mahout context private attribute and pass it in here to implicit. (perhaps via a public getter or val).\n\nas for deinitalization, you are right -- all it does i think it stops the spark context. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33983540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/34008915", "body": "ok what i meant is that this should really be saved as MahoutDistributed context :\n\n```\nval sdc = mahoutSparkContext(...) \n```\n\nwhich is already of type SparkDistributedContext (mahout type), right?\n\nand then use to initialize `sdc = this.sdc` and `sc = this.sdc.sc` later... We still seem to be crating DistributedSparkContext twice here, which is probably not a big deal but feels a bit hacky...\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/34008915/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/34062416", "body": "```\n\"@transient implicit val sdc =  org.apache.spark.repl.Main.interp.asInstanceOf[o.a.m.s.s.MahoutSparkILoop].sdc\"\n\"@transient implicit val sc = sdc.sc\"\n```\n\n?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/34062416/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/34216367", "body": "i honestly don't know if spark declared it as implicit. if it does not, we\ndon't have to either i suppose.\n\nthe reason mahout context is implicit is because all top-level routines\nlike drmParallelize(...) look for it.\n\nOn Wed, Jul 8, 2015 at 1:35 PM, Andrew Palumbo notifications@github.com\nwrote:\n\n> In\n> spark-shell/src/main/scala/org/apache/mahout/sparkbindings/shell/MahoutSparkILoop.scala\n> https://github.com/apache/mahout/pull/146#discussion_r34195169:\n> \n> > -         @transient implicit val sdc: org.apache.mahout.math.drm.DistributedContext =\n> > -            new org.apache.mahout.sparkbindings.SparkDistributedContext(\n> > -            org.apache.spark.repl.Main.interp.createSparkContext())\n> > -    _interp.beQuietDuring {\n> >   +\n> > -      // get the spark context, at the same time create and store a mahout distributed context.\n> > -      _interp.interpret(\"\"\"\n> > -         @transient val sc = {\n> \n> @dlyubimov https://github.com/dlyubimov I've made the necessary changes\n> clean up the the redundant creation of a SparkDistributedContext. Thanks\n> alot for the input.\n> \n> You mentioned is that we could declare the SparkContext as implicit val\n> sc = .... Is there a reason that it should be implicit?\n> \n> I've left it as val sc =... for now since that is the way Spark declares\n> is in this method. Thx.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/146/files#r34195169.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/34216367/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/42918641", "body": "You sure? I never quite got those kryo streams. in regular streams, available != eof\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/42918641/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/35591432", "body": "A elementwise scalar is deprecated now. Better use unaryfunc operator. Then things like A+5 can be represented as \n\nOpUnaryFunc(A, x=> x+5). \n\nUnary function mechanism is more generic and affords for unary function fusion optimizations.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/35591432/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/40705610", "body": "maybe we should expose that in DrmLike. \nI so far had a need for this in logical operators (AbstractBinaryOp$classTagK, AbstractUnaryOp$classTagK) for optimizer's needs. so maybe it needs to be promoted up if it turns out every implementation must keep implicit classtag inside.\n\nBut at least the method name probably should be consistent with the existing ones.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/40705610/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/40707550", "body": "yes you can't put implementation into trait. we'd have to make it optional\nfor the trait (=???) and override in the concrete things. Concrete things\nare of course just of two types: logical operators based on the classes i\nalready mentioned, that already have this support, and the checkpoint\nimplementation, which probably does not yet. let me play with your branch a\nlittle to see what we can do.\n\nOn Tue, Sep 29, 2015 at 11:10 AM, Alexey Grigorev notifications@github.com\nwrote:\n\n> In\n> math-scala/src/main/scala/org/apache/mahout/math/drm/logical/CheckpointAction.scala\n> https://github.com/apache/mahout/pull/137#discussion_r40706926:\n> \n> > @@ -44,5 +44,6 @@ abstract class CheckpointAction[K: ClassTag] extends DrmLike[K] {\n> >      case Some(cp) => cp\n> >    }\n> > -  val classTag = implicitly[ClassTag[K]]\n> \n> I remember trying to attach it to DrmLike, but I had some problems because\n> it's a trait\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/137/files#r40706926.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/40707550/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/42560808", "body": "@alexey : BTW is there such a thing as Flink shell?\n\nOn Tue, Oct 20, 2015 at 12:54 AM, Henry Saputra notifications@github.com\nwrote:\n\n> In pom.xml\n> https://github.com/apache/mahout/pull/137#discussion_r42465218:\n> \n> > @@ -121,6 +121,8 @@\n> >      <scala.compat.version>2.10</scala.compat.version>\n> >      <scala.version>2.10.4</scala.version>\n> >      <spark.version>1.3.1</spark.version>\n> > -    <!-- TODO: Remove snapshot dependency when Flink 0.9.1 is released -->\n> > -    <flink.version>0.9-SNAPSHOT</flink.version>\n> \n> Flink 0.9.1 is out so we could remove the SNAPSHOT label I suppose.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/137/files#r42465218.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/42560808/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "jfarrell": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/mahout/issues/comments/44318648", "body": "MAHOUT-1529 not linking to jira as discussed in INFRA-7801\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/44318648/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "nishkamravi2": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/mahout/issues/comments/44354961", "body": "JIRA: MAHOUT-1565 (https://issues.apache.org/jira/browse/MAHOUT-1565)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/44354961/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/44501707", "body": "Removed MAHOUT_OPTS from bin/mahout and bin/mahout.cmd\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/44501707/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47205705", "body": "@dlyubimov please see if this can be merged and closed\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47205705/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "sscdotopen": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/mahout/issues/comments/44743038", "body": "looks good to me, +1 for including this\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/44743038/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45234269", "body": "Its not allowed to redistribute the movielens dataset.\n\nOn 06/05/2014 05:28 PM, Pat Ferrel wrote:\n\n> I could use a little advice here. The epinions and movielens tests in the examples folder. Should they be put into the build?\n> \n> Pros: good example data.\n> Cons: the reading and writing are not parallel and so only work locally. It is easy to change the Spark context to use a cluster but the data still has to be local. These tests would be easier to maintain if they were attached to the ItemSimilarityDriver, which will handle cluster storage and execution and will be maintained better.\n> \n> I'd rather move them out into an ItemSimilarityDriver examples folder and will do this if no one objects. They will not be build tests, obviously, since they take too long.\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/apache/mahout/pull/8#issuecomment-45234064\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45234269/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45646072", "body": "the cooccurrence analysis code should go to the math-scala not spark module, as it is independent of the underlying engine.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45646072/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45732877", "body": "Can someone have a look at the changes? They passed unit-tests.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45732877/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45788881", "body": "I had some of those fixes included in the experiments that I ran last week. I did not measure the impact of individual fixes explicitly, but changes like directly setting an array of row vectors instead of assigning every row (where entries are potentially added with binary search in sequential sparse vectors) seem to have made a huge difference. \n\nI had jobs that seemed to hang for minutes and when I jstacked on the workers, I saw that the code was busily wasting CPU in assigning entries to sparse rows in matrix deserialization... \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45788881/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45921381", "body": "I think the name _colCounts_ is misleading, we should stick to something like numNonZeroElementsPerColumn or so, not sure here.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45921381/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46095388", "body": "It is  a misconception that this method gives approximate answers. It did not do this before my changes and it doesn't do it with the changes. I just made it faster. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46095388/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46488559", "body": "I dont like the `_||` because it looks like a mathematical operation and is not intuitive IMHO. I prefer something that looks like an annotation to the code and is human readable. So I vote for the `.par()` syntax\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46488559/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46753169", "body": "had two minor comments, other than that this is ready to commit IMHO\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46753169/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46753463", "body": "I think we agreed to remove the specific settings and trust the user with setting their hadoop params for themselves.  So I'm +1 for committing this.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46753463/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/mahout/comments/18797338", "body": "Just a question out of curiosity, why have a default threshold of 25% ?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/18797338/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": [{"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13169806", "body": "I don't think we should assign memory to map & reduce tasks ourselves.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13169806/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13425122", "body": "we can remove the .parallel. import\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13425122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13425161", "body": "no @author tags allowed in ASF code\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13425161/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13425174", "body": "remove @author again\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13425174/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13713951", "body": "drmA is already binary here, so we could use colSums\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13713951/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13713968", "body": "drmB is already binary here, so we could use colSums\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13713968/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13714024", "body": "you can trust getNumNonZeroElements.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13714024/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13791157", "body": "What is the purpose of these additional dependenices?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13791157/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13791178", "body": "typo here, must be initialize\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13791178/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13791184", "body": "no star imports please\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13791184/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14049581", "body": "I'm not comfortable with adding all that stuff just for the mini cluster. Could you rewrite your tests to simply use the local filesystem via Hadoop's Path API? That should be sufficient.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14049581/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14049574", "body": "Wouldn't it be more performant to use a RandomAccessSparseVector for the assign and change it into a SequentialAcessSparseVector later?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14049574/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14049576", "body": "similar issue to line 56\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14049576/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "avati": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/mahout/issues/comments/45652833", "body": "Did that. Verified that cleanup works fine.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45652833/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45653931", "body": "I assumed this is part of MAHOUT-1529 itself (which renamed @sc to @sdc). Let me resubmit with MAHOUT-1529 in the commit message?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45653931/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46732343", "body": "Getting much closer to completion. Things which still do not work:\n- seqfile format parser to read/write off HDFS\n- String key support in DRM (int and long works)\n- Fill in implementation of Par() (currently it is a passthrough)\n- more test cases\n- more code comments\n\nExcept the above, the integration is basically working. I have some more performance enhancement changes in mind, but they will happen later. All remaining items are highlighted with /\\* XXX: */ code comment.\n\nI will soon provide details on how others who are interested can run and test this. In the mean time, considering the above caveats, code review and comments are welcome.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46732343/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48712178", "body": "All the points in the previous comments are now completed. This PR is ready for final review.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48712178/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48753061", "body": "> Are the scalatests implemented in the Spark module that covers math-scala\n> code implemented here somewhere? I'd vote against merge untils those are in\n> all in place and passing.\n> \n> Yes, those were the first tests to pass. You can find them in\n> h2o/src/test/org/apache/mahout/math/.\n\nAlso I may have missed it but there should be clear instructions for how to\n\n> build this and run it. This is like a heart transplant. Before you release\n> the patient make sure all systems are working correctly, the DSL is not the\n> whole body. There should at least be some end-to-end pipelines in examples\n> that anyone can run from a local installation.\n> \n> As mentioned in the email, there is a somewhat simple \"how to build and\n> test\" for both local and distributed mode in h2o/README.md. Larger\n> end-to-end pipelines and examples are TBD.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48753061/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48757859", "body": "> The test issue is with the tests in the spark module that actually test\n> stuff in the math-scala module. Remember our discussion about splitting\n> impl from test for cf? There are several things that cannot be tested\n> without the engine in place.\n> \n> I think we are talking about the same tests here. Please compare for\n> yourself -\n> https://github.com/avati/mahout/blob/MAHOUT-1500/h2o/src/test/scala/org/apache/mahout/math/decompositions/MathSuite.scala\n> and\n> https://github.com/avati/mahout/blob/MAHOUT-1500/spark/src/test/scala/org/apache/mahout/math/decompositions/MathSuite.scala\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48757859/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48762753", "body": "On Fri, Jul 11, 2014 at 10:46 AM, Pat Ferrel notifications@github.com\nwrote:\n\n> Exactly, thanks. I see you've done the same for CF also great.\n> \n> But this illustrates the problem. I need to change 50% of the tests in CF\n> cooccurrence because they were not catching a bug. Now the tests live in\n> two places h2o and spark. And unless I change the tests in both places the\n> build will break. The files look virtually identical except for the\n> imports, which is good. If that's true, I wonder if we could we use a Scala\n> macro to keep the code all in one file? We might be able to take the same\n> code and produce two artifacts that are both run at build time. That would\n> reduce the load on devs for this kind of thing.\n> \n> As we discussed on another email thread, I'm independently working on how\n> to move tests back into math-scala. That effort should address this concern\n> I think?\n\n However currently almost all IO code is spark specific. You must have\n\n> re-implemented drm.writeDrm for h2o. Until this is _not_ a\n> re-implementation but is engine neutral we are going to have a growing\n> problem.\n> \n> Why is this a problem? drm.writeDrm() accepts an engine neutral path, like\n> \"hdfs://..\" or \"file://...\" and the content of what gets written is the\n> well defined sequencefile format no matter what the runtime backend is. And\n> as long as the path and file content are engine neutral, why should\n> pipeline code worry how the IO implementation is done? Again, am I missing\n> something?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48762753/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48771908", "body": "On Fri, Jul 11, 2014 at 11:46 AM, Pat Ferrel notifications@github.com\nwrote:\n\n> So you don't see how changing the drm API or storage format will now break\n> code in two places written for two different engines?\n> \n> Changing DRM API? Yes, of course - that is the nature of the beast of\n> supporting multiple implementations behind a single abstraction. Change in\n> abstraction API will need corresponding change in all backends. That's the\n> reason why APIs must be designed carefully so that future changes to them\n> are estimated to be most minimum. I don't see how this by itself qualifies\n> as an objection.\n\nStorage format? Neither spark nor h2o is defining any storage formats. The\ncurrent APIs read and write to sequence files whose formats are very well\ndefined and standardized. As far the they both read and write that common\nformat from engine neutral locations, I don't see any problems at all.\n\nIf I make the change to drm I can fix spark breakage but not h2o. This bit\n\n> of code is extremely stable and super simple for spark so may be a bad\n> example but new code will not be so stable just the opposite. For each new\n> IO operation (SparkContext dependent) or engine tuning (SparkConf\n> dependent) we will grow the problem. The core will become untouchable or\n> breakage will happen in places one engineer will not be able to fix.\n> \n> Can you please provide a more concrete example for both \"make change do\n> drm\" and \"new IO operation (SparkContext dependent)\"? It is hard for me to\n> visualize the problems you are foreseeing without more specifics.\n\nThis is a real issue, I need to change code in math-scala today, already\n\n> have but it isn't pushed. Who knows what that will break in h2o\n> implementations? I will be changing cooccurrence tests, so have to make\n> them in two places. Maybe I can do that but when they diverge further than\n> this example I won't be able to.\n> \n> Well, as long as you are fixing a bug in cf logic, that should be engine\n> independent. However if you are adding a new DRM API or modifying an\n> existing DRM API - that will need corresponding changes in all the engines.\n> There's no getting around that. That's something we all have to live with,\n> no matter what project it is.\n> \n> You guys need to address these issues as if you were supporting two\n> engines for all Mahout code or you will never see what Mahout committers\n> problems will be.\n\nAs I said before, please provide a concrete example of what the issues are.\nI don't know _what_ to fix yet.\n\nThanks\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48771908/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48945841", "body": "@dlyubimov do you intend to merge this soon? I plan to rebase MAHOUT-1500 on top of this.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48945841/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48945917", "body": "Ping.. Anybody?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48945917/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48974295", "body": "If you take a row view of a sparse row matrix, this code is not even used. RandomAccessSparseVector.java has its own NonDefaultIterator() which exploits sparseness and that gets used in a row view of a sparse matrix. Similarly  DenseVector has its NonDefaultIterator() which works on a dense vector is used in a row view of a DenseMatrix.\n\nHowever for \"custom\" matrix (like H2OBlockMatrix in https://github.com/avati/mahout/blob/MAHOUT-1500/h2o/src/main/java/org/apache/mahout/h2obindings/H2OBlockMatrix.java) which is neither inherited from SparseMatrix nor from DenseMatrix, viewRow() returns a MatrixVectorView. In this case no assumption can be made about sparseness about the custom matrix, and this default implementation must fall-back to a dense-like implementation (which guarantees to give correct results no matter the nature of the matrix)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48974295/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48977010", "body": "Please review https://github.com/apache/mahout/pull/29\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48977010/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49118957", "body": "@tdunning Are there any outstanding concerns? Note that this patch is fixing correctness, not performance.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49118957/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49210699", "body": "Thanks @tdunning . Can this PR get merged please?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49210699/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49210898", "body": "@dlyubimov - review/merge appreciated\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49210898/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49400739", "body": "@andrewpalumbo , this code very much feels like it should be in math-scala/src/main/scala/org/apache/mahout/math/classification/*. There is nothing (yet) specific to DRM in this code, so we can keep it out of sparkbindings, at least this part of the algorithm.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49400739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49651759", "body": "I will look into the comments soon\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49651759/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49658870", "body": "Merged\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49658870/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49679402", "body": "Can this PR get some attention as well please? It is already reviewed to be good. So, just requires merge unless a fresh review is desired.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49679402/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49679408", "body": "Can this PR get some attention as well please? It is already reviewed to be good. So, just requires merge unless a fresh review is desired.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49679408/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49679479", "body": "Oops, merge/comment crossed the wire.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49679479/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49679780", "body": "Please note this PR is fully \"working\" now that #29 and #28 are merged. Please consider this for merge.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49679780/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49680913", "body": "rbind() is not yet added, I wasn't even sure if the DRM api would be accepted before I implemented for H2O. I plan to submit a separate PR for rbind().\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49680913/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49681182", "body": "Removed MathSuite. Re-ran mvn test and everything is passing.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49681182/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49681859", "body": "On Mon, Jul 21, 2014 at 4:49 PM, Dmitriy Lyubimov notifications@github.com\nwrote:\n\n> rbind() is not yet added, I wasn't even sure if the DRM api would be accepted before I\n> implemented for H2O. I plan to submit a separate PR for rbind().\n> \n> ok. contingent on this promise, +1 on merging.\n> \n> given magnitude of this review, i suggest 2 more votes/reviewers.\n> Additional non-binding reviews/sign-offs from 0xdata members are also IMO\n> desirable.\n\nI will ping 0xdata members.\n\nAnd IMO we need to resolve whatever concerns Pat may have with this PR.\n\n\nI assumed the concerns were resolved on the dev@ email list (ref: \"Call for\nvote on integrating h2o\")\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49681859/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49685806", "body": "FYI, I am volunteering to keep h2obindings up to date as new DRM api are\nadded. I Dont think any R like or MATLAB like operators are fundamentally\nimpossible on h2o back end.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49685806/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49760146", "body": "We already discussed this till exhaustion on the mailing list about this\nmodel, advantages and how it has been working successfully in other\nprojects. We even agreed if new api is added, then just add an empty stub\nin h2o bindings which throws unimpl. I am also volunteering to keep the\nbindings up to date.\n\nIf you still do not feel like working together again, I shall rest my case\nat this.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49760146/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49807438", "body": "The missing rows seems to be a spark specific characteristic (For e.g all matrices in h2o are fundamentally sparse (they are just called dense if they happen to have all rows))\n\nI think (not completely sure yet), that the canHaveMissingRows could be moved into DrmRddInput instead of DrmLike and have it propagate recursively through the plan DAG as it is evaluated in tr2phys().\n\nEach operator, like At.scala, can inspect srcA.canHaveMissingRows instead of op.canHaveMissingRows. This way DrmLike would not be polluted.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49807438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49807502", "body": "CbindAB had similar problems like A + B\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49807502/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49808739", "body": "Hmm, I think moving canHaveMissingRows to DrmRddInput should work. Unlike nrow and ncol which can signal an error, havemissingrows silently fixes it (i.e \"take extra step\" instead of \"assert consistency\"). So I don't think it has to be known upfront. fixIntConsistency() is anyways called within a physical operator - so we just need to guarantee that the physical operator can see a reliable canHaveMissingRows value.\n\nSince the plan is always evaluated bottom up at the physical layer, even if intermediate operators are optimized out by the logical optimizer, the flag still propagates DrmRddInput to DrmRddInput as long as the physical operators propagate it. So AewScalar would be seeing a trustable srcA.canHaveMissingRows.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49808739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49810561", "body": "OK. If we were looking at just ensuring correctness in the spark engine\nthen it could have been restricted to just physical operators and\nDrmRddInput to propagate up the missingness-flag in the RDD it represents.\nHowever, If we are looking at doing cost estimation in the optimizer in the\nfuture with this info, then you may want to put it into DrmLike.\n\n+1\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49810561/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49813969", "body": "Andrew - I think have been testing on Java 1.7 (can't say for sure until i\nget to my workstation).\n\nOn Tue, Jul 22, 2014 at 4:07 PM, Andrew Palumbo notifications@github.com\nwrote:\n\n> Hi @avati https://github.com/avati, is there something Java 1.7\n> specific in the dependencies here? I'm getting a test failure in the h2o\n> module:\n> \n> Discovery starting.\n> **\\* RUN ABORTED ***\n> java.lang.UnsupportedClassVersionError: water/MRTask : Unsupported\n> major.minor version 51.0\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/21#issuecomment-49813616.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49813969/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49816008", "body": "@andrewpalumbo  - yes, please use 1.7 JRE. Please let me know how your testing goes.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49816008/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49899401", "body": "Thanks for the comments @cliffclick . I'll work on ArrayUtils and Vec.makeZero() usage.\n\nRegarding row labeling, I wanted to keep the operator inner-loop free of if() and else to skip first row optionally (i.e keep the inner loop focussed on just the math.) However, now that I think, it should be possible to filter out the label vec optionally even before entering MRTask, and have both the matrix and row labels within the same Frame.\n\nI'll work on these comments and re-post. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49899401/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49946698", "body": "@cliffclick I have updated with review comments. Note that even though I did away with Tuple2, I am using a new H2ODrm in place. Having the optional row in the same Frame made things very confusing for a reviewer to instantly identify if a given Frame was with row labels or without. H2ODrm has potential future uses (extra members) as well.\n\nI have also made the drmfromHdfs() api fall back to H2O parser (csv etc) if a given file is not a sequence file format. So this opens up the possibility to tweak the job pipeline to use csv files instead of seqfiles and gain in performance and compression.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49946698/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49950406", "body": "I will raise a PR for h2o implementation of rbind as soon as this PR and #21 get merged\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49950406/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50363968", "body": "Ping. Requesting some review/merge attention from the committers.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50363968/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50531008", "body": "Implemented canHaveMissingRows(). All tests are passing. Please let me know if anything else required for merge.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50531008/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50553289", "body": "@dlyubimov Thanks for merging #30. I have now added Rbind operator and refreshed the PR. All tests are passing. Let me know if this is sufficient for merge.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50553289/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50665695", "body": "> So to be clear, this will require 1.7 on all machines from now on? Not\n> just build and running h2o?\n\nIt requires 1.7 only if you are running h2o (because h2o-core artifact is a\n1.7 binary). You can build in 1.6 with/without h2obindings.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50665695/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50795818", "body": "Is there a code style doc used by mahout? I dont use idea (just emacs)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50795818/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50800742", "body": "Where can I find the Sun coding style? Is it this - http://www.oracle.com/technetwork/java/codeconventions-150003.pdf ?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50800742/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50847925", "body": "@dlyubimov the previous batch of commits address the review comments. Dependency on h2o-core SNAPSHOT is replaced with a published RELEASE, and all the code changes have been done.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50847925/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51406299", "body": "Actually I'm not sure if this would work against H2O, as the code is doing \n\n  observationsPerLabel.map(new MatrixOps(_).colSums)\n\nwhich happens on RDD (and not on DRM, because of the implicit conversion). We would need to generic'ize that somehow.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51406299/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51406691", "body": "Oops, I misread.. the map() happens on Array, my bad!\n\nI must admit I do not (yet) know how this code is working on DRM in a distributed way (i.e, to compare two backends)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51406691/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51415479", "body": "Scala 2.11 port of Spark is in progress [https://issues.apache.org/jira/browse/SPARK-1812]\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51415479/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51415697", "body": "Only meant FYI (in case someone is planning anything). Of course we have to wait for release.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51415697/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51826713", "body": "ping.\n\nAny progress?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51826713/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51835301", "body": "resolved pom.xml merge conflict (spark/scala version update)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51835301/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/52412203", "body": "Addressed review comments from Andrew, except the comment on indentation of catch. Even though indentation around catch is not like \"Java standard\", it is consistent with the rest of Mahout code's style. Let me know if you still want to change it only in h2o module.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/52412203/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/52860273", "body": "PING.\n\nRequesting some attention to this PR.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/52860273/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53345246", "body": "The latest push makes this PR runtime compatible with Java 1.6 (depends on h2o-core 0.1.5 which starts Java6 backward compat)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53345246/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53476236", "body": "@andrewpalumbo re-applied the commit. Not sure how it got missed! Thanks for pointing..\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53476236/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53772200", "body": "@andrewpalumbo - does this look good?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53772200/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/54366692", "body": "@andrewpalumbo - Added scaladoc and javadoc comments. I have also included some variable renaming to replace underscore with camelcasing in the same commit (because the javadoc had to use the right parameter name etc.)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/54366692/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/54669875", "body": "@andrewpalumbo OK thanks\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/54669875/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55166855", "body": "@dlyubimov - I am digging into the project history to understand more about the rule/cost framework and what has been done for aggregate/assign for vector-vector. Any pointers for more context in this direction appreciated.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55166855/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55169860", "body": "Is this the document -\nhttps://docs.google.com/document/d/1g1PjUuvjyh2LBdq2_rKLIcUiDbeOORA1sCJiSsz-JVU/edit#\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55169860/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55348705", "body": "On Wed, Sep 10, 2014 at 12:27 PM, Dmitriy Lyubimov <notifications@github.com\n\n> wrote:\n> \n> In terms of matrices i was thinking important factors to interrogate are\n> vector/hash backing; and in case of vector backing, the vector orientation\n> (column-wise, row-wise). That should be enough to choose algos from, at\n> least those written by Ted.\n\nWhich algos (filenames) are you specifically referring to, just so that I\nam understanding the right thing.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55348705/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55354087", "body": "From what I see, the Matrix-Matrix or Matrix-Vector operations which can possibly be cost optimized are:\n- public Matrix assign(Matrix other, DoubleDoubleFunction function)\n- public Matrix assign(Matrix other)\n- public Vector aggregateRows(VectorFunction f)\n- public double aggregate(final DoubleDoubleFunction combiner, final DoubleFunction mapper)\n- public Vector aggregateColumns(VectorFunction f)\n- public Matrix divide(double x)\n- public Matrix minus(Matrix other)\n- public Matrix plus(Matrix other)\n- public Matrix times(double x)\n- public Matrix times(Matrix other)\n- public Vector times(Vector v)\n- public Vector timesSquared(Vector v)\n\nIf \"public void set(int row, double[] data)\" was instead \"public void set(int row, Vector data)\", that too could have been subject to optimization.\n\nAm I even thinking about the right problem here?\n\nAlso, should this optimization problem and jBLAS backend be tied together for completion/merge?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55354087/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14911836", "body": "So, is this a mandatory check/behavior? In h2o engine, the Matrix provided to mapBlock() is an instance of \"H2OBlockMatrix extends AbstractMatrix\", and neither a DenseMatrix nor SparseMatrix. H2OBlockMatrix is a 0-copy virtual Matrix exposing just the partition's data (created at almost no expense), and creates a copy-on-write Matrix only if modified by the blockmapfunction.\n\nSo the above two tests are failing. Can these two be rather moved into sparkbindings test?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14911836/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15150836", "body": "This fixes the immutability problem, but the missing rows still create the following issues:\n- AewScalar: math errors\n- AewB: java exception\n- CbindAB: java exception\n\nAll three are non-trivial to fix (i.e no one liner fixes).\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15150836/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15900010", "body": "Unfortunately variables do not work. See\nhttps://github.com/apache/spark/pull/996#discussion_r15679675 for similar\ndiscussion.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15900010/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14043524", "body": "With the latest git-push, all these tests are passing\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14043524/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15203859", "body": "Ah, I forgot to git-rm this. Let me do that right away.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/15203859/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16741328", "body": "Oops, pushed the camelcase styling as well. Looks like I had accidentally overwrote a couple of commits when switching between workstation and laptop.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/16741328/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/17191513", "body": "@andrewpalumbo - probably no use of storing. I think I wasn't sure about that while coding. Let me remove it.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/17191513/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/21430551", "body": "Ah, patch coming up..\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/21430551/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/21430555", "body": "Please try this patch:\n\ndiff --git a/h2o/src/main/java/org/apache/mahout/h2obindings/H2OHelper.java b/h2o/src/main/java/org/apache/mahout/h2obindings/H2OHelper.java\nindex 294ec7e..c3b1339 100644\n--- a/h2o/src/main/java/org/apache/mahout/h2obindings/H2OHelper.java\n+++ b/h2o/src/main/java/org/apache/mahout/h2obindings/H2OHelper.java\n@@ -329,7 +329,7 @@ public class H2OHelper {\n       Map<Integer,String> rmap = reverseMap(map);\n\n```\n   for (long r = 0; r < m.rowSize(); r++) {\n```\n-        writer.set(r, rmap.get(r));\n- ```\n     labels.chunkForRow(r).set(r, rmap.get(r));\n  ```\n  \n     }\n  \n     writer.close(closer);\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/21430555/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/21430646", "body": "Argh, the formatting got messed up. Please replace \"writer.set(r, rmap.get(r));\" with \"labels.chunkForRow(r).set(r, rmap.get(r));\"\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/21430646/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/21430707", "body": "Please merge https://github.com/apache/mahout/pull/64 to fix this bug.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/21430707/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/22009885", "body": "I am looking into this. Have been a bit busy, will get back on this soon.\n\nOn Wed, Dec 17, 2014, 14:22 Andrew Palumbo notifications@github.com wrote:\n\n> In h2o/src/main/java/org/apache/mahout/h2obindings/H2OHelper.java\n> https://github.com/apache/mahout/pull/32#discussion-diff-22009635:\n> \n> > @@ -327,9 +327,11 @@ public static H2ODrm drmFromMatrix(Matrix m, int minHint, int exactHint) {\n> >        labels = frame.anyVec().makeZero();\n> >        Vec.Writer writer = labels.open();\n> > \n> > ##        Map<Integer,String> rmap = reverseMap(map);\n> > -      for (long r = 0; r < m.rowSize(); r++) {\n> > -        writer.set(r, rmap.get(r));\n> > -      // TODO: fix BUG here... h20 water.fvec.Vec does not accept String values\n> > -      // TODO: need a new distributed data structure for storing String keys.\n> > -      for (int r = 0; r < m.rowSize(); r++) {\n> > -        //writer.set(r, rmap.get(r));\n> > -        labels.chunkForRow(r).set(r, rmap.get(r));\n> \n> looks like the exception is being thrown here:\n> \n> water.fvec.Chunk.set_impl(Chunk.java:189)\n> boolean set_impl (int idx, String str) { throw new IllegalArgumentException(\"Not a String\"); }\n> \n> I've been looking through some of the more recent code and it seems that\n> the Vec.Writer.set( long i, String str) signature has been phased out.\n> \n> We should probably address this as a separate issue since we have to\n> update to a newer h2o-core artifact, and there may be some other issues in\n> there too.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/32/files#r22009635.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/22009885/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775252", "body": "Updated PR, please check https://github.com/avati/mahout/commit/2bc4098b244ef182b29cba53e6eabb1f6524a29d\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775252/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775259", "body": "Not sure why this PR still says 2 commits, there should be 3 now. Can you take the third commit manually?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775259/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775316", "body": "I am somehow not able to make the changes get reflected in this PR. Let me close this and open a new PR fresh.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775316/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775411", "body": "Argh, I blindly replaced only collections! I could have removed Closeables as well, would be trivial. I'm assuming you will be doing it, based on your previous comment.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775411/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775426", "body": "Done / pushed.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775426/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775438", "body": "For some reason there is a delay in GitHub reflecting branch changes in PR.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/27775438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32265450", "body": "Yes, h2o's broadcast is just serialize + multiple unicast (almost like spark's http based broadcast)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/32265450/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33198231", "body": "Is there a strong reason for the unicode right arrow? For me at least, the only place I can see it right is in this github view. It looks garbled in emacs, less, and every other console based tools I use.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33198231/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33203139", "body": "Can we have a comment here explaining the reason behind special-case for 2.0 and 0.5? This will help future code readers. Same comment applies to rest of the similar changes further below.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/33203139/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "tdunning": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/mahout/issues/comments/45804526", "body": "I didn't understand the suggestion.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45804526/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45910978", "body": "Could we have some examples?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45910978/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45913357", "body": "This discussion isn't getting echoed to the mailing list.  I didn't even know it was happening.\n\nI think that a non-zero counter is nice, but it would be better to have a more general general aggregator of somethings.  We have two instances already of this pattern and there will be more (sum of the abs values is common).\n\nWhy not implement a general aggregator?  THis is different from our current aggregateColumns because that function is not parallelizable.\n\nSomething like def columnAggregator(combiner, mapper) is what I am aiming for.  Positive counter would be m.columnAggregator(_ + _, _ > 0)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45913357/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45934491", "body": "I hate abbreviations.  If you are asking about naming, use the long name.\n\nIf you can assure binary, then going with what we already have would be\nnice.\n\nOn Thu, Jun 12, 2014 at 10:34 AM, Pat Ferrel notifications@github.com\nwrote:\n\n> numNonZeroElementsPerColumn? vs colSums?\n> \n> OK\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/12#issuecomment-45923020.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/45934491/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46383131", "body": "Why the fancy syntax?\n\nWhy not a function or method notation?\n\nAt the very least, why use punctuation in the name rather than spelling it\nout?\n\nOn Tue, Jun 17, 2014 at 1:05 PM, Dmitriy Lyubimov notifications@github.com\nwrote:\n\n> @Sebastian https://github.com/Sebastian, you wanted this, can you take\n> a look oif this serves the purpose?\n> \n> @tdunning https://github.com/tdunning, spark allows finer splits\n> (minSplits parameter) on hdfs loading and on intermediate products. In our\n> case, this patch adds minSplits parameter to fromHDFS() method to allow to\n> specify finer-than-iniitial HDFS load parallelism explicitly.\n> \n> In addition to that, parallelism can also be re-adjusted explicitly or\n> automatically using the operators i mentioned.\n> \n> Automatic parallellism adjustment uses current parallelism and spark\n> cluster default parallelisms as guidelines (i.e. it assumes tasks are\n> already of somewhat reasonable size, per above).\n> \n> i.g. loading matrix from hdfs\n> \n> val a = drmFromHDFS(path, minsplits=100)\n> \n> to make sure number of partitions (i.e. map tasks) is at least 100 on\n> matrix load.\n> \n> This mainly has to do with the fact that algebraic flops often grow\n> asymptotically faster than the input size, so in some cases default cluster\n> size is the best guideline in terms of load balancing.\n> \n> (in fact, since in Spark tasks are super cheap to run, i think it is ok to\n> split 400%, 500% of default parallelism to achieve more even load).\n> \n> Optimizer computes (\"predicts\") parallelism of shuffles based on existing\n> parallelism of products, but after a long chain of operations this\n> predictions may deteriorate (or just not work well for whatever reason). in\n> this case we may readjust it explicitly at any checkpoint of expression:\n> \n>  val drmAtA = (drmA.t %*% drmA) min_|| 100\n> \n> will make sure that this product will be shuffled into at least 100\n> partitions. Analogously,\n> \n> val drmAtA = (drmA.t %*% drmA) exact_|| 100\n> \n> will make sure that this product will be shuffled into exactly 100\n> partitions.\n> \n> Readjusting parallelism also requires implicit optimizer checkpoint, e.g.\n> the latter would be equivalent to\n> \n> val drmAtA = (drmA.t %*% drmA).checkpoint() exact_|| 100\n> \n> Repartitioning does not necessarily invoke shuffle task (e.g. if a\n> partition can be just split into two, it can be run as a map-only thing).\n> \n> This patch also does not contain optimizations causing potentially\n> changing parallelism of any previous operations in the physical pipeline.\n> This is something left todo, something to think about.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/13#issuecomment-46358010.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46383131/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46396097", "body": "Yes.  \n\nBut I was talking about the gratuitous use of non-alpha characters. Excessive use of operator overloading is also a bit of a problem. \n\nJust because you can doesn't mean you should. \n\nSent from my iPhone\n\n> On Jun 17, 2014, at 17:58, Dmitriy Lyubimov notifications@github.com wrote:\n> \n> e.g. one can write things like A.t.%*%(A).exact_||(100)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46396097/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46509902", "body": "On Tue, Jun 17, 2014 at 10:43 PM, Dmitriy Lyubimov <notifications@github.com\n\n> wrote:\n> \n> Ted, are you ready to help with a concrete alternative? This is a very\n> small issue compared to even the patch, lets build a list of alternatives\n> and vote. But lets get it done\n> \n> My additional variants\n> \n> minSplits,...\n> minPar, exactPar, autoPar (consitent with scala's collection.par())\n> \n> To give something to vote down for Ted\n> \n> > =|| :=||\n> > :||=\n> \n> Not ok with me\n> \n> minParts\n> minParallelism\n> minPartitions\n> repartition\n> reshuffle\n> and other do-something kind\n\nminSplits is fine by me.  I strongly discourage abbreviations because they\nare hard for non-native English speakers to generate well and hard for\nnon-native English speakers to understand.\n\nActually, they are often very hard for me to understand and I claim to be a\nnative English speaker some days.\n\nIf more than nine letters is too hard to type (even with an IDE to help\nyou) then minSplits seems to be reasonable common ground.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46509902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46509984", "body": "Uh... what happened to waiting a bit for feedback?\n\nOn Wed, Jun 18, 2014 at 1:26 PM, Dmitriy Lyubimov notifications@github.com\nwrote:\n\n> Thanks.\n> \n> +1 on par(...) too. Great, i will engineer the fix and commit then.\n> \n> On Wed, Jun 18, 2014 at 1:20 PM, Sebastian notifications@github.com\n> wrote:\n> \n> > I dont like the _|| because it looks like a mathematical operation and\n> > is\n> > not intuitive IMHO. I prefer something that looks like an annotation to\n> > the\n> > code and is human readable. So I vote for the .par() syntax\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub\n> > https://github.com/apache/mahout/pull/13#issuecomment-46488559.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/13#issuecomment-46489240.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/46509984/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48968797", "body": "Woops.  Thought this was so clear that it was done.\n\nYes.  Needful.  I will look at the delta instantus.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48968797/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48969184", "body": "I don't think that this goes quite far enough.\n\nFor instance, if you take a row view of a sparse row matrix, you can actually have a really truly sparse iterator.  For other horizontal strides you can have something like that.  This may already be handled by our current sparse matrices.\n\nFor vertical strides, there may not be anything that is all that much better than what this delta provides.  \n\nDoes this patch actually make things faster?  More than just tightening the inner loop would explain?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/48969184/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49189402", "body": "No further comments.  It looks like a good thing.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49189402/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50225044", "body": "This looks pretty good.  I have a few questions:\n\na) is this backwards compatible?  Is there a test to demonstrate this?\n\nb) what documentation impact is there?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50225044/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55193807", "body": "That may be good.  Or not.  In many sparse matrices, different decisions should be applied in different parts of the matrix as the sparsity changes.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55193807/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55364240", "body": "On Thu, Sep 11, 2014 at 7:16 PM, Anand Avati notifications@github.com\nwrote:\n\n> From what I see, the Matrix-Matrix or Matrix-Vector operations which can\n> possibly be cost optimized are:\n> - public Matrix assign(Matrix other, DoubleDoubleFunction function)\n> - public Matrix assign(Matrix other)\n> - public Vector aggregateRows(VectorFunction f)\n> - public double aggregate(final DoubleDoubleFunction combiner, final\n>   DoubleFunction mapper)\n> - public Vector aggregateColumns(VectorFunction f)\n> - public Matrix divide(double x)\n> - public Matrix minus(Matrix other)\n> - public Matrix plus(Matrix other)\n> - public Matrix times(double x)\n> - public Matrix times(Matrix other)\n> - public Vector times(Vector v)\n> - public Vector timesSquared(Vector v)\n> \n> If \"public void set(int row, double[] data)\" was instead \"public void\n> set(int row, Vector data)\", that too could have been subject to\n> optimization.\n> \n> Am I even thinking about the right problem here?\n\nI think you are definitely thinking about the right starting point.\n\n> Also, should this optimization problem and jBLAS backend be tied together\n> for completion/merge?\n> \n> Hmm....\n\nCertainly they can be done in either order.  If optimization comes after\njBLAS then there will be some work to clean up jBLAS support for the\noptimizer.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/55364240/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13782027", "body": "Using the vector aggregation framework will be very inefficient here.  We should either use Seb's suggestion or add properly scalable aggregation that doesn't depend on getting a vector view of a column.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13782027/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13783816", "body": "The issue I have is with the rowAggregation and columnAggregation API.  It enforces row by row evaluation.  A map-reduce API could evaluate in many different orders and could iterate by rows or by columns for either aggregation and wouldn't require the a custom VectorFunction for simple aggregations.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13783816/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "gcapan": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/mahout/issues/comments/47588026", "body": "I'll commit this, since we all agree.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/47588026/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50625655", "body": "Tests pass for me for various profiles, and the code looks good. I am a supporter of engine-agnostic architecture and separation of actual algorithms from backends, and multiple backends (in addition both Spark and H2O being very promising platforms) would force us implement generic solutions for data preprocessing, vectorization, machine learning and big data mining. In summary, my vote is +1 for that contribution. \n\nPS: Not H2O specific, but wanted to add here: I believe the next step should be standardizing minimal Matrix I/O capability (i.e. a couple file formats other than [row_id, VectorWritable] SequenceFiles) required for a distributed computation engine, and adding data frame like structures those allow text columns.  \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50625655/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24001140", "body": "You're right, I'll fix it.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24001140/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "cliffclick": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/mahout/issues/comments/49894450", "body": "This is a very basic port, focused on correctness & completeness, with no effort for performance.\nExpectation Setting: There's easy 2x to 10x speedups in most of the operator inner loops.  The HDFS sequence-file reader/writers are single-threaded-single-node; H2O's internal CSV reader will be easily 100x faster.\nPerformance work should be in later commits.\n\nMinor comments:\nLots of places, esp reduce() calls, could/should call ArrayUtils.add(this,that) instead of a loop over the arrays being added.\n\nH2OHelper.empty_frame looks a ton like it should call \"Vec.makeZero()\" in a loop instead of hand rolling Vecs of zeros; there's a version which will take a hand-rolled layout.  This call probably should move into Frame class directly.\n\nThe technique for row-labeling seems... awkward at best.  Or at least I'm reading that to be the purpose of using Tuple2.  I think this design needs more exploring - e.g. insert a row-column in front of the \"normal\" Frame columns, and teach the follow-on code to skip 1st column.  Note that many datasets have non-numeric cols (e.g. name, address) that cannot participate in math ops, and so most H2O algos already carry forward a notion of a set of columns being worked on.\n\nCliff\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49894450/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49948794", "body": "Looks good to me.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/49948794/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "wobu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/mahout/issues/comments/50226413", "body": "a) it is. We could add an additonal test case for this but in my opinion this isn't necessary.\n\nb) In the documentation (code and [homepage](http://mahout.apache.org/users/basics/creating-vectors-from-text.html)) i didn't find any hint which possibilities exactly you have how to provide the input. So this should be added, yes.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/50226413/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "roengram": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/mahout/issues/comments/51151739", "body": "Thanks for your comment.\nI added MAHOUT_LOCAL flag. If this is set to 1, the original routine runs. Otherwise, workload is copied to HDFS.\nI've tested when MAHOUT_LOCAL=0, but couldn't test MAHOUT_LOCAL=1. It gives me the error I described in the Jira (actually this is why I started this patch in the first place!). Can somebody test this script for MAHOUT_LOCAL=1?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51151739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51574907", "body": "Thank you for your comment.\nI've changed MAHOUT_LOCAL checking so that its existence, not its value, is checked.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51574907/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51575364", "body": "Thanks for your comment.\nI've changed 'hadoop dfs' to 'hadoop fs'.\nAs far as I know, there will be only one clustering result directory. Because we do not know in advance what the exact name will be, we need to use wildcard.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51575364/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "FRosner": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/mahout/issues/comments/51513451", "body": "Is there only one directory that matches the wildcard? What happens if there are multiple ones?\n\n---\n\n(please excuse my questions if they are dumb, I am new to Mahout)\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51513451/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51578323", "body": "I was just wondering about this dfs and fs thing. Not sure if there is some backwards compatibility issue with fs. Maybe some Hadoop specialist can comment on this one :P\n\nOk if you are sure that there will only be exactly one result directory, the changes look fine to me. Guess it solved the problems you were facing :)\n\n:+1: from me\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/51578323/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "srowen": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/mahout/issues/comments/53716190", "body": "I may still have the commit bit for ASF git, but can't merge the pull request myself. (I also realize I'm not yet sure if there's another step? will asfbot merge back to ASF git if merged here?)\n\nAnyone who can merge this is welcome to do so!\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53716190/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53746682", "body": "Ah right, should have RTFM. Thanks! When you say \"beyond what's needed\" were you commenting on the PR, or on the docs? Just checking whether you meant you wanted to discuss the change more. \n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/issues/comments/53746682/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "hasonhai": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/mahout/comments/10410504", "body": "Hi,\nThank you very much for the patch! I succeeded building mahout on my machine.\n\nBut I try some example and it didn't work. Can anybody help me checking these:\n- k-Means Clustering: # mahout org.apache.mahout.clustering.syntheticcontrol.kmeans.Job\n- Canopy Clustering : # mahout org.apache.mahout.clustering.syntheticcontrol.canopy.Job\n- Fuzzy k-Means Clustering: # mahout org.apache.mahout.clustering.syntheticcontrol.fuzzykmeans.Job\n\nThe console told that the path to \"cluster\" doesn't exist. Even though it was always there. I could not find any help from other sources.\n\n```\n15/03/26 12:15:07 INFO mapreduce.Job: Task Id : attempt_1426848955524_0062_m_000000_2, Status : FAILED\nError: java.lang.IllegalStateException: output/clusters-0\n        at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable.iterator(SequenceFileDirValueIterable.java:78)\n        at org.apache.mahout.clustering.classify.ClusterClassifier.readFromSeqFiles(ClusterClassifier.java:208)\n        at org.apache.mahout.clustering.iterator.CIMapper.setup(CIMapper.java:44)\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n       at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n       at java.security.AccessController.doPrivileged(Native Method)\n       at javax.security.auth.Subject.doAs(Subject.java:415)\n       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n       at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.io.FileNotFoundException: File output/clusters-0 does not exist\n        at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:376)\n        at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1485)\n        at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1525)\n        at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:570)\n        at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1485)\n        at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1525)\n        at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator.<init>(SequenceFileDirValueIterator.java:70)\n        at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable.iterator(SequenceFileDirValueIterable.java:76)\n        ... 10 more\n\n15/03/26 12:15:16 INFO mapreduce.Job:  map 100% reduce 0%\n15/03/26 12:15:17 INFO mapreduce.Job:  map 100% reduce 100%\n15/03/26 12:15:17 INFO mapreduce.Job: Job job_1426848955524_0062 failed with state FAILED due to: Task failed task_1426848955524_0062_m_000000\nJob failed as tasks failed. failedMaps:1 failedReduces:0\n\n15/03/26 12:15:17 INFO mapreduce.Job: Counters: 9\n        Job Counters\n                Failed map tasks=4\n                Launched map tasks=4\n                Other local map tasks=3\n                Rack-local map tasks=1\n                Total time spent by all maps in occupied slots (ms)=23087\n                Total time spent by all reduces in occupied slots (ms)=0\n                Total time spent by all map tasks (ms)=23087\n                Total vcore-seconds taken by all map tasks=23087\n                Total megabyte-seconds taken by all map tasks=23641088\nException in thread \"main\" java.lang.InterruptedException: Cluster Iteration 1 failed processing output/clusters-1\n        at org.apache.mahout.clustering.iterator.ClusterIterator.iterateMR(ClusterIterator.java:183)\n        at org.apache.mahout.clustering.kmeans.KMeansDriver.buildClusters(KMeansDriver.java:224)\n        at org.apache.mahout.clustering.kmeans.KMeansDriver.run(KMeansDriver.java:147)\n        at org.apache.mahout.clustering.syntheticcontrol.kmeans.Job.run(Job.java:135)\n        at org.apache.mahout.clustering.syntheticcontrol.kmeans.Job.main(Job.java:60)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\n        at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:152)\n        at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n```\n\nThank a lot if anybody can help.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/10410504/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "sslavic": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/mahout/comments/10684431", "body": "Patch release 0.10.1 is possible, only as bug fix for 0.10.0. If/when needed, we can create necessary branch for the bug fix, from 0.10.0 tag.\nNext minor release will be 0.11.0 because many of the planned tickets are either major new features (Flink support) or breaking backward compatibility, e.g. artifact name changes like MAHOUT-1680 and MAHOUT-1681 or dependency changes in MAHOUT-1685.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/10684431/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/comments/10708902", "body": "directory names of submodules do not match artifactId, e.g. they do not have mahout prefix\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/10708902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/mahout/comments/10905269", "body": "Wasn't it agreed that branch 0.10.x is meant for next bugfix release and master for future major changes?\n\nVersion in POM files was not changed.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/10905269/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "AddictedCS": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/mahout/comments/18771603", "body": "After I've compiled the sources from latest master branch and run the `SimilarityAnalysis` on the dataset I'm using for quite some time I got the following issue:\n\n```\njava.lang.OutOfMemoryError: Java heap space\n    at org.apache.mahout.math.DenseMatrix.<init>(DenseMatrix.java:66)\n    at org.apache.mahout.sparkbindings.drm.package$$anonfun$blockify$1.apply(package.scala:68)\n    at org.apache.mahout.sparkbindings.drm.package$$anonfun$blockify$1.apply(package.scala:53)\n    at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:766)\n    at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:766)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n    at org.apache.spark.scheduler.Task.run(Task.scala:85)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n```\n\nWas suprised to see `DenseMatrix` in the stack trace. Didn't have time to debug it thoroughly I just downgraded to release 0.12.2 and the issue was gone.\n\nSo taking into account there are not that many commits after 0.12.2 you may want to recheck if the change for dense vs sparse matrix works as expected. \n\nJust a note - the dataset I was using is indeed sparse. It's a 100k matrix, so OOTM is expected if DenseMatrix is used.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/comments/18771603/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "HuangXiaomeng": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13791266", "body": "It's just for using  minidfscluster to test HadoopFSDatamodel\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13791266/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13791763", "body": "Thanks, modified it.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/13791763/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14059670", "body": "Hi, sscdotopen\nThanks for your comment. Your suggestion is very valuable. I have remove the dependencies of mini cluster. And refactor the tests to use RawLocalFileSystem.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/14059670/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "hamnis": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/mahout/pulls/comments/23590698", "body": "for 1.2.0 they wont run, so this was to future proof upgrade to 1.2.0. Maybe could have bumped to 1.2.0 directly?\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/23590698/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/23651449", "body": "This was done since the tests failed with an error message similar to:\n\"Please enable this property to make tests work\".\n\nCurrently using my phone, so can't check exact error message.\n\nTo reproduce: set spark version to 1.2.0 and run tests.\nOn 27 Jan 2015 21:09, \"Dmitriy Lyubimov\" notifications@github.com wrote:\n\n> In\n> spark/src/test/scala/org/apache/mahout/sparkbindings/test/DistributedSparkSuite.scala\n> https://github.com/apache/mahout/pull/71#discussion_r23638378:\n> \n> > @@ -39,6 +39,7 @@ trait DistributedSparkSuite extends DistributedMahoutSuite with LoggerConfigurat\n> >            .set(\"spark.kryoserializer.buffer.mb\", \"15\")\n> >            .set(\"spark.akka.frameSize\", \"30\")\n> >            .set(\"spark.default.parallelism\", \"10\")\n> > -          .set(\"spark.driver.allowMultipleContexts\", \"true\")\n> \n> @hamnis https://github.com/hamnis, could you please explain what is\n> significance of this parameter -- \"allowMultipleContexts\" in 1.2.0 so that\n> tests won't run?\n> \n> As far as i remember, current head is using only one context (per\n> suite).so next suite should run another context, but by that time the\n> previous one should already be gone.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/mahout/pull/71/files#r23638378.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/23651449/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24700613", "body": "yes.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24700613/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24700771", "body": "it already exits here: https://github.com/apache/mahout/blob/master/math-scala/src/main/scala/org/apache/mahout/math/drm/RLikeDrmOps.scala#L137. if you upgrade to 2.11 this would crash. and both are imported into scope. Why this does not crash on 2.10 i have no idea.\nNot sure which one that should be removed, but I chose this one.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/24700771/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "tedyu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/mahout/pulls/comments/29378706", "body": "Add @param for biDi\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/29378706/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/29379511", "body": "This line can be dropped.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/29379511/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "alexeygrigorev": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/mahout/pulls/comments/40706926", "body": "I remember trying to attach it to DrmLike, but I had some problems because it's a trait\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/40706926/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/42584189", "body": "I also know there were plans to do Mahout DSL support for the shell, but I'm not sure if it got any further.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/42584189/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "hsaputra": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/mahout/pulls/comments/42465218", "body": "Flink 0.9.1 is out so we could remove the SNAPSHOT label I suppose.\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/42465218/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/42561078", "body": "Flink has Scala shell:\nhttps://ci.apache.org/projects/flink/flink-docs-master/apis/scala_shell.html\n", "reactions": {"url": "https://api.github.com/repos/apache/mahout/pulls/comments/42561078/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}}}}