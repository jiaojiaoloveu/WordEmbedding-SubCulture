{"_default": {"1": {"tillrohrmann": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/776af4a882c85926fc0764b702fec717c675e34c", "message": "[FLINK-8462] [flip6] Filter invalid heartbeat timeouts in TaskExecutor\n\nThis commit properly stops the heartbeating of disconnected RMs and additionally\nignores outdated heartbeat timeouts for old RM connections out.\n\nThis closes #5318."}, {"url": "https://api.github.com/repos/apache/flink/commits/9a0399c888ae75e96c7719b8f079956462023348", "message": "[hotfix] Add not null check to message headers in AbstractRestHandler"}, {"url": "https://api.github.com/repos/apache/flink/commits/016118026ca96abb691b236ca7d08db94c93684a", "message": "[FLINK-8463] [rest] Remove blocking of IO executor in RestClient#submitRequest\n\nInstead of waiting on the ChannelFuture we register a ChannelFutureListener which\nis notified when the channel has been established. This unblocks IO executor threads\nin the RestClient.\n\nThis closes #5319."}, {"url": "https://api.github.com/repos/apache/flink/commits/3920e9a47f7eea09335d644d55053863136a9a9b", "message": "[FLINK-8368] [rest] Simplify SubtaskExecutionAttemptDetailsHandlerTest#testHandleRequest"}, {"url": "https://api.github.com/repos/apache/flink/commits/d8b2c0febfdb1c15e4251247a87c1c606ea2a284", "message": "[FLINK-8368] Migrate org.apache.flink.runtime.rest.handler.legacy.SubtaskExecutionAttemptDetailsHandler to new a REST handler that registered in WebMonitorEndpoint\n\n[FLINK-8368] Add attempts path info that is missing in SubtaskExecutionAttemptDetailsHeaders\n\nThis closes #5270."}, {"url": "https://api.github.com/repos/apache/flink/commits/3c99ae8959f69325bb9b7d810b41c60e42e602c5", "message": "[FLINK-8420] [flip6] Recognize TimeoutException in RetryingRegistration\n\nA timeout exception will trigger an exponential backoff wrt the connection timeout.\nThis will guarantee that we don't overload the network with connection requests but\nalso to quickly connect to a newly available target.\n\nThis closes #5286."}, {"url": "https://api.github.com/repos/apache/flink/commits/ed0716ac710387979735140d18a598240b43085d", "message": "[hotfix] Log failure message only if Yarn application truly failed"}, {"url": "https://api.github.com/repos/apache/flink/commits/dbe0e8286d76a5facdb49589b638b87dbde80178", "message": "[FLINK-8119] [flip6] Wire correct Flip6 components in Flip6YarnClusterDescriptor\n\nLet the Flip6YarnClusterDescriptor create a RestClusterClient as ClusterClient.\nMoreover, this commit makes the YarnResourceManager register under the REST port\nat Yarn.\n\nThis closes #5234."}, {"url": "https://api.github.com/repos/apache/flink/commits/7d986ce0482562efd90fec416c4b2f4638a4058a", "message": "[FLINK-8348] [flip6] Print help for DefaultCLI\n\nThis closes #5233."}, {"url": "https://api.github.com/repos/apache/flink/commits/38d3720863c6187153174d0df57fc414b0cf8e96", "message": "[FLINK-8347] [flip6] Make cluster id used by ClusterDescriptor typesafe\n\nThe ClusterDescriptor uses a typed cluster id for the ClusterClient retrieval.\nMoreover, the ClusterClient and the CustomCommandLine are typed accordingly.\n\nThis closes #5232."}, {"url": "https://api.github.com/repos/apache/flink/commits/2ce64e791b3a2e2b8a8e4ac774f2ca45da5660dc", "message": "[hotfix] Add help command to FlinkYarnSessionCli"}, {"url": "https://api.github.com/repos/apache/flink/commits/402499f06a4b590ac47df64ecc01055c06b0399b", "message": "[FLINK-8349] [flip6] Remove Yarn specific commands from YarnClusterDescriptor\n\nRemove Yarn specific commands from YarnClusterDescriptor. This is a preparational\nstep to make the FlinkYarnSessionCli work with the Flip-6 RestClusterClient.\n\nThis closes #5229."}, {"url": "https://api.github.com/repos/apache/flink/commits/10e900b25ac03876d3f9e78f260d48efe6b9d853", "message": "[FLINK-8342] [flip6] Remove generic type parameter from ClusterDescriptor\n\nThis closes #5228."}, {"url": "https://api.github.com/repos/apache/flink/commits/d7e9dc1931f1f1cedbfee12aebe34dd76e9bac10", "message": "[FLINK-8341] [flip6] Remove not needed options from CommandLineOptions\n\nThis closes #5227."}, {"url": "https://api.github.com/repos/apache/flink/commits/30011b9b110aad0e1c28e7e0a025b73986781a72", "message": "[FLINK-8340] [flip6] Remove passing of Configuration to CustomCommandLine\n\nSince the Configuration does not change over the lifetime of a CustomCommandLine,\nwe can safely pass it as a constructor argument instead of method argument.\n\nThis closes #5226."}, {"url": "https://api.github.com/repos/apache/flink/commits/e2f1ba92decdb27f3aea4e21a7cad7dcc98cea1a", "message": "[FLINK-8339] [flip6] Let CustomCommandLine return ClusterDescriptor\n\nInstead of directly retrieving or deploying a Flink cluster, the\nCustomCommandLine now only returns a ClusterDescriptor which can be used\nfor these operations. This disentangles the ClusterDescriptor and the\nCustomCommandLine a bit better supporting a proper lifecycle management\nof the former.\n\nThis closes #5225."}, {"url": "https://api.github.com/repos/apache/flink/commits/aff43768f3285a5f2bc5593369a7fec3ed77a2af", "message": "[FLINK-8338] [flip6] Make CustomCommandLines non static in CliFrontend\n\nThis commit changes how CustomCommandLines are registered at the CliFrontend.\nHenceforth, the CliFrontend is initialized with the set of CustomCommandLines\ninstead of registering them statically. This improves maintainability and\ntestability.\n\nThis closes #5224."}, {"url": "https://api.github.com/repos/apache/flink/commits/12396f19851e74310c9b5f28870a8de9794511fc", "message": "[FLINK-8333] [flip6] Separate deployment options from command options\n\nThis commit separates the parsing of command options and deployment options into two\nsteps. This makes it easier to make the CustomCommandLines non-static.\n\nMoreover, this commit moves the CliFrontend into the cli sub package.\n\nThis closes #5220."}, {"url": "https://api.github.com/repos/apache/flink/commits/c2492e9b220c6c9a64b47bcdc76a2194d9f4d669", "message": "[FLINK-8332] [flip6] Move savepoint dispose into ClusterClient\n\nMove the savepoint disposal logic from the CliFrontend into the ClusterClient. This gives\na better separation of concerns and allows the CliFrontend to be used with different\nClusterClient implementations.\n\nThis closes #5219."}, {"url": "https://api.github.com/repos/apache/flink/commits/156b8935ef76eb53456cea1d40fd528ccefa21d8", "message": "[FLINK-8329] [flip6] Move YarnClient to AbstractYarnClusterDescriptor\n\nMoves the YarnClient from the YarnClusterClient to the AbstractYarnClusterDescriptor.\nThis makes the latter responsible for the lifecycle management of the client and gives\na better separation of concerns.\n\nThis closes #5216."}, {"url": "https://api.github.com/repos/apache/flink/commits/2ce5b98da04cb3850ff91757cc4b74a98b8ce082", "message": "[FLINK-8328] [flip6] Move Yarn ApplicationStatus polling out of YarnClusterClient\n\nIntroduce YarnApplicationStatusMonitor which does the Yarn ApplicationStatus polling in\nthe FlinkYarnSessionCli. This decouples the YarnClusterClient from the actual communication\nwith Yarn and, thus, gives a better separation of concerns.\n\nThis closes #5215."}, {"url": "https://api.github.com/repos/apache/flink/commits/63343fb8e1920051c537027b14a2fc2d9856f84c", "message": "[hotfix] [travis] Set distinct cache for Flip-6 build profiles"}, {"url": "https://api.github.com/repos/apache/flink/commits/2ee4d06ac879e955df0648dd0988f081a617e077", "message": "[FLINK-7949] Add unit test for AsyncWaitOperator recovery with full queue"}, {"url": "https://api.github.com/repos/apache/flink/commits/8353123b39ea49cc5b3bd9d324972bf6ae02f2a6", "message": "[FLINK-8404] [tests] Mark Flip-6 tests with Flip6 category annotation\n\nMarks all existing Flip-6 test cases with the Flip6 category annotation. That\nway they are only run if the Flip-6 test profile is active.\n\nThis closes #5278."}, {"url": "https://api.github.com/repos/apache/flink/commits/d42759d0a17d17adc5af9c26b939d431acba5b08", "message": "[hotfix] [tests] Fix PageRankITCase, AggregatorsITCase and DataSinkITCase to use fresh result path"}, {"url": "https://api.github.com/repos/apache/flink/commits/a6ee040c5dbf8b93e94dcbf353e0db897ff8fb29", "message": "[hotfix] [tests] Fix JavaProgramTestBase to reset MiniClusterResource#TestEnvironment"}, {"url": "https://api.github.com/repos/apache/flink/commits/244f03f363a6eea709cd45f8e9f495f0ac4eca62", "message": "[hotfix] [tests] Refactor TypeHintITCase to extend AbstractTestBase"}, {"url": "https://api.github.com/repos/apache/flink/commits/98afd1de748a6e19db54d2b9ab54f92f0472709c", "message": "[FLINK-7918] Run AbstractTestBase tests on Flip-6 MiniCluster\n\nThis closes #5095."}, {"url": "https://api.github.com/repos/apache/flink/commits/63d4819e197b1df1651157fd8f86c8ca0540d0b1", "message": "[FLINK-8393] [flip6] Reconnect to last known JobMaster when connection is lost\n\nIn case of a heartbeat timeout or a disconnect call, the TaskExecutor tries to\nreconnect to the last known JobMaster location.\n\nThis closes #5267."}, {"url": "https://api.github.com/repos/apache/flink/commits/4afd445b36f3218d23e1c44e73868a4b12c9be52", "message": "[hotfix] Refactor JobMasterTest to avoid using Mockito"}, {"url": "https://api.github.com/repos/apache/flink/commits/91f2a8d9b6bafae5855254977d1f38e1b89f2389", "message": "[hotfix] Add JavaDocs to OnCompletionActions"}, {"url": "https://api.github.com/repos/apache/flink/commits/79854697d130b9650b2aa679d263b239a2c10521", "message": "[hotfix] Enable checkpointing RPC calls"}, {"url": "https://api.github.com/repos/apache/flink/commits/9541afd2c53fc55cee7f0f45b4e16377803f1388", "message": "[FLINK-8389] [flip6] Release all slots upon closing of JobManager\n\nThis closes #5265."}, {"url": "https://api.github.com/repos/apache/flink/commits/ff67094671e51679ea79e8e518f18348923feb06", "message": "[hotfix] Add retrieval of key sets to DualKeyMap"}, {"url": "https://api.github.com/repos/apache/flink/commits/057edf9e28b656401b985069ebcc428ab55e5fed", "message": "[FLINK-7910] [tests] Generalize Test(Stream)Environment to use JobExecutor\n\nThis commit introduces the JobExecutor interface which abstracts the actual mini cluster\nfrom the Test(Stream)Environment. By letting the Flip-6 MiniCluster as well as the\nFlinkMiniCluster implement this interface, we can run all test base jobs either on the\nFlip-6 mini cluster or on the current mini cluster.\n\nThis closes #4897."}, {"url": "https://api.github.com/repos/apache/flink/commits/51a278778c3536aa9f5030a8f43a7faea6889992", "message": "[FLINK-8392] [rpc] Let termination future be completed by AkkaRpcActor#postStop\n\nRevert the changes introduced by FLINK-7754. An RpcEndpoint's termination future is now\ncompleted from the AkkaRpcActor#postStop method.\n\nThis closes #5266."}, {"url": "https://api.github.com/repos/apache/flink/commits/11287fbf61cd29d0d20ac8f97884d2c08886f71d", "message": "[hotfix] Remove unnecessary exception catching in StreamingProgramTestBase"}, {"url": "https://api.github.com/repos/apache/flink/commits/b90210e3712a54ad85a33dfc308a03e0c4a2a250", "message": "[FLINK-7909] Replace StreamingMultipleProgramsTestBase by AbstractTestBase\n\nThe AbstractTestBase fully subsumes the functionality of the\nStreamingMultipleProgramsTestBase since it now is the most general test base\nfor streaming and batch jobs. As a consequence, we can safely remove the\nStreamingMultipleProgramsTestBase and let all corresponding tests extend from\nAbstractTestBase.\n\nThis closes #4896."}, {"url": "https://api.github.com/repos/apache/flink/commits/3c5c8325b6641854fd86596ffe1fc5641a6757c2", "message": "[FLINK-7909] Unify Flink test bases\n\nIntroduce a MiniClusterResource which is used by the AbstractTestBase to start\nand shut down a FlinkMiniCluster. Additionally, this resource registers the proper\nStream- and ExecutionEnvironment which is now the only way for tests to start\njobs. This change will thus allow to centrally control which FlinkCluster will\nbe started for all test bases."}, {"url": "https://api.github.com/repos/apache/flink/commits/a03cdfabe6eab48509b42a6831a21e488b9c3e80", "message": "[FLINK-7904] Enable Flip6 build profile on Travis\n\nThis adds a new Travis build matrix entry which runs the Flip-6 build profile\n\nReuse caches and split core+test into core and tests Travis build\n\nThis closes #4890."}, {"url": "https://api.github.com/repos/apache/flink/commits/6450fadebdb02442670d1e1b19396689842e52f1", "message": "[hotfix] Remove unnecessary surefire plugin version in flink-connector-elasticsearch5"}, {"url": "https://api.github.com/repos/apache/flink/commits/2d97cc1873b71b237e827d843cd5bd6d40f86d9c", "message": "[FLINK-7903] [tests] Add flip6 build profile\n\nThe flip6 build profile only runs the Flip-6 related test cases. Moreover,\nall Flip-6 related test cases are excluded when not running the flip6 build\nprofile. This should reduce testing time when adding more and more Flip-6\ntest cases.\n\nInclude flink-test-utils-junit in all submodules to make the Category marker interfaces Flip6 and OldAndFlip6 available\n\nThis closes #4889."}, {"url": "https://api.github.com/repos/apache/flink/commits/3fdee00e45480c5471e6dbe0d2cd006fdd046b75", "message": "[FLINK-8330] [flip6] Remove YarnClusterClientV2\n\nThe YarnClusterClientV2 is no longer needed since we have removed FlinkYarnCLI."}, {"url": "https://api.github.com/repos/apache/flink/commits/ce62945ae6c1b1878bebc9a528e63ef7a54cb897", "message": "[FLINK-8330] [flip6] Remove FlinkYarnCLI\n\nThe FlinkYarnCLI is not needed and is, thus, being removed.\n\nThis closes #5217."}, {"url": "https://api.github.com/repos/apache/flink/commits/8e7a71c053135fdce4073eaf8022d5727d87b5fa", "message": "[FLINK-8171] [flip6] Remove work arounds from Flip6LocalStreamEnvironment\n\nIt is no longer needed to wait for the registration of task managers and\nto not use slot sharing when submitting jobs to the Flip-6 MiniCluster.\nTherefore, we can remove these work arounds from the\nFlip6LocalStreamEnvironment.\n\nAdapt comment in RestClusterClient\n\nThis closes #5101."}, {"url": "https://api.github.com/repos/apache/flink/commits/b5db8d90818efb96ac407ccc213f2892f3852321", "message": "[hotfix] Replace HighAvailabilityOptions#HA_ZOOKEEPER_NAMESPACE with HA_CLUSTER_ID"}, {"url": "https://api.github.com/repos/apache/flink/commits/76e3156d014e12094bbb55ecbd024b5edbf4b4cf", "message": "[FLINK-7928] Move Resource out of ResourceSpec"}, {"url": "https://api.github.com/repos/apache/flink/commits/fba72d073f68803dc8e719c69c7994283531fffb", "message": "[FLINK-7878] Hide GpuResource in ResourceSpec"}, {"url": "https://api.github.com/repos/apache/flink/commits/917fbcbee4599c1d198a4c63942fe1d2762aa64a", "message": "[hotfix] [tests] Speed up queryable state IT tests by removing sleep"}, {"url": "https://api.github.com/repos/apache/flink/commits/0ef7fddeff8430fd40d2d7a1b8a6454fd9416ced", "message": "[FLINK-7956] [flip6] Add support for queued scheduling with slot sharing to SlotPool\n\nThis commit adds support for queued scheduling with slot sharing to the\nSlotPool. The idea of slot sharing is that multiple tasks can run in the\nsame slot. Moreover, queued scheduling means that a slot request must not\nbe completed right away but at a later point in time. This allows to\nstart new TaskExecutors in case that there are no more slots left.\n\nThe main component responsible for the management of shared slots is the\nSlotSharingManager. The SlotSharingManager maintains internally a tree-like\nstructure which stores the SlotContext future of the underlying\nAllocatedSlot. Whenever this future is completed potentially pending\nLogicalSlot instantiations are executed and sent to the slot requester.\n\nA shared slot is represented by a MultiTaskSlot which can harbour multiple\nTaskSlots. A TaskSlot can either be a MultiTaskSlot or a SingleTaskSlot.\n\nIn order to represent co-location constraints, we first obtain a root\nMultiTaskSlot and then allocate a nested MultiTaskSlot in which the\nco-located tasks are allocated. The corresponding SlotRequestID is assigned\nto the CoLocationConstraint in order to make the TaskSlot retrievable for\nother tasks assigned to the same CoLocationConstraint.\n\nPort SchedulerSlotSharingTest, SchedulerIsolatedTasksTest and\nScheduleWithCoLocationHintTest to run with SlotPool.\n\nRestructure SlotPool components.\n\nAdd SlotSharingManagerTest, SlotPoolSlotSharingTest and\nSlotPoolCoLocationTest.\n\nThis closes #5091."}, {"url": "https://api.github.com/repos/apache/flink/commits/331ce82c7fa3e58e0445c046db5e977455f3340e", "message": "[hotfix] Speed up RecoveryITCase"}, {"url": "https://api.github.com/repos/apache/flink/commits/401d006516caa2a9d8289e760ccd3a9c564bc795", "message": "[FLINK-8089] Also check for other pending slot requests in offerSlot\n\nNot only check for a slot request with the right allocation id but also check\nwhether we can fulfill other pending slot requests with an unclaimed offered\nslot before adding it to the list of available slots.\n\nThis closes #5090."}, {"url": "https://api.github.com/repos/apache/flink/commits/bc1c375aa061f27a2fbc5a7688b06da70fed5d20", "message": "[FLINK-8088] Associate logical slots with the slot request id\n\nBefore logical slots like the SimpleSlot and SharedSlot where associated to the\nactually allocated slot via the AllocationID. This, however, was sub-optimal because\nallocated slots can be re-used to fulfill also other slot requests (logical slots).\nTherefore, we should bind the logical slots to the right id with the right lifecycle\nwhich is the slot request id.\n\nThis closes #5089."}, {"url": "https://api.github.com/repos/apache/flink/commits/a569f38f16186518b53461842d37b09fb1df45e9", "message": "[FLINK-8087] Decouple Slot from AllocatedSlot\n\nThis commit introduces the SlotContext which is an abstraction for the SimpleSlot\nto obtain the relevant slot information to do the communication with the\nTaskManager without relying on the AllocatedSlot which is now only used by the\nSlotPool.\n\nThis closes #5088."}, {"url": "https://api.github.com/repos/apache/flink/commits/627bcda6957b2ad61b67b98a7d0a1de2c1f3eb29", "message": "[FLINK-8120] [flip6] Register Yarn application with correct tracking URL\n\nThe cluster entrypoints start the ResourceManager with the web interface URL.\nThis URL is used to set the correct tracking URL in Yarn when registering the\nYarn application.\n\nThis closes #5128."}, {"url": "https://api.github.com/repos/apache/flink/commits/e80dd8ea3fef0398048a40c3ffd5136bef204b80", "message": "[FLINK-8262] [tests] Harden IndividualRestartsConcurrencyTest.testLocalFailureFailsPendingCheckpoints\n\nThe problem was a concurrent restart attempt which failed due to not enough\navailable slots. This failure would lead to the job failure and the discarding\nof all pending checkpoints."}, {"url": "https://api.github.com/repos/apache/flink/commits/7bca9e4613ff30ab6a9c11e673a785f3f5c86e69", "message": "[FLINK-8085] Thin out LogicalSlot interface\n\nRemove isCanceled, isReleased method and decouple logical slot from Execution by\nintroducing a Payload interface which is set for a LogicalSlot. The Payload interface\nis implemented by the Execution and allows to fail an implementation and obtaining\na termination future.\n\nIntroduce proper Execution#releaseFuture which is completed once the Execution's\nassigned resource has been released.\n\nThis closes #5087."}, {"url": "https://api.github.com/repos/apache/flink/commits/bb9c64b1222a5e9568cf93186a6420bebcb306f9", "message": "[FLINK-8078] Introduce LogicalSlot interface\n\nThe LogicalSlot interface decouples the task deployment from the actual\nslot implementation which at the moment is Slot, SimpleSlot and SharedSlot.\nThis is a helpful step to introduce a different slot implementation for\nFlip-6.\n\nThis closes #5086."}, {"url": "https://api.github.com/repos/apache/flink/commits/0d551640e65073afd8755e04f7817a6379149251", "message": "[FLINK-8030] Instantiate JobMasterRestEndpoint in JobClusterEntrypoint\n\nThis closes #4988."}, {"url": "https://api.github.com/repos/apache/flink/commits/0e3027d4b87cb5aff5c640ec809f2968b64be732", "message": "[FLINK-8029] Create WebMonitorEndpoint\n\nThe WebMonitorEndpoint is the common rest endpoint used for serving\nthe web frontend REST calls. It will be used by the Dispatcher and\nthe JobMaster to fuel the web frontend.\n\nThis closes #4987."}, {"url": "https://api.github.com/repos/apache/flink/commits/4f32a796f36555e46053a386b813e62cb886d273", "message": "[hotfix] [tests] Speed up TaskManagerServicesTest\n\nThe TaskManagerServicesTest was extremely slow because it uses the PowerMockRunner\nfor a single test case. Additionally some other tests test different settings in\na loop which was quite slow because the PowerMockRunner instrumented all method calls.\nMoving the test which required the PowerMockRunner into a separate test class sped up\nthings by a factor of 50."}, {"url": "https://api.github.com/repos/apache/flink/commits/dd48a40ab7b3a721c7c78a4b0d6efad5b50f947e", "message": "[FLINK-8204] [tests] Harden JobManagerLeaderSessionIDITCase\n\nReplace wait on lock with OneShotLatch to resolve race condition between task\nstart up and unblocking of invokable."}, {"url": "https://api.github.com/repos/apache/flink/commits/2d1d4dd763ddcc8166bf26492e4950089c10f64e", "message": "[FLINK-8194] [akka] Suppress Java serializer used warnings from Akka"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5334", "title": "[FLINK-8471] [flip6] Introduce configuration switch for Flip-6", "body": "## What is the purpose of the change\r\n\r\nAdd `mode` configuration parameter which enables and disables the Flip-6\r\ncode paths. Per default this is set to the old code path.\r\n\r\nThis PR is based on #5321.\r\n\r\n## Brief change log\r\n\r\n- Introduce `mode` configuration parameter with values `flip6` and `old`.\r\n- Updated `config.md`\r\n- Changed `CustomCommandLine` instantiation in `CliFrontend` to respect `mode` configuration parameter\r\n- Changed `YarnClusterDescriptor` instantiation in `FlinkYarnSessionCli` to respect `mode` configuration parameter\r\n- Adapted start-up scripts `start-cluster.sh`, `stop-cluster.sh`, `taskmanager.sh` and `jobmanager.sh`\r\n- Merged Flip-6 and old Mesos start-up scripts into one: `mesos-appmaster-flip6-job.sh` -> `mesos-appmaster-job.sh`, `mesos-appmaster-flip6-session.sh` -> `mesos-appmaster-session.sh` and `mesos-taskmanager-flip6.sh` -> `mesos-taskmanager.sh`\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests and I tested it manually.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5321", "title": "[FLINK-8465] [flip6] Retrieve correct leader component address in ClusterClient", "body": "## What is the purpose of the change\r\n\r\nRename ClusterClient#getJobManagerAddress into #getClusterConnectionInfo. The\r\nreturned LeaderConnectionInfo contains the address of the leading cluster\r\ncomponent. In the old code this is the JobManager whereas in Flip-6 it is the\r\nDispatcher.\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5311", "title": "[FLINK-8454] [flip6] Remove JobExecutionResultCache from Dispatcher", "body": "## What is the purpose of the change\r\n\r\nWith the introduction of the SerializableExecutionGraphStore to the Dispatcher,\r\nit is no longer necessary to store the JobResult separately. In order to\r\ndecrease complexity and state duplication, this commit removes the\r\nJobExecutionResultCache and instead uses the SerializableExecutionGraphStore\r\nto serve completed job information. A side effect of this change is that the\r\nJobExecutionResult is now available as long as the completed Flink job is stored\r\nin the SerializableExecutionGraphStore.\r\n\r\nThis PR is based on #5310.\r\n\r\n## Brief change log\r\n\r\n- Replace information served from `JobExecutionResultCache` with information from `SerializableExecutionGraphStore`\r\n- Adapt `JobExecutionResultHandler`\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5310", "title": "[FLINK-8453] [flip6] Add ArchivedExecutionGraphStore to Dispatcher", "body": "## What is the purpose of the change\r\n\r\nThe ArchivedExecutionGraphStore is responsible for storing completed jobs\r\nfor historic job requests (e.g. from the web ui or from the client). The store\r\nis populated by the Dispatcher once a job has terminated.\r\n\r\nThe FileArchivedExecutionGraphStore implementation persists all\r\nArchivedExecutionGraphs on disk in order to avoid OOM problems. It only keeps\r\nsome of the stored graphs in memory until it reaches a configurable size. Once\r\ncoming close to this size, it will evict the elements and only reload them if\r\nrequested again. Additionally, the FileArchivedExecutionGraphStore defines\r\nan expiration time after which the execution graphs will be removed from disk.\r\nThis prevents excessive use of disk resources.\r\n\r\nThis PR is based on #5309.\r\n\r\n## Brief change log\r\n\r\n- Introduce `ArchivedExecutionGraphStore` and `FileArchivedExecutionGraphStore`\r\n- Add `FileArchivedExecutionGraphStore` to `Dispatcher`\r\n- Store `ArchivedExecutionGraphs` in corresponding `FileArchivedExecutionGraphStore`\r\n- Adapt `Dispatcher` to serve requests for historic jobs\r\n\r\n## Verifying this change\r\n\r\n- Added `FileArchivedExecutionGraphStoreTest`\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n\r\ncc @GJL ", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5309", "title": "[FLINK-8450] [flip6] Make JobMaster/DispatcherGateway#requestJob type safe", "body": "## What is the purpose of the change\r\n\r\nLet JobMasterGateway#requestJob and DispatcherGateway#requestJob return a\r\nCompletableFuture<SerializableExecutionGraph> instead of a\r\nCompletableFuture<AccessExecutionGraph>. In order to support the old code\r\nand the JobManagerGateway implementation we have to keep the return type\r\nin RestfulGateway. Once the old code has been removed, we should change\r\nthis as well.\r\n\r\n## Brief change log\r\n\r\n- Change return type of `RestfulGateway#requestJob` to `CompletableFuture<? extends AccessExecutionGraph>`\r\n- Change return type of `JobMasterGateway#requestJob` and `DispatcherGateway#requestJob` to `CompletableFuture<SerializableExecutionGraph>`\r\n- Introduce `TestingRestfulGateway` as a testing utility\r\n- Adapt `ExecutionGraphCacheTest` to use `TestingRestfulGateway` instead of Mockito mocks\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5308", "title": "[FLINK-8449] [flip6] Extend OnCompletionActions to accept an SerializableExecutionGraph", "body": "## What is the purpose of the change\r\n\r\nThis commit changes the OnCompletionActions interface such that it accepts a\r\nSerializableExecutionGraph instead of a plain JobResult. This allows to\r\narchive the completed ExecutionGraph for further usage in the container\r\ncomponent of the JobMasterRunner.\r\n\r\n## Brief change log\r\n\r\n- Introduce `SerializableExecutionGraph`\r\n- Introduce `DummyExecutionGraph` for testing purposes\r\n- Change `OnCompletionActions` to accept a `SerializableExecutionGraph`\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests, such as `DispatcherTest`.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2742", "title": "[FLINK-4944] Replace Akka's death watch with own heartbeat on the TM side", "body": "This PR introduces the HeartbeatActor which is used by the TaskManager to monitor the\r\nJobManager. The HeartbeatActor constantly sends Heartbeat messages to the JobManager\r\nwhich responds with a HeartbeatResponse. If the HeartbeatResponse fails to be received\r\nfor an acceptable heartbeat pause, then the HeartbeatActor sends a HeartbeatTimeout\r\nmessage to the owner of the HeartbeatActor.\r\n\r\nThe acceptable heartbeat pause can be extended by the HeartbeatActor if it detects that\r\nit has been stalled by garbage collection, for example.\r\n\r\nThe HeartbeatActor is started as a child actor of the TaskManager.\r\n\r\nAdd ClusterOptions\r\n\r\nAdd comments", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/12973040", "body": "I'll fix it. Thanks for the pointer @JonathanH5 :-)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/12973040/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/14182335", "body": "Good point @smarthi. I think nobody so far considered it. Should take a look.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14182335/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/14470837", "body": "Hi @alexeyegorov, there are no binaries for 1.0-SNAPSHOT. But you can easily build it yourself by simply cloning the flink repository and then call `mvn clean package -DskipTests` in the root directory. If you then go to `build-target` you end up in the folder which contains the binary distribution of Flink. Thus, copying this folder to your cluster should do the job.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14470837/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15726940", "body": "I guess this will entail some work correcting the checkstyle violations in the tests, e.g. unused imports etc.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15726940/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/17174247", "body": "Yes will do.\n\nOn Tue, Apr 19, 2016 at 8:44 PM, Ufuk Celebi notifications@github.com\nwrote:\n\n> Can you push this to release-1.0 as well to have it in 1.0.3 or the next\n> 1.0.2 RC?\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/flink/commit/7462a5bfd7cb2dafbbc9eb02a43d3db9f6add30e#commitcomment-17165776\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/17174247/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/17257298", "body": "Yes, will add it right away.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/17257298/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/18669730", "body": "Really nice abstraction @StephanEwen \ud83d\udc4d . I like it a lot :-)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18669730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/20640052", "body": "Had also forgotten about this ;-)", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/20640052/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/20662491", "body": "At the moment, there is no ack being sent back to the TM. What could however happen is that the message never receives the JM and the TM thinks that the checkpoint has been completed. In this case, we would have state lingering around.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/20662491/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/22029297", "body": "I think this makes sense to do as long as we don't fall below the minimum memory requirements for Yarn (I think this should be 768 Mb).", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/22029297/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/23558079", "body": "Why did you exclude this dependency? I think that's the reason why Flink fails with the following exception when trying to start a Flink yarn session with Hadoop `2.7.1`:\r\n```\r\nException in thread \"main\" java.lang.NoClassDefFoundError: javax/servlet/Filter\r\n\tat java.lang.ClassLoader.defineClass1(Native Method)\r\n\tat java.lang.ClassLoader.defineClass(ClassLoader.java:763)\r\n\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\r\n\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\r\n\tat java.net.URLClassLoader.access$100(URLClassLoader.java:73)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:368)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:362)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:361)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat java.lang.ClassLoader.defineClass1(Native Method)\r\n\tat java.lang.ClassLoader.defineClass(ClassLoader.java:763)\r\n\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\r\n\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\r\n\tat java.net.URLClassLoader.access$100(URLClassLoader.java:73)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:368)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:362)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:361)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.apache.hadoop.hdfs.DFSConfigKeys.<clinit>(DFSConfigKeys.java:237)\r\n\tat org.apache.hadoop.hdfs.DFSClient$Conf.<init>(DFSClient.java:509)\r\n\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:638)\r\n\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:619)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:149)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653)\r\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:170)\r\n\tat org.apache.flink.yarn.AbstractYarnClusterDescriptor.startAppMaster(AbstractYarnClusterDescriptor.java:625)\r\n\tat org.apache.flink.yarn.AbstractYarnClusterDescriptor.deployInternal(AbstractYarnClusterDescriptor.java:456)\r\n\tat org.apache.flink.yarn.AbstractYarnClusterDescriptor.deploySessionCluster(AbstractYarnClusterDescriptor.java:362)\r\n\tat org.apache.flink.yarn.cli.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:677)\r\n\tat org.apache.flink.yarn.cli.FlinkYarnSessionCli$1.call(FlinkYarnSessionCli.java:512)\r\n\tat org.apache.flink.yarn.cli.FlinkYarnSessionCli$1.call(FlinkYarnSessionCli.java:509)\r\n\tat org.apache.flink.runtime.security.HadoopSecurityContext$1.run(HadoopSecurityContext.java:44)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\r\n\tat org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)\r\n\tat org.apache.flink.yarn.cli.FlinkYarnSessionCli.main(FlinkYarnSessionCli.java:509)\r\nCaused by: java.lang.ClassNotFoundException: javax.servlet.Filter\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\t... 47 more\r\n```\r\n\r\nThe problem seems to be that `DFSConfigKeys` loads `o.a.h.hdfs.web.AuthFilter` which depends on `javax.servlet`.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/23558079/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/25306076", "body": "Would be great to add an explanation why these tests are ignored.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/25306076/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/26082263", "body": "yes", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/26082263/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/26240373", "body": "Valid point. I'll add the release notes.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/26240373/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/26789565", "body": "You're right. I'll correct it.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/26789565/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/27019488", "body": "Good point @zentol. Will add it.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/27019488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/18420291", "body": "I reinsert the more detailed exception. Initially, I just removed it because of the return statement which disrupts the control flow.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18420291/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18420335", "body": "That is just a debugging relict. I'll change it.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18420335/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18420375", "body": "Good point.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18420375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18420572", "body": "Without this change, the FlinkMiniCluster does not wait properly on the TaskManagers which have to register at the JobManager. The reason for this is that the number of slots to wait for is negative. As a consequence, some of the test cases failed every once in a while.\n\nActually it is not so wise to wait for registered slots instead of registered task managers, because the number of slots are not always known if the number of slots are chosen automatically.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18420572/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18436643", "body": "But if the error message is too misleading, shouldn't it be the responsibility of the function throwing the exception to generate a meaningful error message instead of the job manager's?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18436643/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14607144", "body": "If you check in line 64 whether this.getDelimiter() is not null, I think we should check it here as well. But actually, the DelimitedInputFormat checks upon setting the delimiter whether it is null. So it should be safe to leave the null check in line 64 out.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14607144/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14607248", "body": "Can we have records of length 0?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14607248/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "StefanRRichter": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/517b3f87214168a445b5751cda210ecf3a292fd6", "message": "[FLINK-7720] [checkpoints] Centralize creation of backends and state related resources\n\nThis closes #4745."}, {"url": "https://api.github.com/repos/apache/flink/commits/402a2e30c750e1bcb753643ed66c6df0dd861112", "message": "[FLINK-7719] [checkpoints] Send checkpoint id to task as part of deployment descriptor when resuming"}, {"url": "https://api.github.com/repos/apache/flink/commits/b32b8359ea20812cddbcbffc3b617d1256889cb1", "message": "[FLINK-8385] [checkpointing] Avoid RejectedExecutionException in SharedStateRegistry during disposal from async Zookeeper calls.\n\nThis closes #5256."}, {"url": "https://api.github.com/repos/apache/flink/commits/b5de38cee4e68304e76fd9f9cda09dee86099578", "message": "[FLINK-8385] [checkpointing] Suppress logging of expected exception during snapshot cancellation."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5239", "title": "[FLINK-8360] Implement task-local state recovery", "body": "## What is the purpose of the change\r\n\r\nThis changes introduces the task-local recovery feature. The main idea is to have a secondary, local copy of the checkpointed state, while there is still a primary copy in DFS that we report to the checkpoint coordinator.\r\n\r\nRecovery can attempt to restore from the secondary local copy, if available, to save network bandwidth. This requires that the assignment from tasks to slots is as sticky is possible.\r\n\r\nFor starters, we will implement this feature for all managed keyed states and can easily enhance it to all other state types (e.g. operator state) later, because the basic infrastructure is already in place. This PR is on top of #4745.\r\n\r\n\r\n## Brief change log\r\n\r\n  - *Introduced `TaskExecutorLocalStateStoresManager` per task manager. This class manages one `TaskLocalStateStore` for each task running on the task manager.*\r\n  - *`TaskLocalStateStore` stores and provides the local state for one task. Reporting of checkpointed states goes through this class. The primary state handles are forwarded to the checkpoint coordinator, the optional secondary (local) state is stored in the local store.*\r\n  - *`LocalRecoveryDirectoryProvider` is used by `TaskLocalStateStore` to manage the local state directory/ies.*\r\n- *`StreamTaskStateManager` uses the `TaskLocalStateStore` to restore state for its operators.*\r\n- *File-based local state is created through `DuplicatingCheckpointOutputStream`, a stream that duplicates writes into two internal streams - typically one primary against a DFS and one secondary against local FS.*\r\n- *RocksDB's incremental checkpoints are not just based on one file, but on a directory. As we do not require reference counting for local files (we can use hardlinks), we can deal with checkpoint directories as a whole. We introduced `IncrementalLocalKeyedStateHandle extends DirectoryKeyedStateHandle` for this purpose.*\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n  - *If we activate the local recovery feature on the state backend (via `#setLocalRecoveryMode(...)`, and introduce task failure by user-code exception, we should observe that managed keyed state is recovered from the local FS through the logs.*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes)\r\n  - The serializers: (yes, slightly and in a way that should not matter.)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes)\r\n  - If yes, how is the feature documented? (Documentation pending)\r\n\r\n  ", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4543", "title": "[FLINK-7449] [docs] Additional documentation for incremental checkpoints", "body": "## What is the purpose of the change\r\n\r\nThis PR provides additional documentation for incremental checkpoints.\r\n\r\n\r\n## Brief change log\r\n\r\nAdded documentation.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\nNo\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (docs)\r\n\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/20654494", "body": "I am not sure about the guarantees we have in Akka and the underlying acknowledgement chain, but depending on how this works, is there a possibility that TM acknowledges a checkpoint, the JM receives the acknowledgment, marks the checkpoint as complete but the JM's message to the TM to acknowledge back is lost / timeout, leading to an exception in the TM's call. Would we have a data loss or does it work differently?", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/20654494/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/22558757", "body": "They will trigger `forceSeek`, as they should.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/22558757/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "GJL": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/2bdd16e0564b307d0070bb45458f41af96b70ab1", "message": "[hotfix][docs] Put HADOOP_CONF_DIR in <code> tag\n\nThis closes #5315"}, {"url": "https://api.github.com/repos/apache/flink/commits/598d96c105a34b20c16ace9f4b56d663dac670d6", "message": "[hotfix][tests] Add TestLogger to ExponentialWaitStrategyTest"}, {"url": "https://api.github.com/repos/apache/flink/commits/9eda3db646138ed487f438de64e7fbd86fafbf71", "message": "[hotfix][javadoc] Fix typo in Watermark javadoc"}, {"url": "https://api.github.com/repos/apache/flink/commits/782ec6dc4c3a4d4825c8f99af0b596be766c1312", "message": "[FLINK-8317][flip6] Implement savepoints in RestClusterClient\n\nAllow triggering of savepoints through RestfulGateway. Implement REST handlers\nto trigger and query the status of savepoints. Implement\nsavepoint command in RestClusterClient.\n\n[FLINK-8317][flip6] Rename field QueueStatus#statusId to id\n\n[FLINK-8317][flip6] Simplify initialization of RpcUtils#INF_TIMEOUT\n\n[FLINK-8317][flip6] Add missing fail() to test in SavepointHandlersTest\n\n[FLINK-8317][flip6] Add TestLogger to unit tests\n\n[FLINK-8317][flip6] Replace anonymous with lambda\n\n[FLINK-8317][flip6] Extract string constants to variables in RestClusterClientTest\n\n[FLINK-8317][flip6] Move method RestClusterClient#waitForResource\n\n[FLINK-8317][flip6] Do not wait if resource is already completed\n\n[FLINK-8317][flip6] Only return savepoint location from triggerSavepoint\n\nOnly return the savepoint's location from RestfulGateway#triggerSavepoint. Fix\nmistakes in Javadoc. Rename occurrences of checkpoint to savepoint in\nSavepointHandlers class.\n\n[FLINK-8317][flip6] Declare SavepointHandlers#defaultSavepointDir final\n\nThis closes #5223."}, {"url": "https://api.github.com/repos/apache/flink/commits/06922753a55dc322b96919ebb407d531e2b79d3e", "message": "[FLINK-8299][flip6] Retrieve JobExecutionResult after job submission\n\n[FLINK-8299][flip6] Improve ExponentialWaitStrategy\n\nAdd additional argument validation. Add more unit tests.\n\nThis closes #5207."}, {"url": "https://api.github.com/repos/apache/flink/commits/d7ee60330ae91700d7b2c06eec863e3c4b092222", "message": "[hotfix] Clean up ExecutionGraph\n\n- Remove unnecessary throws clause.\n- Format whitespace."}, {"url": "https://api.github.com/repos/apache/flink/commits/86892b8e76a4e4b26cedf38c0695c53814a7f04f", "message": "[FLINK-8233][flip6] Add JobExecutionResultHandler\n\n    - Allow retrieval of the JobResult cached in Dispatcher.\n    - Implement serializer and deserializer for JobResult.\n\n[FLINK-8233][flip6] Improve JobResultDeserializer and add tests\n\n[FLINK-8233][flip6] Exclude null jobExecutionResult from serialization\n\n[FLINK-8233][flip6] Add TestLogger to JobResultDeserializerTest\n\nThis closes #5194."}, {"url": "https://api.github.com/repos/apache/flink/commits/08e550c98674738ab883ea84c0350093c9765ab6", "message": "[FLINK-8234][flip6] Cache JobExecutionResult in Dispatcher\n\n- Introduce new JobExecutionResult used by JobMaster to forward the information in\n  the already existing JobExecutionResult.\n- Always cache a JobExecutionResult. Even in case of job failures. In case of\n  job failures, the serialized exception is stored additionally.\n- Introduce new methods to RestfulGateway to allow retrieval of cached\n  JobExecutionResults\n\n[FLINK-8234][flip6] Rename JobExecutionResult -> JobResult\n\n[FLINK-8234][flip6] Update MiniClusterJobDispatcher\n\nDo not store job failure exception in a separate field because the JobResult\nalready contains the exception.\n\n[FLINK-8234][flip6] Make JobResult Serializable\n\n[FLINK-8234][flip6] Add Javadoc to JobResult builder\n\n[FLINK-8234][flip6] Add Javadoc to JobResult#serializedThrowable\n\n[FLINK-8234][flip6] Wrap JobResults in SoftReferences\n\nWrap instances of JobResult stored in JobExecutionResultCache in SoftReferences\nso that the GC can free them according to memory demand.\n\n[FLINK-8234][flip6] Fix checkstyle violations\n\n[FLINK-8234][flip6] Add Javadoc to JobResult\n\nThis closes #5184."}, {"url": "https://api.github.com/repos/apache/flink/commits/7ddb674cb17c35f17aa073d3bfd6897d7fc13b9e", "message": "[FLINK-8176][flip6] Start SubmittedJobGraphStore in Dispatcher\n\nImplement SubmittedJobGraphListener interface in Dispatcher\n\nCall start() on SubmittedJobGraphStore with Dispatcher as listener. To enable\nthis, the dispatcher must implement the SubmittedJobGraphListener interface. Add\nsimple unit tests for the new methods. Refactor DispatcherTest to remove\nredundancy.\n\n[FLINK-8176][flip6] Make InMemorySubmittedJobGraphStore thread-safe\n\n[FLINK-8176][flip6] Add method isStarted() to TestingLeaderElectionService\n\n[FLINK-8176][flip6] Return same RunningJobsRegistry instance from TestingHighAvailabilityServices\n\n[FLINK-8176][flip6] Fix race conditions in Dispatcher and DispatcherTest\n\nCheck if jobManagerRunner exists before submitting job.\nReplace JobManagerRunner mock used in tests with real instance.\nDo not run job graph recovery in actor main thread when job graph is recovered\nfrom SubmittedJobGraphListener#onAddedJobGraph(JobID).\n\n[FLINK-8176][flip6] Rename variables in DispatcherTest\n\n[FLINK-8176][flip6] Remove injectMocks in DispatcherTest\n\n[FLINK-8176][flip6] Update Dispatcher's SubmittedJobGraphListener callbacks\n\nAlways attempt the job submission if onAddedJobGraph or onRemovedJobGraph are\ncalled. The checks in submitJob and removeJob are sufficient.\n\nThis closes #5107."}, {"url": "https://api.github.com/repos/apache/flink/commits/8941f636ba1550b1d934278ef0c13e6d9a354781", "message": "[hotfix][Javadoc] Make first sentence in JobSubmissionException Javadoc end with period"}, {"url": "https://api.github.com/repos/apache/flink/commits/87749ca40a8b40f609fb46957be2453797f75ed3", "message": "[hotfix][tests] Extract SubmittedJobGraphStore implementation from JobManagerHARecoveryTest"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5312", "title": "[FLINK-8344][WIP] Add support for HA to RestClusterClient", "body": "WIP", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sunjincheng121": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/4c883e342e59ebe5cf8d8c8d47535013557ef5f7", "message": "[FLINK-8325] [table] Fix COUNT(*) and COUNT(1).\n\nThis closes #5241."}, {"url": "https://api.github.com/repos/apache/flink/commits/20faf262de9bb52aa614ff2d989a49e8ea82b963", "message": "[FLINK-8355] [table] Remove DataSetAggregateWithNullValuesRule.\n\nThis closes #5320."}, {"url": "https://api.github.com/repos/apache/flink/commits/2914e596bf2af968197b5241aa40840e2e9408ce", "message": "[FLINK-6893] [table] Add BIN function support\n\nThis closes #4128."}, {"url": "https://api.github.com/repos/apache/flink/commits/9dd3a859b1609d27ccc80a3da86456e533895b7a", "message": "[FLINK-8331][core] FieldParser do not correctly set EMPT_COLUMN error state.\n\nThis closes #5218"}, {"url": "https://api.github.com/repos/apache/flink/commits/bda0c057b3c005b747de4fb6882b5bf880db90d3", "message": "[FLINK-8323] [table] Fix modulo scalar function bug\n\nThis closes #5212."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4669", "title": "[FLINK-7618][table] Add BINARY supported in FlinkTypeFactory", "body": "## What is the purpose of the change\r\n\r\n*Add BINARY supported in FlinkTypeFactory*\r\n\r\n## Brief change log\r\n\r\n  - *Only improve `FlinkTypeFactory#toTypeInfo` add `BINARY` mapping to `PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO`*\r\n\r\n## Verifying this change\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: ( no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4556", "title": "[FLINK-6465][table]support FIRST_VALUE on Table API & SQL", "body": "## What is the purpose of the change\r\nThis PR. try to add `FIRST_VALUE` aggregate function OVER window on table API&SQL.\r\n\r\n## Brief change log\r\n- *Add `FIRST_VALUE` aggregate function.\r\n- *Add `FIRST_VALUE` test in OVER window.\r\n- *Add `FIRST_VALUE` test case.(only for aggregate function)\r\n\r\n## Verifying this change\r\nThis change added tests and can be verified as follows:\r\n  - *Added integration tests for over window(tableAPI&SQL)\r\n  - *Added test that validates that  `FIRST_VALUE` can deal with BYTE/SHORT/INT/LONG/DOUBLE/STRING/BIGDECIMAL\r\n  \r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: ( no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes)\r\n  - If yes, how is the feature documented? (docs)\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4534", "title": "[FLINK-7358][table]Add implicitly converts support for User-defined function", "body": "## What is the purpose of the change\r\nIn this PR i had Add implicitly converts support for User-defined function.\r\n\r\n## Brief change log\r\n- *Update the udfs document.*\r\n- *Add implicitly converts for user-defined scalarFunction and tableFunction.*\r\n- *Add test case for the changes.*\r\n\r\n## Verifying this change\r\nThis change added tests and can be verified as follows:\r\n - *Added integration tests for user-defined table function.\r\n - *Added test case for user-defined scalar function.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): ( no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no )\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n  - Does this pull request introduce a new feature? (yes)\r\n  - If yes, how is the feature documented? ( docs )\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4210", "title": "[FLINK-7024][table]Add supported for selecting window proctime/rowtim\u2026", "body": "\u2026e on row-based Tumble/Slide window\r\n\r\nThanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-7024][table]Add supported for selecting window proctime/rowtime on row-based Tumble/Slide window\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4183", "title": "[FLINK-6969][table]Add support for deferred computation for group win\u2026", "body": "In this PR. I have add support for deferred computation for group window aggregates.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-6969][table]Add support for deferred computation for group window aggregates\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4157", "title": "[FLINK-6649][table]Improve Non-window group aggregate with update int\u2026", "body": "In this PR. I have add supports updating the calculated data according to the specified time interval on non-window group AGG. If we config the time interval is N seconds, then the next update time relative to the latest update time T, is T+N seconds. For example, the time interval is 2 seconds, the previous update time is T seconds, and the next update time T1> = T + 2 seconds. If no data arrives during T to T + 2, no updates are made.\r\n\r\nThanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[Flink 6649][table]Improve Non-window group aggregate with update interval.\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4127", "title": "[FLINK-6892][table]Add L/RPAD supported in SQL", "body": "In this PR. have Add L/RPAD supported in SQL,For Example:\r\n```\r\nLPAD('hi',4,'??') -> '??hi'\r\nLPAD('hi',1,'??') -> 'h'\r\nRPAD('hi',4,'') -> 'hi'\r\nRPAD('hi',1,'??') -> 'h'\r\n```\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4117", "title": "[FLINK-6813][table]Add TIMESTAMPDIFF supported in SQL", "body": "In this PR. I have Add TIMESTAMPDIFF supported in SQL.\r\n1. timestampDiff(unit, timestamp,timestamp) keep consistent with calcite.\r\n2. timestampDiff(unit, date,date) keep consistent with calcite.\r\n3. timestampDiff(unit, timestamp,date) calcite not support yet.\r\n4. timestampDiff(unit, date,timestamp)  calcite not support yet.\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-6813][table]Add TIMESTAMPDIFF supported in SQL\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zentol": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/2a612d9da3317e4e79c10be7970e6bdb55a1f0e3", "message": "[FLINK-6590][docs] Integrate configuration docs generator\n\nThis closes #5119."}, {"url": "https://api.github.com/repos/apache/flink/commits/b1df177045e24bcb6f063e0b2fdb6dcebad88128", "message": "Revert \"[hotfix][docs] Mention maven dependency for RocksDB state backend\"\n\nThis reverts commit 5623ac66bd145d52f3488ac2fff9dbc762d0bda1."}, {"url": "https://api.github.com/repos/apache/flink/commits/4e0ca93d3f496b410452ab31485ed920e9ab2702", "message": "[FLINK-8082][build] Bump flink version for japicmp plugin\n\nThis closes #5262."}, {"url": "https://api.github.com/repos/apache/flink/commits/f16335d427592007a5b242a1b4b0cfcd5bd36858", "message": "[FLINK-8388][docs] Fix baseUrl for master branch\n\nThis closes #5263."}, {"url": "https://api.github.com/repos/apache/flink/commits/1ef1f321192e11aea62ba0089a42799d2eb127a0", "message": "[FLINK-8320][docs] Clarify that only Java 8 is supported"}, {"url": "https://api.github.com/repos/apache/flink/commits/10aae991833447506e8e8231c32fb9f816920597", "message": "[FLINK-8250][runtime] Remove unused RecordSerializer#instantiateMetrics\n\nThis closes #5162."}, {"url": "https://api.github.com/repos/apache/flink/commits/7c2a32c2863e83d075dde05cd35e93f3e08306c0", "message": "[FLINK-8383][mesos] Disable test-jar shading\n\nThis closes #5258."}, {"url": "https://api.github.com/repos/apache/flink/commits/3b1448ecbb192476b911f0668660d0a97c961cc9", "message": "[hotfix][metrics][docs] Remove unclosed highlight"}, {"url": "https://api.github.com/repos/apache/flink/commits/5b65ca80802a5fdf4e2e77994438d8cf908ae442", "message": "[hotfix][tests] Fix DispatcherTest compilation"}, {"url": "https://api.github.com/repos/apache/flink/commits/c1665c12b49752af2f4d2c624095dcec432efe8e", "message": "[FLINK-8239][tests] StreamTaskTestHarness supports 2-input head operators\n\nThis closes #5153."}, {"url": "https://api.github.com/repos/apache/flink/commits/6e89878166c0b8b8193ae69a675b6d085ffa9fe7", "message": "[FLINK-8238][tests] Forbid multiple setups of StreamTaskTestHarness"}, {"url": "https://api.github.com/repos/apache/flink/commits/5a545dbf16272c36c0e8efbbc07b1aed3c0290b9", "message": "[hotfix][metrics] Refactor CheckpointStatsTrackerTest"}, {"url": "https://api.github.com/repos/apache/flink/commits/0bf0fdc26ea86020929fa64d083dce02ba2a2ae2", "message": "[FLINK-8213][metrics] Improve fallback behaviors\n\nThis closes #8213."}, {"url": "https://api.github.com/repos/apache/flink/commits/493c28571f22d9dde4edbec0ba38f2761fb51335", "message": "[FLINK-8080][metrics] metrics.reporters now optional include list\n\nThis closes #5099."}, {"url": "https://api.github.com/repos/apache/flink/commits/beb11976fe63c20a5dc9f22ea713c05b4d5e9585", "message": "[FLINK-8235][build] Spotbugs exclusion file path now absolute\n\nThis closes #5146."}, {"url": "https://api.github.com/repos/apache/flink/commits/9ef6796e8aceb0c65859199be8a91babe9cedc69", "message": "[hotfix][docs] Exclude flink-docs from maven deployment"}, {"url": "https://api.github.com/repos/apache/flink/commits/610fde722fc91ff760cff3865564a51a4b945f19", "message": "[FLINK-8133][REST][docs] Generate REST API documentation\n\nThis closes #5052."}, {"url": "https://api.github.com/repos/apache/flink/commits/19b1b83292f74cc49f518f18dd94d81768b6ba1a", "message": "[FLINK-8193][quickstart] Cleanup quickstart poms\n\nThis closes #5118."}, {"url": "https://api.github.com/repos/apache/flink/commits/f93dc8aa0a74a5d91c8b158e66e94a29fa464990", "message": "[hotfix][metrics] Cleanup reporter poms"}, {"url": "https://api.github.com/repos/apache/flink/commits/94a0eccbec51aa2273b7d19fa0de3e940c2561e7", "message": "[FLINK-8007][metrics] Move TestMeter into test scope"}, {"url": "https://api.github.com/repos/apache/flink/commits/eda583092481a21c613ef689590eeb7f669f8eef", "message": "[FLINK-7395] [metrics] Count bytesIn/Out without synchronization\n\nThis closes #4504."}, {"url": "https://api.github.com/repos/apache/flink/commits/80348d653b48e1b7d6a0b9275dbfa510eaea151f", "message": "[FLINK-7595] [Savepoints] Allow removing stateless operators\n\nThis closes #4651."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5333", "title": "[FLINK-5886] Python API for streaming applications", "body": "This PR is an extension of #3838 resolving all issues that i found during the review. The change log below is roughly grouped into categories to provide a better overview.\r\n\r\nChange log:\r\n\r\nGeneral:\r\n- rebase branch to current master\r\n- incremented version to 1.5-SNAPSHOT\r\n- fixed kafka-connector dependency declaration\r\n\t- set to provided\r\n\t- scala version set to scala.binary.version\r\n\t- flink version set to project.version\r\n- applied checkstyle\r\n\t- disabled method/parameter name rules for API classes\r\n- assigned flink-python-streaming to 'libraries' travis profile\r\n- copy streaming-python jar to /opt\r\n- change the name of the final jar to flink-streaming-python (previously flink-python)\r\n- replace maven-jar-plugin with maven-shade-plugin\r\n\r\nAPI:\r\n- PDS#map()/flat_map() now return PythonSingleOutputStreamOperator\r\n- renamed PDS#print() to PDS#output()\r\n\t- print is a keyword in python and thus not usable in native python APIs\r\n- added PythonSingleOutputStreamOperator#name()\r\n- removed env#execute methods that accepted local execution argument as they are redundant due to environment factory methods\r\n- narrow visibility of *DataStream constructors\r\n\r\nMoved/Renamed:\r\n- made SerializerMap top-level class and renamed it to AdapterMap\r\n- Moved UtilityFunctions#adapt to AdapterMap class\r\n- renamed UtilityFunctions to InterpreterUtils\r\n- moved PythonobjectInputStream2 to SerializationUtils\r\n- renamed PythonObjectInputStream2 to SerialVersionOverridingPythonObjectInputStream\r\n\r\nJython:\r\n- renamed InterpreterUtils#smartFunctionDeserialization to deserializeFunction\r\n- added generic return type to #deserializeFunction\r\n- #deserializeFunction uses static initialization flag to detect whether it has to load jython instead of waiting for exception to happen\r\n- removed file cleanup in #initAndExecPythonScript as it is the binders' responsibility\r\n\r\nConnectors:\r\n- replaced usage of deprecated serialiation schema interfaces\r\n- P(S/D)Schema#(de)serialize now fails with RuntimeException if schema deserialization fails\r\n- remove kafka code\r\n\t- not really tested, and I'd rather tackle connector support in a follow-up\r\n\r\nFunctions:\r\n- Introduced AbstractPythonUDF class for sharing RichRunction#open()/close() implementations\r\n- PythonOutputSelector now throws FlinkRuntimeException when failing during initialization\r\n- added generic return type to Serializationutils#deserializeObject\r\n- added new serializers for PyBoolean/-Float/-Integer/-Long/-String\r\n- PyObjectSerializer not properly fails when an exceptioin occurs\r\n- improved error printing\r\n- PythonCollector now typed to Object and properly converts non-PyObjects\r\n- jython functions that use a collector now have Object has output type\r\n\t- otherwise you would get ClassCastException if jython returns something that isn't a PyObject\r\n\r\nPythonStreamBinder\r\n- adjusted to follow PythonPlanBinder structure\r\n- client-like main() exception handling\r\n- replaced Random usage with UUID.randomUIID()\r\n- now loads GlobalConfiguration\r\n- local/distributed tmp dir now configurable\r\n\t- introduced PythonOptions\r\n- no longer generate plan.py but instead import it directly via the PythonInterpreter\r\n\r\nEnvironment:\r\n- Reworked static environment factory methods from PythonStreamExecutionEnvironment into a PythonEnvironmentFactory\r\n- program main() method now accepts a PythonEnvironmentFactory\r\n- directories are now passed properly to the environment instead of using static fields\r\n- removed PythonEnvironmentConfig\r\n. #registerJythonSerializers now static\r\n\r\nExamples:\r\n- move examples to flink-streaming-python\r\n- change examples location in dist to examples/python/streaming\r\n- replace ParameterTool usage with argparse\r\n- pass arguments via run instead of constructor\r\n- remove 'if __name__ == '__main__':' block\r\n- remove exception wrapping around source/sink creation\r\n- add WordCount example\r\n\r\nTests:\r\n- removed 'if __name__ == '__main__':' blocks from tests since the condition is never fulfilled\r\n- removed python TestBase class\r\n- removed print statements from tests\r\n- standardized test job names\r\n- cleaned up PythonStreamBinderTest / made it more consistent with PythonPlanBinderTest\r\n- run_all_tests improvements\r\n\t- stop after first failure\r\n\t- print stacktrace on failure\r\n\t- no longer relies on dirname() to get cwd but uses the module file location instead\r\n- added log4j properties file\r\n- added end-to-end test", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5332", "title": "[FLINK-6464][streaming] Stabilize default window operator names", "body": "## What is the purpose of the change\r\n\r\nThis PR modifies the generation of default operator names for window operators to be more concise, and more importantly, stable across job submissions.\r\n\r\nExample: SocketTextWordCount\r\n\r\nBefore:\r\n```\r\nTriggerWindow(TumblingProcessingTimeWindows(5000), ReducingStateDescriptor{serializer=org.apache.flink.api.java.typeutils.runtime.PojoSerializer@591ae253, reduceFunction=org.apache.flink.streaming.examples.socket.SocketWindowWordCount$1@48974e45}, ProcessingTimeTrigger(), WindowedStream.reduce(WindowedStream.java:241))\r\n```\r\n\r\nAfter:\r\n```\r\nWindow(TumblingProcessingTimeWindows(5000), ProcessingTimeTrigger, WindowedStream.reduce(WindowedStream.java:243))\r\n```\r\n\r\n## Brief change log\r\n\r\n* create shared static utility method to generate names\r\n* remove state-descriptor from operator name\r\n* replace `TriggerWindow` with `Window`\r\n* trigger/assigner are now included with their simple class name instead of fully qualified class name + object reference\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5331", "title": "[FLINK-8473][webUI] Improve error behavior of JarListHandler", "body": "## What is the purpose of the change\r\n\r\nThis PR modifies the `JarListHandler` to not crash when the jar storage directory has been deleted. This caused the job-submission-page in the webUI to be completely blank.\r\n\r\nWe now log a warning (to make debugging easier) and continue on as if the directory was empty (so that the page isn't blank).\r\n\r\n## Verifying this change\r\n\r\nManually verified and not covered by tests, like the rest of the handler.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5163", "title": "[FLINK-8254][REST][docs] Bandaid generated documentation", "body": "## What is the purpose of the change\r\n\r\nThis is a bandaid PR that replaces the generated file with a correct version and updates the instructions with a workaround.\r\n\r\n## Verifying this change\r\n\r\nManually verified.\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5161", "title": "[FLINK-7608][metric] Refactor latency statistics metric", "body": "This PR is an extension of #4826.\r\n\r\nThe additions addresses issues that I found while trying it out. Some of these unfortunately may be rather controversial, hence why I'm curious what others think.\r\n\r\n#### 1)\r\nThe first (and probably less-controversial) addition is to register the metric on the job level with user-defined variables for proper integration with tag-based reporters. As described in #4826 the latency metric doesn't fit as a operator/task metric since it describes a relation between 2 operators.\r\n\r\n#### 2)\r\nThe second problem i found was that the latency marker uses `StreamConfig#getVertexID` to retrieve the source/operator vertexID. This is however not the actual runtime ID, but just a counter that is incremented when the job is generated, and thus utterly pointless and impossible to correlate with a job's operators. \r\nThis change requires serializing 2 longs instead of an int along with creating an `OperatorID` during deserialization, making the latency measuring more costly.\r\n\r\n#### 3)\r\nThe third and last change that I made is removing the case distinction between operators and sinks. Previously, operators would ignore the source subtask index and tread all subtasks equally. Only sinks measured latency separately for the source subtask.\r\nDespite the performance benefits I find this design rather questionable:\r\n* latency metric names for operators&sinks are inconsistent, and there is in fact no way to tell whether something is an operator or sink without access to the job source code which means to query the metric you have to resort to trial-and-error\r\n* there are operators with sink semantics (like the write-ahead sink), so we aren't consistent in terms of \"all sinks have detailed latency metrics\" and are leaking implementation details to the outside\r\n* considering sink latency metrics as more important is rather arbitrary and may very well not fit a user's requirements\r\n\r\nInstead, we may want to think about is adding a separate switch for detailed latency metrics.\r\n\r\n/cc @aljoscha @rmetzger @yew1eb \r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5156", "title": "[FLINK-8079][tests] Stop end-to-end test execution after first failure", "body": "## What is the purpose of the change\r\n\r\nThis PR modifies the travis maven script to stop the execution of end-to-end tests once the first failure was detected.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5155", "title": "[FLINK-4812][metrics] Expose currentLowWatermark for all operators", "body": "Revised version of #5125.\r\n\r\n## What is the purpose of the change\r\n\r\nWith this PR all operators expose the current input/output watermark through the metric system.\r\n\r\nInput watermarks for the head operator are measured in the StreamInputProcessors.\r\nInput watermarks for chained operators, and all output watermarks are measured in the output of each operator.\r\n\r\n## Brief change log\r\n\r\n* Introduce new `[Min]WatermarkGauge` classes to measure watermarks\r\n* modified `Output`s used by the `OperatorChain` to create and expose a `WatermarkGauge`\r\n* modified `Stream[Two]InputProcessor` to accept metrics through the constructor instead of a separate method\r\n* added `getInputWatermarkGauge()` method to the `StreamTask` class; the retrieved gauge is given to the `OperatorChain` to register on the operator level\r\n\r\n* added test utility `InterceptingOperatorMetricGroup` class to expose registered metrics\r\n* added tests\r\n* updated documentation\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\nRun\r\n* WatermarkGaugeTest\r\n* MinWatermarkGaugeTest\r\n* OneInputStreamTaskTest\r\n* TwoInputStreamTaskTest\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: ( no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (yes (per-watermark codepaths))\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes)\r\n  - If yes, how is the feature documented? (docs)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5006", "title": "[hotfix][docs][QS] MInor cleanup of QS documentation", "body": "## What is the purpose of the change\r\n\r\nThis PR fixes a few smaller issues in the Queryable State documentation/javadocs.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4975", "title": "[FLINK-7991][examples][kafka] Cleanup kafka10 example jar", "body": "## What is the purpose of the change\r\n\r\nThis PR cleans up the kafka example shading configuration, removing plenty of unnecessary classes from the resulting jar along with several ineffective inclusions.\r\n\r\n## Brief change log\r\n\r\n* remove inclusions for zk, curator, jute, I0Itex, jline and jammer as they are ineffective\r\n* narrow down `org.apache.flink.streaming` inclusion to only include the kafka connector\r\n\r\n## Verifying this change\r\n\r\n* Build jar before and after and compare contents", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4932", "title": "[FLINK-7958][metrics] Allow reporters to define default delimiter", "body": "## What is the purpose of the change\r\n\r\nThis PR allows reporters to define their default delimiter. Previously we always defaulted to `.`.\r\n\r\nReporters may now implement the `DelimiterProvider` interface with which they may define a delimiter. This delimiter is used if no delimiter was explicitly configured for the reporter.\r\n\r\n## Brief change log\r\n\r\n* Add DelimiterProvider interface\r\n* modify Prometheus&JmxReporter to define the default delimiter\r\n* modify delimiter setup in the MetricRegistry\r\n* add a test to the MetricRegistryTest to verify functionality\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\nRun MetricRegistryTest#testDelimiterOverride.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): ( no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: ( no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes)\r\n  - If yes, how is the feature documented? (javadoc)\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4743", "title": "[FLINK-7727] [REST] Improve error logging in StaticFileServerHandlers", "body": "## What is the purpose of the change\r\n\r\nThis PR improves the logging of errors in the StateFileServerHandlers\r\n\r\n## Brief change log\r\n\r\n* separate checks for file existence and path not pointing to a directory\r\n* pointing to a directory now returns `405 Method Not Allowed`\r\n* pointing outside the root directory now returns `403 Forbidden`\r\n* add a debug logging message to each check\r\n* re-order checks, so that we first check whether the path points into the root directory before checking if it exists, to not leak information about resources the user may not access\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4645", "title": "[FLINK-7582] [REST] Netty thread immediately parses responses", "body": "## What is the purpose of the change\r\n\r\nThis PR modifies the RestClient to make the netty receiver threads parse responses (and complete the associated future) immediately after receiving a response, instead of deferring the parsing to a separate executor.\r\n\r\nThis guarantees that we don't buffer responses that haven't been parsed yet. This was previously the case if the executor could not keep up with the response rate, for example because it was used exclusively for sending requests.\r\n\r\n## Brief change log\r\n\r\n* Make sending synchronous\r\n* Netty threads immediately parse messages after receiving them\r\n* remove (now unused) executor in RestClient\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4110", "title": "[FLINK-6900] [metrics] Limit size of metric name components", "body": "This PR modifies the `ScheduledDropwizardReporter` to limit the size of every metric name component to 80 characters, with the same reasoning as #4109.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3899", "title": "[FLINK-6557] Fix RocksDBKeyedStateBackend on Windows", "body": "This PR fixes the `RocksDBKeyedStateBackend` on Windows. The backend was passing file paths generated from a Flink `Path` directly to RocksDB, and by extension to the native file system.\r\n\r\nBesides the actual problem being caused this was questionable anyway since the starting point for all this is actually a `File` (`stateBackend.instanceBasePath`). It is first converted to a String, then to a Path, then to a String again before passing it to RocksDB. We were just roller coasting through the abstraction layers.", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/45841376", "body": "I reworked the serializer/comparator again. It now uses the first bit of every byte to indicate whether there is at least one more byte coming. \n\nThis has the bonus that all letters are serialized as one byte (opposed to the previous version which could only do this for the numbers and a few special characters (which actually made the variable length encoding pointless...))\n\n~~I currently do a selective shift starting on the flag positions to make space for them. I wonder if there is a more efficient way to do that, here's an example how i do it:~~\n\n~~char to send:~~\n~~`0010 0110 1111 1001`~~\n~~1) move the lowest 7 bits into a tmp variable (by doing & with 0000 0000 0111 1111)~~\n~~`0000 0000 0111 1001`~~\n~~2) shift char to the right by 7 positions to omit the lower part~~\n~~`0000 0000 0100 1101`~~\n~~3) shift char to the right by 8 positions (finalizing the shifting of the upper part)~~\n~~`0100 1101 0000 0000`~~\n~~4) char |= tmp~~\n~~`0100 1100 0111 1001`~~\n~~(this would be done resursively for every flag position needed, starting from the right, so up to 3 times)~~\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45841376/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/46012075", "body": "the new serialization seems to take longer, but due to faster deserialization in total it breaks even.\n\nsome comparison benchmarks:\n\nstrings of length 90\n1000 repetitions\ncode for New measurement:\n\n```\nlong startTime = System.nanoTime();\ncmp = StringValue.compareUnicode(in1, in2);\nlong endTime = System.nanoTime();\n```\n\ncode for Old measurement:\n\n```\nlong startTime = System.nanoTime();\ncmp = StringValue.readString(in1).compareTo(StringValue.readString(in2));\nlong endTime = System.nanoTime();\n```\n\nresult = SUM(endTime - startTime) / 1000\n\nequality\nNew 26627\nOld 25782\n\naffix (difference in the beginning of the sring)\nNew 4259\nOld 23431\n\ninfix (difference in the middle of the string)\nNew 13560\nOld 30757\n\nsuffix (difference at the end of the string)\nNew 29385\nOld 28293\n\nnot particularly surprising results. no progress on the prefix-issue :/\n\nadding some more tests now.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/46012075/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50600322", "body": "i feel like we are going in circles^^\n\ntake the `IterableIterator` class, rename it to `Iterator`, add into docu/javadocs \"Iterator on which the for-each statement can be used. the for-each loop starts at the element that next() would return\".\n\ntraversable-once characteristic is obvious, supports for-each statement, solves these odd exception issues andd the differences to a normal iterator can be summed up in 2 lines. bonus: less API breaking, since its just an import that has to be changed.\n\nwe cant use a name that makes all properties obvious (because `IterableIterator` is an odd name). the for-each statement not being obviously usable is imo a lot more desirable than then user creating faulty programs hitting exceptions based on false premises how the `Iterable`works.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50600322/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50606601", "body": "license should be fixed now.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50606601/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50639888", "body": "so for mappable:\n- why not go with the previous name? `GenericMapper`\n- or drop the \"generic\" part? `Mapper`\n- or go even shorter? `Map`\n- maybe append \"SAM\" ? `MapSAM`/`MapperSAM`\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50639888/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/17574494", "body": "this can produce wrong metrics for the Batch API. Metrics directly associated with a job will be reset if at any point a given TM has no tasks for a job.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/17574494/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/17693130", "body": "this change has broken the scope formats.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/17693130/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/17693170", "body": "where is this done now?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/17693170/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/18251972", "body": "will this condition never be true?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18251972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/18257970", "body": "ok.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18257970/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19427630", "body": "This error message is misleading, as it is also printed if the configured size is 0.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19427630/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19845479", "body": "yes it does.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19845479/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19845480", "body": "yes, if the parent thread is a daemon thread.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19845480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19845484", "body": "it can have a different priority if the thread group to which it belongs has a different priority.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19845484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19845489", "body": "will fix it.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19845489/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/20373267", "body": "that's true, preparing a PR for that.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/20373267/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/21839439", "body": "This line shouldn't be removed.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/21839439/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/21839446", "body": "This line shouldn't be removed.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/21839446/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/22120213", "body": "isn't this breaking the checkstyle rules?", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/22120213/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/24399898", "body": "Why do you think this is invalid? http://www.scala-lang.org/old/sites/default/files/linuxsoft_archives/docu/files/api/scala/throws.html", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/24399898/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/25875079", "body": "maybe we should have a wildcard exclude for all files/directories starting with `.`.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/25875079/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/26033587", "body": "remove empty line", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/26033587/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/26062390", "body": "did you intentionally place this import here?", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/26062390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/26885301", "body": "What benefit is there in referencing a module that doesn't exist?", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/26885301/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/27018442", "body": "@tillrohrmann How about adding a null-check for the headers in `AbstractRestHandler`?", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/27018442/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/13638364", "body": "good catch, its a relic from when this method wasn't synchronized. will fix that.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/13638364/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "StephanEwen": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/d6b8505f7efd6f514fd16ff7cbd5f74cfcfde3c4", "message": "[FLINK-8461] [build] Adjust logger configurations for shaded Netty classnames"}, {"url": "https://api.github.com/repos/apache/flink/commits/d57215497f36f87939c1cdf3b090d00d8e59d90b", "message": "[FLINK-8455] [core] Make 'org.apache.hadoop.' a 'parent-first' classloading pattern.\n\nThis change avoid duplication of Hadoop classes between the Flink runtime and the user code.\nHadoop (and transitively its dependencies) should be part of the application class loader.\nThe user code classloader is allowed to duplicate transitive dependencies, but not Hadoop's\nclasses directly.\n\nThis also adds tests to validate parent-first classloading patterns."}, {"url": "https://api.github.com/repos/apache/flink/commits/f9aff5926254ca05da77ea856e72bb2c71dae7fd", "message": "[hotfix] [tests] Add unit tests for ChildFirstClassLoader"}, {"url": "https://api.github.com/repos/apache/flink/commits/1f9c2d9740ffea2b59b8f5f3da287a0dc890ddbf", "message": "[hotfix] [core] Fix checkstyle for 'flink-core' : 'org.apache.flink.configuration'"}, {"url": "https://api.github.com/repos/apache/flink/commits/212ee3d430190e6e771c79a9b94fd8410675a534", "message": "[hotfix] [core] Fix checkstyle for 'flink-core' : 'org.apache.flink.util'"}, {"url": "https://api.github.com/repos/apache/flink/commits/1e6a91a3bf7e734eccdb034ce505b3775b709265", "message": "[hotfix] [core] Move 'ThrowingConsumer' and 'RunnableWithException' to proper package (.util.function)\n\nThis also adds missing stability annotations to the functional interfaces in 'util.function'."}, {"url": "https://api.github.com/repos/apache/flink/commits/edc6f1000704a492629d7bdf8cbfa5ba5c45bb1f", "message": "[FLINK-5823] [checkpoints] State backends now also handle the checkpoint metadata"}, {"url": "https://api.github.com/repos/apache/flink/commits/d19525e90e69ddd257d47371b8ea0319fa4673c8", "message": "[FLINK-5823] [checkpoints] Make RocksDB state backend configurable"}, {"url": "https://api.github.com/repos/apache/flink/commits/1931993bdc1d294a0eb9e1ad727f737cf64fe150", "message": "[hotfix] [rocksdb] Clean up RocksDB state backend code\n\n  - arrange variables to properly express configuration (client side) versus runtime (task manager side)\n  - make all runtime-only fields properly transient\n  - fix confusing variable name for local directories"}, {"url": "https://api.github.com/repos/apache/flink/commits/fa03e78d3a245b40ceb3efffeb3020853e74e48b", "message": "[FLINK-5823] [checkpoints] State backends define checkpoint and savepoint directories, improved configuration"}, {"url": "https://api.github.com/repos/apache/flink/commits/7d820d6fe17341463b2a0f9cd1cea1ef085eed21", "message": "[FLINK-5823] [checkpoints] Pass state backend to checkpoint coordinator"}, {"url": "https://api.github.com/repos/apache/flink/commits/0030d6ab21197077438ba05654a5af353bc1acb7", "message": "[hotfix] [core] Fix broken JavaDoc links in ConfigConstants"}, {"url": "https://api.github.com/repos/apache/flink/commits/e52db8bc411e93c245cc78a278854f2653e5f384", "message": "[FLINK-7925] [checkpoints] Add CheckpointingOptions\n\nThe CheckpointingOptions consolidate all checkpointing and state backend-related\nsettings that were previously split across different classes."}, {"url": "https://api.github.com/repos/apache/flink/commits/7034e9cfcb051ef90c5bf0960bfb50a79b3723f0", "message": "[hotfix] [core] Add a factory method to create Path from local file\n\nThis makes it easier for users and contributors to figure out how\nto create local file paths in way that works cross operating systems."}, {"url": "https://api.github.com/repos/apache/flink/commits/a49f0378c3ad7c9b02ea3a94e44e73e4dcbeafa3", "message": "[hotfix] [core] Pre-compile regex pattern in Path class"}, {"url": "https://api.github.com/repos/apache/flink/commits/1d38e0b49a1936fef477a7a2a65abdd815f2d695", "message": "[hotfix] [checkpoints] Improve performance of ByteStreamStateHandle\n\nThe input stream from ByteStreamStateHandle did not overwrite the 'read(byte[], int, int)' method,\nmeaning that bulk byte reads resulted in many individual byte accesses.\n\nAdditionally, this change avoids accessing the data array through an outer class, but instead adds\na reference directly to the input stream class, avoiding one hop per access. That also allows\na more restricted access level on the fields, which may additionally help the jitter in some cases."}, {"url": "https://api.github.com/repos/apache/flink/commits/0ed264b55560ab573f0086313c37f3110e99d49c", "message": "[hotfix] [hdfs] Avoid re-parsing URIs for all Hadoop File System calls.\n\nPreviously, this converted Flink paths (internally URIs) to strings and\nthen let the Hadoop Paths parse, validate, and normalize the strings to\nURIs again.\n\nNow we simply pass the URIs directly."}, {"url": "https://api.github.com/repos/apache/flink/commits/4319725cfccd40fb063d126fd5fc36dc5feec158", "message": "[hotfix] [core] Improve local fs exists() performance\n\nThis avoids going though an exception in the case of non-existing files."}, {"url": "https://api.github.com/repos/apache/flink/commits/6360875f12a102612bbb6c79bed807712285e116", "message": "[hotfix] [checkpoints] Remove never used method 'close()' on CheckpointStreamFactory\n\nThe fact that the method was never called (and never implemented) strongly suggests\nthat it should be removed, otherwise someone might eventually end up implementing\nit for a new state backend and wonder why it is never called."}, {"url": "https://api.github.com/repos/apache/flink/commits/b82f59f64326e81cd8d1703c859d13485dff6958", "message": "[hotfix] [core] Avoid redundant File path conversion in LocalFileSystem.getFileStatus(Path)"}, {"url": "https://api.github.com/repos/apache/flink/commits/bc588aac3a6cfe756351d8ce7d86143048f6e608", "message": "[hotfix] [tests] Remove unnecessary stack trace printing in StreamTaskTest"}, {"url": "https://api.github.com/repos/apache/flink/commits/3d0ed12edab5e1b89db0829230e69fb6ef841b7e", "message": "[FLINK-8373] [core, hdfs] Ensure consistent semantics of FileSystem.mkdirs() across file system implementations."}, {"url": "https://api.github.com/repos/apache/flink/commits/c7c72704ab8827245d08850edf3d9a448d18097f", "message": "[FLINK-8374] [yarn,tests] Whitelist meaningless exception that may occur during Akka shutdown."}, {"url": "https://api.github.com/repos/apache/flink/commits/bc2efdd7dd47bc42ef1a43310ba5f2df60bfc424", "message": "[hotfix] [core] Add comments for class loading config options in CoreOptions"}, {"url": "https://api.github.com/repos/apache/flink/commits/28d9b20da6e0b79306ef3ebbbb406371da15392f", "message": "[FLINK-8264] [core] Add 'scala.' to the 'parent-first' classloading patterns.\n\nThis closes #5166"}, {"url": "https://api.github.com/repos/apache/flink/commits/840cbfbf0845b60dbf02dd2f37f696f1db21b1e9", "message": "[FLINK-8263] [quickstarts] Correctly set 'flink-core' to provided in Scala Quickstart pom.xml"}, {"url": "https://api.github.com/repos/apache/flink/commits/a2ed8768c67b33d0b3a365d4ee768d7301af5153", "message": "[hotfix] [quickstarts] Fix indentation in Java Quickstart pom.xml"}, {"url": "https://api.github.com/repos/apache/flink/commits/9f0b790d42925959b54e7edf5e6bf6d3cfb67ec7", "message": "[hotfix] [quickstarts] Exclude not only slf4j api, but also bridges"}, {"url": "https://api.github.com/repos/apache/flink/commits/d2a3a2790c974de5e0246f30adef7d34f82d5ab5", "message": "[FLINK-8261] [quickstarts] Fix typos in exclusion patterns."}, {"url": "https://api.github.com/repos/apache/flink/commits/2d4762c6c0bc73845549575f21fd3f8dbc466aa9", "message": "[FLINK-8198] [core] Fix condition for parsing ConnectionLimitingSettings"}, {"url": "https://api.github.com/repos/apache/flink/commits/23ea197b751a98d10a1bd549175f2566f3a7c227", "message": "[hotfix] [core] Fix remaining checkstyle issues for 'core.fs'"}, {"url": "https://api.github.com/repos/apache/flink/commits/b051a4c887c46b7f25a2fc3e2a866792d17ae1ac", "message": "[FLINK-8196] [build] Remove 'javax.servlet' dependency exclusion."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5330", "title": "[FLINK-8406] [bucketing sink] Fix proper access of Hadoop File Systems", "body": "## What is the purpose of the change\r\n\r\nFixes the access to Hadoop file systems when initializing the BucketingSink.\r\n\r\n## Verifying this change\r\n\r\nThe PR adds a unit test for the problem.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / **no)**\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no)**\r\n  - The serializers: (yes / **no** / don't know)\r\n  - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)\r\n  - The S3 file system connector: (yes / **no** / don't know)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes / **no)**\r\n  - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/45891773", "body": "Nice one!\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45891773/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45892412", "body": "`taskmanager.io.numInThreads` is a bit ambiguous with respect to disk I/O versus net I/O threads?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45892412/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45919574", "body": "Looks good. I will merge that for the 0.6 release.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45919574/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45920114", "body": "Nice. Will merge that for 0.6\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45920114/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45962652", "body": "Merged in 856d5569c62c9a74be24750444c589ee11c4ceb0\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45962652/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45963021", "body": "Merged in edf6c4c18303b30e6d0180fd45f78828966511a8\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45963021/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45997121", "body": "Okay, that looks good. The other flags (max perm gen size, mark sweep GC, mark sweep class unloading) are unaffected by that?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45997121/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45997219", "body": "When we pass the flags, we make sure that Xms and -Xmx are the same, to prevent the JVM from being funny and resizing the heap from time to time. I think we loose that with this option. But I guess its all right, on cluster setups we would expect those to be set anyways.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45997219/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45997480", "body": "Looks good, will merge\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45997480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45997906", "body": "This is a nice change. Do you have some micro-benchmark numbers on how the methods perform in comparison to the old `writeString()` methods?\n\nAlso, it would be interesting what the benefit of doing binary comparisons is. So, a microbenchmark of sorting the strings with a normalizedKeySorter. And a case where all prefixes are equal.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45997906/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45998219", "body": "Also, can you add a few more strings where some of the branches get executed? Like collisions on the high byte of a code point, but differences in the low bytes (and vice versa) ?\n\nThis code needs to be absolutely robust, or it will lead to impossible-to-debug situations.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45998219/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45998342", "body": "Let's not do this ourselves. I think it is fine. If you want a robust setup, set the values. Our YARN scripts also set them automatically, based on the container size, afaik.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45998342/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45998358", "body": "I will merge this...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45998358/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45998419", "body": "This is merged in f952638360cf8c86e334fb5fc56342272ceb62e5\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45998419/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50344119", "body": "Nice one, looks very good to me.\n\nI think we should make this part of the first release candidate.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50344119/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50344798", "body": "An enhancement could be to exclude some of the hadoop dependencies for Hadoop 1. In Hadoop 1, there is no separation between core, mapred, hdfs, client, etc. So we have all the dependencies (JSP, jettison, jetty, hsqldb), which are most likely not needed.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50344798/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50500024", "body": "Okay, then let us merge it into master. I think that afterwards, we can fork the 0.6 release branch to prepare the first release candidate.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50500024/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50500138", "body": "We decided on a feature freeze a while back. This will go into the next version.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50500138/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50500769", "body": "Wow, this looks like some serious stuff.\n\nIt also looks well tested :-) If your cluster tests indicate that it works, I am in favor of adding it soon.\n\nThis is rather critical and readily tested, so should it go into the release candidate?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50500769/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50517219", "body": "I guess that (a) is also the only variant to make it work in Eclipse, since m2e does not seem to evaluate these profiles. Eclipse users with java < 8 need to close that subproject then.\n\nCan we make a subproject of a flink-tests (and still have code in flink-tests) ?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50517219/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50518588", "body": "Throwing an error for a second `iterator()` call would be possible, but quite some change.\n\nFor example in CoGroup, I use the simply `Collections.emptySet()` as the Iterable for empty sides. The `KeyGroupedIterable` for the reduce function also simply returns itself as the iterator, which means the original lightweight grouping logic can stay the same.\n\nAlso, the some of old internal tests (Record-based) make use of the possibility to mix the _Iterable_ and the _Iterator_. I could change that, if needed.\n\nWe could make this change, but I would like to hear some opinion on whether this is actually more a confusing trap (\"Why the heck do I get an empty set the second time I iterate over the input\"), or whether this is actually understood that these inputs are what Scala calls a _TraversableOnce_\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50518588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50582983", "body": "In that sense, having an `Iterator`rather than an `Iterable` makes things more obvious (the traversable-once characteristic). \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50582983/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50585614", "body": "In `DataSet` you have checks for Map, FlatMap, GroupReduce, to check that no currently unsupported Lambdas are used.\n\nCan you add the same checks for Join, Cross, CoGroup?\n\nI will start working on a java-8 only subproject.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50585614/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50636733", "body": "I agree, the names are not perfect. Anyone has a good for names?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50636733/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50667122", "body": "I found a good way to detect lambdas. It only works if the SAM interface is serializable, but ours are always.\n\nThe trick is to search for the `writeReplace` method from serializable objects and see whether it returns a `SerializedLambda`.\n\n```\npublic interface LambdaFunction extends java.io.Serializable {\n    String doComputation(Integer value);\n}\n\npublic static void main(String[] args) throws Exception {\n    LambdaFunction func = (theInteger) -> \"string \" + String.valueOf(theInteger);\n\n    for (Class<?> clazz = func.getClass(); clazz != null; clazz = clazz.getSuperclass()) {\n        try {\n            Method replaceMethod = clazz.getDeclaredMethod(\"writeReplace\");\n            replaceMethod.setAccessible(true);\n            Object serializedForm = replaceMethod.invoke(func);\n\n            if (serializedForm instanceof SerializedLambda) {\n                SerializedLambda sl = (SerializedLambda) serializedForm;\n                System.out.println(sl);\n                break;\n            }\n        }\n        catch (NoSuchMethodError e) {\n            // fall through the loop and try the next class\n        }\n        catch (Throwable t) {\n            t.printStackTrace();\n            return;\n        }\n    }\n}\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50667122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50670878", "body": "Simple type extraction from Lambdas (limited to non-generic parameters):\n\n``` java\n    public static Class<?>[] getLambdaParameters(Object lambda) {\n        for (Class<?> clazz = lambda.getClass(); clazz != null; clazz = clazz.getSuperclass()) {\n            try {\n                Method replaceMethod = clazz.getDeclaredMethod(\"writeReplace\");\n                replaceMethod.setAccessible(true);\n                Object serializedForm = replaceMethod.invoke(lambda);\n\n                if (serializedForm instanceof SerializedLambda) {\n                    SerializedLambda sl = (SerializedLambda) serializedForm;\n                    return getTypesFromSerializedLambda(sl);\n                }\n            }\n            catch (NoSuchMethodException e) {\n                // fall through the loop and try the next class\n            }\n            catch (Throwable t) {\n                throw new RuntimeException(t);\n            }\n        }\n\n        throw new IllegalArgumentException(\"Not a serialized Lambda\");\n    }\n\n    public static Class<?>[] getTypesFromSerializedLambda(SerializedLambda sl) throws Exception {\n        String sig = sl.getImplMethodSignature();\n\n        if (!sig.startsWith(\"(\")) {\n            throw new Exception(\"Parse Error\");\n        }\n\n        String parameters = sig.substring(1, sig.indexOf(')'));\n        String[] params = parameters.split(\";\");\n\n        List<Class<?>> classes = new ArrayList<>();\n\n        for (String p : params) {\n            if (!p.startsWith(\"L\")) {\n                throw new Exception(\"Parse Error\");\n            }\n\n            p = p.substring(1);\n            p = p.replace('/', '.');\n            classes.add(Class.forName(p));\n        }\n\n\n        return (Class<?>[]) classes.toArray(new Class<?>[classes.size()]);\n    }\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50670878/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50675228", "body": "I have implemented the suggestion to let the `Iterables` return the `Iterator` once, and throw a meaningful exception otherwise.\n\nThe old record API remains as it was before.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50675228/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50731751", "body": "I am with you. Mapper, Reducer is fine. And Join, Cross, Filter (without _-er_) would also be fine.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50731751/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50836623", "body": "I have addressed the comments and made the Iterables traversable for a single time\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50836623/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/6755062", "body": "As a general comment: I think such blocks can also catch `Throwable`, if it is crucial that they never fail and return just \"unknown\" in the worst case.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/6755062/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/7998721", "body": "Doesn't this fail the hadoop 2.5.0 builds?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/7998721/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/8118729", "body": "I agree. Must have been accidentally. Is that the reason for the shutdown hang?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/8118729/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/8201282", "body": "An iterator in scala allows \"for comprehensions\", correct?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/8201282/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/8554785", "body": "Why is there a `4` as a parameter here and nowhere else?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/8554785/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/8554795", "body": "Should this be a `4` or no argument at all?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/8554795/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/9381873", "body": "This seems a bit wrong here. The caller calls the `next()` function, which creates an element (potentially large) to pass it to the `next(T reuse)` function, which in turn ignores the value.\n\nI would move the logic to the `next()` function and have the `next(T reuse)` function simply call the `next()` function, ignoring any reusable element.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/9381873/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/9381927", "body": "Similar as for the inline comment above, I would switch the logic between `next()` and `next(T reuse)` here to prevent creating unnecessary and ignored objects\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/9381927/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/9582468", "body": "I think it would actually be fine to throw an error in that case and abort the start. There is obviously something very wrong, and this is a good point to loudly mention it, rather than having people wonder about it later why the JVMs are so small...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/9582468/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/12961159", "body": "Would be good if that one was moved to `flink-core` as well, if the class it tests was moved to `flink-core`.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/12961159/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/12961618", "body": "If the method is only to draw a serialization copy, we could also add that to the `InstantiationUtil` or use `commons-lang`'s `SerializationUtil` class.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/12961618/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/12965831", "body": "At some points there were thoughts about a \"hadoop-free\" version. How would this play together?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/12965831/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/12965891", "body": "I think this is a good addition. In the future (Hadoop not present), we may have to go for reflection, true.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/12965891/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/14333396", "body": "true, so this gives formatting errors on Windows\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14333396/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/14478619", "body": "BTW: I think the usual style for git commit messages is to write what the commit does, not what the original problem was. So something like \"Fix Cancel Button for YARN\" would be better...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14478619/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15679008", "body": "That's a good idea!\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15679008/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15890284", "body": "So, actually is that exception comes, we can safely ignore it?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15890284/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15932805", "body": "In the current master, the `DataStream` class imports the correct `o.a.f.api.common.operators.Keys`.\n\nhttps://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java#L43\n\nIs it possible that your Maven cache has mixed versions?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15932805/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15944874", "body": "No problem, it's quite a common problem that maven caches get inconsistent.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15944874/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/17644300", "body": "Sorry, this was a partial change that accidentally sneaked into this patch.\nMy bad :-(\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/17644300/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/18116237", "body": "It makes the code nice and clean if you use a final field with a `SerializableObject`.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18116237/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/18252297", "body": "The pattern is\n- cache the reference on stack (immutable against concurrent modifications)\n- set the heap reference to null\n- proceed based on the stack reference\n\nI think that should work. If the heap reference was non null initially before, the stack reference is non null, and the condition is true.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18252297/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/18252310", "body": "The change I made was to clear the heap references earlier. Less chance of redundant work when concurrent disposals happen.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18252310/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/18339131", "body": "Quick question in this: Why don't we make the default behavior of the JMX reporter to not start an extra server, and only start an extra server if a port parameter is set?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18339131/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/18339788", "body": "Let's leave the remainder as it is for now, at least as long as we do not have the name collisions fully resolved. I am filing a followup issue about JMX and names.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18339788/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/18778229", "body": "Does this have the wrong issue tag?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18778229/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/18789532", "body": "There is some followup discussion on this in the JIRA issue: https://issues.apache.org/jira/browse/FLINK-3677\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18789532/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/18789807", "body": "Can I re-iterate on this issue?\n\nDo we need tests here that fire up Kafka clusters? Those are very time intensive, they are tough to harden so that the build is reliable, and what do they really add here?\nAssuming the the Kafka connector works, this should test the \"add on\" by the TableSource. That can be done with a mock FlinkKafkaConsumer.\n\nI think it would be very good to update this.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18789807/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/18811350", "body": "Does it make sense to mention that one can set the parallelism of the source independently to the number of Kinesis shards?\n\nThe original reporters of the issue thought they would have to set the parallelism of the entire program to the number of Kinesis shards.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18811350/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/18811359", "body": "You could have just used `Arrays.asList(\"**/another_file.bin\", \"**/dataFile1.txt\")` here. The function takes varargs.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18811359/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/18829377", "body": "Just FYI: For immutable lists without Guava (and without copying the elements), you can always use `Collections.unmodifiableList(list)`.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18829377/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19375216", "body": "I am confused here. What does it help to make this \"serializable\", if the most important member (the KafkaTestEnvironment) is transient? Seems this cannot be serialized anyways...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19375216/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19375231", "body": "Is this a test for the Kafka producer? Otherwise, it seems that using directly the Kafka Producer rather than running a dedicated Flink program is simpler and more robust for a test.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19375231/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19375265", "body": "Can this commented-out code be removed?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19375265/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19376219", "body": "That seems like a hack that throws the next person looking into this code off. I think pulling the function out into a dedicated (static, non-anonymous) class is the clean way to go.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19376219/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19844923", "body": "That means that the thread numbering starts at `2`, right?", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19844923/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19844935", "body": "Can new threads ever be daemons by default?", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19844935/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19844939", "body": "Can a new thread ever have a different priority be default? Is that inherited from the parent thread or thread group?", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19844939/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/20372951", "body": "It would be nice to consistently use the `OperatingSystem` class in the code, rather than re-implement the OS name parsing.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/20372951/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/20606338", "body": "It is not a problem, because for processing time, nothing is ever late, by definition. Allowing for late elements (even when nothing is late) is not a problem.\r\n\r\nThat way, it behaves even more consistently between event time and processing time.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/20606338/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/21003603", "body": "Should we make `flushOnCheckpoint` true by default?", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/21003603/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/21018833", "body": "Great to have this in!\r\nI would suggest to mention in the JavaDocs of `MapState.size()` that this can be a potentially expensive operation, because it may entail iterating over many entries in some state backends (like RocksDB).", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/21018833/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/21337504", "body": "I think this results in unnecessary boxing of `time`.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/21337504/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/21347624", "body": "True, I just noticed it here and in the spirit of continuous improvement made a comment ;-)", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/21347624/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/22021616", "body": "Just saw that the default values used in the options here are different than those in the shell scripts.\r\nDoes it make sense to keep those in sync?", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/22021616/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/22558742", "body": "What about negative seek values here?", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/22558742/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/22558746", "body": "Good change, but I think this could use a guarding unit test.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/22558746/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/26715452", "body": "I don't think this is needed. The thread status already tells you if it is finished...", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/26715452/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/26715541", "body": "Let's factor this out to https://github.com/apache/flink/tree/master/flink-core/src/main/java/org/apache/flink/util/function\r\n\r\nThis class is probably helpful in other contexts as well...", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/26715541/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/26715572", "body": "By common style, constructors use `this.` for all parameters. I would vote to uphold that style.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/26715572/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/26715575", "body": "This method is probably better in https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/util/LambdaUtil.java\r\n\r\nAnother exception suppressing lambda method is already there...", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/26715575/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/26740277", "body": "So far, each profile had a different cache identifier. That way profiles did not interfere with each other's caches. Not sure if this is more a theoretical danger, but this commit breaks with that assumption.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/26740277/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/26867793", "body": "@zentol @rmetzger I think this is not correct. The RocksDB state backend is in `lib` by default. This is only relevant for \"running in the IDE\". The text suggests you need to add this to your user jar.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/26867793/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/18399289", "body": "Still has the javadoc license headers\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18399289/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18400363", "body": "At all these points, we already have the user code class loader in a protected variable\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18400363/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18402388", "body": "This was purposefully added to get a better (simpler) error message in the most common failure case. \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18402388/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18402505", "body": "Why change it from 2_cores to 3_cores?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18402505/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18402567", "body": "This seems to be a solution taken from stackoverflow. I think the code should attribute that.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18402567/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18402745", "body": "I think the logic with the lock map is dangerous, and most likely even not more efficient than a simple lock.\n\nPrevious versions of the LibraryCacheManager has errors that the locks were not always removed, leading to endless spins. Manual spinning is not required any more, the JVM does adaptive spinning that automatically degrades to queued waiting, which is better than anything we could hope to implement manually.\n\nSince we do not expect congestion on this class, even the most simple global lock might do (and would be lightweight for the JVM). \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18402745/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18402807", "body": "Is that really in the sense of what the streaming folks had in mind? Probably makes little difference when everything is in the same JVM, but in general, I think such edits should not be mixed into other pull requests.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18402807/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18421462", "body": "Ah, makes sense. I guess it is also a good idea to have only one task manager by default.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18421462/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18436668", "body": "I think this was an exception from java, where we had no influence on the error message.\nAny way that gives a good error message is fine with me.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18436668/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18435533", "body": "Does this indicate that you found and fixed another bug on the way (screwing up fields orders through the sorting?)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18435533/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18435640", "body": "This throws my JUnit tests off, because the class matches the *Test pattern for test cases, but contains no test method.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18435640/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18523098", "body": "You can use the `LocalCollectorOutputFormat` as a workaround.\n\nWe should start adding such \"distributed-to-local\" operations to the API in the next version. The runtime is adding functionality for that.\nWith the blob-manager in place, we should be able to do this nicely and robustly.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18523098/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19032254", "body": "I would add some null checks here. Input Splits are user generated an can contained arbitrary nonsense in general\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19032254/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19032266", "body": "Would add a null check here.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19032266/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19032342", "body": "Would either serialize/deserialize the hostname as well, or at least set it to null.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19032342/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19032889", "body": "Eager computation of the FQDN (in the constructor) on the taskmanager would be a good idea. Otherwise, in corner cases, reverse name lookup falls to the jobmanager (which might get overloaded doing too many reverse lookups).\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19032889/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/16959909", "body": "This is the wrong header, check the other files.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/16959909/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18512444", "body": "It was inconsistently interpreted as second before. Now it is msecs, so\nnothing really changes...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18512444/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19967614", "body": "It would be better to do this via\n`return new Tuple2<Long, Long>(left.f0, right.f1);`\n\nThat avoids creating new boxed Longs entirely, just reusing the previous ones.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19967614/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20003612", "body": "The parsers are often stressed badly when reading large CSV files, so we think a lot about performance here.\n\nI am wondering if a hash set is the best choice to check for these three elements. Computing hash, table lookup, entry lookup, comparison, ...\n\nMight be cheaper to just hardwire the check for those types...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20003612/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "yew1eb": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/25f7aee13f022ee349cc96f7ac3b32760fa5017f", "message": "[FLINK-8156] [build] Bump commons-beanutils version to 1.9.3\n\nThis closes #5113"}, {"url": "https://api.github.com/repos/apache/flink/commits/988bdde17fec8896aae4bc041a10d5e30a4cb702", "message": "[FLINK-7777][build] Bump japicmp to 0.11.0\n\nThis closes #5302."}, {"url": "https://api.github.com/repos/apache/flink/commits/a31e8a305cbff9102dcafda7849a379af081e742", "message": "[hotfix][docs][metrics] Fix Threads.Count metric reference\n\nThis closes #5213."}, {"url": "https://api.github.com/repos/apache/flink/commits/1eaef6abf4839194a12b19a038a1ec480a037783", "message": "[FLINK-8359] [docs] Update copyright date in NOTICE\n\nThis closes #5238"}, {"url": "https://api.github.com/repos/apache/flink/commits/f4e4cd6cb311ce04c4cec40ff9de2ab40deac84c", "message": "[FLINK-7907][docs][metrics] Add scala examples\n\nThis closes #7907."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5144", "title": "[Minor][cleanup] Remove unnecessary semicolons", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5076", "title": "[FLINK-7574][build] POM Cleanup flink-clients", "body": "Re-created #4712.\r\n\r\n## What is the purpose of the change\r\n\r\nThis PR changes the `flink-clients` pom to\r\n\r\n-  contain unused dependencies\r\n-  contain all used dependencies\r\n\r\n\r\n## Brief change log\r\n\r\n- *Define the `maven-dependency-plugin` plugin version in root POM pluginManagement*\r\n- *Enable `dependency:analyze-only` at root POM plugins*\r\n- *Disable the dependency analysis of flink submodules, except the `flink-clients`*\r\n- *Cleanup`flink-clients` dependencies(used but undeclared / declared but unused)*\r\n- *[travis] Only execute `dependency:analyze-only` in \"Misc\" profile*\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (**yes** / no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)\r\n  - The serializers: (yes / **no** / don't know)\r\n  - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)\r\n  - The S3 file system connector: (yes / **no** / don't know)", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5072", "title": "[FLINK-7984][build] Bump snappy-java to 1.1.4", "body": "## What is the purpose of the change\r\n\r\nThis PR bumps the snappy-java version to 1.1.4.\r\nsnappy-java-1.1.4 (2017-05-22):\r\n- Fix a 1% performance regression when snappy is used in PIE executables.\r\n- Improve compression performance by 5%.\r\n- Improve decompression performance by 20%.\r\n- See more: [Snappy-java Release Notes](https://github.com/xerial/snappy-java/blob/master/Milestone.md)\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change should be covered by travis and the end-to-end tests.\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4826", "title": "[FLINK-7608][metric] Refactor latency statistics metric", "body": "A detailed description of this PR, see  [#issue FLINK-7608: LatencyGauge change to histogram metric](https://issues.apache.org/jira/browse/FLINK-7608)\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests.\r\n\r\nbuild flink from source, run a test job [LatencyStatsJob.java](https://gist.github.com/yew1eb/3329239f866b691364f4d11a7f0a846a).\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4769", "title": "[FLINK-7758][kafka][hotfix] Fix bug Kafka09Fetcher add kafkaMetricGroup", "body": "## What is the purpose of the change\r\n\r\n*Fix bug Kafka09Fetcher no judge the useMetrics is true when add kafkaMetricGroup*\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests, such as **KafkaConsumerThreadTest**, **Kafka09FetcherTest**, **Kafka10FetcherTest**.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4719", "title": "[FLINK-3829][build][WIP] POM Cleanup flink-java", "body": "## What is the purpose of the change\r\nThis PR changes the flink-java pom to\r\n\r\n- not contain unused dependencies\r\n- contain all used dependencies\r\n\r\n## Brief change log\r\n## Verifying this change\r\n_mvn dependency:analyze_\r\n\r\n```\r\n[INFO] --- maven-dependency-plugin:2.10:analyze (default-cli) @ flink-java ---\r\n[WARNING] Used undeclared dependencies found:\r\n[WARNING]    org.apache.flink:flink-metrics-core:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    com.esotericsoftware.kryo:kryo:jar:2.24.0:compile\r\n[WARNING]    org.apache.flink:flink-annotations:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    commons-cli:commons-cli:jar:1.3.1:compile\r\n[WARNING]    org.hamcrest:hamcrest-core:jar:1.3:test\r\n[WARNING] Unused declared dependencies found:\r\n[WARNING]    org.hamcrest:hamcrest-all:jar:1.3:test\r\n[WARNING]    org.apache.flink:force-shading:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    org.powermock:powermock-module-junit4:jar:1.6.5:test\r\n[WARNING]    com.google.code.findbugs:jsr305:jar:1.3.9:compile\r\n[WARNING]    log4j:log4j:jar:1.2.17:test\r\n[WARNING]    org.powermock:powermock-api-mockito:jar:1.6.5:test\r\n[WARNING]    org.slf4j:slf4j-log4j12:jar:1.7.7:test\r\n```\r\n\r\nAfter the change:\r\n\r\n```\r\n[INFO] --- maven-dependency-plugin:2.10:analyze (default-cli) @ flink-java ---\r\n[WARNING] Used undeclared dependencies found:\r\n[WARNING]    org.hamcrest:hamcrest-core:jar:1.3:test\r\n[WARNING] Unused declared dependencies found:\r\n[WARNING]    org.hamcrest:hamcrest-all:jar:1.3:test\r\n[WARNING]    org.apache.flink:force-shading:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    org.powermock:powermock-module-junit4:jar:1.6.5:test\r\n[WARNING]    com.google.code.findbugs:jsr305:jar:1.3.9:compile\r\n[WARNING]    log4j:log4j:jar:1.2.17:test\r\n[WARNING]    org.powermock:powermock-api-mockito:jar:1.6.5:test\r\n[WARNING]    org.slf4j:slf4j-log4j12:jar:1.7.7:test\r\n```\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes)\r\n\r\n## Documentation\r\nnone", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4718", "title": "[FLINK-3832][build][WIP] POM Cleanup flink-streaming-scala", "body": "## What is the purpose of the change\r\nThis PR changes the flink-streaming-scala pom to\r\n\r\n- not contain unused dependencies\r\n- contain all used dependencies\r\n\r\n## Brief change log\r\n## Verifying this change\r\n_mvn dependency:analyze_\r\n\r\n```\r\n[INFO] --- maven-dependency-plugin:2.10:analyze (default-cli) @ flink-streaming-scala_2.11 ---\r\n[WARNING] Used undeclared dependencies found:\r\n[WARNING]    org.apache.flink:flink-core:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    com.esotericsoftware.kryo:kryo:jar:2.24.0:compile\r\n[WARNING]    org.apache.flink:flink-test-utils-junit:jar:1.4-SNAPSHOT:test\r\n[WARNING]    org.apache.flink:flink-annotations:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    org.apache.flink:flink-runtime_2.11:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    org.apache.flink:flink-java:jar:1.4-SNAPSHOT:compile\r\n[WARNING] Unused declared dependencies found:\r\n[WARNING]    org.hamcrest:hamcrest-all:jar:1.3:test\r\n[WARNING]    org.apache.flink:force-shading:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    com.google.code.findbugs:jsr305:jar:1.3.9:compile\r\n[WARNING]    log4j:log4j:jar:1.2.17:test\r\n[WARNING]    org.powermock:powermock-api-mockito:jar:1.6.5:test\r\n[WARNING]    org.slf4j:slf4j-log4j12:jar:1.7.7:test\r\n[WARNING]    org.scala-lang:scala-compiler:jar:2.11.11:compile\r\n[WARNING]    org.powermock:powermock-module-junit4:jar:1.6.5:test\r\n[WARNING]    org.mockito:mockito-all:jar:1.10.19:test\r\n[WARNING]    org.scala-lang:scala-reflect:jar:2.11.11:compile\r\n[WARNING]    org.slf4j:slf4j-api:jar:1.7.7:compile\r\n[WARNING]    org.apache.flink:flink-runtime_2.11:test-jar:tests:1.4-SNAPSHOT:test\r\n```\r\n\r\nAfter the change:\r\n\r\n```\r\n[INFO] --- maven-dependency-plugin:2.10:analyze (default-cli) @ flink-streaming-scala_2.11 ---\r\n[WARNING] Unused declared dependencies found:\r\n[WARNING]    org.hamcrest:hamcrest-all:jar:1.3:test\r\n[WARNING]    org.apache.flink:force-shading:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    org.powermock:powermock-module-junit4:jar:1.6.5:test\r\n[WARNING]    com.google.code.findbugs:jsr305:jar:1.3.9:compile\r\n[WARNING]    log4j:log4j:jar:1.2.17:test\r\n[WARNING]    org.mockito:mockito-all:jar:1.10.19:test\r\n[WARNING]    org.powermock:powermock-api-mockito:jar:1.6.5:test\r\n[WARNING]    org.slf4j:slf4j-api:jar:1.7.7:compile\r\n[WARNING]    org.slf4j:slf4j-log4j12:jar:1.7.7:test\r\n```\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes)\r\n\r\n## Documentation\r\nnone", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4717", "title": "[FLINK-3831][build][WIP] POM Cleanup  flink-streaming-java", "body": "## What is the purpose of the change\r\nThis PR changes the flink-streaming-java pom to\r\n\r\n- not contain unused dependencies\r\n- contain all used dependencies\r\n\r\n## Brief change log\r\n## Verifying this change\r\n_mvn dependency:analyze_\r\n\r\n```\r\n[INFO] --- maven-dependency-plugin:2.10:analyze (default-cli) @ flink-streaming-java_2.11 ---\r\n[WARNING] Used undeclared dependencies found:\r\n[WARNING]    org.apache.flink:flink-metrics-core:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    com.esotericsoftware.kryo:kryo:jar:2.24.0:compile\r\n[WARNING]    org.apache.flink:flink-annotations:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    org.scala-lang:scala-library:jar:2.11.11:compile\r\n[WARNING]    commons-io:commons-io:jar:2.4:compile\r\n[WARNING]    org.hamcrest:hamcrest-core:jar:1.3:test\r\n[WARNING]    com.fasterxml.jackson.core:jackson-databind:jar:2.7.4:compile\r\n[WARNING]    org.apache.flink:flink-java:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    org.apache.flink:flink-shaded-hadoop2:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    com.data-artisans:flakka-actor_2.11:jar:2.3-custom:compile\r\n[WARNING]    org.apache.flink:flink-optimizer_2.11:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    org.apache.commons:commons-lang3:jar:3.3.2:compile\r\n[WARNING]    org.powermock:powermock-core:jar:1.6.5:test\r\n[WARNING] Unused declared dependencies found:\r\n[WARNING]    org.apache.flink:force-shading:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    log4j:log4j:jar:1.2.17:test\r\n[WARNING]    org.slf4j:slf4j-log4j12:jar:1.7.7:test\r\n```\r\n\r\nAfter the change:\r\n\r\n```\r\n[INFO] --- maven-dependency-plugin:2.10:analyze (default-cli) @ flink-streaming-java_2.11 ---\r\n[WARNING] Used undeclared dependencies found:\r\n[WARNING]    org.hamcrest:hamcrest-core:jar:1.3:test\r\n[WARNING]    org.powermock:powermock-core:jar:1.6.5:test\r\n[WARNING] Unused declared dependencies found:\r\n[WARNING]    org.apache.flink:force-shading:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    log4j:log4j:jar:1.2.17:test\r\n[WARNING]    org.slf4j:slf4j-log4j12:jar:1.7.7:test\r\n\r\n```\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes)\r\n\r\n## Documentation\r\nnone", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4716", "title": "[FLINK-3833][build][WIP] POM Cleanup flink-test-utils", "body": "## What is the purpose of the change\r\nThis PR changes the flink-test-utils pom to\r\n\r\n- not contain unused dependencies\r\n- contain all used dependencies\r\n\r\n## Brief change log\r\n## Verifying this change\r\n_mvn dependency:analyze_\r\n\r\n```\r\n[INFO] --- maven-dependency-plugin:2.10:analyze (default-cli) @ flink-test-utils_2.11 ---\r\n[WARNING] Used undeclared dependencies found:\r\n[WARNING]    org.apache.flink:flink-core:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    org.apache.flink:flink-shaded-hadoop2:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    io.netty:netty:jar:3.8.0.Final:compile\r\n[WARNING]    org.apache.flink:flink-annotations:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    org.scala-lang:scala-library:jar:2.11.11:compile\r\n[WARNING]    commons-io:commons-io:jar:2.4:compile\r\n[WARNING]    com.data-artisans:flakka-actor_2.11:jar:2.3-custom:compile\r\n[WARNING]    org.apache.flink:flink-shaded-netty:jar:4.0.27.Final-1.0:compile\r\n[WARNING]    org.apache.flink:flink-optimizer_2.11:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    org.apache.flink:flink-java:jar:1.4-SNAPSHOT:compile\r\n[WARNING] Unused declared dependencies found:\r\n[WARNING]    org.apache.flink:flink-clients_2.11:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    org.hamcrest:hamcrest-all:jar:1.3:test\r\n[WARNING]    org.apache.flink:force-shading:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    org.powermock:powermock-module-junit4:jar:1.6.5:test\r\n[WARNING]    log4j:log4j:jar:1.2.17:test\r\n[WARNING]    org.mockito:mockito-all:jar:1.10.19:test\r\n[WARNING]    org.powermock:powermock-api-mockito:jar:1.6.5:test\r\n[WARNING]    org.apache.curator:curator-test:jar:2.12.0:compile\r\n[WARNING]    org.slf4j:slf4j-log4j12:jar:1.7.7:test\r\n```\r\n\r\nAfter the change:\r\n\r\n```\r\n[INFO] --- maven-dependency-plugin:2.10:analyze (default-cli) @ flink-test-utils_2.11 ---\r\n[WARNING] Used undeclared dependencies found:\r\n[WARNING]    io.netty:netty:jar:3.8.0.Final:compile\r\n[WARNING] Unused declared dependencies found:\r\n[WARNING]    org.hamcrest:hamcrest-all:jar:1.3:test\r\n[WARNING]    org.apache.flink:force-shading:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    org.powermock:powermock-module-junit4:jar:1.6.5:test\r\n[WARNING]    log4j:log4j:jar:1.2.17:test\r\n[WARNING]    org.mockito:mockito-all:jar:1.10.19:test\r\n[WARNING]    org.powermock:powermock-api-mockito:jar:1.6.5:test\r\n[WARNING]    org.slf4j:slf4j-log4j12:jar:1.7.7:test\r\n```\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes)\r\n\r\n## Documentation\r\nnone", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4715", "title": "[FLINK-3830][build][WIP] POM Cleanup flink-scala ", "body": "## What is the purpose of the change\r\nThis PR changes the flink-scala pom to\r\n\r\n- not contain unused dependencies\r\n- contain all used dependencies\r\n\r\n## Brief change log\r\n## Verifying this change\r\n_mvn dependency:analyze_\r\n\r\n```\r\n[INFO] \u2014 maven-dependency-plugin:2.10:analyze (default-cli) @ flink-scala_2.11 \u2014\r\n[WARNING] Used undeclared dependencies found:\r\n[WARNING]     org.apache.flink:flink-metrics-core:jar:1.4-SNAPSHOT:compile\r\n[WARNING]     org.apache.flink:flink-shaded-hadoop2:jar:1.4-SNAPSHOT:compile\r\n[WARNING]     org.apache.flink:flink-test-utils-junit:jar:1.4-SNAPSHOT:test\r\n[WARNING]     com.esotericsoftware.kryo:kryo:jar:2.24.0:compile\r\n[WARNING]     org.apache.flink:flink-annotations:jar:1.4-SNAPSHOT:compile\r\n[WARNING]     org.apache.commons:commons-lang3:jar:3.3.2:compile\r\n[WARNING] Unused declared dependencies found:\r\n[WARNING]     org.hamcrest:hamcrest-all:jar:1.3:test\r\n[WARNING]     org.apache.flink:force-shading:jar:1.4-SNAPSHOT:compile\r\n[WARNING]     org.powermock:powermock-module-junit4:jar:1.6.5:test\r\n[WARNING]     com.google.code.findbugs:jsr305:jar:1.3.9:compile\r\n[WARNING]     log4j:log4j:jar:1.2.17:test\r\n[WARNING]     org.mockito:mockito-all:jar:1.10.19:test\r\n[WARNING]     org.powermock:powermock-api-mockito:jar:1.6.5:test\r\n[WARNING]     org.apache.flink:flink-test-utils_2.11:test-jar:tests:1.4-SNAPSHOT:test\r\n[WARNING]     org.slf4j:slf4j-log4j12:jar:1.7.7:test\r\n```\r\n\r\nAfter the change:\r\n\r\n```\r\n[INFO] --- maven-dependency-plugin:2.10:analyze (default-cli) @ flink-scala_2.11 ---\r\n[WARNING] Unused declared dependencies found:\r\n[WARNING]    org.hamcrest:hamcrest-all:jar:1.3:test\r\n[WARNING]    org.apache.flink:force-shading:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    org.powermock:powermock-module-junit4:jar:1.6.5:test\r\n[WARNING]    com.google.code.findbugs:jsr305:jar:1.3.9:compile\r\n[WARNING]    log4j:log4j:jar:1.2.17:test\r\n[WARNING]    org.mockito:mockito-all:jar:1.10.19:test\r\n[WARNING]    org.powermock:powermock-api-mockito:jar:1.6.5:test\r\n[WARNING]    org.slf4j:slf4j-log4j12:jar:1.7.7:test\r\n```\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes)\r\n\r\n## Documentation\r\nnone", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4714", "title": "[FLINK-7577][build][WIP] POM Cleanup flink-core", "body": "## What is the purpose of the change\r\nThis PR changes the flink-core pom to\r\n\r\n- not contain unused dependencies\r\n- contain all used dependencies\r\n\r\n## Brief change log\r\n## Verifying this change\r\n_mvn dependency:analyze_\r\n\r\n```\r\n[INFO] --- maven-dependency-plugin:2.10:analyze (default-cli) @ flink-core ---\r\n[WARNING] Used undeclared dependencies found:\r\n[WARNING]    org.hamcrest:hamcrest-core:jar:1.3:test\r\n[WARNING]    org.powermock:powermock-core:jar:1.6.5:test\r\n[WARNING]    org.powermock:powermock-reflect:jar:1.6.5:test\r\n[WARNING]    org.objenesis:objenesis:jar:2.1:compile\r\n[WARNING] Unused declared dependencies found:\r\n[WARNING]    org.xerial.snappy:snappy-java:jar:1.1.1.3:compile\r\n[WARNING]    org.hamcrest:hamcrest-all:jar:1.3:test\r\n[WARNING]    org.apache.flink:force-shading:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    log4j:log4j:jar:1.2.17:test\r\n[WARNING]    org.joda:joda-convert:jar:1.7:test\r\n[WARNING]    org.slf4j:slf4j-log4j12:jar:1.7.7:test\r\n```\r\n\r\nAfter the change:\r\n\r\n```\r\n[INFO] --- maven-dependency-plugin:2.10:analyze (default-cli) @ flink-core ---\r\n[WARNING] Used undeclared dependencies found:\r\n[WARNING]    org.hamcrest:hamcrest-core:jar:1.3:test\r\n[WARNING]    org.powermock:powermock-core:jar:1.6.5:test\r\n[WARNING]    org.powermock:powermock-reflect:jar:1.6.5:test\r\n[WARNING]    org.objenesis:objenesis:jar:2.1:compile\r\n[WARNING] Unused declared dependencies found:\r\n[WARNING]    org.xerial.snappy:snappy-java:jar:1.1.1.3:compile\r\n[WARNING]    org.hamcrest:hamcrest-all:jar:1.3:test\r\n[WARNING]    org.apache.flink:force-shading:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    log4j:log4j:jar:1.2.17:test\r\n[WARNING]    org.slf4j:slf4j-log4j12:jar:1.7.7:test\r\n```\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes)\r\n\r\n## Documentation\r\nnone", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4713", "title": "[FLINK-3828][build][WIP] POM Cleanup flink-runtime", "body": "## What is the purpose of the change\r\n\r\nThis PR changes the flink-runtime pom to\r\n\r\nnot contain unused dependencies\r\ncontain all used dependencies\r\n\r\n## Brief change log\r\n\r\n## Verifying this change\r\n_mvn dependency:analyze_\r\n```\r\n[INFO] --- maven-dependency-plugin:2.10:analyze (default-cli) @ flink-runtime_2.11 ---\r\n[WARNING] Used undeclared dependencies found:\r\n[WARNING]    org.apache.flink:flink-metrics-core:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    org.apache.curator:curator-client:jar:2.12.0:compile\r\n[WARNING]    io.netty:netty:jar:3.8.0.Final:compile\r\n[WARNING]    com.google.code.findbugs:annotations:jar:2.0.1:test\r\n[WARNING]    com.esotericsoftware.kryo:kryo:jar:2.24.0:compile\r\n[WARNING]    com.typesafe:config:jar:1.2.1:compile\r\n[WARNING]    org.apache.flink:flink-annotations:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    commons-io:commons-io:jar:2.4:compile\r\n[WARNING]    org.hamcrest:hamcrest-core:jar:1.3:test\r\n[WARNING]    org.powermock:powermock-api-support:jar:1.6.5:test\r\n[WARNING]    javax.xml.bind:jaxb-api:jar:2.2.2:compile\r\n[WARNING]    commons-collections:commons-collections:jar:3.2.2:compile\r\n[WARNING]    com.fasterxml.jackson.core:jackson-annotations:jar:2.7.4:compile\r\n[WARNING]    org.apache.hadoop:hadoop-common:jar:2.4.1:compile\r\n[WARNING]    org.apache.curator:curator-recipes:jar:2.12.0:compile\r\n[WARNING]    org.apache.hadoop:hadoop-auth:jar:2.4.1:compile\r\n[WARNING]    org.powermock:powermock-core:jar:1.6.5:test\r\n[WARNING]    org.apache.curator:curator-framework:jar:2.12.0:compile\r\n[WARNING]    org.powermock:powermock-reflect:jar:1.6.5:test\r\n[WARNING] Unused declared dependencies found:\r\n[WARNING]    org.apache.flink:flink-shaded-hadoop2:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    com.data-artisans:flakka-slf4j_2.11:jar:2.3-custom:compile\r\n[WARNING]    org.reflections:reflections:jar:0.9.10:test\r\n[WARNING]    org.javassist:javassist:jar:3.18.2-GA:compile\r\n[WARNING]    org.apache.flink:force-shading:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    com.google.code.findbugs:jsr305:jar:1.3.9:compile\r\n[WARNING]    log4j:log4j:jar:1.2.17:test\r\n[WARNING]    com.twitter:chill_2.11:jar:0.7.4:compile\r\n[WARNING]    org.apache.flink:flink-shaded-curator-recipes:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    org.slf4j:slf4j-log4j12:jar:1.7.7:test\r\n```\r\nAfter the change:\r\n\r\n```\r\n[INFO] --- maven-dependency-plugin:2.10:analyze (default-cli) @ flink-runtime_2.11 ---\r\n[WARNING] Used undeclared dependencies found:\r\n[WARNING]    org.apache.curator:curator-client:jar:2.12.0:compile\r\n[WARNING]    io.netty:netty:jar:3.8.0.Final:compile\r\n[WARNING]    com.typesafe:config:jar:1.2.1:compile\r\n[WARNING]    org.hamcrest:hamcrest-core:jar:1.3:test\r\n[WARNING]    org.powermock:powermock-api-support:jar:1.6.5:test\r\n[WARNING]    javax.xml.bind:jaxb-api:jar:2.2.2:compile\r\n[WARNING]    org.apache.hadoop:hadoop-common:jar:2.4.1:compile\r\n[WARNING]    org.apache.curator:curator-recipes:jar:2.12.0:compile\r\n[WARNING]    org.apache.hadoop:hadoop-auth:jar:2.4.1:compile\r\n[WARNING]    org.powermock:powermock-core:jar:1.6.5:test\r\n[WARNING]    org.apache.curator:curator-framework:jar:2.12.0:compile\r\n[WARNING]    org.powermock:powermock-reflect:jar:1.6.5:test\r\n[WARNING] Unused declared dependencies found:\r\n[WARNING]    org.javassist:javassist:jar:3.18.2-GA:compile\r\n[WARNING]    org.apache.flink:force-shading:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    log4j:log4j:jar:1.2.17:test\r\n[WARNING]    com.twitter:chill_2.11:jar:0.7.4:compile\r\n[WARNING]    org.apache.flink:flink-shaded-curator-recipes:jar:1.4-SNAPSHOT:compile\r\n[WARNING]    org.slf4j:slf4j-log4j12:jar:1.7.7:test\r\n```\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes)\r\n\r\n## Documentation\r\nnone", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bowenli86": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/14840809b13f4f03a487cc4483b5f76ec98b4f7b", "message": "[FLINK-7938] Introduce addAll() to ListState\n\nThis closes #5281."}, {"url": "https://api.github.com/repos/apache/flink/commits/907361d862c77a70ff60d27e7fcc13647eac0e6d", "message": "[FLINK-8175] remove flink-streaming-contrib and migrate its classes to flink-streaming-java/scala\n\nupdate doc\n\nmove classes to /experimental\n\nupdate license header\n\nreorg scala class level\n\nenforce stylecheck and change API annotation\n\nThis closes #5112."}, {"url": "https://api.github.com/repos/apache/flink/commits/30734d55660bfe00c39138584f0e576e711ad791", "message": "[FLINK-8217] [kinesis] Properly annotate APIs of flink-connector-kinesis\n\nThis closes #5138."}, {"url": "https://api.github.com/repos/apache/flink/commits/77e63e6a76937c81c2641a5c46a9a53c0b57b309", "message": "[FLINK-6951] [kinesis] Shade httpcomponents dependency for Kinesis connector\n\nThis closes #4150."}, {"url": "https://api.github.com/repos/apache/flink/commits/d53a722e769e8ff6009d53208bf6702ec3e4a6f5", "message": "[FLINK-8271] [kinesis] Remove usage of deprecated Kinesis APIs\n\nThis closes #5171."}, {"url": "https://api.github.com/repos/apache/flink/commits/438e4e37425688e2689fcb35488f819d729903cc", "message": "[FLINK-7475] [state] Introduce ListState#update()\n\nThis closes #4963."}, {"url": "https://api.github.com/repos/apache/flink/commits/c57e56f183bd923e6947c70f533a2919c888565b", "message": "[FLINK-8216] [kinesis] Unify test utils in flink-connector-kinesis\n\nThis closes #5130."}, {"url": "https://api.github.com/repos/apache/flink/commits/a7465f04ff2afa3774d7e3f746faadf9a5500fed", "message": "[FLINK-8218] [kinesis] Move flink-connector-kinesis examples from /src to /test\n\nThis closes #5131."}, {"url": "https://api.github.com/repos/apache/flink/commits/01d0d256d633108177ddc77288287e1201e31de0", "message": "[hotfix][javadocs] Clarify replacement for deprecated FoldingState\n\nThis closes #5129."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5326", "title": "[FLINK-8365] [State Backend] Relax List type in HeapListState and HeapKeyedStateBackend", "body": "## What is the purpose of the change\r\n\r\n`stateTable` in `HeapListState` and `HeapKeyedStateBackend#createListState()` are both strongly typed to `ArrayList` right now. Relaxing that to `List` so we can avoid extra copies in some cases.\r\n\r\nWell, the copies in `HeapListState#update()` cannot be completely avoided. When users pass in an `AbstractList` to `update()` when there's no state before, it will break and we have to convert it to an `ArrayList` explicitly\r\n\r\n## Brief change log\r\n\r\nRelax List type in HeapListState and HeapKeyedStateBackend\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests, such as *MemoryStateBackendTest*.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\nnone\r\n\r\n## Documentation\r\n\r\nnone", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5325", "title": "[FLINK-8469] [State Backend] [RocksDB] relocate and unify RocksDB option params in RocksDBPerformanceTest", "body": "## What is the purpose of the change\r\n\r\n- the two unit tests in RocksDBPerformanceTest share the same lots of option params\r\n- the params are put in try-with-resource clause, which is not good.\r\n\r\nThis PR moves out RocksDB option params out of try-with-resource clause, and unifies them in RocksDBPerformanceTest\r\n\r\n## Brief change log\r\n\r\nThis PR moves out RocksDB option params out of try-with-resource clause, and unifies them in RocksDBPerformanceTest\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests, such as *RocksDBPerformanceTest*.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\nnone\r\n\r\n## Documentation\r\n\r\nnone", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5323", "title": "[FLINK-8441] [State Backend] [RocksDB] change RocksDBListState to serialize values and separators in stream to avoid extra bytes copying", "body": "## What is the purpose of the change\r\n\r\nCurrently, `RocksDBListState#update()` and `addAll()` will both serialize values into a list of bytes, manually merge the list of bytes with separators to an array of bytes, and then write to RocksDB. It results in extra step of copying bytes back and forth.\r\n\r\n## Brief change log\r\n\r\nChanged `RocksDBListState#update()` and `addAll()` to serialize values and separators directly into `keySerializationStream`, which avoids extra copying.\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests, such as `RocksDBStateBackendTest`\r\n\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\nnone\r\n\r\n## Documentation\r\n\r\nnone\r\n\r\ncc @StefanRRichter ", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5301", "title": "[FLINK-8267] [Kinesis Connector] update Kinesis Producer example for setting Region key", "body": "## What is the purpose of the change\r\n\r\nUpdate doc to guide users to use KPL's native keys to set regions. \r\n\r\nThis originates from a bug that we forgot to set region keys explicitly for kinesis connector, which has been fixed. According to the previous discussion between @tzulitai and I, we want to remove AWSConfigConstants in 2.0 because it basically copies/translates config keys of KPL/KCL, which doesn't add much value. \r\n\r\nGuide users to use KPL's native keys to set regions can be the first step that facilitates the migration.\r\n\r\n## Brief change log\r\n\r\n- updated doc\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n\r\n\r\ncc @tzulitai ", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5300", "title": "[FLINK-8411] [State Backends] HeapListState#add(null) will wipe out entire list state", "body": "## What is the purpose of the change\r\n\r\n`HeapListState#add(null)` will result in the whole state being cleared or wiped out. There's never a unit test for `List#add(null)` in `StateBackendTestBase`\r\n\r\n## Brief change log\r\n\r\n- changed ListState impls such that `add(null)` will be explicitly ignored\r\n- added unit tests to test `add(null)`\r\n- updated javaDoc\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests, such as `StateBackendTestBase`.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: yes\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (JavaDocs)\r\n\r\n\r\nNote! **This work depends on FLINK-7983 and its PR at https://github.com/apache/flink/pull/5281**\r\n\r\ncc @StefanRRichter ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "NicoK": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/03c797a1d279a6c09c988755f94566a3d46cbb17", "message": "[FLINK-7518][network] pass our own NetworkBuffer to netty\n\nThis is using a composite buffer to assemble header+content and avoids an\nunnecessary buffer copy from our (Network)Buffer class backed by a MemorySegment\nto Netty's ByteBuf class.\n\nThis closes #4615."}, {"url": "https://api.github.com/repos/apache/flink/commits/905d98e37c015d24d097113b6e6e8194bd88cbd8", "message": "[hotfix][network][tests] remove Mockito mocks in RecordWriterTest"}, {"url": "https://api.github.com/repos/apache/flink/commits/758336553248d2414a88b071ebd5748c1c32d282", "message": "[hotfix][tests] separate tests inside SlotCountExceedingParallelismTest\n\nThis way, in case of failures, we may have better pointers on what test code is\ncurrently executed."}, {"url": "https://api.github.com/repos/apache/flink/commits/665347cf867f0b807fe9d4787b5ef42b0581b510", "message": "[FLINK-8395][network] add a read-only sliced ByteBuf implementation based on NetworkBuffer\n\nThis closes #5288."}, {"url": "https://api.github.com/repos/apache/flink/commits/db440f2434423a23207ba666b33f4ccb55adede5", "message": "[hotfix][network] rename Buffer#retain() and #recycle in preparation for FLINK-8396 and FLINK-8395\n\nSince these two methods also exist in Netty's ByteBuf, we would otherwise get\ninto overloading conflicts.\n\nAlso add Buffer#readableBytes() and Buffer#setAllocator()."}, {"url": "https://api.github.com/repos/apache/flink/commits/9d0dfcba639a206fb7bf06df3b6af48719794d5d", "message": "[hotfix][network] clarify BufferResponse#size() uses (by removing it)\n\nThis field was only used by the code paths on the receiver and was inconsistent\nwith what was added on the sending side. We should use the contained buffer's\nreadableBytes() instead, depending on the actual use case.\n\nThis closes #4613."}, {"url": "https://api.github.com/repos/apache/flink/commits/435930164f32199f345d01d7094647755bc5f455", "message": "[hotfix][io] remove duplicate code between SynchronousBufferFileReader and BufferReadRequest"}, {"url": "https://api.github.com/repos/apache/flink/commits/85bea23ace3133e1b2d239c4ee87270a201b9b6a", "message": "[FLINK-7520][network] let our Buffer class extend from netty's buffer class\n\nFor this, use a common (flink) Buffer interface and an implementation\n(NetworkBuffer) that implements netty's buffer methods as well. In the future,\nwith this, we are able to avoid unnecessary buffer copies when handing buffers\nover to netty while keeping our MemorySegment logic and configuration.\n\nFor the netty-specific part, the NetworkBuffer also requires a ByteBuf allocator\nwhich is otherwise not needed in our use cases, so if the buffer is handed over\nto netty, it requires a byte buffer allocator to be set."}, {"url": "https://api.github.com/repos/apache/flink/commits/1a5a355a873d88d1fe1903503d81140918e0e07e", "message": "[hotfix][tests] replace InputChannelTestUtils#createMockBuffer() with TestBufferFactory#createBuffer()\n\nThis eliminates one more unnecessary buffer mock."}, {"url": "https://api.github.com/repos/apache/flink/commits/b3fc79392343ff1ba364254b194ec70d2bf43dc0", "message": "[hotfix][tests] make SpillableSubpartitionTest use TestBufferFactory.createBuffer\n\n(this simplifies the test setups)"}, {"url": "https://api.github.com/repos/apache/flink/commits/997fab6247a0d0216a69b698ed049656aa358535", "message": "[hotfix][tests] do not use a mocked BufferRecycler for unpooled memory segments\n\nThe mock will actually keep references to the segments instead of freeing them."}, {"url": "https://api.github.com/repos/apache/flink/commits/705ba2e88632c1ca909cd5b8ebd646ba299c994e", "message": "[hotfix][tests] replace DiscardingRecycler with FreeingBufferRecycler"}, {"url": "https://api.github.com/repos/apache/flink/commits/76abcaa55d0d6ab704b7ab8164718e8e2dcae2c4", "message": "[FLINK-8350][config] replace \"taskmanager.tmp.dirs\" with \"io.tmp.dirs\"\n\nThis replaces \"taskmanager.tmp.dirs\" with the new \"io.tmp.dirs\"\nconfiguration parameter to define temporary directories in (cluster)\nenvironments for all components, i.e. JobManager, JobMaster, Dispatcher,...\n\nPlease note that this (kind of internal and thus undocumented) configuration\nparameter is set by our YARN and Mesos integrations.\n\n[FLINK-8350][cluster] initialise \"io.tmp.dirs\" for JobManager as well\n\nIn a YARN and Mesos environment, this initialises Flink's temporary directory\nconfiguration with YARN/Mesos application-specific paths for JobManager,\nJobMaster, Dispatcher, etc. components as well (Mesos integration actually still\nlacks a proper integration of this, but once done, the new hooks fall in place\njust fine)."}, {"url": "https://api.github.com/repos/apache/flink/commits/46ed5e31585499cd0f0b4bb0718460ba47dcb926", "message": "[hotfix] replace misuse of ConfigConstants.DEFAULT_TASK_MANAGER_TMP_PATH as a temporary folder in unit tests\n\n-> use JUnit's TemporaryFolder instead."}, {"url": "https://api.github.com/repos/apache/flink/commits/a7c407ace4f6cbfbde3e247071cee5a755ae66db", "message": "[FLINK-8279][blob] fall back to TaskManager temp directories first\n\nInstead of falling back to java.io.tmpdir directly, the BLOB server and cache\nprocesses fall back to the TaskManager temp directories (given by\nConfigConstants#TASK_MANAGER_TMP_DIR_KEY) directly, before falling back\nto ConfigConstants#DEFAULT_TASK_MANAGER_TMP_PATH (set to java.io.tmpdir).\n\nIn a Mesos/YARN environment, this means that we will use the designated temp\ndirectories for our jobs instead of the system-wide java.io.tmpdir. These\ndirectories may also offer some more space.\n\nThis closes #5176."}, {"url": "https://api.github.com/repos/apache/flink/commits/4c3f6075bcad0e85db947d7db7817a678d21a85e", "message": "[FLINK-8371][network] always recycle Buffers when releasing SpillableSubpartition\n\nThere were places where Buffer instances were not released upon\nSpillableSubpartition#release() with a view attached to a non-spilled\nsubpartition:\n\n1) SpillableSubpartition#buffer:\n  SpillableSubpartition#release() delegates the recycling to the view, but\n  SpillableSubpartitionView does not clean up the 'buffers' queue (the\n  recycling was only done by the subpartition if there was no view).\n2) SpillableSubpartitionView#nextBuffer:\n  If this field is populated when the subpartition is released, it will neither\n  be given out in subsequent SpillableSubpartitionView#getNextBuffer() calls\n  (there was a short path returning 'null' here), nor was it recycled\n\n-> similarly to the PipelinesSubpartition implementation, make\n   SpillableSubpartition#release() always clean up and recycle the buffers\n-> recycle SpillableSubpartitionView#nextBuffer in\n   SpillableSubpartitionView#releaseAllResources()\n\nThis closes #5261."}, {"url": "https://api.github.com/repos/apache/flink/commits/50f09ad3ef68a7976d3e80d612ff7aed623a13b0", "message": "[hotfix][tests] move assertions out of the finally block\n\nThere was a potential for them to mask exceptions."}, {"url": "https://api.github.com/repos/apache/flink/commits/544c9703d97668b8d4a952501756db52156ff2ef", "message": "[FLINK-8252][benchmarks] convert network benchmarks to streaming benchmarks\n\nThis allows us to use the output flushing interval as a parameter to evaluate,\ntoo.\n\nThis closes #5259."}, {"url": "https://api.github.com/repos/apache/flink/commits/6cfb75874c2b625021eb478b29722d15f4c63f9f", "message": "[hotfix][checkstyle] only ignore checkstyle in existing packages under runtime.io.network\n\n- ignore runtime.io.(async|disk)\n- ignore runtime.io.network.(api|buffer|netty|partition|serialization|util)\n-> everything else will be checked against the ruleset\n- fix checkstyle errors in classes directly under runtime.io.network"}, {"url": "https://api.github.com/repos/apache/flink/commits/50301254182283433d52b5359b1afa6093d0514b", "message": "[FLINK-8280][checkstyle] enable and fix checkstyle in BlobServer and BlobUtils\n\nThis closes #5175."}, {"url": "https://api.github.com/repos/apache/flink/commits/2558ae51140af04d241c77945bf9747b763c0ee8", "message": "[FLINK-8221][network-benchmarks] Define latency network benchmarks in Flink project\n\nThis closes #5255."}, {"url": "https://api.github.com/repos/apache/flink/commits/fd13ed09d4dfeea04be3acb7856fe97ac4ae6c32", "message": "[FLINK-8346][docs] add v4 signature workaround for manual S3 setups\n\nThis closes #5231"}, {"url": "https://api.github.com/repos/apache/flink/commits/fcdd56e548ddd1bd7475970bdc5718b7b18d9803", "message": "[FLINK-7517][network] let NettyBufferPool extend PooledByteBufAllocator\n\nPreviously, NettyBufferPool only wrapped PooledByteBufAllocator but then, any\nallocated buffer's alloc() method was returning the wrapped\nPooledByteBufAllocator which allowed heap buffers again. By extending the\nPooledByteBufAllocator, we prevent this loop hole.\n\nThis also fixes the invariant that a copy of a buffer should have the same\nallocator.\n\nThis closes #4594."}, {"url": "https://api.github.com/repos/apache/flink/commits/622daa447755b984644212f56c5540253a10c149", "message": "[FLINK-7499][io] also let AsynchronousBufferFileWriter#writeBlock() recycle the buffer in case of failures\n\nThis fixes a double-recycle in SpillableSubpartitionView and also makes sure\nthat even if adding the (asynchronous) write operation fails, the buffer is\nproperly freed in code that did not perform this cleanup. It avoids code\nduplication of this cleanup and it is also more consistent to take over\nresponsibility of the given buffer even if an exception is thrown.\n\n[FLINK-7499][io] complete the idiom of ResultSubpartition#add() taking over ownership of the buffer\n\nThe buffer will now always be released once and at the right time and the caller\nmust not worry about the buffer release if a called function threw an exception.\n\nThis closes #4581."}, {"url": "https://api.github.com/repos/apache/flink/commits/79bcdffc057d366f31860d7690abac2819d84bd1", "message": "[hotfix] only update buffer statistics in SpillableSubpartition#add() if successful"}, {"url": "https://api.github.com/repos/apache/flink/commits/7fa3b55eaad1d7a93d2993405f1e1210e545da0b", "message": "[hotfix] add some more buffer recycling checks in SpillableSubpartitionTest"}, {"url": "https://api.github.com/repos/apache/flink/commits/57cef728dbec5c806ad4068e25f97d9b53b2d1af", "message": "[FLINK-7427][network] integrate PartitionRequestProtocol into NettyProtocol\n\n- removes one level of (unneeded) abstraction for clarity\n\nThis closes #4528."}, {"url": "https://api.github.com/repos/apache/flink/commits/1a98e327ea504f1422935c12a3342997145b9292", "message": "[FLINK-8295] [cassandra] [build] Properly shade netty for the datastax driver\n\ncom.datastax.driver.core.NettyUtil expects netty to be present either at its\noriginal package or relocated to com.datastax.shaded.netty. By relocating it\nto this package we make sure the driver follows its designated path.\n\nThis closes #5183."}, {"url": "https://api.github.com/repos/apache/flink/commits/42b01140e09dd1dd106eac302b3aaa8ef756754b", "message": "[hotfix][tests] Remove Task-related \"@PrepareForTest\" annotations"}, {"url": "https://api.github.com/repos/apache/flink/commits/94123cec718a3edbf1199fd703627f0081e39065", "message": "[FLINK-7749][network] Refactor ResultPartitionWriter into an interface\n\nThis closes #5127."}, {"url": "https://api.github.com/repos/apache/flink/commits/175e1b3871b13fee3e423aef87cb45ceed409783", "message": "[FLINK-7748][network] Properly use the TaskEventDispatcher for subscribing to events\n\nPreviously, the ResultPartitionWriter implemented the EventListener interface\nand was used for event registration, although event publishing was already\nhandled via the TaskEventDispatcher. Now, we use the TaskEventDispatcher for\nboth, event registration and publishing.\n\nIt also adds the TaskEventDispatcher to the Environment information for a task\nto be able to work with it (only IterationHeadTask so far).\n\nThis closes #4761."}, {"url": "https://api.github.com/repos/apache/flink/commits/c5efb1f208979fc5a787ba55dc185a7bf2991388", "message": "[FLINK-8241][tests] Remove ResultPartitionWriter-related \"@PrepareForTest\" annotations\n\nThis closes #5147."}, {"url": "https://api.github.com/repos/apache/flink/commits/30fc069add77ae6783a87b6920c59e739903296f", "message": "[FLINK-8145][tests] fix various IOManagerAsync instances not being shut down\n\nThis closes #5064."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5314", "title": "[FLINK-8425][network] fix SpilledSubpartitionView not protected against concurrent release calls", "body": "## What is the purpose of the change\r\n\r\nIt seems like `SpilledSubpartitionView` is not protected against concurrently calling `releaseAllResources()` as the other `ResultSubpartitionView` implementations. These may happen due to failures, e.g. network channels breaking, and will probably only result in some unexpected exceptions being thrown, e.g. from reading from a closed file reader.\r\n\r\nPlease note that this PR is based on #4552 which also touches this code area.\r\n\r\n## Brief change log\r\n\r\n- apply simple synchronization in `SpilledSubpartitionView`\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): **no**\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**\r\n  - The serializers: **no**\r\n  - The runtime per-record code paths (performance sensitive): **no** (per buffer, but in spilled cases)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**\r\n  - The S3 file system connector: **no**\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? **no**\r\n  - If yes, how is the feature documented? **not applicable**\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5243", "title": "[FLINK-8362][elasticsearch] shade all dependencies", "body": "## What is the purpose of the change\r\n\r\nThe Elasticsearch connectors have some dependencies that need to be available and should not conflict with other user or system code. Similarly to the cassandra connector and the S3 file systems, it should thus shade all its dependencies and become self-contained.\r\n\r\n## Brief change log\r\n\r\n- shade dependencies of `flink-connector-elasticsearch`, `flink-connector-elasticsearch2`, and `flink-connector-elasticsearch5` and relocate to our namespace\r\n- add a test into `travis_mvn_watchdog.sh` to verify classes are shaded (one gap though: since `flink-connector-elasticsearch` and `flink-connector-elasticsearch-base` use the same package, we cannot really test that the latter is shaded correctly there)\r\n- update documentation\r\n\r\n## Verifying this change\r\n\r\nThis can be verified as follows:\r\n\r\nIn the shaded jar files,\r\n- check for non-shaded `service` files (also their contents)\r\n- check for non-shaded `.class` files (this test is automated in `travis_mvn_watchdog.sh`, but `flink-connector-elasticsearch` should be verified manually as well - see the note above)\r\n- check for unnecessary files in `META-INF/maven` (only `org.apache.flink/flink-connector-elasticsearch<variant>_<scala_version>/` should remain)\r\n- manually test the connectors and verify they are working without adding other dependencies to a user jar (TODO! @tzulitai since you have been working with the connector in the past, can you chime in here?)\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): **no**\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**\r\n  - The serializers: **no**\r\n  - The runtime per-record code paths (performance sensitive): **no**\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**\r\n  - The S3 file system connector: **no**\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? **no**\r\n  - If yes, how is the feature documented? **docs**", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4529", "title": "[FLINK-7428][network] avoid buffer copies when receiving messages", "body": "## What is the purpose of the change\r\n\r\nThe `LengthFieldBasedFrameDecoder` used so far creates one additional copy in its `#extractFrame()` method which is avoidable in our use case and changed by this PR (based upon #4528).\r\n\r\n## Brief change log\r\n\r\n- let `NettyMessageDecoder` inherit from `LengthFieldBasedFrameDecoder` (instead of being an additional step in the pipeline)\r\n- override `LengthFieldBasedFrameDecoder#extractFrame()` similarly to the `ObjectDecoder` class provided by netty itself\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests, such as NettyMessageSerializationTest as well as many other tests involving network communication, e.g. IT cases.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (yes: network \"deserialisation\")\r\n  - The runtime per-record code paths (performance sensitive): (yes)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes: network communication)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (JavaDocs)\r\n\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/20200802", "body": "was it necessary to increase this dependency?", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/20200802/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/20200919", "body": "seems that `./build_docs -p` is broken, i.e. it does neither enable auto-regeneration nor serve the docs locally", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/20200919/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "kl0u": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/7da32d19f9623ca98c8a4ba76e7c406bf9318d4d", "message": "[FLINK-8049] [rest] REST client reports netty exceptions on shutdown.\n\nThis closes #5057."}, {"url": "https://api.github.com/repos/apache/flink/commits/642e11a9cd31c83fbbabe860871c714f3d4ca981", "message": "[FLINK-8050] [rest] REST server reports netty exceptions on shutdown."}, {"url": "https://api.github.com/repos/apache/flink/commits/a3fd548e9c76c67609bbf159d5fb743d756450b1", "message": "[FLINK-7880][QS] Wait for proper resource cleanup after each ITCase."}, {"url": "https://api.github.com/repos/apache/flink/commits/74d052bb045031363652116ab8226d8ac00e0cd0", "message": "[FLINK-7974][QS] Wait for QS abstract server to shutdown."}, {"url": "https://api.github.com/repos/apache/flink/commits/5760677b3bb26245ca4816548833da0257ec7c7a", "message": "[FLINK-7975][QS] Wait for QS client to shutdown."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5230", "title": "[FLINK-8345] Add iterator of keyed state on broadcast side of connected streams.", "body": "R @aljoscha \r\n\r\n## What is the purpose of the change\r\n\r\nThis PR adds support for broadcast state and exposes it through the `DataStream API`. The user can now create a `BroadcastStream`, i.e. a stream whose elements are broadcasted to all downstream tasks, connect it with a keyed or non-keyed datastream, and from the broadcast side, he can read/write to the broadcast state, while on the non-broadcasted he can only read the broadcast state.\r\n\r\n## Brief change log\r\n\r\nAt the state backend level, the `OperatorStateStore` has now a new method `getBroadcastState()` which returns a handle to the broadcast state.\r\n\r\nAt the API level, there are some new commands added to the `Datastream API` and a new type of `ProcessFunction` which operates on connected streams with broadcast state.\r\n\r\n## Verifying this change\r\n\r\nThere are tests added.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: yes\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: yes (a new type of state)\r\n  - The S3 file system connector: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? yes\r\n  - If yes, how is the feature documented? not documented\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3669", "title": "[FLINK-6215] Make the StatefulSequenceSource scalable.", "body": "So far this source was computing all the elements to\r\nbe emitted and stored them in memory. This could lead\r\nto out-of-memory problems for large deployments. Now\r\nwe split the range of elements into partitions that\r\ncan be re-shuffled upon rescaling and we just store\r\nthe next offset and the end of each one of them upon\r\ncheckpointing.\r\n\r\nThe current version of the PR has no backwards compatibility,\r\nas this becomes tricky given that we change the semantics\r\nof the state that we store.\r\n\r\nI believe that this is ok, given that it is a fix that has to go in\r\nthe 1.3 and we are not sure if people are actually using it in \r\nproduction, i.e. in settings that need backwards compatibility.\r\n\r\nWhat do you think @aljoscha @StephanEwen ?\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2415", "title": "[FLINK-4407] Implement the trigger DSL", "body": "This PR consists of two main commits one for [https://issues.apache.org/jira/browse/FLINK-4415](FLINK-4415) and another for [https://issues.apache.org/jira/browse/FLINK-4407](FLINK-4407)\n\nThe first paves the way for second by :\n1) adding the onFire() method to the Trigger interface and \n2) allowing the WindowOperator to distinguish between the purging (`TriggerResult.PURGE`) the window state and garbage collecting a window\n\nThe `onFire()` method is needed so that different triggers can be composed in order to compose more complex triggering policies, e.g. `All(trigger1, trigger2, \u2026)` which finally decides to fire when all individual triggers propose to fire. In this case, when a trigger says fire, at the end it may not do so, so its state (e.g. the counter for the `CountTrigger`) should not be reset to 0. With the `onFire()`, the proposal of firing, is decoupled from cleaning up the state, or doing anything needed after the trigger fires.\n\nFor the second, purging simply clears the contents of the window but leaves the window metadata intact, while garbage collecting a window means cleaning up also any window related metadata. This distinction is needed because the two state clean ups serve different purposes. On one hand, the window is garbage collected, i.e. it is as it never existed, when the allowed lateness expires. At this point all window related data and metadata are cleaned up by the `WindowOperator.cleanup()` method.  On the other, a trigger may decide to purge the state of the window at any point in the lifetime of the window (discarding mode). In this case, the window contents should be cleared up, but the window metadata should remain, as a new element should be assigned to the correct window. This is especially useful for Session windows, where the window boundaries depend on previously seen elements. In this case, clearing everything by using the `cleanup()` method mentioned above would lead to a new element with timestamp t that arrives after purging, creating a new window with boundaries [t\u2026t + gap].\n\nThe second commit introduces the trigger DSL, as described [https://cwiki.apache.org/confluence/display/FLINK/FLIP-9%3A+Trigger+DSL](here) with the addition of the `Repeat.Once` and `Repeat.Forever`.\n\nThe main classes are the `DslTrigger`, which is extended by all the newly introduced triggers in the library, the `DslTriggerRunner` which extends the classic `Trigger` interface and is responsible for running the triggering policy specified by the user using the `DslTriggers`s, and the `DslTriggerInvokable` which serves at decoupling specification of the trigger in the dsl and implementation. \n\nIn a nutshell, the user will specify a possibly composite trigger using the `DslTriggers` and the `DslTriggerRunner` will take this specification, and build a tree of `DslTriggerInvokable`s (method createTriggerTree), with a node for each `DslTrigger` in the specification.\n\nWhen building the tree, the `DslTriggerRunner` will make sure that the state descriptors of the individual `DslTrigger`s do not collide (stateDescriptor disambiguation  [https://docs.google.com/document/d/1vESGQ913oR-DnE1jmFiihvLBU6_UDo-1DRgoHtSgu30/edit#](here)) by using the translateStateDescriptor() when creating the `DslTriggerInvokable` of each `DslTrigger`.\n\nIn addition, given that the entry point to the dsl trigger tree is the runner, and the invokable of each trigger is the one that knows the actual (disambiguated) state descriptors of the dsl trigger and its children in the tree, all methods of the `DslTrigger` take as an argument a `DslTriggerContext` that wraps the Trigger.TriggerContext, but makes sure that all accesses to state and subsequent child `DslTriggerInvokable`s are consistent with the tree structure.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/25306091", "body": "@zentol  reported instabilities so for now I ignored them until I fix them.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/25306091/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "casidiablo": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/1440e4febd589e320f846a2725e98aec8ee43e7f", "message": "[hotfix][akka] Fix typo in AkkaUtils method\n\nAlso, removed unused code:\n\n- `StandaloneHaServices#RESOURCE_MANAGER_RPC_ENDPOINT_NAME`, since `ResourceManager#RESOURCE_MANAGER_NAME` is used instead\n- AkkaRpcServiceUtils#createInetSocketAddressFromAkkaURL()\n\nThis closes #5133."}, {"url": "https://api.github.com/repos/apache/flink/commits/4d0d7f925abaa2d02814a52bfb666f445bd25ea7", "message": "[FLINK-8162] [kinesis, metrics] Add Kinesis' millisBehindLatest metric"}, {"url": "https://api.github.com/repos/apache/flink/commits/87749b93c9b05574737cd96b4a37ed1b71a74031", "message": "[hotfix] Fix typo in TestableKinesisDataFetcher\n\nThis closes #5178"}, {"url": "https://api.github.com/repos/apache/flink/commits/15a0dc4aecea4a5cff26abef00a067577a27c985", "message": "[hotfix] [docs] Fix typos in MemorySegment class\n\nThis closes #5199"}, {"url": "https://api.github.com/repos/apache/flink/commits/7f99a0df669dc73c983913c505c7f72dab3c0a4d", "message": "[hotfix] [doc] Fix typo in TaskManager and EnvironmentInformation doc\n\nThis closes #5135."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5073", "title": "[FLINK-8124] Make Trigger implementations more generic", "body": "# What is the purpose of the change\r\n\r\nFlink provides implementations of the `Trigger<T, W extend Window>` class that are unnecessarily\r\nspecific. `EventTimeTrigger`, for instance, extends `Trigger<Object, TimeWindow>`.\r\n\r\nSince most window assigners provided also implement `WindowAssigner<Object, TimeWindow>` this\r\nis usually not a problem. However, when implementing custom `WindowAssigner` it could be problematic:\r\n\r\n```java\r\npublic class CustomWindowAssigner extends WindowAssigner<SomePojo, AnotherWindowImpl> {\r\n    ...\r\n\r\n\t@Override\r\n\tpublic Trigger<SomePojo, AnotherWindowImpl> getDefaultTrigger(StreamExecutionEnvironment env) {\r\n\t\treturn EventTimeTrigger.create(); // compiler complains about typing\r\n\t}\r\n\r\n    ...\r\n}\r\n```\r\n\r\n## Brief change log\r\n\r\nThis commit makes `Trigger` implementations generic, keeping them compatible with the current\r\nwindowing implementations as well as custom ones.\r\n\r\n  - `EventTimeTrigger<T, W extends Window>`\r\n  - `ProcessingTimeTrigger<T, W extends Window>`\r\n  - `ContinuousEventTimeTrigger<T, W extends Window>`\r\n  - `ContinuousProcessingTimeTrigger<T, W extends Window>`\r\n  - `CountTrigger<T, W extends Window>`\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests, such as `WindowTranslationTest` or `DataStreamTest`.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: yes\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\r\n  - The S3 file system connector: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? no\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "greghogan": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/0755324c2807673f3c482234535c9e0df24d7feb", "message": "[hotfix] [docs] Fix typos\n\nThis closes #5289."}, {"url": "https://api.github.com/repos/apache/flink/commits/3bc293efc2fbf989c4b044a39a072de7d6f679ea", "message": "[hotfix] Fix many many typos\n\nFix typos from the IntelliJ \"Typos\" inspection.\n\nThis closes #5242"}, {"url": "https://api.github.com/repos/apache/flink/commits/cc8f70b17aec29f9d92dd537402b4d1c15ad10a7", "message": "[hotfix] [build] Always include Kafka 0.11 connector\n\nNow that Flink only supports builds for Scala 2.11+ we can\nunconditionally enable the Kafka 0.11 connector.\n\nThis closes #5195"}, {"url": "https://api.github.com/repos/apache/flink/commits/d3cd51a3f9fbb3ffbe6d23a57ff3884733eb47fa", "message": "[FLINK-8223] [build] Update Hadoop versions\n\nUpdate 2.7.3 to 2.7.5 and 2.8.0 to 2.8.3\n\nThis closes #5137"}, {"url": "https://api.github.com/repos/apache/flink/commits/8987de3b241d23bbcc6ca5640e3cb77972a60be4", "message": "[FLINK-8222] [build] Update Scala version\n\nThis is an incremental upgrade to the Scala security release 2.11.12.\n\nThis closes #5136"}, {"url": "https://api.github.com/repos/apache/flink/commits/a355df6e33f402beac01c2908cb0c64cfeccadb2", "message": "[FLINK-5506] [gelly] Fix CommunityDetection NullPointerException\n\nDouble.MIN_VALUE != min(double)\n\nThis closes #5126"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5292", "title": "[FLINK-8422] [core] Checkstyle for org.apache.flink.api.java.tuple", "body": "## What is the purpose of the change\r\n\r\nUpdate TupleGenerator for Flink's checkstyle and rebuild Tuple and TupleBuilder classes.\r\n\r\n## Brief change log\r\n\r\n`TupleGenerator` has been updated to write Flink-checkstyle compliant code.\r\n\r\nThe following non-generated files were manually updated: `Tuple`, `Tuple0`, `Tuple0Builder`, `Tuple2Test`\r\n\r\n`org.apache.flink.api.java.tuple` was removed from the checkstyle suppressions for `flink-core`.\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage. Re-running `TupleGenerator` should replicate the newly updated files.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5291", "title": "[FLINK-8361] [build] Remove create_release_files.sh", "body": "## What is the purpose of the change\r\n\r\nThe monolithic create_release_files.sh does not support building without Hadoop and has been superseded by the scripts in tools/releasing.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5279", "title": "[hotfix] [build] Print cache info", "body": "Print the size of the Maven cache copied for each TravisCI job.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5205", "title": "[FLINK-8037] Fix integer multiplication or shift implicitly cast to long", "body": "## What is the purpose of the change\r\n\r\nFixes potential overflow flagged by the IntelliJ inspection \"Integer multiplication or shift implicitly cast to long\".\r\n\r\n## Brief change log\r\n\r\n- mark integer literal as long\r\n- cast multiplied integer to long\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / **no**)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)\r\n  - The serializers: (yes / **no** / don't know)\r\n  - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)\r\n  - The S3 file system connector: (yes / **no** / don't know)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes / **no)**\r\n  - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2617", "title": "[FLINK-4705] Instrument FixedLengthRecordSorter", "body": "Updates comparators with support for key normalization.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rmetzger": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/5623ac66bd145d52f3488ac2fff9dbc762d0bda1", "message": "[hotfix][docs] Mention maven dependency for RocksDB state backend\n\nThis closes #5293."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2705", "title": "[FLINK-2597][FLINK-4050] Add wrappers for Kafka serializers, test for partitioner and documentation", "body": "This pull requests addresses the following JIRAs:\n- [FLINK-2597\n  Add a test for Avro-serialized Kafka messages](https://issues.apache.org/jira/browse/FLINK-2597)\n- [FLINK-4050 FlinkKafkaProducer API Refactor](https://issues.apache.org/jira/browse/FLINK-4050)\n\nThe PR adds:\n- `KafkaSerializerWrapper` and `KafkaDeserializerWrapper` wrappers for using Kafka serializers with Flink\n- A test case involving Confluent's `KafkaAvroSerializer` and `KafkaAvroDeserializer`. They also use the schema registry from confluent (which I'm mocking with a simple test http server in the test).\n- A test validating that we are properly calling Kafka partitioners with the producer\n- Documentation for everything mentioned above.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/45640561", "body": "Travis build (in my account) https://travis-ci.org/rmetzger/incubator-flink/builds/27237521\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45640561/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45661714", "body": "merged in c0e76bc0cc296b23df98491ea730a73b43577ddf\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45661714/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45949247", "body": "Merged into master and release-0.5.2.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45949247/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45949335", "body": "Merged into master and 0.5.1.\n\nPlease close the pull request (only the author of the PR can close it)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45949335/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45949389", "body": "Merged into master and 0.5.1.\nPlease close the pull request.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45949389/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45949498", "body": "Thank you.\nMerged into master and 0.5.1.\n\nPlease close the pull request (I can not do it)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45949498/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/46041318", "body": "Thank you for the pull request.\nI spend quite some time today to debug it and to get it to run. Everything seems fine, except for one thing: I can not get a plan preview for the Java K-Means example.\nI don't know if I'm doing anything wrong, and I spend quite some time with the debugger. Our system is able to generate a JSON plan, its also sent it to the browser, but still, the plan does not show up.\n![jonathan](https://cloud.githubusercontent.com/assets/89049/3273402/e23855d4-f323-11e3-9df8-b6abaf7ad599.png)\nThere is no JavaScript error. The standalone tool is able to visualize it (the file that is also located in the file system).\nCan you try and see if you can reproduce it?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/46041318/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/46086997", "body": "Cool. Good catch.\nI think we have already a JSON parser in the Maven dependencies (\"jackson-core-asl\"), can you add a test case for the `PlanJSONDumpGenerator` that verifies that the JSON is valid (without parse errors).\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/46086997/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/49445856", "body": "Yes, If you want to undo the changes from the last commit, just nuke it away and force push into the branch that the PR is based on. The commit will then disappear. \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/49445856/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50237717", "body": "Thank you for your pull request!\n\nTravis indicates that your code is not compliant to our coding guidelines (http://flink.incubator.apache.org/docs/0.6-SNAPSHOT/coding_guidelines.html): https://s3.amazonaws.com/archive.travis-ci.org/jobs/30827003/log.txt\nThe problems are listed here\n\n```\n[INFO] There are 18 checkstyle errors.\n[ERROR] PiEstimation.java[26:n/a] Using the '.*' form of import should be avoided - org.apache.flink.api.java.functions.*.\n[ERROR] PiEstimation.java[53:n/a] Line has leading space characters; indentation should be performed with tabs only.\n[ERROR] PiEstimation.java[54:n/a] Line has leading space characters; indentation should be performed with tabs only.\n[ERROR] PiEstimation.java[55:n/a] Line has leading space characters; indentation should be performed with tabs only.\n[ERROR] PiEstimation.java[56:n/a] Line has leading space characters; indentation should be performed with tabs only.\n[ERROR] PiEstimation.java[57:n/a] Line has leading space characters; indentation should be performed with tabs only.\n[ERROR] PiEstimation.java[58:n/a] Line has leading space characters; indentation should be performed with tabs only.\n[ERROR] PiEstimation.java[61:n/a] Line has leading space characters; indentation should be performed with tabs only.\n[ERROR] PiEstimation.java[62:n/a] Line has leading space characters; indentation should be performed with tabs only.\n[ERROR] PiEstimation.java[65:n/a] Line has leading space characters; indentation should be performed with tabs only.\n[ERROR] PiEstimation.java[66:n/a] Line has leading space characters; indentation should be performed with tabs only.\n[ERROR] PiEstimation.java[67:n/a] Line has leading space characters; indentation should be performed with tabs only.\n[ERROR] PiEstimation.java[68:n/a] Line has leading space characters; indentation should be performed with tabs only.\n[ERROR] PiEstimation.java[69:n/a] Line has leading space characters; indentation should be performed with tabs only.\n[ERROR] PiEstimation.java[71:n/a] Line has leading space characters; indentation should be performed with tabs only.\n[ERROR] PiEstimation.java[72:n/a] Line has leading space characters; indentation should be performed with tabs only.\n[ERROR] PiEstimation.java[74:n/a] Line has leading space characters; indentation should be performed with tabs only.\n[ERROR] PiEstimation.java[75:n/a] Line has leading space characters; indentation should be performed with tabs only.\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50237717/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50352447", "body": "Since you did not add an additional dependency, I assume users have to put a maprfs jar in their `lib/` folder?\n\nCan you add a little bit of documentation to the `docs/` directory on how to use the maprfs ?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50352447/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50442883", "body": "Ah, good.\n\nYes, I'm going to create binaries for major Hadoop distributions with the next release.\nIf its just setting this MapR version for hadoop2, I can also create a package for MapR (as long as we can do it, license wise)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50442883/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50507587", "body": "I'm also in favor of merging this soon.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50507587/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50511543", "body": "I see two options regarding the Java8 tests: \na) We create a \"flink-java8-tests\" maven module that is only included into the build if java8 is present (we can do this via build profiles that activate at certain java versions).\n\nb) We integrate the java8 tests into the regular \"flink-tests\" module, into a separate package and do some maven includes / excludes tricks with build profiles.\n\nI vote for option a) it is cleaner and easier to do.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50511543/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50512992", "body": "Very nice change!\n\nWhile reading through the examples, I was wondering if it would make sense to throw an exception if the user is trying to create a second iterator(). Or maybe an exception if hasNext() == false and the user wants to create a second iterator.\nI don't know if  a) there are usecases for creating two iterators\nand b) users will be confused by the behavior (similar to the mutable objects)\n\nIn addition to that, we should copy the nicely written comment of the pull request into the documentation.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50512992/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50811704", "body": "So when adding the mapr dependency, it pulls the following MapR specific files:\n\n```\n./hadoop-hdfs-2.3.0-mapr-4.0.0-FCS.jar\n./zookeeper-3.4.5-mapr-1401.jar\n./hadoop-annotations-2.3.0-mapr-4.0.0-FCS.jar\n./hadoop-common-2.3.0-mapr-4.0.0-FCS.jar\n./hadoop-auth-2.3.0-mapr-4.0.0-FCS.jar\n./maprfs-2.3.0-mapr-4.0.0-FCS.jar\n```\n\nI think the regular hadoop dependencies with `mapr` versions are fine, but I'm not quite sure about the `maprfs` package. The contained namespaces are not `org.apache.` but `com.mapr`. \nSadly, non of the license tags inside the `pom.xml` is stating anything about the license.\nhttp://repository.mapr.com/nexus/content/repositories/mapr-public/com/mapr/hadoop/maprfs/2.3.0-mapr-4.0.0-FCS/\n\nSo I will not add a MapR distribution for the 0.6 release to our download page. The licensing situation is just too uncertain. I'll ask on their user's forum.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50811704/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/6757204", "body": "You are right. I'll change this with my next pull request.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/6757204/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/8117401", "body": "@StephanEwen : Is there a reason why you removed this code-block?\nI think its a good idea to stop the web interface's Jetty-server?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/8117401/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/8118793", "body": "Ok. My latest push contains a fix for the accidential removal.\n\nThe shutdown hang is independent of this.\n\nSent from my iPhone\n\n> On 10.10.2014, at 19:31, Stephan Ewen notifications@github.com wrote:\n> \n> I agree. Must have been accidentally. Is that the reason for the shutdown hang?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/8118793/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/9297423", "body": "Yes. But we can discuss it ;)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/9297423/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/12965859", "body": "Currently, flink-runtime has a dependency on Hadoop, so I can assume its always available.\nEven for a binary Flink release without build in Hadoop dependencies, we would assume Hadoop to be present (from the classpath).\nFor a Flink release without any Hadoop, we can either remove this again or use some reflection / fake hadoop class magic (added via maven) if needed.\nBut for now, I would like to have this in the code base because it helps debugging user issues.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/12965859/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15535103", "body": "I would call this `fs.default-scheme`.\n\nAlso, please add a Javadoc comment\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15535103/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15535215", "body": "Did you IDE autoformat the code? That's something which makes reviewing the PRs much harder.\n\nYou didn't update the Flink documentation.\n\nI need to to further review of this :)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15535215/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/18116751", "body": "Thank you for the hint. I'll use it in the future.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18116751/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/18778263", "body": "Mh, I stumbled across the broken log upload when I tried fixing that issue ... so I thought the commit is related to my efforts fixing the issue ;)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18778263/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19376002", "body": "I marked the class Serializable to trick the ClosureCleaner to accept enclosed source (which is accessing a field of the outer class).\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19376002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19376443", "body": "I'll push a hotfix with a fix for this.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19376443/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19376450", "body": "I'll look into the issue again. If I recall correctly, there were some issues mocking everything needed for the producer.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19376450/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19376877", "body": "I'll probably have fix for this as well.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19376877/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19377574", "body": "Yes, I think these tests were never active. They are from the first pull request adding watermark support, but were commented out while merging.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19377574/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/13617215", "body": "here, you could have also done `if(content.isDir() && distPath.endsWith(\"/\"))` to save one `if` statement.\nBut you don't need to update the PR for that ;)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/13617215/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/13617314", "body": "Can you add a comment here? Why are you swallowing the exceptions here? It is very likely that an exception is thrown here, and the user should know.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/13617314/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/13845277", "body": "I think we should log the exception as well here (I guess the error is unlikely and needs review by the user)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/13845277/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/13845310", "body": "I would also forward the exception here\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/13845310/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14036891", "body": "we should mark the `recordReader` as transient as well. Even though we do custom serialization, it is helpful as a flag when reading the code (similar to key and value, the transient flag has actually no meaning here, since we do custom ser)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14036891/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14036963", "body": "typo \"problen\".\nAlso, the `catch` statement probably needs to move up to the closing bracket.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14036963/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14037351", "body": "copy pasted typo\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14037351/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14037477", "body": "Is it necessary to have two examples for both the \"mapred\" and \"mapreduce\" variant?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14037477/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14190720", "body": "This is not good ;)\nCan you log the exception as a warn?\nLOG.warn(\"Json object creation failed\", e); ?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14190720/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14190922", "body": "Same here.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14190922/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14295367", "body": "you have to divide by 1048576 here. Dividing by 1024 will turn bytes to kilobytes. The interface shows megabytes.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14295367/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14295464", "body": "can you divide the time by 1000 to make it seconds instead of miliseconds? (Its okay if the interface only says 0 or 1). The \"Seconds since last heartbeat\" column is mainly useful for detecting machines that are not reporting in fast enough.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14295464/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14295494", "body": "I would suggest to have a refresh rate of 2000 ms.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14295494/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14295519", "body": "Change \"Time since last Heartbeat\" into \"Seconds since last heartbeat\"\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14295519/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14295541", "body": "Change \"Free Memory (mb)\" into \"TaskManager Heapsize (mb)\".\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14295541/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14295897", "body": "I talked to stephan, he suggested to do a refresh rate of 10000ms here (10 seconds).\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14295897/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14324245", "body": "Thats fine. We don't want to overload the taskmanager with these requests.\nHadoop is also not automatically refreshing the page. If a task manager has not reported in for 30 seconds, the user would still notice.\nIn addition, the user can just reload the page to get an updated version.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14324245/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14442947", "body": "Can you remove the \"(non-Javadoc)\" javadoc comments? They don't add any value, because its obvious that there is no Javadoc.\n\nWe had quite a lot of these comments in our project in the past and did a huge search/replace session to remove all of them.\n\nAs a side note: It seems that you've limited with width of your code. We are currently not limiting the width, so you don't need to wrap long lines.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14442947/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14443022", "body": "Can you turn this into `System.err.println` without the newline and tabs in between?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14443022/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14443047", "body": "I don't think you need this line. The reader selects all fields by default.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14443047/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14443079", "body": "It would be cool to ship the example with a build-in dataset. This way, users don't need to generate / provide the input data. \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14443079/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14443106", "body": "Can you remove these Javadocs? Every modern IDE allows to jump to the method definition in the interface.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14443106/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14443290", "body": "Why are you not using a `Tuple9<String,String, ...>` for the DataSet ?\nThis works also fine but you are loosing the compile-time type checking. Basically the typeparamters will make line 105: https://github.com/apache/incubator-flink/pull/55/files#diff-70cc9ad1c8f8e2e82718452620b0d33eR105 safer.\n\nI think its fine in this case to use the `Tuple` but I want to note that.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14443290/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14445872", "body": "Can you change the code that it does not require any parameters at all? (The user still has to figure out how to pass parameters to the program)\nSo if the user does not specify any arguments, we use `getExampleInputTuples()` to get some input data. We print the result using dataSet.print() to stdout.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14445872/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/16664393", "body": "You are adding these dependencies to the LICENSE file, which ends up in the source distribution.\n\nYou probably want to put them into the DEPENDENCIES file? and as well into the `flink-dist/src/main/flink-bin/` directory?\n@StephanEwen is our expert on crafting license files ;) Ask him if in doubt\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/16664393/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/16664509", "body": "Okay, this file is probably correct.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/16664509/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/17596956", "body": "We usually don't define simple \"key/value\" style entries like this (in three lines).\nIt is quite common for maven to have\n\n``` xml\n<groupId>org.apache.maven.plugins</groupId>\n```\n\nand so on ..\n\nIts minor, but we should treat our pom files with a bit more love (they are quite messy in some places)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/17596956/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14732236", "body": "There is a typo in `diver`\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14732236/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18384247", "body": "I would recommend the re-throw such exceptions as RuntimeExceptions! If you are printing the stack-trace, just the stack trace (without error message) will appear on some TaskManager's log files. \nif you throw a RuntimeE., the job execution will abort and the user will see the exception.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18384247/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18385767", "body": "can you add another headline here, that quickly explains the usage?\nJust how to read data from the environment to a data set using a hadoop input format (something like a copy-past skeleton).\nI think its better if you have a) a general definition how to use it\nand b) an example that people see how to use it in context.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18385767/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18384610", "body": "I think these dependencies can be \"tests\" scoped?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18384610/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18384650", "body": "can you re-rethrow these exceptions as Runtime exceptions?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18384650/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18384707", "body": "We are using camelCased variable names in Flink.\nwould be cool if you could change all these variable names ;)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18384707/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18384726", "body": "again, rethrow them as runtime exceptions\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18384726/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18518034", "body": "Is there a facility to access the results? (a collection from a `DataSet`) ?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18518034/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18435958", "body": "The sorting of the Tuple key fields has happened in the \"FieldPositionKeys\" class, before the optimizer. \nThis test is taking its inputs from a program written with the Java API. Since I've removed the sorting of Tuple keys, the input to the optimizer has changed with this test case.\nI think thats the reason why the key-field order has changed for this test.\n\nThe key definition in the input plan is `where(1, 0)`. \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18435958/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18454177", "body": "Thank you. I've fixed this\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18454177/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18454201", "body": "thanks\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18454201/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18454205", "body": "thanks\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18454205/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18533375", "body": "why are you skipping here?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18533375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18539170", "body": "Okay, I see.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18539170/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14237434", "body": "I think this is not very efficient since you are creating the field parser for every record.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14237434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18507333", "body": "Is this number intentionally set so high?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18507333/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18546835", "body": "Ah, I was confused by the comment. It says \"number of seconds\".\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18546835/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20072695", "body": "I think the configuration value is not important enough to put it into the default configuration file (I think its a good idea not to overwhelm users with a huge configuration file)\nI really like that you added the configuration value to the documentation.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20072695/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20072728", "body": "I would call the variable `DEFAULT_TASK_MANAGER_TMP_DIR_THRESHOLD_MB` so that you can directly see the unit from the variable name\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20072728/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20077721", "body": "The comment looks truncated by an autoformatter.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20077721/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20089597", "body": "Why do we need this additional profile?\nCan't users select the hadoop2 profile and then set the specific hadoop and hbase versions through properties, like `-Dhbase.version=0.98.1-cdh5` ?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20089597/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20089750", "body": "Why did you change the permissions of this file?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20089750/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20091789", "body": "The linux file permissions.\nSo github says in the changed file view that the permissions of the file were changed from 644 to 755 (it says \"100644 \u2192 100755\" in the box's header)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20091789/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20093842", "body": "What I dislike about the profile is that its very specific about the version. We basically need to manually maintain the CDH versions and force users into specific CDH versions.\n\nWould it be possible to add the `<dependencyManagement>` section with the hadoop-core dependency into the `hadoop2` profile and set the hadoop.core.version to hadoop-2.2.0 by default? \nThis way users could actually specify their specific hadoop versions if they want to build flink against a particular CDH build?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20093842/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20136851", "body": "I think that's fine if you add a little comment explaining the variable.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20136851/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "tiemsn": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/b4599156415f2ad1eee58ffce9a5e9fa54bbdd4e", "message": "[FLINK-8399] [runtime] use independent configurations for the different timeouts in slot manager\n\nThis closes #5271."}, {"url": "https://api.github.com/repos/apache/flink/commits/3b01686851e8281642924c4a620fb43b008de174", "message": "[FLINK-8288] [runtime] register job master rest endpoint url to yarn\n\nThis closes #5186."}, {"url": "https://api.github.com/repos/apache/flink/commits/5643d156cea72314c2240119b30aa32a65a0aeb7", "message": "[FLINK-7928] [runtime] extend the resources in ResourceProfile for precisely calculating the resource of task manager\n\nSummary:\nResourceProfile denotes the resource requirements of a task. It should contains:\n1. The resource for the operators: the resources in ResourceSpec (please refer to jira-7878)\n2. The resource for the task to communicate with its upstreams.\n3. The resource for the task to communicate with its downstreams.\nNow the ResourceProfile only contains the first part. Adding the last two parts.\n\nTest Plan: UnitTests\n\nReviewers: haitao.w\n\nDifferential Revision: https://aone.alibaba-inc.com/code/D330364\n\nThis closes #4991."}, {"url": "https://api.github.com/repos/apache/flink/commits/5b9ac9508b5d16f85b76a6de940458d385e23f0d", "message": "[FLINK-7878] [api] make resource type extendible in ResourceSpec\n\nSummary:\nNow, flink only support user define CPU and MEM,\nbut some user need to specify the GPU, FPGA and so on resources.\nSo it need to make the resouce type extendible in the ResourceSpec.\nAdd a extend field for new resources.\n\nTest Plan: UnitTest\n\nReviewers: haitao.w\n\nDifferential Revision: https://aone.alibaba-inc.com/code/D327427\n\nmake Resource abstract and add GPUResource FPGAResource\n\nThis closes #4911.\n\nAdd a resource spec builder and remove FPGAResource"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tzulitai": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/4ade82631b43c28262c303c4ba028270652f4db3", "message": "[hotfix] [kafka] Add missing serialVersionUIDs to all Kafka connector Serializable classes"}, {"url": "https://api.github.com/repos/apache/flink/commits/5a318de97734df52dcc641c0594ead9df18b5e2f", "message": "[hotfix] [kinesis] Add serialVersionUID to KinesisPartitioner"}, {"url": "https://api.github.com/repos/apache/flink/commits/ac0facc8754ab8bf41f6be96b4241e0a9078f52f", "message": "[hotfix] [kafka] Remove stale comment on publishing procedures of AbstractFetcher\n\nThe previous comment mentioned \"only now will the fetcher return at\nleast the restored offsets when calling snapshotCurrentState()\". This is\na remnant of the previous fetcher initialization behaviour, where in the\npast the fetcher wasn't directly seeded with restored offsets on\ninstantiation.\n\nSince this is no longer true, this commit fixes the stale comment to\navoid confusion."}, {"url": "https://api.github.com/repos/apache/flink/commits/69fff746ac99ec3ad428edf4500e38de17f2b797", "message": "[FLINK-8306] [kafka, tests] Fix mock verifications on final method\n\nPreviously, offset commit behavioural tests relied on verifying on\nAbstractFetcher::commitInternalOffsetsToKafka(). That method is actually\nfinal, and could not be mocked.\n\nThis commit fixes that by implementing a proper mock AbstractFetcher,\nwhich keeps track of the offset commits that go through.\n\nThis closes #5284."}, {"url": "https://api.github.com/repos/apache/flink/commits/37cdaf976ff198a6e5c1d0e6e38a50de185cec1e", "message": "[FLINK-8296] [kafka] Rework FlinkKafkaConsumerBaseTest to not rely on Java reflection\n\nReflection was mainly used to inject mocks into private fields of the\nFlinkKafkaConsumerBase, without the need to fully execute all operator\nlife cycle methods. This, however, caused the unit tests to be too\nimplementation-specific.\n\nThis commit reworks the FlinkKafkaConsumerBaseTest to remove test\nconsumer instantiation methods that rely on reflection for dependency\ninjection. All tests now instantiate dummy test consumers normally, and\nlet all tests properly execute all operator life cycle methods\nregardless of the tested logic.\n\nThis closes #5188."}, {"url": "https://api.github.com/repos/apache/flink/commits/faaa135c9ff4e677157a9d58a91afacd64b0ca1f", "message": "[FLINK-8162, FLINK-8364] [metric, doc] Improve Kafka / Kinesis metrics doc\n\n- Add available user variables to table\n- Fix wordings to be more fluent\n- Fix incorrect metric type for Kinesis millisBehindLatest\n- Add description that Kafka commit failures do not affect Flink's\n  checkpoint integrity."}, {"url": "https://api.github.com/repos/apache/flink/commits/8e23264a4511d33723a756abc209c289fafbe97d", "message": "[FLINK-8162] [kinesis] Add unit test for Kinesis shard metrics reporting\n\nThis closes #5182."}, {"url": "https://api.github.com/repos/apache/flink/commits/03841fdece53f0b2264c8a46ae860e7689cabb49", "message": "[FLINK-8162] [kinesis] Move shard metric gauges registration to KinesisDataFetcher\n\nThis commit refactors the registration of shard metric gauges to the\nKinesisDataFetcher, instead of being handled by the ShardConsumer.\nOverall, this achieves better separation of concerns.\n\nThis commit also consolidates all metrics related constant strings to a\nseparate KinesisConsumerMetricConstants class, with comments that the\nmetric names should not be touched to maintain backwards compatibility\nfor the consumer's shipped metrics."}, {"url": "https://api.github.com/repos/apache/flink/commits/82a9ae596e185a3fd0b6bc9ee59d3a3a8022960a", "message": "[FLINK-8324] [kafka, metrics] Make clear which metrics are legacy and should not be touched\n\nThis commit consolidates all metrics related constant string names in a\nKafkaConsumerMetricConstants class, to give a better overview code-wise\nwhich metrics are exported.\n\nThat makes it more clear in the code which metrics are kept for\ncompatibility reasons. It also additionally states that metric names\nshould not be changed, otherwise metrics compatibility will be broken.\n\nThis closes #5214."}, {"url": "https://api.github.com/repos/apache/flink/commits/542419ba07b1c0b0ba68b636d14de8f1a00aaae1", "message": "[FLINK-8283] [kafka] Stabalize FlinkKafkaConsumerBaseTest::testScaleUp()\n\nPreviously, the testScaleUp() test was taking too much resources and\ncausing test resources to be terminated before the test could finish.\nThis commit lowers the intensity of the test, while still retaining the\nverified behaviour (i.e., when restoring the Kafka consumer with higher\nparallelism and more Kafka partitions).\n\nThis closes #5201."}, {"url": "https://api.github.com/repos/apache/flink/commits/102537df77b12b6541a738b95571d95b5303110b", "message": "[hotfix] [kafka] Add serialVersionUID to FlinkKafkaProducer010"}, {"url": "https://api.github.com/repos/apache/flink/commits/f7a6df1a63880774132357e711abb30c0d831ee7", "message": "[hotfix] [kafka] Fix stale Javadoc link in FlinkKafkaProducer09\n\nThe previous link was referencing a non-existent constructor signature."}, {"url": "https://api.github.com/repos/apache/flink/commits/dfe3e623ea15471bc84c56256b3872df301372c0", "message": "[hotfix] [kafka] Properly deprecate FlinkKafkaProducer010Configuration\n\nFlinkKafkaProducer010Configuration is the return type of the deprecated\nwriteToKafkaWithTimestamp factory methods. Therefore, the class should\nalso be deprecated as well."}, {"url": "https://api.github.com/repos/apache/flink/commits/8d42197b662efaf58d92a3073d8c319f8a2a793e", "message": "[FLINK-8260] [kafka] Reorder deprecated / regular constructors of FlinkKafkaProducer010\n\nThis commit moves deprecated factory methods of the\nFlinkKafkaProducer010 behind regular constructors, for better navigation\nand readability of the code.\n\nThis closes #5179."}, {"url": "https://api.github.com/repos/apache/flink/commits/9f68e790fc28197f89638cc83d1612f8f7a796a8", "message": "[FLINK-8287] [kafka] Improve Kafka producer Javadocs / doc to clarify partitioning behaviour"}, {"url": "https://api.github.com/repos/apache/flink/commits/b49ead387a30ff0d4f204e82fd8430012380eb77", "message": "[FLINK-8260] [kafka] Fix usage of deprecated instantiation methods in FlinkKafkaProducer docs"}, {"url": "https://api.github.com/repos/apache/flink/commits/7b5fdbd6501c33f74700d79aedc9411d160191ba", "message": "[FLINK-8116] [DataStream] Provide proper checkpointed source function example in Javadocs\n\nThis closes #5121."}, {"url": "https://api.github.com/repos/apache/flink/commits/7e90074502d0ac99dfedfbcc33190fc5a4898630", "message": "[hotfix] [kafka] Fix outdated Javadoc reference to non-existing restoreState method"}, {"url": "https://api.github.com/repos/apache/flink/commits/c40b84dfcf748d04f4e3866bf2c60b183c9cf6b3", "message": "[FLINK-8190] [kafka] Add constructors to expose topic pattern-based subscription\n\nThis closes #5117."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5336", "title": "(release-1.4) [FLINK-8419] [kafka] Register metrics for dynamically discovered Kafka partitions", "body": "## What is the purpose of the change\r\n\r\nDifferent version of #5335, which is targeted for `release-1.4`.\r\nThis version does not include the new offset metrics added in #5214 for the new partitions (those new metrics are only added in `master`).\r\n\r\n## Brief change log\r\n\r\n- db39ec2: Preliminary cleanup of the registration of the `KafkaConsumer` user scope metric group. This commit refactors that for better separation of concerns and lessen code duplication.\r\n- 1acfc15: Register offset metrics for new partitions in `addDiscoveredPartitions`\r\n\r\n## Verifying this change\r\n\r\nNo new tests were added.\r\nBy manually running a Flink job using the Kafka consumer and repartitioning the Kafka topic, you should be able to see metrics for the newly added partitions.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / **no**)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)\r\n  - The serializers: (yes / **no** / don't know)\r\n  - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)\r\n  - The S3 file system connector: (yes / **no** / don't know)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes / **no**)\r\n  - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5335", "title": "(master) [FLINK-8419] [kafka] Register metrics for dynamically discovered Kafka partitions", "body": "## What is the purpose of the change\r\n\r\nThis PR fixes that offset metrics (i.e. current offset, committed offsets) were not registered for partitions that were dynamically discovered in the `FlinkKafkaConsumerBase`.\r\n\r\nThis version is targeted for merge to `master`.\r\nAnother version targeted for `release-1.4`, which does not include the new offset metrics added in #5214, will be separately opened.\r\n\r\n## Brief change log\r\n\r\n- 54f3cfd3eb9f925266de37219ab56703a26795d6: Preliminary cleanup of the registration of the `KafkaConsumer` user scope metric group. This commit refactors that for better separation of concerns and lessen code duplication.\r\n- bf1e4ce: Register offset metrics for new partitions in `addDiscoveredPartitions`\r\n- 7e90a75: Minor hotfix to inappropriate access modifiers in the `AbstractFetcher`\r\n\r\n## Verifying this change\r\n\r\nNo new tests were added.\r\nBy manually running a Flink job using the Kafka consumer and repartitioning the Kafka topic, you should be able to see metrics for the newly added partitions.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / **no**)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)\r\n  - The serializers: (yes / **no** / don't know)\r\n  - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)\r\n  - The S3 file system connector: (yes / **no** / don't know)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes / **no**)\r\n  - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5329", "title": "[FLINK-8409] [kafka] Fix potential NPE in KafkaConsumerThread", "body": "## What is the purpose of the change\r\n\r\nThis PR fixes a race condition that may lead to a NPE in the async callbacks for Kafka offset committing.\r\n\r\nThe following lines in the KafkaConsumerThread::setOffsetsToCommit(...) suggests a race condition with the asynchronous callback from committing offsets to Kafka:\r\n\r\n```\r\n// record the work to be committed by the main consumer thread and make sure the consumer notices that\r\nif (nextOffsetsToCommit.getAndSet(offsetsToCommit) != null) {\r\n    log.warn(\"Committing offsets to Kafka takes longer than the checkpoint interval. \" +\r\n        \"Skipping commit of previous offsets because newer complete checkpoint offsets are available. \" +\r\n        \"This does not compromise Flink's checkpoint integrity.\");\r\n}\r\nthis.offsetCommitCallback = commitCallback;\r\n```\r\n\r\nIn the main consumer thread's main loop, `nextOffsetsToCommit` will be checked if there are any offsets to commit. If so, an asynchronous offset commit operation will be performed. The NPE happens in the case when the commit completes, but `this.offsetCommitCallback = commitCallback;` is not yet reached.\r\n\r\nThis PR fixes this by making setting the `commitCallback` and `nextOffsetsToCommit` an atomic operation.\r\n\r\n## Verifying this change\r\n\r\nNo new tests were added.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / **no**)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)\r\n  - The serializers: (yes / **no** / don't know)\r\n  - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)\r\n  - The S3 file system connector: (yes / **no** / don't know)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes / **no**)\r\n  - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5282", "title": "[FLINK-6352] [kafka] Timestamp-based offset configuration for FlinkKafkaConsumer", "body": "## What is the purpose of the change\r\n\r\nThis PR is based on @zjureel's initial efforts on the feature in #3915.\r\n\r\nThis version mainly differs in that:\r\n- When using timestamps to define the offset, the actual offset is eagerly determined in the `FlinkKafkaConsumerBase` class.\r\n- The `setStartFromTimestamp` configuration method is defined in the `FlinkKafkaConsumerBase` class, with `protected` access. Kafka versions which support the functionality should override the method with `public` access.\r\n- Timestamp is configured simply as a long value, and not a Java `Date`.\r\n\r\n**Overall, the usage of the feature is as follows:**\r\n```\r\nFlinkKafkaConsumer011<String> consumer = new FlinkKafkaConsumer011<>(...);\r\nconsumer.setStartFromTimestamp(1515671654453L);\r\n\r\nDataStream<String> stream = env.addSource(consumer);\r\n...\r\n```\r\n\r\nOnly versions 0.10 and 0.11 supports this feature.\r\n\r\n**Semantics:**\r\n- The provided timestamp cannot be larger than the current timestamp.\r\n- For a partition, the earliest record which `record timestamp >= provided timestamp` is used as the starting offset.\r\n- If the provided timestamp is larger than the latest record in a partition, that partition will simply be read from the head.\r\n- For all new partitions that are discovered after the initial startup (due to scaling out Kafka), they are all read from the earliest possible record and the provided timestamp is not used.\r\n\r\n## Brief change log\r\n\r\n- d012826 @zjureel's initial efforts on the feature.\r\n- 7ac07e8 Instead of lazily determining exact offsets for timestamp-based startup, the offsets are determined eagerly in `FlinkKafkaConsumerBase`. This commit also refactors the `setStartFromTimestamp` method to live in the base class.\r\n- 32d46ef Change to just use long values to define timestamps, instead of using Java `Date`\r\n- 7bb44a8 General improvement for the `runStartFromTimestamp` integration test.\r\n\r\n## Verifying this change\r\n\r\nNew integration tests `Kafka010ITCase::testStartFromTimestamp` and `Kafka011ITCase::testStartFromTimestamp` verifies this new feature.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / **no**)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)\r\n  - The serializers: (yes / **no** / don't know)\r\n  - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)\r\n  - The S3 file system connector: (yes / **no** / don't know)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (**yes** / no)\r\n  - If yes, how is the feature documented? (not applicable / **docs** / **JavaDocs** / not documented)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5269", "title": "[FLINK-6004] [kinesis] Allow FlinkKinesisConsumer to skip non-deserializable records", "body": "## What is the purpose of the change\r\n\r\nThis PR is based on #5268, which includes fixes to harden Kinesis unit tests. Only the last commit is relevant.\r\n\r\nIn the past, we allowed the Flink Kafka Consumer to skip corrupted Kafka records which cannot be deserialized. In reality pipelines, it is entirely normal that this could happen.\r\n\r\nThis PR adds this functionality to the Flink Kinesis Consumer also.\r\n\r\n## Brief change log\r\n\r\n- Clarify in Javadoc of `KinesisDeserializationSchema` that `null` can be returned if a message cannot be deserialized.\r\n- If `record` is `null` in `KinesisDataFetcher::emitRecordAndUpdateState(...)`, do not collect any output for the record.\r\n- Add `KinesisDataFetcherTest::testSkipCorruptedRecord()` to verify feature.\r\n\r\n## Verifying this change\r\n\r\nAdditional `KinesisDataFetcherTest::testSkipCorruptedRecord()` test verifies this change.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / **no**)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)\r\n  - The serializers: (yes / **no** / don't know)\r\n  - The runtime per-record code paths (performance sensitive): (**yes** / no / don't know)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)\r\n  - The S3 file system connector: (yes / **no** / don't know)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (**yes** / no)\r\n  - If yes, how is the feature documented? (not applicable / docs / **JavaDocs** / not documented)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5268", "title": "[FLINK-8398] [kinesis, tests] Harden KinesisDataFetcherTest unit tests", "body": "## What is the purpose of the change\r\n\r\nPrior to this PR, many of the `KinesisDataFetcherTest` unit tests relied on thread sleeps to wait until a certain operation occurs to allow the test to pass. This test behaviour is very flaky, and should be replaced with `OneShotLatch`.\r\n\r\n## Brief change log\r\n\r\n- 94b4591: Several minor cleanups of confusing implementations / code smells in the `KinesisDataFetcherTest` and related test classes. The commit message explains what exactly was changed.\r\n- 547d19f: Remove thread sleeps in unit tests, and replace them with `OneShotLatch`.\r\n\r\n\r\n## Verifying this change\r\n\r\nNo test coverage should have been affected by this change.\r\nThe existing tests in `KinesisDataFetcherTest` verifies this.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / **no**)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)\r\n  - The serializers: (yes / **no** / don't know)\r\n  - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)\r\n  - The S3 file system connector: (yes / **no** / don't know)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes / **no**)\r\n  - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)\r\n\r\n  ", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4510", "title": "[FLINK-7124] [flip-6] Add test to verify rescaling JobGraphs works correctly", "body": "## What is the purpose of the change\r\n\r\nThis pull request adds test to verify that rescaling `JobGraph`s to arbitrary valid DOPs works correctly, such that the generated `ExecutionGraph` of each rescale is correct.\r\n\r\n## Brief change log\r\n\r\n- Add `ExecutionGraphRescalingTest` class.\r\n  - Contains a test that consecutively rescales (up and down) a given `JobGraph`.\r\n  - Contains a test that rescales to DOP beyond max parallelism. `TODO` ignored for now since the test doesn't properly fail as expected.\r\n- `[hotfix]` refactor `ExecutionGraphConstructionTest` to share utility graph validation code.\r\n\r\n## Verifying this change\r\n\r\nThe changes themselves are a verification of existing features. See above.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): **no**\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**\r\n  - The serializers: **no**\r\n  - The runtime per-record code paths (performance sensitive): **no**\r\n  - Anything that affects deployment or recovery: **no**\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? *no*\r\n  - If yes, how is the feature documented? *not applicable*\r\n\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/18811598", "body": "From the original Kinesis consumer documentation, I don't think it's necessary to mention this, because there's already this in the first paragraph: \"Each subtask of the consumer is responsible for fetching data records from multiple Kinesis shards. The number of shards fetched by each subtask will change as shards are closed and created by Kinesis.\"\n\nPerhaps its because the new warning notice is a bit confusing? Especially this: \"Currently, resharding can not be handled transparently (i.e., without failing and restarting jobs) if there are idle consumer\n subtasks, which occur when the total number of shards is lower than the configured consumer parallelism.\"\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18811598/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/18811645", "body": "AFAIK, the original issue reporters set the parallelism to the number of Kinesis shards because they realized only then will the checkpoint state size not unboundly grow. They did not seem to have this misunderstanding in the beginning.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18811645/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19258651", "body": "This doesn't seem right to me. `getPartitionableState` is only called 4 times throughout this test, so I think this should be `thenReturn(listState1, listState1, listState2, listState3)`.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19258651/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19258652", "body": "This should be `listState3.get`?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19258652/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19258653", "body": "Should be `snapshot3`\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19258653/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19258654", "body": "This is actually passing only because `state3` wasn't correctly initialized, so actually `state3` and `snapshot3` are both empty.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19258654/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19258663", "body": "@StefanRRichter Found some bugs in `KafkaConsumerBaseTest#testSnapshotState()` here, the code happened to workaround and bypass the bugs and asserts :P \nI'll fix this test as part of [FLINK-4723](https://issues.apache.org/jira/browse/FLINK-4723) since I'll need to change this test over there.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19258663/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19297476", "body": "Thanks for the fix! Seems like I wasn't quite concentrated on the review :/ \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19297476/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19300270", "body": "@fhueske Just realized, the setted committed offset to `partition` needs to be incremented by 1 also. This hotfix broke that. I'll hotfix this now ...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19300270/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/21009882", "body": "@StephanEwen Yes. We discussed that in https://issues.apache.org/jira/browse/FLINK-5728 with some other aspects regarding the `setFlushOnCheckpoint` and `setLogFailuresOnly` methods, so it was kept as a separate issue.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/21009882/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "zhangminglei": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/4ceabed9ad108acd6b67ec59e2f079669ab73046", "message": "[FLINK-8276] [kafka] Properly annotate APIs for Kafka connector\n\nThis closes #5173."}, {"url": "https://api.github.com/repos/apache/flink/commits/9b5fce6b1d55205054abbdf274df7af72d1fd263", "message": "[FLINK-8199] [elasticsearch] Properly annotate APIs of Elasticsearch connector\n\nThis closes #5124."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5298", "title": "[FLINK-8433] [doc] Remove ununsed CheckpointedRestoring interface", "body": "## What is the purpose of the change\r\n\r\n*Update the document, as the ```CheckpointedRestoring``` interface introduced in the doc is unused since flink 1.4.*\r\n\r\n\r\n## Brief change log\r\n\r\n  - *Update the ```state.md``` file.*\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): ( no )\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: ( no)\r\n  - The S3 file system connector: ( no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (docs / JavaDocs)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4705", "title": "[FLINK-6444] [build] Add a check that '@VisibleForTesting' methods ar\u2026", "body": "## What is the purpose of the change\r\n\r\nThis PR fix the issue which some methods are annotated with @VisibleForTesting. But still called from non-test class.\r\n\r\n## Brief change log\r\nCreate a test class named `CheckVisibleForTestingUsage#testCheckVisibleForTesting` to verify and prevent this issue from happening in the future\r\n\r\n## Verifying this change\r\nThis change is a trivial work with a test coverage.\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4482", "title": "[FLINK-4534] Fix synchronization issue in BucketingSink", "body": "## What is the purpose of the change\r\n\r\nFix lacking of synchronization in BucketingSink#close method.\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial work without any test coverage.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4356", "title": "[FLINK-5486] Fix lacking of synchronization in BucketingSink#handleRe\u2026", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4316", "title": "[FLINK-6105] Use InterruptedIOException instead of IOException", "body": "This is my first commit of this. It might be have other places also do this change, but I am not sure now.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4166", "title": "[FLINK-6857] [types] Add global default Kryo serializer configuration\u2026", "body": "\u2026 to StreamExecutionEnvironment\r\n\r\nThanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [ ] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3755", "title": "[FLINK-6351] [YARN] Refactoring YarnFlinkApplicationMasterRunner by c\u2026", "body": "\u2026ombining AbstractYarnFlinkApplicationMasterRunner in one class.\r\n\r\nThanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [ ] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3750", "title": "[FLINK-6345] [Streaming] Migrate from Java serialization for Continuo\u2026", "body": "\u2026usFileReaderOperator's state.\r\n\r\nThanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [ ] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tony810430": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/6f6b3c8f030d11b08b05a9a93de02c787b102499", "message": "[FLINK-8324] [kafka, metrics] Add new offsets metrics that can be scoped by topic and partition"}, {"url": "https://api.github.com/repos/apache/flink/commits/6033de01ae620ebc9735c552ce85ccd1687793d7", "message": "[FLINK-5982] [runtime] Refactor AbstractInvokable and StatefulTask to accept Environment and State in the constructor.\n\nThis is the first steo towards implementing an RAII pattern for all task runtime classes.\n\nThis closes #3633"}, {"url": "https://api.github.com/repos/apache/flink/commits/e20536586aa21ad7cfdb805b259827b2a15e3c92", "message": "[FLINK-8326] CheckpointCoordinatorTest#testRestoreLatestCheckpointedStateScaleOut() didn't use the correct parameter to trigger test function"}, {"url": "https://api.github.com/repos/apache/flink/commits/784dbbeeeb0736c29225b58ec01f1cf95234e881", "message": "[FLINK-7692][metrics] Support user-defined variables\n\nThis closes #5115."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4828", "title": "[FLINK-4816] [checkpoints] Executions failed from \"DEPLOYING\" should retain restored checkpoint information", "body": "## What is the purpose of the change\r\nThis PR is base on #3478 and added some improvements.\r\n\r\n## Brief change log\r\n  - Rebased #3478 to the latest master branch.\r\n  - Checked if CheckpointCoordinator is exist.\r\n  - Added corresponding tests.\r\n\r\n## Verifying this change\r\n  - Updated tests in `CheckpointCoordinatorTest` and `ExecutionVertexDeploymentTest`.\r\n  - Added a new test in `ExecutionVertexDeploymentTest` for deploying failed after restoring.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): **no**\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**\r\n  - The serializers: **don't know**\r\n  - The runtime per-record code paths (performance sensitive): **don't know**\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **yes**\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? **no**\r\n  - If yes, how is the feature documented? **not documented**\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3474", "title": "[FLINK-4714] [runtime] [streaming] Set task state to RUNNING after state has been restored", "body": "The changes in this PR are\r\n\r\n- add `open()` method in `AbstractInvokable` to let the invokable can be initialized during `DEPLOYING` state in `Task`. The default behavior is do nothing.\r\n- separate `invokable.invoke()` into `invokable.open()` and `invokable.invoke()` in `Task`, and transit task state from `DEPLOYING` to `RUNNING` between them.\r\n- `TaskCanceler` should be used to cancel the invokable in `DEPLOYING` state because invokable might be called in that state now.\r\n- update `StreamTask` and `StreamTaskTestHarness` to solve this issue in FLINK-4714", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2925", "title": "[FLINK-4574] [kinesis] Strengthen fetch interval implementation in Kinesis consumer", "body": "I used Timer to implement it.\r\n\r\nIf \"flink.shard.getrecords.intervalmillis\" is set by default value, which is 0, the timer will schedule ShardConsumerFetcher once and run it forever.\r\nIf \"flink.shard.getrecords.intervalmillis\" is greater than 0, the timer will schedule ShardConsumerFetcher at a fixed ratio by using timer.scheduleAtFixedRate, which makes sure two consecutive function call would be a fixed interval.\r\nBut if the getRecords took too much time and couldn't be finished on time, ShardConsumerFetcher would log the warning and drop the next delayed task.\r\n\r\nIdeally :\r\n|----p1----|----p2----|----p3----|\r\n|=====>...|====>......|====>......|\r\n  task1.........task2.........task3\r\n\r\ntask2 is delayed by task1: task2 will be dropped\r\n|----p1----|----p2----|----p3----|\r\n|============>..... |====>......|\r\n  task1...........................task3", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "elbaulp": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/4496248ace3e10808aa56a2c1f91a7cb59b38423", "message": "[hotfix] [doc] Fixed doc typo in DataStream API\n\nThis closes #5283.\nThis closes #5191."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5299", "title": "[Hotfix][Doc][DataStream API] Fix Scala code example in Controlling Latency section", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bartektartanus": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/6593e0f9e6db9d862afadeeae79e0afa2590fd73", "message": "[FLINK-7949] Make AsyncWaitOperator recoverable also when queue is full\n\nStart emitter thread BEFORE filling up the queue of recovered elements.\nThis guarantees that we won't deadlock inserting the recovered elements,\nbecause the emitter can already start processing elements.\n\nThis closes #4924."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "twalthr": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/6c078c0e53346fd2ac178b133b352d0ee9a39e24", "message": "[hotfix] [docs] Update latest stable version URL"}, {"url": "https://api.github.com/repos/apache/flink/commits/00ad0eb120026817c79a5fed2c71dd6aa10e9ba6", "message": "[FLINK-6893] [table] Add BIN function to Table API and fix bugs"}, {"url": "https://api.github.com/repos/apache/flink/commits/fddedda78ad03f1141f3e32f0e0f39c2e045df0e", "message": "[hotfix] [docs] Update documentation about joins"}, {"url": "https://api.github.com/repos/apache/flink/commits/e6fbfdc41c12d6ef95fd059dd16b5a3c3130be1c", "message": "[FLINK-8381] [table] Document more flexible schema definition\n\nThis closes #5257."}, {"url": "https://api.github.com/repos/apache/flink/commits/49c6d10f186fb722d2a4003ce4d2219c01f55871", "message": "[FLINK-6094] [table] Add checks for hashCode/equals and little code cleanup"}, {"url": "https://api.github.com/repos/apache/flink/commits/fb29898cd3507b2b94dd8bbf3dbfd2132b643a1d", "message": "[FLINK-8203] [FLINK-7681] [table] Make schema definition of DataStream/DataSet to Table conversion more flexible\n\nThis closes #5132."}, {"url": "https://api.github.com/repos/apache/flink/commits/f88da4d04c328c46b9c94dc76d3e7a3e4e87b2ea", "message": "[FLINK-8139] [table] Check types for hashCode/equals only for stateful operations"}, {"url": "https://api.github.com/repos/apache/flink/commits/664b8816c11658200e779fefebe5ebf12609f855", "message": "[FLINK-8258] [table] Add missing ScalaDocs"}, {"url": "https://api.github.com/repos/apache/flink/commits/e30066dbd1ebf3c5780df89d766554042c8345a7", "message": "[FLINK-7452] [types] Add helper methods for all built-in Flink types to Types\n\nThis closes #4612."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5240", "title": "[FLINK-8240] [table] Create unified interfaces to configure and instatiate TableSources", "body": "## What is the purpose of the change\r\n\r\nThis PR presents the inital version of the new unified TableSource API. The API is based on a descriptor approach. A descriptor allows for describing parameters and behavior. They contain no logic but only store information and translate it to normalized string-based properties.\r\n\r\nThe following example shows how a CSV table source could be specified in the future:\r\n\r\n```\r\ntableEnv\r\n      .createTable(\r\n        Schema()\r\n          .field(\"myfield\", Types.STRING)\r\n          .field(\"myfield2\", Types.INT))\r\n      .withConnector(\r\n        FileSystem()\r\n          .path(\"/path/to/csv\"))\r\n      .withEncoding(\r\n        CSV()\r\n          .field(\"myfield\", Types.STRING)\r\n          .field(\"myfield2\", Types.INT)\r\n          .quoteCharacter(';')\r\n          .fieldDelimiter(\"#\")\r\n          .lineDelimiter(\"\\r\\n\")\r\n          .commentPrefix(\"%%\")\r\n          .ignoreFirstLine()\r\n          .ignoreParseErrors())\r\n      .withRowtime(\r\n        Rowtime()\r\n          .field(\"rowtime\")\r\n          .timestampFromDataStream()\r\n          .watermarkFromDataStream())\r\n      .withProctime(\r\n        Proctime()\r\n          .field(\"myproctime\"))\r\n```\r\n\r\nThey get translated into:\r\n\r\n```\r\n\"schema.0.name\" -> \"myfield\",\r\n\"schema.0.type\" -> \"VARCHAR\",\r\n\"schema.1.name\" -> \"myfield2\",\r\n\"schema.1.type\" -> \"INT\",\r\n\"connector.type\" -> \"filesystem\",\r\n\"connector.path\" -> \"/path/to/csv\",\r\n\"encoding.type\" -> \"csv\",\r\n\"encoding.fields.0.name\" -> \"myfield\",\r\n\"encoding.fields.0.type\" -> \"VARCHAR\",\r\n\"encoding.fields.1.name\" -> \"myfield2\",\r\n\"encoding.fields.1.type\" -> \"INT\",\r\n\"encoding.quote-character\" -> \";\",\r\n\"encoding.field-delimiter\" -> \"#\",\r\n\"encoding.line-delimiter\" -> \"\\r\\n\",\r\n\"encoding.comment-prefix\" -> \"%%\",\r\n\"encoding.ignore-first-line\" -> \"true\",\r\n\"encoding.ignore-parse-errors\" -> \"true\",\r\n\"rowtime.0.name\" -> \"rowtime\",\r\n\"rowtime.0.timestamp.type\" -> \"stream-record\",\r\n\"rowtime.0.watermark.type\" -> \"preserving\",\r\n\"proctime\" -> \"myproctime\"\r\n```\r\n\r\nThis PR also reworks the discovery of table sources by deprecating the `@TableType` annotation and reflection-based discovery with `TableSourceFactory` interfaces and standard Java Service Provider Interfaces (SPI). Now the table factories can use the above properties to create table sources from. The `ExternalCatalogTable` class has been reworked to use the new descriptor-based approach as well, however, we should be fully source code backwards compatible.\r\n\r\nI agree that there are more tests missing and we should also decide where and how the validation should happen. I think it should happen mostly in the table source builders. We could also introduce some global dictionary class to use constants for properties instead of strings at different positions.\r\n\r\nWhat do you think?\r\n\r\n## Brief change log\r\n\r\n  - Adds descriptors for schema, connectors, encoding, statistics, metadata, proctime, and rowtime\r\n  - Adds table factory discovery based on unified properties\r\n\r\n## Verifying this change\r\n\r\n - Added `DescriptorsTest`\r\n - ExternalCatalog tests are still working\r\n - More tests will follow...\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\r\n  - The S3 file system connector: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? yes\r\n  - If yes, how is the feature documented? ScalaDocs", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5097", "title": "[FLINK-6909] [FLINK-7450] [types] Improve the handling of POJOs and clean-up type extraction", "body": "## Contribution Checklist\r\n\r\nThis PR solves a variety of issues that are related to the type extraction of POJOs and tuples.\r\n\r\n## What is the purpose of the change\r\n\r\nMake the type extraction more stable and help users with more detailed exceptions.\r\n\r\n## Brief change log\r\n\r\n- Until now, subclasses of tuples where not properly checked for additional fields and serializability: Non-static subclasses of tuples or classes with no default constructor were valid types.\r\n- Even existing tests and Gelly classes where not implemented correctly.\r\n- I fixed bugs related to bounded generic fields in POJOs.\r\n- The type extractor has been refactored and simplified in order to have more consistent behavior. E.g., getForClass was unable to determine subclasses of tuples.\r\n- Type extraction tests have been refactored to remove all warnings that were present.\r\n- I tested generated Lombok POJOs. \r\n- Class cast execeptions in CSV reader have been fixed as well.\r\n- I added a utility method `PojoTypeInfo.ensurePojo(MyPojo.class)` that validates if a type is a POJO and throws an exception with reasons why the given field is no POJO. I think this can help users a lot to avoid common mistakes.\r\n\r\n\r\n## Verifying this change\r\n\r\nTests have been added to `TypeExtractorTest`, `PojoTypeExtractionTest`. Existing tests still run.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): yes\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: yes\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\r\n  - The S3 file system connector: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? no\r\n  - If yes, how is the feature documented? not applicable\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4832", "title": "[hotfix] [docs] Enable creating an empty SNAPSHOT project easily", "body": "Improves the documentation for allowing to create an empty SNAPSHOT project in one command. This does not affect stable releases, if we are fine with using this command. I will also adapt the `quickstart-SNAPSHOT.sh` script.\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3589", "title": "[FLINK-5047] [table] Add count sliding group-windows for batch tables", "body": "This PR adds the missing sliding group-windows for row intervals. I also added an additional table to the documentation to make it more transparent what is supported.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/17597175", "body": "Sorry, that was a copy/paste error. I will fix it in the next commit.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/17597175/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "genged": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/b7f3497d384ce337f1765dd0253548937101fad4", "message": "[FLINK-6926] [table] Add support for MD5, SHA1 and SHA256\n\nThis closes #4810."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5324", "title": "[FLINK-6926] [table] Add support for SHA-224, SHA-384, SHA-512", "body": "## What is the purpose of the change\r\n\r\nThis pull request implements SHA224, SHA384, SHA512 support in Flink SQL as discussed in FLINK-6926\r\n\r\n## Brief change log\r\n\r\n  - Added SHA224, SHA384, SHA512 SQL functions\r\n  - Added relevant unit tests\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n  - Added SQL expression tests\r\n  - Added HashFunctionsTest with testAllApis\r\n  - Validated both correct calculation and behavior for null input\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: don't know\r\n  - The runtime per-record code paths (performance sensitive): don't know\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\r\n  - The S3 file system connector: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? yes\r\n  - If yes, how is the feature documented? docs\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pnowojski": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/3345aa7e1adb58b3845bb72e9ee6e0ee9d8fe32b", "message": "[FLINK-8375][network] Remove unnecessary synchronization\n\nSynchronized blocks in ResultPartition could affect only:\n1. totalNumberOfBuffers and totalNumberOfBytes counters\n2. subpartition add(), finish() and release() calls.\n\nHowever:\n1. counters were not used anywhere - they are removed by this commit\n2a. add(), finish() and release() methods for PipelinedSubpartition were already threads safe\n2b. add(), finish() and release() methods for SpillableSubpartition were made thread safe in\nthis commit, by basically pushing synchronized section down one level.\n\nThis closes #5260."}, {"url": "https://api.github.com/repos/apache/flink/commits/c816191113d813156467f3e33856636ef0bcce38", "message": "[FLINK-8220][network-benchmarks] Define network benchmarks in Flink project"}, {"url": "https://api.github.com/repos/apache/flink/commits/81d3e72eb8cd9eb591f6ea67bc904dd0ad200a17", "message": "[hotfix][util] Added suppressExceptions for lambda functions"}, {"url": "https://api.github.com/repos/apache/flink/commits/435d9d320ba320b40eb328b59e32cda6cc2c531b", "message": "[hotfix][test] Add timeout for joining with CheckedThread"}, {"url": "https://api.github.com/repos/apache/flink/commits/c6945c2ef48d4c2cad3fc935435c1ab83e834969", "message": "[FLINK-8178][network] Introduce not threadsafe write only BufferBuilder\n\nWhile Buffer class is used in multithreaded context it requires synchronisation.\nPreviously it was miss-leading and unclear, suggesting that RecordSerializer should\ntake into account synchronisation of the Buffer that's holding. With NotThreadSafe\nBufferBuilder there is now clear separation between single-threaded writing/creating\na BufferBuilder and multithreaded Buffer handling/retaining/recycling.\n\nThis increases throughput of network stack by factor of 2, because previously\nmethod getMemorySegment() was called twice per record and it is a synchronized\nmethod on recycleLock, while RecordSerializer is sole owner of the Buffer at\nthis point, so synchronisation is not needed."}, {"url": "https://api.github.com/repos/apache/flink/commits/0888bb622e275ac6ff2408c2ae5014fd787b5dbd", "message": "[FLINK-8214][streaming-tests] Collect results into proper mock in StreamMockEnvironment"}, {"url": "https://api.github.com/repos/apache/flink/commits/af6bdb606e825d0d66ba532bcb9d8335f9f4c54b", "message": "[FLINK-8210][network-tests] Collect results into proper mock in MockEnvironment"}, {"url": "https://api.github.com/repos/apache/flink/commits/d5d4da1b3785c493e441eeeb99b94718374b1556", "message": "[hotfix][runtime] Remove unused methods"}, {"url": "https://api.github.com/repos/apache/flink/commits/5cf37782f482f5dd51ad599a083eaa62dfcd805a", "message": "[FLINK-8209][network-tests] Make LocalBufferPoolDestroyTest less implementation dependent"}, {"url": "https://api.github.com/repos/apache/flink/commits/97db0bf9c1448a7e672f5d0235e301d03e1cf7d2", "message": "[FLINK-8208][network-tests] Reduce mockito usage in RecordWriterTest"}, {"url": "https://api.github.com/repos/apache/flink/commits/409ea2314d780436a404b3d8b61768e20af7485a", "message": "[hotfix][network-tests] Simplify TestPooledBufferProvider"}, {"url": "https://api.github.com/repos/apache/flink/commits/91c72b9dc611e73790147256adf859c177afe862", "message": "[FLINK-8207][network-tests] Unify TestInfiniteBufferProvider and TestPooledBufferProvider"}, {"url": "https://api.github.com/repos/apache/flink/commits/1f60a1de563ccca4ea0309fbd5c6c3531090ddc9", "message": "[hotfix][network] Drop redundant this reference usages"}, {"url": "https://api.github.com/repos/apache/flink/commits/091a37052b7045b3ed28c68bfea109024a5d1871", "message": "[FLINK-8298][tests] Properly shutdown MockEnvironment to release resources\n\nThis closes #5193."}, {"url": "https://api.github.com/repos/apache/flink/commits/e8d1aa57a8246de0b78e799a02c08f4007fb3a92", "message": "[FLINK-8268][streaming][tests] Improve TwoPhaseCommitSinkFunctionTest stability by using custom in memory storage"}, {"url": "https://api.github.com/repos/apache/flink/commits/8aca84c7266ca88272d54d86137a6560f7afc2d7", "message": "[hotfix][docs] Update debugging classloading doc to Java 8\n\nSince Java 8 Metaspace has replaced PermGen\n\nThis closes #5158."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5316", "title": "[FLINK-8022][kafka-tests] Disable at-least-once tests for Kafka 0.9", "body": "For some reasons this test is sometimes failing in Kafka09 while the same code works in Kafka010. Disabling this test because everything indicates those failures might be caused by unfixed bugs in Kafka 0.9 branch\r\n\r\n## Verifying this change\r\n\r\nThis change disables two tests.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / **no**)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)\r\n  - The serializers: (yes / **no** / don't know)\r\n  - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)\r\n  - The S3 file system connector: (yes / no ****/ don't know)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes / **no**)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4801", "title": "[FLINK-7812] Log system resources metrics", "body": "## What is the purpose of the change\r\n\r\nThis PR adds various system resources metrics, useful for analysing issues on machines/clusters for which there are no detailed external resources logging systems.\r\n\r\n## Verifying this change\r\n\r\nThis change was mostly manually tested, since it's difficult to write automated tests for this feature. However there is some trivial `SystemResourcesCounterTest` to make sure that codes executes without obvious mistakes.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (**yes** / no)\r\n\r\nIt adds `ohci` dependency.\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (**yes** / no)\r\n  - If yes, how is the feature documented? (not applicable / **docs** / **JavaDocs** / not documented)\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4723", "title": "[FLINK-7684] Avoid data copies in MergingWindowSet", "body": "## What is the purpose of the change\r\n\r\nPreviously MergingWindowSet uses ListState of tuples to persists it's mapping. This is inefficient because this ListState of tuples must be converted to a HashMap on each access.\r\n\r\nFurthermore, for some cases it might be inefficient to check whether mapping has changed before saving it on state.\r\n\r\nFixing those two issues improve session windows [benchmarks](https://github.com/dataArtisans/flink-benchmarks) results by 10 - 20%\r\n\r\nFirst commit comes from different PR #4722 \r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests, such as *(please describe tests)*.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: **yes** (it changes how WindowOperator is being serialized)\r\n  - The runtime per-record code paths (performance sensitive): **yes**\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? yes\r\n  - If yes, how is the feature documented? JavaDocs\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4626", "title": "[FLINK-7561][streaming] Implement PreAggregationOperator", "body": "## What is the purpose of the change\r\n\r\nTo improve performance in certain situations this PR adds basic implementation of pre-aggregation operator for DataStream API.\r\n\r\n## Verifying this change\r\n\r\nThis change added `PreAggregationOperatorTest`\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency):  no \r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? yes\r\n  - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented): extensive JavaDocs.\r\n\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zhijiangW": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/718a2ba0bb1867d7053d813eb76cf6796b35d86f", "message": "[FLINK-7468][network] Implement sender backlog logic for credit-based\n\nTHis closes #4559."}, {"url": "https://api.github.com/repos/apache/flink/commits/1752fdb339df4e4d0a5063b24c460abdc0a44264", "message": "[FLINK-7416][network] Implement Netty receiver outgoing pipeline for credit-based"}, {"url": "https://api.github.com/repos/apache/flink/commits/268867ce620a2c12879749db2ecb68bbe129cad5", "message": "[FLINK-7406][network] Implement Netty receiver incoming pipeline for credit-based"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5317", "title": "[FLINK-8458] Add the switch for keeping both the old mode and the new credit-based mode", "body": "## What is the purpose of the change\r\n\r\n*After the whole feature of credit-based flow control is done, we should add a config parameter to switch on/off the new credit-based mode. To do so, we can roll back to the old network mode for any expected risks.*\r\n\r\n*The parameter is defined as taskmanager.network.credit-based-flow-control.enabled and the default value is true. This switch may be removed after next release.*\r\n\r\n*This PR is based on #4552  whose commit is also included for passing travis.*\r\n\r\n## Brief change log\r\n\r\n  - *Abstract the `NetworkClientHandler` interface for different implementations in two modes*\r\n  - *Abstract the `NetworkSequenceViewReader` interface for different implementations in two modes*\r\n  - *Define the `taskmanager.network.credit-based-flow-control.enabled` in `TaskManagerOptions`*\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests*.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4552", "title": "[FLINK-7456][network] Implement Netty sender incoming pipeline for credit-based", "body": "## What is the purpose of the change\r\n\r\nOn sender side, it maintains credit from receiver's `PartitionRequest` and `AddCredit` messages, then sends buffer based on credit and network capacity. This PR is mainly involved in incoming pipeline logic for credit-based.\r\n\r\n## Brief change log\r\n\r\n  - *Each subpartition view maintains current credit and a boolean field to mark whether it is already registered available for transfer*\r\n  - *Update current credit in processing `PartitionRequest` and `AddCredit` messages*\r\n  - *The mechanism of enqueue the subpartition view and update the registered status field*\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n  - *Added test to verify that current credit is updated correctly and subpartition view is enqueued when received `AddCredit` message*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3340", "title": "[FLINK-5703] [Job Manager] ExecutionGraph recovery via reconciliation with TaskManager reports", "body": "This is part of [Non-disruptive JobManager Failures via Reconciliation ](https://issues.apache.org/jira/browse/FLINK-4911). \r\n\r\nThe design doc for this part is attached in the JIRA and it mainly contains the following work:\r\n\r\n- `RECONCILING` state transition for job status and execution.\r\n- The data structure recovery for `ExecutionGraph`, `ExecutionVertex`,`Execution` based on `TaskManager` reports. \r\n\r\nSome parts are left for the whole feature:\r\n\r\n- The related modifications in `TaskManger` side will be submitted in another JIRA, including not cancel the tasks when notified job leader changed and report the task status to `JobManager`, etc.\r\n- This PR will not make an effect on the current master branch, so it is safe to merge. The reconcile logic should be triggered by `JobManagerRunner` when grants leadership, but it is dependent on [Determine whether the job starts from last JobManager failure](https://issues.apache.org/jira/browse/FLINK-5501) , so I will modify the `JobManagerRunner` logic after [FLINK-5501](https://issues.apache.org/jira/browse/FLINK-5501) is merged.\r\n- For `SUSPEND` state, the current logic can not cover it because it is not very clear now. For example, it should support transition from `SUSPEND` to `CREATED`  or `RECONCILING` and should not cancel the executions when revoke leadership. I want to solve the `SUSPEND` issue completely in another separate JIRA.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "StevenLangbroek": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/40ba6261b9b151459b3402087d20bb9ccba73599", "message": "[FLINK-8352] [web-dashboard] Report error on jar submission failure\n\nThis closes #5264."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xccui": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/222e6945b795cf45ee5aa5228fb207e980854046", "message": "[FLINK-7797] [table] Add support for windowed outer joins for streaming tables.\n\nThis closes #5140."}, {"url": "https://api.github.com/repos/apache/flink/commits/f399b3fbb30107ae4b0d6e40ef0bcbf365b533bb", "message": "[FLINK-8122] [table] Name all built-in table sinks and sources\n\nThis closes #5068."}, {"url": "https://api.github.com/repos/apache/flink/commits/0695929bf81fae42f8ebf2033678f9bd9d186c05", "message": "[FLINK-8258] [table] Enable query configuration for batch queries\n\nThis closes #5169."}, {"url": "https://api.github.com/repos/apache/flink/commits/e8edbcafae6561423d0f8f321d2b64805f09357b", "message": "[FLINK-8257] [conf] Unify the value checks for setParallelism()"}, {"url": "https://api.github.com/repos/apache/flink/commits/d74869f8a4642fec0064ae0d0a2911aa2dea3ce4", "message": "[FLINK-8278] [docs] Fix Scala examples of metrics docs (var initialization)."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5210", "title": "[FLINK-8316] [table] The CsvTableSink and the CsvInputFormat are not in sync", "body": "## What is the purpose of the change\r\n\r\nThis PR adds an extra parameter (`trailingDelim`) to `CsvTableSink` to enable appending a trailing field delimiter for each row.\r\n\r\n## Brief change log\r\n\r\n  - Adds an extra boolean parameter `trailingDelim` to `CsvTableSink`.\r\n  - Adds a related test case to `org.apache.flink.table.runtime.batch.table.TableSinkITCase`.\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change can be verified by the added test case in `TableSinkITCase`.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (**yes**)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes)\r\n  - If yes, how is the feature documented? (JavaDocs)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5032", "title": "[FLINK-8090] [DataStream] Improve the error message for duplicate state name", "body": "## What is the purpose of the change\r\n\r\nThis PR improves the error message when users trying to access two states of different types, but with an identical name. However, it cannot detect two states of the same type and name, but are registered via two descriptors.\r\n\r\n## Brief change log\r\n\r\nRefactor `DefaultKeyedStateStore.java` to raise an error message when the required state type does not match the real state type.\r\n\r\n## Verifying this change\r\n\r\nThe change can be verified by existing tests.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4934", "title": "[FLINK-7800] [table] Enable window joins without equi-join predicates", "body": "## What is the purpose of the change\r\n\r\nThis PR enables the stream window joins without equi-join predicates.\r\n\r\n## Brief change log\r\n\r\n  - Push down all predicate checks, except for the priority-related one below, from `FlinkLogicalRule` to DataSet/DataStream rules, so that we can organize them in a flat mode. \r\n  - Create a `JoinPlanUtil` tool to provide utilities (e.g., equi-predicate checker) for join plan translation.\r\n  - Give priority to join plans with equi-join predicates by forbidding equi-conditions on `RexCall` operands in `FlinkLogicalRule`.\r\n  - Enable stream joins without euqi-predicates for Table API by checking the table environment in `Join.testJoinCondition` (`operators.scala`).\r\n  - Add related tests.\r\n  - Update the documents.\r\n\r\n## Verifying this change\r\n\r\nThis change can be verified by the added tests in `JoinTest` and `JoinITCase`.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (**no**)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (**no**)\r\n  - The serializers: (**no**)\r\n  - The runtime per-record code paths (performance sensitive): (**yes**)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (**no**)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (**yes**)\r\n  - If yes, how is the feature documented? (**docs**)", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "okumin": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/7b96d10094d0a6f87a3e912e28ce8b68b310d28f", "message": "[hotfix][docs] Fix DataStream iterations documentation\n\n* Fix a scala example which is using a wrong variable\n* Remove partitioning descriptions\n  * partitioning parameters are already removed from\n  IterativeStream#closeWith/DateStream#iterate\n  * https://github.com/apache/flink/pull/988\n  * https://github.com/apache/flink/pull/4655\n\nThis closes #5249."}, {"url": "https://api.github.com/repos/apache/flink/commits/ae925b63f25dce989766dbbc155abfbb0e7c992c", "message": "[hotfix] [docs] Fix Scala code snippets in docs.\n\n* remove unneeded semi-colons\n* add `()` to `print` method\n    * typically, methods with some side-effects are invoked with `()`\n* fix a few misc issues\n\nThis closes #5221."}, {"url": "https://api.github.com/repos/apache/flink/commits/473112ced3e9af7fb30a9120034f43daa7b5b3a0", "message": "[hotfix] [docs] Remove duplicated 'program' in docs/dev/api_concepts.md\n\nThis closes #5222."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Matrix42": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/3cdc5d1dce7c8cf0db0352d86867f73f6ccd9be9", "message": "[FLINK-8292] Remove unnecessary force cast in DataStreamSource\n\nThis closes #5180."}, {"url": "https://api.github.com/repos/apache/flink/commits/0ae70baabb70422ac7528deafe19c08d8a984594", "message": "[hotfix] [doc] Fix typo in filesystems.md\n\nThis closes #5237"}, {"url": "https://api.github.com/repos/apache/flink/commits/d12158e8c996aeb3a4900e4f57347a5b6f8fa1cb", "message": "[hotfix] [javadoc] Fix typo in StreamExecutionEnvironment javadoc\n\nThis closes #5164."}, {"url": "https://api.github.com/repos/apache/flink/commits/72cd5921684e6daac4a7dd7916698eeee98b56d5", "message": "[hotfix][javadoc] Fix typo in StreamElement javadoc\n\nThis closes #5152."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ankitiitb1069": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/ac3b721bf65450ab4ecd90c05d9bb3946d2e447f", "message": "[FLINK-8116] [DataStream] Fix stale comments referring to Checkpointed interface"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "EronWright": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/9ae4c5447a2f5aae2b65d5860f822d452a9d5af1", "message": "[FLINK-8265] Missing jackson dependency for flink-mesos"}, {"url": "https://api.github.com/repos/apache/flink/commits/d6f5ea30ece3495894b0e6d2847b810040b37658", "message": "[FLINK-8174] Mesos RM unable to accept offers for unreserved resources\n\n[FLINK-8174] Mesos RM unable to accept offers for unreserved resources\n- added test `OfferTest`\n\n[FLINK-8174] Mesos RM unable to accept offers for unreserved resources\n- fix `LaunchCoordinatorTest`\n\n[FLINK-8174] Mesos RM unable to accept offers for unreserved resources\n- improved javadocs\n\n[FLINK-8174] Mesos RM unable to accept offers for unreserved resources\n- rename `print` to `toString`\n- precalculate offer resource values\n- extend TestLogger\n\nThis closes #5114."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5208", "title": "[FLINK-8265] [Mesos] Missing jackson dependency for flink-mesos", "body": "## What is the purpose of the change\r\nFix for missing dependency in Mesos deployments.\r\n\r\n## Brief change log\r\n- updated `pom.xml` of `flink-mesos` to include `com.fasterxml.jackson.core:*` as a shaded and relocated dependency.\r\n\r\n## Verifying this change\r\n- verify that `flink-dist_2.11-1.4-SNAPSHOT.jar` includes classes in `org/apache/flink/mesos/shaded/com/fasterxml/jackson/`.\r\n- This change is a trivial rework / code cleanup without any test coverage.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n  - Dependencies (does it add or upgrade a dependency): yes\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive):  no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\r\n  - The S3 file system connector: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? no\r\n  - If yes, how is the feature documented? not applicable\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4767", "title": "[FLINK-7738] [flip-6] Create WebSocket handler (server, client)", "body": "## What is the purpose of the change\r\n\r\nIntroduces WebSocket support for the FLIP-6 REST server and client.\r\n\r\nThe basic idea is to integrate websockets into the REST architecture.   A REST resource (identified with normal `MessageParameters` parsed from the incoming HTTP request) responds to the `GET` method by upgrading the channel to a websocket.   For example, a REST resource of `/jobs/:jobid/events` may be understood as a websocket resource providing job events for the given `jobid`.  A new type of handler based on`AbstractWebSocketHandler` serves to parse the message parameters, upgrade the channel, and handle ongoing messaging.\r\n\r\nThe websocket resource contract is defined using a new variant of `RestHandlerSpecification`  called `WebSocketSpecification`.\r\n\r\nWIthin the server, Netty's `ChannelGroup` is leveraged to act as an event bus to easily dispatch an outbound message to one or more channels based on a routing key.  In the above example, the routing key might be `jobid`, meaning that a given channel is listening to events related to a certain job.   It is expected that a concrete subclass of `RestServerEndpoint` create one or more `KeyedChannelRouter` instances as needed for its handlers, and then write messages as it sees fit.\r\n\r\nThe client was similarly adapted to open a `WebSocket` with associated listeners.  Consider the client-side work to be a stop-gap pending further discussion.\r\n\r\nThe `RestEndpointITCase` test was enhanced with an end-to-end demonstration.   A separate unit test for `AbstractWebSocketHandler` was also introduced.\r\n\r\n## Brief change log\r\n- Introduce `AbstractWebSocketHandler` to handle inbound and outbound websocket messages.\r\n- Introduce `WebSocketSpecification` as a contract for REST handlers that open a websocket.\r\n- Move path and query string parameter parsing into the `MessageParameters` class.\r\n- Improve `RedirectHandler` to support a composition style of use.\r\n- Introduce `KeyedChannelRouter` to route websocket messages to interested channels.\r\n- Update `RestClient` with a new method, `sendWebSocketRequest`.\r\n- Introduce `WebSocket` and `WebSocketListener`.\r\n- Update `RestEndpointITCase` with end-to-end websocket test.\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n- `RestEndpointITCase`\r\n- `AbstractWebSocketHandlerTest`\r\n- `RedirectHandlerTest`\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive):no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? no\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4765", "title": "[FLINK-7753] [flip-6] close REST channel on server error", "body": "\r\n\r\n## What is the purpose of the change\r\nImprove REST server handling of unexpected errors.\r\n\r\n## Brief change log\r\n- Close the connection when returning a 500 response. \r\n- Obey rfc2616#section-14.10 which says that HTTP 1.1 responses should explicitly say `Connection: close` if keep-alive was requested but not agreed to.\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? no\r\n\r\n\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/21361837", "body": "You took the wrong shortcut here to access the Flink configuration.   This code erroneously says, \"get the configured framework name *from the JM's dynamic configuration only*\".  This code would fail if the `mesos.rm.framework.name` property were instead set in the JM's static configuration.\r\n\r\nIt is undesirable to use the raw configuration at this stage anyway.   You probably notice that Flink 'eagerly' processes its configuration into typed objects like `MesosConfiguration`   Let's use that here too.   Please change `LaunchableMesosWorker` constructor to take `MesosConfiguration` as a parameter.   Use `mesosConfig.frameworkInfo().getName()` when setting `ENV_FRAMEWORK_NAME`.   Change `MesosFlinkResourceManager::createLaunchableMesosWorker` to pass the `mesosConfig` that it has.\r\n\r\n\r\nPlease adjust the handling of `TASK_MANAGER_HOSTNAME_KEY` in the same way.\r\n\r\nThanks!", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/21361837/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/21361863", "body": "I would be happy to see this renamed to `mesos.resourcemanager.tasks.hostname` for simplicity.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/21361863/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "Aegeaner": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/3c91de518c4ce95d180400ea36a840be10fe6a86", "message": "[FLINK-8139] [table] Check for proper equals() and hashCode() for stateful operators\n\nThis closes #5065."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5116", "title": "[FLINK-8191] [Kafka Connector] Add a RoundRobinPartitioner to be shipped with the Kafka\u2026", "body": "Add a RoundRobinPartitioner to be shipped with the Kafka connector\r\n\r\n\r\n*Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*\r\n\r\n*Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*\r\n\r\n## Contribution Checklist\r\n\r\n  - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.\r\n  \r\n  - Name the pull request in the form \"[FLINK-XXXX] [component] Title of the pull request\", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.\r\n  Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.\r\n\r\n  - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.\r\n  \r\n  - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).\r\n\r\n  - Each pull request should address only one issue, not mix up code from multiple issues.\r\n  \r\n  - Each commit in the pull request has a meaningful commit message (including the JIRA id)\r\n\r\n  - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.\r\n\r\n\r\n**(The sections below can be removed for hotfixes of typos)**\r\n\r\n## What is the purpose of the change\r\n\r\nWe should perhaps consider adding a round-robin partitioner ready for use to be shipped with the Kafka connector, along side the already available FlinkFixedPartitioner.\r\n\r\n\r\n## Brief change log\r\n\r\nAdd a `FlinkRoundRobinPartitioner` class in _org.apache.flink.streaming.connectors.kafka.partitioner_ package.\r\n\r\n\r\n## Verifying this change\r\n\r\nVerified by new added unit test `FlinkRoundRobinPartitionerTest`.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / **no**)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)\r\n  - The serializers: (yes / **no** / don't know)\r\n  - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)\r\n  - The S3 file system connector: (yes / **no** / don't know)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (**yes** / no)\r\n  - If yes, how is the feature documented? (not applicable / docs / **JavaDocs** / not documented)\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dianfu": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/9e3bac39e01722d22e3c08bef593987244609a72", "message": "[FLINK-8226] [cep] Dangling reference generated after NFA clean up timed out SharedBufferEntry\n\nThis closes #5141"}, {"url": "https://api.github.com/repos/apache/flink/commits/c4acbb838ffc9582436f976bfb7eaff838a0ed87", "message": "[FLINK-8227] Optimize the performance of SharedBufferSerializer\n\nThis closes #5142"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5080", "title": "[FLINK-8159] [cep] Add rich support for SelectWrapper and FlatSelectWrapper", "body": "## What is the purpose of the change\r\n\r\n*This pull request add the rich support for SelectWrapper and FlatSelectWrapper. It the wrapped functions are rich function, it should process correctly.*\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): ( no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4563", "title": "[FLINK-7479] [cep] Support to retrieve the past event by an offset", "body": "## What is the purpose of the change\r\n\r\n*Currently, it's already able to retrieve events matched to the specifed pattern in IterativeCondition.Context. While there are also requirements to retrieve events by an physical offset. The retrieved events may not be matched to any pattern.*\r\n\r\n\r\n## Brief change log\r\n\r\n  - *Add API retain() in Pattern*\r\n  - *Buffer the past events in NFA.process*\r\n  - *Access the past events by the newly added API getEventByOffset in IterativeCondition.Context*\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n  - *Added test in IterativeConditionsITCase*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes)\r\n  - If yes, how is the feature documented? (docs / JavaDocs)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4513", "title": "[FLINK-6938] [cep] IterativeCondition should support RichFunction interface", "body": "## What is the purpose of the change\r\n\r\n*The core idea is that the StateTransition is unique in a NFA graph. So we store the conditions with a map which mapping from StateTransition to IterativeCondition, so the conditions can not serialized with NFA state. If I missed something, please point out.\r\n\r\nThis PR also includes FLINK-6938: IterativeCondition supports RichFunction interface.*\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests*.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): ( no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (JavaDocs)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4502", "title": "[FLINK-7062] [table, cep] Support the basic functionality of MATCH_RECOGNIZE", "body": "## What is the purpose of the change\r\n\r\n*This pull request adds the basic support of  MATCH_RECOGNIZE.*\r\n\r\n\r\n## Brief change log\r\n\r\n*(for example:)*\r\n  - *The MATCH_RECOGNIZE clause is transformed to CEP job with the existing CEP API*\r\n\r\n\r\n## Verifying this change\r\n\r\n\r\nThis change added tests and can be verified as follows:\r\n  - *Added test that validates MATCH_RECOGNIZE is parsed correctly and expected results are got*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes)\r\n  - If yes, how is the feature documented? (JavaDocs)\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4172", "title": "[FLINK-6983] [cep] Do not serialize States with NFA", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Xpray": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/91f00ec91b55b28a59114e88127af2f85f279eb5", "message": "[FLINK-8312][table] Fix ScalarFunction varargs length exceed 254\n\nThis closes #5206"}, {"url": "https://api.github.com/repos/apache/flink/commits/edf10c714fb1fe7a58e96a4c7006d0df48954b79", "message": "[FLINK-8301][table] Support Unicode in codegen for TableAPI && SQL\n\nThis closes #5203"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5174", "title": "[FLINK-8274][TableAPI & SQL] Fix Java 64K method compiling limitation\u2026", "body": "[FLINK-8274][TableAPI & SQL] Fix Java 64K method compiling limitation for CommonCalc'\r\n\r\n\r\n## What is the purpose of the change\r\n\r\nThe genereted code in ```CommonCalc``` consists of Three parts, reusable code per code, ```filterCondition.code``` and ```projection.code```, ```projection.code``` is most likely the longest parts, this issue intend to split ```projection.code``` to split function calls\r\n\r\n## Brief change log\r\n\r\n  - *add a configuration to ```TableConfig```, which indicates the max length of generated code*\r\n  - *the default value for maxGenerateCodeLength is 48k*\r\n\r\n## Verifying this change\r\n\r\n\r\nThis change is already covered by existing tests, such as:\r\n\r\nall tests that involve with  the ```CommonCalc```\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n  - *CalcWithSplitCodeGenITCase which set maxGenerateCodeLength to 1*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): \r\n     no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: \r\n    no\r\n  - The serializers: \r\n    no\r\n  - The runtime per-record code paths (performance sensitive): \r\n    no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: \r\n    no\r\n  - The S3 file system connector: \r\n    no\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? \r\n    no\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4672", "title": "[Flink-7621][TableAPI & SQL] Fix Inconsistency of CaseSensitive Configuration", "body": "*Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*\r\n\r\n*Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*\r\n\r\n## Contribution Checklist\r\n\r\n  - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.\r\n  \r\n  - Name the pull request in the form \"[FLINK-XXXX] [component] Title of the pull request\", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.\r\n  Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.\r\n\r\n  - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.\r\n  \r\n  - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).\r\n\r\n  - Each pull request should address only one issue, not mix up code from multiple issues.\r\n  \r\n  - Each commit in the pull request has a meaningful commit message (including the JIRA id)\r\n\r\n  - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.\r\n\r\n\r\n**(The sections below can be removed for hotfixes of typos)**\r\n\r\n## What is the purpose of the change\r\n\r\nfix Inconsistency of CaseSensitive Configuration \r\n\r\n## Brief change log\r\n\r\nuse tableEnv's config to deal with case sensitive\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature?  no\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4308", "title": "[FLINK-7163] Make LogicalTableFunctionCall immutable", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3880", "title": "[FLINK-6457] Clean up ScalarFunction and TableFunction interface", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "walterddr": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/2142eeda9df262e989951c4b31273cbd9346567f", "message": "[FLINK-8215] [table] Support implicit type widening for array/map constructors in SQL\n\nThis closes #5148."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "eskabetxe": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/3397bd66abdc529a2d5940a09f9feee035fd0b90", "message": "[FLINK-8249] [kinesis] Fix setting region on KinesisProducerConfiguration\n\nThis closes #5160."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5290", "title": "[Flink-8424][Cassandra Connector] upgrade of cassandra version and driver to latest", "body": "## What is the purpose of the change\r\n\r\nUpdate Cassandra version and driver to something new,\r\nthe driver is current in 3 minor versions behind\r\nthe version is 1 major version behind\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests, such as *(please describe tests)*.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): yes\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\r\n  - The S3 file system connector: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature?  no\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "joerg84": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/402af2a07eb8144406e25e727dc56e65732ac8b2", "message": "[hotfix] [docs] Consistent capitalization in Mesos documentation.\n\nThis closes #5157."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "aljoscha": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/5221a70e0b20ec50fc1e63947289c12df71f0ff2", "message": "[hotfix] Properly delete temp flink dir in create_source_release.sh"}, {"url": "https://api.github.com/repos/apache/flink/commits/f6e24ab60c3d434186d9cd9906c6562433617ff7", "message": "[FLINK-8186] Exclude flink-avro from flink-dist; fix AvroUtils loading\n\nBefore, AvroUtils were loaded when the class was loaded which didn't\ntake into account the user-code ClassLoader. Now, we try loading avro\nutils with the Thread context ClassLoader."}, {"url": "https://api.github.com/repos/apache/flink/commits/7c5a6941b4f78332dcda93299c2edca209ed4b2d", "message": "[hotfix] Remove empty line in UtilsTest"}, {"url": "https://api.github.com/repos/apache/flink/commits/aa1f83333f31c01d8fd4ae5bdaaf48c44aadc221", "message": "[FLINK-8177] Replace TestingContainer by mock in YARN UtilsTest\n\nThe Container Interface was extended in Hadoop 2.9, meaning that the\ntest would not run when compiling with Hadoop 2.9. Using a mock fixes\nthis problem."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4920", "title": "[FLINK-7944] Allow configuring Hadoop classpath", "body": "R: @StephanEwen ", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3616", "title": "[FLINK-6188] Correctly handle PARALLELISM_DEFAULT in stream operator", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2573", "title": "Refactor StreamOperator Hierachy to make Keys Explicit", "body": "This is more of a preview PR, I'm not yet completely done with testing and making sure that everything works.\n\nR: @StephanEwen \n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/64377709", "body": "ASM 4.x was used in the ClosureCleaner that was removed in the rewrite of the Scala API. I have an open pull request that re-adds the ClosureCleaner: #156. I will see whether this also works with ASM 5.x, then we can bump the version to this.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/64377709/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/64380788", "body": "Ok, seems to also work with ASM 5, so you can change the version there.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/64380788/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/95590430", "body": "Hi @qmlmoon,\nsorry for the long wait on this PR. Could you please rebase on top of the current master and also get rid of the merge commits in the process?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/95590430/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/96960370", "body": "Thanks for working with this on me. Very nice contribution. :smile: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/96960370/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/64371088", "body": "@rmetzger Can you have a look and see whether this fits into your Pojo work?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/64371088/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/48483961", "body": "I think it should work, yes.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/48483961/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/64368523", "body": "Yes, I would like to encourage people to not use the KeySelectors. The implementation is cumbersome and expressions already provide good performance and should only get better once we have generated code for expression comparators.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/64368523/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/49417330", "body": "I might be able to provide that as part of [FLINK-987](https://issues.apache.org/jira/browse/FLINK-987).\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/49417330/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/49990091", "body": "There is still some work to be done. I'm trying to get rid of the lock/unlock.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/49990091/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50257908", "body": "Fixed the typo.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50257908/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/54287301", "body": "Closing it since @rmetzger took over.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/54287301/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50636024", "body": "Maybe it's just me but I don't like the names of the interfaces. Reducible, for example, to me suggests that the thing can be reduced. But the UDF does the reducing and is not reducible.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50636024/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50646910", "body": "I like Mapper best. A \"Map\" is something different in most programming languages. :smile: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50646910/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50741310", "body": "Another possibility would be MapUDF and friends...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50741310/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/51597079", "body": "Yes, this should disable it completely.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/51597079/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/55533326", "body": "Irrelevant because of Scala Rework Pull Request.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/55533326/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/56364075", "body": "Worked like a Charm. No comments :smile: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/56364075/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/54850915", "body": "This approach only works when using the Eclipse Java compiler. When using javac, which maven and intellij use, the generic type information is not preserved.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/54850915/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/55730576", "body": "I think this can be merged. Any objections?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/55730576/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/56824470", "body": "Any objections to finally merging this? If not, I will rebase it on master, squash it, and fix the documentation and quick start to work with the latest changes to our doc.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/56824470/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/56825241", "body": "Ok, if you have stuff to do... :smile: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/56825241/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/56830566", "body": "Yes, I'm working on it. But I have to rebase it on the updated documentation, your massive job manager stuff and the scala api stuff. :sweat: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/56830566/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/55709398", "body": "Does it have to be Javadoc? Since in Scala they normally use ScalaDoc.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/55709398/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/55713612", "body": "But people using the Scala API would probably like to have ScalaDoc? I'll look into genjavadoc nevertheless.\n\nFor the documentation, I would like to rewrite the Java documentation to have tabs for selecting Java or Scala for the code examples. Since the concepts are exactly the same now. What do you think?\n\nRegarding tabs in Scala source: I think it's highly unusual.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/55713612/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/55717633", "body": "Where does the hadoop2 build fail? Both on travis and on my machine \"mvm -Dhadoop.profile=2 clean verify\" works.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/55717633/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/55720667", "body": "I'm integrating this: http://www.scalastyle.org\n\nI'll use the spark scalastyle config. Do you think I have to mention it anywhere? It's also Apache Licensed after all...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/55720667/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/55725441", "body": "I added Scalastyle.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/55725441/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/55732651", "body": "Yeah, I'm saying in a comment that it's adapted from Spark.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/55732651/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/56271915", "body": "What would you say about merging this now?\n\n@rmetzger I think I addressed all your (correct) complaints. :smiley_cat: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/56271915/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/56947586", "body": "When using a rebalance operation, the plan visualiser does not actually say \"rebalance\". Can someone who knows the optimiser whether this is an actual bug. This also existed before I did this rewrite.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/56947586/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/57191203", "body": "Manually merged.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/57191203/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/57191189", "body": "Manually merged.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/57191189/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/57945878", "body": "I would look over this tomorrow. If you want another set of eyes. :smile: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/57945878/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/58889728", "body": "+1\nLooks good to me.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/58889728/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/59532259", "body": "Yes, It would be good to have those. But it is also good to know that all the existing Scala Tests still run.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/59532259/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/60230861", "body": "I added some Test cases.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/60230861/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/64392545", "body": "Any comments, anyone want to do some further tests. Or should we merge this rather sooner than later.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/64392545/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/64540293", "body": "Ok, I added a notice to the ITCase about the origin of the tests: Spark.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/64540293/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/64773937", "body": "Manually merged.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/64773937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/60249029", "body": "This is not complete, we would also need special support for some Collections: BitSet, OrderedSet, OrderedMap. And also for Some and Either.\n\nSo I'm closing this PR for now.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/60249029/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/61784331", "body": "Any comments on this? I would really like to get this in soon.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/61784331/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/61805253", "body": "Manually merged.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/61805253/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/64364702", "body": "Still nothing happening... :see_no_evil: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/64364702/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/62862942", "body": "Right now there are no jar files since the scala examples depend on the example data of the java examples. I think this is a good solution, if we want to have the Scala examples as jars.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/62862942/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/64355493", "body": "I don't like the name of the method: \"enableComments()\". It does not enable comments, what would that even mean. In my opinion it should be called \"ignoreComments()\".\n\nAlso, there is no support for this in the Scala API. We could add it by adding an optional parameter to readCsvFile(), like this:\n\n``` scala\ndef readCsvFile[T <: Product : ClassTag : TypeInformation](\n      filePath: String,\n      lineDelimiter: String = \"\\n\",\n      fieldDelimiter: Char = ',',\n      ignoreFirstLine: Boolean = false,\n      ignoreComments: String = null,\n      lenient: Boolean = false,\n      includedFields: Array[Int] = null): DataSet[T] = {\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/64355493/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/76738603", "body": "I'm the next person to be looking at this. Hopefully wan can merge it after I've looked at it. :smile: \n\n@zentol Do you want to keep in in the current location or do you want to move it to flink-python?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/76738603/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/76973194", "body": "Yes, this sounds good? Another thing: it has probably already come up but I just want to make sure, you implement CoGroup and Reduce the way you do because of performance, correct? That is, you don't do any work in the user code of a ReduceOperator but you do it in a chained MapPartition because there you get all the elements which makes communication with the python process more efficient. Same with CoGroup, where you implement your own grouping logic in python from the raw input streams.\n\nOverall I like the architecture, the communication between the host and the guest language is well abstracted and I can see this being reused for other languages.\n\nCould you rename the CoGroupPython\\* classes to something more generic? Because they really are a part of the generic language binding stuff and not specific to python, correct?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/76973194/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/76981299", "body": "You could call it CoGroupRaw, just an idea...\n\nOnce that and the split into the python and generic part is done I vote for merging this. The API looks good and other stuff, such as getting rid of the type annotations can be worked on afterwards. I think it would be good to get people that are interested to try it out.\n\nAlso, the code is very well commented and documented. :smile_cat: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/76981299/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/77127742", "body": "Thanks, I have two last requests, sorry for that.\n\nCould you rename flink-generic to flink-language-binding-generic? The problem is, that the package name is now flink-generic, it pops up like this in maven central and so on without the information that it is actually a sub package of flink-language-binding. This could be quite confusing.\n\nIn MapFunctin.py and FilterFunction.py you use map() and filter() respectively. These operations are not lazy, i.e. in map() it does first apply the user map-function to every element in the partition and then it collects the results. This can become a problem if the input is very big. Instead we should iterate over the iterator and output each element after mapping. This keeps memory consumption low. Same applies to filter().\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/77127742/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/90858070", "body": "What about the timeout? Are we confident that a longer timeout will be sufficient a large number of possible jobs?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/90858070/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/90863375", "body": "processes are terminated when a user cancels a job? so is the timeout still required or not? :smile: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/90863375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/90900778", "body": "Yeah, and several minutes doesn't cut it, as we've seen. On a WordCount example on not very large data a 5 minute timeout was not enough.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/90900778/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/94400959", "body": "I'll test it again on a cluster. Could you please elaborate a bit. Is the timeout still in? Communication is through TCP instead of the mapped files. but still with the same basic interface of writing basic values for communication?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/94400959/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/94402978", "body": "I was referring to the way that communication is handled between the java host and the generic language client: Communication between them is not based on a fixed set of Messages (for example, messages defined using something like Protobuf or Avro) but instead the knowledge about how messages are structured is implicit in the code that does the messaging. So the java side expects a sequence of primitives (integers, strings) in a certain order and the python side knows that order and sends them in this order.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/94402978/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/94443054", "body": "I just ran it on the cluster. Works like a charm. :smile: \n\nFor word count, python takes 12 minutes, java about 2:40. But this should be expected, I guess.\n\nGood to merge now, in my opinion.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/94443054/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/94798439", "body": "I merged it. :smile: \n\nThanks a lot @zentol for staying with this for so long. Great work!\n\nP.S. Could you please close this PR, I always forget adding the \"closes #...\" message.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/94798439/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/64330026", "body": "This looks good now. Any objections to merging this right away?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/64330026/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/64404219", "body": "I merged it, but the sync is rather slow...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/64404219/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/64663589", "body": "@rmetzger I hope I addressed your concerns. :dancers: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/64663589/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/64809571", "body": "Any thoughts about this?\n\nI myself am not 100% happy with this. When the user uses a subclass that cannot be analysed by our TypeExtractor the whole program will fail with an Exception. At Runtime, not at pre-flight time.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/64809571/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/64811314", "body": "Thats true. :D Before the job would just produce incorrect results because the PojoSerializer would only serialise the fields it knows about.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/64811314/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/65228151", "body": "I ran some tests to measure the performance impact. I tried our regular WordCount example, which uses Tuples, a POJO WordCount and a POJO WordCount where the user function emits a subclass of the type of the DataSet.\n\nThe results:\n- WordCount Tuple: 440 sec\n- WordCount POJO (w/o this PR): 540 sec\n- WordCount POJO (w/ this PR): 550 sec\n- WordCount POJO Subclass: 2443 sec\n- WordClass POJO Subclass with Tagging: 830 sec\n\nThe \"tagging\" version is a mockup of how it would behave if we allowed users to register classes, so that only an ID has to be transferred. Similar to how Kryo does it.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/65228151/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/66791457", "body": "I now also added a way for users to register classes and actual support for class tags. The performance numbers I posted earlier are still valid: \n- WordCount Tuple: 440 sec\n- WordCount POJO (w/o this PR): 540 sec\n- WordCount POJO (w/ this PR): 550 sec\n- WordCount POJO Subclass: 2443 sec\n- WordClass POJO Subclass with Tagging: 830 sec\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/66791457/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/69055954", "body": "Yes, I see your point in also registering it at Kryo. Can we get hold of the Kryo instance inside the ExecutionEnvironment? Or we could use the same mechanism I use for the Pojos to copy the list when creating a generic Kryo Serializer.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/69055954/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/69056074", "body": "Why now registerClass? You can only register Classes, that's the name for this concept in Java.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/69056074/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/69159459", "body": "I renamed it to registerType and extended it to KryoSerializer.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/69159459/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/70216890", "body": "No objections, your honour.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/70216890/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/71436404", "body": "I will have to rework this now that the support for registering Types and Serializers at Kryo was merged.\n\nThe POJO subclass with tagging is slower because we do additional checks and lookups: Upon serialisation we perform a map lookup to check whether the subclass is actually a registered class. When deserialising we have to fetch the correct subclass serialiser from an array of subclass serialisers.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/71436404/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/72006969", "body": "We could try and analyse the type as a POJO. If it is a POJO add it to the PojoSerializer, otherwise, add it to the KryoSerializer.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/72006969/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/72020646", "body": "I took @fhueske's rebase from #316.\n\nI also added a switch to the TypeExtractor to force it to use Kryo, even though it could analyse a type as a POJO. In some cases this might be preferable. For example, when using interfaces with subclasses that cannot be analyzed as POJO. \n\nWhat do you think?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/72020646/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/72161285", "body": "Then it fails at runtime, which makes me very uneasy. But then again, stuff can always fail at runtime when the user uses some strange subclass. Even more so without this PR.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/72161285/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/72433217", "body": "Yes, it is not a good solution But what you propose isn't either: If we use Kryo for those subclasses that we cannot handle then nothing works anymore. The whole reason we have support for POJOs is that we can theoretically compare them in their binary representation. We are not doing this right now (the PojoComparator is always comparing in deserialised form) but we added it with that goal in mind.\n\nIf we don't want to do that anymore we can just get rid of the whole POJO serialisation code and use Kryo for everything.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/72433217/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/73679224", "body": "Ok, I'll merge this, and then you can make your changes on top of this.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/73679224/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/64776347", "body": "The new functionality is not used anywhere? Or am I missing something?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/64776347/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/66622241", "body": "I also ported the Merge-Join to the new switchable execution paradigm. Now everything should be ported.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/66622241/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/69054881", "body": "Somehow, the order of the commits that github shows here is not correct. Also, the two other commits are in master by now and should not show up here at all.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/69054881/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/69055407", "body": "I also have performance numbers for our default WordCount and an optimised version that uses our Value Types:\n\nWordCount\nNo Reuse: 440 sec\nReuse: 400 sec\n\nWordCount with Value Types\nNo Reuse: 460 sec\nReuse: 330 sec\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/69055407/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/66750006", "body": "I moved the cast and added a Validate.\\* check.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/66750006/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/67168154", "body": "The cleaner visits all methods, yes. It does not know about how we use those functions. I'll change one of the tests to access the outer object through another method.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/67168154/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/67318180", "body": "Code looks good, other than the remark I had about cleaning the user function for type extraction (which is unnecessary).\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/67318180/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/67820873", "body": "Why does Streaming use it's own classes for field selectors? I.e. FieldsKeySelector, CaseClassKeySelector. Streaming also has it's own selector classes in the Java API.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/67820873/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/67822151", "body": "But in the batch API we don't have special KeySelectors, everything is handled uniformly in Keys.java. The field keys and expression keys support Java Tuples, Scala Tuples, Pojos and Case Classes.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/67822151/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/67822657", "body": "Also, as @mbalassi mentioned, we should really think about the directory structure now. Having Java Streaming in addons, Scala Streaming in flink-scala and the examples also scattered all over the place does not make things very manageable.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/67822657/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/67822719", "body": "I like the work, overall. It's just some nitpicking here and there. :smile: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/67822719/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/67827360", "body": "Yes, @rmetzger added some magic when he unified Pojo and Tuple keys. TypeComparator has a method extractKeys() that does exactly that. It extracts the key fields of a tuple or object and stores them in an array. I think all the infrastructure is there. You can just use Keys.java and create TypeComparators, those you can use for everything else. Maybe @rmetzger can have a look and make some suggestions for how this could best be implemented in the Streaming API.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/67827360/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/67834220", "body": "Plus, you get all the support for nesting tuples, pojos and case classes that is already there. :smile_cat: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/67834220/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/69008392", "body": "Also encountered this in Travis.\n\n+1 for merging this quickly\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/69008392/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/69163748", "body": "+1 I had a look over it, looks good.\n\nThe map driver I really forgot... and it's the easiest operator to port. :see_no_evil: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/69163748/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/69378698", "body": "This should go into the 0.8.0 since it fixes a very critical bug.\n\nTime is of the essence here. :D\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/69378698/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/69487377", "body": "Ok, if no one has any objections I will merge this and also cherry-pick in onto the 0.8 release branch.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/69487377/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/69988955", "body": "Ok, I looked at the existing LICENSE and NOTICE files and they don't contain any entries for apache licences projects. jodatime and the kaffee serialisers are also apache licenced, that's why I didn't add any entries for them either.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/69988955/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/70063665", "body": "I added to LICENSE AND NOTICE and also addressed the other issues.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/70063665/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/71490297", "body": "Ok, closing it ...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/71490297/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/69759167", "body": "Right now, what a user does is:\n\n```\nKryoSerializer.addDefaultSerializer(..., ...)\n```\n\nI like this better, and would also change my Pojo Subclass Serializer Pull Request to adhere to this convention. \n\nThe other option (what I currently have in the Pojo Subclass PR) is to have:\n\n```\nenv.addDefaultSerializer(..., ...)\n```\n\nWhat do you think?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/69759167/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/69897397", "body": "The reason I don't like option 2 is that it introduces knowledge about Kryo at the ExecutionEnvironment while it is right now agnostic of types and serialisers. \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/69897397/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/69897430", "body": "But I also see that option 2 is more discoverable.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/69897430/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/70065763", "body": "I added register methods at the ExecutionEnvironment.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/70065763/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/69989899", "body": "I also find it quite helpful to have Serialised repeated in such cases.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/69989899/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/95521509", "body": "I'm against merging this, to me this seems like a lot of code for something that can be achieved using a simple mapper. Also, if we add this, then we should also have a version for streaming, so that would mean adding a DataStreamUtils, or have TupleUtils serve both DataSet and DataStream.\n\nIn general I'm against all the specialised code for dealing with Tuples, such as the aggregation code and the join/projection code.\n\nBut maybe that's just me... What do you think of my comments?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/95521509/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/95566780", "body": "Yes, I think we should start a discussion there. I just wanted to give the reasons for my opinion here.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/95566780/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/102077233", "body": "I think the consensus was that we don't want to have such a method in the DataSet API. We can, however, put a utility for this in flink-contrib. This utility should work for both batch and streaming? Any other opinions? Please correct me if I'm wrong.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/102077233/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/70071567", "body": "Did you do anything besides trying to add all release-0.8 commits to the master?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/70071567/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/71445361", "body": "It's almost the same, except for the change to handle Interfaces and Abstract Classes with GenericTypeInfo, correct?\n\nThe part that changes the KryoSerializer must be adapted because of my recently merged PR that allows registering types and serializers at Kryo.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/71445361/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/70676663", "body": "Why do you want to move them?\nOn Jan 20, 2015 4:42 AM, \"Henry Saputra\" notifications@github.com wrote:\n\n> FYI @aljoscha https://github.com/aljoscha\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/flink/pull/324#issuecomment-70600130.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/70676663/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/72036147", "body": "+1, can you merge it @hsaputra or should I?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/72036147/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/72039131", "body": "+1 looks good to me\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/72039131/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/73077279", "body": "Do you think that with the additional checking logic this would really make up for one superfluous duplication? \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/73077279/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/73079161", "body": "Ok, then I'll add this.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/73079161/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/73501310", "body": "Manually merged.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/73501310/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/72752435", "body": "Nope, sorry, also have no Idea why this is happening.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/72752435/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/72618070", "body": "But I extended the test with a SerializerTest for both Interfaces (traits) and abstract classes. The other thing I will change, yes.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/72618070/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/72642370", "body": "You're right.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/72642370/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/73020223", "body": "I addressed the comments. What do the others think about overloading readFile()? I made it like this on purpose. So that the user sees in the API that they are using Hadoop input formats or that they can be used.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/73020223/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/73039917", "body": "@StephanEwen  If I add the exclusions then users that just add flink-java as a dependency will get weird errors when using Hadoop InputFormats. \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/73039917/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/73040767", "body": "I think if executing it in an IDE the dependencies are not there. Since flink-java does not depend on flink-runtime, which has the hadoop dependencies.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/73040767/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/74505787", "body": "Looks good except for the one remark I had.\n\nDid you run any performance tests on this? What is the overhead of using an Avro POJO vs. a plain POJO vs. using the Kryo Serializer?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/74505787/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/74744132", "body": "What exactly are you running? TPC-H Query 3? Maybe we should test how fast Kryo would be with the PojoComparator.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/74744132/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/49417330", "body": "I might be able to provide that as part of [FLINK-987](https://issues.apache.org/jira/browse/FLINK-987).\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/49417330/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/49990091", "body": "There is still some work to be done. I'm trying to get rid of the lock/unlock.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/49990091/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50257908", "body": "Fixed the typo.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50257908/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50636024", "body": "Maybe it's just me but I don't like the names of the interfaces. Reducible, for example, to me suggests that the thing can be reduced. But the UDF does the reducing and is not reducible.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50636024/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50646910", "body": "I like Mapper best. A \"Map\" is something different in most programming languages. :smile: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50646910/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50741310", "body": "Another possibility would be MapUDF and friends...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50741310/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/100144884", "body": "I think this would not solve our problems. I will start a discussion thread on the dev list.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/100144884/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/100953248", "body": "I think I addressed all the (reasonable) comments you made? The user facing API does not change in any way from this and I tried to pick consistent names for the internal tasks and operators. \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/100953248/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/101663155", "body": "I rebased this PR on top of the latest work on the checkpointing. What are the opinions on merging this?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/101663155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/102333346", "body": "Yes, I was aware of that new test. :smile: The PollingSourceFunction I didn't implement because I think this is orthogonal to this work.\n\nFor the operators, I think it's not a problem because I think they refer to different concepts. The TwoInputOperator is a physical operator that happens to have two inputs. The CoStreamMap is a higher level concept that implements a Map over two connected streams. And it just so happens that this concept can be implemented using the \"physical\" two input operator.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/102333346/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/103530579", "body": "Manually merged\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/103530579/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/101145293", "body": "It does not treat Java classes the same. For some strange reason, the Scala Type analysis in Scala 2.10 can sometimes not \"see\" the fields of classes defined in Java.\n\nThe Scala Type analysis treats Java Tuples as what they are, POJOs. We could maybe add special case handling to make it treat Java tuples as Java tuples.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/101145293/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/101199537", "body": "See here: https://github.com/apache/flink/pull/669\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/101199537/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/100880185", "body": "I like it. But I think it needs some functionality for verifying parameters. To let the user specify some parameters that always need to be there and a description of the parameter. Similar to how other tools print the \"usage\" when you don't give correct arguments.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/100880185/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/100926127", "body": "Yes, something like this.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/100926127/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/102327285", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/102327285/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/100940455", "body": "Thanks for your contribution!\n\nCould you maybe enhance it along these lines: http://stackoverflow.com/questions/6080437/case-insensitive-scala-parser-combinator\n\nBecause right now it would only support as and AS, but not As or aS. I know the latter is a rather academic example, but still...\n\nWould you also be interested in adding SQL style aggregations, for example COUNT(field), MAX(field) and so on? I could open a Jira for this.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/100940455/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/100950435", "body": "I created this issue for the aggregations: https://issues.apache.org/jira/browse/FLINK-2000. I think I can only assign you if you have a Jira account. Do you have one?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/100950435/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/101544907", "body": "I'll check it out.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/101544907/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/101617351", "body": "For some reason it doesn't like the keywork2Parser when you use String Interpolation. If you replace the line with:\n\n```\n    (\"\"\"(i?)\\Q\"\"\" + kw.key + \"\"\"\\E\"\"\").r\n```\n\nit should work.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/101617351/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/102076699", "body": "Can you post a link to the failed run? Or is it on your local machine? Some of the streaming test cases fail sometimes right now, this is a known problem. \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/102076699/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/102374728", "body": "I'm running my own tests on travis and then I'll merge it.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/102374728/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/103036619", "body": "Any comments? Do we want this?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/103036619/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/104182293", "body": "I didn't want to merge it without any comments. But please, go ahead. :smile: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/104182293/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/104207389", "body": "This excludes static fields in Scala Pojo analysis (because static fields should not be serialised/deserializerd), removes legacy code from Scala Type Descriptors and makes Scala Type Analysis work with Java Tuples (because several people complained about them not being supported in the Scala API, most prominently they are needed to port Gelly to Scala.)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/104207389/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/101652722", "body": "But what does restoreInitialState mean? What is the initial state? The state when the operator was first created?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/101652722/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/101654445", "body": "Nah, you can change it of course. :smile_cat: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/101654445/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/109932208", "body": "Why? There is still a naming difference between methods that to similar things at different levels.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/109932208/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/109936539", "body": "No problemo :smile: It's only internal, though, now user-facing.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/109936539/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/138898146", "body": "It's still valid, I think the names are still off\n\n```\npublic abstract class StreamTask<OUT, O extends StreamOperator<OUT>> extends AbstractInvokable implements\n        OperatorStateCarrier<StateHandle<Serializable>>, CheckpointedOperator, CheckpointNotificationOperator {\n```\n\nThe StreamTask implements interfaces that have Operator in their names, and they are inconsistent.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/138898146/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/139238695", "body": "I rebased it and consolidated the state related interfaces into one interface.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/139238695/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/141413305", "body": "ok, I'm merging it\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/141413305/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/105504716", "body": "Do we want to have this in now? I can cherry pick it on top of the latest master and merge it.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/105504716/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/105530009", "body": "Ok, I'm running the last Travis tests and then I'm merging :smile: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/105530009/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/102337633", "body": "I'm running a last check on travis on my branch. Then I'm merging this. Thanks for your work!\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/102337633/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/102452915", "body": "That's because I rebased your commit which changes the signature of the commit. But if you look here https://github.com/apache/flink/commits/master you can see that your commit is among the most recent ones. :smile: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/102452915/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/8201320", "body": "Yes, they are not like Java Iterators. They allow all the usual collection operations. In addition, you can get a buffered iterator and access the first element, as in:\n\n```\nval it: Iterator = // ...\nval buffered = it.buffered\nval first = buffered.head\nfor (v <- buffered) {\n  // iterate over all element, including first\n}\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/8201320/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/10661408", "body": "Dammit, I fixed it in my janino PR\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/10661408/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/16956214", "body": "Done\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/16956214/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/17254978", "body": "Could do, but it does not really impact performance very much.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/17254978/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/18339338", "body": "That seems like a good idea, yes. Still not have the JMXReporter active by default or now have it active by default again?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18339338/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19948322", "body": "I think this won't work because now we don't have all the special serialisation logic that `SimpleStateDescriptor` has. What you could do is override `getDefaultValue()` and return `null` there. Then, in `getInitialValue()` you return the default value and you don't need the extra field.\r\n\r\nWhat do you think?", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19948322/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/20626829", "body": "That's a good point but the `WindowOperator` protects against this by only considering the allowed lateness when the `WindowAssigner` advertises itself as event-time aware: https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperator.java#L615", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/20626829/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/21347344", "body": "This commit didn't change that. But it's true, I'll push a commit that changes it to\r\n```\r\nif (fireTimestamp != null && fireTimestamp == time) {\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/21347344/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/21351102", "body": "Of course \ud83d\ude03 ", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/21351102/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/25902242", "body": "could, but that's not strictly necessary anymore because I also introduced the temporary `git clone`.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/25902242/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/26036557", "body": "sorry about that, I pushed hot fixes", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/26036557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/18446562", "body": "Should get rid of this TODO.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18446562/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18448587", "body": "This seems fishy. When it encounters a CompositeType it will add this CompositeType to the list of flat fields.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18448587/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18451679", "body": "Should still use the Scala example...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18451679/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18451706", "body": "Should also still use the Scala example.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18451706/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29851924", "body": "As I said, forgot to transition the FileMonitoringFunction. :scream: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29851924/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29852045", "body": "I thought Jodatime was the answer to Javas bad support for time. And rolling this yourself can get quite tricky, I think.\n\nPlus, the Google Dataflow API is also using jodatime Instant for their timestamps.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29852045/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29852972", "body": "But the code was like this before my change. Then we should work on getting a proper ExecutionConfig over to runtime.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29852972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29853002", "body": "I'll add a Jira issue.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29853002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29854531", "body": "Changing\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29854531/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29854565", "body": "No, this one is a bit harder since the Flume source has its own loop.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29854565/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29854686", "body": "Leftover, since I started this whole thing to add watermarks. I could leave it in, since in my upcoming work I will use this? :sweat_smile: \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29854686/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29854771", "body": "done\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29854771/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29854951", "body": "But what do you want me to do?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29854951/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29855036", "body": "I think it's fine in streaming. open(), receiveElement(), close() are all called in the inner loop of StreamTask.invoke()\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29855036/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29855135", "body": "Working on it.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29855135/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29855250", "body": "Seems reasonable.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29855250/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29856097", "body": "But then the naming scheme would be rather inconsistent: SingleInputStreamOperator and CoStreamOperator. What is the origin of all these Co\\* prefixes in the user functions, operations and operators?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29856097/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29856312", "body": "This test never tested any actual networking functionality. I actually started a job with a socket source and it still works as it should.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/29856312/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/30580847", "body": "I also think that it is too much. I think you could change just one field and then check at runtime whether this field is actually changed.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/30580847/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/31463699", "body": "This is because I'm waiting since the recent TwitterSource PR is in.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/31463699/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/31911518", "body": "Yes, no idea what I did there. Will change it.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/31911518/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/31911662", "body": "But why wouldn't we allow the user to explicitly set a higher parallelism. The call is quite explicit and I think I would be more surprised as a user if my source did not run in parallel even though I set a higher parallelism. \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/31911662/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/31913451", "body": "We discussed this quickly and came to the solution that we remove method generateParallelSequence from the StreamExecutionEnvironment since the batch API also only has generateSequence which can be used for \"parallelism 1\" sources and parallel sources.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/31913451/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/32103307", "body": "I will look into it. The problem is that the DataStream and all the other classes don't have a reference to the Scala StreamExecutionEnvironment.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/32103307/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/32369781", "body": "Could you please change this back to the simple while loop. I know that using the fancy Scala features is tempting but the performance of a simple while loop should be better than creating a range and iterating over it using foreach.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/32369781/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/32369782", "body": "See above comment about while loop.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/32369782/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/32371124", "body": "I think it would be easier to do\n\n```\ntarget.writeBoolean(source.readBoolean())\n```\n\nhere, instead of going through the extra abstraction of the BooleanSerializer.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/32371124/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/32371128", "body": "I think it would be good to also add a row that has null values, since the change actually introduces that.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/32371128/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/32495592", "body": "Boolean.compare() could be problematic because it is only guaranteed to return a value smaller than 0 or larger then 0 if the two values are not equal.\n\nWhat about?\n\n```\n$resultTpe $resultTerm = ${childCode.resultTerm} != null ? 1 : 0;\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/32495592/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/32495598", "body": "See above.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/32495598/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/33129956", "body": "But it seems to affect the partitioning of the stream since the constructor of KeyedDataStream calls partitionByHash() on the DataStream. (This also applied to the other keyBy() methods)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/33129956/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/33129995", "body": "I don't get what this explanation is trying to say.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/33129995/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/33139196", "body": "But the actual operators are still there.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/33139196/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/33164735", "body": "It's difficult, in a lot of parts of the system we wrap things inside a RuntimeException to let it pass through methods that don't declare thrown Exceptions. (In my opinion the whole checked exceptions business is not a good solution in Java.)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/33164735/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "uce": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/e7ca1055afc46e4a6ccb3492758de36bfd6419a3", "message": "[FLINK-7762] [connector-wikiedits] Make WikipediaEditsSourceTest proper test\n\nThe WikipediaEditsSourceTest unnecessarily implements an integration\ntest that starts a FlinkMiniCluster and executes a small Flink program.\n\nThis simply creates a source and executes run in a separate thread until\na single WikipediaEditEvent is received.\n\nThis closes #5102."}, {"url": "https://api.github.com/repos/apache/flink/commits/ec0c416a69398c9ef3bfd2be51d0dc6677e19f7b", "message": "[FLINK-8167] [connector-wikiedits] Harden WikipediaEditsSource\n\n- Minor eager sanity checks\n- Use UUID suffix for nickname. As reported in FLINK-8167, the current\n  nickname suffix can result in nickname clashes which lead to test\n  failures."}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/45997673", "body": "Yes, the other flags are set as before. The `Xms` and `Xmx` flags are only set, when they are found in `stratosphere-conf.yaml`.\n\nWhile doing the PR, I came to the same conclusion as your second comment. But we can actually also do something more fancy and figure out the available memory ourselves and set it (thereby circumventing the defaults).\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45997673/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/49601334", "body": "I've rebased this PR on the renamed master, but it is just a single commit (not the most elegant way to do this... if someone complains I'll fix it).\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/49601334/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/49739600", "body": "Looks very nice. I had some minor comments and didn't look at the tests yet.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/49739600/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50497817", "body": "I got around to testing this on a 25 node cluster and fixed a small remaining problem. Looks good to me now.\n\nAlthough it uses more coarse-grained locking than the current master, the performance did not suffer in comparison. We should improve it when it becomes a problem.\n\nThe connections are reliably closed after the idle timeout and the debug logs indicate that they have also been closed/opened during execution jobs without problems.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50497817/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50498033", "body": "I've tested this on a 25 node cluster and didn't run into any problems during compiliation or runtime.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50498033/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50507758", "body": "OK, let me merge it then.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50507758/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50507788", "body": "OK, let me merge it in the next batch.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50507788/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50553315", "body": "Great news :-)\n\nI think it's very good that we plan to have the breaking changes with the next release already.\n\nI made a line comment regarding (3).\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50553315/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50589778", "body": "I agree with @StephanEwen that the Iterator makes it more obvious.\n\nStill, I vote for Iterable and the exception in order to support foreach loops.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50589778/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/10656403", "body": "Just skimmed over the scaladoc and found a typo in `join`: t**w**o is missing the w.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/10656403/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/13328780", "body": "In case of an Exception, won't the client receive both a `JobResultFailure` and `JobResultSuccess` afterwards outside of the catch block?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/13328780/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/14621810", "body": "This will lead to having an error message at the end of each standalone job manager log, no? Maybe it's OK...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14621810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/14621819", "body": "Should we merge this to 0.10.2 as well?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14621819/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15678832", "body": "Should we add a checkstyle rule to not allow `junit.framework` imports?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15678832/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15702432", "body": "If you like, I can add a prominent note linking to a Wiki page, where we list all required changes. Would you like to add the Wiki page?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15702432/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15703496", "body": "OK, I've updated the docs.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15703496/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15707254", "body": "The thing is that this requires checkstyle to be enabled for the tests as well. I would like that. Let me ask on the mailing list what the others think...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15707254/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15888699", "body": "Thanks for looking into it.\n\n`ZKPaths.deleteChildren` actually looks like it already handles this. The `KeeperException.NoNodeException` can only happen if the initial path does not exist. Therefore I think that we can skip the retry loop.\n\n``` java\npublic static void deleteChildren(ZooKeeper zookeeper, String path, boolean deleteSelf) throws InterruptedException, KeeperException\n{\n    PathUtils.validatePath(path);\n\n    List<String> children = zookeeper.getChildren(path, null);\n    for ( String child : children )\n    {\n        String fullPath = makePath(path, child);\n        deleteChildren(zookeeper, fullPath, true);\n    }\n\n    if ( deleteSelf )\n    {\n        try\n        {\n            zookeeper.delete(path, -1);\n        }\n        catch ( KeeperException.NotEmptyException e )\n        {\n            //someone has created a new child since we checked ... delete again.\n            deleteChildren(zookeeper, path, true);\n        }\n        catch ( KeeperException.NoNodeException e )\n        {\n            // ignore... someone else has deleted the node it since we checked\n        }\n    }\n}\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15888699/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15927499", "body": "Please consult this page and use the new Maven artifacts (with Scala suffixes): https://cwiki.apache.org/confluence/display/FLINK/Maven+artifact+names+suffixed+with+Scala+version\n\nDoes it resolve the issue?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15927499/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/16235140", "body": "I saw this one failing with the higher timeout last week. Is it OK with you to give it more time (like 60secs)?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/16235140/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/16897467", "body": "Did not add this to `master` yet as the ASF might support `rsync` for `home.apache.org` again until `1.1`...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/16897467/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/16955170", "body": "Can you please cherrypick this to `release-1.0` as well?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/16955170/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/17165776", "body": "Can you push this to `release-1.0` as well to have it in 1.0.3 or the next 1.0.2 RC?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/17165776/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/17254739", "body": "Should we add this to `release-1.0` as well?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/17254739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/17256873", "body": "Can we add this to release-1.0 as well?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/17256873/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/17784955", "body": "Just noticed that `SubtaskState` cleanup only catches and logs exceptions (`TaskForState` before, too). Does anyone recall what the reasoning for this is? Is it OK to remove the catch or rethrow the Exception? When discarding a savepoint without the proper class loader for example, this will only show up in the logs, but the savepoint disposal will be marked as a success.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/17784955/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/20198048", "body": "This commit accidentally removed the newly added docs about Externalized Checkpoints (in `setup/fault_tolerance.md`. @alpinegizmo @StephanEwen Where would you like to move that now?", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/20198048/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/20198072", "body": "OK, just found it in `setup/checkpoints.md`. Perfect place for it \ud83d\udc4d ", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/20198072/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/26179876", "body": "I know that the project historically did not consider the REST API as a public API, but I would vote to note down all of these breaking REST API changes for the upcoming 1.5 release notes in order to have a good migration path for users. I just ran into this when pointing a tool that was using the `jobsoverview` endpoint of 1.4 to the latest master and had to look into what happened when I got a 404.\r\n\r\nIt might also be worth to add redirects in 1.5. and only remove them in the release after that (cc @zentol).", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/26179876/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/14194380", "body": "I would suggest to just move this tuple to the `testSwapValues()` method since you only use it there. I also think the name is rather verbose.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14194380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14288928", "body": "comment doesn't add any value\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14288928/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14288968", "body": "I think this comment is a bit ambiguous... I would say either remove the `@return` or just copy the `Shallow copy of the Tuple2 with swapped values.`.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14288968/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14288993", "body": "Can't you just use JUnit's assertEquals, i.e. `Assert.assertEquals(fullTuple2.f0, swappedTuple2.f1)`?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14288993/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14289113", "body": "I found it confusing that the method is called createTempFile, but returns FileInputSplit.\n\nThis is up to you (aka matter of taste), but I find it clearer if the tests are after each other and the helper methods come at the end.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14289113/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14289124", "body": "Unnecessary empty line\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14289124/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20142322", "body": "trivial: we can do `this.id = checkNotNull(channelID);` directly\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20142322/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20142356", "body": "trivial: comment ends apruptly\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20142356/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "hequn8128": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5327", "title": "[FLINK-8428] [table] Implement stream-stream non-window left outer join", "body": "\r\n## What is the purpose of the change\r\n\r\nImplement stream-stream non-window left outer join for sql/table-api. A simple design doc can be found [here](https://docs.google.com/document/d/1u7bJHeEBP_hFhi8Jm4oT3FqQDOm2pJDqCtq1U1WMHDo/edit?usp=sharing)\r\n\r\n\r\n## Brief change log\r\n\r\n  - Add left join\r\n    - with non-equal predicates\r\n    - without non-equal predicates\r\n  - Adapt retraction rules to left join. Outer join will generate retractions\r\n  - Adapt `UpsertTableSink`. Table mode of dynamic table produced by left join is Update Mode, even if the table does not include a key definition\r\n  - Add inner join test cases which consistent with test cases in batch.\r\n  - Add left join test cases which consistent with test cases in batch.\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n  - Added integration tests for left join with or without non-equal predicates.\r\n  - Added HarnessTests left join with or without non-equal predicates.\r\n  - Add tests for AccMode generate by left join.\r\n  - Add tests for UpsertSink followed left join.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes)\r\n  - If yes, how is the feature documented? (already docs)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5244", "title": "[FLINK-8366] [table] Use Row instead of String as key when process upsert results", "body": "\r\n## What is the purpose of the change\r\n\r\nThis pr fix the bug in `TableSinkITCase.upsertResults()`. In `upsertResults()` function, we use String as key to upsert results. This will make (1,11) and (11,1) have the same key (i.e., 111).\r\n\r\n\r\n## Brief change log\r\n\r\n  - Use Row instead of String to avoid the String problem.\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4674", "title": "[FLINK-7627] [table] SingleElementIterable should implement with Serializable", "body": "\r\n## What is the purpose of the change\r\n\r\n*This pull request is a bugfix which implements `SingleElementIterable` with `Serializable`*\r\n\r\n\r\n## Brief change log\r\n\r\n  - *Implement SingleElementIterable with Serializable*\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n  - *Added test that validates that `SingleElementIterable` can be serialized, otherwise exceptions will be throwed*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no )\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "eastcirclek": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5307", "title": "[FLINK-8431] [mesos] Allow to specify # GPUs for TaskManager in Mesos", "body": "## What is the purpose of the change\r\n\r\nThis PR introduces a new configuration property named \"mesos.resourcemanager.tasks.gpus\"  to allow users to specify # of GPUs for each TaskManager process in Mesos. The configuration property is necessary because TaskManagers that do not specify to use GPUs cannot see GPUs at all when Mesos agents are configured to isolate GPUs as shown in [1].\r\n\r\n[1] http://mesos.apache.org/documentation/latest/gpu-support/#agent-flags\r\n\r\n## Brief change log\r\n\r\n* Modify MesosTaskManagerParameters instead of ContaineredTaskManagerParameters to confine this problem to Mesos\r\n* Augment data types: (1) offers from Mesos and (2) task requests of Mesos frameworks \r\n* Add GPU_RESOURCES to the list of framework capabilities if \"mesos.resourcemanager.tasks.gpus\" > 0. Otherwise, LaunchCoordinator gets no offers from Mesos masters that are configured to prevent Mesos frameworks without GPU_RESOURCES from being given resources offers of GPU-equipped agents.\r\n\r\n## Verifying this change\r\n\r\nI tested it by launching a standalone Flink cluster using ./bin/mesos-appmaster.sh. I tested the following scenarios with Mesos configured with --filter_gpu_resources.\r\n\r\n* When mesos.resourcemanager.tasks.gpus is not specified or is set to 0.0\r\nLaunchCoordinator isn't given any offer because MesosFlinkResourceManager does not enable GPU_RESOURCES capability when mesos.resourcemanager.tasks.gpus is not specified or it is set to 0.\r\n* When mesos.resourcemanager.tasks.gpus is smaller than or equal to the available GPUs on a node \r\nGiven offers, LaunchCoordinator aggregates offers of different roles from the same node and puts aggregated offers to Fenzo for scheduling resources over nodes. When notified of the success of scheduling from Fenzo, LaunchCoordinator allocates resources of different roles to tasks and then populate Protos.TaskInfo using the allocated resources which is then wired to the Mesos master.\r\n* When mesos.resourcemanager.tasks.gpus is bigger than the available GPUs on a node \r\nGiven offers, LaunchCoordinator aggregates offers of different roles from the same node and puts aggregated offers to Fenzo. However, Fenzo notifies LaunchCoordinator of the failure of scheduling with the following messages:\r\n    AssignmentFailure {resource=Other, asking=3.0, used=0.0, available=2.0, message=gpus}.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): yes, it includes an upgrade (Fenzo)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: yes (JobManager and TaskManager on Mesos)\r\n  - The S3 file system connector: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? no\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "suez1224": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5306", "title": "[FLINK-8390][security]remove unused integration test code", "body": "## What is the purpose of the change\r\n\r\nRemove unused integration test code from the main code.\r\n\r\n\r\n## Brief change log\r\n  - Remove unused integration test code\r\n\r\n\r\n## Verifying this change\r\n  - Run RollingSinkSecuredITCase, YARNSessionFIFOSecuredITCase, Kafka09SecuredRunITCase, and all IT cases pass.\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: ( no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)\r\n  - The S3 file system connector: ( no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? ( no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5274", "title": "[FLINK-8401][Cassandra Connector]Refactor CassandraOutputFormat to allow subclass to customize the fai\u2026", "body": "## What is the purpose of the change\r\n\r\nRefactor CassandraOutputFormat to allow subclass to customize the failure handling logic.\r\n\r\n\r\n## Brief change log\r\n  - Added to protected methods for handling cassandra write result, these allow subclass to override them to customize.\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: ( no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: ( no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5272", "title": "[Flink-8397][Connectors]Support Row type for Cassandra OutputFormat", "body": "\r\n## Brief change log\r\n  - Add CassandraOutputFormatBase\r\n  - Add CassandraRowOutputFormat\r\n  - Add CassandraTupleOutputFormat\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified by CassandraConnectorITCase.{testCassandraBatchTupleFormats, testCassandraBatchRowFormats}\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: ( no)\r\n  - The serializers: ( / )\r\n  - The runtime per-record code paths (performance sensitive): (no \r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no \r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes \r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5172", "title": "[FLINK-8275] [Security] fix keytab local path in YarnTaskManagerRunner", "body": "## Brief change log\r\n\r\n  - Set the local keytab path in YarnTaskManagerRunner to the correct local path.\r\n\r\n## Verifying this change\r\n  - Manually verified the change by running in a production secure cluser with 1 JobManager and 2 TaskManagers. Both the JobManager and the Taskmanagers can start, and verify through kerberos metrics.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4724", "title": "[FLINK-7690] [Cluster Management] Do not call actorSystem.awaitTermination from akka's main message han\u2026", "body": "## What is the purpose of the change\r\n\r\n*During shutdown, do not wait for the ActorSystem's termination within the main akka message handling thread as it will block the job from terminating forever.*\r\n\r\n\r\n## Brief change log\r\n\r\n  - *call ActorSystem.awaitTermination from a different thread.*\r\n\r\n\r\n## Verifying this change\r\n\r\n  - *Manually verified the change by running the example streaming WordCount job on local YARN cluster.*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: yes\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? no\r\n  - If yes, how is the feature documented?  N/A\r\n\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nicktoker": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5305", "title": "Release 1.3", "body": "when asyncIO have timeout all operator fail and break the all stream\r\nneed to add option to control when timeout  happen and choose  what to do in this case\r\nignore this stream element and continue to next one ........ or beak the flow\r\none timeout don't need break the stream\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "maqingxiang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5304", "title": "[FLINK-8290]Modify clientId to groupId in flink-connector-kafka-0.8", "body": "Now the Clientid that consumes the all topics are constant(\"flink-kafka-consumer-legacy-\" + broker.id()), and it is not easy for us to look at kafka's log, so I recommend that it be modified to groupid.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shuai-xu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5297", "title": "[FLINK-8434] Take over the running task manager after yarn app master failvoer", "body": "\r\n## What is the purpose of the change\r\n\r\n*This pull request makes the yarn resource manager could take over the running container from previous attempt.*\r\n\r\n## Verifying this change\r\n\r\nThis change is tested manually.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? ( no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5170", "title": "[FLINK-8266] [runtime] add network memory to ResourceProfile for the input and output memory of a task", "body": "\r\n## What is the purpose of the change\r\n\r\nThis pull request adds a network memory field to ResourceProfile. So job master can set the network memory of a task according to the number of input channels and output sub partitions.\r\n\r\n## Brief change log\r\n\r\n*(for example:)*\r\n  - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*\r\n  - *Deployments RPC transmits only the blob storage reference*\r\n  - *TaskManagers retrieve the TaskInfo from the blob cache*\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change can be verified by running ResourceProfileTest:\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicabled)\r\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5139", "title": "[FLINK-8224] [runtime] shutdown application when job terminated in job mode", "body": "## What is the purpose of the change\r\n\r\nThis current job cluster entrypoint doesn't call resource manage to shutdown the application. So resource manger has no change to set the application status to the outer resource management system such as YARN/Mesos. This may make the YARN still consider the application as running even the job is finished.\r\n\r\n## Verifying this change\r\n\r\nThis change is tested manually.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3580", "title": "[FLINK-5791] Support an optimal matching based slot manager for flip6 yarn mode", "body": "This pr is for jira-#[5791](https://issues.apache.org/jira/browse/FLINK-5791)\r\n\r\nMain changes:\r\n1. Add a UNIVERSAL resource profile which can match any other one.\r\n2. TaskExecutor will be initialized with the UNIVERSAL resource profile.\r\n3. Add a Optimal matching based slot manager which match slots to the request with closest resource", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jelmerk": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5296", "title": "[FLINK-8432] [filesystem-connector] Add support for openstack's swift filesystem", "body": "## What is the purpose of the change\r\n\r\nAdd support for OpenStack's cloud storage solution without Hadoop dependencies\r\n\r\n## Brief change log\r\n\r\n- Added new module below flink-filesystems\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n- Added integration tests for simple reading and writing and listing directories\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: yes\r\n  - The serializers: don't know\r\n  - The runtime per-record code paths (performance sensitive): don't know\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: don't know\r\n  - The S3 file system connector: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? yes\r\n  - If yes, how is the feature documented? not documented\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dyanarose": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5295", "title": "[FLINK-8384] [streaming] Session Window Assigner with Dynamic Gaps", "body": "## What is the purpose of the change\r\n\r\nThis PR adds the ability for the Session Window assigners to to have dynamic inactivity gaps in addition to the existing static inactivity gaps.\r\n\r\n**Behaviour of dynamic gaps within existing sessions:**\r\n- scenario 1 - the new timeout is prior to the old timeout. The old timeout (the furthest in the future) is respected.\r\n- scenario 2 - the new timeout is after the old timeout. The new timeout is respected.\r\n- scenario 3 - a session is in flight, a new timeout is calculated, however no new events arrive for that session after the new timeout is calculated. This session will not have its timeout changed\r\n\r\n\r\n## Brief change log\r\n\r\n**What's New**\r\n-  SessionWindowTimeGapExtractor\\<T\\> - Generic Interface with one extract method that returns the time gap\r\n- DynamicEventTimeSessionWindows\\<T\\> - Generic event time session window\r\n- DynamicProcessingTimeSessionWindows\\<T\\> - Generic processing time session window\r\n- TypedEventTimeTrigger\\<T\\> - Generic event time trigger\r\n- TypedProcessingTimeTrigger\\<T\\> - Generic processing time trigger\r\n- Tests for all the above\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n - added tests for the typed triggers that duplicate the existing trigger tests to prove parity\r\n - added unit tests for the dynamic session window assigners that mimic the existing static session window assigner tests to prove parity in the static case\r\n - added tests to the WindowOperatorTest class to prove the behaviour of changing inactivity gaps\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no, though the two typed trigger classes are marked `@Public(Evolving)`)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes)\r\n  - If yes, how is the feature documented? (docs && JavaDocs)\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ChrisChinchilla": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5277", "title": "[hotfix][docs]Review to reduce passive voice, improve grammar and formatting", "body": "Small review of the runtime concept doc to reduce passive voice, reduce future tense, improve grammar and formatting. Let me know if it needs backporting to any other branch or if there are any other issues", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5045", "title": "[hotfix][docs] Review of concepts docs for grammar and clarity", "body": "Spending some time doing a brief review of a few docs sections, just a start and I think more could be done.\r\n\r\nI didn't change formatting or line breaks as much as I would like to as they're quite messy and inconsistent, but I'll save that for another time.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5024", "title": "[hotfix][docs] Readme review to clarify heading formatting", "body": "I clarified how to format a heading casing wise.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4847", "title": "[hotfix][doc]CEP docs review to remove weasel words, fix passive voice, typos\u2026", "body": "As requested, backport from 1.4.\r\n\r\nRef - https://github.com/apache/flink/pull/4816", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/336456854", "body": "@StephanEwen I noticed that just as I submitted it, will look next week :)", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/336456854/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/345096459", "body": "Hmm, not sure what happened to the commit history there. Let me know if you like my changes but need something cleaner and I'd try and do some git magic\u2026", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/345096459/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/346191578", "body": "@greghogan Will start this PR afresh.", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/346191578/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/346903075", "body": "@greghogan @zentol I will get these updates to you asap.\r\n\r\nBut I will have to strongly disagree on the 'you' and passive voice points. I've been working as a technical writer for some time, and this a point we're all fairly strong on pushing as in the long run it does make documents much clearer, less ambiguous and easier to read.\r\n\r\nGranted a LOT of the Flink documentation doesn't follow this style right now, and that's something I personally would love to change over time, as I really think it would help people understand better.\r\n\r\nIdeally I would love to create a more comprehensive style guide and explanation of this (and I have boilerplates I've used with other projects) and change this voicing over time.\r\n\r\nThoughts?", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/346903075/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/346903743", "body": "@greghogan That said, the whole 'we' vs 'you' discussion is purely a stylistic preference (if that's what you meant) as long as it's consistent and clear who you mean, but the comment about removing passive voice remains :)", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/346903743/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/347387413", "body": "Changes pushed.", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/347387413/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "paulzwu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5247", "title": "[FLINK-8356] Need to add the commit in flush() in JDBCOutputFormat", "body": "*Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*\r\n\r\n*Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*\r\n\r\n## Contribution Checklist\r\n\r\n  - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.\r\n  \r\n  - Name the pull request in the form \"[FLINK-XXXX] [component] Title of the pull request\", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.\r\n  Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.\r\n\r\n  - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.\r\n  \r\n  - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).\r\n\r\n  - Each pull request should address only one issue, not mix up code from multiple issues.\r\n  \r\n  - Each commit in the pull request has a meaningful commit message (including the JIRA id)\r\n\r\n  - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.\r\n\r\n\r\n**(The sections below can be removed for hotfixes of typos)**\r\n\r\n## What is the purpose of the change\r\n\r\n*(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*\r\n\r\n\r\n## Brief change log\r\n\r\n*(for example:)*\r\n  - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*\r\n  - *Deployments RPC transmits only the blob storage reference*\r\n  - *TaskManagers retrieve the TaskInfo from the blob cache*\r\n\r\n\r\n## Verifying this change\r\n\r\n*(Please pick either of the following options)*\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n*(or)*\r\n\r\nThis change is already covered by existing tests, such as *(please describe tests)*.\r\n\r\n*(or)*\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n*(example:)*\r\n  - *Added integration tests for end-to-end deployment with large payloads (100MB)*\r\n  - *Extended integration test for recovery after master (JobManager) failure*\r\n  - *Added test that validates that TaskInfo is transferred only once across recoveries*\r\n  - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)\r\n  - The serializers: (yes / no / don't know)\r\n  - The runtime per-record code paths (performance sensitive): (yes / no / don't know)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)\r\n  - The S3 file system connector: (yes / no / don't know)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes / no)\r\n  - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tygrash": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5235", "title": "[FLINK-6206] [runtime] Changed log level to WARN for task state transition log", "body": "#### What is the purpose of the change\r\nChanged log level to WARN for task state transition log\r\n\r\n#### Verifying this change\r\nTrivial change\r\n  \r\n  ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zjureel": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5209", "title": "[FLINK-7711] Port JarListHandler to WebMonitorEndpoint", "body": "## What is the purpose of the change\r\n\r\nPort JarListHandler to WebMonitorEndpoint\r\n\r\n## Brief change log\r\n\r\n  - *Create JarWithProgramUtils from PackageProgram to check jar file*\r\n  - *Create new JarListHandler and JarListInfo*\r\n  - *Add uploadDir to RestHandlerConfiguration*\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n  - *Added test case JarListInfoTest*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5149", "title": "[FLINK-7858][flp6] Port JobVertexTaskManagersHandler to REST endpoint", "body": "## What is the purpose of the change\r\n\r\nPort JobVertexTaskManagersHandler to REST endpoint\r\n\r\n## Brief change log\r\n\r\n  - *Add JobVertexTaskManagersHandler class*\r\n  - *Add JobVertexTaskManagersInfo and related class*\r\n\r\n## Verifying this change\r\nThis change added tests and can be verified as follows:\r\n\r\n  - *Added test case JobVertexTaskManagersInfoTest*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5035", "title": "[FLINK-7857][flp6] Port JobVertexDetails to REST endpoint", "body": "\r\n## What is the purpose of the change\r\n\r\nPort JobVertexDetails to REST endpoint\r\n\r\n## Brief change log\r\n  - *Add JobVertexDetailsHandler*\r\n  - *Add JobVertexDetailsInfo and related class*\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n  - *Added test case JobVertexDetailsInfoTest*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4893", "title": "[FLINK-7856][flip6] Port JobVertexBackPressureHandler to REST endpoint", "body": "## What is the purpose of the change\r\n\r\nPort JobVertexBackPressureHandler to REST endpoint\r\n\r\n## Brief change log\r\n\r\n  - *Add JobVertexBackPressureInfo class to describe the json format response*\r\n  - *Add JobVertexBackPressureHandler to deal with back pressure in rest server*\r\n\r\n\r\n## Verifying this change\r\nThis change added tests and can be verified as follows:\r\n\r\n  - *Added test case JobVertexBackPressureInfoTest*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4675", "title": "[FLINK-7386] FIx Elasticsearch 5 connector is not compatible with Elasticsearch 5.2+ client", "body": "## What is the purpose of the change\r\n\r\nAdd flink-connector-elasticsearch5.3 to support Elasticsearch 5.3 and later version\r\n\r\n## Brief change log\r\n  - *Add createRequestIndex method in ElasticsearchApiCallBridge*\r\n  - *Add flink-connector-elasticsearch5.3 project*\r\n  - *Add BulkProcessorIndexer in connector-elasticsearch5.3 to convert ActionRequest to DocWriteRequest*\r\n\r\n## Verifying this change\r\nThis change added tests and can be verified as follows:\r\n  - *Add ElasticsearchSinkITCase test case*\r\n  - *Add ELasticsearchSinkExample in connector-elasticsearch5.3 to send request to Elasticsearch 5.3 and later versions*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes)\r\n  - If yes, how is the feature documented? (docs)\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4621", "title": "[FLINK-7495] Call to AbstractUdfStreamOperator#initializeState() in the beginning", "body": "## What is the purpose of the change\r\n\r\nCall to AbstractUdfStreamOperator#initializeState() in the beginning\r\n\r\n## Brief change log\r\n\r\n  - *Call to AbstractUdfStreamOperator#initializeState() in the beginning*\r\n\r\n\r\n## Verifying this change\r\n\r\nNo need test case\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (don't know)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4476", "title": "[FLINK-7307] Add proper command line parsing tool to ClusterEntrypoint", "body": "## What is the purpose of the change\r\n\r\nAdd a proper command line parsing tool `CommandLineParser` to the entry point of the `ClusterEntrypoint#parseArguments`\r\n\r\n\r\n## Brief change log\r\n\r\n  - *Add command line parsing tool `CommandLineParser`*\r\n  - *Use `CommandLineParser` instead of `ParameterTool` in `ClusterEntrypoint#parseArguments`*\r\n\r\n\r\n## Verifying this change\r\n\r\n*(Please pick either of the following options)*\r\nThis change added tests and can be verified as follows:\r\n\r\n*(example:)*\r\n  - *Added test case `CommandLineParserTest`*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4376", "title": "[FLINK-6521] Add per job cleanup methods to HighAvailabilityServices", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [ ] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4204", "title": "[FLINK-6522] Add ZooKeeper cleanup logic to ZooKeeperHaServices", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [ ] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3915", "title": "[FLINK-6352] Support to use timestamp to set the initial offset of kafka", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [ ] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dubin555": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5202", "title": "[FLINK-8302][table]Add SHIFT_LEFT and SHIFT_RIGHT supported in SQL", "body": "In this PR, SHIFT_LEFT and SHIFT_RIGHT are added.\r\n\r\n## General\r\nThe pull request references the related JIRA issue, [FLINK-8302]Support shift_left and shift_right in TableAPI\r\nThe pull request addresses only one issue\r\nEach commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n## Tests & Build\r\nFunctionality added by the pull request is covered by tests\r\n\r\n## Documentation\r\nDoes this pull request introduce a new feature? (yes )\r\nIf yes, how is the feature documented? (docs / JavaDocs)", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "je-ik": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5185", "title": "[FLINK-8297] [flink-rocksdb] optionally use RocksDBMapState internally for storing lists", "body": "## What is the purpose of the change\r\n\r\nEnable storing lists not fitting to memory per single key.\r\n\r\n## Brief change log\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n  passes additional tests for RocksDBStateBackend.enableLargeListsPerKey()\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: yes\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no, backward compatible\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\r\n  - The S3 file system connector: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? yes\r\n  - If yes, how is the feature documented? JavaDocs\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "packet23": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5145", "title": "[FLINK-8230] [ORC] Improved mapping of Orc records to Flink Table rows.", "body": "## What is the purpose of the change\r\n\r\nImproved mapping of Orc records to Flink table rows and ensured nested structs are correctly mapped.\r\n\r\n## Brief change log\r\n\r\n  - Introduced OrcRowReader to be used on a single input split\r\n  - OrcRowReader was adapted from org.apache.orc.mapred.OrcMapredRecordReader\r\n  - Added a bunch of test datasets, mostly from orc distribution\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n  - OrcRowReaderTest maps test datasets to rows comparing count\r\n  - OrcRowReader is also covered by existing flink-orc unit tests.\r\n  - Manually verified the change by running a 4 node cluster on YARN, reading an existing full dataset\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: don't know\r\n  - The runtime per-record code paths (performance sensitive): yes\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: don't know\r\n  - The S3 file system connector: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? no\r\n  - If yes, how is the feature documented? not applicable\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4606", "title": "[FLINK-7539] Made AvroOutputFormat default codec configurable.", "body": "Implement site-wide codec defaults.\r\n\r\n## Brief change log\r\n\r\n- Added connector.avro.output.codec option\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: yes\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? yes\r\n  - If yes, how is the feature documented? docs and JavaDocs", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sihuazhou": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5074", "title": "[FLINK-7873] [runtime] Introduce local recovery", "body": "## What is the purpose of the change\r\n\r\nThis PR fixes [FLINK-7873](https://issues.apache.org/jira/browse/FLINK-7873). Current recover strategy will always read checkpoint data from remote FileStream (HDFS). This will cost a lot of bandwidth when the state is so big (e.g. 1T). What's worse, if this job performs recover again and again, it can eat up all network bandwidth and do a huge hurt to cluster. So, I proposed that we can cache the checkpoint data locally, and read checkpoint data from local cache as well as we can, we read the data from remote only if we fail locally. The advantage is that if a execution is assigned to the same TaskManager as before, it can save a lot of network, and obtain a faster recovery.\r\n\r\n## Brief change log\r\n\r\n  - *Add CheckpointCacheManager for TM to manage Local Checkpoint Data for each TM*\r\n  - *Add CheckpointCache for Task to manage Local Checkpoint Data for each Task*\r\n  - *Add CachedCheckpointStreamFactory to write checkpoint data to both DFS and local disk*\r\n  - *Add CachedStreamStateHandle to read checkpoint data from local or remote*\r\n  - Here is a doc for detail: [local_recovery.docx](https://docs.google.com/document/d/1-yZvTNV6_Nx1XUh3zwAFZqGgF2nkXckTGJy4tx-WzVQ/edit?usp=sharing)\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n- Add tests in `CheckpointCacheManagerTest.java`, `CheckpointCacheTest.java`, `CachedCheckpointStreamFactoryTest.java`, `SharedCacheRegistryTest.java`.\r\n- Compile this PR and deploy it on a cluster, trigger failure randomly. (I tested this on a yarn cluster and with `a naive Scheduler mechanism` that allocates slot only according to state.)\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (Yes)\r\n  - doc link :[local recovery](https://docs.google.com/document/d/1-yZvTNV6_Nx1XUh3zwAFZqGgF2nkXckTGJy4tx-WzVQ/edit?usp=sharing)", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4979", "title": "RMQSource support disabling queue declaration", "body": "## What is the purpose of the change\r\n\r\nThis PR fixs [FLINK-8018](https://issues.apache.org/jira/browse/FLINK-8018), RabbitMQ connector should support disabling the call of queueDeclare or not, in case that user does not have sufficient authority to declare the queue.\r\n\r\n## Brief change log\r\n\r\n  - *Add queueDeclaration in RMQConnectionConfig to support enable or disable queue declaration, the default value is true*\r\n\r\n## Verifying this change\r\n\r\nThis is a trivial change.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n  - Does this pull request introduce a new feature? (no)", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4949", "title": "[FLINK-7866] [runtime] Weigh list of preferred locations for scheduling", "body": "## What is the purpose of the change\r\n\r\nThis PR fixs [FLINK-7866](https://issues.apache.org/jira/browse/FLINK-7866). Currently, scheduler only use the list of preferred locations to decide where to schedule a task, this can be optimized by weigh the locations. That way, we would obtain better locality in some cases, moreover this PR also introduce `CandidateLocation` and `CandidateLocationEvaluator` to enable us to weigh location for `ExecutionVertex` by both state and input.\r\n\r\nA simple weigh example:\r\n- Preferred locations list: {{[location1, location2, location2]}}\r\n- Weighted preferred locations list {{[(location2 , 2), (location1, 1)]}}\r\n\r\n\r\n## Brief change log\r\n\r\n  - *Add CandidateLocation to represent a possible preferred location*\r\n  - *Add CandidateLocationEvaluator to evaluate a candidate location, currently there are only INPUT_ONLY and STATE_ONLY evaluator, but this can easily be extended*\r\n  - *add evaluation logic when allocate slot for `Execution`, it first gets a set of candidate locations, which are then measured by the evaluator, finally, return a location list that order by the weighted result desc*\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n- add `testCandidateLocationEvaluateResult` test in `ExecutionTest` to make sure the evaluate logic.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "taizilongxu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5050", "title": "[FLINK-4822] Ensure that the Kafka 0.8 connector is compatible with k\u2026", "body": "\r\n## What is the purpose of the change\r\n\r\nWhen we deploy the taskmanager in docker of our cluster, it's hard to locate which taskmanager cosnume the right partition of kafka except looking up the  log in docker, so I just add the owner in zk path when PeriodOffsetCommitter the offset.\r\n\r\n\r\n## Brief change log\r\n\r\n  - add the registerPartitionOwnership  when  commit the offset to zookeeper, and store the info like :  /consumers/[group_id]/owner/[topic]/[partition_id] \r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency):  no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`:no\r\n  - The serializers:  no \r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\r\n  - The S3 file system connector: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? no\r\n  - If yes, how is the feature documented? not documented\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/24300072", "body": "I thinks the code is wrong in scala, it's not work, is there any one to fix it?", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/24300072/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "ifndef-SleePy": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5048", "title": "[Flink-7871] [flip6] SlotPool should release unused slots to RM", "body": "## What is the purpose of the change\r\n\r\n* This pull request makes SlotPool release unused slots back to RM if the slots is idle for some time\r\n\r\n## Brief change log\r\n\r\n* Add a TimerService in SlotPool\r\n* Each available slot will register a timer in TimerService, and will unregister when the slot is removed or becomes allocated\r\n* If timeout happened, the slot will be released by notifying TM\r\n* Add notifySlotUnused method in TaskExecutorGateway\r\n* Make some fixed params configurable in SlotPool\r\n\r\n## Verifying this change\r\n\r\nThis change added tests in AvailableSlotsTest\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no, but add a new `@Public(Evolving)` class SlotOptions)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3296", "title": "[FLINK-5784] Fix bug about registering JobManagerLeaderListener in LeaderRetrievalService multiple times", "body": "[FLINK-5784] Fix bug about registering JobManagerLeaderListener in LeaderRetrievalService multiple times.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lincoln-lil": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5046", "title": "[FLINK-6101] [table] Support select GroupBy fields with arithmetic ex\u2026", "body": "## What is the purpose of the change\r\n\r\nSupport select GroupBy fields with arithmetic expressions(include UDF)\r\n\r\n## Brief change log\r\n- Using an internal alias for groupBy fields with arithmetic expressions(include UDF) in groupBy().\r\n\r\n## Verifying this change\r\n`AggregateITCase` verifies\r\n- select GroupBy fields with arithmetic expression/UDF\r\n  \r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "docete": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5009", "title": "[FLINK-8045] Add Internal DATE/TIME/TIMESTAMP as internal representation of DATE/TIME/TIMESTAMP", "body": "## Purpose of this PR\r\nAdd internal representation for DATE/TIME/TIMESTAMP, and allow transferring internal type between operators.\r\n\r\n\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/277609236", "body": "@fhueske Yes, I have checked the execution plan. It's very similar to your description:\r\n\r\nTake example for SQL \"select sum(distinct a), sum(distinct b), sum(c) from expr\", where expr is a table, and it has 3 fields: a, b, c.\r\n\r\nThe explaination for the query is:\r\n```\r\n== Abstract Syntax Tree ==\r\nLogicalAggregate(group=[{}], EXPR$0=[SUM(DISTINCT $0)], EXPR$1=[SUM(DISTINCT $1)], EXPR$2=[SUM($2)])\r\n  LogicalTableScan(table=[[expr]])\r\n\r\n== Optimized Logical Plan ==\r\nDataSetCalc(select=[EXPR$0, EXPR$1, EXPR$2])\r\n  DataSetSingleRowJoin(where=[true], join=[EXPR$2, EXPR$0, EXPR$1], joinType=[NestedLoopJoin])\r\n    DataSetSingleRowJoin(where=[true], join=[EXPR$2, EXPR$0], joinType=[NestedLoopJoin])\r\n      DataSetAggregate(select=[SUM(c) AS EXPR$2])\r\n        DataSetUnion(union=[a, b, c])\r\n          DataSetValues(tuples=[[{ null, null, null }]], values=[a, b, c])\r\n          DataSetScan(table=[[_DataSetTable_0]])\r\n      DataSetAggregate(select=[SUM(a) AS EXPR$0])\r\n        DataSetUnion(union=[a])\r\n          DataSetValues(tuples=[[{ null }]], values=[a])\r\n          DataSetAggregate(groupBy=[a], select=[a])\r\n            DataSetCalc(select=[a])\r\n              DataSetScan(table=[[_DataSetTable_0]])\r\n    DataSetAggregate(select=[SUM(b) AS EXPR$1])\r\n      DataSetUnion(union=[b])\r\n        DataSetValues(tuples=[[{ null }]], values=[b])\r\n        DataSetAggregate(groupBy=[b], select=[b])\r\n          DataSetCalc(select=[b])\r\n            DataSetScan(table=[[_DataSetTable_0]])\r\n\r\n== Physical Execution Plan ==\r\nStage 8 : Data Source\r\n\tcontent : collect elements with CollectionInputFormat\r\n\tPartitioning : RANDOM_PARTITIONED\r\n\r\nStage 14 : Data Source\r\n\tcontent : collect elements with CollectionInputFormat\r\n\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\tStage 13 : Map\r\n\t\tcontent : from: (a, b, c)\r\n\t\tship_strategy : Forward\r\n\t\texchange_mode : BATCH\r\n\t\tdriver_strategy : Map\r\n\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\tStage 12 : FlatMap\r\n\t\t\tcontent : select: (a)\r\n\t\t\tship_strategy : Forward\r\n\t\t\texchange_mode : PIPELINED\r\n\t\t\tdriver_strategy : FlatMap\r\n\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\tStage 11 : Map\r\n\t\t\t\tcontent : prepare select: (a)\r\n\t\t\t\tship_strategy : Forward\r\n\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\tdriver_strategy : Map\r\n\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\t\tStage 10 : GroupCombine\r\n\t\t\t\t\tcontent : groupBy: (a), select: (a)\r\n\t\t\t\t\tship_strategy : Forward\r\n\t\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\t\tdriver_strategy : Sorted Combine\r\n\t\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\t\t\tStage 9 : GroupReduce\r\n\t\t\t\t\t\tcontent : groupBy: (a), select: (a)\r\n\t\t\t\t\t\tship_strategy : Hash Partition on [0]\r\n\t\t\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\t\t\tdriver_strategy : Sorted Group Reduce\r\n\t\t\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\t\t\t\tStage 7 : Union\r\n\t\t\t\t\t\t\tcontent : \r\n\t\t\t\t\t\t\tship_strategy : Redistribute\r\n\t\t\t\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\t\t\t\t\tStage 6 : Map\r\n\t\t\t\t\t\t\t\tcontent : prepare select: (SUM(a) AS EXPR$0)\r\n\t\t\t\t\t\t\t\tship_strategy : Forward\r\n\t\t\t\t\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\t\t\t\t\tdriver_strategy : Map\r\n\t\t\t\t\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\t\t\t\t\t\tStage 5 : GroupCombine\r\n\t\t\t\t\t\t\t\t\tcontent : select:(SUM(a) AS EXPR$0)\r\n\t\t\t\t\t\t\t\t\tship_strategy : Forward\r\n\t\t\t\t\t\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\t\t\t\t\t\tdriver_strategy : Group Reduce All\r\n\t\t\t\t\t\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\t\t\t\t\t\t\tStage 4 : GroupReduce\r\n\t\t\t\t\t\t\t\t\t\tcontent : select:(SUM(a) AS EXPR$0)\r\n\t\t\t\t\t\t\t\t\t\tship_strategy : Redistribute\r\n\t\t\t\t\t\t\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\t\t\t\t\t\t\tdriver_strategy : Group Reduce All\r\n\t\t\t\t\t\t\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\nStage 19 : Data Source\r\n\tcontent : collect elements with CollectionInputFormat\r\n\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\tStage 20 : Map\r\n\t\tcontent : from: (a, b, c)\r\n\t\tship_strategy : Forward\r\n\t\texchange_mode : BATCH\r\n\t\tdriver_strategy : Map\r\n\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\tStage 18 : Union\r\n\t\t\tcontent : \r\n\t\t\tship_strategy : Redistribute\r\n\t\t\texchange_mode : PIPELINED\r\n\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\tStage 17 : Map\r\n\t\t\t\tcontent : prepare select: (SUM(c) AS EXPR$2)\r\n\t\t\t\tship_strategy : Forward\r\n\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\tdriver_strategy : Map\r\n\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\t\tStage 16 : GroupCombine\r\n\t\t\t\t\tcontent : select:(SUM(c) AS EXPR$2)\r\n\t\t\t\t\tship_strategy : Forward\r\n\t\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\t\tdriver_strategy : Group Reduce All\r\n\t\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\t\t\tStage 15 : GroupReduce\r\n\t\t\t\t\t\tcontent : select:(SUM(c) AS EXPR$2)\r\n\t\t\t\t\t\tship_strategy : Redistribute\r\n\t\t\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\t\t\tdriver_strategy : Group Reduce All\r\n\t\t\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\t\t\t\tStage 3 : FlatMap\r\n\t\t\t\t\t\t\tcontent : where: (true), join: (EXPR$2, EXPR$0)\r\n\t\t\t\t\t\t\tship_strategy : Forward\r\n\t\t\t\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\t\t\t\tdriver_strategy : FlatMap\r\n\t\t\t\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\nStage 25 : Data Source\r\n\tcontent : collect elements with CollectionInputFormat\r\n\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\tStage 30 : Map\r\n\t\tcontent : from: (a, b, c)\r\n\t\tship_strategy : Forward\r\n\t\texchange_mode : BATCH\r\n\t\tdriver_strategy : Map\r\n\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\tStage 29 : FlatMap\r\n\t\t\tcontent : select: (b)\r\n\t\t\tship_strategy : Forward\r\n\t\t\texchange_mode : PIPELINED\r\n\t\t\tdriver_strategy : FlatMap\r\n\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\tStage 28 : Map\r\n\t\t\t\tcontent : prepare select: (b)\r\n\t\t\t\tship_strategy : Forward\r\n\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\tdriver_strategy : Map\r\n\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\t\tStage 27 : GroupCombine\r\n\t\t\t\t\tcontent : groupBy: (b), select: (b)\r\n\t\t\t\t\tship_strategy : Forward\r\n\t\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\t\tdriver_strategy : Sorted Combine\r\n\t\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\t\t\tStage 26 : GroupReduce\r\n\t\t\t\t\t\tcontent : groupBy: (b), select: (b)\r\n\t\t\t\t\t\tship_strategy : Hash Partition on [0]\r\n\t\t\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\t\t\tdriver_strategy : Sorted Group Reduce\r\n\t\t\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\t\t\t\tStage 24 : Union\r\n\t\t\t\t\t\t\tcontent : \r\n\t\t\t\t\t\t\tship_strategy : Redistribute\r\n\t\t\t\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\t\t\t\t\tStage 23 : Map\r\n\t\t\t\t\t\t\t\tcontent : prepare select: (SUM(b) AS EXPR$1)\r\n\t\t\t\t\t\t\t\tship_strategy : Forward\r\n\t\t\t\t\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\t\t\t\t\tdriver_strategy : Map\r\n\t\t\t\t\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\t\t\t\t\t\tStage 22 : GroupCombine\r\n\t\t\t\t\t\t\t\t\tcontent : select:(SUM(b) AS EXPR$1)\r\n\t\t\t\t\t\t\t\t\tship_strategy : Forward\r\n\t\t\t\t\t\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\t\t\t\t\t\tdriver_strategy : Group Reduce All\r\n\t\t\t\t\t\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\t\t\t\t\t\t\tStage 21 : GroupReduce\r\n\t\t\t\t\t\t\t\t\t\tcontent : select:(SUM(b) AS EXPR$1)\r\n\t\t\t\t\t\t\t\t\t\tship_strategy : Redistribute\r\n\t\t\t\t\t\t\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\t\t\t\t\t\t\tdriver_strategy : Group Reduce All\r\n\t\t\t\t\t\t\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\t\t\t\t\t\t\t\tStage 2 : FlatMap\r\n\t\t\t\t\t\t\t\t\t\t\tcontent : where: (true), join: (EXPR$2, EXPR$0, EXPR$1)\r\n\t\t\t\t\t\t\t\t\t\t\tship_strategy : Forward\r\n\t\t\t\t\t\t\t\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\t\t\t\t\t\t\t\tdriver_strategy : FlatMap\r\n\t\t\t\t\t\t\t\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\t\t\t\t\t\t\t\t\tStage 1 : FlatMap\r\n\t\t\t\t\t\t\t\t\t\t\t\tcontent : select: (EXPR$0, EXPR$1, EXPR$2)\r\n\t\t\t\t\t\t\t\t\t\t\t\tship_strategy : Forward\r\n\t\t\t\t\t\t\t\t\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\t\t\t\t\t\t\t\t\tdriver_strategy : FlatMap\r\n\t\t\t\t\t\t\t\t\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n\t\t\t\t\t\t\t\t\t\t\t\tStage 0 : Data Sink\r\n\t\t\t\t\t\t\t\t\t\t\t\t\tcontent : org.apache.flink.api.java.io.DiscardingOutputFormat\r\n\t\t\t\t\t\t\t\t\t\t\t\t\tship_strategy : Forward\r\n\t\t\t\t\t\t\t\t\t\t\t\t\texchange_mode : PIPELINED\r\n\t\t\t\t\t\t\t\t\t\t\t\t\tPartitioning : RANDOM_PARTITIONED\r\n\r\n```", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/277609236/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/277910114", "body": "HI @fhueske ,\r\nAgree. If we have more than one distinct agg with groupings, do the partition first and reuse the subsets would improve the performance. \r\nCould we merge this PR first and create another JIRA to track the grouping cases? \r\nWe need a workaround to support distinct aggs ASAP.", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/277910114/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/282567705", "body": "@fhueske Fixed according to your comments", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/282567705/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/288664327", "body": "@wuchong @haohui Could u help to check FLINK-6173 ?", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/288664327/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "juanmirocks": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4997", "title": "[FLINK-8046] [flink-streaming-java] Have filter of timestamp compare with strictly SMALLER (NOT smaller or equal)", "body": "## What is the purpose of the change\r\n\r\nThis change fixes the wrong ignoring of files with same exact timestamp. This change also matches the doc header of the method (`shouldIgnore`): \"...if the modification time of the file is smaller than...\".\r\n\r\nWithout this change, some files with same exact timestamp (because they were written at the same exact long time) will be ignored, which is unexpected by the user.\r\n\r\nAlso you would find the funny log of `Ignoring file:/XXX, with mod time= 1510321363000 and global mod time= 1510321363000`\r\n\r\n## Brief change log\r\n\r\n* Comparison is done with strictly SMALLER (<)\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\r\n  - The S3 file system connector: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? no\r\n  - If yes, how is the feature documented? JavaDocs", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "KurtYoung": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4940", "title": "[FLINK-7959] [table] Split CodeGenerator into CodeGeneratorContext and ExprCodeGenerator.", "body": "\r\n## What is the purpose of the change\r\n\r\nSplit current `CodeGenerator` into two dedicated classes, `CodeGeneratorContext` and `ExprCodeGenerator`. The `CodeGeneratorContext` is responsible for maintaining various reusable statements that could be insert into the final generated class, while the `ExprCodeGenerator` is responsible for generating codes for `RexNode` and generating result convertion codes.\r\n\r\n## Brief change log\r\n\r\n  - *Remove `CodeGenerator` and introduce `ExprCodeGenerator` and `CodeGeneratorContext`*\r\n  - *Make some code generator static*\r\n\r\n## Verifying this change\r\n\r\n*(Please pick either of the following options)*\r\n\r\nThis change is already covered by existing tests.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no )\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Aitozi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4935", "title": "[Flink-7945][Metrics&connector]Fix per partition-lag metric lost in kafka connector ", "body": "## What is the purpose of the change\r\n\r\n*When used KafkaConnector, we cant get per partition lag metric. But it has been exposed after kafka 0.10.2 [https://issues.apache.org/jira/browse/KAFKA-4381](url). After read the kafka code, i found that the per partition lag is register after `KafkaConsumer#poll` method be invoked, so i change the metric register time in flink , and after this, with kafka-connector10 and kafka-connector11 we can see the correct lag metric. *\r\n\r\n## Brief change log\r\n\r\n  - *Change the kafka metric register time in Flink kafka-connector*\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change is already run through the test case\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (yes)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "godfreyhe": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4820", "title": "[FLINK-7834] [table] cost model extends network cost and memory cost", "body": "## What is the purpose of the change\r\n\r\n*This pull request makes cost model (RelOptCost) extends network cost and memory cost*\r\n\r\n\r\n## Brief change log\r\n\r\n  - *The FlinkCost extends RelOptCost and adds network cost and memory cost*\r\n  - *Updates DataSetCost*\r\n  - *Adds getFlinkCostFactory method in TableEnvironment to get  specific FlinkCostFactory*\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes)\r\n  - If yes, how is the feature documented? (JavaDocs)\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4682", "title": "[FLINK-7620] [table] Supports user defined optimization phase", "body": "## What is the purpose of the change\r\n\r\n*This pull request makes Flink supports user defined optimization phase. before this PR, the optimization phases are hardcode in BatchTableEnvironment and StreamTableEnvironment, It's hard to add a new optimization phase or adjust the phase-orders. This PR makes it more flexible for developer and user.*\r\n\r\n\r\n## Brief change log\r\n\r\n  - *Adds new interface FlinkOptimizeProgram which represents an optimization phase to transform a relational expression into another relational expression*\r\n  - *Moves the optimization phases from BatchTableEnvironment/StreamTableEnvironment to FlinkBatchPrograms /FlinkStreamPrograms using new implementation*\r\n  - *Removes the useless code(such as getLogicalOptRuleSet, replaceNormRules). Updates the corresponding implementations*\r\n  - *Updates existed test cases, adds new test cases*\r\n\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests, such as *IT cases could validates new implementation about  the optimization phases*.\r\n\r\nThis change added tests and can be verified as follows:\r\n- *Added test that validates the correct of FlinkChainedPrograms implementation*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes)\r\n  - If yes, how is the feature documented? (JavaDocs)\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4667", "title": "[FLINK-5859] [table] Add PartitionableTableSource for partition pruning", "body": "## What is the purpose of the change\r\n\r\nThis pull request adds PartitionableTableSource for partition pruning when optimizing the query plan. That way both query optimization time and execution time can be reduced obviously, especially for a large partitioned table.\r\n\r\n## Brief change log\r\n\r\n  - *Adds PartitionableTableSource which extends FilterableTableSource*\r\n  - *Adds setRelBuilder method in FilterableTableSource class*\r\n  - *Adds implementation for partition pruning and extracting partition predicates*\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n  - *Added integration tests for PartitionableTableSource on batch and stream sql*\r\n  - *Added test that validates the correct of partition pruning*\r\n  - *Added test that validates the correct of extracting partition predicates*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes)\r\n  - If yes, how is the feature documented? (JavaDocs)\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4572", "title": "[Flink-7243] [connectors] Add ParquetInputFormat ", "body": "## What is the purpose of the change\r\n\r\nAdd ParquetInputFormat to read from parquet files\r\n\r\n## Brief change log\r\n  - *Supports primary parquet types*\r\n  - *Supports reading data with given columns instead of all columns*\r\n  - *Supports reading data with filter*\r\n  - *The return type of ParquetInputFormat supports Row, Tuple, POJO*\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n - *Added test that validates the correct of input split result with file or directory*\r\n - *Added test for reading with filter*\r\n - *Added test for  RowParquetInputFormat/TupleParquetInputFormat/PojoParquetInputFormat*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes)\r\n  - If yes, how is the feature documented? (JavaDocs)\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3860", "title": "[FLINK-6516] [table] using real row count instead of dummy row count when optimizing plan", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3818", "title": "[FLINK-5514] [table] Implement an efficient physical execution for CUBE/ROLLUP/GROUPING SETS", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "razvan100": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4809", "title": "[FLINK-7803][Documentation] Add missing savepoint information", "body": "This fixes FLINK-7803 by emphasizing the savepoint save location should be on a distributed file-system.\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "1m2c3t4": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4784", "title": "FLINK-7736: fix some lgtm.com alerts", "body": "\r\n## What is the purpose of the change\r\n\r\nlgtm.com performs deep analysis on more than 50,000 open source projects including many of the apache projects, identifying bugs and other opportunities for improvement of the code. This PR addresses 14 of the more straightforward ones found (see https://issues.apache.org/jira/browse/FLINK-7736 for details)\r\n\r\n\r\n## Brief change log\r\n\r\nFixed the following alerts:\r\n\r\n1) dereferenced variable is always null, in TaskSlotTable\r\n2-3) array index out of bounds, in KVStateRequestSerializer and Utils\r\n4) inconsistent equals and hashCode, in ArchivedJson\r\n5-6) close input, in JarListHandler and SocketTextStreamFunction\r\n7) close output, in JarFileCreator\r\n8) unused format argument, in YarnApplicationMasterRunner\r\n9) useless type test, in GroupReduceNode\r\n10-11) useless comparison, in TaskExecutor and FieldAccessor\r\n12-14) Result of integer multiplication cast to long, in MemoryManager and twice in InPlaceMutableHashTable\r\n\r\nAlso added a new test\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n  - *Added test that verifies that hashCode and equals are consistent in ArchivedJson\r\n\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? no\r\n  - If yes, how is the feature documented? not applicable\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "asicoe": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4725", "title": "[FLINK-7689] [Streaming Connectors] Added metrics to JDBCOutputFormat in order to be able to m\u2026", "body": "Added metrics to JDBCOutputFormat in order to be able to measure JDBC batch flush rate, latency and size.\r\n\r\n## What is the purpose of the change\r\nThis pull request adds some useful metrics to the flink-jdbc output format.\r\n\r\n## Brief change log\r\n\r\n- Added metrics to JDBCOutputFormat in order to be able to measure jdbc batch flush rate, latency and size.\r\n- Fixed failing existing tests.\r\n- Added tests for metrics setup.\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n1. Run the JDBCOutputFormatTest suite.\r\n2. Write a flink app that depends on flink-jdbc from this pr and flink-table and use it to write some records in a JDBC compliant database. Open the Flink UI and see the stats in the Task Metrics.\r\n \r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? no\r\n  - If yes, how is the feature documented? not applicable\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mlipkovich": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4683", "title": "[FLINK-5944] Support reading of Snappy files", "body": "## What is the purpose of the change\r\n\r\nSupport reading of Snappy compressed text files (both Xerial and Hadoop snappy)\r\n\r\n## Brief change log\r\n\r\n  - *Added InputStreamFactories for Xerial and Hadoop snappy*\r\n  - *Added config parameter to control whether Xerial or Hadoop snappy should be used*\r\n\r\n## Verifying this change\r\n\r\n  - *Manually verified the change by running word count for text files compressed using different Snappy versions*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? yes\r\n  - If yes, how is the feature documented? JavaDocs \r\n\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "beyond1920": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4681", "title": "[FLINK-7636][Table API & SQL]Introduce Flink RelOptTable, and remove tableSource from all TableSourceScan node constructor", "body": "## What is the purpose of the change\r\n\r\nThere are two ways to fetch TableSource of TableSourceScan node (e.g FlinkLogicalTableSourceScan, PhysicalTableSourceScan and its subclass):\r\n1. \r\nval relOptTable: RelOptTable = getTable()\r\nval tableSourceTable = relOptTable.unwrap(classOf[TableSourceTable[_]])\r\nval tableSouce = tableSourceTable.tableSource\r\n\r\nthe result of getTable() is instance of RelOptTableImpl now, and it will not change after RelNode tree is built.\r\n2.  TableSourceScan node contains a tablesource as constructor parameter, so we could fetch the tablesource directly later.\r\n \r\nThe returned tableSource is different with each other through above two ways after apply project push(PPD) down or filter push down(FPD).  It is very confusing and will cause problems.\r\n\r\nThe pr aims to fix the problem by introducing FlinkRelOptTable to replace RelOptTableImpl, and remove tableSource parameter from TableSourceScan's constructor. After PPD or FPD,  a new FlinkRelOptTable instance which contains a new TableSourceTable will be passed to TableSourceScan constructor. \r\n\r\n## Brief change log\r\n\r\n  - *Adds FlinkRelOptTable to replace default RelOptTable implementation (RelOptTableImpl)*\r\n  - *Adds FlinkCalciteCatalogReader, which is subclass of  CalciteCatalogReader. It overrides getTable method to return FlinkRelOptTable instance instead of RelOptTableImpl instance*\r\n  - *Removes tableSource parameter from TableSourceScan's constructor. A new FlinkRelOptTable instance which contains a new TableSourceTable will be passed to TableSourceScan constructor.*\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n  - *Added test that validates that FlinkRelOptTable instance is returned once call getTable method of FlinkCalciteCatalogReader*\r\n  - *Other change is already covered by existing tests, such as (TableSourceTest, TableSourceITCase)*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3569", "title": "[flink-6036]Let catalog support partition", "body": "This pr aims to let catalog support partition.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ggevay": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4680", "title": "[FLINK-7629] [scala] Fix KeyedStream.aggregate for nested field expressions", "body": "## What is the purpose of the change\r\n\r\nMake `KeyedStream.aggregate` (and `sum`, `maxBy`, etc.) able to handle nested field expressions.\r\n\r\n\r\n## Brief change log\r\n\r\nThe first commit fixes a bug in `RecursiveProductFieldAccessor`: the `fieldType` field should contain the type of the innermost field. I.e., if the field expression is `\"field1.field2.field3\"`, then it should be the type of `field3`, and not `field1`.\r\n\r\nThe second commit modifies the string overload of `KeyedStream.aggregate` to not use `fieldNames2Indices` to resolve the given field expression, but directly create a `SumAggregator` or `ComparableAggregator`, whose constructors call `FieldAccessorFactory.getAccessor`, which correctly resolves nested field expressions like `\"field1.field2.field3\"`.\r\n\r\n## Verifying this change\r\n\r\n- Extended the tests in `FieldAccessorTest` to catch the `fieldType` issue.\r\n- Added a test which calls `KeyedStream.sum` with a nested field expression.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? yes\r\n  - If yes, how is the feature documented? The feature was already documented in the JavaDocs of the aggregation methods of `KeyedStream`. This PR just brings the functionality up-to-date with the documentation.\r\n\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "raymondtay": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4666", "title": "[FLINK-7613][Documentation] Fixed typographical error", "body": "*Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*\r\n\r\n*Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*\r\n\r\n## Contribution Checklist\r\n\r\n  - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.\r\n  \r\n  - Name the pull request in the form \"[FLINK-XXXX] [component] Title of the pull request\", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.\r\n  Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.\r\n\r\n  - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.\r\n  \r\n  - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).\r\n\r\n  - Each pull request should address only one issue, not mix up code from multiple issues.\r\n  \r\n  - Each commit in the pull request has a meaningful commit message (including the JIRA id)\r\n\r\n  - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.\r\n\r\n\r\n**(The sections below can be removed for hotfixes of typos)**\r\n\r\n## What is the purpose of the change\r\n\r\n*(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*\r\n\r\n\r\n## Brief change log\r\n\r\n*(for example:)*\r\n  - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*\r\n  - *Deployments RPC transmits only the blob storage reference*\r\n  - *TaskManagers retrieve the TaskInfo from the blob cache*\r\n\r\n\r\n## Verifying this change\r\n\r\n*(Please pick either of the following options)*\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n*(or)*\r\n\r\nThis change is already covered by existing tests, such as *(please describe tests)*.\r\n\r\n*(or)*\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n*(example:)*\r\n  - *Added integration tests for end-to-end deployment with large payloads (100MB)*\r\n  - *Extended integration test for recovery after master (JobManager) failure*\r\n  - *Added test that validates that TaskInfo is transferred only once across recoveries*\r\n  - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)\r\n  - The serializers: (yes / no / don't know)\r\n  - The runtime per-record code paths (performance sensitive): (yes / no / don't know)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes / no)\r\n  - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "uybhatti": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4660", "title": "[FLINK-7050][table] Add support of RFC compliant CSV parser for Table Source", "body": "## What is the purpose of the change\r\n\r\nCurrently CsvInputFormat is not compliant with RFC 4180 standards and we can't correctly parse fields containing double quotes, line delimiter or field delimiter.\r\n\r\n\r\n## Brief change log\r\n\r\n  - RFCCsvInputFormat is added to support RFC 4180 standards.\r\n  - Also we added RFCRowCsvInputFormat, RFCTupleCsvInputFormat and RFCCsvTableSource for this purpose.\r\n  - New dependency for Jackson parser is added in pom.xml of flink-table. \r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n  - RFCCsvInputFormatTest and RFCRowCsvInputFormatTest are added to test the added CSV Input Format functionality.\r\n  - TableSourceTest#testRFCCsvTableSourceBuilder is added for RFCCsvTableSource.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): **yes**\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**\r\n  - The serializers: **no**\r\n  - The runtime per-record code paths (performance sensitive): **no**\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **yes**\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? **yes**\r\n  - If yes, how is the feature documented? **not documented yet**\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gallenvara": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4649", "title": "[FLINK-6116] Watermarks don't work when unioning with same DataStream.", "body": "## What is the purpose of the change\r\n\r\nIn self-union case, the stream edges between the source and target will be regard as the single one. The `streamOutputMap` in `StreamGraph` will create only one `RecordWriterOutput` instance.\r\n\r\n\r\n## Brief change log\r\n\r\n  - Add edge subId for differentiating streamEdge between the same source and target node.\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n`testUnion` case in `DataStreamTest`\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not documented)\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3110", "title": "[FLINK-2184] Cannot get last element with maxBy/minBy.", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [X] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [X] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n\r\nExtend the minBy/maxBy with first parameter. @fhueske sorry for the late update for this pr, can you help with review work?", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/1909", "title": "[FLINK-3783] [core] Support weighted random sampling with reservoir.", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\nIn addition to going through the list, please provide a meaningful description of your changes.\n- [x] General\n  - The pull request references the related JIRA issue\n  - The pull request addresses only one issue\n  - Each commit in the PR has a meaningful commit message\n- [x] Documentation\n  - Documentation has been added for new functionality\n  - Old documentation affected by the pull request has been updated\n  - JavaDoc for public methods has been added\n- [x] Tests & Build\n  - Functionality added by the pull request is covered by tests\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\n\nIn default random sampling, all items have the same probability to be selected. But in weighted random sampling, the probability of each item to be selected is determined by its weight with respect to the weights of the other items. This is reference paper: http://arxiv.org/pdf/1012.0256.pdf. The test of WRS is defining 10 items with different probability and counting every item in the reservoir to get the probability after sampling and comparing with expected probabilities to verify its correctness. Also, the paper describe method with exponential jumps, i think we can implement it in future.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/14352150", "body": "@tillrohrmann  @smarthi  I have done some work of `flink-benchmark` with JMH. Can you tell me why to remove the `flink-benchmark` module. Does JMH have some weakness?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14352150/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/14373918", "body": "@chiwanpark  @sachingoel0101  Thanks for your explaination.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14373918/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "bbayani": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4628", "title": "[FLINK-7486]:[flink-mesos]:Support for adding unique attribute / grou\u2026", "body": "\u2026p_by attribute constraints\r\n\r\nJIRA issue: FLINK-7486\r\n## What is the purpose of the change\r\n- To be able to add soft constraint to balance the tasks across mesos-agents on basis of host constraint \r\n\r\n## Brief change log\r\n- Added new config 'mesos.constraints.soft.balanced'\r\n- Parsed the config in MesosTaskManagerParams and prepared list of BalancedConstraintParam objects\r\n- Prepared Set of TaskIds from workersInNew and workersInLaunch in MesosResourceManager/MesosFlinkResourceManager. Used this Set in coTaskGetter to get co-task-ids\r\n- Updated LaunchableMesosWorker to return soft constraints from BalancedConstraintParam objects\r\n\r\n## Verifying this change\r\n  - Manually verified the change by running flink on mesos cluster. The mesos worker had attributes AZ and hostname set. After adding the constraint, taskmanagers were launched on mesos-workers with unique AZ/hostname values.\r\n \r\n## Does this pull request potentially affect one of the following parts:\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: don't know\r\n  - The runtime per-record code paths (performance sensitive): don't know\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: yes (flink deployment on mesos)\r\n\r\n## Documentation\r\n  - Does this pull request introduce a new feature? yes\r\n  - If yes, how is the feature documented? docs\r\n     The feature is controlled by config 'mesos.constraints.soft.balanced' in flink-conf.yaml\r\n      The config is documented in docs.\r\n\r\n@EronWright : PTAL. Thanks!\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dawidwys": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4610", "title": "[FLINK-7130] Removed event serializer from NFA and SharedBuffer", "body": "## What is the purpose of the change\r\n\r\n* The purpose is to remove usage of event serializer from `NFA` and `SharedBuffer` classes, as it should not be used in the logic part. \r\n\r\n\r\n## Brief change log\r\n\r\n*(for example:)*\r\n  - Removed NonDuplicatingTypeSerializer\r\n  - Removed eventSerializer from `NFA`\r\n  - Removed eventSerializer from `SharedBuffer`\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): ( / no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: ( no )\r\n  - The runtime per-record code paths (performance sensitive): ( no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4587", "title": "[FLINK-7511] [cep] Remove dead code after dropping backward compatibi\u2026", "body": "## What is the purpose of the change\r\n\r\nThe purpose is to remove a dead code in CEP library due to dropping backwards compatibility with Flink <= 1.2. Also in this PR I added appropriate note in CEP docs.\r\n\r\n## Brief change log\r\n\r\n  - removed dead code\r\n  - added docs notes\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage. The current test base should cover that change.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency):  no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: yes\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no \r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature?  no\r\n  - If yes, how is the feature documented? not applicable\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4514", "title": "[FLINK-7384][cep] Unify event and processing time handling in the Abs\u2026", "body": "\r\n## What is the purpose of the change\r\n\r\nThis PR introduces a new configuration parameter that allows partial matches to be timeouted without any subsequent events in the stream. Just based on the time. The timers in case of custom operators are also registered less often.\r\n\r\n## Verifying this change\r\n\r\nThis change changed existing tests and can be verified as follows:\r\n\r\nThe *TODO* was removed from testCEPOperatorCleanupProcessingTime\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (yes, but just in CEP)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes)\r\n  - If yes, how is the feature documented? (docs / JavaDocs)\r\n\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sjwiesman": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4607", "title": "[FLINK-6306][connectors] Sink for eventually consistent file systems", "body": "## What is the purpose of the change\r\n\r\nThis pull request implements a sink for writing out to an eventually consistent filesystem, such as Amazon S3, with exactly once semantics. \r\n\r\n\r\n## Brief change log\r\n  - The sink stages files on a consistent filesystem (local, hdfs, etc) .\r\n  - Once per checkpoint, files are copied to the eventually consistent filesystem. \r\n  - When a checkpoint completion notification is sent, the files are marked consistent. Otherwise, they are left because delete is not a consistent operation.\r\n  - It is up to consumers to choose their semantics; at least once by reading all files, or exactly once by only reading files marked consistent. \r\n\r\n\r\n## Verifying this change\r\nThis change added tests and can be verified as follows:\r\n\r\n  - Added tests based on the existing BucketingSink test suite. \r\n  - Added tests that verify semantics based on different checkpointing combinations (successful, concurrent, timed out, and failed). \r\n  - Added integration test that verifies exactly once holds during failure. \r\n  - Manually verified by having run in production for several months. \r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper:no \r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? yes\r\n  - If yes, how is the feature documented? JavaDocs\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zhuganghuaonnet": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4535", "title": "[FLINK-7474] [ Streaming Connectors] AzureEventhubs-connector, support read from and write to Azure eventhubs", "body": "*Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*\r\n\r\n*Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*\r\n\r\n## Contribution Checklist\r\n\r\n  - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.\r\n  \r\n  - Name the pull request in the form \"[FLINK-1234] [component] Title of the pull request\", where *FLINK-1234* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.\r\n  Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.\r\n\r\n  - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.\r\n  \r\n  - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).\r\n\r\n  - Each pull request should address only one issue, not mix up code from multiple issues.\r\n  \r\n  - Each commit in the pull request has a meaningful commit message (including the JIRA id)\r\n\r\n  - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.\r\n\r\n\r\n**(The sections below can be removed for hotfixes of typos)**\r\n\r\n## What is the purpose of the change\r\n\r\n*(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*\r\n\r\n\r\n## Brief change log\r\n\r\n*(for example:)*\r\n  - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*\r\n  - *Deployments RPC transmits only the blob storage reference*\r\n  - *TaskManagers retrieve the TaskInfo from the blob cache*\r\n\r\n\r\n## Verifying this change\r\n\r\n*(Please pick either of the following options)*\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n*(or)*\r\n\r\nThis change is already covered by existing tests, such as *(please describe tests)*.\r\n\r\n*(or)*\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n*(example:)*\r\n  - *Added integration tests for end-to-end deployment with large payloads (100MB)*\r\n  - *Extended integration test for recovery after master (JobManager) failure*\r\n  - *Added test that validates that TaskInfo is transferred only once across recoveries*\r\n  - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)\r\n  - The serializers: (yes / no / don't know)\r\n  - The runtime per-record code paths (performance sensitive): (yes / no / don't know)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes / no)\r\n  - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yunfan123": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4467", "title": "[FLINK-7359] flink-storm should update ComponentConfiguration to stor\u2026", "body": "Pass the storm defined component config to init storm in runtime.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rtudoran": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4380", "title": "Time sort with offset/fetch without retraction", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x ] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [x ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3609", "title": "[FLINK-6073] - Support for SQL inner queries for proctime", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x ] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [x ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/21554770", "body": "@fhueske @sunjincheng121 This is not ok or fair to create this.  Me and @stefanobortoli  already created this and this creates conflict over our branches without bringing anything. And in fact it is exactly the code we created. \r\n...i think it only creates additional work and delays our merging...", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/21554770/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "RebornHuan": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4374", "title": "repalce map.put with putIfAbsent", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [ ] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "patricklucas": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4299", "title": "[FLINK-7155] [metrics] Add Influxdb reporter", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "XuPingyong": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4273", "title": "[FLINK-7065] Separate the flink-streaming-java module from flink-clients", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4267", "title": "[FLINK-7018][streaming] Refactor streamGraph to make interfaces clear", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4241", "title": "[FLINK-7015] [streaming] separate OperatorConfig from StreamConfig", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vschafer": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4269", "title": "FLINK-6557 Fixing tests for RocksDB state backend on Windows", "body": "This is an alternate fix for issue FLINK-6557 to https://github.com/apache/flink/pull/3899 which seems to have failed in CI testing. The proposed fix uses the same technique used to solve this issue in other parts of the Flink codebase, making it consistent and easy to refactor in the future.\r\n\r\nDescription from the commit message:\r\n\"Fixing tests for RocksDB state backend on Windows. The original path to RocksDB contains \"/\" character at the beginning of the String, which is not accepted by the RocksDB implementation. The fix uses logic consistent with existing handling of this issue in other parts of the codebase.\"", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mpouttuclarke": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4228", "title": "Flink-7035 Automatically identify AWS Region, simplified constructors, added test properties support", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wuchong": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4189", "title": "[FLINK-6958] [async] Async I/O hang when the source is bounded and collect timeout", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [x] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/flink/pulls/4145", "title": "[FLINK-6938][FLINK-6939] [cep] Not store IterativeCondition with NFA state and support RichFunction interface", "body": "The core idea is that the `StateTransition` is unique in a NFA graph. So we store the conditions with a map which mapping from `StateTransition` to `IterativeCondition`, so the conditions can not serialized with NFA state. If I missed something, please point out.\r\n\r\nThis PR also includes FLINK-6938: IterativeCondition supports RichFunction interface. \r\n\r\n\r\nThanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [x] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dbrinegar": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4188", "title": "[FLINK-7009] dogstatsd mode in statds reporter", "body": "* converts output to ascii alphanumeric characters with underbar,\r\ndelimited by periods\r\n* reports all Flink variables as tags\r\n* compresses overly long segments with a first-ten plus hash symbol\r\n* compresses Flink ID values to first eight characters\r\n* removes object references from names, for correctness\r\n* drops negative and invalid values\r\n* handles LatencyGauge values", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "DmytroShkvyra": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/4001", "title": "[FLINK-5476] Fail fast if trying to submit a job to a non-existing Fl\u2026", "body": "Fail fast if trying to submit a job to a non-existing Flink cluster\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-5476] Fail fast if trying to submit a job to a non-existing Flink cluster\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "deepakmarathe": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3977", "title": "[FLINK-5490] Not allowing sinks to be cleared when getting the execut\u2026", "body": "\u2026ion plan via ExecutionEnvironment.\r\n\r\nThanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [ ] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chenzio": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3971", "title": "[FLINK-6477][web] Waiting the metrics retrieved from the TaskManagers", "body": "problem:\r\nFlink web page of Taskmanager metrics can not display normal, when we first login.\r\n\r\nanalysis:\r\nThe web-frontend communicates with the backend for requesting metrics, and the metrics retrieved from the TaskManagers. The first call to load metrics starts a new metrics update, and gets nothing as a result. now we wait to get the result and response to the web-frontend.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "397090770": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3869", "title": "[FLINK-6502] Add support ElasticsearchSink for DataSet", "body": "Currently, Flink only support writing data in `DataStream` to ElasticSearch through `ElasticsearchSink`,  it will be very useful if Flink internal support writing data in `DataSet` to ElasticSearch. See [http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/ElasticsearchSink-on-DataSet-td12980.html](http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/ElasticsearchSink-on-DataSet-td12980.html)\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zohar-mizrahi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3838", "title": "[FLINK-5886] Python API for streaming applications", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [x] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jinglining": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3830", "title": "Optimize aggregate function get type", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [ ] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "huawei-flink": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3783", "title": "[FLINK-6388] Add support for DISTINCT into Code Generated Aggregations", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [x] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shijinkui": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3777", "title": "[FLINK-6387] [webfrontend]Flink UI support access log", "body": "Record the use request to the access log. Append use access to the log file.\r\n\r\n- [X] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-6387] [webfrontend]Flink UI support access log\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [X] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [X] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "haohui": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3765", "title": "[FLINK-6373] Add runtime support for distinct aggregation over grouped windows", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3764", "title": "[FLINK-6335] Parse DISTINCT over grouped windows in stream SQL.", "body": "This PR only supports parsing distinct aggregations over grouped windows. It essentially allows the `DataStreamAggregateRule` to translate the supported queries.\r\n\r\nThe runtime support will be added in FLINK-6373.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ramkrish86": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3760", "title": "FLINK-5752 Support push down projections for HBaseTableSource (Ram)", "body": "Ran mvn clean verify -DskipTests\r\nIn this patch \r\n`Arrays.sort(nestedFields[i]);`\r\n\r\nAm doing this before doing addColumns for the new projected table source, because here the cols appears in a reverse sorted way and when we apply that for the new projected table source it creates an assertion error while creating the new calcite program with the projected cols\r\n`assert expr.getType().getFieldList().get(field.getIndex()) == field;`\r\n{edit} the above line is in RexFieldAccess\r\nSo doing this sort helps in correcting those issues and the tests run fine. But this nestedFields that is being passed to the projectNestedFields() API is created by\r\n`def getProjectedFields: Array[Array[String]] ` under RexProgramExtractor. Trying to understand what it does. \r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3478", "title": "Flink 4816 Executions failed from \"DEPLOYING\" should retain restored checkpoint information", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [ ] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n\r\nJust does what ever the description says. Feedback/suggestions are welcome. Ran 'mvn clean verify -DskipTests' no static comment failures.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3334", "title": "FLINK-4810 Checkpoint Coordinator should fail ExecutionGraph after \"n\" unsuccessful checkpoints", "body": "unsuccessful checkpoints\r\n\r\nThanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [ ] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n\r\n\r\nRan mvn clean verify. Did not add test cases to know the first level feedback. ", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2510", "title": "FLINK-3322 Allow drivers and iterators to reuse the memory segments", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\nIn addition to going through the list, please provide a meaningful description of your changes.\n- [ ] General\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\n  - The pull request addresses only one issue\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\n- [ ] Documentation\n  - Documentation has been added for new functionality\n  - Old documentation affected by the pull request has been updated\n  - JavaDoc for public methods has been added\n- [ ] Tests & Build\n  - Functionality added by the pull request is covered by tests\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\n\nUpdated PR based on @ggevay 's feedback. Now the drivers are all made of type ResettableDrivers and we use the existing APIs to reuse the driver and the iterators. The JoinTaskITerator has now a new method called reset() that allows to reset the iterator's state.\nIf we go with the approach then it makes sense to remove the ResettableDriver and add all the API in it to the Driver interface itself. That is for a later PR. \n@ggevay - Pls review and give your valuable feedback.\nI tried mvn clean verify and got the tests to pass in flink-runtime and flink-gelly-examples. One failed 'org.apache.flink.runtime.io.disk.ChannelViewsTest.testWriteAndReadLongRecords' and when I ran locally it passed again. \n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2496", "title": "FLINK-4615 Reusing the memory allocated for the drivers and iterators", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\nIn addition to going through the list, please provide a meaningful description of your changes.\n- [ ] General\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\n  - The pull request addresses only one issue\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\n- [ ] Documentation\n  - Documentation has been added for new functionality\n  - Old documentation affected by the pull request has been updated\n  - JavaDoc for public methods has been added\n- [ ] Tests & Build\n  - Functionality added by the pull request is covered by tests\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\n\nFirst PR for initial review. Helps to reuse the memory for the iterators that are created by the drivers. If some one could point me to more test cases other than the ConnectedComponents example can see how things work. Suggestions/feedback are welcome. \n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2495", "title": "FLINK-3322 - Make sorters to reuse the memory pages allocated for iterative tasks", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\nIn addition to going through the list, please provide a meaningful description of your changes.\n- [ ] General\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\n  - The pull request addresses only one issue\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\n- [ ] Documentation\n  - Documentation has been added for new functionality\n  - Old documentation affected by the pull request has been updated\n  - JavaDoc for public methods has been added\n- [ ] Tests & Build\n  - Functionality added by the pull request is covered by tests\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\n\nThis is part1 for FLINK-3322 where only the Sorters are made to reuse the memory pages. As @ggevay  pointed out we have to handle the iterators also where the memory pages are allocated. I have a seperate PR for that because that involves touching lot of places. But am open to feedback here. It is fine with me to combine both also but it was making the changes much bigger. \nI would like to get the feed back here on this apporach. \nHere a SorterMemoryAllocator is now passed to the UnilateralSortMergers. That will allocate the required memory pages and it will allocate the required read, write and large buffers. As per the existing logic the buffers will be released. But if the task is an iterative task we wait for the tasks to be released until a close or termination call happens for the iterative task. \nIn case of pages that were grabbed in between for keysort or record sort those will be put back to the respective pages so that we have the required number of pages through out the life cycle of the iterative task.\n\nAs said this is only part 1. We need to address the iterators also. But that according to me touches more places. I have done the changes for that but it is not in a shape to be pushed as a PR but am open to feed back here. Thanks all. \n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2412", "title": "FLINK-4462 Add RUN_TIME retention policy for all the flink annotations", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\nIn addition to going through the list, please provide a meaningful description of your changes.\n- [ ] General\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\n  - The pull request addresses only one issue\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\n- [ ] Documentation\n  - Documentation has been added for new functionality\n  - Old documentation affected by the pull request has been updated\n  - JavaDoc for public methods has been added\n- [ ] Tests & Build\n  - Functionality added by the pull request is covered by tests\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\n\n(Ram)\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mtunique": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3756", "title": "[FLINK-2013] Create generalized linear model framework", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [x] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3565", "title": "[FLINK-5413] [table] Convert TableEnvironmentITCases to unit tests", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3428", "title": "[FLINK-1743] Add multinomial logistic regression to machine learning library", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [x] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n\r\n\r\n1. Refactor `MultipleLinearRegression` to extend `GeneralizedLinearModel`\r\n2. Implement `LogisticRegression`\r\n3. Change `LogisticLoss` with label in {0, 1}\r\n4. Add `LogisticLinerRegression` test case\r\n\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "guoweiM": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3672", "title": "[Flink-6072] TestCase CheckpointStateRestoreTest::testSetState should be fail", "body": "This case should be fail because three `KeyGroupStateHandles` which have same `KeyGroupRange[0,0]` are received by `CheckpointCoordinator`.\r\n\r\nThis pull request includes:\r\n1. Use three different `KeyGroupRange`s in the test case.\r\n2. Check whether the `KeyGroupRange` of `TaskState` has overlap.\r\n3. Check whether the `KeyGroupRange` of `SubTaskState` is different.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "p4nna": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3659", "title": "[FLINK-5785] Add an Imputer for preparing data", "body": "Adds an imputer class including tests which is able to impute values into sparse DataSets of Vectors. One can choose if the median, the mean or the most frequent value of a vector or row should be inserted", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "barcahead": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3654", "title": "[FLINK-6152] [yarn] don't throw exception if client can't receive cluster status", "body": "Currently if client can't get jobmanager status, it will throw exception and trigger shutdown hook. In the shutdown hook method, config file, keytab file and properties file will all be deleted. As result, AM restarting will fail. \r\nI fix this issue by only responding cluster status failure from yarn client, which tolerates am restarting.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3391", "title": "[FLINK-5758] [yarn] support port range for web monitor", "body": "1. `jobmanager.web.port` now supports port range and has a string type default value - \"8081\"\r\n2. a helper method `createServerFromPorts` is defined to select port and create corresponding server, also refactor method `BootstrapTools. startActorSystem` using this helper method\r\n3.  In `YarnApplicationMasterRunner.runApplicationMaster`, first create web monitor to have a fixed web port, store it in config and then create jobmanager. So jobmanager knows the web port, this port info is useful when new jobmanager is elected. ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "clarkyzl": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3623", "title": "[FLINK-6196] [table] Support dynamic schema in Table Function", "body": "Type: Improvement\r\nPriority: Major\r\nComponents: table, udtf\r\nProblem Definition: Support dynamic schema in Table Function\r\nDesign:\r\n1. Modified the getResult() interfaces of an UDTF. Suport java sytle arguments of a list. only literals will be passed to the UDTf.\r\n1. Define the TableFunction and TableFunctionCall clearly. A TableFunction is an object that the ResultType and parameters are not determined. A TableFunctionCall is an object that thre ResultType and paramenters are determined.\r\n1. Implement the TableAPI, Expression and SQL style call stack of the getResultType.\r\n\r\nImpact Analysis:\r\nUDTF, an interface has changed.\r\n\r\nTest:\r\nAll tests done.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3473", "title": "[FLINK-5833] [table] Support for Hive GenericUDF", "body": "Type: New Feature\r\nPriority: Major\r\nComponents: table, udf\r\nProblem definition: Make Flink call Hive User-Defined Functions, support for simple hive Generic UDFs.\r\n\r\nDesign:\r\n1. This patch is based on FLINK-5881 and FLINK-5832, we need variable arguments to call hive udfs. We also need a HiveFunctionWrapper in FLINK-5832 to create Hive Generic UDFs.\r\n1. Added a ScalarFunction called HiveGenericUDF to call Hive Generic UDFs. Use primitive java object inspectors for the generic UDFs.\r\n1. Moved the code to flink-hcatalog\r\n\r\nImpact Analysis:\r\nA new feature, had few impacts on exsting features. We updated hcatalog dependencies from 0.12.0 to 0.13.0.\r\nTest:\r\n`mvn clean verify` is done on local.\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3456", "title": "[FLINK-5832] [table] Support for simple hive UDF", "body": "Type: New Feature\r\nPriority: Major\r\nComponents: table, udf, hive\r\nProblem definition: Make Flink call Hive User-Defined Functions, support for simple hive UDFs.\r\n\r\nDesign:\r\n1. This patch is based on FLINK-5881, we need variable arguments to call hive udfs.\r\n2. Added a HiveFunctionWrapper to create Hive simple UDFs.\r\n3. Added a ScalarFunction called HiveSimpleUDF to call Hive Simple UDFs. Use primitive java object inspectors for the simple UDFs.\r\n4. A few modification of type comparation, make type of Object equal any types.\r\n5. Added a new method in TableEnvironment to rigister hive UDFs.\r\n\r\nImpact Analysis:\r\nA new feature, had few impacts on exsting features, except for the comparation of types.\r\n\r\nTest:\r\n`mvn clean verify` is done on local.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "WangTaoTheTonic": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3617", "title": "[FLINK-6192]reuse zookeeper client created by CuratorFramework", "body": "Now in yarn mode, there're three places using zookeeper client(web monitor, jobmanager and resourcemanager) in ApplicationMaster/JobManager, while there're two in TaskManager. They create new one zookeeper client when they need them.\r\n\r\nI believe there're more other places do the same thing, but in one JVM, one CuratorFramework is enough for connections to one zookeeper cluster, so we need a singleton to reuse them.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3614", "title": "[FLINK-6189][YARN]Do not use yarn client config to do sanity check", "body": "Now in client, if #slots is greater than then number of \"yarn.nodemanager.resource.cpu-vcores\" in yarn client config, the submission will be rejected.\r\n\r\nIt makes no sense as the actual vcores of node manager is decided in cluster side, but not in client side. If we don't set the config or don't set the right value of it(indeed this config is not a mandatory), it should not affect flink submission.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3599", "title": "[FLINK-6174][HA]introduce a SMARTER leader latch to make JobManager less sensitive to disconnect to zookeeper", "body": "Now in yarn mode, if we use zookeeper as high availability choice, it will create a election service to get a leader depending on zookeeper election.\r\n\r\nWhen zookeeper leader crashes or the connection between JobManager and zookeeper instance was broken, JobManager's leadership will be revoked and send a Disconnect message to TaskManager, which will cancle all running tasks and make them waiting connection rebuild between JM and ZK.\r\n\r\nIn yarn mode, we have one and only JobManager(AM) in same time, and it should be alwasy leader instead of elected through zookeeper. We can introduce a new leader election service in yarn mode to achive that.\r\n\r\n**Update:**\r\nIn case of \"split brain\" issue, we cannot directly set one JM to leader alltime. Instead i introduce a smarter leader latch that will cache the suspend state and wait a connection timeout duration until the connection to zookeeper is back.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3335", "title": "[FLINK-5818][Security]change checkpoint dir permission to 700", "body": "Now checkpoint directory is made w/o specified permission, so it is easy for another user to delete or read files under it, which will cause restore failure or information leak.\r\n\r\nIt's better to lower it down to 700.\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n![chp-filesystem-session](https://cloud.githubusercontent.com/assets/5276001/23019741/d753e8e0-f47e-11e6-9f2e-2cd35de35ef1.JPG)\r\n\r\n# Discussion Details\r\n## Preconditions\r\n1. Every flink job(session or single) can specify a directory storing checkpoint, called `state.backend.fs.checkpointdir`.\r\n2. Different jobs can set same or different directories, which means their checkpoint files can be stored in one same or different directories, with **sub-dir** created with their own job-ids.\r\n3. Jobs can be run by different users, and users has requirement that one could not read chp files written by another user, which will cause information leak.\r\n4. In some condition(which is relatively rare, I think), as @StephanEwen said, users has need to access other users\u2019 chp files for cloning/migrating jobs.\r\n5. The chp files path is like: `hdfs://namenode:port/flink-checkpoints/<job-id>/chk-17/6ba7b810-9dad-11d1-80b4-00c04fd430c8`\r\n\r\n## Solutions \r\n### Solution #1 (would not require changes)\r\n1. Admins control permission of root directory via HDFS ACLs(set it like: user1 can read&write, user2 can only read, \u2026).\r\n2. This has two disadvantages: a) It is a** huge burden for Admins** to set different permissions for large number of users/groups); and b) sub-dirs inherited permissions from root directory, which means **they are basically same**, which make it hard to do fine grained control.\r\n### Solution #2 (this proposal)\r\n1. We don\u2019t care what permission of the root dir is. It can be create while setup or job running, as long as it is available to use.\r\n2. We control every sub-dir created by different jobs(which are submitted by different users, in most cases), and set it to a lower value(like \u201c700\u201d) to prevent it to be read by others.\r\n3. If someone wanna migrate or clone jobs across users(again, this scenario is rare in my view), he should ask admins(normally HDFS admin) to add ACLs(or whatever) for this purpose.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "RalphSu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3615", "title": "[FLINK-2720] support flink-storm metrics", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [ ] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3568", "title": "[FLINK-6119]: ClusterClient Detach Mode configurable in client-api", "body": "Author: ralphsu@apache.org\r\n\r\nThis closes #FLINK-6119\r\n\r\nThanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [ ] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shixiaogang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3558", "title": "[FLINK-6096][checkpoint] Refactor the migration of old-versioned savepoints", "body": "1. The migrated classes in `SavepointV0` are moved to the package `org.apache.flink.migration.v0`. In the future, each deprecated savepoint version will have its own migration package.\r\n2. A mapping is deployed in `SavepointV0` to record all deprecated classes. `MigrationInstantiationUtil` now will use the mapping to correctly deserialized those deprecated classes.\r\n3. Unused methods in migrated classes are removed.\r\n4. The formats in old snapshots are added in the comments to help understand the restoring.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3380", "title": "[FLINK-5865][state] Throw original exception in the states", "body": "The wrapping of `RuntimeException` is removed so that we can avoid redundant stack printed in the log.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3359", "title": "[FLINK-5544][streaming] Add InternalTimerService implemented in RocksDB", "body": "- Refactor the methods defined in `InternalTimerService`. Some common implementation in `HeapInternalTimerService` now is moved in `InternalTimerService`.\r\n- Implement `RocksDBInternalTimerService` which stores timers in RocksDB and sorts them with an in-momory heap.\r\n- Implement `InternalTimerServiceTestBase` to verify the implementation of `InternalTimerService`.\r\n- Update `AbstractStreamOperator` to allow the usage of customized `InternalTimerService`.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ymarzougui": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3557", "title": "[FLINK-6087] Fix file-path filtering in ContinuousFileMonitoringFunction", "body": "The Files Filter is only applied to directories, so there is no way to filter individual files. This fixes it.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nragon": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3554", "title": "Detached (Remote)StreamEnvironment execution", "body": "Detached execution option from api", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "heytitle": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3511", "title": "[Flink-5734] code generation for normalizedkey sorter", "body": "This pull-request is the implementation of applying code generation to `NormalizedKeySorter`. It is built on top of [FLINK-3722](https://github.com/apache/flink/pull/2628) and based on [FLIP: Code Generation for NormalizedKeySorter](https://cwiki.apache.org/confluence/display/FLINK/FLIP-18%3A+Code+Generation+for+improving+sorting+performance).\r\n\r\n![image](https://cloud.githubusercontent.com/assets/1214890/23800329/40ddc1ee-05ac-11e7-923b-3f0b5684bf3d.png)\r\nResult from  [SortPerformance.java](https://github.com/heytitle/flink/blob/7c8f82f3c11bfba3a20b34fe5315a9d0cb827805/flink-tests/src/test/java/org/apache/flink/test/manual/SortPerformance.java)", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/24528598", "body": "Is it possible that the class will be disappear after the first `get()` from if's condition?", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/24528598/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/24529391", "body": "Can we simplify it to the code below?\r\n```\r\nClass generatedClass = null\r\nWeakReference<Class> fromCache = generatedClassCache.getOrDefault(cacheKey, null);\r\n\r\ngeneratedClass = fromCache != null ? fromCache.get() : null;\r\n\r\nif ( genenetedClass == null ) {\r\n// cache miss\r\n...\r\n}\r\n```\r\n\r\nSo, we don't need to introduce `cacheHit` variable.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/24529391/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/24687576", "body": "\ud83d\udc4d ", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/24687576/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "billliuatuber": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3413", "title": "[FLINK-5668] Reduce dependency on HDFS at job startup time", "body": "In current implementation, Job manager writes task manager configuration into a file and upload it to HDFS. This file's used to bootstrap taskmanager.\r\n\r\nIn this PR, it switches to use system environment instead of HDFS file to pass the configuration from job manager to task manager, which reduce the dependency on HDFS. \r\n  ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wenlong88": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3388", "title": "[FLINK-5815] Add resource files configuration for Yarn Mode", "body": "This PR add three common resource configuration options to yarn mode, which allow user to set single file resource from both local filesystem and remote hdfs filesystem as what we can do using mapreduce, including:\r\n1. add -yfiles . -ylibjars for adding local resource file to yarn per-job cluster, user can provide a list of file paths to add some local jars or dictionary files to yarn distributed cache.\r\n2. add -yarchives for adding remote resource files to yarn per-job cluster, user can provide a list of uri of files which can be stored on hdfs, user can rename the file by fragment of the uri.\r\nall of the files will be distributed to every TM and JM by yarn and added to classpath of TM and JM.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mariusz89016": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3356", "title": "[FLINK-5253] Remove special treatment of \"dynamic properties\"", "body": "This PR removes special treatment of 'dynamic properties' (aka special way of encoding them as environment variable). Instead, these properties are appended to _flink-conf.yaml_ file.\r\n\r\nLink to the issue: https://issues.apache.org/jira/browse/FLINK-5253", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "skonto": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3313", "title": "[FLINK-5588][ml] add a data normalizer to ML library", "body": "- Adds a Normalizer (per sample). \r\n- Throws an exception if NaN values are used, following the scikit-learn approach. An Imputer will be added later. Also I plan to make the rest of preprocessing algos compatible in another PR.\r\n- Supports p norm value as a double. L1,L2, LMax norms are predefined for easier use. ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sachingoel0101": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3192", "title": "[FLINK-1731][ml] Add KMeans clustering(Lloyd's algorithm)", "body": "This is a breakoff from https://github.com/apache/flink/pull/757 to add the lloyd's algorithm first.\r\nI will follow this up with initialization schemes in the above linked PR. \r\n\r\nTo address a few comments from the previous PR:\r\nWe cannot use `DataSet[LabeledVector]` instead of `DataSet[Seq[LabeledVector]]` because the model here is of type `Seq[LabeledVector]` and the semantics of pipeline require as such. ", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/1421", "title": "[FLINK-2928][web-dashboard] Fix confusing job status visualization in job overview.", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/1186", "title": "[FLINK-1966][ml]Add support for Predictive Model Markup Language", "body": "1. Adds an interface to allow exporting of models to PMML format.\n2. Implements export methods for the existing SVM and Regression algorithms.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/13848736", "body": "`a\\nb` isn't a very good test. It will fail on windows.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/13848736/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/13855651", "body": "`a\\nb` isn't a very good way to verify existence of elements. It will fail on windows because the output will actually be `a\\r\\nb` there.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/13855651/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/14352258", "body": "@gallenvara JMH is under GPL license which isn't compatible with Apache License. [FLINK-2973]\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14352258/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "mushketyk": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3124", "title": "[FLINK-5281] Extend KafkaJsonTableSources to support nested data", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [x] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n\r\n\r\nI've added support for serialization and deserialization of nested Rows in Json serializer/deserializer. I tried not to change interfaces, but I wonder if would make sense to replace all `String[] fieldNames, TypeInformation<?>[] fieldTypes` pairs with `RowTypeInfo`. This will change the interface (but we are doing this anyway) but will make interfaces clearer and will help to avoid some code duplication.\r\n@fhueske, what do you think about this?", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3032", "title": "[FLINK-5244] Implement methods for BipartiteGraph transformations", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [x] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n\r\n\r\nThis PR implements map, filter, subgraph, and join methods for BipartiteGraph. There are few more methods that we can implement, but I think that PR is already too big at this stage.\r\n\r\nDo we need to support `translate` methods like `translateTopVertexValues`, `translateTopVertexIds`? If so would it make sense to an interface `BipartiteGraphAlgorithm` and ability to run these algorithms in `BipartiteGraph`?", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/3024", "title": "[FLINK-5362] Implement methods to access BipartiteGraph properties", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [x] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n\r\n\r\nI've implemented methods to access `BipartiteGraph` properties similarly to methods that we have in `Graph` class.\r\n\r\nI am not sure about the method name `getTuples` that I added to the class. `Graph` class has a similar method `getTriples`, but in case of a bipartite graph we do not return triplets (tuples with three values). Should I keep names consistent?", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2988", "title": "[FLINK-4649] Implement bipartite graph metrics", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [x] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2987", "title": "[FLINK-4647] Read bipartite graph", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [x] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2986", "title": "[FLINK-4648] Implement BipartiteGraph generator", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [x] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2985", "title": "[FLINK-5104] Bipartite graph validation", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [x] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [x] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [x] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n\r\n\r\nThis PR duplicates some the user documentation for the graph added in https://github.com/apache/flink/pull/2984 but I think this won't be an issue when documentation PR is pushed to the `master`.\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2448", "title": "[FLINK-3030][web frontend] Enhance dashboard to show execution attempts", "body": "I've added support for displaying execution attempts in the dashboard. \n\nThe JIRA item also includes that we should add accumulators for different attempts, but it seems that accumulators are going to be deprecated (judging from this JIRA item https://issues.apache.org/jira/browse/FLINK-4527) so I didn't implement that.\n\nHere are few screenshots of the new feature:\nA list of opened attempts are collapsed:\n\n![attempts collapsed](https://cloud.githubusercontent.com/assets/592286/18147690/c287947e-6fcd-11e6-8968-ff47283635b7.png)\n\nA list of opened attempts is opened:\n\n![opened attempts](https://cloud.githubusercontent.com/assets/592286/18147738/e9e3d8d4-6fcd-11e6-94c1-00036df3348b.png)\n\nIf there is no attempts the button is not displayed:\n\n![no attempts](https://cloud.githubusercontent.com/assets/592286/18147715/d5251c1e-6fcd-11e6-9b95-b72d80c05051.png)\n- [x] General\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\n  - The pull request addresses only one issue\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\n- [x] Documentation\n  - Documentation has been added for new functionality\n  - Old documentation affected by the pull request has been updated\n  - JavaDoc for public methods has been added\n- [x] Tests & Build\n  - Functionality added by the pull request is covered by tests\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2431", "title": "[FLINK-4521][web frontend] Fix \"Submit new Job\" panel in development mode", "body": "Submit panel was completely empty in the development mode. This happened because client-side code sent AJAX requests to a development HTTP server (`localhost:3000`) and not to the the JobServer (`localhost:8081`) and development server rejected all AJAX requests.\n\nTo fix the issue I did the following:\n- Updated `server.js` to pass both GET and POST requests to the JobServer (`localhost:8081`)\n- Changed all AJAX requests to use development HTTP server\n- Added a helper function to build a correct URL for both dev and prod modes\n\nAlso, it seems that CoffeeScript and JavaScript code were out of sync in the `master`, so this PR contains a significant number of changes to `index.js`.\n- [x] General\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\n  - The pull request addresses only one issue\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\n- [x] Documentation\n  - Documentation has been added for new functionality\n  - Old documentation affected by the pull request has been updated\n  - JavaDoc for public methods has been added\n- [x] Tests & Build\n  - Functionality added by the pull request is covered by tests\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/18789946", "body": "Hi Stephan.\n\nI think you are right.\nI'll update the testing as soon as possible and create a different PR to address your comment.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18789946/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "NickolayVasilishin": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3040", "title": "[FLINK-3850] Add forward field annotations to DataSet ", "body": "Add forward field annotations to DataSet operators generated by the Table API\r\n\r\n  - Added field forwarding at most of `DataSetRel` implementations.\r\n  - String with forwarded fields allowed to be empty at `SemanticPropUtil.java`\r\n  - Wrapper for indices based on types moved to object class `FieldForwardingUtils`\r\n  - In most cases forwarding done only for conversion\r\n\r\n   `BatchScan`: forwarding at conversion\r\n   `DataSetAggregate`: forwarding at conversion\r\n   `DataSetCalc`: forwarding based on unmodified at RexCalls operands\r\n   `DataSetCorrelate`:  forwarding based on unmodified at RexCalls operands\r\n   `DataSetIntersect`:  forwarding at conversion\r\n   `DataSetJoin`: forwarding based on fields which are not keys\r\n   `DataSetMinus`: forwarding at conversion\r\n   `DataSetSingleRowJoin`: forwarded all fields from multi row dataset, single row used via broadcast\r\n   `DataSetSort`: all fields forwarded + conversion\r\n\r\nI hope, I've understood the meaning of forward fields right: fields, that are not used for computations. So I assumed, that these fields are not used in `RexCalls` or as `join keys`. Also I forwarded fields in type conversions.\r\nThe most complex thing was to determine correct input and output field names.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tonycox": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/3012", "title": "[FLINK-2186] Add readCsvAsRow methods to CsvReader and scala ExecutionEnv", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [ ] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n\r\nRework CSV import to support very wide files", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hzyuemeng1": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2994", "title": "[FLINK-5324] [yarn] JVM Opitons will work for both YarnApplicationMas\u2026", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [ ] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n\r\n\u2026terRunner and YarnTaskManager with yarn mode", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bitchelov": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2941", "title": "[FLINK-3555] Web interface does not render job information properly", "body": "**Before:** When a window is minimized, the content turned out to be beyond\r\n![before](https://cloud.githubusercontent.com/assets/5806063/21186242/eab7aaa2-c224-11e6-9bab-c3a60466110b.gif)\r\n\r\n**After::shipit:** \r\n\r\n![after](https://cloud.githubusercontent.com/assets/5806063/21183431/e8f1fc10-c218-11e6-9447-350fad88da49.gif)\r\n\r\n\r\n\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "joseprupi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2885", "title": "[FLINK-1707] Affinity propagation", "body": "This is the first commit of the bulk implementation for the Affinity Propagation algorithm. The algorithm calculations seem to be ok for the example in the pull request (it has been compared with the scikit implementation). There are some items pending and just doing the pull request to follow up the implementation and to review the graph.\r\n\r\nThe graph for the implmentation:\r\n\r\nhttps://docs.google.com/drawings/d/1PC3S-6AEt2Gp_TGrSfiWzkTcL7vXhHSxvM6b9HglmtA/edit?usp=sharing\r\n\r\nStill pending:\r\n\r\n- Convergence\r\n- Greg suggestions:\r\n\r\n* Generic label type instead of Long.\r\n\r\n* Line 37, the join of similarities and messages (availabilities), we can perform a combinable reduce for the top 2 scores (and top label) rather than fully sorting all scores. There is an outstanding ticket, FLINK-2549, to add a topK operator. I expect that with k = 2 a custom implementation would be faster than topK implemented with a min-heap.\r\n\r\n* Enable object reuse, reuse objects, and use LongValue and DoubleValue rather than Long and Double. Rather than \"return new ...\" we can create the object as a class field and then update and return this object in the function.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Renkai": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2847", "title": "[FLINK-5031]Consecutive DataStream.split() ignored", "body": "I think this is a way to solve this issue, but might not be the best one.Since I'm knowing the code base enough, I hope someone may review it and give some advice.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2729", "title": "[FLINK-4883]Prevent UDFs implementations through Scala singleton objects", "body": "The code changes should be work, but I was quite a lot confused for these reasons\r\n\r\n\t1. The Scala wrapper for DataStream and DataSet do  almost the same thing, but they have a lot of differences in code style,name rule and code structures.\r\n\t2. Some of the DataSet operators are surrounded by clean function, while some else are not, I think they might they might be forgotten to add.\r\n\t3. Scala and Java have different version of  ClosureCleaner,I think one ClosureCleaner should be enough, or the Java version of ClosureCleaner be a basic version, Scala version can call the Java version directly and add some additional logics. \r\n\r\nI think the Scala wrapper of DataSet could be rewrite to make it harmonic with the wrapper of DataStream, maybe we can create another issue ticket to do it.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gaborhermann": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2838", "title": "[FLINK-4712] [FLINK-4713] [ml] Ranking recommendation & evaluation (WIP)", "body": "Please note that this is a work-in-progress PR for discussing API design decisions. We propose here a class hierarchy for fitting the ranking evaluations into the proposed evaluation framework (see [PR](https://github.com/apache/flink/pull/1849)).\r\nThe features are mostly working, but documentation is missing and minor refactoring is needed. The evaluations currently work with top 100 rankings (burnt-in), and we still need to fix that. We need feedback for two main solutions, so we can go on with the PR. Thanks for any comment!\r\n\r\n### `RankingPredictor`\r\n\r\nWe have managed to rework the evaluation framework proposed by @thvasilo, so that ranking predictions would fit in. Our approach is to use separate `RankingPredictor` and `Predictor` traits. One main problem however remains: there is no common superclass for `RankingPredictor` and `Predictor` so the pipelining mechanism might not work. A `Predictor` can only be at the and of the pipeline, so this should not really be a problem, but I do not know for sure. An alternative solution would be to have different objects `ALS` and `RankingALS` that give different predictions, but both extends only a `Predictor`. There could be implicit conversions between the two. I would prefer the current solution if it does not break the pipelining. @thvasilo what do you think about this?\r\n\r\n(This seems to be a problem similar to having a `predict_proba` function in scikit learn classification models, where the same model for the same input gives two different predictions: a `predict` for discrete predictions and `predict_proba` for giving a probability.)\r\n\r\n### Generalizing `EvalutateDataSetOperation`\r\n\r\nOn the other hand, we seem to have solved the scoring issue. The users can evaluate a recommendation algorithm such as ALS by using a score operating on rankings (e.g. nDCG), or a score operating on ratings (e.g. RMSE). They only need to modify the `Score` they use in their code, and nothing else.\r\n\r\nThe main problem was that the evaluate method and `EvaluateDataSetOperation` were not general enough. They prepare the evaluation to `(trueValue, predictedValue)` pairs (i.e. a `DataSet[(PredictionType, PredictionType)]`), while ranking evaluations needed a more general input with the true ratings (`DataSet[(Int,Int,Double)]`) and the predicted rankings (`DataSet[(Int,Int,Int)]`).\r\n\r\nInstead of using `EvaluateDataSetOperation` we use a more general `PrepareOperation`. We rename the `Score` in the original evaluation framework to `PairwiseScore`. `RankingScore` and `PairwiseScore` has a common trait `Score`. This way the user can use both a `RankingScore` and a `PairwiseScore` for a certain model, and only need to alter the score used in the code.\r\n\r\nIn case of pairwise scores (that only need true and predicted value pairs for evaluation) `EvaluateDataSetOperation` is used as a `PrepareOperation`. It prepares the evaluation by creating `(trueValue, predicitedValue)` pairs from the test dataset. Thus, the result of preparing and the input of `PairwiseScore`s will be `DataSet[(PredictionType,PredictionType)]`. In case of rankings the `PrepareOperation` passes the test dataset and creates the rankings. The result of preparing and the input of `RankingScore`s will be `(DataSet[Int,Int,Double], DataSet[Int,Int,Int])`. I believe this is a fairly acceptable solution that avoids breaking the API.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2819", "title": "[FLINK-4961] [ml] SGD for Matrix Factorization (WIP)", "body": "Please note, that this is a work-in-progress PR, to discuss some design questions. There are minor things to be done including the documentation (Scala docs are done). Apart from these and the questions worth discussing the PR is ready.\r\n\r\nSome notes:\r\n- Generalized matrix factorization methods into `MatrixFactorization` abstract class (this slightly modifies `ALS`).\r\n- The algorithm could be executed in parts with `MLTools.persist`, just like in `ALS` (to use less memory).\r\n- The algorithm uses random block ID initialization, and shuffles also the data when doing the updates. However, the algorithm can be made deterministic by setting a seed.\r\n- The objective function is simply squared loss with L2 regularization in contrast to `ALS`s weighted-lambda-regularization. This could be extended later to use other regularization methods too, as SGD is more flexible in terms of loss functions.\r\n- The same methods could be used for dynamically changing the learning rate as in the `GradientDescent` implementation.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2542", "title": "[FLINK-4613] [ml] Extend ALS to handle implicit feedback datasets", "body": "This extension of the ALS algorithm changes some parts of the code if `implicitPrefs` flag is set to true. Mainly the local parts parts are changed: the `Xt * X` computation takes into consideration the confidence, thus computing `Xt * (C - I) * X` instead (see the paper by Hu et al. for details). The `Xt * X` matrix is precomputed and broadcasted, and that is the only thing that affects distributed execution.\n\nNote, that we use a temporary directory in the test, because there would not be enough memory segments to perform a hash join for prediction. I assume that memory segments are not freed up after the training if no temporary directory is set, but I did not investigate the issue as using a tempdir is a simple workaround.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zhuhaifengleon": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2833", "title": "[FLINK-5103] [Metrics] TaskManager process virtual memory and physical memory used size gauge", "body": "This PR add TaskManger process virtual memory and physical memory used size gauge metrics.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2753", "title": "[FLINK-4840] [metrics] Measure latency of record processing and expose it as a metric", "body": "This PR introduce record processing time metric for measuring a task running performance. The latency is processing time cost of all chained operator for a task.\r\nfollowing Metrics on the TaskIOMetricGroup:\r\n1. recordProcessLatency(ms): Histogram measuring the processing time per record of a task. It is the processing time of chain if a chained task. \r\n2. recordProcTimeProportion(ms): Meter measuring the proportion of record processing time for infor whether the main cost", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gyfora": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2796", "title": "[FLINK-4873] Configurable ship path for YARN cluster resources", "body": "This PR makes the resource shipping directory configurable for YARN clusters by adding a new config option:  `yarn.resource.ship.path`\r\n\r\nCurrently the HDFS home directory is used by default which makes it practically impossible to use the YARN cluster without HDFS (which is actually quite feasible if you have a different distributed file system).", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tfournier314": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2740", "title": "[FLINK-4964] [ml] <Implement StringIndexer>", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\r\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\r\nIn addition to going through the list, please provide a meaningful description of your changes.\r\n\r\n- [ ] General\r\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\r\n  - The pull request addresses only one issue\r\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\r\n\r\n- [ ] Documentation\r\n  - Documentation has been added for new functionality\r\n  - Old documentation affected by the pull request has been updated\r\n  - JavaDoc for public methods has been added\r\n\r\n- [ ] Tests & Build\r\n  - Functionality added by the pull request is covered by tests\r\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\r\n\r\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2684", "title": "[FLINK-4865] [ml] Add EvaluateDataSet operation for LabeledVector", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\nIn addition to going through the list, please provide a meaningful description of your changes.\n- [ ] General\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\n  - The pull request addresses only one issue\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\n- [ ] Documentation\n  - Documentation has been added for new functionality\n  - Old documentation affected by the pull request has been updated\n  - JavaDoc for public methods has been added\n- [ ] Tests & Build\n  - Functionality added by the pull request is covered by tests\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kalmanchapman": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2735", "title": "[FLINK-2094] [ml] implements Word2Vec for FlinkML", "body": "This pr implements Word2Vec for FlinkML - addressing Jira Issue [Flink-2094](https://issues.apache.org/jira/browse/FLINK-2094)\r\n\r\nWord2Vec is a word embedding algorithm that generates vectors\r\nto reflect the contextual and semantic values of words in a text.\r\n\r\nfind out more detail about word2vec here:\r\nhttps://arxiv.org/pdf/1411.2738v4.pdf\r\n\r\nThis implementation uses an abstracted embedding algorithm\r\nwhich I've called a ContextEmbedder - based on the original Word2Vec algorithms - \r\nto allow users to extend embedding to reflect problems outside of words.\r\nWord2Vec is an implementation of the ContextEmbedder", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mxm": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2732", "title": "[FLINK-4272] Create a JobClient for job control and monitoring", "body": "Also includes: [FLINK-4274] Expose new JobClient in the DataSet/DataStream API\r\n\r\n- rename JobClient class to JobClientActorUtils\r\n\r\n- introduce JobClient interface with two implementations\r\n\r\n  - JobClientEager: starts an actor system right away and monitors the job\r\n    - Move ClusterClient#cancel, ClusterClient#stop,\r\n      ClusterClient#getAccumulators to JobClient\r\n\r\n  - JobClientLazy: starts an actor system when requests are made by\r\n    encapsulating the eager job client\r\n\r\n- Java and Scala API\r\n  - JobClient integration\r\n  - introduce ExecutionEnvironment#executeWithControl()\r\n  - introduce StreamExecutionEnvironment#executeWithControl()\r\n\r\n- report errors during job execution as JobExecutionException instead of\r\n  ProgramInvocationException and adapt test cases\r\n\r\n- provide finalizers to run code upon shutdown of client\r\n\r\n- use ActorGateway in JobListeningContext\r\n\r\n- add test case for JobClient implementations", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/12961579", "body": "Thanks for noticing. This requires some refactoring because the test depends on a method in `CommonTestUtils`. Is it desired to have a CommonTestUtils clases in each runtime and core? As far as I can see the methods are not runtime or core-specific, so the two can be combined into one class residing in core.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/12961579/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/12974542", "body": "@StephanEwen Do you think #1081 is feasible?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/12974542/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/14181408", "body": "HI Alexey, \n\nYes, these changes are in the newest SNAPSHOT. You might have to update your Maven cache.\n\nCheers,\nMax \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14181408/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/14239173", "body": "Are you still having this problem? The newest flink-storm version as of today is from Nov 06.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14239173/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/14240638", "body": "You're welcome.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14240638/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/14333154", "body": "\\n is not compatible with a non Unix environment.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14333154/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/14385157", "body": "Adjusted in 9a911c88ec4b11300d72f41a7c51569e1d228a87.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14385157/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/14475546", "body": "@tillrohrmann @alexeyegorov Actually there are 1.0-SNAPSHOT binaries available. See http://flink.apache.org/contribute-code.html#snapshots-nightly-builds\n\nPlease keep in mind that these are targeted at developers and not officially released.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14475546/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/14475824", "body": "Seems like the 1.0-SNAPSHOT binaries are missing. Looking into the problem right now.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14475824/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/14559720", "body": "You can now get the latest nightly binaries here: http://flink.apache.org/contribute-code.html#snapshots-nightly-builds\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14559720/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15702724", "body": "That would be great if you could do that. I just wrote an email to the user and dev mailing list.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15702724/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15704256", "body": "Thanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15704256/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/17643795", "body": "@StephanEwen I think this has been left there on purpose because the ExecutionConfig is declared Public and this would break the backwards-compatibility.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/17643795/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/17643812", "body": "@StephanEwen Why does this change touch critical parts of the system without a jira issue? Plus, the commit message is not even related to these changes.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/17643812/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/17643841", "body": "This breaks the master and ignores the PR #2037.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/17643841/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/17644775", "body": "No worries, that stuff can happen. Fixing this with #2037 (underlying issue why this bug could occur).\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/17644775/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/17936226", "body": "It does work fine for most cases. However, there are rare race conditions which render our CI useless.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/17936226/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/18829963", "body": "Thanks. I've considered that option but I didn't want to change the semantic of the code which copies the values.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/18829963/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19436429", "body": "Thanks for the notice! Fixed.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19436429/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "philippbussche": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2683", "title": "[FLINK-4888][metrics] instantiated job manager metrics missing important job statistics", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jaxbihani": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2568", "title": "[FLINK-4636] Add boundary check for priorityqueue for cep operator", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\nIn addition to going through the list, please provide a meaningful description of your changes.\n- [ ] General\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\n  - The pull request addresses only one issue\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\n- [ ] Documentation\n  - Documentation has been added for new functionality\n  - Old documentation affected by the pull request has been updated\n  - JavaDoc for public methods has been added\n- [ ] Tests & Build\n  - Functionality added by the pull request is covered by tests\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\n\nWhen numberPriorityQueueEntries=0, creation of priority queue object\nfails as its constructor throws exception when size is passed as 0.\nWe check for this condition and skip creating object as it doesn't serve\nany purpose in that case.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "smarthi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2422", "title": "FLINK-4499: [WIP] Introduce findbugs maven plugin", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\nIn addition to going through the list, please provide a meaningful description of your changes.\n- [X] General\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\n  - The pull request addresses only one issue\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\n- [ ] Documentation\n  - Documentation has been added for new functionality\n  - Old documentation affected by the pull request has been updated\n  - JavaDoc for public methods has been added\n- [X] Tests & Build\n  - Functionality added by the pull request is covered by tests\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\n", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/14182299", "body": "Consider using Google Calipers for micro benchmarking ?? - https://github.com/google/caliper  \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14182299/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "delding": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2332", "title": "[FLINK-2055] Implement Streaming HBaseSink", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\nIn addition to going through the list, please provide a meaningful description of your changes.\n- [x] General\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\n  - The pull request addresses only one issue\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\n- [x] Documentation\n  - Documentation has been added for new functionality\n  - Old documentation affected by the pull request has been updated\n  - JavaDoc for public methods has been added\n- [x] Tests & Build\n  - Functionality added by the pull request is covered by tests\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cresny": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2288", "title": "Feature/s3 a fix", "body": "Unlike the HDFS and S3n FileSystem implementations, the S3a FileSystem does not implement a recursive copyFromLocal. Because of this, both semi-async RocksDB snapshots (https://issues.apache.org/jira/browse/FLINK-4228) and Yarn application staging fail if the configured FileSystem is S3a. This fix works around the problem by pulling the recursion out to Flink instead of making any assumptions about FileSystem implementation.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "doflink": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2267", "title": "[FLINK-4205] Create a simple stratified sampling function for DataSet", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\nIn addition to going through the list, please provide a meaningful description of your changes.\n- [ ] General\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\n  - The pull request addresses only one issue\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\n- [ ] Documentation\n  - Documentation has been added for new functionality\n  - Old documentation affected by the pull request has been updated\n  - JavaDoc for public methods has been added\n- [ ] Tests & Build\n  - Functionality added by the pull request is covered by tests\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Xazax-hun": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2211", "title": "[WIP][FLINK-3599] Code generation for PojoSerializer and PojoComparator", "body": "The current implementation of the serializers can be a\nperformance bottleneck in some scenarios. These performance problems were\nalso reported on the mailing list recently [1].\n\nE.g. the PojoSerializer uses reflection for accessing the fields, which is slow [2].\n\nFor the complete proposal see [3].\n\nThis pull request implements code generation support for PojoComparators and PojoSerializers. On my machine I could measure about 10% performance improvements for the WordCountPojo example. This pull request does not implement distribution of the generated code to the task managers yet.\n\n[1] http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/Tuple-performance-and-the-curious-JIT-compiler-td10666.html\n\n[2] https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/PojoSerializer.java#L369\n\n[3] https://docs.google.com/document/d/1VC8lCeErx9kI5lCMPiUn625PO0rxR-iKlVqtt3hkVnk\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/1769", "title": "[FLINK-3322] MemoryManager creates too much GC pressure with iterative jobs.", "body": "This fix uses the approach suggested by G\u00e1bor G\u00e9vay on the mailing list: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/Memory-manager-behavior-in-iterative-jobs-tt10066.html\n\nInstead of making the GC to free the segments, they are returned to the pool and allocation of new segments are modified to work accordingly. Note that, the size of the pool is never decreasing, but right now that is unlikely to cause any trouble. If once it is desired to decrease the size of the pool dynamically soft references can be used.\n\nSome benchmarks using the new test case:\npreAllocateMemory == false\nBefore the patch:  7s with 500m heap, 55s with 5000m heap\nAfter the patch: 5s with 500m heap, 8s with 5000m heap\n\npreAllocateMemory == true\nBefore the patch: 4s with 500m heap, 8s with 5000m heap\nAfter the patch: 5s with 500m heap, 8s with 5000m heap\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vpernin": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2192", "title": "Flink 4034 Maven dependency convergence", "body": "https://issues.apache.org/jira/browse/FLINK-4034\nGreen build : https://travis-ci.org/vpernin/flink/builds/141717643\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "fobeligi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2178", "title": "[Flink-1815] [gelly] Add methods to read and write a Graph as adjacency list", "body": "Added functionality for Jira Issue [FLINK-1815] Add methods to read and write a Graph as adjacency list\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chobeat": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2152", "title": "[FLINK-3920] Distributed Linear Algebra: block-based matrix", "body": "Second part of the distributed linear algebra contribution. This  PR introduces block-partitioned matrices, operations on them (multiplication, per-row operations) and conversions from and to row-partitioned matrices.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alkagin": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2149", "title": "[FLINK-4084] Add configDir parameter to CliFrontend and flink shell script", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\nIn addition to going through the list, please provide a meaningful description of your changes.\n- [ X] General\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\n  - The pull request addresses only one issue\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\n- [X ] Documentation\n  - Documentation has been added for new functionality\n  - Old documentation affected by the pull request has been updated\n  - JavaDoc for public methods has been added\n- [X ] Tests & Build\n  - Functionality added by the pull request is covered by tests\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\n\nModified flink shell script and CliFrontend to provide an option `configDir` to specify a custom configuration folder. \n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thormanrd": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2114", "title": "FLINK-3839 Added the extraction of jar files from a URL spec that is \u2026", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\nIn addition to going through the list, please provide a meaningful description of your changes.\n- [ ] General\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\n  - The pull request addresses only one issue\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\n- [ ] Documentation\n  - Documentation has been added for new functionality\n  - Old documentation affected by the pull request has been updated\n  - JavaDoc for public methods has been added\n- [ ] Tests & Build\n  - Functionality added by the pull request is covered by tests\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\n\n\u2026a directory in the form of file:///path/to/jars/<*>\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rvdwenden": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2070", "title": "[FLINK-4016] initialize FoldApplyWindowFunction properly", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "danielblazevski": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2052", "title": "[FLINK-3996] Add addition, subtraction and multiply by scalar to DenseVector.scala and SparseVector.scala", "body": "Small change to add vector operations.  With this small change, can now do things like:\n\n``` scala\nval v1 = DenseVector(0.1, 0.1)\nval v2 = DenseVector(0.2, 0.2)\nval v3 = v1 + v2\n```\n\ninstead of what is now has to be done:\n\n``` scala\nval v1 = DenseVector(0.1, 0.1)\nval v2 = DenseVector(0.2, 0.2)\nval v3 = (v1.asBreeze + v2.asBreeze).fromBreeze\n```\n\nDid not add a test, not sure if I should add a test to any Suite for such a small change?  There is no JIRA issue on this. @chiwanpark \n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/2050", "title": "[FLINK-1934] Add approximative k-nearest-neighbours (kNN) algorithm to machine learning library", "body": "I added approximate knn algorithms. In another PR, there are two exact methods, one basic algorithms using a prirority queue and another using a quadtree (see: #1220 ).\n\nFor this PR, I added z-value based knn and LSH (Locality Sensitive Hashing) based knn. Z-values are good for low-to-moderate dimension. For details, see the paper [2] someone put on the exact JIRA issue: \nhttps://issues.apache.org/jira/browse/FLINK-1745\nhttps://www.cs.utah.edu/~lifeifei/papers/mrknnj.pdf\n\nThe z-value approach isn't applicable for larger dimensions, so I used -- as the paper suggests -- a more standard LSH approach.\n\nThe paper describes a fairly sophisticated MapReduce (MR) design, which I did not use. Using the same MR design pattern as the exact method, I found really good performance improvement! In JIRA, I ran this by @tillrohrmann, and he was OK with a less optimized version for now. Here is a link for a talk I recently gave on this, which includes links for the video and slides:\nhttp://www.meetup.com/ny-scala/events/231163636/\n\nBecause both the LSH and z-value use the same MR design pattern as the exact versions, I reformatted the codebase from the PR for exact version a bit to make it more modular.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mans2singh": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/2031", "title": "FLINK-3967 - Flink Sink for Rethink Db", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\nIn addition to going through the list, please provide a meaningful description of your changes.\n- [ ] General\n  - The pull request references the related JIRA issue (\"[FLINK-XXX] Jira title text\")\n  - The pull request addresses only one issue\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\n- [ ] Documentation\n  - Documentation has been added for new functionality\n  - Old documentation affected by the pull request has been updated\n  - JavaDoc for public methods has been added\n- [ ] Tests & Build\n  - Functionality added by the pull request is covered by tests\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gna-phetsarath": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/1990", "title": "[FLINK-3655] Multiple File Paths for InputFileFormat.", "body": "I had to create a new PR, because I messed up my branches.\n\nThis addresses [FLINK-3655] Multiple File Paths for InputFileFormat but does not implement file name globbing.\n\nAlso, this branch does not use guava.\n\nThanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\nIn addition to going through the list, please provide a meaningful description of your changes.\n- [x] General\n  - The pull request references the related JIRA issue (\"[FLINK-3655] Multiple File Paths for InputFileFormat.\")\n  - The pull request addresses only one issue\n  - Each commit in the PR has a meaningful commit message (including the JIRA id)\n- [x] Documentation\n  - Documentation has been added for new functionality\n  - Old documentation affected by the pull request has been updated\n  - JavaDoc for public methods has been added\n- [x] Tests & Build\n  - Functionality added by the pull request is covered by tests\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\n\nRemoved Guava.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sbcd90": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/1962", "title": "[FLINK-3857][Streaming Connectors]Add reconnect attempt to Elasticsearch host", "body": "- [x] General\n  - The pull request references the related JIRA issue (\"[FLINK-3857] Add reconnect attempt to Elasticsearch host\")\n- [x] Documentation\n  - Documentation added based on the changes made.\n- [x] Tests & Build\n  - Functionality added by the pull request is covered by tests\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gaoyike": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/1924", "title": "[FLINK-3802] Add Very Fast Reservoir Sampling", "body": "Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.\nIf your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the [How To Contribute guide](http://flink.apache.org/how-to-contribute.html).\nIn addition to going through the list, please provide a meaningful description of your changes.\n- [x] General\n  - The pull request references the related JIRA issue\n  - The pull request addresses only one issue\n  - Each commit in the PR has a meaningful commit message\n- [x] Documentation\n  - Documentation has been added for new functionality\n  - Old documentation affected by the pull request has been updated\n  - JavaDoc for public methods has been added\n- [ ] Tests & Build\n  - Functionality added by the pull request is covered by tests\n  - `mvn clean verify` has been executed successfully locally or a Travis build has passed\n\nA in memory implementation of Very Fast Reservoir Sampling, the algorithm works well then the size of streaming data is much larger than size of reservoir.\n\n  The algorithm runs in random sampling with P(R/j) where in R is the size of sampling and j is the current index of streaming data.\n  The algorithm consists of two part:\n    (1) Before the size of streaming data reaches threshold, it uses regular reservoir sampling\n    (2) After the size of streaming data reaches threshold, it uses geometric distribution to generate the approximation gap\n        to skip data, and size of gap is determined by  geometric distribution with probability p = R/j\n\n   Thanks to Erik Erlandson who is the author of this algorithm and help me with implementation.\n\nReference: http://erikerlandson.github.io/blog/2015/11/20/very-fast-reservoir-sampling/\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thvasilo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/1849", "title": "[FLINK-2157] [ml] Create evaluation framework for ML library", "body": "Using this PR instead of #871 due to rebase issues.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/11408683", "body": "Just noticed: The example still uses CoCoA() instead of SVM()\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/11408683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/11537562", "body": "Example should be program, cannot currently perform in the shell\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/11537562/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/11537565", "body": "TODO: Need to transform the tuples to LabeledVector\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/11537565/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/11537784", "body": "This way of reading in CSVs can get unwieldy fast. We need a more concise way to do this,\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/11537784/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "fs111": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/1694", "title": "added support for Main-Class separated by / instead of .", "body": "I ran into this while trying something out with cascading-flink:\n\nThe Main-Class in a jar manifest may be written as \"package/subpackage/SomeClass\", which confuses the flink runner, since it assumes that classes are always written with \".\" as the separator.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "senorcarbone": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/1668", "title": "[FLINK-3257] Add Exactly-Once Processing Guarantees for Iterative DataStream Jobs", "body": "# **Motivation and Algorithm**\n\nThis is a first version of the adapted snapshot algorithm to support iterations. It is correct and works in practice...well, when memory capacity is enough for its logging requirements but I am working on that, hopefully with a little feedback from you. Before we go into the implementation details let me describe briefly the new algorithm.\n## Algorithm\n\nOur existing checkpoint algorithm has a very fluid and straightforward protocol. It just makes sure that all checkpoint barriers are aligned in each operator so that all records before barriers (pre-shot) are processed before taking a snapshot. Since waiting indefinitely for all records in-transit within a cycle of an execution graph can violate termination (crucial liveness property) we have to...save any unprocessed records for later and get done with the snapshot. In this take of the algorithm on Flink we assign that role to the `Iteration Head`. The steps this version varies from the vanilla algorithm are simply the following:\n\n(1) An `Iteration Head` receives a barrier from the system runtime (as before) and:\n-  Goes into **Logging Mode**. That means that from that point on every record it receives from its `Iteration Sink` is buffered in its own operator state (log) and **not** forwarded further until it goes back to normal mode.\n- **Forwards** the barrier to its downstream nodes (this guarantees liveness, otherwise we have a deadlock).\n\n(2) Eventually, the `Iteration Head` receives a barrier back from its `Iteration Sink`. At that point:\n- It **checkpoints** the log as its operator state.\n- Flushes to its outputs all pending records from the log and resets it.\n- Goes back to _normal_ forwarding mode.\n\n(3) When the `Iteration Head` starts/restarts it looks at its initial operator state (the log) and flushes any records that are possibly pending.\n## Example\n\nThis is just a very simple example topology. We have a single source **S** and a mapper **M** within an iteration with a head **H** and tail **T**. This is the bare minimum we need for now to check how this works.\n\n![ftloops-topology](https://cloud.githubusercontent.com/assets/858078/13151679/7f150538-d66b-11e5-98c8-7bbe2243b810.png)\n\nIn the diagram below you can see the sequence of possible events, containing both barrier and record transmissions. For completeness I included the `Runtime` as a separate entity, this is in our case the Checkpoint Coordinator who periodically initialises checkpointing in all tasks without inputs. \n\nThe point that this diagram tries to make is the following:\nRecord **R1** (or any record received in _normal mode_) gets forwarded to **M**, the only consumer of **H** before the checkpoint barrier. On the other hand, **R2** is not forwarded to **M** until **H** has finished snapshotting (or during the same atomic block anyway). In case of a failure **R2** is not lost, rather than saved for later. \n\n![diagram](https://cloud.githubusercontent.com/assets/858078/13151664/7361a638-d66b-11e5-94e9-64f70a8130d7.png)\n\nA very brief description of a consistent/correct snapshot in our context could be summed up in the following sentence:\n\n> Every record prior to a snapshot has been either processed by an operator (state **depends** on record) or **included** in the snapshot as event in transit (see Chandy Lamport algorithm). \n\nThis algorithm guarantees this property and as explained earlier, it also terminates.\n## Current Implementation Details\n\nIn the current prototype I tried to keep changes to a minimum until we agree on the pending issues (see below) so there is plenty of room for improvement, engineering-wise. The important changes are the following:\n- Obviously there is no more checking for iterations during a fault tolerant `StreamGraph` construction.\n- the `triggerCheckpoint` function of `StreamTask` was split (please do not freak out). That was necessary in order to extract the checkpointing logic from the barrier handling logic (which should be obviously decoupled in the updated algorithm). The actual state snapshotting now happens inside `checkpointStatesInternal`.\n- The `StreamIterationTail` has to process and forward `Either<StreamRecord<IN>, CheckpointBarrier>` objects to `StreamIterationHead` which takes the necessary steps depending on the event according to the algorithm. An _internal_ task abstraction `ForwardingOneInputStreamTask` was introduced for forwarding barriers to the tail. \n- The `StreamIterationHead` uses a simple `UpstreamLogger` operator internally for its logging needs, nothing special about it.\n- A `StreamIterationCheckpointingITCase` was added that checks exactly-once guarantees, loops included. More tests will follow upon demand.\n\nI am sure we can improve this implementation but let's focus on the non-trivial problems first.\n## Open/Pending Issues\n\nDuring stress-testing I found the following issues that we need to discuss hoping that some of you are maybe eager to propose something.\n- I realised that while flooding my cyclic topology with records I was still **losing** data, even at the **absence of task failures**. The problem seemed to be the way we transferring events from the `StreamIterationTail` to the `Head`. Apparently the tail **offers** records to the head with a specified  timeout on a blocking queue of size 1(!?). What is going on there? The Head makes sense to timeout on reading, but, why does the `StreamIterationTail` throw away events like there is no tomorrow? \n  Since at-most-once processing guarantees are not what we want I just commented out the _offer_ part until I understand why it is there. I think it is safe (deadlock-wise) to simply **put** records to the queue, right?...I can also fire a new JIRA issue reporting this if needed.\n- I feel that we really need to spill log state to disk (`BufferSpiller`?), or/and restrict iterative jobs to out-of-core backend and forbid in-memory. The upstream log size can very easily pass the boundaries of the allowed in-memory state (e.g. when millions records in transit are snapshotted). Any ideas on that are welcome!\n\nThanks a lot for looking into this. Really looking forward to your suggestions.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "f-sander": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/1565", "title": "[FLINK-3128] [flink-ml] Add Isotonic Regression To ML Library", "body": "Adds isotonic regression to the ml library.\nIt's a port of the implementation in Apache Spark.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wangyangjun": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/1527", "title": "[FLINK-3109]Join two streams with two different buffer time -- Java i\u2026", "body": "Java implementation of jira [FLINK-3109](https://issues.apache.org/jira/browse/FLINK-3109)\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "HilmiYildirim": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/1350", "title": "[Flink-3007] Implemented a parallel version of Hidden Markov Models. It uses a par\u2026", "body": "Implemented a parallel version of Hidden Markov Models. It uses a parallel version of the Baum-Welch algorithm to train the model and uses the Viterbi algorithm to predict the sequence labels.\n\nThe prediction quality of my examples is low. The reason for this is probably the small size of the model prameters\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ChengXiangLi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/1161", "title": "[FLINK-2549] add topK operator to DataSet.", "body": "The topK operator is implemented with `mapPartition()` followed by `reduceGroup()`, each map tasks select top k elements, and transfer to singleton reduce task(no `group()` before `reduceGroup()`), and then select top k elements in reduce task. \nThe main part of this implementation:\n1. An out-of-core self managed memory based PriorityQueue implementation.\n2.  Use runtime resources to build PriorityQueue for UDF. topK is not real 'native' operator actually, it contains UDF which use self managed memory based PriorityQueue to select top k elements.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "HuangWHWHW": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/1157", "title": "[FLINK-2720][storm-compatibility]Add Storm-CountMetric for storm metrics", "body": "I added the Storm-CountMetric for the first step about storm metrics.\n1.Do a wrapper `FlinkCountMetric` for the CountMetric.\n2.There is a real metric LongCounter in FlinkCountMetric class.\n3.Push the RuntimeContext in `FlinkTopologyContext` for registering the metric.\n4.Add a simple ut for the  `FlinkCountMetric` .\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "MoeweX": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/1156", "title": "[FLINK-1719] [ml] Add Multinomial Naive Bayes Classification", "body": "This pull request is related to [FLINK-1719](https://issues.apache.org/jira/browse/FLINK-1719).\nMultinomial Naive Bayes was successfully implemented @tillrohrmann and different ideas proposed by other authors were incorporated.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/46086799", "body": "Hi, thank you for your time ... . \nI checked it and found out that the json file created by the system is corrupted\n![bildschirmfoto 2014-06-14 um 14 33 55](https://cloud.githubusercontent.com/assets/5738978/3278453/5d1245f2-f3c0-11e3-80b6-65f773df18ce.png)\nThe comma under the step-function line is wrong. That is why jquery does not recognize this file as a json and the ajax request ends with an error. I will try to fix it, but I wonder how the old webclient is able to handle this, because the error occures in code I did not changed.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/46086799/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/46086894", "body": "By the way, the tool is able to work with the json, when you don't use the ajax request (where jquery checks whether it is a json):\n![bildschirmfoto 2014-06-14 um 14 40 35](https://cloud.githubusercontent.com/assets/5738978/3278469/3d3bb5d2-f3c1-11e3-9149-2be4082b05af.png)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/46086894/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50474736", "body": "@StephanEwen, @rmetzger : this should be merged before the release because it improves the webclient a lot.\n\nThe zoom feature is not perfect, because it pans the view a little bit to the left. I was not able to fix this, but it is better than no zooming without a mouse (i spend quit a lot of time on this :/) ... . The old zoom functionality is still working.\n\nRelated issues from JIRA (may be incomplete): \nhttps://issues.apache.org/jira/browse/FLINK-954\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50474736/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50509106", "body": "ok, I am fine with everything ... . So I will continue to integrate more improvements ... . \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50509106/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/12972151", "body": "Hi Till,\n\nas reported by me and @FelixNeutatz: the stop-script has the same error...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/12972151/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/14324168", "body": "If we do 10 seconds those heartbeats would be wrong ... . \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14324168/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "asfbot": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/45938921", "body": "Robert Metzger  on dev@flink.incubator.apache.org replies:\nTesting Github <--> ML integration.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45938921/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/46052624", "body": "Ufuk Celebi  on dev@flink.incubator.apache.org replies:\n+1\n\nBut imo this will soon be subsumed by the upcoming RPC rework.\n\nSent from my iPhone\nl/14 we should not limit the heap space of the job submission client to 512M=\nB.\nsize(DataOutputSerializer.java:243)\nite(DataOutputSerializer.java:87)\nJobGraph.java:706)\n1)\nava:469)\nent.java:258)\ntEnvironment.java:50)\njava:79)\nImpl.java:62)\nAccessorImpl.java:43)\nckagedProgram.java:384)\nModeForExecution(PackagedProgram.java:302)\nva:327)\nava:927)\nent_fix\nr\ne\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/46052624/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/46053094", "body": "Robert Metzger  on dev@flink.incubator.apache.org replies:\nThats good. We have the same problem on the receiver side. I assume the new\nRPC service is not transferring the user-jar by first allocating it as a\nwhole in memory.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/46053094/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "alpinegizmo": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/267431121", "body": "Max, this latest commit allows one to take advantage of ruby 2.0 if it is available, but is backwards compatible to 1.9. Hopefully this will satisfy everyone. The only issue I can see is that if one runs the build_docs script with ruby 2.x then Gemfile.lock will be updated with newer versions which probably should not be checked in.", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/267431121/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/267533430", "body": "I've concluded this was the wrong approach. I'll open another pull request to deal with these issues.", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/267533430/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/267614077", "body": "I've been using the incremental build option for several weeks, and find it to be a significant productivity booster. And yes, it requires ruby 2.0 or higher, while the buildbot is on some flavor of 1.9. Perhaps no one cares, but for what it's worth, ruby 1.9 and ruby 2.0 are old enough that they no longer maintained, even for critical security patches.\r\n\r\nI'm planning to continue to invest significant time in improving the docs, so I'd like to get this in, but you're right, having two build environments is suboptimal (and actually we already have two, in a way, since the dockerized build uses ruby 2.0). I'm hoping we can get the buildbot upgraded to something less ancient and then drop the 1.9-based Gemfile. @mxm has filed a ticket to that effect; we'll see.", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/267614077/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/273436327", "body": "Very helpful. But I did stumble on this sentence -- I'm not sure what \"of a queue\" is telling me. \r\n\r\n> This limitation is mainly due to RabbitMQ's approach to dispatching messages of a queue across multiple consumers.\r\n\r\nWould it be accurate to say \"... dispatching messages from a single queue to multiple consumers\" ?\r\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/273436327/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/277303323", "body": "@uce I tried to choose styling that looks decent with all previous releases. Here's a screenshot against 1.0.\r\n![old-release-warning](https://cloud.githubusercontent.com/assets/43608/22600510/1ef86282-ea3b-11e6-8fea-853e317a08e9.png)\r\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/277303323/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/277304118", "body": "@uce Some old releases still have strong google power -- especially 0.8. I don't know if replacing those pages with a redirect will cause the page rank to plummet or not.", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/277304118/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/277264544", "body": "+1 looks good to me", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/277264544/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/281980820", "body": "Would be better to spell metadata as one word. Otherwise looks great to me.\r\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/281980820/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/298689733", "body": "I prefer a conservative approach for as long as we still have to support building the docs on Ruby 1.9, so I'd keep the number of updated gems as small as possible.\r\n\r\nAs for adding support for Ruby 2.4, both docs/Gemfile and docs/ruby2/Gemfile should be updated, along with their corresponding (and derived) Gemfile.lock files. Moreover, the octokit downgrade is not needed. \r\n\r\nI'm not sure what motivates some of the proposed gem upgrades. I was able to get the docs to build on ruby 2.4 by making changing both Gemfiles as shown below. This leaves the classifier-reborn, ffi, libv8, posix-spawn, rb-inotify, redcarpet, and sass gems as they are rather than upgrading them. Do we need newer versions of any of those?\r\n\r\n-gem 'json'\r\n+gem 'json', '2.0.4'\r\n+gem 'yajl-ruby', '1.2.2'\r\n+gem 'jekyll-coffeescript', '1.0.2'\r\n\r\nWith these upgrades, the build script still works for me on Ruby 1.9.3 and 2.3.3, as well as 2.4.0.", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/298689733/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/305506313", "body": "This looks good, thank you. However, I now understand what was going on with the downgrade of the octokit dependency. I think we should first merge https://github.com/apache/flink/pull/4043 and then rebase this PR. @zentol @StephanEwen ", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/305506313/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/306556442", "body": "Yes, let's get this done.\n\nOn Jun 6, 2017 19:03, \"Stephan Ewen\" <notifications@github.com> wrote:\n\n> @alpinegizmo <https://github.com/alpinegizmo> @zentol\n> <https://github.com/zentol> Now that #4043\n> <https://github.com/apache/flink/pull/4043> is merged, can we rebase this\n> one?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/apache/flink/pull/3720#issuecomment-306551741>, or mute\n> the thread\n> <https://github.com/notifications/unsubscribe-auth/AACqWFH4ZhNVBH2qCD6bsOrQaukYd3Dfks5sBYZKgaJpZM4M8pJW>\n> .\n>\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/306556442/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/299657075", "body": "@greghogan I'd be happy to list other blogs; any suggestions? I've considered at a few, but the problem with the ones I've examined is that the proportion of Flink-related posts is rather small, and the posts aren't tagged, so we can't link directly the subset that's relevant.", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/299657075/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/299425398", "body": "@greghogan Thanks for the feedback! ", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/299425398/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/300462975", "body": "@greghogan Yes, I think we should use spaces rather than tabs in code samples in the docs -- and fewer than 8. I looked around at all of the code in the docs, and made sure that all of it is now inside of a code highlighting block. I only changed indentation in cases where it really bothered me. Making the indentation 100% consistent didn't seem worth the trouble, and wouldn't very enforceable going forward (unless the highlight plugin is capable of doing the indentation for us).", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/300462975/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/301509046", "body": "@zentol \"The time that the job has not been running in a failing/recovering situation.\" is confusing. How about this:\r\n\r\nThe time the job has spent in a failing/recovering situation.\r\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/301509046/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/302394907", "body": "@zentol Ok, that's an important distinction, but now I see another ambiguity. Does one of these work?\r\n\r\n  For jobs currently in a failing/recovering situation, the time elapsed during this outage.\r\n\r\n  The time elapsed during the job's most recent failure.\r\n\r\nIf my job fails and isn't running for 5 minutes and then recovers, after recovery what does `downtime` report? 0, or 5 minutes?", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/302394907/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/305470904", "body": "Looks good to me.", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/305470904/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/303395942", "body": "@greghogan Yeah, I'm not entirely satisfied with \"predefined\" and \"bundled\", but those were the terms already used in the documentation, and I didn't want to introduce new terminology unless it's decisively better. Any suggestions? For the connectors coming from Bahir (and possibly elsewhere) I'm thinking about either something like \"Other Connectors\" or \"3rd Party Connectors\", or simply \"Connectors in Apache Bahir\" (assuming there aren't any non-bahir 3rd party connectors worth mentioning).", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/303395942/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/305468774", "body": "@tzulitai I've reviewed the new subsection. I think it's fine as is.", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/305468774/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/305440983", "body": "I read Eron's request a bit differently. Suppose you've taken a savepoint simply to manage a rescaling or redeployment. How quickly after resuming from the savepoint is it safe to delete it? I assume it is necessary to wait until the savepoint has been read -- but how to know when that is done? And should one wait somewhat longer, e.g., until the new job has been successfully checkpointed?", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/305440983/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/305500635", "body": "https://rubygems.org/gems/jekyll-gist/versions/1.4.0 shows that jekyll-gist 1.4.0 depends on octokit 4.2.x", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/305500635/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/309038510", "body": "FYI, the reference solutions to the batch training exercises are failing, and it looks to me like it's because of this issue.", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/309038510/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/319927600", "body": "@twalthr Duh, of course, you're right. \r\n\r\n+1", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/319927600/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/320185483", "body": "+1 for adding this section to the docs. And another +1 for adding something about testing watermarks / timestamps -- this is a very frequently asked question.\r\n\r\nCould, perhaps, include a link to https://github.com/ottogroup/flink-spector/\r\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/320185483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/320221646", "body": "+1", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/320221646/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/322854404", "body": "+1 I like these improvements.", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/322854404/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/334921369", "body": "Yes, looks good to me. +1", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/334921369/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/336147297", "body": "+1", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/336147297/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/339262625", "body": "I can't vouch for the accuracy of the info, but +1 for adding this level of detail.", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/339262625/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/345972152", "body": "lgtm", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/345972152/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/349250715", "body": "Very pleased to see this coming to fruition. ", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/349250715/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/350713319", "body": "+1", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/350713319/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/96628840", "body": "\"at the end\" rather than \"in the end\"", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96628840/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96629618", "body": "As in Flink-1.1, `snapshotState()` is called whenever a checkpoint \r\nis performed, but now `initializeState()` (which is the counterpart of the `restoreState()`) is called every time \r\nthe user-defined function is initialized, rather than only in the case that we are recovering from a failure. \r\nGiven this, `initializeState()` is not only the place where different types of state are initialized, \r\nbut also where state recovery logic is included. An implementation of the `CheckpointedFunction` interface for `BufferingSink` is presented below.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96629618/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96630180", "body": "There the `ListState` is cleared of all objects included by the previous checkpoint,\r\nand is then filled with the new ones we want to checkpoint.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96630180/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96630446", "body": "straightforward is one word, no hyphen", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96630446/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96630636", "body": "threshold is frequently misspelled in this example", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96630636/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99368591", "body": "**in-place** would be better than **inplace** (here and also in the next paragraph)", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99368591/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99369022", "body": "best practices\r\n\r\nrather than best-practises", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99369022/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99369329", "body": "used as the basis", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99369329/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99369385", "body": "for the CEP operator", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99369385/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99369468", "body": "only the head operator", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99369468/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99369716", "body": "Another **important** precondition is that all the savepoint data must be accessible from the new installation and reside under the same absolute path. ", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99369716/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99369845", "body": "jon -> job", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99369845/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99370593", "body": "As the last step of job migration, you resume from the savepoint taken above on the updated cluster.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99370593/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99370751", "body": "is currently fixed", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99370751/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99370939", "body": "This limitation might be removed in a future bugfix release.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99370939/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99375422", "body": "no way\r\n\r\nafter your job has been started, except", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99375422/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99375568", "body": "application's\r\n\r\nmetadata for its ability", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99375568/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99375767", "body": "choose", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99375767/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99376488", "body": "While this is comfortable from a user perspective, it is also very fragile, as changes to the JobGraph (e.g.\r\nexchanging an operator) will result in new UUIDs.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99376488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99376685", "body": "to use a RocksDB", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99376685/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99376976", "body": "without stopping stream processing\r\n\r\nworse performance than, for example, the memory-based state backends", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/99376976/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/101010804", "body": "support", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/101010804/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/101010905", "body": "machine's", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/101010905/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/101011358", "body": " typically guarantee durability in the presence of at most *n* concurrent node failures,", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/101011358/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/101011563", "body": "(such that the file ...)", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/101011563/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/101011728", "body": "while others do not", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/101011728/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/101011921", "body": "Crashes that cause the OS cache to lose data are considered fatal to the local machine and are not covered by the local file system's guarantees as defined by Flink.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/101011921/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/101012059", "body": "or seeking within output streams", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/101012059/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/101038332", "body": "I suggest a minor rewording:\r\n\r\nApache Maven offers the [maven-shade-plugin](https://maven.apache.org/plugins/maven-shade-plugin/), which allows one to change the package of a class *after* compiling it ...", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/101038332/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102703065", "body": "It's generally agreed that metadata is one word.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102703065/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/115125659", "body": "That's embarrassing; glad you caught that.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/115125659/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/115699196", "body": "That was a mistake.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/115699196/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/115701327", "body": "After spending more time with this, my conclusion is that getting into \"exactly once processing semantics\" is off-topic in a short section whose purpose is to introduce the idea of state backends. What I've done instead is to beef up the linking between the various related pages.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/115701327/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/117942372", "body": "We can't count on people reading (or remembering) this overview page, so I think it's important for each connector page to continue to link to those instructions.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/117942372/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/117953990", "body": "I'm thinking that this connectors overview page is currently the best place to put together an summary of everything in Flink relating to connections to external systems. I'll rework things a bit -- see if it helps. ", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/117953990/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/118900486", "body": "TyoeSerializerConfigSnapshot => TypeSerializerConfigSnapshot", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/118900486/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/118901207", "body": "    This section is targeted as a guideline for users who require custom serialization for their state\r\n\r\n\"using\" doesn't really work in this context. You could say \"the use of\" instead, but it doesn't add anything.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/118901207/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/118903664", "body": "I think you meant to say \"mistake\" rather than \"mistaken\", but how about a simpler construction:\r\n\r\n  The version of the serializer's configuration snapshot is **not** related to upgrading the serializer.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/118903664/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/118904745", "body": "I don't understand the use of \"confront\" in this context. Perhaps you mean something like \"determine the compatibility of\" or \"verify the compatibility of\" ?", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/118904745/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/118905107", "body": "I suggest dropping \"with previous configuration\"", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/118905107/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/118906172", "body": "I imagine you meant to say something like \"Do not confuse the version of the serializer's configuration snapshot as being related to upgrading the serializer.\" But I suggest something simpler like \"The version of the serializer's configuration snapshot is **not** related to serializer upgrades.\"", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/118906172/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119149629", "body": "*Ingestion time* is the time that events enter Flink; internally, it is treated similarly to event time.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119149629/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119150890", "body": "is it no longer possible to link directly to the windowing section of the table docs?", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119150890/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119151221", "body": "They are defined when creating a table from a `DataStream` or are pre-defined when using a `TableSource`. Once a time attribute has been defined, it can be referenced as a field and can be used in time-based operations.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119151221/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119151823", "body": "As long as a time attribute is not modified and is simply forwarded ...\r\n\r\n... and thus can not be used for further time-based operations.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119151823/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119151964", "body": "It requires neither timestamp extraction nor watermark generation.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119151964/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119152127", "body": "... as a processing time attribute", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119152127/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119152241", "body": "... as a processing time attribute", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119152241/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119152552", "body": "define table source with a processing time attribute\r\n\r\n(same change again below for scala)", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119152552/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119152769", "body": "It also ensures replayable results of the table program ...", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119152769/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119153027", "body": "Additionally, event time allows for unified syntax for table programs in both batch and streaming environments. A time attribute in a streaming environment can be a regular field of a record in a batch environment.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119153027/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119153392", "body": "The Table API & SQL assume ...\r\n\r\n... and is hidden from the end user of the API.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119153392/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119153620", "body": "... because it is no long needed after ...", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119153620/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119153932", "body": "extract the timestamp from the first field, and assign watermarks based on knowledge of the stream", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119153932/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119154160", "body": "// the first field is still useful, and should be kept\r\n// declare an additional logical field as an event time attribute", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119154160/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119154561", "body": "// the first field has been used for timestamp extraction, and is no longer necessary\r\n// replace the first field with a logical event time attribute\r\n\r\n(make the same change for scala)", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119154561/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119154720", "body": "same as above", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119154720/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119155039", "body": "extract timestamp and assign watermarks based on knowledge of the stream\r\n  \r\n            \r\n  Write", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119155039/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119155113", "body": "(same as above)", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119155113/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119344251", "body": "a full, self-contained backup of the state backend", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119344251/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119344563", "body": "retrieval", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119344563/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119344615", "body": "retrieval", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119344615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119358154", "body": "Up to Flink 1.2, this was a more tedious task which was performed with the savepoint tool.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119358154/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119359938", "body": "... that allows the composition of queries ...\r\n\r\nOR\r\n\r\n... that allows one to compose queries ...", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119359938/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119360012", "body": "... and later use the Table API to analyze the patterns, or you can scan, filter, and aggregate ...", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119360012/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119360356", "body": "Please note that the Table API and SQL are not yet feature complete and are being active developed. Not all operations are supported by every combination of \\[Table API, SQL\\] and \\[stream, batch\\] input.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119360356/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119360512", "body": "... bundled in ...", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119360512/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119360977", "body": "... we do *not* recommend building a fat-jar that includes the ...\r\n\r\nInstead, we recommend configuring Flink ...", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119360977/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119361348", "body": "Reading from tables and ...", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119361348/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119362779", "body": "I prefer the original", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119362779/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119364031", "body": "\"Common\" has two meanings in English -- \"ordinary\" and \"shared\". Moving this word to the beginning of the sentence confuses the meaning, at least for me.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119364031/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119364489", "body": "+1, and maybe change \"steps\" to \"pattern\" or \"structure\" ?", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119364489/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119365049", "body": "Or maybe drop the string; not sure it adds anything.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119365049/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119365529", "body": "I think this means to say \"It is not possible to combine tables of ...\"", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119365529/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119367038", "body": "A `TableEnvironment` has an internal catalog of tables, organized by table name. Table API or SQL queries can access tables which have been registered in the catalog, by referencing them by name.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119367038/reactions", "total_count": 1, "+1": 0, "-1": 1, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119367358", "body": "A `TableEnvironment` allows you to register a table from various sources:", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119367358/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119368601", "body": "\"such as\" or \"for example\", but not both\r\n\r\nif you want both, then it would be \"such as, for example, `catalog.database.table`\"", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119368601/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119369422", "body": "... where `groupBy(...)` specifies a grouping of `table`, and `select(...)` specifies the projection on the grouping of `table`.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119369422/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119369854", "body": "add a comma after [Apache Calcite](https://calcite.apache.org)", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119369854/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119369993", "body": "... as a Table.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119369993/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119372403", "body": "A batch `Table` can only be written to a `BatchTableSink`, while a streaming table requires ...", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119372403/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119375304", "body": "... For instance, it is possible to query an external table (for example from a RDBMS), do some pre-processing, such as filtering, projecting, aggregating, or joining with meta data, and then further process the data with the DataStream or DataSet APIs (and any of the libraries built on top of these APIs, such as CEP or Gelly). Inversely, a Table API or SQL query can also be applied to the result of a DataStream or DataSet program.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119375304/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119376157", "body": "In this way, custom DataStream and DataSet programs can be run on the result of a Table API or SQL query.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119376157/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119377917", "body": "A `Table` that is the result of a streaming query will be updated dynamically, i.e., it is changed as new elements arrive on the query's input streams. ...", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119377917/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119378199", "body": "... can only be used if ...", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119378199/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119379215", "body": "... and the name of the attribute must be specified.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119379215/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119380355", "body": "Flink supports Scala's built-in tuples and provides its own tuple classes for Java. ...", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119380355/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119381099", "body": "Renaming the original POJO fields requires the keyword `AS` because POJO fields have no inherent order. The name mapping requires the original names, and cannot be done by positions.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119381099/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119381768", "body": "Field names of rows can be => Field names can be", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119381768/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119382325", "body": "... The optimizations currently performed include projection and filter push-down, subquery decorrelation, and other kinds of query rewriting. Flink does not yet optimize the order of joins, but executes them in the same order as defined in the query ...", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/119382325/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/126431179", "body": "@zentol I find that \"in code\" reads rather awkwardly, and I don't see how it adds any value, since the details of how to do per-job configuration are shown below. Nevertheless, this topic can be a bit confusing, so I would suggest something more like this (assuming I got the details right):\r\n\r\nThe default state backend, if you specify nothing, is the jobmanager. If you wish to establish a different default for all jobs on your cluster, you can do so by defining a new default state backend in **flink-conf.yaml**. The default state backend can be overridden on a per-job basis, as shown below.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/126431179/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/131085493", "body": "drop the word \"a\" in \"implement a custom serialization logic\" so that it reads \"implement custom serialization logic\"", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/131085493/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/131086039", "body": "\"varying across compilers and depends\" ==> \"which varies across compilers and depends\"", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/131086039/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/131087040", "body": "\"and to allow [savepoints]\"", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/131087040/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/131352677", "body": "\"throw an exception\" (typo)", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/131352677/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/131354369", "body": "To get this to work in my outside-of-flink project, I had to modify this as:\r\n\r\n    <dependency>\r\n      <groupId>org.apache.flink</groupId>\r\n      <artifactId>flink-test-utils_{{site.scala_version_suffix}}</artifactId>\r\n      <version>${flink.version}</version>\r\n     </dependency>", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/131354369/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/136843813", "body": "Do we want to still mention FoldFunction here without mentioning it has been deprecated? Maybe we should talk about AggregateFunction instead.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/136843813/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/136845008", "body": "The difference between per-key per-window state and per-key global state deserves more explanation. It would be even better to sketch out scenarios of what one might do with these accessors. There's a fair bit of power being exposed here, and that's not at all obvious.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/136845008/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/136972599", "body": "\"this has methods\" => \"which has methods\"", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/136972599/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/136973577", "body": "A `ProcessWindowFunction` gets an `Iterable` containing all the elements of the window, and a `Context` object with access to time and state information, which enables it to provide more flexibility than other window functions.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/136973577/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/137000137", "body": "Sure, that sounds fine.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/137000137/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139684257", "body": "on top of the normal load from the pipeline\u2019s data processing work.\r\n\r\n(add \"the\")", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139684257/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139685466", "body": "to keep recovery efficient!", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139685466/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139685795", "body": "the high level idea is to accept a small amount of redundant state writing that incrementally introduces\r\nmerged/consolidated replacements for previous checkpoints.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139685795/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139686793", "body": "when old checkpoint data OR when previously checkpointed data", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139686793/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139687055", "body": "and iterate over state modifications", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139687055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139687153", "body": "which can be regarded", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139687153/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139687800", "body": "called a memtable. ... Once a memtable is full, ... After a memtable is written to disk, it becomes immutable and is then called a ...", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139687800/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139687994", "body": "contains all of their net information.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139687994/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139688818", "body": "job manager", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139688818/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139689544", "body": "two new sstable files have been created by RocksDB,", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139689544/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139689818", "body": "point to existing files in the ``cp-1`` directory,", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139689818/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139690107", "body": "the counts ... are decreased by 1", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139690107/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139690574", "body": "has yet been confirmed.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139690574/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139690806", "body": "and the confirmation has reached the backend.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139690806/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139691118", "body": "This upload policy can result in the same sstable file being uploaded more than once, from different checkpoints.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139691118/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139691558", "body": "Only the copy that was uploaded by the first-confirmed checkpoint survives, and we can\r\nreplace references to the duplicates in all checkpoints that register afterwards.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139691558/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139691882", "body": "doctor -> directory", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139691882/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139692206", "body": "can create", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/139692206/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/142630633", "body": "I think \"were marked\" better communicates the intent here, as it is saying that the parts of the API that were marked as public and stable in previous releases are still supported (regardless of how they are currently marked).\r\n\r\nOtherwise, +1.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/142630633/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ndimiduk": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/163010606", "body": "Sure thing.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/163010606/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/164882643", "body": "Should I post a second PR for merging to 0.10-SNAPSHOT, or will you handle cherry-pick to active release branches? Thanks a lot @fhueske !\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/164882643/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/170982280", "body": "Reworded the commit message for the appropriate subtask. This patch applies cleanly on both master and the release-0.10 branches.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/170982280/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/183143954", "body": "I just ran into this as well. Do you mind backporting to 0.10 branch too?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/183143954/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/183421908", "body": "My timeline for adopting Flink 1.0 depends heavily on the amount of code\nchange required to adopt. I haven't looked closely at my exposure to API\nand dependency changes. I also usually avoid jumping directly to production\nwith Apache X.0.0 community releases, preferring to cut their teeth in a\nstage env while community can further stabilize.\n\nOn Friday, February 12, 2016, Robert Metzger notifications@github.com\nwrote:\n\n> Once this PR has been merged (I expect that some adoptions might still be\n> required) I can try and see if its straightforward to backport. Are you\n> planning to stick to Flink 0.10 for a longer period after 1.0 has been\n> released?\n> (I'm asking because so far most users were migrating to the latest release)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/flink/pull/1623#issuecomment-183314810.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/183421908/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "thedrow": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/157639954", "body": "No I just hoped it's the right way to do it.\nI'll look into it.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/157639954/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/158617310", "body": "@rmetzger That should do it.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/158617310/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/162522026", "body": "Thanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/162522026/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "cowtowncoder": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/216337481", "body": "@aljoscha @fhueske Nothing special, just thought I'd start with smallest step, given that this is my first contribution here.\nBut given that 2.7.4 is out now, I agree that going right there does make most sense and should be safe.\nIt also looks like most usage is via Tree API (JsonNodes), some streaming; most changes are typically in databinding and very few compatibility issues occur outside databinding. So to me upgrade seems safe either way. And since Elastic-client already uses 2.7 that would allow unification of versions as well.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/216337481/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/216415361", "body": "@smarthi Updated as suggested.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/216415361/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "petervandenabeele": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/165415675", "body": "Hmmm, one of the Travis tests failed\n- I presume this is a flickering test from the context of other PR tests passing around that time\n- my patch is only a small documentation change\n\nWill it be merged as is, or should I try to trigger a new Travis test by pushing an essentially empty commit in this PR ?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/165415675/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "radekg": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/232685225", "body": "Sure, the problems are the following:\n- https://github.com/apache/flink/pull/2231/commits/06936d7c5acc0897348019161c9ced4596a0a4dd#diff-aba21cf86694f3f2cd85e2e5e9b04972R305 in 0.9, `consumer.assign` (https://github.com/apache/flink/pull/2231/commits/06936d7c5acc0897348019161c9ced4596a0a4dd#diff-aba21cf86694f3f2cd85e2e5e9b04972R180) takes a `List`, in 0.10 it takes `Collection`\n- for unit tests: https://github.com/apache/flink/pull/2231/commits/06936d7c5acc0897348019161c9ced4596a0a4dd#diff-ab65f3156ed8820677f3420152b78908R130, if we use 0.9 kafka version with 0.10 client, the concrete client tests fail as they catch wrong exception type in: https://github.com/TheWeatherCompany/flink/blob/06936d7c5acc0897348019161c9ced4596a0a4dd/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java#L185\n\nSilly stuff. Everything else works just fine. Fell free to reuse this stuff.\n\nFYI: I'd be confused it I was to use a class indicating 0.9 when working with 0.10, that's the reason I assembled separate module. 0.9 is done and there's no future work required, it makes sense to have 0.10. Just my opinion.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/232685225/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/233903857", "body": "@tzulitai yes, this pr does not deal with 0.10 specific timestamps. It makes a simple consumer application work.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/233903857/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/234642923", "body": "Merged with `upstream/master` and I'm getting this when running `mvn clean verify`:\n\n```\n[INFO] -------------------------------------------------------------\n[ERROR] COMPILATION ERROR :\n[INFO] -------------------------------------------------------------\n[ERROR] /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:[30,69] cannot find symbol\n  symbol:   class DefaultKafkaMetricAccumulator\n  location: package org.apache.flink.streaming.connectors.kafka.internals.metrics\n[ERROR] /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:[105,17] constructor AbstractFetcher in class org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher<T,KPH> cannot be applied to given types;\n  required: org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext<T>,java.util.List<org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition>,org.apache.flink.util.SerializedValue<org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks<T>>,org.apache.flink.util.SerializedValue<org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks<T>>,org.apache.flink.streaming.api.operators.StreamingRuntimeContext,boolean\n  found: org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext<T>,java.util.List<org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition>,org.apache.flink.util.SerializedValue<org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks<T>>,org.apache.flink.util.SerializedValue<org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks<T>>,org.apache.flink.streaming.api.operators.StreamingRuntimeContext\n  reason: actual and formal argument lists differ in length\n[ERROR] /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:[192,49] cannot find symbol\n  symbol:   class DefaultKafkaMetricAccumulator\n  location: class org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher<T>\n[ERROR] /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:[193,65] cannot find symbol\n  symbol:   variable DefaultKafkaMetricAccumulator\n  location: class org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher<T>\n[INFO] 4 errors\n[INFO] -------------------------------------------------------------\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO]\n[INFO] force-shading ...................................... SUCCESS [  1.210 s]\n[INFO] flink .............................................. SUCCESS [  4.416 s]\n[INFO] flink-annotations .................................. SUCCESS [  1.551 s]\n[INFO] flink-shaded-hadoop ................................ SUCCESS [  0.162 s]\n[INFO] flink-shaded-hadoop2 ............................... SUCCESS [  6.451 s]\n[INFO] flink-shaded-include-yarn-tests .................... SUCCESS [  7.929 s]\n[INFO] flink-shaded-curator ............................... SUCCESS [  0.110 s]\n[INFO] flink-shaded-curator-recipes ....................... SUCCESS [  0.986 s]\n[INFO] flink-shaded-curator-test .......................... SUCCESS [  0.200 s]\n[INFO] flink-test-utils-parent ............................ SUCCESS [  0.111 s]\n[INFO] flink-test-utils-junit ............................. SUCCESS [  2.417 s]\n[INFO] flink-core ......................................... SUCCESS [ 37.825 s]\n[INFO] flink-java ......................................... SUCCESS [ 23.620 s]\n[INFO] flink-runtime ...................................... SUCCESS [06:25 min]\n[INFO] flink-optimizer .................................... SUCCESS [ 12.698 s]\n[INFO] flink-clients ...................................... SUCCESS [  9.795 s]\n[INFO] flink-streaming-java ............................... SUCCESS [ 43.709 s]\n[INFO] flink-test-utils ................................... SUCCESS [  9.363 s]\n[INFO] flink-scala ........................................ SUCCESS [ 37.639 s]\n[INFO] flink-runtime-web .................................. SUCCESS [ 19.749 s]\n[INFO] flink-examples ..................................... SUCCESS [  1.006 s]\n[INFO] flink-examples-batch ............................... SUCCESS [ 14.276 s]\n[INFO] flink-contrib ...................................... SUCCESS [  0.104 s]\n[INFO] flink-statebackend-rocksdb ......................... SUCCESS [ 10.938 s]\n[INFO] flink-tests ........................................ SUCCESS [07:34 min]\n[INFO] flink-streaming-scala .............................. SUCCESS [ 33.365 s]\n[INFO] flink-streaming-connectors ......................... SUCCESS [  0.106 s]\n[INFO] flink-connector-flume .............................. SUCCESS [  5.626 s]\n[INFO] flink-libraries .................................... SUCCESS [  0.100 s]\n[INFO] flink-table ........................................ SUCCESS [02:31 min]\n[INFO] flink-connector-kafka-base ......................... SUCCESS [ 10.033 s]\n[INFO] flink-connector-kafka-0.8 .......................... SUCCESS [02:06 min]\n[INFO] flink-connector-kafka-0.9 .......................... SUCCESS [02:12 min]\n[INFO] flink-connector-kafka-0.10 ......................... FAILURE [  0.197 s]\n[INFO] flink-connector-elasticsearch ...................... SKIPPED\n[INFO] flink-connector-elasticsearch2 ..................... SKIPPED\n[INFO] flink-connector-rabbitmq ........................... SKIPPED\n[INFO] flink-connector-twitter ............................ SKIPPED\n[INFO] flink-connector-nifi ............................... SKIPPED\n[INFO] flink-connector-cassandra .......................... SKIPPED\n[INFO] flink-connector-redis .............................. SKIPPED\n[INFO] flink-connector-filesystem ......................... SKIPPED\n[INFO] flink-batch-connectors ............................. SKIPPED\n[INFO] flink-avro ......................................... SKIPPED\n[INFO] flink-jdbc ......................................... SKIPPED\n[INFO] flink-hadoop-compatibility ......................... SKIPPED\n[INFO] flink-hbase ........................................ SKIPPED\n[INFO] flink-hcatalog ..................................... SKIPPED\n[INFO] flink-examples-streaming ........................... SKIPPED\n[INFO] flink-gelly ........................................ SKIPPED\n[INFO] flink-gelly-scala .................................. SKIPPED\n[INFO] flink-gelly-examples ............................... SKIPPED\n[INFO] flink-python ....................................... SKIPPED\n[INFO] flink-ml ........................................... SKIPPED\n[INFO] flink-cep .......................................... SKIPPED\n[INFO] flink-cep-scala .................................... SKIPPED\n[INFO] flink-scala-shell .................................. SKIPPED\n[INFO] flink-quickstart ................................... SKIPPED\n[INFO] flink-quickstart-java .............................. SKIPPED\n[INFO] flink-quickstart-scala ............................. SKIPPED\n[INFO] flink-storm ........................................ SKIPPED\n[INFO] flink-storm-examples ............................... SKIPPED\n[INFO] flink-streaming-contrib ............................ SKIPPED\n[INFO] flink-tweet-inputformat ............................ SKIPPED\n[INFO] flink-operator-stats ............................... SKIPPED\n[INFO] flink-connector-wikiedits .......................... SKIPPED\n[INFO] flink-yarn ......................................... SKIPPED\n[INFO] flink-dist ......................................... SKIPPED\n[INFO] flink-metrics ...................................... SKIPPED\n[INFO] flink-metrics-dropwizard ........................... SKIPPED\n[INFO] flink-metrics-ganglia .............................. SKIPPED\n[INFO] flink-metrics-graphite ............................. SKIPPED\n[INFO] flink-metrics-statsd ............................... SKIPPED\n[INFO] flink-fs-tests ..................................... SKIPPED\n[INFO] flink-java8 ........................................ SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 25:46 min\n[INFO] Finished at: 2016-07-22T21:52:55+02:00\n[INFO] Final Memory: 159M/1763M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project flink-connector-kafka-0.10_2.10: Compilation failure: Compilation failure:\n[ERROR] /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:[30,69] cannot find symbol\n[ERROR] symbol:   class DefaultKafkaMetricAccumulator\n[ERROR] location: package org.apache.flink.streaming.connectors.kafka.internals.metrics\n[ERROR] /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:[105,17] constructor AbstractFetcher in class org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher<T,KPH> cannot be applied to given types;\n[ERROR] required: org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext<T>,java.util.List<org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition>,org.apache.flink.util.SerializedValue<org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks<T>>,org.apache.flink.util.SerializedValue<org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks<T>>,org.apache.flink.streaming.api.operators.StreamingRuntimeContext,boolean\n[ERROR] found: org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext<T>,java.util.List<org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition>,org.apache.flink.util.SerializedValue<org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks<T>>,org.apache.flink.util.SerializedValue<org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks<T>>,org.apache.flink.streaming.api.operators.StreamingRuntimeContext\n[ERROR] reason: actual and formal argument lists differ in length\n[ERROR] /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:[192,49] cannot find symbol\n[ERROR] symbol:   class DefaultKafkaMetricAccumulator\n[ERROR] location: class org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher<T>\n[ERROR] /Users/rad/dev/twc/flink/flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java:[193,65] cannot find symbol\n[ERROR] symbol:   variable DefaultKafkaMetricAccumulator\n[ERROR] location: class org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher<T>\n[ERROR] -> [Help 1]\n[ERROR]\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR]\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n[ERROR]\n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :flink-connector-kafka-0.10_2.10\n```\n\nAny advice?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/234642923/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/234653461", "body": "Thanks, running `verify` again.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/234653461/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/234662081", "body": "Travis is going to run.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/234662081/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/234709404", "body": "Tests are failing for random setups on travis. Seems to be something scala related.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/234709404/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/238887983", "body": "@rmetzger it's absolutely fine to reuse the code. If I can help in any way, please let me know.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/238887983/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "netguy204": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/298644480", "body": "+1 I'm looking forward to this fix as I think I'm encountering this bug in production.\r\n\r\nI bundle my jobs into a single JAR file with multiple mains. I submit the jobs to the cluster sequentially (once the cluster accepts one I submit the next). My JAR also has two dependency JARs that I provide via HTTP using the -C switch to flink.\r\n\r\nWhen a job fails it automatically restarts but it seems to cause other jobs from the same JAR to fail and restart as well. The error is always some variation of:\r\n\r\n```\r\njava.lang.IllegalStateException: zip file closed\r\n\tat java.util.zip.ZipFile.ensureOpen(ZipFile.java:669)\r\n\tat java.util.zip.ZipFile.getEntry(ZipFile.java:309)\r\n\tat java.util.jar.JarFile.getEntry(JarFile.java:240)\r\n\tat sun.net.www.protocol.jar.URLJarFile.getEntry(URLJarFile.java:128)\r\n\tat java.util.jar.JarFile.getJarEntry(JarFile.java:223)\r\n\tat sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1005)\r\n\tat sun.misc.URLClassPath$JarLoader.findResource(URLClassPath.java:983)\r\n\tat sun.misc.URLClassPath.findResource(URLClassPath.java:188)\r\n\tat java.net.URLClassLoader$2.run(URLClassLoader.java:569)\r\n\tat java.net.URLClassLoader$2.run(URLClassLoader.java:567)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.net.URLClassLoader.findResource(URLClassLoader.java:566)\r\n\tat java.lang.ClassLoader.getResource(ClassLoader.java:1093)\r\n\tat java.net.URLClassLoader.getResourceAsStream(URLClassLoader.java:232)\r\n        .... backtrace from some arbitrary point in my code that never is doing anything with reflection ...\r\n```\r\n\r\nThe class load that triggers the fault is arbitrary. The same job may fail and restart multiple times in the same day with a different failing class load.", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/298644480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/299552892", "body": "@StephanEwen Yes, I do have at least objects and classes being stored in a static context. Any easy example (that has also bitten me a few times) is the class cache that Avro maintains: \r\n\r\nhttps://github.com/apache/avro/blob/master/lang/java/avro/src/main/java/org/apache/avro/specific/SpecificData.java#L146\r\n\r\nThe Avro API's, unless told otherwise, will use a singleton instance of SpecificData and will access that shared cache.\r\n\r\nWould something like that be enough to cause the classloader to pass between jobs?", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/299552892/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "omnisis": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/77671415", "body": "Whats the current status of this ticket?  Starting to checkout Flink and native Mesos support would be a huge win for my use case.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/77671415/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "kfleischmann": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/49445209", "body": "how can i revert my changes from the wordcount example? I just revert to the last version, is that correct?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/49445209/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "atsikiridis": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/49561327", "body": "This PR is now ASF compliant. The parallelism of the tasks is not fully mapped though. Hopefully I can solve this properly soon... ( @fhueske has given me some input in the tracker issue  https://issues.apache.org/jira/browse/FLINK-838)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/49561327/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50000102", "body": "Hi @fhueske, concerning this last issue with the configurable interfaces it has been fixed in my branch with this distributed cache, but not here. I will merge them today. Thank you for reviewing.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50000102/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50402587", "body": "Hi, @fhueske I hope I haven't missed anything we discussed. If I have, please tell me :)\n- Your test with the configuration above passes now. I added a similar test case.\n- The parallelism is now how we discussed.\n- I reworked the \"different combiner\" case.\n- I did not include custom partioning and grouping of values here. However, I would like to present something more detailed in my tracker issue.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50402587/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50551442", "body": "Ahh @fhueske squashing was a complete brain stop sorry :(\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50551442/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/15377406", "body": "What if this unified partitioner / groupingcomparator gives us   `( (partition, keyOfGroup), (K,V) )`  Would that also be wrong ?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15377406/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15377471", "body": "Ok, I understand.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15377471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15377533", "body": "I mean that the `KeySelector.getKey()` should return `(partition, keyOfGroup)`\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15377533/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15544868", "body": "The default value for `JobConf.getCombinerClass()` is `null` (this is not the case with Mappers and Reducers that have their respective identity functions as default). Even if the user wants the combiner to be the same as the reducer he has to explicitly say so. Otherwise, the job runs with no combine phase. Do you suggest that the combiner class should be the reducer class by default and that we should never skip reducing?\n\nMoreover, in the client there is:\n\n``` java\n        final Class<? extends Reducer> combinerClass = hadoopJobConf.getCombinerClass();\n        if (combinerClass != null) {\n            reduceOp.setCombinable(true);\n        }\n```\n\nso if there is no combiner the function is not combinable for Flink. Isn't that correct?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15544868/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15545161", "body": "Actually, the `IdentityReducer` is the default reducer value by the conf for a Hadoop Job. I think we as well should run it by default, as the user expects sorted data with that job (even he doesn't specify a Reducer).\n\nIn order to go map-only ( and no sorting) the user must set `JobConf.setNumReduceTasks(0)`. And you are right, this must be supported properly.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15545161/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15558369", "body": "I investigated why this is happening.\n\nWhen we run tests, our tests run in a `ContextEnvironment` (e.g. a test environment) and for this environment the values in `GlobalConfiguration` have been already set.\n\nHowever, in the case when we just run the user's job, since there is no test environment the local or remote environment have not been created yet.\n\nThis makes it impossible to use the task's maximum task slots since no environment has been started.\n\nThe solution would be to do perform this check for the limit of maximum tasks in parallel after this value has been initialized by its environment (after the job has actually started).\n\nI have been working on some extensions and minor customizations of the environments (think of a `HadoopLocalEnvironment`, for example) in order to support monitoring and counters with a plan to show them to you in the next PR. \n\nWith these extensions, this would be definitely possible. Should I merge the relevant changes in this PR? Unless you think it is not necessary and have a different approach in mind.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15558369/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15741334", "body": "So would it be safe to assume that if the slots have not been set ( = -1) this is obviously an IDE run and slots should be equal to the max DOP ?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15741334/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15741355", "body": "With a warning perhaps?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15741355/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15930089", "body": "This ensures that each task gets a seperate `JobConf` object. This is related to https://mail-archives.apache.org/mod_mbox/incubator-flink-dev/201407.mbox/browser\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15930089/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "fhueske": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/49999194", "body": "I checked whether a MapFunction can be configured using a custom key in the JobConf.\nThe following code does not work correctly (for multiple reasons):\n\n``` java\npublic class MyMapper implements Mapper<LongWritable, Text, IntWritable, IntWritable> {\n\n    int x = 0;\n\n    @Override\n    public void configure(JobConf arg0) {\n        x = arg0.getInt(\"myX\", 100);\n        System.out.println(\"x=\"+x);\n    }\n\n    @Override\n    public void map(LongWritable arg0, Text arg1, OutputCollector<IntWritable, IntWritable> arg2, Reporter arg3)\n            throws IOException {\n        System.out.println(\"map x=\"+x);\n    }\n}\n\npublic class MyStupidHadoopJob {\n\n    public static void main(String[] args) throws Exception {\n\n        final JobConf conf = new JobConf();\n\n        conf.setInputFormat(org.apache.hadoop.mapred.TextInputFormat.class);\n        org.apache.hadoop.mapred.TextInputFormat.addInputPath(conf, new Path(...));\n\n        conf.setOutputFormat(TextOutputFormat.class);\n        TextOutputFormat.setOutputPath(conf, new Path(...));\n\n        conf.setMapperClass(MyMapper.class);\n\n        conf.setMapOutputKeyClass(IntWritable.class);\n        conf.setMapOutputValueClass(IntWritable.class);\n        conf.setInt(\"myX\", 50);\n\n        conf.set(\"mapred.textoutputformat.separator\", \" \");\n        conf.setOutputKeyClass(Text.class);\n        conf.setOutputValueClass(LongWritable.class);\n\n        FlinkHadoopJobClient.runJob(conf);\n    }\n}\n\n```\n\nI found the following:\n- The `configure()` method is called (twice for DOP=1), i.e., the `println()` is executed.\n- The JobConf object passed to `configure()` seems to be incorrect. `x` is assigned 100, i.e., the key `\"myX\"` is not set in the passed configuration.\n- When executing the mapper, the `println()` statements print \"map x=0\", i.e., `x` is not initialized in MapFunction instances which are called.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/49999194/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50000110", "body": "I'd also update the `HadoopDummyReporter` and throw a `UnsupportedOperationException` whenever one of its methods is called.\nOtherwise users might rely on the returned values.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50000110/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50000152", "body": "Ah cool :-)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50000152/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50073451", "body": "Nice one!\nI'd prefer to use a ReduceFunction instead of a GroupReduceFunction because it is automatically combinable (which is usually important if you do a Reduce without groupBy) and shorter.\nHowever, you would need to add another MapFunction for the final division.\n\nYou could also replace the first MapFunction by a FilterFunction and emit only a record if the test is passed. This would mean that no records with 0-valued ints are emitted and therefore the summing becomes cheaper.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50073451/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50326508", "body": "Looks good to me. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50326508/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50459914", "body": "Hi @atsikiridis,\n\nplease add new commits to this PR and do not squash and force push the branch. Otherwise, all previous code comments are gone and not longer available.\n\nThe `FlinkHadoopJobClient.submitJob()` method looks much better and I checked that Configuration values are correctly passed.\n\nThere are a few issues that I commented inline.\nAlso please add JavaDocs to all public methods of user-facing classes and check if you can get rid of some of the `rawType` warnings (use `@SuppressWarnings(\"rawtype\")` if not possible).\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50459914/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50535230", "body": "I think throwing a meaningful exception when accessing the iterator a second time is the more intuitive and user-friendly option. I think it is easier to get a function right, if an exception explicitly tells you what you are doing wrong rather than debugging the code and wondering why the stupid iterator is empty...\n\n+1 to throw an exception when the iterator is requested the second time.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50535230/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50731057", "body": "I don't find Joiner, Crosser, and FlatJoiner too bad. Filterer sounds strange though...\nFooFunctional would also be fine with me.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50731057/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/15909792", "body": "the number of keys is independent of the number of aggregation fields.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15909792/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15909835", "body": "You need two loops here\n1. to copy all grouping fields from the last input row into the out row\n2. to copy the results of all aggregate functions into the out row\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15909835/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15909895", "body": "`groupingKeys` hold only the field indicies of the grouping keys, not the actual values. You need to do something like `outValue.setField(i, currentValue.getField(groupingKeys(i))`. For that you need to remember one input row (first or last doesn't matter, all have the same values in the grouping fields).\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15909895/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15909932", "body": "This copies the field index of the aggregation fields. We only need the value of the aggregation.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15909932/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15909944", "body": "The row should be initialized with `new Row(groupingKeys.length + aggregates.length)`\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15909944/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15913051", "body": "should be `aggregates(i - groupingKeys.length)` or you iterate `for (i <- 0 to aggregates.length)` and add groupingKeys.length...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15913051/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/15913064", "body": "Looks (mostly) good\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15913064/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19307176", "body": "Outch, I should have been more careful with this :-( \nThanks for fixing it @tzulitai!\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19307176/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19435898", "body": "This change is breaking the build right now:\n`cannot find symbol:   variable HA_ZOOKEEPER_NAMESPACE_KEY`\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19435898/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/19438191", "body": "Thanks for the quick fix!\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/19438191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/21555986", "body": "Hi @rtudoran, \r\n\r\nthis PR added support for event-time OVER RANGE windows. Stefano and you are working on processing time OVER RANGE/ROWS windows which require a different implementation, no?\r\nI assume you are upset because the function was changed to support the row and range case. However, in the function the actual code for this part is still missing. This is where your code would go.\r\n\r\nI know working concurrently with others on the same code can mean a lot of rebasing and change merging. However, we merge PRs once they are ready to be merged. ", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/21555986/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/15341292", "body": "The code contains many rawtype warnings. \nPlease try to get rid of them (e.g., `DataSet<?>` instead of `DataSet`) and add a `@SuppressWarnings(\"rawtypes\")` to the method if that's not possible.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15341292/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15341686", "body": "The partitioning / grouping code does not look right to me.\nIt does `mapOut.groupBy(partitioner).reduceGroup(identity).groupBy(groupingKeySelector)`\n\n`groupBy()` does both, partitioning and grouping, in the Flink API. The partitioning of the first `groupBy()` is destroyed by the second `groupBy()`. So the only thing that will happen is that the data is shuffled and sorted twice which is very expensive. We need to find a clever way to combine the HadoopPartitioner and the HadoopGroupingKeySelector into a single `groupBy()` operation or extend the Flink API to be able to specify both independently.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15341686/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15342004", "body": "The case of different combiner and reducer needs to be handled differently.\nYou do `mapOut.groupBy(hadoopGrouper).reduceGroup(combiner).reduceGroup(reducer)`.\n\nIn this case, the combiner is called as it was a reducer, i.e., after the data was partitioned and sorted. The purpose of a combiner is to reduce the data before shipping it over the network and (completely) sorting it.\nMoreover, the reduce is finally called on the fully reduced data as an AllReduce, which puts all data into a single group which will cause wrong results.\n\nThe solution here is to have a HadoopReduceFunction that can deal with a Reduce and Combine function.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15342004/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15390287", "body": "The issue is that Flink uses the same key for partitioning and grouping, i.e., it would also partition on `keyOfGroup` and group on `partition`. Of course, partitioning and grouping are separate operations in the execution layer, but the API gives the same key for both to the optimizer.\n\nIf neither the partitioner or sorter are custom, MapReduce will also use the same key for both operations, i.e., behave the same way as Flink.\n\nFor this PR, I'd simply throw an exception if you encounter a user-defined partitioner or comparator and support only the default case. We need to think carefully how we can add support for custom partitioners and comparators and might need to extend the API or add a hook somewhere to enable that.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15390287/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15512807", "body": "the number of map tasks should be the minimum of\n- user specified value `conf.getNumMapTasks()`\n- the number of splits `conf.getInputFormat().getSplits(conf, 0).length`\n- the number of slots `TASK_SLOTS`\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15512807/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15512850", "body": "When I run a job from Eclipse using a LocalExecutionEnvironment, `TASK_SLOTS` is set to -1 and execution fails.\nThis might be a problem with the LocalExecutionEnvironment, but should be resolved.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15512850/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15512920", "body": "Set it to `mapParallelism` to avoid the recomputation of the input splits.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15512920/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15512975", "body": "should be `if (reduceTasks <= TASK_SLOTS) {`?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15512975/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15513139", "body": "Add complete JavaDocs to all public methods including parameter, return, and throws descriptions.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15513139/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15514228", "body": "A combiner may not change the type of its input key and value. \nIt should be `private Reducer<KEYIN,VALUEIN,KEYIN,VALUEIN> combiner;`\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15514228/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15514510", "body": "The constructor initializes `this.combiner` to the combine class registered in the `JobConf`. If the combiner is the same class as the reducer it is still registered in the `JobConf`, right?\n\nIn that case, the combine method may not be called if no combiner is registered, i.e.,  `this.combiner == null`. Instead, this is a invalid system behavior and an exception should be raised.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15514510/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15515319", "body": "Hadoop supports MapOnly jobs. \nPlease check if a Reduce class is set in the `JobConf`. If not, finish the job after the Mapper by adding a DataSink with DOP = mapDOP.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15515319/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15515585", "body": "I saw MapOnly jobs are currently supported by injecting an IdentityReducer. This is a very expensive way to do it, because the data is shuffled over the network and sorted. MapOnly jobs aim to avoid that.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15515585/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15515720", "body": "Use a Reducer that is not derived from the combiner and has different return types to check if the combiner/reducer type handling is correct.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15515720/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15549615", "body": "Ah, OK! \nThanks for the explanation, let's do it like that :-)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15549615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15549954", "body": "No, I am suggesting to throw an exception instead of calling the reduce method.\n\nThe code should not be executed anyways if combinable is not set to true. \nHowever, if it (for any reason) happens, it is better to throw a meaningful exception than to call the Reduce function and silently continue with the program.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15549954/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15569898", "body": "My intuition is that it would be good to have as many slots as requested tasks if a job is locally run from an IDE. If you run a program with DOP > 1 from an IDE, you want to test if it is working correctly when run in parallel or debug it. So in this case the slot mechanism should be somehow deactivated (or as many slots created as the max DOP of the job to run).\nI'm not so familiar with the scheduling code. @StephanEwen can this be done somehow?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15569898/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15863231", "body": "Yeah, let's go for now with your suggestion, i.e., use max dop as number of slot if the parameter has not been initialized before. Please also add the warning.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/15863231/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18535782", "body": "That test checks if the combiner is correctly executed and will fail if not.\nThe collection-based execution does not run a combiner, because it does only make sense in a distributed setting to reduce the amount of shipped data.\nHence, the test fails if run on collections.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/18535782/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19725351", "body": "Why did you go for an empty `configure()` method? \nCreating a table and a scanner does probably include communication with the HBase master, which does not come for free. Depending on how expensive these operations are, it might make sense to only do this once.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19725351/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19725427", "body": "Have you checked if using an empty configuration is working in a distributed setting? \nWhere does the hostname of the HBase master come from? Is it maybe using a default value (localhost) which works on in a local setup only?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19725427/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19739899", "body": "Just checked. Result should be a Hadoop data type which should be natively handled by Flink (not as a generic Object which is done via Avro). \nSo, Flink should be capable of handling `Result`. Might be a problem with Flink's internal type handling.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19739899/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19740411", "body": "Since Result is a native Hadoop type (implements Writable) it should be treated as such and serialized with its own logic and not with Avro. Something on Flink's side does not seem to work correctly. I'll have a look at that. Until then, I suggest to put this PR aside.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19740411/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19745184", "body": "I had a look at the outdated API. It seems that `Result` does no longer implement the Hadoop Writable interface. Instead, they do their serialization with ProtoBuf... I haven't figured out, how they tell Hadoop how to serialize Result objects.\n\nCan you post the complete Stack trace of the `NoSuchMethodException`?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19745184/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19747240", "body": "Hi,\n\ncan you try to replace line 86 in `GenericTypeInfo` by \n\n`return new KryoSerializer<T>(this.typeClass);`\n\nand check if that would solve the problem?\n\nWe want to switch from Avro to Kryo for serialization of generic objects in the future anyways, but are currently waiting for some improvements that would make Kryo better perform.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19747240/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19830452", "body": "Hmm, that sounds like a bug in Flink. \nThe `Configuration` passed to `InputFormats` in `configure()` should not be empty.\nDo you want to create a JIRA issue for the bug you found?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19830452/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19832577", "body": "Great! \nFor the TableOutputFormat, it might be better to have something like the JDBCOutputFormat or the CSVOutputFormat, i.e., it receives a Tuple data type and assigns the individual fields of the tuples to HBase keys and/or columns. However, I'm not familiar with HBase's API.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19832577/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19844539", "body": "The HBaseTableOutputFormat should be of type <T extends Tuple> like the CSVOutputFormat or the JDBCOutputFormat. Depending on the configuration of the OutputFormat, the fields of the tuple are used as row keys or written to specified columns of the HBase table. The JDBCOutputFormat is doing something similar by writing Tuple data to a relational database table.\nIn order to make this work, the OutputFormat requires a bit of configuration including the Hbase table, schema information, key information, etc.\nDoes that make sense?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19844539/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19844652", "body": "No, haven't tried to reproduce it. \nIs the Configuration object empty (no keys and values) or is the object just null?\nI can also open an issue, just wanted to make sure that we do not forget about it.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19844652/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20208168", "body": "Different NOTE styles. Here: **NOTE**, later _Note_\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20208168/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "mariemayadi": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/50126545", "body": "I really like the filter suggestion. It is indeed a more suitable fit for the records emission.\n+1 for ReduceFunction Vs GroupReduceFunction. Fair point. (I was trying to avoid having to use an additional MapFunction :)\n\nWill fix it.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50126545/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "warneke": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/50386469", "body": "When you build Flink with the hadoop2 version set to 2.3.0-mapr-4.0.0-FCS, all dependencies are included in the library folder automatically through the regular Maven dependency resolution. There are no special configuration settings necessary. To be honest, I never tried to use the vanilla build and add the dependencies for MapR later on.\n\nIf people don't want to compile Flink themselves, I assume they are also not too keen on manually fixing Hadoop dependencies. How about we provide precompiled versions of Flink releases for the three major Hadoop players? I think you guys started it for Cloudera anyway, right? I would volunteer to cover the MapR part.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50386469/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50389191", "body": "Yes, but do you really want to start to track down which dependency is used by which subsystem of Hadoop1? This is probably both cumbersome and error-prone.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50389191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "ktzoumas": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/50586842", "body": "These checks are made in the \"with\" methods for Join, Cross, CoGroup, which is where the UDF is provided\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50586842/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/50726582", "body": "Thank you everyone for the terrific feedback. Lambda detection is changed to use SerializedLambda as Stephan suggested (that was grant!).\n\nIn terms of naming, I do like Mapper, Reducer, FlatMapper, and CoGrouper. The problematic ones are \"Filterer\", \"Joiner\", \"Crosser\", and \"FlatJoiner\". We can live with those, or call them simply \"Filter\", \"Join\", etc, thus having an inconsistent naming scheme (we should probably not use the name \"Map\" as it would cause frequent conflicts with Java Maps). \n\nA previous thought was MapFunctional, ReduceFunctional, etc, referring to functional interfaces. Any thoughts on this?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/50726582/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "physikerwelt": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/7689854", "body": "for DRIVERNAME = \"org.mariadb.jdbc.Driver\" it worked with FLOAT_TYPE_INFO instead of DOUBLE_TYPE_INFO\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/7689854/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/7689856", "body": "test with org.mariadb.jdbc.Driver see inline comment\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/7689856/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "mbalassi": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/8215458", "body": "Thanks, @hsaputra. :)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/8215458/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/8599203", "body": "Thanks.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/8599203/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/9725249", "body": "This way you use the generate pom that is in the outside repo and might be from a different branch. Not a big difference, but might lead to unexpected results. :)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/9725249/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/16704912", "body": "Re-read the mailing list thread and updated the licensing and deps accordingly.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/16704912/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "aalexandrov": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/9297124", "body": "Is removing the .gitignore a standard procedure for the release branches?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/9297124/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "mjsax": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/11303380", "body": "Sorry for the long diff. It's due to inconsistent line-breaks within the file... now all line-breaks are UNIX format.\n\nThe actuall change is from line 144 to 153. (144 to 147 is new; 153 is changed -> added programClass to response String)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/11303380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "alexeyegorov": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/14180954", "body": "Hey,\n\nare these changes already in the newest SNAPSHOT? As I'm getting an error locating in `FlinkLocalCluster.java`. I have the following line still in there:\n\n```\nJobGraph jobGraph = topology.getStreamGraph().getJobGraph(topologyName);\n```\n\nwhere `StreamGraph` API is already changed to `getJobGraph()`.\n\nCheers\nAlexey\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14180954/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/comments/14182283", "body": "Hey,\n\nI have cleared my cache while I was waiting for your answer. Still getting this error. But after looking into the snapshots repo with a colleague we've found that [flink-storm](https://repository.apache.org/content/groups/snapshots/org/apache/flink/flink-storm/1.0-SNAPSHOT/) is from 23 October, while [flink-streaming-java](https://repository.apache.org/content/groups/snapshots/org/apache/flink/flink-streaming-java/1.0-SNAPSHOT/) package is more up to date (3 November) as I am getting right `StreamingGraph` version and wrong `FlinkLocalCluster` version.\n\nHope this is a the reason for my problem and you could fix it. ;) \n\nCheers\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14182283/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/comments/14239582", "body": "Yesterday I saw that it was updated on Nov 04 in the evening and it worked. Thanks! ;) \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14239582/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/comments/14469239", "body": "Hey, as I get no answer on another question, maybe you can still help me: I would love to test 1.0-SNAPSHOT on my test cluster, but I am not able to find download link. I'm somehow not sure how to build right those sources or jars, that I need for the start!? If the scripts like start-cluster.sh or start-local.sh are still the same compared to 0.9.1, then how could I go further from what I have? \nI would really appreciate your help!\nThanks in advance!\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14469239/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/comments/14496327", "body": "@tillrohrmann @mxm thank you both. It's good know where I can find binaries I build on my own (was looking for them in my target folder). I was also trying to find some of them on the website, but it is somehow difficult, e.g. this [page](https://ci.apache.org/projects/flink/flink-docs-master/) states that pre-built snapshot can be downloaded there, but what you find is 0.9.1 stable version.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14496327/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/comments/15927084", "body": "has this commit been added to the current 1.0-SNAPSHOT? I become a problem while building my project and find that `org.apache.flink.streaming.api.datastream.DataStream` still imports `import org.apache.flink.api.java.operators.Keys;` while in this commit I see that it has been moved from package `...java.operators` to package `...common.operators`. I hope you can fix it asap.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15927084/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/comments/15932065", "body": "@uce yes, I've already added _2.10 to my dependencies that are scala dependent... but I still get the build problem.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15932065/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/comments/15941109", "body": "@StephanEwen ok, I'm sorry. I was able to build everything only after removing .m2/repository and clearing all caches. Seems to have been my own problem. ;) \n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15941109/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "chiwanpark": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/14352242", "body": "@gallenvara AFAIK JMH is licensed under GPL (http://hg.openjdk.java.net/code-tools/jmh/file/bcec9a03787f/LICENSE). GPL is not compatible with Apache License 2.0. So we had to remove `flink-benchmark` module.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14352242/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "hsaputra": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/14623683", "body": "+1\nThis should be easy rebase to the branch.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/14623683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "hash-X": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/15155284", "body": "This line of code `while (input.hasNext()){` don't have a good style, it should be like this `while (input.hasNext()) {`\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15155284/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "stefanobaghino": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/15677128", "body": "I can further improve the examples using `case classes` in the Scala example. Doing it while retaining the usage of inheritance would involve making a `trait` for both the `Point` and `Centroid` to inherit from.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/15677128/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "vijikarthi": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/17935989", "body": "This version (2.18.1) has some issues running scoped test case. For example it does not run, \"mvn verify -pl flink-yarn-tests -Pinclude-yarn-tests -Dtest=YARNSessionFIFOITCase#testJavaAPI\". I have reverted the version 2.19.1 and it was working fine.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/17935989/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "mindprince": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/20600038", "body": "@StephanEwen This would set allowedLateness for Processing time windows as well. Do we want that?", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/20600038/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/20615018", "body": "I thought `allowedLateness` affected when we purged windows. Wouldn't this result in keeping processing time windows around for longer than we should?", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/20615018/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "Fokko": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/20632930", "body": "Did not know this, thanks.", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/20632930/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "TwitRco": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/20650607", "body": "\r\n\r\n![avatar](https://cloud.githubusercontent.com/assets/1036668/22395721/3932e6a8-e546-11e6-8c56-ab878d8b3773.png)\r\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/20650607/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "qmlmoon": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/14237611", "body": "thanks, good point! I think it should in open method\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14237611/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14293149", "body": "Yes, you are right. I also updated this in CsvInputFormatTest in the latest commit\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/14293149/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "jkirsch": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/20007010", "body": "You are right\nhttps://microbenchmarks.appspot.com/runs/b3ab8918-7226-4527-b019-c622a728d144\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20007010/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "fpompermaier": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/19740015", "body": "I tried to remove HBaseResult and HBaseKey using directly Tuple2<ImmutableBytesWritable, Result> as you suggested but this does not work because of Avro serialization..at the moment I was trying to use the following:\n\n@Override\n    public Tuple2<ImmutableBytesWritable, Result> nextRecord(\n            Tuple2<ImmutableBytesWritable, Result> reuse) throws IOException {\n        if (this.tableRecordReader == null)\n        {\n            throw new IOException(\"No table record reader provided!\");\n        }\n\n```\n    try {\n        if (this.tableRecordReader.nextKeyValue())\n        {\n            Result currentValue = tableRecordReader.getCurrentValue();\n            currentValue.setExists(Boolean.TRUE);//fix null pointer in avro serialization\n            ImmutableBytesWritable currentKey = tableRecordReader.getCurrentKey();\n            reuse.setField(currentKey, 0);\n            reuse.setField(currentValue, 1);\n            return reuse;\n        } else\n        {\n            this.endReached = true;\n        }\n    } catch (InterruptedException e) {\n        LOG.error(\"Table reader has been interrupted\", e);\n        throw new IOException(e);\n    }\n    return null;\n}\n```\n\nHow do I have to proceed?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19740015/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19743935", "body": "Yes, probably there's something not handled properly.\nHowever Result in this case is of type org.apache.hadoop.hbase.client.Result..is this the cause of the problem?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19743935/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19745693", "body": "```\nclient.JobClient: java.lang.RuntimeException: java.lang.NoSuchMethodException: org.apache.hadoop.hbase.Cell.<init>()\n    at org.apache.avro.specific.SpecificData.newInstance(SpecificData.java:316)\n    at org.apache.avro.specific.SpecificData.newRecord(SpecificData.java:332)\n    at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:173)\n    at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:151)\n    at org.apache.avro.reflect.ReflectDatumReader.readObjectArray(ReflectDatumReader.java:150)\n    at org.apache.avro.reflect.ReflectDatumReader.readJavaArray(ReflectDatumReader.java:135)\n    at org.apache.avro.reflect.ReflectDatumReader.readArray(ReflectDatumReader.java:125)\n    at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:153)\n    at org.apache.avro.generic.GenericDatumReader.readField(GenericDatumReader.java:193)\n    at org.apache.avro.reflect.ReflectDatumReader.readField(ReflectDatumReader.java:230)\n    at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:183)\n    at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:151)\n    at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:142)\n    at org.apache.flink.api.java.typeutils.runtime.AvroSerializer.deserialize(AvroSerializer.java:126)\n    at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:115)\n    at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:30)\n    at org.apache.flink.runtime.plugable.DeserializationDelegate.read(DeserializationDelegate.java:56)\n    at org.apache.flink.runtime.io.network.serialization.AdaptiveSpanningRecordDeserializer.getNextRecord(AdaptiveSpanningRecordDeserializer.java:71)\n    at org.apache.flink.runtime.io.network.channels.InputChannel.readRecord(InputChannel.java:182)\n    at org.apache.flink.runtime.io.network.gates.InputGate.readRecord(InputGate.java:176)\n    at org.apache.flink.runtime.io.network.api.MutableRecordReader.next(MutableRecordReader.java:51)\n    at org.apache.flink.runtime.operators.util.ReaderIterator.next(ReaderIterator.java:53)\n    at org.apache.flink.runtime.operators.DataSinkTask.invoke(DataSinkTask.java:172)\n    at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:235)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NoSuchMethodException: org.apache.hadoop.hbase.Cell.<init>()\n    at java.lang.Class.getConstructor0(Class.java:2892)\n    at java.lang.Class.getDeclaredConstructor(Class.java:2058)\n    at org.apache.avro.specific.SpecificData.newInstance(SpecificData.java:310)\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19745693/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19776871", "body": "That worked like a charm :)\nSo now the only thing left out is the GenericTableOutputFormat..does it need to implement  OutputFormat<Tuple2<ImmutableBytesWritable, Result>>? In that case, how could writeRecord(Tuple2<ImmutableBytesWritable, Result> record) be implemented?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19776871/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19843418", "body": "Have you been able to reproduce it or do I have to trace the stack of the calls?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19843418/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19843584", "body": "The problem with this is that an HBase DataSource is of type <ImmutableBytesWritable, Result> but result is not writable..How do you suggest to proceed? How should be typed of the OutputFormat? Tuple2<ImmutableBytesWritable, Result> or another type?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19843584/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19958851", "body": "The configure method is called by the following:\n\nDataSourceNode.computeOperatorSpecificDefaultEstimates -> EMPTY!\nInputFormatVertex.initializeOnMaster\nDataSourceTask.initInputFormat\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/19958851/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20078222", "body": "Ok, now I should have fixed that\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20078222/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20090375", "body": "Unfortunately cloudera hbase 0.98.1-cdh5.1.3 requires hadoop-commons 2.3.0-cdh5.1.3  which requires hadoop-core 2.3.0-mr1-cdh5.1.3. Without specifying this profile for cloudera it is not possible to manage properly this dependency.\nI don't know why Cloudera did this...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20090375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20090518", "body": "Which permissions?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20090518/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20094591", "body": "Yes you could but then you have to introduce the hadoop.core.version variable also in the root pom..\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20094591/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20094611", "body": "Thas was unwanted.. :)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20094611/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20137745", "body": "Ok done and pushed\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/20137745/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}}}}