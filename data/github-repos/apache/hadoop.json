{"_default": {"1": {"macroadster": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/97fe3cc187cb9f773777ca79db6f1c7e4d1d5a68", "message": "YARN-7729.  Add support for setting Docker PID namespace mode.  (Contributed by Billie Rinaldi)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/3d65dbe032e202361d613344ccc6d9c5f99ba395", "message": "YARN-5366. Improve signal handling and delete delay for Docker on Yarn.\n           (Contributed by Shane Kumpf)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/41b581012a83a17db785343362c718363e13e8f5", "message": "YARN-7616. Map YARN application status to Service Status more accurately.  (Contributed by Gour Saha)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/94a2ac6b719913aa698b66bf40b7ebbe6fa606da", "message": "YARN-7466.  addendum patch for failing unit test.  (Contributed by Chandni Singh)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/989c75109a619deeaee7461864e7cb3c289c9421", "message": "YARN-7543.  Add check for max cpu limit and missing file for YARN service.\n            (Contributed by Jian He)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/438c1d333ebc0a3071bb556532ed959a4bd1e6d6", "message": "YARN-7540.  Route YARN service CLI function through YARN Service API. (Contributed by Eric Yang)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/d30d57828fddaa8667de49af879cde999907c7f6", "message": "YARN-6669.  Implemented Kerberos security for YARN service framework.  (Contributed by Jian He)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "flyrain": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/22ee6f77e5affbb308287f465ae8d145fc73ff60", "message": "YARN-7755. Clean up deprecation messages for allocation increments in FS config. Contributed by Wilfred Spiegelenburg."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/370f1c6283813dc1c7d001f44930e3c79c140c54", "message": "YARN-6486. FairScheduler: Deprecate continuous scheduling. (Contributed by Wilfred Spiegelenburg)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/b2029353537fc8da9ab67834568cb2e24924cf5a", "message": "HADOOP-15157. Zookeeper authentication related properties to support CredentialProviders. (Contributed by Gergo Repas)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/261", "title": "YARN-2162. add ability to optionally configure maxResources in terms \u2026", "body": "\u2026of percentage", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/276015225", "body": "Thanks Karthik for the review, a new patch posted for your comments.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/276015225/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/281796712", "body": "Thanks Karthik for the review. Push a new commit for your comments.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/281796712/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/291559957", "body": "Committed", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/291559957/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/291560028", "body": "Committed", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/291560028/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/291560202", "body": "Committed", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/291560202/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/291560336", "body": "Committed", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/291560336/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/291560416", "body": "Committed.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/291560416/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/291560475", "body": "Committed", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/291560475/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/291560534", "body": "Committed.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/291560534/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/291560589", "body": "Committed.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/291560589/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99944929", "body": "It should be, but allowPreemptionFrom is introduced after 2.8.x. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99944929/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97685895", "body": "Sure.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97685895/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97919802", "body": "Yes,  my original thought is to do that in another JIRA. The depth and parent-child policy are not the same. It mighty a good idea to combine them since the logic of depth checking only prevent fifo policy to be non-leaf queue. The current implementation seems a bit heavy. I can do it in this JIRA.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97919802/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97919979", "body": "My first thought of this one is similar to the depth checking, planned to refactor it in next JIRA. Another question in my mind is - do we need to initialize policy every time setting the policy?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97919979/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97920043", "body": "Good idea! ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97920043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100194434", "body": "Fixed.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100194434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100194505", "body": "Add a new test case to check if fifo policy is only for leaf queues.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100194505/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98765429", "body": "Add one unit test to check if RM transition to standby.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98765429/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "rkanter": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/836643d793c68bf1bee883abece84f024591da7c", "message": "MAPREDUCE-6995. Uploader tool for Distributed Cache Deploy documentation (miklos.szegedi@cloudera.com via rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/d716084f4503bf826ef10424d7025ea1ff4ee104", "message": "MAPREDUCE-7032. Add the ability to specify a delayed replication count (miklos.szegedi@cloudera.com via rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/5ac109909a29fab30363b752b5215be7f5dc616b", "message": "YARN-7479. TestContainerManagerSecurity.testContainerManager[Simple] flaky in trunk (ajisakaa via rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/e404650f489727d2df9a8813fddc4e0d682fbbee", "message": "MAPREDUCE-7030. Uploader tool should ignore symlinks to the same directory (miklos.szegedi@cloudera.com via rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/2aa4f0a55936239d35babd84da2a0d1a261bc9bd", "message": "YARN-7645. TestContainerResourceUsage#testUsageAfterAMRestartWithMultipleContainers is flakey with FairScheduler (rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/f8e7dd9b10f0b1b9d80e6196eb2b0296b523d8f4", "message": "YARN-7557. It should be possible to specify resource types in the fair scheduler increment value (grepas via rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/7a550448036c9d140d2c35c684cc8023ceb8880e", "message": "YARN-7622. Allow fair-scheduler configuration on HDFS (gphillips via rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/382215c72b93d6a97d813f407cf6496a7c3f2a4a", "message": "YARN-7577. Unit Fail: TestAMRestart#testPreemptedAMRestartOnRMRestart (miklos.szegedi@cloudera.com via rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/2316f526902e827b6c1b92a5bddef72d211bc742", "message": "MAPREDUCE-7018. Apply erasure coding properly to framework tarball and support plain tar (miklos.szegedi@cloudera.com via rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/d8863fc16fa3cbcdda5b99f79386c43e4fae5917", "message": "YARN-5594. Handle old RMDelegationToken format when recovering RM (rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/c83fe4491731c994a4867759d80db31d9c1cab60", "message": "YARN-4813. TestRMWebServicesDelegationTokenAuthentication.testDoAs fails intermittently (grepas via rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/3b78607a02f3a81ad730975ecdfa35967413271d", "message": "MAPREDUCE-6994. Uploader tool for Distributed Cache Deploy code changes  (miklos.szegedi@cloudera.com via rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/75a3ab88f5f4ea6abf0a56cb8058e17b5a5fe403", "message": "HADOOP-13493. Compatibility Docs should clarify the policy for what takes precedence when a conflict is found (templedf via rkanter)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "haibchen": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/cce71dceef9e82d31fe8ec59648b2a4a50c8869a", "message": "MAPREDUCE-6984. MR AM to clean up temporary files from previous attempt in case of no recovery. (Gergo Repas via Haibo Chen)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/4aca4ff759f773135f8a27dbaa9731196fac5233", "message": "YARN-5094. some YARN container events have timestamp of -1."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/8ee7080e5da15d8841d5f7bbf72ca033905c9751", "message": "YARN-7665. Allow FS scheduler state dump to be turned on/off separately from FS debug log. (Wilfred Spiegelenburg via Haibo Chen)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/f725b9e267c604fbcec09956e3a39caf04798809", "message": "YARN-7716. metricsTimeStart and metricsTimeEnd should be all lower case in the doc."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/2f6c038be6c23522ea64fc4e415910fb72493eb2", "message": "YARN-7602. NM should reference the singleton JvmMetrics instance."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "steveloughran": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/d689b2d99c7b4d7e587225638dd8f5af0a690dcc", "message": "HADOOP-15114. Add closeStreams(...) to IOUtils (addendum).\nContributed by Ajay Kumar."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/e5a1ad6e24807b166a40d1332c889c2c4cb4c733", "message": "HADOOP-14788. Credentials readTokenStorageFile to stop wrapping IOEs in IOEs.\nContributed by Ajay Kumar."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/1093a73689912f78547e6d23023be2fd1c7ddc85", "message": "HADOOP-13974. S3Guard CLI to support list/purge of pending multipart commits.\nContributed by Aaron Fabbri"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/de630708d1912b3e4fa31e00f5d84a08a580e763", "message": "HADOOP-15123. KDiag tries to load krb5.conf from KRB5CCNAME instead of KRB5_CONFIG.\nContributed by Vipin Rathor.\n\n(cherry picked from commit 1ef906e29e0989aafcb35c51ad2acbb262b3c8e7)\n(cherry picked from commit f61edab1d0ea08b6d752ecdfb6068103822012ec)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/f274fe33ea359d26a31efec42a856320a0dbb5f4", "message": "Revert \"HADOOP-13974. S3Guard CLI to support list/purge of pending multipart commits.\"\n\nThis reverts commit 35ad9b1dd279b769381ea1625d9bf776c309c5cb."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/a0c71dcc33ca7c5539d0ab61c4a276c4f39e5744", "message": "HADOOP-15079. ITestS3AFileOperationCost#testFakeDirectoryDeletion failing\nafter OutputCommitter patch.\nContributed by Steve Loughran"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/2e0a451a8dd64ccd7199b2b435e1b6c704a9d3ae", "message": "HADOOP-15033. Use java.util.zip.CRC32C for Java 9 and above\nContributed by Dmitry Chuyko,"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/1a09da7400295cba7f9a39b58eb8fff735222935", "message": "HADOOP-15163. Fix S3ACommitter documentation\nContributed by Alessandro Andrioni.\n\n(cherry picked from commit 100e8a1ae1d930dde084af7d1281e491c7f124ec)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/b62a5ece95a6b5bbb17f273debd55bcbf0c5f28c", "message": "HADOOP-15161. s3a: Stream and common statistics missing from metrics\nContributed by Sean Mackrory"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/52babbb4a0e3c89f2025bf6e9a1b51a96e8f8fb0", "message": "HADOOP-15086. NativeAzureFileSystem file rename is not atomic.\nContributed by Thomas Marquardt"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/c8ff0cc304f07bf793192291e0611b2fb4bcc4e3", "message": "HADOOP-13282. S3 blob etags to be made visible in S3A status/getFileChecksum() calls.\nContributed by Steve Loughran"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/ef450df443f1dea1c52082cf281f25db7141972f", "message": "HADOOP-15113. NPE in S3A getFileStatus: null instrumentation on using closed instance.\nContributed by Steve Loughran."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/1ba491ff907fc5d2618add980734a3534e2be098", "message": "HADOOP-14965. S3a input stream \"normal\" fadvise mode to be adaptive"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/35ad9b1dd279b769381ea1625d9bf776c309c5cb", "message": "HADOOP-13974. S3Guard CLI to support list/purge of pending multipart commits.\nContributed by Aaron Fabbri"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/3150c019aef21edf132a4f72260417f36036d89a", "message": "HADOOP-15071 S3a troubleshooting docs to add a couple more failure modes.\nContributed by Steve Loughran"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/283", "title": "HADOOP-14965 s3a input stream \"normal\" fadvise mode to be adaptive", "body": "This makes the {{S3AInputStream.inputPolicy}} non-final, and on the first backwards seek on a Normal input, switches it to Random (logging @ info in the process). If seeks are forward(), it just skips forwards, as sequential input does.\r\n\r\nThe input stream instrumentation counts the #of times the policy was changed (including the first), and the current value, where it is picked up in tests (so there's no need to add a test accessor as an input stream feature itself). \r\n\r\nThe test `ITestS3AInputStreamPerformance.testRandomIONormalPolicy` broke as the instrumentation showed only 1 TCP abort, not 4. This is a success, as it shows the policy is adapting.\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/204", "title": "HADOOP-13371 S3a Globber, WiP", "body": "This is what I'd done up to the point I stopped looking at it; copies over the FileSystem.globber and then adds an initial scale test. \r\n\r\n1. I think we could do a great scale test against the landsat repo\r\n1. I chose not to try and tweak the FileSystem.globber class for subclassing, so we can do much more in here, and to set things up for having a globStatus call which would return a remote iterator rather than an array", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/163", "title": "HADOOP-13227 outputstream specification", "body": "WiP specification of output streams.\r\n\r\nAlongside the docs, this will include tests and tightening of behaviours of output streams which don't comply with the `java.io.OutputStream` specification or other parts of the specification (i.e. the extra methods are inconsistent with HDFS). Object stores are special; their behaviours will be documented and (unreconcilable) differences with filesystems handled in tests.\r\n\r\nNote that HDFS does not currently follow `java.io.OutputStream.write(int)`'s required behaviour: it will *not* fail on a write on a closed stream. This will have to be corrected", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/72", "title": "YARN-4563", "body": "Attempt to document YARN security, including HADOOP_TOKEN_FILE_LOCATION propagation\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/66", "title": "YARN-2571 RM to support YARN registry", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/61", "title": "YARN-4430 registry security validation can fail when downgrading to insecure would work", "body": "This makes the check -> warn, leaving ZK to decide how to handle the problem\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/55", "title": "HADOOP-9844 NPE on sasl", "body": "existing patch rebased to trunk\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/158895900", "body": "wrong JIRA\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/158895900/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/197274477", "body": "Please can you file a separate JIRA for these two changes, link to HADOOP-9991 and include justification. Version updates are a traumatic issue in Hadoop and done fairly reluctantly.\n\nFWIW, updating netty-all from beta to final makes sense just from a release perspective; updating the other jetty less so \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/197274477/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/197340633", "body": "sorry, I should have been clearer\n-create a new Hadoop JIRA for this, mark it as a dependency of HADOOP-9991. thanks\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/197340633/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/252596738", "body": " Patch 002: create a fake user, create an FS and verify that the user and owner on the root path is that username\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/252596738/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/257839874", "body": "@elad best to discuss that on the JIRA itself; FWIW, EMR's S3 connector is Amazon's codebase, so what we do in the ASF doesn't have any impact there. We are striving to be better :-)\n\nStatus: we've gone and broken it in Hadoop branch-2 by replacing `S3AFastOutputStream` with `S3ABlockOutputStream`. Sorry.\n\n@fedecz \u2014could you bring this up to sync with branch-2 and I'll add reviewing it to my todo list. thanks\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/257839874/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 1, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/275529507", "body": "Can you comment on that in a JIRA, not a PR? Thanks", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/275529507/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/275530017", "body": "That's https://issues.apache.org/jira/secure/Dashboard.jspa ; project HADOOP, component fs/s3 /  \r\n\r\nThey should be deleted as soon as the upload completes; the `close()` call that the AWS httpclient makes on the input stream triggers the deletion. Though there aren't tests for it, as I recall.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/275530017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/276340054", "body": "Overall, production code looks good \u2014just some minor tweaks needed. Test code all LGTM, as do docs. As stated on the JIRA, I'd really like tests which verify that sse-c credentials are making it up. Anything we can do to expand coverage of the actual S3 state helps the users, and it helps catch any regressions in our code or the SDK library.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/276340054/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/287562808", "body": "OK, I see what you've done here: hidden a filter in Path and then used it later on. I like the trick\r\n\r\nAt the same time, I think it's not going to get past anyone else: its making a fundamental change to a core class, one that get serialized around and created a lot. It's not going to be allowed.\r\n\r\nThat's OK though, for the following reason: we have the freedom to add/extend the methods in S3aFS itself, so can do one which takes a filter as a parameter. If we do it right, we can get this into the FS spec, or at least start negotiating on that topic (needs: spec, tests, etc), while implementing it in S3AFS without waiting.\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/287562808/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/289421484", "body": "What an s3a globber can do is skip the recursive treewalk of pseudo-directories as it does the regexp. Instead it could ask for all children of a path, before doing the filtering. Look at HADOOP-13208 for the details.\r\n\r\nThis patch is the preamble: a copy of the existing globber, and the initial performance test which would be the baseline for measuring speedups. Look at some of the other dir operations to see how we measure that: it's not elapsed time, but in number of HTTP requests made.\r\n\r\nI should warn that this is not trivial to do well. I put the work aside once I came up in my head of some example directory layouts which would overload a a naive \"ask for all then filter\" scheme; any deep& wide tree where the wildcard was near the top of the tree and very selective would end up asking for way too much data, and discarding it all. Listing is complex\r\n\r\nfor now we're doing s3guard, which switches to dynamo DB for consistency as well as performance. If you do want to improve client performance, this is somewhere where we would all benefit from you getting involved. I don't see any of us going near other bits of speedup until after s3guard is in, because everything will be stamping on the same lines of code and making merging a pain. And s3guard is probably going to obsolete a lot of the speedup proposed here on any bucket which has the DDB backing tables.\r\n\r\nWe might also find that on a s3guard-enabled system, the glob algorithm changes, so again' that'd be something we'd want to include in the work.\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/289421484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/289721016", "body": "Changes to Path aren't going to happen. Sorry. changes within the s3a code base cause damage restricted to s3a://, so there is less resistance to that change. \r\n\r\nThis PR isn't going to get in. Sorry", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/289721016/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/327456708", "body": "I think some new Abstract FS Contract test will be needed to see how filesystems actually handle \":\" chars; having the path support it is just one part of the problem", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/327456708/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/329170251", "body": "In HADOOP-13786 I'm wrapping every single s3 client call with retry policy, then expanding the inconsistent client to generate more faults (initially throttle, later connection setup/response parsing). I'd really like this work to actually await that, as without it this code isn't going to be resilient to large copies where you are much more likely to hit parallel IO. And we need to make sure there's a good failure policy set up there", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/329170251/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/329426367", "body": "1. I don't see why this needs to be blocked waiting for all of the '13786 patch to go in\r\n2. but the rename/throttling stuff: yes.\r\n3. And yes, I do think I can pull out the retry logic once we've got it stable (the committer code is stressing it). But it will be java 8...whoever wants it in branch-2 gets to convert the closure around every s3 call to an anonymous Callable\r\n\r\nParallel execution: we've got another transfer manager issuing COPY requests, so more HTTPS requests to an S3 bucket/shard. The more requests to a single shard, the likelier you are to hit failures. Now, I think the xfer manager does handle the 503 replies which come back, but the rest of the FS client doesn't\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/329426367/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/346150364", "body": ":)\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/346150364/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274270", "body": "this is three chained conditions which could be merged through `&&`\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274270/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274678", "body": "could you move the preconditions checks into `S3ObjectAttributes` and invoke them from both output streams? That'd reduce duplicate code and perhaps aid future maintenance\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274678/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274920", "body": "we're ok with .\\* on static imports here, so you can just skip this bit of the patch\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274920/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70273746", "body": "why was this cut? This is directly referred to in {{TestS3AEncryption}}, a file which this patch doesn't touch. I don't think a clean build of this patch is going to work. Have you tried it?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70273746/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70278452", "body": "looks like an accidental paste in of a bit of HADOOP-13224's doc changes\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70278452/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70433629", "body": "Well it's being covered in HADOOP-13224, so it's best to pull it here and review that patch instead\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70433629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70463002", "body": "sorry, wrong JIRA. https://issues.apache.org/jira/browse/HADOOP-13324\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70463002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78158957", "body": "might be good to add an example here. e.g what the final options of distcp are going to be. Will there be two -Xmx commands? if so, which wins? Because I suspect that's a JVM decision\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78158957/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77982045", "body": "moved this line down as it was failing sometimes\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77982045/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82171213", "body": "I know it can't happen, but like to close off all failure routes of a close() call. + I think it may have dated from when some IOE was thrown. Anyway, throwing again.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82171213/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82171318", "body": "OK\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82171318/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82171498", "body": "good catch\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82171498/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82172310", "body": "did that everywhere; updated. Also in the troubleshooting s3a memory, just pointed back to the thread tuning entry.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82172310/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82172547", "body": "correct! swapped order of log and action\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82172547/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82172654", "body": "afraid so, that bit had missed the push as I forgot to --force the patch up at the end of the day. Will push it up with all the comments here, after another test run\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82172654/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83244015", "body": "done\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83244015/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83244207", "body": "correct. your diligence in reading javadocs is appreciated\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83244207/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83244375", "body": "renamed\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83244375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83414208", "body": "you know, now that you can have a queue per stream, it could be set to something\nbigger. This is something we could look at in the docs, leaving out of the XML so as\nto have a single topic. This phrase here describes the number of active threads, which\nis different \u2014and will be more so once there's other work (COPY, DELETE) going on there.\n\nSo: wont change here\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83414208/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83418594", "body": "OK. I've set that limit in Constants and will log @ error if the #of blocks exceeds it. We'll see what happens.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83418594/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83419491", "body": "..again, not changing it in either place,  as once renames() parallelize, life gets more complex\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83419491/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83419925", "body": "adding link to the s3a_fast_upload_thread_tuning section\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83419925/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83419999", "body": "yep.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83419999/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83420236", "body": "Does it? Never knew that. I'd thought it was server side. Will change. Also, we could make that an async operation; it's not needed to bring up the FS.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83420236/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83423063", "body": "I've just cut that section entirely. That's  harsh, but, well, it the fast output stream was always marked as experimental ... we've learned from the experiment and are now changing behaviour here, which is something we can look at covering in the release notes. I'll add that to the JIRA.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83423063/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/80909422", "body": "fixed\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/80909422/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/80910631", "body": "actually no! I couldn't get the put command to read off stdin according to the (Existing) docs, so I looked in the source code and saw that you needed a special \"-\" as the source, otherwise the s3a url :// was being treated as a source and the command failing as the destination was missing.  It does make sense \u2014and is consistent with other tools, just not documented completely.\n\nI'd mentioned this in the \"put\" section, but I've expanded the comment here and added the - to the DNF form of the put command. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/80910631/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88898686", "body": "no, it's to have a link which can be referred to later", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88898686/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88898982", "body": "ok", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88898982/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88899130", "body": "it is needed to indent the paragraph and code sample under the bullet point.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88899130/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/89502705", "body": "we can't pull values out of public interfaces like this, even if its not supported. They'd need to be tagged as deprecated", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/89502705/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98646037", "body": "\"is enabled\"", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98646037/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98646282", "body": "I'd like to see case: statements here; it may help readability", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98646282/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98646348", "body": "just cut this", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98646348/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98646557", "body": "I'd rather these lines reverted to the previous ones, even if they are > 80 chars long. That is a soft limit on style checks: we break it if it keeps the code looking nice", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98646557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98646873", "body": "If this can be made package private, I'd like it to be. Otherwise it must be tagged\r\n```\r\n@InterfaceAudience.Private\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98646873/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99231072", "body": "S3aUtils is already statically imported; no need to qualify.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99231072/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99232624", "body": "I wouldn't adjust the indentation, because it amplifies the diff and makes merging harder. Lines 105-110 should be the same as the original.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99232624/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99235308", "body": "We're generally moving towards `LambdaTestUtils.intercept()`, because if a closure fails, intercept() will print what came back. Less important for voids though, and on branch-2 & java7, not so compelling.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99235308/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99235609", "body": "I do like shared constants here, with the constant in the production code, test reading it. Stops the tests being brittle to change in the message. A flaw with expectMessage is that it looks for the whole string, doesn't it; again, we prefer a .contains() as its not brittle to exceptions adding more diags", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99235609/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "rohithsharmaks": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/c5bbd6418ed1a7b78bf5bd6c1e0fad1dc9fab300", "message": "YARN-7753. [UI2] Application logs has to be pulled from ATS 1.5 instead of ATS2. Contributed by Sunil G."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/d09058b2fd18803d12f0835fdf78aef5e0b99c90", "message": "YARN-6736. Consider writing to both ats v1 & v2 from RM for smoother upgrades. Contributed by Aaron Gresch."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/c2d6fa36560d122ff24dd7db84f68f4ba3fb8123", "message": "YARN-7699. queueUsagePercentage is coming as INF for getApp REST api call. Contributed by Sunil G."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/c9bf813c9a6c018d14f2bef49ba086ec0e60c761", "message": "YARN-7692. Skip validating priority acls while recovering applications. Contributed by Sunil G."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/13ad7479b0e35a2c2d398e28c676871d9e672dc3", "message": "YARN-7674. Update Timeline Reader web app address in UI2. Contributed by Sunil G."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/09d996fdd429a85822a06ab87f0e2322d0d7ca68", "message": "YARN-7190. Ensure only NM classpath in 2.x gets TSv2 related hbase jars, not the user classpath. Contributed by Varun Saxena."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/e65ca92fb6897a3004a235d7b69e308399189aae", "message": "YARN-7482. Max applications calculation per queue has to be retrospected with absolute resource support. Contributed by Sunil G."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "linyiqun": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/9e4f52d32319828c153a3ea658520b946988ae31", "message": "HDFS-12973. RBF: Document global quota supporting in federation. Contributed by Yiqun Lin."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/9afb8025d6549f0ade0ae7d36f5e67cd20c500f4", "message": "HDFS-12972. RBF: Display mount table quota info in Web UI and admin command. Contributed by Yiqun Lin."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/d98a2e6e2383f8b66def346409b0517aa32d298d", "message": "HDFS-12934. RBF: Federation supports global quota. Contributed by Yiqun Lin."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/bf5c94899537011465350d5d999fad9ffaeb605d", "message": "HDFS-11848. Enhance dfsadmin listOpenFiles command to list files under a given path. Contributed by Yiqun Lin."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/2a48b3594c502c4dcf201f2b60386383c0d9ae91", "message": "HDFS-12948. DiskBalancer report command top option should only take positive numeric values. Contributed by Shashikant Banerjee."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/e040c97b7743469f363eeae52c8abcf4fe7c65d5", "message": "HDFS-12937. RBF: Add more unit tests for router admin commands. Contributed by Yiqun Lin."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/25a36b74528678f56c63be643c76d819d6f07840", "message": "HDFS-12930. Remove the extra space in HdfsImageViewer.md. Contributed by Rahul Pathak."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/ee028bfdf1c88a27cd925bed93ebb599a164dd2e", "message": "HDFS-12895. RBF: Add ACL support for mount table. Contributed by Yiqun Lin."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/91c96bdf8eb9a06193b719186b527563091d7666", "message": "HDFS-12883. RBF: Document Router and State Store metrics. Contributed by Yiqun Lin."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "szegedim": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/bc93ac229e17b1be440052217e51820b95c179ec", "message": "YARN-7139. FairScheduler: finished applications are always restored to default queue. Contributed by Wilfred Spiegelenburg."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/a68e445dc682f4a123cdf016ce1aa46e550c7fdf", "message": "YARN-7717. Add configuration consistency for module.enabled and docker.privileged-containers.enabled. Contributed by Eric Badger."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/41049ba5d129f0fd0953ed8fdeb12635f7546bb2", "message": "YARN-7758. Add an additional check to the validity of container and application ids passed to container-executor. Contributed by Yufei Gu."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/2dcfc1876e4d73cf85a6b1b7de694b1b4cc54494", "message": "YARN-7705. Create the container log directory with correct sticky bit in C code. Contributed by Yufei Gu."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/bc285da107bb84a3c60c5224369d7398a41db2d8", "message": "YARN-7590. Improve container-executor validation check. Contributed by Eric Yang."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/12d0645990a878f78216235c800ae4e157796160", "message": "HADOOP-15060. TestShellBasedUnixGroupsMapping.testFiniteGroupResolutionTime flaky. Contributed by Miklos Szegedi."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/55066cc53dc22b68f9ca55a0029741d6c846be0a", "message": "YARN-7689. TestRMContainerAllocator fails after YARN-6124. Contributed by Wilfred Spiegelenburg."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/7f515f57ede74dae787994f37bfafd5d20c9aa4c", "message": "YARN-7585. NodeManager should go unhealthy when state store throws DBException. Contributed by Wilfred Spiegelenburg."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/80440231d49e518ab6411367d7d8474155ecca2b", "message": "YARN-6894. RM Apps API returns only active apps when query parameter queue used. Contributed by Gergely Nov\u00e1k."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/626b5103d44692adf3882af61bdafa40114c44f7", "message": "YARN-7688. Miscellaneous Improvements To ProcfsBasedProcessTree. Contributed by BELUGA BEHR."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/33ae2a4ae1a9a6561157d2ec8a1d80cb5c50ff2d", "message": "YARN-7687. ContainerLogAppender Improvements. Contributed by BELUGA BEHR."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/4bb765ee2e9fa16bb18c70d8f62ab885d883f414", "message": "HADOOP-15122. Lock down version of doxia-module-markdown plugin. Contributed by Marton Elek."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/b82049b4f0065b76c3eb590d57eb5bf0ebc2f204", "message": "YARN-7580. ContainersMonitorImpl logged message lacks detail when exceeding memory limits. Contributed by Wilfred Spiegelenburg."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/240", "title": "YARN-6674 Add memory cgroup configurations for opportunistic containers", "body": "YARN-6674", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/232", "title": "YARN-6673 Add cpu cgroup configurations for opportunistic containers", "body": "YARN-6673 Add cpu cgroup configurations for opportunistic containers", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/227", "title": "YARN-6668", "body": "YARN-6668 Measure CPU and memory usage with cgroups in ContainersMonitorImpl", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/200", "title": "YARN-6302 Fail the node, if Linux Container Executor is not configured properly", "body": "YARN-6302 Fail the node, if Linux Container Executor is not configured properly", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/309582385", "body": "I created #240 ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/309582385/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "billierinaldi": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/37f4696a9cc9284b242215f56a10990e1028d40c", "message": "YARN-7740. Fix logging for destroy yarn service cli when app does not exist and some minor bugs. Contributed by Jian He"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/53f2768926700d2a27ce6223f1ccbfd3be49fc29", "message": "YARN-7724. yarn application status should support application name. Contributed by Jian He"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/4fb1f45f21916ca1b1fc6652a2ad562ac996b7b8", "message": "YARN-7731. RegistryDNS should handle upstream DNS returning CNAME. Contributed by Eric Yang"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/dc54747d70fc4dc77767051d0f8f89ccda7ba3c0", "message": "YARN-7704. Document improvement for registry dns. Contributed by Jian He"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sunilgovind": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/06cceba1cb07340c412c4467439c16ea6812e685", "message": "YARN-7738. CapacityScheduler: Support refresh maximum allocation for multiple resource types. Contributed by Wangda Tan."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/8e5472b1e63f1c50e253e64702468da2bb38e476", "message": "YARN-7750. [UI2] Render time related fields in all pages to the browser timezone. Contributed by Vasudevan Skm."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/128d773a2315fa6baaa3a52b13c53c77e741b69c", "message": "YARN-7727. Incorrect log levels in few logs with QueuePriorityContainerCandidateSelector. Contributed by Prabhu Joseph."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/afd8caba2730262cb8c5d7c4a5d2d1081b671f1d", "message": "YARN-7722. Rename variables in MockNM, MockRM for better clarity. Contributed by Lovekesh bansal"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/783a01eb4a155044a54a30a636b86b3ab2b33044", "message": "YARN-7718. DistributedShell failed to specify resource other than memory/vcores from container_resources. Contributed by Wangda Tan."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/01f3f2167ec20b52a18bc2cf250fb4229cfd2c14", "message": "YARN-7242. Support to specify values of different resource types in DistributedShell for easier testing. Contributed by Gergely Nov\u00e1k."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/0c75d0634bcbdc29e804035b3b84ae6a38d6a110", "message": "YARN-7619. Max AM Resource value in Capacity Scheduler UI has to be refreshed for every user. Contributed by Eric Payne."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/d62932c3b2fcacc81dc1f5048cdeb60fb0d38504", "message": "YARN-7032. [ATSv2] NPE while starting hbase co-processor when HBase authorization is enabled. Contributed by Rohith Sharma K S."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/fe5b057c8144d01ef9fdfb2639a2cba97ead8144", "message": "YARN-7620. Allow node partition filters on Queues page of new YARN UI. Contributed by Vasudevan Skm."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/6681dd10075f732a99e0e1f980368fc58ba45c68", "message": "YARN-7633. Documentation for auto queue creation feature and related configurations. Contributed by Suma Shivaprasad."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/890d3d06456a026d9551a0cf15ce3986b0641454", "message": "YARN-7638. Unit tests related to preemption for auto created leaf queues feature.Contributed by Suma Shivaprasad."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/2564b4d07f63de142fe1909d61904831c108667c", "message": "YARN-7119. Support multiple resource types in rmadmin updateNodeResource command. Contributed by Manikandan R."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/cb87e4dc927731e32b0bbcf678bb5600835ff28d", "message": "YARN-7643. Handle recovery of applications in case of auto-created leaf queue mapping. Contributed by Suma Shivaprasad."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/10fc8d2a7dc975ce70de63f88a2674389314f197", "message": "YARN-7383. Node resource is not parsed correctly for resource names containing dot. Contributed by Gergely Nov\u00e1k."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/880cd754a542ac33855dd26e2e8cc26909acacb9", "message": "YARN-7536. em-table improvement for better filtering in new YARN UI. Contributed by Vasudevan Skm. This closes #313."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/fa4ecd8b9847f1cd4395f500b8693233a48f8a33", "message": "YARN-7641. Allow searchable filter for Application page log viewer in new YARN UI. Contributed by Vasudevan Skm. This closes #312."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/8bb83a8f625953e2a45db4bbbfb95cd41bac6af5", "message": "Queue ACL validations should validate parent queue ACLs before auto-creating leaf queues. Contributed by Suma Shivaprasad."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/5c87fb2f627ee64f5713d25cbea4a8956ed4de9c", "message": "YARN-7635. TestRMWebServicesSchedulerActivities fails in trunk. Contributed by Sunil G."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/312ceebde8ef8881fc43d82a096fb852f833a206", "message": "YARN-7632. Effective min and max resource need to be set for auto created leaf queues upon creation and capacity management. Contributed by Suma Shivaprasad."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/532bbf4602e418276961cfc4f6b5e3f70e9cedc0", "message": "YARN-7533. Documentation for absolute resource support in Capacity Scheduler. Contributed by Sunil G."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/4db4a4a165d3936d59f5e67947c6bfbc9c3270a5", "message": "YARN-7575. NPE in scheduler UI when max-capacity is not configured. Contributed by Sunil G."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/daa1cdd062657a47acbf4b23f895860296241199", "message": "YARN-7564. Cleanup to fix checkstyle issues of YARN-5881 branch. Contributed by Sunil G."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/a957f1c60e1308d1d70a1803381994f59949c5f8", "message": "YARN-7438. Additional changes to make SchedulingPlacementSet agnostic to ResourceRequest / placement algorithm. Contributed by Wangda Tan"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/99ccca341f3669b801428dea0acdba597f34c668", "message": "YARN-7092. Render application specific log under application tab in new YARN UI. Contributed by Akhil PB."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/f9f317b70209d20161d66a73ddea3334d2b92f96", "message": "YARN-7586. Application Placement should be done before ACL checks in ResourceManager. Contributed by Suma Shivaprasad."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/81f6e46b2fb54659a08864677ad14f80fe4e452d", "message": "YARN-6907. Node information page in the old web UI should report resource types. Contributed by Gergely Nov\u00e1k."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/30f2646b159d0d8d192e33d38434b7056855b468", "message": "YARN-7594. TestNMWebServices#testGetNMResourceInfo fails on trunk. Contributed by Gergely Nov\u00e1k."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/556aea3f367bdbd4e4db601bea0ca9bf2adde063", "message": "YARN-7487. Ensure volume to include GPU base libraries after created by plugin. Contributed by Wangda Tan."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/4653aa3eb31fb23fa19136173685464d71f86613", "message": "YARN-7546. Layout changes in Queue UI to show queue details on right pane. Contributed by Vasudevan Skm."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/59", "title": "HADOOP-12321", "body": "Rebasing as per latest trunk, This is an aggregated patch with all changes together.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/347519480", "body": "Committed to trunk with sha id 641ba5c7a1471f8d799b1f919cd41daffb9da84e", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/347519480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "brahmareddybattula": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/08332e12d055d85472f0c9371fefe9b56bfea1ed", "message": "HADOOP-15150. in FsShell, UGI params should be overidden through env vars(-D arg). Contributed by Brahma Reddy Battula."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/880b9d24ff7b5f350ec99bac9b0862009460b486", "message": "HDFS-8693. refreshNamenodes does not support adding a new standby to a running DN. Contributed by Ajith S."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/6d16a99ae8821c13eec90132e2c63a96fce4b08a", "message": "HDFS-11751. DFSZKFailoverController daemon exits with wrong status code. Contributed by Bharat Viswanadham"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/14279774", "body": "Hi Vinod,\n\nCan I get the Jira Id for this commit..?\n\nThanks.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/14279774/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/19446926", "body": "again \"import javax.servlet.ServletContext\" is missed which brokes branch-2 compliation and MetricsServlet changes are not present.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19446926/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/19461785", "body": "Please update the CHANGES.txt\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19461785/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/19461789", "body": "Please update the CHANGES.txt\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19461789/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/19462196", "body": "Please update the CHANGES.txt\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19462196/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/22364971", "body": "looks jjira id is missed in commit message", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/22364971/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "aajisaka": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/cdaf92c9f57560219b8f915a19ad8603ddf2a505", "message": "HADOOP-15177. Update the release year to 2018. Contributed by Bharat Viswanadham."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/fbbbf59c82658e18dad7e0e256613187b5b75d0f", "message": "YARN-7735. Fix typo in YARN documentation. Contributed by Takanobu Asanuma."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/d2d8f4aeb3e214d1a96eeaf96bbe1e9301824ccd", "message": "HADOOP-15133. [JDK9] Ignore com.sun.javadoc.* and com.sun.tools.* in animal-sniffer-maven-plugin to compile with Java 9."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/5ab632baf52f0ecc737845051b382f68bf1385bb", "message": "HDFS-12949. Fix findbugs warning in ImageWriter.java."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/001008958d8da008ed2e3be370ea4431fd023c97", "message": "YARN-7664. Several javadoc errors. Contributed by Sean Mackrory."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/c2e8a5c2296a1e5033f5750d9d0a0c668ec7162f", "message": "MAPREDUCE-7000. Moving logging APIs over to slf4j in hadoop-mapreduce-client-nativetask. Contributed by Jinjiang Ling."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/d4cae977a2471ad7b8f803617e41b6f94df19c11", "message": "MAPREDUCE-6998. Moving logging APIs over to slf4j in hadoop-mapreduce-client-jobclient. Contributed by Gergely Nov\u00e1k."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/6cca5b3bcb440095f12d3eda88101fa250ac000a", "message": "HDFS-12889. Addendum patch to add missing file."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/44b06d34a537f8b558007cc92a5d1a8e59b5d86b", "message": "HDFS-12889. Router UI is missing robots.txt file. Contributed by Bharat Viswanadham."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/9f1bdafedb60f83598819c6a682f659d6e168eb0", "message": "HADOOP-14985. Remove subversion related code from VersionInfoMojo.java. Contributed by Ajay Kumar."}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/123165687", "body": "I've committed the patch in https://issues.apache.org/jira/browse/HADOOP-12081 to trunk and branch-2. Would you close this issue?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/123165687/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142562", "body": "Thank you for the pull request. I reviewed the patch (A) and the another patch in YARN-4434 jira (B) and decided to commit the patch (B) because the patch (B) replaces \"i.e. the entire disk\" with \"i.e. 90% of the disk\" as well.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142562/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142634", "body": "I've committed the patch (B), so would you close this pull request?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142634/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/205168263", "body": "Thank you for rebasing! Mostly looks good to me. Two comments:\n- Can we separate the patch into common change and hdfs change? That way the patch becomes smaller and the review becomes easier.\n- I'm thinking we should replace not only `test.build.data` but also `test.build.dir` with `GenericTestUtils.getTestDir`. I'm okay with doing this by separate jira(s).\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/205168263/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/205680037", "body": "Thank you for the update! I'm +1 if the above comments are addressed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/205680037/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/225666551", "body": "Mostly looks good to me. Would you fix the checkstyle warnings? I'm +1 if that is addressed.\n(I commented here because ASF JIRA is down now) \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/225666551/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/227961467", "body": "I've committed this. Would you close this pull request?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/227961467/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/227961533", "body": "Would you close this pull request?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/227961533/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/227961831", "body": "Now HDFS-9459 is fixed.\n@cnauroth Would you close this pull request? \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/227961831/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/227969989", "body": "@iSultan Thanks for reporting this. Even though the fix is minor, please created a JIRA at https://issues.apache.org/jira/browse/MAPREDUCE\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/227969989/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/236324571", "body": "Hi @SahilKang, thank you for reporting this issue and creating the patch.\n- Would you fix the checkstyle warnings? https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6651/artifact/patchprocess/diff-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core.txt\n- Would you fix org.apache.hadoop.mapred.TextOutputFormat as well?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/236324571/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/237516401", "body": "+1 pending Jenkins. Thanks @SahilKang for the update.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/237516401/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/255941697", "body": "HADOOP-13434 has been fixed. Hi @omalley, would you close this pull request?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/255941697/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/255976199", "body": "This issue has been fixed by 58ed4fa5449872d65efd52d840f02dd60af2771a. Hi @chu11, Would you close this pull request?\nThank you for reporting this.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/255976199/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/273997803", "body": "The patch is committed. Hi @lukmajercak , would you close this pull request?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/273997803/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/284467506", "body": "Hi @adyatlov, the patch looks good to me. Your next step is to file an issue on [ASF Jira](https://issues.apache.org/jira/browse/HADOOP/) and rename the title of this PR to \"HADOOP-XXXXX (issue number): fix spelling\" to integrate with ASF Jira.\r\n\r\nHowToContribute\r\nhttps://wiki.apache.org/hadoop/HowToContribute#Contributing_your_work", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/284467506/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/284469376", "body": "Hi @zhang-yuan, this issue exists in branch-2.5 and branch-2.6 but these branches are effectively EOL. Would you close this pull request?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/284469376/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/284615681", "body": "Thanks!", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/284615681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/286032233", "body": "Committed this to trunk. Thanks @adyatlov for your first contribution to Apache Hadoop.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/286032233/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/306817486", "body": "Would you target branch-2.8 instead of branch-2.8.1?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/306817486/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/309369709", "body": "Thanks @pgaref for the PR and it looks good to me. Would you file a issue as a sub-task of https://issues.apache.org/jira/browse/YARN-5079 ?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/309369709/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/309369920", "body": "Would you close this PR?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/309369920/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/309369976", "body": "Would you close this PR?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/309369976/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/309626354", "body": "I've committed this to yarn-native-services branch. Would you close this PR?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/309626354/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/314324235", "body": "Now this PR is merged. Hi @maobaolong, would you close this PR?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/314324235/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/320161741", "body": "Hi @wenxinhe, would you close this PR?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/320161741/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/320162823", "body": "Thanks!", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/320162823/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/356975145", "body": "@dchuyko Thank you for your contribution! Would you close this pull request?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/356975145/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58334727", "body": "I'm thinking the string concatenation by `+` is unnecessarily.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58334727/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58492524", "body": "- The code creates String by `File.getAbsolutePath()` and then creates File by `new File(String)`. It can be simplified by the following and `TEST_ROOT_DIR` can be removed.\n\n```\n private static final File TEST_DIR = new File(GenericTestUtils.getTestDir(), \"fu\");\n```\n- `cacheDir` is unused.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58492524/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/59335222", "body": "I found some other occurrences of `/tmp/xxx`, which is used as the root dir of `FileContextTestHelper`. Would you replace them? I'm thinking the following is fine:\n\n```\nreturn new FileContextTestHelper(GenericTestUtils.getTempPath(\"TestWebHdfsFileContextMainOperations\"));\n```\n\nI'm okay if the replace is done in separate jira(s).\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/59335222/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/62836827", "body": "It looks to me that the javac warning is related because the patch removes `@SuppressWarnings(\"unchecked\")`.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/62836827/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67744731", "body": "Thank you for updating the pull request! Mostly looks good to me.\n(nit) Unused argument `resourceManager` can be removed. I'm +1 if that is addressed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67744731/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "vinayakumarb": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/09efdfe9e13c9695867ce4034aa6ec970c2032f1", "message": "HDFS-9049. Make Datanode Netty reverse proxy port to be configurable. Contributed by Vinayakumar B."}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/205285404", "body": "Rebased,\nand separated the common changes and hdfs changes.\nPushed only common changes in this PR.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/205285404/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/226459736", "body": "Merged to trunk.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/226459736/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "ajfabbri": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/268ab4e0279b3e40f4a627d3dfe91e2a3523a8cc", "message": "HADOOP-15141 Support IAM Assumed roles in S3A. Contributed by Steve Loughran."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/6555af81a26b0b72ec3bee7034e01f5bd84b1564", "message": "HADOOP-14475 Metrics of S3A don't print out when enabled. Contributed by Younger and Sean Mackrory."}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/345876619", "body": "Looks good so far.. Commented on a couple of minor typos.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/345876619/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/346132661", "body": "These last two commits look fine as well.  I'm +1 as of commit d5dcf98", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/346132661/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "ChenSammi": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/9195a6e302028ed3921d1016ac2fa5754f06ebf0", "message": "HADOOP-15027. AliyunOSS: Support multi-thread pre-read to improve sequential read from Hadoop to Aliyun OSS performance. (Contributed by Jinhu Wu)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/67b2661e3d73a68ba7ca73b112bf6baea128631e", "message": "HADOOP-15080.  Aliyun OSS: update oss sdk from 2.8.1 to 2.8.3 to remove its dependency on Cat-x json-lib."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mukulhorton": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/2e1e9017aae7d88b443f7efb7217cf3b56ab0075", "message": "HADOOP-15172. Fix the javadoc warning in WriteOperationHelper.java\n\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tasanuma": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/1a9c5d479e2259ac024da392b020967112c5af55", "message": "MAPREDUCE-7034. Moving logging APIs over to slf4j the rest of all in hadoop-mapreduce\n\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "arp7": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/7016dd44e0975274856dc19f19815123c4b2a352", "message": "HDFS-13016. globStatus javadoc refers to glob pattern as \"regular expression\". Contributed by Mukul Kumar Singh."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/b278f7b29305cb67d22ef0bb08b067c422381f48", "message": "HDFS-12984. BlockPoolSlice can leak in a mini dfs cluster. Contributed by Ajay Kumar."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/addbcd8cd44de25f9fcb1920183155609908aa91", "message": "HADOOP-15114. Add closeStreams(...) to IOUtils. Contributed by Ajay Kumar."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/739d3c394d772783fe23b386274d58954c9a0236", "message": "HDFS-12987. Document - Disabling the Lazy persist file scrubber.. Contributed by Karthik Palanisamy."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/4379113bda98eab47b00d17d175d57074230aac9", "message": "HADOOP-15093. Deprecation of yarn.resourcemanager.zk-address is undocumented. Contributed by Ajay Kumar."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/4ad39ec3084ab45fb9bbace13082c88666a76a4c", "message": "Revert \"HADOOP-10054. ViewFsFileStatus.toString() is broken. Contributed by Hanisha Koneru.\"\n\nThis reverts commit 37efa67e377e7fc251ee0088098f4b1700d21823."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/d82874851ea98caeb0ef5c23b7bc5d6fc14145ba", "message": "HADOOP-15155. Error in javadoc of ReconfigurableBase#reconfigureProperty. Contributed by Ajay Kumar."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/b4d11337c9271f29dd403d2393812f2ab6f35b35", "message": "HDFS-12351. Explicitly describe the minimal number of DataNodes required to support an EC policy in EC document.. Contributed by Hanisha Koneru."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/dfe0cd86553bd2688603ea382ea593171d520471", "message": "HADOOP-15152. Typo in javadoc of ReconfigurableBase#reconfigurePropertyImpl. Contributed by Nanda kumar."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/e00c7f78c1c00467319ce5b92e4a3ebc691d246e", "message": "HADOOP-14976. Set HADOOP_SHELL_EXECNAME explicitly in scripts."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/168", "title": "HDFS-11182. Update DataNode to use DatasetVolumeChecker.", "body": "Preliminary patch for Jenkins runs.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/165", "title": "HDFS-11149. Support for parallel checking of FsVolumes.", "body": "Initial patch for Jenkins test run. Will add some more tests for stalled checks.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/162", "title": "HDFS-11148. Update DataNode to use StorageLocationChecker at startup.", "body": "Change-Id: I664e5c719921ef5e0891bc392f37ee67639a8660", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/83", "title": "HDFS-9941. Do not log StandbyException on NN, other minor logging fixes.", "body": "v1 patch includes the following fixes:\n# Suppress StandbyException log messages for NameNode.\n# {{saveAllocatedBlock}} logs the block locations (DN xfer addresses).\n# {{logBlockReplicationInfo}} logs to the blockStateChangeLog instead of {{DecomissionManager#LOG}}. Also added a log level guard.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/236274524", "body": "+1 lgtm.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/236274524/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/258954518", "body": "Renamed it to shutdownAndWait.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/258954518/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/268151428", "body": "@xiaoyuyao I pushed one more commit to improve the logging. Now we log at warn if there is a volume failure and at debug if there is no failure.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/268151428/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/20178778", "body": "Done, thanks a lot again @xiaoyuyao!", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/20178778/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606373", "body": "Consider returning a List.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606373/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606818", "body": "See HDFS-9478 which is fixing exception handling when constructing callqueue instances. We could use a similar fix for createScheduler.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606818/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 1}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831863", "body": "The dn.getConf() object is not referenced outside the constructor so you can just pass a reference to that object. Also DNConf need not keep a reference to the dn. I think you can just revert all changes to this file.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831863/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831889", "body": "If we revert changes to DNConf we can just replace this with `this.dnConf = new DNConf(getConf())`.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831889/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/86861747", "body": "Thanks for taking a look @anuengineer. The method covers both shutdown and awaitTermination semantics. I could call it shutdownAndAwaitTermination() to make it clearer.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/86861747/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88357779", "body": "Yes there's new tests in TestStorageLocationChecker added by HDFS-11119 which cover this better with different combinations of failed/healthy storages.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88357779/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93129689", "body": "CountDownLatch#countDown returns no value so there is no easy way to detect when the count falls to zero and the callback can be invoked (it must be invoked once only). I was using an AtomicLong to detect the 0->1 transition but it had a bug.\r\n\r\nThe semaphore approach fixes it. We still need the CountDownLatch which we can use as an event. I could have used an Object mutex instead but that would have required extra code to deal with the spurious wakeup problem which CountDownLatch does not suffer from.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93129689/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93130106", "body": "Yes. FsVolumeList is part of the fsdataset.impl package and its methods are only invoked from FsDatasetImpl so it is safe to assume that the volume is an FsVolumeImpl.\r\n\r\nAt least one existing method also makes the same assumption (see copyReplicaWithNewBlockIdAndGS).\r\n```\r\n  private File[] copyReplicaWithNewBlockIdAndGS(\r\n      ReplicaInfo replicaInfo, String bpid, long newBlkId, long newGS)\r\n      throws IOException {\r\n    String blockFileName = Block.BLOCK_FILE_PREFIX + newBlkId;\r\n    FsVolumeImpl v = (FsVolumeImpl) replicaInfo.getVolume();\r\n```", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93130106/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93133957", "body": "I think this logic can be simplified. Will post an updated patch shortly.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93133957/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93168774", "body": "Will address it in a follow up patch.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93168774/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93168784", "body": "Good point, will push an update shortly to improve the logging.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93168784/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93169191", "body": "By the way regarding the reuse, I really wanted to do that too but it's non-trivial because the handling logic is different in both paths. It probably should have never been made different but reconciling them now is a bit of work. We can look at it in a separate Jira.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93169191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "wangdatan": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/edcc3a95d5248883492f2960f4fd22e09612ee9c", "message": "YARN-7468. Provide means for container network policy control. (Xuan Gong via wangda)\n\nChange-Id: I73678c343f663412917758feef35d8308c216e76"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/a81144daa012e13590725f67f53e35ef84a6f1ec", "message": "YARN-7666. Introduce scheduler specific environment variable support in ApplicationSubmissionContext for better scheduling placement configurations. (Sunil G via wangda)\n\nChange-Id: I0fd826490f5160d47d42af2a9ac0bd8ec4e959dc"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/7467e8fe5a95230986fed9d748769304af3f2b61", "message": "YARN-7555. Support multiple resource types in YARN native services. (wangda)\n\nChange-Id: I330e6ee17a73962dcaadd766a3e72d2888681731"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/44825f096068dfc17fe4dcfb7db3d775b08fa7a0", "message": "YARN-7629. TestContainerLaunch# fails after YARN-7381. (Jason Lowe via wangda)\n\nChange-Id: Ia6a3f05c9a7e797d8190123d304ecc4e2b018e33"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/631b5c2db733b0733a779e843b8035f68d0fcdf3", "message": "YARN-5418. When partial log aggregation is enabled, display the list of aggregated files on the container log page. (Xuan Gong via wangda)\n\nChange-Id: I1befb0bbaeb89fb315bafe3e2f3379663f8cf1ec"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/04b84da2456fb8c716e728b16db4851e2e87ec25", "message": "YARN-7443. Add native FPGA module support to do isolation with cgroups. (Zhankun Tang via wangda)\n\nChange-Id: Ic4b7f9f3e032986b8f955139c9fe4d3a6c818a53"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/adca1a72e4eca2ea634551e9fb8e9b878c36cb5c", "message": "YARN-7591. NPE in async-scheduling mode of CapacityScheduler. (Tao Yang via wangda)\n\nChange-Id: I46689e530550ee0a6ac7a29786aab2cc1bdf314f"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/a8316df8c05a7b3d1a5577174b838711a49ef971", "message": "YARN-7520. Queue Ordering policy changes for ordering auto created leaf queues within Managed parent Queues. (Suma Shivaprasad via wangda)\n\nChange-Id: I482f086945bd448d512cb5b3879d7371e37ee134"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/f548bfffbdcd426811352d6920ee5fe50cd0182c", "message": "YARN-7420. YARN UI changes to depict auto created queues. (Suma Shivaprasad via wangda)\n\nChange-Id: I8039d3772a191ddede132cd1f8b08a8ca2e275b7"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/b38643c9a8dd2c53024ae830b9565a550d0ec39c", "message": "YARN-7473. Implement Framework and policy for capacity management of auto created queues. (Suma Shivaprasad via wangda)\n\nChange-Id: Icca7805fe12f6f7fb335effff4b121b6f7f6337b"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/74665e3a7d7f05644d9a5abad5a3f2d47597d6c8", "message": "YARN-7274. Ability to disable elasticity at leaf queue level. (Zian Chen via wangda)\n\nChange-Id: Ic8d43e297f0f5de788b562f7eff8106c5c35e8d2"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/1012b901c8eeeb46c7c792c94ed0befca1c860b4", "message": "YARN-7544. Use queue-path.capacity/maximum-capacity to specify absolute min/max resources. (Sunil G via wangda)\n\nChange-Id: I685341be213eee500f51e02f01c91def89391c17"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/b7b8cd53242da8d47ba4a6d99d906bdb2a1a3494", "message": "YARN-7538. Fix performance regression introduced by Capacity Scheduler absolute min/max resource refactoring. (Sunil G via wangda)\n\nChange-Id: Ic9bd7e599c56970fe01cb0e1bba6df7d1f77eb29"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/7462c38277fa415703fc9074c1288d3bec73609c", "message": "YARN-7483. CapacityScheduler test cases cleanup post YARN-5881. (Sunil G via wangda)\n\nChange-Id: I9741a6baf5cb7352d05636efb6c0b24790e7589a"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/034b312d9f19024d2eabd377210d17d4080ef70e", "message": "YARN-7411. Inter-Queue preemption's computeFixpointAllocation need to handle absolute resources while computing normalizedGuarantee. (Sunil G via wangda)\n\nChange-Id: I41b1d7558c20fc4eb2050d40134175a2ef6330cb"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/aa3f62740f71e6e5b2a424a9f4654c1a4ba1dbe6", "message": "YARN-7332. Compute effectiveCapacity per each resource vector. (Sunil G via wangda)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/d52627a7cbddfd981db973e223aefffde1ebf82d", "message": "YARN-7254. UI and metrics changes related to absolute resource configuration. (Sunil G via wangda)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/5e798b1a0ddceeaf54703b94052501867156e979", "message": "YARN-6471. Support to add min/max resource configuration for a queue. (Sunil G via wangda)\n\nChange-Id: I9213f5297a6841fab5c573e85ee4c4e5f4a0b7ff"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/0311cf05358cd75388f48f048c44fba52ec90f00", "message": "YARN-7381. Enable the configuration: yarn.nodemanager.log-container-debug-info.enabled by default in yarn-default.xml. (Xuan Gong via wangda)\n\nChange-Id: I1ed58dafad5cc276eea5c0b0813cf04f57d73a87"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/7225ec0ceb49ae8f5588484297a20f07ec047420", "message": "YARN-6507. Add support in NodeManager to isolate FPGA devices with CGroups. (Zhankun Tang via wangda)\n\nChange-Id: Ic9afd841805f1035423915a0b0add5f3ba96cf9d"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/a63d19d36520fa55bf523483f14329756f6eadd3", "message": "YARN-6124. Make SchedulingEditPolicy can be enabled / disabled / updated with RMAdmin -refreshQueues. (Zian Chen via wangda)\n\nChange-Id: Id93656f3af7dcd78cafa94e33663c78d410d43c2"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/c9a54aab6b1ad91b14de934178018d8e7eecd001", "message": "YARN-7573. Gpu Information page could be empty for nodes without GPU. (Sunil G via wangda)\n\nChange-Id: I7f614e5a589a09ce4e4286c84b706e05c29abd14"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jian-he": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/c05b84703b7754b6c2fbcec39f22b5d7802563ec", "message": "YARN-7671. Improve Diagonstic message for stop yarn native service. Contributed by Chandni Singh"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/836e3c45e8232fc4c8c795c0f93d2f3d7353f392", "message": "Revert \"YARN-7540.  Route YARN service CLI function through YARN Service API. (Contributed by Eric Yang)\"\n\nThis reverts commit 438c1d333ebc0a3071bb556532ed959a4bd1e6d6."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/3ebe6a7819292ce6bd557e36137531b59890c845", "message": "YARN-7565. Yarn service pre-maturely releases the container after AM restart. Contributed by Chandni Singh"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bibinchundatt": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/b26e30ab1655a8cdf369862a5512db5c2a88ec46", "message": "YARN-7508. NPE in FiCaSchedulerApp when debug log enabled in async-scheduling mode. Contributed by Tao Yang."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "TolerableCoder": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/59ab5da0a0337c49a58bc9b2db9d1a89f4d5b9dd", "message": "YARN-4227. Ignore expired containers from removed nodes in FairScheduler. (Wilfred Spiegelenburg via rchiang)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "manojpec": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/73ff09b79a5cf9932edc21c58f3a730f7379086b", "message": "HDFS-12985. NameNode crashes during restart after an OpenForWrite file present in the Snapshot got deleted."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/5c28804b953463ebdd1e9d32ad50f11887b6e277", "message": "HDFS-12629. NameNode UI should report total blocks count by type - replicated and erasure coded."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/42a1c98597e6dba2e371510a6b2b6b1fb94e4090", "message": "HDFS-11847. Enhance dfsadmin listOpenFiles command to list files blocking datanode decommissioning."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/76e664e931bf0784620b69bc588bd51cf2a024e6", "message": "HDFS-12959. Fix TestOpenFilesWithSnapshot redundant configurations."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/ef7d334d364378070880e647eaf8bac2f12561ee", "message": "HDFS-12825. Fsck report shows config key name for min replication issues (Contributed by Gabor Bota)."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ywskycn": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/2ee0d64aceed876f57f09eb9efe1872b6de98d2e", "message": "HDFS-12945. Switch to ClientProtocol instead of NamenodeProtocols in NamenodeWebHdfsMethods. Contributed by Wei Yan."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/a78db9919065d06ced8122229530f44cc7369857", "message": "HDFS-12932. Fix confusing LOG message for block replication. Contributed by Chao Sun."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/333ef303ff0caf9adfd378652a8f966377901768", "message": "YARN-6851. Capacity Scheduler: document configs for controlling # containers allowed to be allocated per node heartbeat."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xiaoyuyao": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/3ba985997d1dc37e5ba017dd0ab1d36083b5f77b", "message": "HDFS-12931. Handle InvalidEncryptionKeyException during DistributedFileSystem#getFileChecksum. Contributed by Mukul Kumar Singh."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/37efa67e377e7fc251ee0088098f4b1700d21823", "message": "HADOOP-10054. ViewFsFileStatus.toString() is broken. Contributed by Hanisha Koneru."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/86", "title": "HADOOP-12916", "body": "Create a github PR for code review of v04 patch on the JIRA. \n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/260535953", "body": "Update PR to fix the findbugs and unit test failure. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/260535953/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/265297854", "body": "close as change commit to trunk.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/265297854/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/20178230", "body": "Looks good to me. +1. Can you update the patch on Apache?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/20178230/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57619429", "body": "Good point. I will address that in the next patch.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57619429/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87705381", "body": "OzoneClientUtils.getScmRpcTimeOutInMilliseconds(conf) can be encapsulated inside SCMConnectionManager constructor as we have passed the conf parameter anyway. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87705381/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87705447", "body": "Can we add a Log.error() after the Thread.currentThread.interrupt() for troubleshooting of shutdown issues in the future?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87705447/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87705496", "body": "AtomicLong instead of AtomicInteger?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87705496/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87705545", "body": "Can you document the same assumption that the state is transited by value + 1.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87705545/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87705548", "body": "rpcTimeout parameter can be removed\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87705548/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87705573", "body": "NIT: extra blank line.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87705573/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87715316", "body": "I don't think we return a List here. Can you clarify here as well as \"Returns a lit of tasks that..;\"\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87715316/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87715382", "body": "Should we update the states document here without upgrade/decommission state since they are not implemented yet.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87715382/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87715439", "body": "javadoc needs update? init container task-> init datanode state\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87715439/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87828098", "body": "Should we return null after mark DatanodeStates.SHUTDOWN instead of continue attempt to read the container ID?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87828098/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87828369", "body": "Add a LOG.trace() for containerID read from file successfully. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87828369/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87828584", "body": "Add a trace for ContainerID created?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87828584/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87834673", "body": "\"if any endpoint state has moved to Register state,...\", this seems not being enforced in the code below. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87834673/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87836578", "body": "NIT: maybe use a builder pattern to avoid creating RegisterEndpointTask without ContainerNodeID.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87836578/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87836750", "body": "Builder pattern as suggested earlier.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87836750/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87837201", "body": "Builder pattern.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87837201/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87837890", "body": "NIT: Consider returning the new state from rpcEndPoint.setState so that we can consolidate logic here?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87837890/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87840114", "body": "The second parameter can be removed as ScmHeartbeatInterval is available within rpcEndpoint.logIfNeeded() with RpcEndpoint#conf.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87840114/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87842991", "body": "Can you elaborate the comment on \"HDFS node\"?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87842991/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87849676", "body": "NIT: formatting: address can be moved to the line above with its type.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87849676/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87850730", "body": "As commented earlier, we will need some negative cases to ensure datanode will start if there is any valid scm address in the OZONE_SCM_NAMES string.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87850730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87851382", "body": "Can we use the GenericTestUtils.waitFor() to avoid the unpredictable Thread.sleep()?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87851382/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87851602", "body": "NIT: extra blank line.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87851602/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87853234", "body": "This can be used as a helper function to avoid duplication among cases in testHeartbeatTask*\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87853234/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93122724", "body": "The usage of semaphore here seems like a countUpLatch. Have you hit any problem with the existing CountDownLatch approach?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93122724/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93123146", "body": "Is it a safe cast from FsVolumeSpi to FsVolumeImpl? Can we add some log here in case the cast fail?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93123146/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93124387", "body": "NIT: maybe wrap the common test prep code with a helper for testMinGapIsEnforcedForSyncChecks() and testMinGapIsEnforcedForASyncChecks().", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93124387/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93132992", "body": "Thanks for the explanation.  Looks good to me. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93132992/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93165428", "body": "The Datanode#storageLocationChecker is only needed during the datanode startup. We don't need to pass it as a parameter to DataNode constructor and keep it running during the lifetime of the datanode until datanode shutdown. This can be done as an optimization later. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93165428/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93165658", "body": "Can we reuse the synchronize version of the DatasetVolume checker for datanode startup handling? This way, we don't need to maintain two checkers for Datanode? This can be done in as a follow up if possible.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93165658/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93165887", "body": "Can we log a warn instead of info for the failed volumes that got removed?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93165887/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93171107", "body": "Thanks. The new logic looks good to me.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93171107/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93171132", "body": "Agree. Let's do that in a follow up jira.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93171132/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93171165", "body": "Thanks for fixing that.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/93171165/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "xslogic": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/c0c7cce81d5609f6347bff67929d5026d5893d75", "message": "YARN-7691. Add Unit Tests for ContainersLauncher. (Sampada Dehankar via asuresh)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/a55884c68eb175f1c9f61771386c086bf1ee65a9", "message": "YARN-7542. Fix issue that causes some Running Opportunistic Containers to be recovered as PAUSED. (Sampada Dehankar via asuresh)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/37ca4169508c3003dbe9044fefd37eb8cd8c0503", "message": "YARN-7587. Skip dispatching opportunistic containers to nodes whose queue is already full. (Weiwei Yang via asuresh)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/143", "title": "YARN-4597: initial commit", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83769612", "body": "Removing this... since it takes a while and Opportunistic containers are not used for reducers in any case..\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83769612/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87664283", "body": "I chose a LinkedHashMap, since it is essentially an indexed Queue. We might need to check if a container exists in the queue or remove a specific container from the queue (for eg. when AM asks to kill a queued container).\nAlso, like you noted, we would also need to implement hashCode/equals for Container...\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87664283/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87664536", "body": "Ditto :)\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87664536/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87665434", "body": "Same as above... \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87665434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87665464", "body": "Ditto.. as above.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87665464/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87668067", "body": "Actually that message might be a bit misleading. It should be re-worded to 'There are no sufficient resources at the moment to start guaranteed...'. You can only reach here if kill signals have been sent to Opp. containers, but the completed container event hasn't been received by the Scheduler yet. At which point the guaranteed container will start.\nIt is never possible to allocate a guaranteed container on node that cannot start it.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87668067/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87668681", "body": "My IDE is very obstinate about it :) Can we leave it as it is.. especially since the checkstyle anyway ignores it ?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87668681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87668870", "body": "as mentioned earlier.. can we leave it as it is, since it is ignored by checkstyle ? \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87668870/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "cdouglas": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/7fe6f83c8f0f67b1456c37d94b0de807e81a904a", "message": "HADOOP-15117. open(PathHandle) contract test should be exhaustive for default options"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/5e81f32d1155ea96c892099008cfeb50799082eb", "message": "HADOOP-15106. FileSystem::open(PathHandle) should throw a specific exception on validation failure"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/fc7ec80d85a751b2b2b261a2b97ec38c7b58f1df", "message": "Merge branch 'HDFS-9806' into trunk"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/4b3a785914d890c47745e57d12a5a9abd084ffc1", "message": "HDFS-12903. [READ] Fix closing streams in ImageWriter. Contributed by Virajith Jalaparti"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/e515103a83e12ad4908c0ca0b4b1aa4a87e2a840", "message": "Revert \"HDFS-12903. [READ] Fix closing streams in ImageWriter\"\n\nThis reverts commit c1bf2654b0e9118985b8518b0254eac4dd302a2f."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/2298f2d76b2cafd84c8f7421ae792336d6f2f37a", "message": "HDFS-12874. Documentation for provided storage. Contributed by Virajith Jalaparti"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/693169ef34f856a27dc09d90a45fb4ec5b66ed2c", "message": "HDFS-12882. Support full open(PathHandle) contract in HDFS"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/42307e3c3abbfe0b83d9a2581deba327435b910f", "message": "HDFS-11576. Block recovery will fail indefinitely if recovery time > heartbeat interval. Contributed by Lukas Majercak"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/f9d195dfe9cc2c3e4659c3475319ac7c937b5c44", "message": "HADOOP-14600. LocatedFileStatus constructor forces RawLocalFS to exec a process to get the permissions. Contributed by Ping Liu"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/53bbef3802194b7a0a3ce5cd3c91def9e88856e3", "message": "Revert \"HDFS-11576. Block recovery will fail indefinitely if recovery time > heartbeat interval. Contributed by Lukas Majercak\"\n\nThis reverts commit 5304698dc8c5667c33e6ed9c4a827ef57172a723."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/5304698dc8c5667c33e6ed9c4a827ef57172a723", "message": "HDFS-11576. Block recovery will fail indefinitely if recovery time > heartbeat interval. Contributed by Lukas Majercak"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/0780fdb1ebdddd19744fbbca7fb05f8fe4bf4d28", "message": "HDFS-12877. Add open(PathHandle) with default buffersize"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/0e560f3b8d194c10dce06443979df4074e14b0db", "message": "HDFS-12681. Make HdfsLocatedFileStatus a subtype of LocatedFileStatus"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xiao-chen": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/81127616c571b7cd25686e60a1105f96ca0626b7", "message": "HADOOP-15149. CryptoOutputStream should implement StreamCapabilities."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/5bf7e594d7d54e5295fe4240c3d60c08d4755ab7", "message": "HDFS-9023. When NN is not able to identify DN for replication, reason behind it can be logged."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/e1cb278cd0287ce47f923941147d17540b199a99", "message": "HDFS-12910. Secure Datanode Starter should log the port when it fails to bind. Contributed by Stephen O'Donnell and Nanda kumar."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/19e089420999dd9d97d981dcd0abd64b6166152d", "message": "HADOOP-15056. Fix TestUnbuffer#testUnbufferException failure. Contributed by Jack Bearden."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/56b1ff80dd9fbcde8d21a604eff0babb3a16418f", "message": "HDFS-12872. EC Checksum broken when BlockAccessToken is enabled."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/404eab4dc0582e0384b93664ea6ee77ccd5eeebc", "message": "HDFS-12396. Webhdfs file system should get delegation token from kms provider. Contributed by Rushabh S Shah."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "varunsaxena": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/c0aeb666a4d43aac196569d9ec6768d62139d2b9", "message": "YARN-7662. [ATSv2] Define new set of configurations for reader and collectors to bind (Rohith Sharma K S via Varun Saxena)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "JunpingDu": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/a7f8caf58ed47574861d455a9d9e1521e35c10b9", "message": "Add 2.8.3 release jdiff files.\n\n(cherry picked from commit c89f99aade575ab1f6a9836df719cda272293d90)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/a409425986fc128bb54f656b05373201545f7213", "message": "YARN-7558. yarn logs command fails to get logs for running containers if UI authentication is enabled. Contributed by Xuan Gong."}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/14097788", "body": "Hi @rkanter, do we have a public JIRA to track this issue?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/14097788/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "szetszwo": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/c7499f2d242c64bee8f822a22161d956525f7153", "message": "HDFS-12347. TestBalancerRPCDelay#testBalancerRPCDelay fails very frequently.  Contributed by Bharat Viswanadham"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/b1c7654ee40b372ed777525a42981c7cf55b5c72", "message": "HDFS-12594. snapshotDiff fails if the report exceeds the RPC response limit. Contributed by Shashikant Banerjee"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xkrogen": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/94576b17fbc19c440efafb6c3322f53ec78a5b55", "message": "HDFS-12818. Support multiple storages in DataNodeCluster / SimulatedFSDataset. Contributed by Erik Krogen."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yangwwei": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/928964102029e96406f5482e8900802f38164501", "message": "YARN-7617. Add a flag in distributed shell to automatically PROMOTE opportunistic containers to guaranteed once they are started. Contributed by Weiwei Yang."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/89b6c482c1720a7f7ac86ce022c403825a086fa0", "message": "YARN-7642. Add test case to verify context update after container promotion or demotion with or without auto update. Contributed by Weiwei Yang."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/7efc4f76885348730728c0201dd0d1a89b213e9c", "message": "YARN-7647. NM print inappropriate error log when node-labels is enabled. Contributed by Yang Wang."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/a2edc4cbf5c1f7bc38315c52391362fbbc48fab1", "message": "YARN-7608. Incorrect sTarget column causing DataTable warning on RM application and scheduler web page. Contributed by Gergely Nov\u00e1k."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/e411dd6666041a4ea68ab34e734802271497ae6c", "message": "YARN-7607. Remove the trailing duplicated timestamp in container diagnostics message. Contributed by Weiwei Yang."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/40b0045ebe0752cd3d1d09be00acbabdea983799", "message": "YARN-7610. Extend Distributed Shell to support launching job with opportunistic containers. Contributed by Weiwei Yang."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/05c347fe51c01494ed8110f8f116a01c90205f13", "message": "YARN-7611. Node manager web UI should display container type in containers page. Contributed by Weiwei Yang."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vinoduec": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/95d4ec7fc07605d1ed6eabf066cd5413eb3fe465", "message": "YARN-7565. Addendum to fix an incompatible change. Contributed by Wangda Tan."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/f19638333b11da6dcab9a964e73a49947b8390fd", "message": "HADOOP-15059. Undoing the switch of Credentials to PB format as default - done via HADOOP-12563 for supporting 2.x to 3.x upgrades."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "subru": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/17ba74be29193ac15474f73baaaf4e647a95078b", "message": "YARN-7630. Fix AMRMToken rollover handling in AMRMProxy. Contributed by Botong Huang."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/670e8d4ec7e71fc3b054cd3b2826f869b649a788", "message": "YARN-6704. Add support for work preserving NM restart when FederationInterceptor is enabled in AMRMProxyService. (Botong Huang via Subru)."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kihwal": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/f5a72424c0009c454aab6759c30f74b397a7e935", "message": "HDFS-12907. Allow read-only access to reserved raw for non-superusers. Contributed by Rushabh S Shah."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "umbrant": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/d447152d4925a5f84d28a8ebd561286b39134d75", "message": "Update CHANGES, RELEASENOTES, jdiff for 3.0.0 release."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jojochuang": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/55fc2d6485702a99c6d4bb261a720d1f0498af2b", "message": "HDFS-12891. Do not invalidate blocks if toInvalidate is empty. Contributed by Zsolt Venczel."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/d6c31a3e6b60c4b8af9ae4661f16614805654e59", "message": "HDFS-11915. Sync rbw dir on the first hsync() to avoid file lost on power failure. Contributed by Vinayakumar B."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/0faf50624580b86b64a828cdbbb630ae8994e2cd", "message": "HDFS-12836. startTxId could be greater than endTxId when tailing in-progress edit log. Contributed by Chao Sun."}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/282060103", "body": "Hello @MonsterSupreme thanks for reporting the issue.\r\nCan you please file a jira? Also, it looks like your PR needs a rebase.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/282060103/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/330019090", "body": "Hi, @qinchen123 thanks for the PR. Please read the wiki How to Contribute https://wiki.apache.org/hadoop/HowToContribute\r\nBasically, you need to file a jira first and link your PR to the jira. Thanks", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/330019090/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/331049233", "body": "Hi @liumihust  thanks for the PR. Would you please follow How To Contribute wiki https://wiki.apache.org/hadoop/HowToContribute and start by filing a jira? Thanks!", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/331049233/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "shvachko": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/60fd0d7fd73198fd610e59d1a4cd007c5fcc7205", "message": "HDFS-12638. Delete copy-on-truncate block along with the original block, when deleting a file being truncated. Contributed by Konstantin Shvachko."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Yiran-wu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/331", "title": "YARN-7773. Fix YARN Federation used Mysql as state store throw except\u2026", "body": "YARN-7773 YARN Federation used Mysql as state store throw exception, Unknown column 'homeSubCluster' in 'field list'\r\n\r\n\r\n#331 \r\n\r\nsubmitApplication appIdapplication_1516277664083_0014 try #0 on SubCluster cluster1 , queue: root.bdp_federation\r\n[2018-01-18T23:25:29.325+08:00] [ERROR] store.impl.SQLFederationStateStore.logAndThrowRetriableException(FederationStateStoreUtils.java 158) [IPC Server handler 44 on 8050] : Unable to insert the newly generated application application_1516277664083_0014\r\ncom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'homeSubCluster' in 'field list'\r\nat sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)\r\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\nat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\nat com.mysql.jdbc.Util.handleNewInstance(Util.java:425)\r\nat com.mysql.jdbc.Util.getInstance(Util.java:408)\r\nat com.mysql.jdbc.SQLError.createSQLException(SQLError.java:944)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909)\r\nat com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527)\r\nat com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680)\r\nat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484)\r\nat com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1858)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2079)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2013)\r\nat com.mysql.jdbc.PreparedStatement.executeLargeUpdate(PreparedStatement.java:5104)\r\nat com.mysql.jdbc.CallableStatement.executeLargeUpdate(CallableStatement.java:2418)\r\nat com.mysql.jdbc.CallableStatement.executeUpdate(CallableStatement.java:887)\r\nat com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)\r\nat com.zaxxer.hikari.pool.HikariProxyCallableStatement.executeUpdate(HikariProxyCallableStatement.java)\r\nat org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore.addApplicationHomeSubCluster(SQLFederationStateStore.java:547)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\nat com.sun.proxy.$Proxy31.addApplicationHomeSubCluster(Unknown Source)\r\nat org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade.addApplicationHomeSubCluster(FederationStateStoreFacade.java:345)\r\nat org.apache.hadoop.yarn.server.router.clientrm.JDFederationClientInterceptor.submitApplication(JDFederationClientInterceptor.java:334)\r\nat org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService.submitApplication(RouterClientRMService.java:196)\r\nat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:218)\r\nat org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:419)\r\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\r\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2076)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2072)\r\nat java.security.AccessController.doPrivileged(Native Method)\r\nat javax.security.auth.Subject.doAs(Subject.java:422)\r\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1803)\r\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2070)\r\n[2018-01-18T23:25:29.326+08:00] [ERROR] server.router.RouterServerUtil.logAndThrowException(RouterServerUtil.java 55) [IPC Server handler 44 on 8050] : Unable to insert the ApplicationId application_1516277664083_0014 into the FederationStateStore\r\norg.apache.hadoop.yarn.server.federation.store.exception.FederationStateStoreRetriableException: Unable to insert the newly generated application application_1516277664083_0014\r\nat org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils.logAndThrowRetriableException(FederationStateStoreUtils.java:159)\r\nat org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore.addApplicationHomeSubCluster(SQLFederationStateStore.java:593)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\nat com.sun.proxy.$Proxy31.addApplicationHomeSubCluster(Unknown Source)\r\nat org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade.addApplicationHomeSubCluster(FederationStateStoreFacade.java:345)\r\nat org.apache.hadoop.yarn.server.router.clientrm.JDFederationClientInterceptor.submitApplication(JDFederationClientInterceptor.java:334)\r\nat org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService.submitApplication(RouterClientRMService.java:196)\r\nat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:218)\r\nat org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:419)\r\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\r\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2076)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2072)\r\nat java.security.AccessController.doPrivileged(Native Method)\r\nat javax.security.auth.Subject.doAs(Subject.java:422)\r\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1803)\r\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2070)\r\nCaused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'homeSubCluster' in 'field list'\r\nat sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)\r\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\nat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\nat com.mysql.jdbc.Util.handleNewInstance(Util.java:425)\r\nat com.mysql.jdbc.Util.getInstance(Util.java:408)\r\nat com.mysql.jdbc.SQLError.createSQLException(SQLError.java:944)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909)\r\nat com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527)\r\nat com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680)\r\nat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484)\r\nat com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1858)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2079)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2013)\r\nat com.mysql.jdbc.PreparedStatement.executeLargeUpdate(PreparedStatement.java:5104)\r\nat com.mysql.jdbc.CallableStatement.executeLargeUpdate(CallableStatement.java:2418)\r\nat com.mysql.jdbc.CallableStatement.executeUpdate(CallableStatement.java:887)\r\nat com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)\r\nat com.zaxxer.hikari.pool.HikariProxyCallableStatement.executeUpdate(HikariProxyCallableStatement.java)\r\nat org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore.addApplicationHomeSubCluster(SQLFederationStateStore.java:547)\r\n... 20 more \r\n\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wuzhilon": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/330", "title": "YARN PROXYSERVER throw IOEXCEPTION", "body": "When we more than ten users simultaneously submitted to view the proxyserver, there will be stuck, and then it will throw IO exception", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "skmvasu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/329", "title": "YARN-7760. precompute master node URL", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/328", "title": "YARN-7742 - Remove duplicate entries", "body": "[YARN-7742]", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/327", "title": "YARN-7749. Fix GPU sidebar", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/323", "title": "YARN-7750. Use local time to render dates", "body": "Use local time to render dates", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/315", "title": "YARN-7648. Fix attempts UI when app run fails", "body": "Fixes applications tab rendering when the app attempts have failed", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/345628306", "body": "@sunilgovind ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/345628306/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/345631578", "body": "@sunilgovind  Please review", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/345631578/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/345657860", "body": "@sreenaths @wangdatan Please help me with reviewing this. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/345657860/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/345952481", "body": "@sunilgovind @wangdatan ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/345952481/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/346751095", "body": "There are some issues with the patch generated. I've created another PR #298, by squash merging all commits. This patch is applying cleanly to trunk. @sunilgovind ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/346751095/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/347769410", "body": "@sunilgovind @wangdatan Rebased from trunk. Please review", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/347769410/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/347817486", "body": "@sunilgovind Rebased Akhil's patch to the new layout. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/347817486/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/348401101", "body": "Solved by #305 ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/348401101/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/348865150", "body": "Committed to trunk with sha 641ba5c7a1471f8d799b1f919cd41daffb9da84e", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/348865150/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/349907966", "body": "@sunilgovind Please review\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/349907966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/350665627", "body": "<img width=\"1439\" alt=\"screen shot 2017-12-11 at 2 46 35 pm\" src=\"https://user-images.githubusercontent.com/567228/33823736-2448d88e-de82-11e7-82f6-c62a64cf5190.png\">\r\n<img width=\"1059\" alt=\"screen shot 2017-12-11 at 2 46 41 pm\" src=\"https://user-images.githubusercontent.com/567228/33823737-2489c998-de82-11e7-9b07-21e68b91b855.png\">\r\n\r\nMakes the nodelabel dropdown searchable", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/350665627/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/351295577", "body": "Jenkins applies the patch commit by commit and not able to recover from the conflicts. Closing this in favour of https://github.com/apache/hadoop/pull/313\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/351295577/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/351349929", "body": "Merged to trunk with SHA 99ccca341f3669b801428dea0acdba597f34c668\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/351349929/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/357597836", "body": "@sunilgovind ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/357597836/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/357606848", "body": "This is closed already", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/357606848/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "gerashegalov": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/326", "title": "YARN-7747 use injected GuiceFilter instances", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/325", "title": "HADOOP-15166: simplify minicluster start", "body": "add minicluster subcommand to simplify its usage", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xshaun": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/324", "title": "Fix NullPointerException caused by null-builder", "body": "Sometimes occurs java.lang.NullPointerException leading to app failed.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gehaijiang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/321", "title": "Branch 3.0   operation shell script   ERROR", "body": "run  ./stop-dfs.sh  \r\n\r\ntext:\r\n\r\nStopping namenodes on [10.50.132.145 10.50.132.146 10.50.132.147]\r\nERROR: Both HADOOP_WORKERS and HADOOP_WORKER_NAMES were defined. Aborting.\r\nStopping datanodes\r\n10.50.132.147: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.151: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.146: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.150: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.154: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.145: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.152: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.148: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.149: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.153: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\nStopping journal nodes [10.50.132.145 10.50.132.146 10.50.132.147]\r\nERROR: Both HADOOP_WORKERS and HADOOP_WORKER_NAMES were defined. Aborting.\r\nStopping ZK Failover Controllers on NN hosts [10.50.132.145 10.50.132.146 10.50.132.147]\r\nERROR: Both HADOOP_WORKERS and HADOOP_WORKER_NAMES were defined. Aborting.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mmolimar": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/319", "title": "HADOOP-15142. Register FTP and SFTP as FS services", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "medb": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/318", "title": "[HADOOP-11032] Migrate Guava's Stopwatch to Hadoop's StopWatch", "body": "After this change Hadoop could build against Guava 21.0\r\n\r\nJustification for migration is the same as previous migrations here:\r\nhttps://issues.apache.org/jira/browse/HADOOP-11032 ", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/316", "title": "HADOOP-15124. Improve FileSystem.Statistics performance", "body": "This is PR for https://issues.apache.org/jira/browse/HADOOP-15124", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dwshmilyss": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/317", "title": "Branch 2.8.3", "body": "test\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "denis-zhdanov": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/314", "title": "POC: replace explicit method parameters null-checks by a declarative approach", "body": "This *PR* shows an approach when explicit *null*-checks (*Preconditions.checkNotNull()*) are generated automatically by the [Traute](http://traute.oss.harmonysoft.tech/) *javac* plugin for method parameters marked by *Nonnull* annotation.  \r\n\r\nExample: consider the [FSDataOutputStreamBuilder.permission()](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java#L151) - its bytecode looks like if it's compiled from the source below:  \r\n\r\n```java\r\npublic B permission(@Nonnull final FsPermission perm) {\r\n    if (perm == null) {\r\n        throw new NullPointerException(\"String Argument 'perm' of type FsPermission (#0 out of 1, zero-based) is marked by @javax.annotation.Nonnull but got null for it\");\r\n    }\r\n    Preconditions.checkNotNull(perm);\r\n    permission = perm;\r\n    return getThisBuilder();\r\n}\r\n```  \r\n\r\nDetails:  \r\n\r\n```\r\njavap -c ./hadoop-common-project/hadoop-common/target/classes/org/apache/hadoop/fs/FSDataOutputStreamBuilder.class\r\n...\r\n  public B permission(org.apache.hadoop.fs.permission.FsPermission);\r\n    Code:\r\n       0: aload_1\r\n       1: ifnonnull     14\r\n       4: new           #16                 // class java/lang/NullPointerException\r\n       7: dup\r\n       8: ldc           #31                 // String Argument 'perm' of type FsPermission (#0 out of 1, zero-based) is marked by @javax.annotation.Nonnull but got null for it\r\n      10: invokespecial #18                 // Method java/lang/NullPointerException.\"<init>\":(Ljava/lang/String;)V\r\n      13: athrow\r\n      14: aload_0\r\n      15: aload_1\r\n      16: putfield      #3                  // Field permission:Lorg/apache/hadoop/fs/permission/FsPermission;\r\n      19: aload_0\r\n      20: invokevirtual #32                 // Method getThisBuilder:()Lorg/apache/hadoop/fs/FSDataOutputStreamBuilder;\r\n      23: areturn\r\n```  \r\n\r\nSo, the idea is to do the following:  \r\n1. Go through the project's codebase and find all places where *Preconditions.checkNotNull()* is called for method parameter\r\n2. Ensure that target method parameter is marked by the *Nonnull* annotation (e.g. [ActiveStandbyElector.isStaleClient()](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java#L1124) is not marked by it, so, we need to add the annotation)\r\n3. Remove *Preconditions.checkNotNull()* call  \r\n\r\nBenefits:  \r\n* the code becomes cleaner without that explicit checks\r\n* the code is better documented as it's immediately clear what method parameters must be not-*null*\r\n* IDEs highlight possible *NPE* for method parameters marked by *Nonnull* annotations\r\n\r\nPlease let me know if you like the idea, I'm fine with providing a *PR* which applies the solution to the whole project's codebase then.\r\n\r\nTESTED: mvn clean package & ensured that resulting\r\n        bytecode has the checks", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/352248914", "body": "Does anybody from the team look for new *PR*s?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/352248914/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "addisonj": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/311", "title": "[HADOOP-15096] Don't create user with lastlog", "body": "This fixes a problem where in certain cases, the useradd command can create a very large diff that can blow up the host disk size.\r\n\r\nThe reason for this is that lastlog is a sparse file, but AUFS under docker apparently doesn't deal with those well and creates a very large file.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jiayuhan-it": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/309", "title": "MAPREDUCE-7017:Too many times of meaningless invocation in TaskAttemptImpl#resolveHosts", "body": "MRAppMaster uses TaskAttemptImpl::resolveHosts to determine the dataLocalHosts for each task when the location of data split is IP, which will call a lot of times ( taskNum * dfsReplication) of function InetAddress::getByName and most of the funcition calls are redundant. When the job has a great number of tasks and the speed of DNS resolution is not fast enough, it will take a lot of time at this stage before the job running.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vetriselvan1187": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/304", "title": "[YARN-7578] waitForDiskHealthCheck sleep time is extended from 1000ms\u2026", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "animenon": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/300", "title": "[HADOOP-15099] YARN Federation Link fix", "body": "The fix is for YARN Federation link on [Apache Hadoop YARN page](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html).\r\n\r\nFederation Link was `.Federation.html`, removed the `.`, hence fixing the 404 Error I saw on the site.", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/350055548", "body": "Closing as no reviews are done in over a year.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/350055548/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "rgoers": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/299", "title": "Support Log4j 2 and Logback", "body": "This patch relates to https://issues.apache.org/jira/browse/HADOOP-12956. It makes the logging implementation in hadoop common optional and provides support for event counters in Log4j 2 and Logback in addition to Log4j 1.", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/359450853", "body": "It has been almost 2 months with no response. Is there a reason no one has looked at this?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/359450853/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "maobaolong": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/297", "title": "Fix Checkstyle error, rename a argument", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/296", "title": "Fix constants variable name", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/295", "title": "Fix checkstyle problem", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/220", "title": "HADOOP-14406. Support slf4j logger log level get and set", "body": "https://issues.apache.org/jira/browse/HADOOP-14406", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/305764009", "body": "@brahmareddybattula  Thank you advance.  Now I upload the patch file here, please take a look, any review comment will help me.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/305764009/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/314379824", "body": "@aajisaka OK.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/314379824/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "mchataigner": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/290", "title": "HADOOP-14128. fix renameInternal in ChecksumFs", "body": "AbstractFs.rename(source, destination, options) calls\r\nrenameInternal(source, destination, overwrite)\r\n\r\nThis patch adds this method to ChecksumFs to rename the crc file in\r\naddition to the file itself to avoid crc missmatch when use for example\r\nin LocalFs.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yuxintan": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/288", "title": "HDFS-12749. Catch IOException to schedule BlockReport correctly when \u2026", "body": "\u2026DN re-register. (Contributed by TanYuxin)", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wenxinhe": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/287", "title": "HDFS-10323. transient deleteOnExit failure in ViewFileSystem due to close() ordering", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/249", "title": "HDFS-12125. Document the missing -removePolicy command of ec.", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/245", "title": "HADOOP-14624. Add GenericTestUtils.DelayAnswer that accept slf4j logger API", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/231", "title": "HADOOP-14508. TestDFSIO throws NPE when set -sequential argument.", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/306820179", "body": "OK, I'll try immediately.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/306820179/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/309414121", "body": "I closed it, @aajisaka, Thanks for your remind.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/309414121/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "retroverse": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/286", "title": "Update and rename README.txt to README.md", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ggribeler": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/285", "title": "MAPREDUCE-6752: Bad logging practices in mapreduce", "body": "Changed the log level of the method that is only used for debugging purpose as discussed here:\r\n\r\nhttps://issues.apache.org/jira/browse/MAPREDUCE-6752\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bberton-ciandt": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/284", "title": "HADOOP-14157 fix parseUrl to replace '\\' for '/'", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sutirupa": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/281", "title": "Automatic decompression of HDFS files tagged with extended attribute using Snappy", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mgarnara": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/280", "title": "MAPREDUCE-6973. Appropriate comments on creating _SUCCESS file.", "body": "@jlowe ", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/336728670", "body": "Since Jenkins isn't picking up and commenting on the pull request, reopening again", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/336728670/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "zhuqunzhou": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/277", "title": "add check compressor's byte size when finish compress", "body": "org.apache.hadoop.io.compress.BlockCompressorStream#finish is a public function,so other apps can call the method directly,such as `flume`,but when compressor.getBytesRead() == 0 then it will write a null data,and then the data after the null data will not be read as they lost. So,please add the check in the method.\r\nThank you.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "liumihust": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/276", "title": "Update DiskBalancer.java", "body": "BlockIteratorImpl.nextBlock() will look for the blocks in the source volume, if there are no blocks any more, it will return null up to DiskBalancer.getBlockToCopy(). However, the DiskBalancer.getBlockToCopy() will check whether it's a valid block.\r\nWhen I look into the FsDatasetSpi.isValidBlock(), I find that it doesn't check the null pointer! In fact, we firstly need to check whether it's null or not, or exception will occur.\r\nThis bug is hard to find, because the DiskBalancer hardly copy all the data of one volume to others. Even if some times we may copy all the data of one volume to other volumes, when the bug occurs, the copy process has already done.\r\nHowever, when we try to copy all the data of two or more volumes to other volumes in more than one step, the thread will be shut down, which is caused by the bug above.\r\nThe bug can fixed by two ways:\r\n1)Before the call of FsDatasetSpi.isValidBlock(), we check the null pointer\r\n2)Check the null pointer inside the implementation of FsDatasetSpi.isValidBlock()", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/255", "title": "Parallel Block Copy for DiskBalancer", "body": "The default plan executing is in sequential order,which means each step is executed one by one, because one volume can't be changed by two or more step at the same time.\r\n\u201cAnything worth doing is, sooner or later, worth doing concurrently\u201d\r\nHowever, We find a way to execute the plan in parallel,which is not only  much more efficient but also safe.The detail is here:\r\nhttps://github.com/liumihust/Parallel-Block-Copy-HDFS  ", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/254", "title": "Liumihust patch add null pointer exception in FsDatasetImpl.checkBlock()", "body": "Additional PR for the same bug", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/253", "title": " [ Indiscoverable bug in HDFS] FsDatasetSpi.isValidBlock() lacks null pointer check inside and neither do the callers", "body": "Dear Hadoop Developers,\r\nI'm from Alibaba, China. Recently, I meet a scenario where user want to migrate all the data in the old volumes to newly added volumes. Although HDFS now has a DiskBalancer tool, but it doesn't meet the requirement of us. So, we develop a new tool DiskMigration, which can migrate all the data in the current volumes to the new volumes and keep balance of data distribution at the same time.\r\nAfter introduce the work I'm doing, now we get to the point of the bug of the newest version hadoop3.0:\r\nBlockIteratorImpl.nextBlock() will look for the blocks in the source volume, if there are no blocks any more, it will return null up to DiskBalancer.getBlockToCopy(). However, the DiskBalancer.getBlockToCopy() will check whether it's a valid block.\r\nWhen I look into the FsDatasetSpi.isValidBlock(), I find that it doesn't check the null pointer! In fact, we firstly need to check whether it's null or not, or exception will occur.\r\nThis bug is hard to find, because the DiskBalancer hardly copy all the data of one volume to others. Even if some times we may copy all the data of one volume to other volumes, when the bug occurs, the copy process has already done.\r\nHowever, when we try to copy all the data of two or more volumes to other volumes in more than one step, the thread will be shut down, which is caused by the bug above.\r\nThe bug can fixed by two ways:\r\n1)Before the call of FsDatasetSpi.isValidBlock(), we check the null pointer\r\n2)Check the null pointer inside the implementation of FsDatasetSpi.isValidBlock()", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "soumabrata-chakraborty": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/275", "title": "YARN-6475: Refactor long methods in hadoop-yarn-server-nodemanager", "body": "Refactor all methods in hadoop-yarn-server-nodemanager that exceed 150 lines in length. Also fixes the method length related Checkstyle violations.", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/300569019", "body": "@templedf committed to trunk \r\n\r\nMessage:\r\nSUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #11717 (See https://builds.apache.org/job/Hadoop-trunk-Commit/11717/)\r\n\r\nClosing PR", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/300569019/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/329986135", "body": "Created by mistake! Please ignore!", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/329986135/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "thideeeee": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/271", "title": "YARN-2554. RM web proxy uses the client truststore specified in ssl-client.xml", "body": "I want to raise the issue again since the issue affects other application which runs on YARN. Actually, I see this problem when we run Spark app on Yarn.\r\nSpark launches Spark context web UI with custom SSL certificate when we enable SSL with \"spark.ssl.trustStore\" and \"spark.ssl.keyStore\" properties. In this case, Yarn web proxy cannot connect the Spark context web UI since the web proxy cannot verify the SSL cert (\"javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed\" error is returned).\r\n\r\nWe should add an option to set SSL trust store to Yarn RM web proxy. I added an updated patch, and this patch lets web proxy use an SSL custom trust-store if it is configured in ssl-client.xml", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yufeldman": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/269", "title": "HADOOP-14217. Support colon in Hadoop Path", "body": "", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/327513127", "body": "will do", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/327513127/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/328944139", "body": "I have added few tests to FileSystemContractBaseTest and updated HDFS and WebHDFS to not run those tests, as HDFS and subsequently WebHdfs currently do not support colon in any portion of the path including FileName(s).\r\nI did test with RawLocalFileSystem, S3(a,n). Since I can't test Azure, Swift, other FSs would be great to get feedback whether they support colon and can run those tests.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/328944139/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/335528107", "body": "@vrozov I don't think we can change existing constructors, only add one and one can use at their discretion, but we can't change all the code that is already written and uses existing constructors, especially the code that we use in our applications implicitly (two, three, ..., times removed from what we are trying to do)", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/335528107/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/335553206", "body": "@vrozov  I agree in principal that in Hadoop we do not consider that \"child\" can have scheme and it would help handling cases with \":/\" in \"child portion of path, but  according to URI resolve method it can be different configurations of merging parent and child.\r\nWe would need very extensive testing to cover those cases. Also I believe there is method in Path:\r\n`mergePaths` that people are using to signify what you were saying.\r\nI would like to hear more opinions on changing behavior of `Path(Path, Path)` to make it behave more like `mergePaths` method\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/335553206/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/335659984", "body": "@vrozov I don't disagree with you. I would like to get consensus (more input from stakeholders) on changing this behavior. This JIRA is least invasive.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/335659984/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/336001549", "body": "@vrozov how about having a hangout meeting - maybe it's simpler that way?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/336001549/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "wangzhen11aaa": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/267", "title": "Branch 2.7.4", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dosoft": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/266", "title": "HDFS-12315: Use Path instead of String to check closedFiles set", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/265", "title": "HDFS-12314: Fixed typo in the testAddOneNewVolume()", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/264", "title": "HDFS-12309. Fixed incorrect checkNotNull()", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/263", "title": "MAPREDUCE-6940: Pass allSplits parameter", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "LarryLo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/260", "title": "YARN-6969. Remove method getMinShareMemoryFraction and getPendingCont\u2026", "body": "Please add me as a contributor in jira so that I can assign this task to me. ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dennishuo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/259", "title": "MAPREDUCE-6931. Remove TestDFSIO \"Total Throughput\" calculation.", "body": "Previously it failed to convert ms to seconds and thus reports aggregate\r\nthroughput as 1/1000x actual numbers. Also, make all the bytes-to-mb\r\nand milliseconds-to-seconds conversions consistent in the reporting\r\nmessages to help avoid this type of error in the future.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yonatan-py": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/252", "title": "replication example revised", "body": "the placing of the third replica is confusing, is it put on the same rack as the first replica, or are the second and third put on a different replica.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mandusm": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/250", "title": "HADOOP-14661. Added support for AWS S3 Requester Pays Buckets", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jirimutu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/248", "title": "Branch 2.6.0", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "RebornHuan": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/236", "title": "HADOOP-12084: revise multiple FTPFileSystem issue  ", "body": "revise multiple FTPFileSystem issue  when using filesystem CACHE.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gzsombor": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/226", "title": "HDFS-11924 : Pass FsAction to the external AccessControlEnforcer", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/225", "title": "HDFS-11924 : Pass FsAction to the external AccessControlEnforcer", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "amuttsch": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/224", "title": "HADOOP-9320 Fix Hadoop native build failure on ARM hard-float", "body": "Moved the ARM JVM float ABI detection from `HadoopCommon.cmake` to `HadoopJNI.cmake`, because `${JAVA_JVM_LIBRARY}` is not available in `HadoopCommon.cmake` yet and therefore the build fails.\r\n\r\nThis commit fixes this issue.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "scutojr": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/222", "title": "YARN-6583 Hadoop-sls failed to start because of premature state of RM", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sanjaypujare": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/219", "title": "YARN-6457  use existing conf object as a resource for sslConf object in WebApps for the builder to use in HttpServer2", "body": "use the passed conf object as a resource in local sslConf so it can be overridden", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/213", "title": "YARN-6457  use existing conf object as sslConf object in WebApps for the builder to use for the HttpServer2", "body": "", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/299786725", "body": "closed as another PR will replace this", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/299786725/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "lingjinjiang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/212", "title": "[YARN-6478] Fix a spelling mistake", "body": "Fix a spelling mistake in FileSystemTimelineWriter.java.\r\nThe \"writeSummmaryEntityLogs\" should be \"writeSummaryEntityLogs\".\r\n\r\nhttps://issues.apache.org/jira/browse/YARN-6478", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "oza": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/210", "title": "HADOOP-14284. Shade Guava everywhere.", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/84055438", "body": "@gliptak please report this issue to JIRA: https://issues.apache.org/jira/browse/YARN\n\nPlease see the for more detail: http://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/84055438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/87997731", "body": "@s-bolz thank you for your contribution. Please use Hadoop's JIRA to contribute Hadoop project.\n\nhttps://issues.apache.org/jira/browse/HADOOP\nhttp://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/87997731/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/90585021", "body": "@gliptak yes, these patches have been submitted successfully. Now committers reviewed your patches, so please check them.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/90585021/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/107206717", "body": "@Guchige  thanks for your contribution. We use JIRA instead of github and don't accept any patches via github for now. Could you attach diff file based on the PR? \n\nThe documenation, How To Contribe in Hadoop Wiki, is useful.  http://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/107206717/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/155797735", "body": "@kemiya-yx  thanks for your PR. Currently, we accept all contributions via JIRA.\n\nPlease read how to contribute page on wiki: http://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/155797735/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/184275448", "body": "This PR is for a review.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/184275448/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/52916706", "body": "I found one incompatible change about Jersey - after upgrading from 1.12 to 1.13, the root element whose content is empty collection is changed from null to empty object({}). Related to following change: https://java.net/jira/browse/JERSEY-1168\n\nThis change of lines fixes tests and assertions about JSONObject.NULL to address the change above. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/52916706/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67776639", "body": "@aajisaka thank you for the comment. The fix I made is just for avoiding warning.  \nCan we do this on another jira? Your comment is a just minor refactoring, so we can do it on separate issue.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67776639/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "kazuyukitanimura": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/207", "title": "HADOOP-14237. S3A Support Shared Instance Profile Credentials Across All Instances", "body": "Hi @steveloughran @liuml07 \r\n\r\nYet another patch that I made a few months back.\r\nI explained the issue at https://issues.apache.org/jira/browse/HADOOP-14237\r\n\r\nThis pull request is aiming for more open discussions rather than a complete solution. It would be great if you could offer your thoughts.", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/287826028", "body": "Hi @steveloughran \r\nThank you for sharing this S3A globber. I started reading the code, but at high level what are things already done, and needs to be done?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/287826028/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/287832853", "body": "Thanks @steveloughran \r\n\r\nI understand your point that getting the sign off for the core class changes is not easy. At the same time, #204 seems to be a big change. I was wondering if there is a way to meet at somewhere in the middle. I meant to provide a minimal strategy rather than a full complete solution in this pull request, because I thought it is important to provide the end users a way to glob things on S3. It easily hits OOM with the current code.\r\n\r\nMeanwhile, I will keep trying to contribute to #204, which seems to be a right long term solution.\r\n\r\nAlso, I made a few other fixes related to S3A. My current employer just allowed me to spend 20% of my time to contribute back to the community. I hope you don't mind that I mention your name in the pull requests that I am going to file.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/287832853/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/290936018", "body": "Closing in favor of #204 ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/290936018/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/290936280", "body": "Closing PR as stated in Jira", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/290936280/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/290936311", "body": "Closing PR as stated in Jira", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/290936311/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "pvillard31": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/198", "title": "YARN-6261 - Catch user with no group when getting queue from mapping \u2026", "body": "\u2026definition", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/279956072", "body": "Committed as part of 8e53f2b9b08560bf4f8e81e697063277dbdc68f9. Closing.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/279956072/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "pdkluitel": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/197", "title": "Hadoop 6685", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jinhyukchang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/191", "title": "Fixed location of service provider configuration file on Azure Data lake Filesystem.", "body": "Currently, the provider configuration file is in wrong location -- should be under services folder -- and ADL file system cannot be loaded without registering manually into the configuration.\r\nhttps://docs.oracle.com/javase/tutorial/ext/basics/spi.html#register-service-providers\r\n\r\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/284466316", "body": "Sure, just created a Jira [ticket](HADOOP-14149)", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/284466316/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "lhcxx": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/190", "title": "Huichun/mx net", "body": "https://github.com/apache/hadoop/compare/trunk...lhcxx:huichun-MXNet?expand=1#files_bucket", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jeanzhou": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/189", "title": "Merge pull request #1 from apache/trunk", "body": "update from origin", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "MonsterSupreme": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/185", "title": "Fix \"E: Unable to locate package software-properties-common\" error.", "body": "When executing `./start-build-env.sh` command, the output is:\r\n\r\n```\r\nSending build context to Docker daemon 9.216 kB\r\nStep 1/18 : FROM ubuntu:trusty\r\ntrusty: Pulling from library/ubuntu\r\nc60055a51d74: Pull complete\r\n755da0cdb7d2: Pull complete\r\n969d017f67e6: Pull complete\r\n37c9a9113595: Pull complete\r\na3d9f8479786: Pull complete\r\nDigest: sha256:8f5f12335124c1b78e4cf2f8860d395f75ba279bae70a3c18dd470e910e38ec5\r\nStatus: Downloaded newer image for ubuntu:trusty\r\n ---> b969ab9f929b\r\nStep 2/18 : WORKDIR /root\r\n ---> 9ec52cfdcff6\r\nRemoving intermediate container f3b5b3ad6552\r\nStep 3/18 : RUN apt-get install -y software-properties-common\r\n ---> Running in 3311c06d0ece\r\nReading package lists...\r\nBuilding dependency tree...\r\nReading state information...\r\nE: Unable to locate package software-properties-common\r\nThe command '/bin/sh -c apt-get install -y software-properties-common' returned a non-zero code: 100\r\n```\r\n\r\nIt's because there is no `apt-get update` before executing `apt-get install -y software-properties-common` in `dev-support/docker/Dockerfile` file:\r\n\r\n```\r\nFROM ubuntu:trusty\r\n\r\nWORKDIR /root\r\n\r\n# Add \"RUN apt-get update\" here to fix the problem.\r\nRUN apt-get update\r\nRUN apt-get install -y software-properties-common\r\nRUN add-apt-repository -y ppa:webupd8team/java\r\nRUN apt-get update\r\n...\r\n```\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wwjiang007": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/174", "title": "1", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "slachterman": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/173", "title": "Details for ADLS connectivity using Client Keys", "body": "I did not find the token endpoint as described in the previous version of this document, I found the URL from the ADLS REST API documentation https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-get-started-rest-api\r\n\r\nAdded dfs.adls.oauth2.access.token.provider.type, I saw errors regarding a missing dfs.adls.oauth2.access.token.provider value without this property set.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "subahugu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/172", "title": "HDFS-11234: Made the socket buffer size configurable with the config \u2026", "body": "\u2026node fs.hdfs.data.socket.size to be set in core-site.xml. If the node is not found, the default value is -1, so socket buffer size would not be set.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/171", "title": "HDFS-11227: Set read timeout for peer", "body": "Read timeout is not set with peer in org.apache.hadoop.hdfs.BlockReaderFactory, so BlockReader read doesn't timeout.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "LantaoJin": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/159", "title": "HDFS-11111. Delete items in .Trash using rm should be forbidden witho\u2026", "body": "\u2026ut safety option", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/148", "title": "HDFS-11060. Make DEFAULT_MAX_CORRUPT_FILEBLOCKS_RETURNED configurable", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sahilTakiar": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/157", "title": "HADOOP-13600. S3a rename() to copy files in a directory in parallel", "body": "* Creates two separate `TransferManager`s - one for file uploads and one for file copies; this way they can be both configured separately; users may want to set the # of parallel uploads to a lower value than the # of parallel copies because uploads require actual I/O while copies do not\r\n* Main modifications are to the `innerRename` method\r\n* Instead of renaming a directory file by file, use the copy `TranferManager` to upload them in parallel\r\n* Copies are submitted to the `TransferManager` and then tracked using the returned `Copy` object\r\n* Deletes are handled via a `BlockingQueue` - once each copy is complete it adds a key to the queue, once `MAX_ENTRIES_TO_DELETE` keys need to be deleted, then the `removeKeys` method is invoked\r\n* A separate thread is spawned to read from the delete queue and issue the delete requests\r\n* A `ProgressListener` is used to track when a copy has been completed, once a copy finishes, the key to delete is added to the delete queue\r\n* Some other re-factoring to make the above possible", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/329364153", "body": "Updates:\r\n* Moved the parallel rename logic into a dedicated class called `ParallelDirectoryRenamer`\r\n* A few other bug fixes, the core logic remains the same\r\n\r\n@steveloughran your last comment on HADOOP-13786 suggested you may move the retry logic out into a separate patch? Are you planning to do that? If not, do you think this patch requires waiting for all the work in HADOOP-13786 to be completed?\r\n\r\nIf there are concerns with retry behavior, we could also set the default value of the copy thread pool to be 1, that way this feature is essentially off by default.\r\n\r\nAlso what do you mean by \"isn't going to be resilient to large copies where you are much more likely to hit parallel IO\"? What parallel IO are you referring to?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/329364153/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/329504363", "body": "@steveloughran ok that makes sense. Thanks for the explanation. Let me know if you need any help with pulling the retry logic out.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/329504363/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "SahilKang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/152", "title": "HDFS-11115 Remove bytes2String and string2Bytes", "body": "Since StandardCharsets makes converting between (utf-8) bytes and strings\r\ntrivial, let's remove the methods:\r\n    - org.apache.hadoop.hdfs.DFSUtilClient.bytes2String\r\n    - org.apache.hadoop.hdfs.DFSUtilClient.string2Bytes", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/237151452", "body": "@aajisaka, this latest commit should fix the two checkstyle warnings, and I'll send another commit soon with the analogous changes made to org.apache.hadoop.mapred.TextOutputFormat.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/237151452/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "anuengineer": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/151", "title": "Ozone:SCM: Add support for registerNode in datanode", "body": "", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/260042052", "body": "+1, LGTM\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/260042052/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/262327267", "body": "Arpit, thanks for updating the patch. +1.  LGTM.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/262327267/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/86860005", "body": "Just trying to understand this a little better, From the signature and implementation this function looks more a awaitTermination in executorService. That is this function will wait for a while and cancel and task if the timeout occurs, in that case would you consider calling this await or awaitTermination. Java \"join\" seems to imply a wait without timeouts. Just making sure that the intended was indeed a shutdown/await pattern.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/86860005/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87886544", "body": "fixed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87886544/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87886708", "body": "fixed\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87886708/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87886949", "body": "fixed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87886949/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87887084", "body": "fixed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87887084/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87887167", "body": "done\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87887167/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87887266", "body": "removed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87887266/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87887762", "body": "fixed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87887762/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87887907", "body": "fixed\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87887907/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87888087", "body": "fixed\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87888087/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87888369", "body": "fixed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87888369/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87888622", "body": "fixed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87888622/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87888654", "body": "fixed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87888654/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87889744", "body": "fixed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87889744/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87916350", "body": "fixed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87916350/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87916368", "body": "fixed\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87916368/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87916420", "body": "fixed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87916420/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88354093", "body": "Presuming that we will have an equivalent or better test using storageLocationChecker ? \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88354093/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "ferhui": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/150", "title": "HADOOP-13773, set heap args for HADOOP_CLIENT_OPTS when HADOOP_HEAPSI\u2026", "body": "jira url is https://issues.apache.org/jira/browse/HADOOP-13773", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "szape": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/139", "title": "YARN-5721. NPE at AMRMClientImpl.getMatchingRequests", "body": "The following NPE was thrown using a Spark 2.1.0-SNAPSHOT (as client) by changing Hadoop dependency to the latest (by the time the ERROR has been generated).\n\n{{2016-10-10 11:33:53,392 ERROR yarn.ApplicationMaster: Uncaught exception:\njava.lang.NullPointerException\nat org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.getMatchingRequests(AMRMClientImpl.java:668)\nat org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.getMatchingRequests(AMRMClientImpl.java:651)\nat org.apache.spark.deploy.yarn.YarnAllocator.getPendingAtLocation(YarnAllocator.scala:210)\nat org.apache.spark.deploy.yarn.YarnAllocator.getPendingAllocate(YarnAllocator.scala:203)\nat org.apache.spark.deploy.yarn.YarnAllocator.updateResourceRequests(YarnAllocator.scala:318)\nat org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:278)\nat org.apache.spark.deploy.yarn.ApplicationMaster.registerAM(ApplicationMaster.scala:350)\nat org.apache.spark.deploy.yarn.ApplicationMaster.runExecutorLauncher(ApplicationMaster.scala:418)\nat org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:250)}}\n\nWe've also pulled the latest code (1 hour ago) from the repository, and ran a test for getMatchingRequests. Same NPE has been encountered.\n\ngetMatchingRequests should never throw an NPE even if it has been called right after the client has been started.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/134", "title": "YARN-5025. Container move (relocation) between nodes", "body": "Support for relocating containers has become a must-have requirement for most multi-service applications, since the inevitable concept-drifts make SLAs hard to be satisfied. The relocation and co-location of services (long running containers) can help to reduce bottlenecks in a multi-service cluster, especially where data-intensive, streaming applications interfere.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lewismc": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/137", "title": "HADOOP-10225", "body": "Hi Folks,\nThis PR is an attempt to address https://issues.apache.org/jira/browse/HADOOP-10225.\nThe patch can be tested by running\n\n```\nmvn release:clean release:prepare -DautoVersionSubmodules=true -DdryRun=true\n```\n\nPlease let me know once it has been tested and I can squash commits into one.\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/252730180", "body": "@umbrant CC\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/252730180/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/252731249", "body": "Just squashed all the commits so it is much cleaner.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/252731249/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "QwertyManiac": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/135", "title": "HADOOP-13694. Add support for AES-192 in OpensslCipher.", "body": "- Adds equivalent support for 192-bit AES/CTR/NoPadding codec in the OpensslCipher\n- Adds test-cases to cover 192-bit (24-bytes) and 256-bit (32-bytes) keys to both JCE and OpenSSL crypto tests\n- Enhances the error message when an invalid Key or IV size is passed into OpensslCipher\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/262997362", "body": "Fixed via 07825f2b49384dbec92bfae87ea661cef9ffab49", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/262997362/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/282332263", "body": "Done via e8694deb6ad180449f8ce6c1c8b4f84873c0587a.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/282332263/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/282453974", "body": "Rebased the patch.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/282453974/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/282454084", "body": "Done via e24ed47d9a19f34a4dd8d4bad9b5c78ca3dd1c2e on trunk.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/282454084/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88025559", "body": "Thank you for the review! Done in the new commit.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88025559/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88025569", "body": "Thank you for the review! Done in the new commit.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88025569/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "rajeshdb": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/127", "title": "Branch 2.7.1", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chu11": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/123", "title": "MAPREDUCE-6769. Fix forgotten name conversion from \"slave\" to \"worker\" in mapred script,", "body": "most notably fixing environment variable name change and function name\nchange.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "albericliu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/122", "title": "HDFS-10753.Method invocation in log can be replaced by variable becau\u2026", "body": "\u2026se the variable's toString method contains more info\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "surekav": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/117", "title": "Branch 2.7", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "fedecz": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/113", "title": "[HADOOP-13075] Adding support for SSE-KMS and SSE-C", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/110", "title": "[HADOOP-13075] Adding support for SSE-KMS, SSE-C and SSE-S3", "body": "https://issues.apache.org/jira/browse/HADOOP-13075\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70430407", "body": "As you said, that constant is only used in that test which I did change. I changed it to abstract and created 3 different implementations: one for SSE-S3, SSE-KMS and SSE-C. Basically I'm running all the tests in TestS3AEncryption, but with different encryption algorithms depending on the concrete class.\nYes, it builds and all test pass.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70430407/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70431908", "body": "Nope, this is the section _Other Issues_ in the documentation, so I wanted to document if the user was having that warning, he/she should specify the endpoint in the config. I guess I could set a title to describe it better instead of just pasting the warning.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70431908/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432238", "body": "true, but not all of them can be merged. I'm relying in else clauses as well depending on some of the conditions being false. I'll try to rewrite it though and will see how it looks.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432238/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432364", "body": "will do\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432364/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70439085", "body": "I don't see anything related to this patch in that ticket's patch, are you sure that's the one? I'm looking at the attached patch in HADOOP-13224\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70439085/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "pradeep1288": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/106", "title": "HADOOP-11823: dont check for verifier in RpcDeniedReply", "body": "When RPC returns a denied reply, the code should not check for a verifier. It is a bug as it doesn't match the RPC protocol. (See Page 33 from NFS Illustrated book).\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/228132992", "body": "I will separate out the changes for HADOOP-12345 and HADOOP-11823 and submit two pull requests\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/228132992/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/229571345", "body": "This has been checked in\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/229571345/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "apetresc": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/100", "title": "HADOOP-13278. S3AFileSystem mkdirs does not need to validate parent path components", "body": "According to S3 semantics, there is no conflict if a bucket contains a key named `a/b` and also a directory named `a/b/c`. \"Directories\" in S3 are, after all, nothing but prefixes.\n\nHowever, the `mkdirs` call in `S3AFileSystem` does go out of its way to traverse every parent path component for the directory it's trying to create, making sure there's no file with that name. This is suboptimal for three main reasons:\n- Wasted API calls, since the client is getting metadata for each path component \n- This can cause _major_ problems with buckets whose permissions are being managed by IAM, where access may not be granted to the root bucket, but only to some prefix. When you call `mkdirs`, even on a prefix that you have access to, the traversal up the path will cause you to eventually hit the root bucket, which will fail with a 403 - even though the directory creation call would have succeeded.\n- Some people might actually have a file that matches some other file's prefix... I can't see why they would want to do that, but it's not against S3's rules.\n\n[I've opened a ticket](https://issues.apache.org/jira/browse/HADOOP-13278) on the Hadoop JIRA. This  pull request is a simple patch that just removes this portion of the check. I have tested it with my team's instance of Spark + Luigi, and can confirm it works, and resolves the aforementioned permissions issue for a bucket on which we only had prefix access.\n\nThis is my first ticket/pull request against Hadoop, so let me know if I'm not following some convention properly :)\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ramtinb": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/94", "title": "HDFS-10382 In WebHDFS numeric usernames do not work with DataNode", "body": "In WebHDFS for cat operation, we have 2 sequential HTTP requests.\nThe first HTTP request is handled by NN and the second one by DN.\nUnlike the NN, the DN is not using the suggested domain pattern from the configuration!\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xiaobingo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/92", "title": "HDFS-9895. Push up DataNode#conf to base class", "body": "Please kindly review the patch v001, see also https://issues.apache.org/jira/browse/HDFS-9895.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Tartarus0zm": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/87", "title": "modify docker launch script", "body": "First launch the docker container and then get the container pid\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ceefour": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/85", "title": "HADOOP-9991. Update netty to 3.7.1.Final to sync with zookeper", "body": "Update netty to 3.7.1.Final because hadoop-client 2.7.2 depends on zookeeper 3.4.6 which depends on netty 3.7.x. Related to https://github.com/apache/hadoop/pull/84\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/84", "title": "HADOOP-12927. Update netty-all to 4.0.34.Final", "body": "ZooKeeper 3.4.6 uses netty 3.7.0.Final\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/197323519", "body": "@steveloughran This pull request now only updates netty-all (minor version update).\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/197323519/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/197358937", "body": "@steveloughran created HADOOP-12927\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/197358937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "costin": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/82", "title": "Disable setsid in case of Security Exceptions", "body": "If spawning an external process is not permitted by the JVM security manager, disable setsid.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "arif505": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/77", "title": "Branched to archive the Avro RPC work. On trunk Avro RPC is removed a\u2026", "body": "\u2026fter creating this branch.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HADOOP-6659@1214008 13f79535-47bb-0310-9956-ffa450edef68\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rainforc": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/71", "title": "fix  NullPointerException in Balancer", "body": "", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/177856169", "body": "Balancer will throw NullPointerException when datanode is down.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/177856169/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "make5020": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/69", "title": "Creating updated branch for Hadoop-6671", "body": "git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HADOOP-6671-2@956573 13f79535-47bb-0310-9956-ffa450edef68\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "emopers": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/64", "title": "NNStorage does not synchronize iteration on a synchronized list", "body": "In line 839 of NNStroage.java#reportErrorsOnDirectories, the synchronized list, `sds`\nis iterated in an unsynchronized manner, but according to [Oracle Java 7 API specification](http://docs.oracle.com/javase/7/docs/api/java/util/Collections.html#synchronizedList%28java.util.List%29),\nthis is not thread-safe and can lead to non-deterministic behavior.\nThis pull request adds a fix by synchronizing the iteration on `sds`. The synchronized list is passed to method `reportErrorsOnDirectories` from [here](https://github.com/facebookarchive/hadoop-20/blob/2a29bc6ecf30edb1ad8dbde32aa49a317b4d44f4/src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSImage.java#L508)\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "iSultan": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/63", "title": "Fix a typo", "body": "I think there is a typo unless you typed intentionally. Is `slowTaskRelativeTresholds` meant to be `slowTaskRelativeThresholds`?\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/60", "title": "Add space after jobId in line 1011", "body": "One space is needed before `Job Transitioned from` in line 1011. The log output has no space between `jobId` and `Job Transitioned from`.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cnauroth": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/49", "title": "HDFS-9443. Disabling HDFS client socket cache causes logging message \u2026", "body": "\u2026printed to console for CLI commands.\n\nThis is a trivial patch to change the log statement to debug level.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/45787996", "body": "I think you'll still need to incorporate the fixes I suggested earlier in a JIRA comment: remove the extra space character at the end of the `\"target \"` string literal, and switch from `File.pathSeparator` to `File.separator`.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/45787996/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82086168", "body": "If I understand correctly, this can't throw the exception unless we have a bug in our code.  Is it better to let the `IllegalStateException` be thrown so that we see that sooner?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82086168/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82086784", "body": "Call `super.flush()` to trigger the validation check for `Writing` state.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82086784/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82087028", "body": "I was thinking you could remove the `buffer.reset()`, because the next line is dropping the reference to `buffer` anyway.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82087028/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82090027", "body": "I tried an `mvn site` build, and it looks like the new troubleshooting sections still aren't nested correctly.  I believe it should be `###` instead of `##`.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82090027/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82091762", "body": "I think `activeBlock` is always `null` when this log statement executes.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82091762/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82093069", "body": "Is this revision missing the changes to restore/un-deprecate `fs.s3a.fast.upload`?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82093069/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82688403", "body": "I think we can achieve this change without adding a member variable in the subclass.  (See more specific notes to follow.)  Removing this member variable would reduce memory footprint.  Admittedly, the memory cost is probably not significant, but as we start thinking about the possibility of caching `FileStatus` instances client-side for things like S3Guard, then the per-instance memory cost of each `FileStatus` could become more significant.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82688403/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82688624", "body": "Consider removing the member variable and changing this line of code to:\n\n```\nthis.setOwner(owner);\nthis.setGroup(owner);\n```\n\n(These are `protected` methods in the base class.)\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82688624/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82688875", "body": "If we implement the above comments, then we can completely remove the override of `getOwner` here.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82688875/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82688916", "body": "The override of `getGroup` could be removed too.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82688916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82689328", "body": "Unused import?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82689328/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82690355", "body": "Similar to the earlier comment:\n\n```\nthis.setOwner(owner);\nthis.setGroup(owner);\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/82690355/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "BELUGABEHR": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/46", "title": "Simplify logging logic", "body": "Simplify logging logic\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bobhansen": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/43", "title": "HDFS-9144: libhdfs++ refactoring", "body": "Code changes for HDFS-9144 as described in the JIRA.  Removing some templates and traits and restructuring the code for more modularity.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hash-X": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/42", "title": "AltFileInputStream.java replace FileInputStream.java in apache/hadoop/HDFS", "body": "A brief description\nLong Stop-The-World GC pauses due to Final Reference processing are observed.\nSo, Where are those Final Reference come from ?\n\n1 : `Finalizer`\n2 : `FileInputStream`\n\nHow to solve this problem ?\n\nHere is the detailed description,and I give a solution on this.\nhttps://issues.apache.org/jira/browse/HDFS-8562\n\nFileInputStream have a method of finalize , and it can cause GC pause for a long time.In our test,G1 as our GC. So,in AltFileInputStream , no finalize. A new design for a inputstream use in windows and non-windows.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/36", "title": "Later hadoop use flat buffer", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Shubh91": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/41", "title": "CheckingTheChanges", "body": "check\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ejono": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/39", "title": "HADOOP-12527. Upgrade Avro dependency to 1.7.7.", "body": "", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/230940028", "body": "@benmccann, is that actually necessary for this patch? I don't actually know. Also, that line you point to currently has an open-ended max version, so it's already going to use the latest, right?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/230940028/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "liufengdb": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/37", "title": "bail out and avoid to access root in s3a", "body": "The original do while loop does not have any effect. \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "MobinRanjbar": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/33", "title": "Showing the path is empty message in ls command", "body": "When a user lists an empty folder with 'hadoop fs -ls /tmp/emptyfolder', a message will appear that says 'The path is empty'. Otherwise, nothing will appear and the user does not know it has been done or not.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "allfro": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/32", "title": "See HADOOP-12406", "body": "Note: I am not an expert at JAVA, Class loaders, or Hadoop. I am just a hacker. My solution might be entirely wrong.\nAbstractMapWritable.readFields throws a ClassNotFoundException when reading custom writables. Debugging the job using remote debugging in IntelliJ revealed that the class loader being used in Class.forName() is different than that used by the Thread's current context (Thread.currentThread().getContextClassLoader()). The class path for the system class loader does not include the libraries of the job jar. However, the class path for the context class loader does. The proposed patch changes the class loading mechanism in readFields to use the Thread's context class loader instead of the system's default class loader.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "MjAbuz": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/31", "title": "Learning", "body": "Need support\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/137352630", "body": "Learning\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/137352630/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "huahuiyang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/30", "title": "update Shell.java to avoid OOM", "body": "avoid out of memory(such as NodeManager OOM), in case running a command which has verbose error log continuously\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alexVengrovsk": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/29", "title": "HadoopArchives.java polishing", "body": "`HadoopArchives.java` was edited.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Guchige": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/20", "title": "YARN-3678", "body": "use 'ps' command to check the process before it is killed \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "s-bolz": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/17", "title": "Branch 2.5.0", "body": "extended distcp to create a file containing all files that would be / have been deleted from the target using the -delete flag\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "weyo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/16", "title": "Adjust exception output to offer additional information", "body": "It is not a easy job to find out detailed exception messages of _db_ module under this version.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Xia-Hu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/14", "title": "YARN-3126. FairScheduler: queue's usedResource is always more than the maxResource limit.", "body": "This change add a check, for whether queue's usedResource may run over its maxResource Limit after a new resource allocation. \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vinayrpet": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/13", "title": "Merging from apache trunk", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "peiyuefeng": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/8", "title": "Branch 2", "body": "test\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ashahab-altiscale": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/7", "title": "YARN-1964 Launching containers from docker", "body": "This adds a new ContainerExecutor called DockerContainerExecutor.\nThis executor launches a container in a docker container, providing\na full filesystem namespace and software isolation for the container.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/6", "title": "YARN-1964 Launching containers from docker", "body": "This adds a new ContainerExecutor called DockerContainerExecutor.\nThis executor launches a container in a docker container, providing\na full filesystem namespace and software isolation for the container.\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416472", "body": "It has been merged to trunk and branch-2.6\n\nOn Wed, Dec 10, 2014 at 12:01 AM, Harry Zhang notifications@github.com\nwrote:\n\n> I'm really interested in this feature, but why it is not merged yet?\n> But why I can see this patch has been added to 2.6.0?\n> \n> http://hadoop.apache.org/releases.html#18+November%2C+2014%3A+Release+2.6.0+available\n> \n> I'm really confused ....\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/hadoop/pull/7#issuecomment-66416298.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416472/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66424694", "body": "No, but feel free to contribute that guide to hadoop.\n\nOn Wed, Dec 10, 2014 at 1:06 AM, Harry Zhang notifications@github.com\nwrote:\n\n> So is it possible for me to launch Dockers in Yarn to run Spark jobs now?\n> Is there a guide for me to do so?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/hadoop/pull/7#issuecomment-66422169.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66424694/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "sebastiancadena": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/4", "title": "Update TaskInputOutputContext.java javadoc", "body": "Update javadoc to reflect the current code structure.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sam-s": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/3", "title": "[HADOOP-10724] better interoperation with `sort -h`", "body": "do not insert a space between number and units in StringUtils.TraditionalBinaryPrefix.long2String to work better with `sort -h`\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "piaoyu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/1", "title": "MAPREDUCE-6096.SummarizedJob Class Improvment", "body": "https://issues.apache.org/jira/browse/MAPREDUCE-6096\n\nSummarizedJob class should be Improvment\n\nWhen I Parse the JobHistory in the HistoryFile,I use the Hadoop System's map-reduce-client-core project org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser class and HistoryViewer$SummarizedJob to Parse the JobHistoryFile(Just Like job_1408862281971_489761-1410883171851_XXX.jhist) \nand it throw an Exception Just Like \nException in thread \"pool-1-thread-1\" java.lang.NullPointerException\nat org.apache.hadoop.mapreduce.jobhistory.HistoryViewer$SummarizedJob.<init>(HistoryViewer.java:626)\nat com.jd.hadoop.log.parse.ParseLogService.getJobDetail(ParseLogService.java:70)\n\nAfter I'm see the SummarizedJob class I find that attempt.getTaskStatus() is NULL \uff0c\nSo I change the order of attempt.getTaskStatus().equals (TaskStatus.State.FAILED.toString()) to \nTaskStatus.State.FAILED.toString().equals(attempt.getTaskStatus()) \nand it works well .\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Markiry": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/62241738", "body": "I'm very sorry, I have some mistakes.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/62241738/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "resouer": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416298", "body": "I'm really interested in this feature, but why it is not merged yet? \nBut why I can see this patch has been added to 2.6.0?\nhttp://hadoop.apache.org/releases.html#18+November%2C+2014%3A+Release+2.6.0+available\n\nI'm really confused ....\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416298/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66422169", "body": "So is it possible for me to launch Dockers in Yarn to run Spark jobs now? Is there a guide for me to do so?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66422169/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66567203", "body": "Thanks! I'd like to.  \n\nBTW, what's the current status of this great yarn-docker work. \n1. Can I use it now?\n2. Do I need to use customized docker?\n3. What I can do if I want to contribute to make it better?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66567203/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "modeyang": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/67595055", "body": "hi, ashahab-altiscale,\nhas any way or plan  apply docker container to  mapreduce workers within hadoop ? \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/67595055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "gliptak": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/89670158", "body": "@oza could you validate if these are submitted correctly? thanks\n\nhttps://issues.apache.org/jira/browse/YARN-3444\nhttps://issues.apache.org/jira/browse/HADOOP-11801\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/89670158/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "omalley": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/152589471", "body": "Testing comments on github pull requests.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/152589471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "yxkemiya": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/156015267", "body": "@oza Thanks, I will close this PR.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/156015267/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "vesense": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/159192789", "body": "Reported the issue to JIRA: https://issues.apache.org/jira/browse/YARN-4387\nSo, close this PR.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/159192789/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "zhe-thoughts": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163342122", "body": "Thanks for reporting this. Even though the fix is minor, please created a JIRA at https://issues.apache.org/jira/browse/HADOOP\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163342122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "bwtakacy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163488815", "body": "OK.\nI will close this PR.\n\nThanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163488815/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "raviprak-altiscale": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/168770287", "body": "Thanks for your contribution emopers. Could you please follow these steps? https://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/168770287/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "2899722744": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/182214122", "body": "sync\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/182214122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/240984564", "body": "sync\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/240984564/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "h4ck3rm1k3": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187164944", "body": "I am having some problems with further testing. Closing this PR for now until I fix and test more. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187164944/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187167034", "body": "https://github.com/h4ck3rm1k3/hadoop-archive-org-bucket-fs/issues/1 this is the problem that I found. Needs more work. Need to create test cases inside of hadoop for this. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187167034/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "makefu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/197495374", "body": "hi all, any comments on my PR? \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/197495374/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "zhudebin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/228069480", "body": "ok\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/228069480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "templedf": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/229993530", "body": "Looks like the right fix, but can you simplify the logic a little, i.e. (len != 2) && (len != 4)?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/229993530/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/269520095", "body": "LGTM +1", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/269520095/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77861935", "body": "Can we please have one exit point?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77861935/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77863007", "body": "Seems like this should be more defensive, i.e. check for the type before casting.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77863007/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77864635", "body": "Maybe put the scheduler into the context?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77864635/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77866033", "body": "Drop the else.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77866033/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77868830", "body": "It would be nice to replace this _continue_ by wrapping the next few lines in the _if_.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77868830/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77871756", "body": "This seems convoluted.  why not:\n\n{code}\nboolean starved = false;\nResource starvation = none();\nif (Resources.greaterThan(policy.getResourceCalculator(),\n    scheduler.getClusterResource(), getDemand(), getMinShare())) {\n  starvation = Resources.subtract(getMinShare(), getResourceUsage());\n  starved = Resources.greaterThan(policy.getResourceCalculator(),\n    scheduler.getClusterResource(), starvation, none());\n}\n{code}\n\nNets out to the same thing, but the logic isn't as hard (for me) to follow.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77871756/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77872281", "body": "Single exit point, please\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77872281/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77901500", "body": "Feels like this should be a Resources.isNone() call.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77901500/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77901904", "body": "You should call interrupt();\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77901904/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77902532", "body": "You could replace this return with starvedApp = null, and then put a guard at the beginning of identifyContainersToPreempt(), which is arguably a good thing to do in any case.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77902532/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77918966", "body": "Seems like this line should be pushed down into FSChildQueue.updateInternal()\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77918966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77919341", "body": "I don't like this name.  It feels to me like an identifyX() should return something.  I'd rather have a name that says what it does, e.g. updateStarvedApplications().\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77919341/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77919411", "body": "Feels like FSContext should have a wrapper method for addStarvedApp().\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77919411/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77919563", "body": "Spurious space before the colon\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77919563/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77923085", "body": "Unused import\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77923085/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78411782", "body": "You can make it:\n\n```\n    if (!starved ||\n        (now - lastTimeAtMinShare < getMinSharePreemptionTimeout())) {\n      starvation = Resources.clone(Resources.none());\n\n      if (!starved) {\n        setLastTimeAtMinShare(now);\n      }\n    }\n\n    return starvation;\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78411782/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78413426", "body": "I think I was a little confused.  How about this:\n\n```\n      try{\n        starvedApp = context.getStarvedApps().take();\n        if (!Resources.none().equals(starvedApp.getStarvation())) {\n          List<RMContainer> containers = identifyContainersToPreempt(starvedApp);\n          if (containers != null) {\n            preemptContainers(containers);\n          }\n        }\n      } catch (InterruptedException e) {\n        LOG.info(\"Preemption thread interrupted! Exiting.\");\n        interrupt();\n      }\n```\n\nIt does some extra work inside the _try_, but the logic is much simpler.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78413426/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78413869", "body": "I think you should do whatever makes the code cleanest and easiest to maintain.  I don't think making the context a glorified hash map helps you in any notable way here.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78413869/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78414451", "body": "It's stylistic, and there's no guideline about multiple exit points that I know of, so I won't push it.  I don't think this form is very future-safe, though.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78414451/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78414618", "body": "It might be clearer if you swapped the _if_ and _else_.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78414618/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78645877", "body": "Missed this before.  I'd much rather see:\n\n```\nint ret = 1;\n\nif (Resources.fitsIn(app1.getStarvation(), app2.getStarvation())) {\n  ret = -1;\n}\n\nreturn ret;\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78645877/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78646528", "body": "I'd prefer the line break after the first or second arg.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78646528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100632402", "body": "Is it supported to have different NMs heartbeat at different intervals?  I kinda assume so, though I doubt it's a good idea.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100632402/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100633456", "body": "The name and description should include the units.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100633456/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100639340", "body": "Can we assume that resource requests are this unique?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100639340/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100639608", "body": "Why is this check inside the loop?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100639608/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100640868", "body": "This is super obtuse logic.  Please document it thoroughly.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100640868/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100641581", "body": "Here you're subtracting rr * floor(pending / rr), which isn't what you want if ratio > numContainers.  Should just subtract rr * numContainers in that case.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100641581/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100641784", "body": "\\>= ?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100641784/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100642853", "body": "Not a fan of modifying an arg.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100642853/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100643061", "body": "Does the value of minShareStarvation() really change in the loop?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100643061/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100646588", "body": "Extra space after the equals", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100646588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100647513", "body": "Why wouldn't we want to do this in every iteration after the first?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100647513/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100648444", "body": "Name should include a unit", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100648444/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/101159725", "body": "Given that ratio is the number of containers that fit in \"pending,\" ratio is probably a bad name.  That was a good chunk of my initial confusion.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/101159725/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/101160719", "body": "I would make both or neither final", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/101160719/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/101161043", "body": "I love you.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/101161043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/101161807", "body": "assertTrue() without a message is evil", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/101161807/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/101171109", "body": "Maybe use a Guava MultiMap?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/101171109/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/101171746", "body": "Seems like you could use a second check here to make sure that this isn't actually a 1-node rack, e.g. make sure the rack name isn't the resource name.  What's worse, marking an unvisited request as visited or vice versa?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/101171746/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "pippobaudos": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/229999594", "body": "Thanks @templedf  I have updated the pull request following the suggestion\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/229999594/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "benmccann": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/230914717", "body": "Also upgrade avro-maven-plugin? https://github.com/apache/hadoop/blob/trunk/pom.xml#L225\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/230914717/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/233479215", "body": "No, I don't think it's necessary. It just seems like it'd be good to do at some point\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/233479215/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "elad": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/247779965", "body": "Hello, what's the status of this patch? Would be really great if we could use S3 SSE-C with Hadoop on EMR. :)\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/247779965/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "zzvara": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/254246512", "body": "Looks good and fixes the problem.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/254246512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "HorizonNet": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/258602598", "body": "This one was already applied to trunk via [this commit](https://github.com/apache/hadoop/commit/773c60bd7bd00651dc3016799b424b9bd2233eb3).\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/258602598/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "liuml07": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/261158914", "body": "+1\n\nWill commit this week.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/261158914/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/264601296", "body": "Can you create a JIRA on https://issues.apache.org/jira/browse/HDFS and change the title of this PR  to link the JIRA here? See https://wiki.apache.org/hadoop/GithubIntegration for more information.\r\n\r\nFor the code, the DataNode logging has switched to slf4j. Please use placeholders and guard statement like `if (LOG.isTraceEnabled())` is unnecessary. Please refer to https://issues.apache.org/jira/browse/HDFS-8971 for examples.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/264601296/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/284138256", "body": "Can you file a JIRA and link this to it? See https://wiki.apache.org/hadoop/GithubIntegration", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/284138256/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88332309", "body": "objec -> object\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88332309/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88338833", "body": " `<a name=\"ObjectStores\" />` is accidently here I guess?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88338833/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88342205", "body": "The `-diff/-rdiff` option is not supported\n\nYes there is an `rdiff` options that is just added.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88342205/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88343643", "body": "The indention is unnecessary?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/88343643/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "ameks94": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/263344464", "body": "Update PR to fix the checkstyle and whitespace tests failure.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/263344464/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/301451025", "body": "I realized that current solution is not good (to allow RM's launch even with broken app's data).\r\nIt's better to crash RM in case application's file with app's state is broken. This case we can specify more detailed information about which file is broken (Maybe to give the recommendation to remove application's folder with broken data to allow RM to be launched successfully)\r\nSecond, the most important part of the fix should be to find the reason of file's crashing and to find the way to prevent file's crash.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/301451025/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "javeme": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/269766779", "body": "NOTE: The following is a test about foreach with int[]/ArrayList, and the test results are expected(the second for-loop is also executed correctly):\r\n\r\n\timport java.util.ArrayList;\r\n\t\r\n\tpublic class TestForeach {\r\n\t\r\n\t\tpublic static void main(String[] args) {\r\n\t\t\t\r\n\t\t\t// test foreach twice with int[]\r\n\t\t\tint list1[] = new int[]{1, 2};\r\n\t\t\t\r\n\t\t\tSystem.out.println(\"==== int[] 1\");\r\n\t\t\tfor(int i : list1) {\r\n\t\t\t\tSystem.out.println(i);\r\n\t\t\t}\r\n\t\t\t\r\n\t\t\tSystem.out.println(\"===int[] 2\");\r\n\t\t\tfor(int i : list1) {\r\n\t\t\t\tSystem.out.println(i);\r\n\t\t\t}\r\n\t\t\t\r\n\t\t\t// test foreach twice with ArrayList\r\n\t\t\tArrayList<String> list = new ArrayList<String>();\r\n\t\t\tlist.add(\"1\");\r\n\t\t\tlist.add(\"2\");\r\n\t\t\tIterable<String> list2 = list;\r\n\t\r\n\t\t\tSystem.out.println();\r\n\t\t\tSystem.out.println(\"===ArrayList 1\");\r\n\t\t\tfor(String i : list2) {\r\n\t\t\t\tSystem.out.println(i);\r\n\t\t\t}\r\n\t\t\t\r\n\t\t\tSystem.out.println(\"===ArrayList 2\");\r\n\t\t\tfor(String i : list2) {\r\n\t\t\t\tSystem.out.println(i);\r\n\t\t\t}\r\n\t\t}\r\n\t\r\n\t}\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/269766779/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/270348153", "body": "According to [Daniel Templeton](https://issues.apache.org/jira/browse/MAPREDUCE-6827?focusedCommentId=15795763&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15795763), we think it is expected.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/270348153/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "orngejaket": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/275288717", "body": "Accidently included others commits due to trying to resolve unpushable commit.  Update to the pull request is in e7303f9eff316d3d4329de9011cfd4348ac97a23.  Will correct this tomorrow and resubmit cleaner patch.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/275288717/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/276435073", "body": "> As stated on the JIRA, I'd really like tests which verify that sse-c credentials are making it up. \r\n\r\nThe ITestS2AEncryptionSSEC tests currently just check to make sure that md5 of the encryption key is filled in.  I'll change it to do a md5 on the key to verify it.  The AWS UI won't show those files as encrypted in the properties or metadata.  But the Java SDK can see if that one field is filled in with the md5.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/276435073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/279430934", "body": "Closing because changes are already pushed in.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/279430934/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98723398", "body": "I assume that I can break that convention elsewhere?   I've got several lines that come in at 82-84 chars.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98723398/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "mojodna": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/275497390", "body": "@steveloughran I'm trying this as part of 3.0.0-alpha2 (it's exactly what I was looking for after running into the same OOM problems) and wondering when it cleans up the disk-cached blocks.\r\n\r\nI'm generating a ~50GB file on an instance with ~6GB free when the process starts. My expectation is that local copies of the blocks would be deleted after those parts finish uploading, but I'm seeing more than 15 blocks in `/tmp` (and none of them have been deleted thus far).\r\n\r\nI can't confirm that any parts have finished uploading, though I suspect they have.\r\n\r\nI see that `DiskBlock` deletes temporary files when closed, but is it closed after the block has finished uploading or when the entire file has been fully written to the FS?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/275497390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/275531446", "body": "Done: https://issues.apache.org/jira/browse/HADOOP-14028", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/275531446/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "kambatla": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/280252967", "body": "Committed this to trunk and branch-2.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/280252967/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77927367", "body": "Adding another variable and breaking out of the for loop seems more complicated that it is worth. Leaving it as is unless you insist. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77927367/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77927875", "body": "Did that initially. That bloated up the patch quite a lot, and is somewhat orthogonal. Will file a follow-up JIRA for that work. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77927875/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77928093", "body": "Filed YARN-5625. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77928093/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77928395", "body": "I am not clear on the convention. Always thought there should be a space before and after the colon. Online references seem to use both. Leaving it as is. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77928395/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77928672", "body": "The else is required. We are iterating through all apps with unmet demand. Since the list is sorted by fairshare starvation, we could stop iterating when we hit an app that is at or above its fairshare. Added a comment to clarify that. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77928672/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77929020", "body": "I was tempted to, but then thought FSContext is just a structure to hold context and should only have fields with getters and setters. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77929020/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77929638", "body": "You are right that:\n\nstarvation = desiredShare - currentUsage;\nstarved = starvation > 0;\n\nHowever, you are missing desiredShare = min(minShare, demand). Essentially, the missing case is when the demand is less than minShare but still more than current allocation. \n\nThat said, I see a minor simplification. Will do that. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77929638/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77930153", "body": "Adding a new variable and returning that seems like an overkill. Can't use the same variable or initialize to none() as that requires creating a new object. \n\nLeaving it as is. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77930153/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77930328", "body": "Good call! Added that. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77930328/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77930405", "body": "FairScheduler#serviceStop calls preemptionThread.interrupt. Are you referring to that? \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77930405/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77930504", "body": "Are you suggesting having the run method return a boolean? \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77930504/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78478634", "body": "Actually, this can be simplified further. Improvement in updated patch. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78478634/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/79765051", "body": "I don't understand the reasons, but fixing it anyway. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/79765051/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87639061", "body": "Should this be at DEBUG level?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87639061/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87639089", "body": "Should this be at DEBUG level?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87639089/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87639223", "body": "Why is this extra? What is the base? \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87639223/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87640580", "body": "Why do these need to be a map, why not a queue? Is it because there could be multiple Container objects corresponding to one ContainerId? \n\nIf yes, would it make sense to implement equals and hashCode on Container to ensure we check only ContainerId? \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87640580/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87640635", "body": "Same comment about it being a queue here..\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87640635/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87641199", "body": "Shouldn't be this higher than WARN? \n\nThis seems like a pretty bad case that we should never run into. Should we take more drastic measures? Crash the NM especially if failfast is true?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87641199/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87642982", "body": "One line for imports? \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87642982/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87643223", "body": "imports should be on a single line? \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87643223/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87823624", "body": "Sounds reasonable.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87823624/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97617498", "body": "s/preempt/preempting", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97617498/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97618432", "body": "Nit: I would prefer setNumAMContainersPerNode.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97618432/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99942869", "body": "Why is this necessary? ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99942869/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99943541", "body": "Can we add a new test that verifies the exact scenario in the JIRA description? ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99943541/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99943676", "body": "Should the check of the allowPreemptionFrom flag also be part of this method? ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99943676/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99967724", "body": "Aah, I keep forgetting branch-2.8 was cut years ago. :(", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99967724/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99967958", "body": "Can we add this test to TestFairSchedulerPreemption instead? ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99967958/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99968314", "body": "Maybe, we could make this message more clear. \"Parent queue is null. Looks like we are checking if root can be preempted.\"\r\n\r\nAlternatively, can we make the if check (parent != null && ...)? That way, else would capture the null case and things should work fine? ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99968314/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97623212", "body": "Should we drop this line altogether? ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97623212/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97640737", "body": "Should this check be in setPolicy or the FSQueue constructor instead? \r\n\r\nFor instance, FSLeafQueue#setPolicy already checks if the level is appropriate. This brings up another point - do we need this check of parent-child policies AND the depth? Should we get rid of depth either in this JIRA or a follow-up? ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97640737/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97641519", "body": "Initialize should likely go to setPolicy", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97641519/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97641557", "body": "setPolicy should likely be called in the constructor. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97641557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97641755", "body": "It makes sense for this method to be boolean. I am not sure we should be throwing an exception here. The caller can decide that based on the return value. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97641755/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97641888", "body": "Based on my earlier comment, this should be `return ! (childPolicy instanceof DominantResourceFairnessPolicy`", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97641888/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97641986", "body": "return false", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97641986/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97642235", "body": "Can this be verifyAndSetPolicyFromConf, and part of FSQueue instead of FSParentQueue. \r\n\r\nIn that case, we will not need a separate call to setPolicy. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/97642235/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99953234", "body": "Unused import. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99953234/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99953469", "body": "s/are/is", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99953469/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99953540", "body": "Like that we are adding a non-abstract method. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99953540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99953850", "body": "s/policies/policy", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99953850/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99954044", "body": "IMO, we should either (1) not say anything about other policies or (2) list the policies that are allowed. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99954044/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99954456", "body": "s/function/method - there is one other instance of this in the javadoc\r\n\r\ns/don't/does not\r\n\r\nInstead of saying there is different logic, can we call out what method does that for easier code navigability? And, it might be worth mentioning why that logic is separated, either here or at the other method.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99954456/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99954759", "body": "It might be worthwhile to point out the intended caller for this method. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99954759/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99955335", "body": "TestFairScheduler is awfully long. Can we please add these methods elsewhere? TestSchedulingPolicy and TestQueueManager are potential candidates. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99955335/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99955463", "body": "Are all the cases in this test covered by other tests added here? If not, can we keep the test, maybe rename it, and capture the cases that are not covered? ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99955463/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100712803", "body": "Minor language comment. Let us use one of the following two:\r\n- when creating a new queue or reloading the allocation file.\r\n- when a new is created or the allocation file is reloaded.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100712803/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98557903", "body": "Should we log t.getName() instead of t itself? ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98557903/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98558016", "body": "New line at the end of the file? ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98558016/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98558323", "body": "Are we sure we can call this synchronously? Would it be more appropriate to do the transition to standby in a different thread, like in RMStateStore? If yes, may be we could update RMStateStore to use this too? ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98558323/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98558451", "body": "Not sure if this javadoc is required. This is a convention we generally follow. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98558451/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98564857", "body": "If we are anyways creating the MockRM, why not start it and have some thread throw an uncaught exception and verify the RM behavior in two cases (two tests?) - exit and transition to standby? ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98564857/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98580210", "body": "My bad. I see this is being done in a new thread. \r\n\r\nThat said, may be, we could consider abstracting this out so this handler and the RMStateStore just call that abstraction? ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98580210/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98808165", "body": "Instead of using an anonymous class, can we define this as a separate Thread and name it for easier debugging? ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98808165/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98808465", "body": "Nice tests!", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98808465/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98941743", "body": "Sorry for not identifying this earlier. We should make this thread-safe in case this is triggered by two critical threads failing at the same time. \r\n\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98941743/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98942179", "body": "Also, would it make sense to create an instance of the Runnable on transition to active, and start a new thread on a need-to basis. If all threads use a single instance of the Runnable, may be it is easier to coordinate?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98942179/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98942361", "body": "Naming the Runnable a Thread sounds confusing. Can we change it to TransitionToStandbyRunnable or some such? ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/98942361/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99948742", "body": "- s/shutdowns/shuts down\r\n- s/makes RM transition/ transitions the RM\r\n- s/if any uncaught exception.../if a critical thread throws an uncaught exception. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99948742/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99949581", "body": "Let us add javadoc for this class, and include details on how we use the same runnable.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99949581/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99949771", "body": "Add more detail here: \"Run this only once, even if multiple threads end up triggering this simultaneously.\"", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99949771/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99949838", "body": "Maybe, rename this to hasAlreadyRun? And, again add some javadoc here too? ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/99949838/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100468005", "body": "We don't need to expose this outside YARN at all. This should be @Private. Let us remove @Evolving altogether. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100468005/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100672925", "body": "The NM heartbeat interval is set on the RM. The RM tells the NMs when next to heartbeat. The NMs heartbeat out-of-band as containers complete. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100672925/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100672986", "body": "Done", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100672986/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673210", "body": "Yeah, looks like there is indeed a bug here. \r\n\r\nConsider an app asks for one container each on two nodes on the same rack:\r\n- If this code encounters either of these node-local requests, it ignores the other node and the rack requests. Ignoring the other node-local request is undesired. \r\n- if this code encounters the rack-local request, it ignores the node-local requests. This is desired. \r\n\r\nMaybe, on encountering a node-local request, we should mark the rack and ANY as \"visited\". What do we do when we encounter rack or ANY first? Let me think more about this.  ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673210/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673232", "body": "To check if the previous iteration identified enough RRs to meet all of pending.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673232/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673269", "body": "Agree. Did. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673269/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673282", "body": "Good catch. Updated it. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673282/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673288", "body": "> and >= shouldn't really matter. Updated to >=", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673288/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673296", "body": "Good catch. That was a mistake, intended it to be the argument. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673296/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673303", "body": "Fixed. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673303/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673331", "body": "Was avoiding cloning the object, but agree that is error-prone. Updated. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673331/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673353", "body": "We might have to consider a few apps (iterations) to exhaust the pending minshare. Once we do, we should set the remaining app's minshares to 0.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673353/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673366", "body": "Fixed. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/100673366/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "zhang-yuan": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/284577905", "body": "oh\u2026sorry for that", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/284577905/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "adyatlov": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/285018712", "body": "Thank you, @aajisaka. Done.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/285018712/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "pgaref": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/309509776", "body": "Sure @aajisaka , I just submitted a patch. ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/309509776/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/309628876", "body": "Sure thanks! ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/309628876/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "shawnam": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/328946722", "body": "I'm going to go the patch route.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/328946722/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "johannes-altiscale": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/334279959", "body": "SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #13013 (See https://builds.apache.org/job/Hadoop-trunk-Commit/13013/)", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/334279959/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "vrozov": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/335526234", "body": "It will be good to change Path constructors that take `String child` as an argument to use `new Path(null, null, child)` instead of `new Path(child)`. It should help to handle cases like `new Path(\"file:/\", \"abc:/def\");`.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/335526234/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/335537049", "body": "@yufeldman My suggestion is not to change all the code that is already written. I refer to `Path(String parent, String child)` constructor on line 114 that calls `new Path(child)`. There is no need to parse `child`, only `parent` may have scheme and authority.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/335537049/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/335631774", "body": "@yufeldman I am not sure that URI `resolve()` method (RFC2396 5.2) is directly applicable here even though `Path` uses `URI.resolve()` in its implementation. `Path` constructor refers to `parent` and `child`, that is different from constructing absolute URI from base and current, where current may be an absolute URI. For `Path(parent, child)`, I would not expect `child` to be an absolute Path.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/335631774/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/336008265", "body": "@yufeldman sure, if that will be easier.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/336008265/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "juanrh": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/347298546", "body": "Pushed in https://github.com/apache/hadoop/commit/b46ca7e73b8bac3fdbff0b13afe009308078acf2", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/347298546/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "jebat9999": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/352928115", "body": "Better you asked another persons", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/352928115/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/26379606", "body": "Detail...please...about my payment", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26379606/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "dchuyko": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/353372453", "body": "Ok, I changed the patch according to the review.\r\n\r\n1. There's now isJavaSpecAtLeast() in Shell. This is a part I like.\r\n\r\n2. In DataChecksum there are now: racy state \"useJava9Crc32C\" which is set to the above by default and falls to false on error. New Java9Crc32CFactory now holds static method handles stuff to let it be initialized on first usage. And there is now also LOG to possibly log errors that never happen. To me it looks probably like too much meat for code that won't be called ever. And in case of NOP logger no one will see the error if it happens still. From the other hand all that may look more safe.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/353372453/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/356604364", "body": "A copy of the patch from the pull request was attached as HADOOP-15033.010.patch in Jira. The patch passed genericqa checks and shows same performance improvement a before. Please review this latest version of pull request.", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/356604364/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/356908854", "body": "Many thanks for the review!", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/356908854/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "Abdimaliksk": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/357871858", "body": "ok bro", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/357871858/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ballonike": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/358293361", "body": "Ways to fix things ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/358293361/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "AshB2108": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/359235756", "body": "Hi \r\n\r\nI would like to take up this issue . Can you guys let me know how to setup the env in eclipse.\r\nI have installed mvn and ant .Cloned the repository and imported the project in eclipse .\r\n\r\nI need a guide to export the code and debug it ..\r\nThanks in Advance :-) ", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/359235756/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/359235781", "body": "Hi\r\n\r\nI would like to take up this issue . Can you guys let me know how to setup the env in eclipse.\r\nI have installed mvn and ant .Cloned the repository and imported the project in eclipse .\r\n\r\nI need a guide to export the code and debug it ..\r\nThanks in Advance :-)", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/359235781/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "tonyzeng20151": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/9905533", "body": "Hi\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/9905533/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "zhangminglei": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/11830060", "body": "Good\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/11830060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "svnpenn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/13832916", "body": "This line is insane because GnuWin32 doesn\u2019t even provide a shell:\n\nhttp://gnuwin32.sourceforge.net/faq.html#How_do_I_run_shell_scripts\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/13832916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "msemelman": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/15730983", "body": "It would be nice to see an example.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/15730983/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "DasiyShang": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/17624303", "body": "just to study\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/17624303/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "yuj": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/18033277", "body": "Could someone explain why 'others' are not allowed to even read the logs inside App dirs?  Would 0775 work?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/18033277/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "tgravescs": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/18041857", "body": "others aren't allowed to see logs because the application could be logging something that is secure/sensitive.  ie financial data, passwords, etc.  Ideally they aren't but we need to protect other applications from reading this without explicit authorization.  \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/18041857/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "ybank": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/19199596", "body": "Just came across this when I am doing code analysis. Very minor issue, though.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19199596/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "nfouka": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/21319790", "body": ".", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/21319790/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "donnyw88": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/25191011", "body": "[license.txt](https://github.com/apache/hadoop/files/1415870/license.txt)\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/25191011/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "1004770753": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/26564582", "body": "baga", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26564582/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "PrinceKK300188": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/26697240", "body": "Why", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26697240/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/26697248", "body": "Fuck", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26697248/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "adsontag": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/26788523", "body": "Kill", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26788523/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "aw-was-here": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78162457", "body": "If there is an Xmx in HADOOP_CLIENT_OPTS and an Xmx in MAPRED_DISTCP_OPTS, then the mapred distcp final HADOOP_OPTS will definitely have two Xmx flags.  After HADOOP-13365, we'll be in a position to potentially de-dupe user provided settings like we do for other things.  But until de-dupe, you're correct that it's a JVM decision.  In the past, that decision has been last one wins and I doubt Oracle could change it if they wanted to at this point without major ramifications.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78162457/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "pieterreuse": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83198713", "body": "I'm nitpicking here, but wouldn't it make more sense to define DestState here instead of on line 272? Moving that line here would improve code readability imo but wouldn't change any behaviour.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83198713/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83199198", "body": "This method is passed an object, not a class. You probably meant \"If this _object_ implements ...\"\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83199198/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83199799", "body": "The lazy creation in this method is nice, but the \"maybe\" in its name gives a false impression of arbitrariness involved. \"createBlockIfNeeded\" might be a better naming option.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83199799/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83416183", "body": "I'm not familiar with DeprecationDelta's, but this _null_ value gave rise to a nullpointerexception on **all** unit tests when fs.s3a.threads.core was in my config. Replacing this _null_ with _\"\"_ (empty string) resolved my issue, but I'm not 100% sure that is the right thing to do here.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83416183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83427881", "body": "That indeed fixes the problems I had, thx for looking into this.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83427881/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "thodemoor": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83377114", "body": "The total max block (memory/disk) consumption, across all streams, is bounded by`fs.s3a.multipart.size * ( fs.s3a.fast.upload.active.blocks + fs.s3a.max.total.tasks +  1)` bytes for an instance of S3AFileSystem.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83377114/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83379622", "body": "FYI Up to 10k. That's AWS's limit on the number of parts in a single multipartupload.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83379622/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83381587", "body": "but bounded by ... \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83381587/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83384100", "body": "Memory usage is bounded to ...\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83384100/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83384132", "body": "idem as in pom.xml\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83384132/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83384371", "body": "idem\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83384371/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83384867", "body": "As a (probably better) alternative to my other comments, we could explain the bound on the memory consumption here once and link to it.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83384867/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83385370", "body": "To me, the wording here gives the impression this is a server-side operation but the purging happens on the client by listing all uploads and then sending a delete call with the ones to be purged. Consequently, this can cause a (slight) delay when instantiating an s3a FS instance and there are lots of active uploads (to purge).\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83385370/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83415072", "body": "Completely agree. A bit further down I propose to add a single explanation in the javadoc and link to there in the various other locations\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83415072/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83419562", "body": "We can . With the min part size of 5MB you need a 50GB upload to test this. Will take a while vs. AWS. We can test this cheaply, but of course vs our S3-clone, but at least that will test the log @ error.\n@pieterreuse please add this to our testplan \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83419562/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83421605", "body": "The grunt work is done in `com.amazonaws.services.s3.transfer.TransferManager#abortMultipartUploads`\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83421605/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83421755", "body": "And yes making async is again a very good idea here.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/83421755/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "kkaranasos": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87647259", "body": "I had named that extra because some containers might already have been marked for killing, and here you will keep the additional ones.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87647259/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "karth295": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87856515", "body": "You need to free `keyLenErrMsg` here.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87856515/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87856607", "body": "Same here -- you need to free `ivLenErrMsg`\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/87856607/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "yuanboliu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/80836707", "body": "`crc` should be `-crc`\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/80836707/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/80839392", "body": "hadoop fs -copyFromLocal -d -f ~/datasets/devices.orc s3a://bucket/datasets/\nThe symbol \"~\" is redundant, right?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/80839392/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/80839511", "body": "`hadoop fs -put -d -f - wasb:` should be `hadoop fs -put -d -f wasb:`\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/80839511/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}}}}