{"_default": {"1": {"sershe-apache": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/3bbf35f8ecfecc6690832ee43f4e2d2bcdad7660", "message": "HIVE-18500 : annoying exceptions from LLAP Jmx view in the logs (Sergey Shelukhin, reviewed by Jason Dere)"}, {"url": "https://api.github.com/repos/apache/hive/commits/90d236affcb19b52ca66029e6646c5d751dc5f02", "message": "HIVE-18231 : validate resource plan - part 2 - validate action and trigger expressions (Harish Jaiprakash, reviewed by Sergey Shelukhin)"}, {"url": "https://api.github.com/repos/apache/hive/commits/6d890faf22fd1ede3658a5eed097476eab3c67e9", "message": "HIVE-18488 : LLAP ORC readers are missing some null checks (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/900da82915209630ec7c194619028e5fa1b87d28", "message": "HIVE-18438 : WM RP: it's impossible to unset things (Sergey Shelukhin, reviewed by Harish Jaiprakash, Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/572eaccba316f9ae911b6fda01c09547ccbb1157", "message": "HIVE-18457 : improve show plan output (triggers, mappings) (Sergey Shelukhin, reviewed by Harish Jaiprakash, Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/78d5572f8e165a907c63e1ef8579f591b8c34563", "message": "HIVE-18452 : work around HADOOP-15171 (Sergey Shelukhin, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/72684f10da8f124dfa624b16f3ffebeb2155664d", "message": "HIVE-18273 : add LLAP-level counters for WM (Sergey Shelukhin, reviewed by Harish Jaiprakash and Gunther Hagleitner)"}, {"url": "https://api.github.com/repos/apache/hive/commits/fde503dcaa1f3e322eb4dc34bfffbec89aefd240", "message": "HIVE-18437 : use plan parallelism for the default pool if both are present (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/7eac320ee8bf6165fda43950c4bb8d765a208c81", "message": "HIVE-18229 : add the unmanaged mapping command (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/69d32f58902fd055fd574d56d88a897f127deb92", "message": "HIVE-18418 : clean up plugin between DAGs (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/cfd2b149a82d3e3dfbd1f0608ace514195ec5cf2", "message": "HIVE-18274 : add AM level metrics for WM (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/134294310215e79bec5f189a5eb6cb7ea6734b4a", "message": "HIVE-18420 : LLAP IO: InputStream may return 0 bytes (Sergey Shelukhin via Prasanth J)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/19e6a8b560c67313ff358e2c183aa47b443fc033", "message": "HIVE-18417 : better error handling in TezSessionState cleanup (Sergey Shelukhin via Jason Dere)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/e29aea6fa20c1096ac9449ca04669f927a8a7101", "message": "HIVE-18269 : LLAP: Fast llap io with slow processing pipeline can lead to OOM (Sergey Shelukhin, reviewed by Jason Dere and Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/c106f750ec43839c4e9a033057f5e10f1a5c67f2", "message": "HIVE-18004 : investigate deriving app name from JDBC connection for pool mapping (Sergey Shelukhin, reviewed by Harish Jaiprakash, Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/aa7a9caf347fd413715f9f474861647c69746f6c", "message": "HIVE-18326 : LLAP Tez scheduler - only preempt tasks if there's a dependency between them (Sergey Shelukhin, reviewed by Eric Wohlstadter, Jason Dere)"}, {"url": "https://api.github.com/repos/apache/hive/commits/a6b88d9d2ff74a3767fe316e72d1dd3f69fc6b4a", "message": "HIVE-18096 : add a user-friendly show plan command (Harish Jaiprakash, reviewed by Sergey Shelukhin)"}, {"url": "https://api.github.com/repos/apache/hive/commits/a6435cca3ab11d39d7887eae8e077c386ea9133c", "message": "HIVE-18275 : add HS2-level WM metrics (Sergey Shelukhin, reviewed by Thejas M Nair)"}, {"url": "https://api.github.com/repos/apache/hive/commits/20c9a3905f4b1b627c935ad54a53a7a59015587c", "message": "Revert \"HIVE-18326 : LLAP Tez scheduler - only preempt tasks if there's a dependency between them (Sergey Shelukhin, reviewed by Eric Wohlstadter, Jason Dere)\"\n\nThis reverts commit 3f5148d6aae94f2ae9db2aacccb302211834c699."}, {"url": "https://api.github.com/repos/apache/hive/commits/3f5148d6aae94f2ae9db2aacccb302211834c699", "message": "HIVE-18326 : LLAP Tez scheduler - only preempt tasks if there's a dependency between them (Sergey Shelukhin, reviewed by Eric Wohlstadter, Jason Dere)"}, {"url": "https://api.github.com/repos/apache/hive/commits/006d69d8867b06e27f8890698af4390366b71f3b", "message": "HIVE-18360 : NPE in TezSessionState (Sergey Shelukhin, reviewed by Jason Dere)"}, {"url": "https://api.github.com/repos/apache/hive/commits/5cd047b55d4901f2e1d2a0542c3183fa50bb0ef2", "message": "HIVE-18095 : add a unmanaged flag to triggers (applies to container based sessions) (Sergey Shelukhin, reviewed by Harish Jaiprakash and Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/0652c5e83ba22eac8f22275f640c12c64578df93", "message": "HIVE-18257 : implement scheduling policy configuration instead of hardcoding fair scheduling (Sergey Shelukhin, reviewed by Prasanth Jayachandran and Harish Jaiprakash)"}, {"url": "https://api.github.com/repos/apache/hive/commits/ed1cf112a5a9e022f12c0f28e953435e1248b876", "message": "HIVE-18318 : LLAP record reader should check interrupt even when not blocking (Sergey Shelukhin, reviewed by Gopal Vijayaraghavan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/4e43ec7c4c623537bf46b74ba5c01f314db72162", "message": "HIVE-18078 : WM getSession needs some retry logic (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/3407e723af7855bf075022be076a5ba48b9ee088", "message": "HIVE-18230 : create plan like plan, and replace plan commands for easy modification (Sergey Shelukhin, reviewed by Harish Jaiprakash and Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/025ced9bc1a116d726459d5bba1907c0e4c9e560", "message": "HIVE-18203 : change the way WM is enabled and allow dropping the last resource plan (Sergey Shelukhin, reviewed by Harish Jaiprakash and Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/e120bd8b0b8b74516651f3ae9e4e7d3a170b0d4d", "message": "HIVE-18003 : add explicit jdbc connection string args for mappings (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/89dbf4e904592318da954eaf94548ec1b130e17c", "message": "HIVE-18153 : refactor reopen and file management in TezTask (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/d6ce23d53d9605c3b28f155e4d0810c9c26d587c", "message": "HIVE-18075 : verify commands on a cluster (Harish Jaiprakash, reviewed by Sergey Shelukhin)"}, {"url": "https://api.github.com/repos/apache/hive/commits/96c2fa86b3ab09d611dbcf786d7abdbaeecfe14f", "message": "HIVE-18240 : support getClientInfo/setClientInfo in JDBC (Sergey Shelukhin, reviewed by Vaibhav Gumashta)"}, {"url": "https://api.github.com/repos/apache/hive/commits/0d8323357c9bf944f80823d8607d03f1b1f5fc52", "message": "HIVE-18179 : Implement validate resource plan (part 1) (Harish Jaiprakash, reviewed by Sergey Shelukhin)"}, {"url": "https://api.github.com/repos/apache/hive/commits/fb85336f703ceb1318fed172a3b8fec2b7ce16ca", "message": "HIVE-18220 : Workload Management tables have broken constraints defined on postgres schema (Deepesh Khandelwal, reviewed by Sergey Shelukhin)"}, {"url": "https://api.github.com/repos/apache/hive/commits/5f12cb84406e50a4f939bf394d8514cd2257d52e", "message": "HIVE-17856 : MM tables - IOW is not ACID compliant (Steve Yeom, reviewed by Sergey Shelukhin and Eugene Koifman)"}, {"url": "https://api.github.com/repos/apache/hive/commits/4ccea29bdc9f1f0af3611e3abba1e8af5b898ef8", "message": "HIVE-17905 : propagate background LLAP cluster changes to WM (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/b6760b017cf726a82d727d2970cdcfc0e7eebcb2", "message": "HIVE-18076 : killquery doesn't actually work for non-trigger WM kills (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/3500196b92e07331c3c8309b81ab9d8342bcd767", "message": "HIVE-18134 : some alter resource plan fixes (Sergey Shelukhin, reviewed by Prasanth Jayachandran and Harish Jaiprakash)"}, {"url": "https://api.github.com/repos/apache/hive/commits/5add3a7ec92ad091c7e0d496905b06bdc44992e2", "message": "HIVE-17954 : Implement pool, user, group and trigger to pool management API's (Harish Jaiprakash, reviewed by Sergey Shelukhin) ADDENDUM"}, {"url": "https://api.github.com/repos/apache/hive/commits/a21742ebb4aad3d550bca86d4249d8b7b7fbdb18", "message": "HIVE-18029 : beeline - support proper usernames based on the URL arg (Sergey Shelukhin, reviewed by Vaibhav Gumashta)"}, {"url": "https://api.github.com/repos/apache/hive/commits/2b730524f19420df120ec73ef972ad244ae380c1", "message": "HIVE-18073 : AM may assert when its guaranteed task count is reduced (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/44ef599155efa998b59b0723b2bb705bf60a1f21", "message": "HIVE-17954 : Implement pool, user, group and trigger to pool management API's (Harish Jaiprakash, reviewed by Sergey Shelukhin)"}, {"url": "https://api.github.com/repos/apache/hive/commits/388543b3e47770575a83459248aa0ddb25483819", "message": "HIVE-18109 : fix identifier usage in parser (Sergey Shelukhin via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/179e32f0bd1ea915bb49aec389cba698af92150b", "message": "HIVE-18050 : LlapServiceDriver shoud split HIVE_AUX_JARS_PATH by ':' instead of ',' (Aegeaner, reviewed by Sergey Shelukhin)"}, {"url": "https://api.github.com/repos/apache/hive/commits/82fee0f7cf7e0dd5bbc32f4ab31da31625a8dab7", "message": "HIVE-18072 : fix various WM bugs based on cluster testing - part 2 (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/21008897c5b7e79aff253967cff3dd308fb901ce", "message": "HIVE-17631 : upgrade orc to 1.4.1 (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/477e9c5ff94da173a4f9e641f206f81814635dfb", "message": "HIVE-18071 : add HS2 jmx information about pools and current resource plan (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/404993a98bd8bbe7488b1d4a0e40e242ebe8dc73", "message": "HIVE-17904 : handle internal Tez AM restart in registry and WM (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/d2958495ae59bdc3a91ed17155369af7fc5c7866", "message": "HIVE-18002 : add group support for pool mappings (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/25f3c3f9673dc48b648b8c25ba28457139261c0e", "message": "HIVE-17906 : use kill query mechanics to kill queries in WM (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/3914a1b29248093dfebabfb1852f443941e489c9", "message": "HIVE-18028 : fix WM based on cluster smoke test; add logging (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/a42314deb07a1c8d9d4daeaa799ad1c1ebb0c6c9", "message": "HIVE-17902 : add notions of default pool and start adding unmanaged mapping (Sergey Shelukhin, reviewed by Prasanth Jayachandran, Harish Jaiprakash)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "prasanthj": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/12818520e65c40d7b451918c59be52e61b8dc3cc", "message": "HIVE-18458: Workload manager initializes even when interactive queue is not set (Prasanth Jayachandran reviewed by Sergey Shelukhin)"}, {"url": "https://api.github.com/repos/apache/hive/commits/ba0217ff17501fb849d8999e808d37579db7b4f1", "message": "HIVE-17952: Fix license headers to avoid dangling javadoc warnings (Andrew Sherman reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/cc50d62a0c133876738e7ce6f0ddccd21763c3e6", "message": "HIVE-18384: ConcurrentModificationException in log4j2.x library (Prasanth Jayachandran reviewed by Sergey Shelukhin)"}, {"url": "https://api.github.com/repos/apache/hive/commits/fe4bd04044282af5711cc8c4e6cc6ebc237c8dca", "message": "HIVE-18266: LLAP: /system references wrong file for THP (Prasanth Jayachandran reviewed by Sergey Shelukhin)"}, {"url": "https://api.github.com/repos/apache/hive/commits/a3060b30705d1bd55c6be5357f7575534c84e6b0", "message": "HIVE-18088: Add WM event traces at query level for debugging (Prasanth Jayachandran reviewed by Sergey Shelukhin)"}, {"url": "https://api.github.com/repos/apache/hive/commits/f68ebdce7850b748fdbdb4548ce64cbb2d2a47c4", "message": "HIVE-18188: Fix TestSSL failures in master (Prasanth Jayachandran reviewed by Sergey Shelukhin)"}, {"url": "https://api.github.com/repos/apache/hive/commits/c03001e9876b8a82914e6e025bbc1903a290792f", "message": "HIVE-18170: User mapping not initialized correctly on start (Prasanth Jayachandran reviewed by Sergey Shelukhin)"}, {"url": "https://api.github.com/repos/apache/hive/commits/07fe7e210cb444aec43cb5adda37f8f7cd26f243", "message": "HIVE-18160: Jar localization during session initialization is slow (Prasanth Jayachandran reviewed by Sergey Shelukhin)"}, {"url": "https://api.github.com/repos/apache/hive/commits/6c79fa75bef225f7ed1ac5c858879323d2e36825", "message": "HIVE-18025: Push resource plan changes to tez/unmanaged sessions (Prasanth Jayachandran  reviewed by Sergey Shelukhin)"}, {"url": "https://api.github.com/repos/apache/hive/commits/68d35fd60aa8ccde66d759e296ad73fa44c4b353", "message": "HIVE-17882: Resource plan retrieval looks incorrect (Harish Jaiprakash reviewed by Sergey Shelukhin)"}, {"url": "https://api.github.com/repos/apache/hive/commits/51249505c7b3db6bfa586cb1b6dee268f2ed6ce6", "message": "HIVE-17809: Implement per pool trigger validation and move sessions across pools (Prasanth Jayachandran reviewed by Sergey Shelukhin)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/97", "title": "HIVE-14585: Add travis.yml and update README to show build status", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/170674388", "body": "@kirill-vlasov Could you please create a JIRA here https://issues.apache.org/jira/browse/HIVE/ and add the JIRA id in the title of the PR?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/170674388/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/239525701", "body": "+1, lgtm\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/239525701/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/242536543", "body": "Thanks for the patch @mbalassi!\nCreated https://issues.apache.org/jira/browse/HIVE-14647\nWill commit it shortly. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/242536543/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/242536562", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/242536562/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/242542346", "body": "@mbalassi Can you plz prefix the PR heading with \"HIVE-14647: \" so that I can merge it directly?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/242542346/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/265300686", "body": "LGTM, +1", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/265300686/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/15379454", "body": "@phoenixhadoop hive trunk is updated with new hive-log4j2.properties based configuration. If log4j.configurationFile is not explicitly set then by default hive-log4j2.properties file will be looked up in CLASSPATH. Typically, HIVE_CONF_DIR will be set to conf directory which contains hive-log4j2.properties file. HIVE_CONF_DIR is automatically added to CLASSPATH by the hive script. Are you still seeing this issue?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/15379454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/44605629", "body": "Would it be easier to support bulk writes? As isPresent is called for every column.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44605629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44606414", "body": "Actually never mind, since the output stream is buffered. This won't be big win.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44606414/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607051", "body": "What happens if the vec is null and started is false? A run can start with null structs (struct column) right?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607051/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607177", "body": "Again ignore this :) I guess that case is handled by super(). Correct me if I am wrong.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607177/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607540", "body": "It will be good to add tests that uses WriterOptions.setSchema() to specify the types instead of OI. I see that you have fixed bunch of OI casts that would have cause NPE otherwise (if TypeDescription is set through writer).\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607842", "body": "Do we need to bump up the writer version? To differentiate ORC files written with row-by-row vs vector writer?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607842/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44994511", "body": "Yes. Sorry I missed that file.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44994511/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45911544", "body": "Can we put all the interfaces in api package?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45911544/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45920318", "body": "This isn't a shim right? Can this be moved over to OrcUtils?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45920318/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60957625", "body": "Does it clone the options/properties alone? or the fs input stream as well? Can you please add a comment as in what does it clone and what not.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60957625/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60958043", "body": "Should we check if it has hasMaximumLength()?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60958043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424417", "body": "Is there a minimum supported hadoop version for orc? If we don't support older hadoop version then this shim might not be required.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424417/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424431", "body": "Where do we close this stream?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424431/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424440", "body": "nit: bad alignment\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424440/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424450", "body": "ql dependency?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424450/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424466", "body": "Can this be moved to common instead?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424466/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424488", "body": "Never mind. This is already part of storage-api. So not a problem.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "pvary": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/15dd29451303bc0028203aee3d144bb6b28be773", "message": "HIVE-18367: Describe Extended output is truncated on a table with an explicit row format containing tabs or newlines. (Andrew Sherman via Peter Vary)"}, {"url": "https://api.github.com/repos/apache/hive/commits/fbb3ed15fd0f078369c1fcb2edd68326d272758a", "message": "HIVE-18372: Create testing infra to test different HMS instances (Peter Vary, reviewed by Marta Kuczora, Vihang Karajgaonkar and Adam Szita)"}, {"url": "https://api.github.com/repos/apache/hive/commits/6938fcabf0944817fed8241c48b57fb3f5d98c69", "message": "HIVE-18443: Ensure git gc finished in ptest prep phase before copying repo (Adam Szita, via Peter Vary)"}, {"url": "https://api.github.com/repos/apache/hive/commits/b826072edb429214e9fe073dea9381449396f05d", "message": "HIVE-18355: Add builder for metastore Thrift classes missed in the first pass - FunctionBuilder (Peter Vary, reviewed by Alan Gates)"}, {"url": "https://api.github.com/repos/apache/hive/commits/8e32172957c01e095445d0718023feda5c3d86c4", "message": "HIVE-18370: standalone-metastore gen dir contains two annotation/package-info.java which causes IDEA build fail (Peter Vary, reviewed by Zoltan Haindrich)"}, {"url": "https://api.github.com/repos/apache/hive/commits/98c95c67ac2a5c67f703dba382812400732939cb", "message": "HIVE-18356: Fixing license headers in checkstyle (Peter Vary, reviewed by Prasanth Jayachandran, Andrew Sherman)"}, {"url": "https://api.github.com/repos/apache/hive/commits/856d88db9a5c65c902ce6982710839e07d8548a9", "message": "HIVE-18263: Ptest execution are multiple times slower sometimes due to dying executor slaves (Adam Szita, reviewed by Barna Zsombor Klara)"}, {"url": "https://api.github.com/repos/apache/hive/commits/e86c77af5ffa80b55f46eb3b69b0365fbf79ab5a", "message": "HIVE-18228: Azure credential properties should be added to the HiveConf hidden list (Andrew Sherman, via Peter Vary)"}, {"url": "https://api.github.com/repos/apache/hive/commits/f17c9b46878523d2a3928b95299682dd1342d769", "message": "HIVE-18212: Make sure Yetus check always has a full log (Adam Szita, via Peter Vary)"}, {"url": "https://api.github.com/repos/apache/hive/commits/c7aa7b64add25f864c3e61756b54f7ada57edfa3", "message": "HIVE-16748: Integreate YETUS to Pre-Commit (Adam Szita, reviewed by Barna Zsombor Klara, Peter Vary)"}, {"url": "https://api.github.com/repos/apache/hive/commits/5011da72dd27e0513534c3fc9b361dea27d231ea", "message": "HIVE-17785: Encription tests are not running (Peter Vary, reviewed by Barna Zsombor Klara)"}, {"url": "https://api.github.com/repos/apache/hive/commits/4b9a1eacc58ff20ce048fa728bb43b8bf4e8620e", "message": "HIVE-17969: Metastore to alter table in batches of partitions when renaming table (Adam Szita, via Peter Vary)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kgyrtkirk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/deb3adce6e064740b41af90eefe85ca3e923f466", "message": "HIVE-18413: addendum"}, {"url": "https://api.github.com/repos/apache/hive/commits/d6c6d96f98d998298618a643bd0d571d4e290dc3", "message": "HIVE-18222: Update checkstyle rules to be less peeky (Zoltan Haindrich reviewed by Ashutosh Chauhan)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/ad1243bef60f1b1f5f15a511761959df9abae615", "message": "HIVE-18413 : Grouping of an empty result set may only contain null values (Zoltan Haindrich via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/2402f3ca55e3f304e041ff53591f846634fbc9ac", "message": "HIVE-18149 HIVE-18416: addendum"}, {"url": "https://api.github.com/repos/apache/hive/commits/8412748a7ee6264b5365b211affe44af35cae9b0", "message": "HIVE-18330 : Fix TestMsgBusConnection - doesn't test tests the original intention (Zoltan Haindrich via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/b0e653aece3b8e5031fdb513655337ebdc95b6dd", "message": "HIVE-18354: Fix test TestAcidOnTez  (Zoltan Haindrich, reviewed by Eugene Koifman)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/76be4647f23ad2085f7efb12a00e77c998d26517", "message": "HIVE-18305: travis-ci builds are timing out (Zoltan Haindrich, reviewed by Prasanth Jayachandran)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/d0fa7d5515bde6819fcdb54648d678b2448b7731", "message": "HIVE-18108: in case basic stats are missing; rowcount estimation depends on the selected columns size (Zoltan Haindrich, reviewed by Ashutosh Chauhan)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/4ecf2a7554bcfc0fbff19b40b3e95bb2f5a3c6d5", "message": "HIVE-18149: addendum; update missed TestAcidOnTez"}, {"url": "https://api.github.com/repos/apache/hive/commits/31a805bdbc4334c65fa10b1f8c525216db790227", "message": "HIVE-18149: addendum for HIVE-18310"}, {"url": "https://api.github.com/repos/apache/hive/commits/64229b91ba21ce733c31f16dfce6db920e612e6e", "message": "HIVE-15393: addendum: querystring hashcode changes"}, {"url": "https://api.github.com/repos/apache/hive/commits/0e2dab913989a2ccc4fde1f2ff7dfea097f4d493", "message": "HIVE-18224: Introduce interface above driver (Zoltan Haindrich, reviewed by Ashutosh Chauhan)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/1c3698317d332e59fdcfdb3c0285a54f7d3fe46a", "message": "HIVE-18248: Clean up parameters; addendum: add missed files"}, {"url": "https://api.github.com/repos/apache/hive/commits/e26b932536e57ba11813b6bc96f9b9707538963a", "message": "HIVE-18149: Stats: rownum estimation from datasize underestimates in most cases (Zoltan Haindrich, reviewed by Ashutosh Chauhan)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/82590226a89eeac7aa0ace8c311a8d4f4794c5bc", "message": "HIVE-13567: addendum to fix HIVE-18208"}, {"url": "https://api.github.com/repos/apache/hive/commits/3bbc24d250b3402491db1c2ac54730dfec22a3bd", "message": "HIVE-13567 : Enable auto-gather column stats by default (Zoltan Haindrich, Pengcheng Xiong via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/95dadac9fa81702ce3af0d8759bee5082f2f2013", "message": "HIVE-15939: Make cast expressions comply more to sql2011 (Zoltan Haindrich, reviewed by Ashutosh Chauhan)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/c2fc0fb88b794f07d9d4b1cd5f4ef3e1b8737911", "message": "HIVE-18138: Fix columnstats problem in case schema evolution (Zoltan Haindrich, reviewed by Ashutosh Chauhan)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/f63124188ef0965d85ed0af315cab840d1e9af3f", "message": "HIVE-18005: Improve size estimation for array() to be not 0 (Zoltan Haindrich, reviewed by Vineet Garg)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/3df202f293b86227d6f8a26bfd24e90b92edb44b", "message": "HIVE-18163: Stats: create materialized view should also collect stats (Zoltan Haindrich, reviewed by Jesus Camacho Rodriguez)"}, {"url": "https://api.github.com/repos/apache/hive/commits/be1f847334a6ce559f1b07f8930568217333a77e", "message": "HIVE-18141: Fix StatsUtils.combineRange to combine intervals (Zoltan Haindrich, reviewed by Ashutosh Chauhan)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/d22784a9e58d37a5e096029ad55f82ed46491500", "message": "HIVE-18105: Aggregation of an empty set doesn't pass constants to the UDAF (Zoltan Haindrich, reviewed by Ashutosh Chauhan)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/c5539a1b19e603818011cb49d11def0e51f161ce", "message": "HIVE-18063: Make CommandProcessorResponse an exception instead of a return class (Zoltan Haindrich, reviewed by Ashutosh Chauhan)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/89b6566b009d13bbcd86486085bcdc860bd1e6ff", "message": "Revert \"HIVE-18063: Make CommandProcessorResponse an exception instead of a return class (Zoltan Haindrich, reviewed by Ashutosh Chauhan)\"\n\nThis reverts commit ed4e757638981b03d0240edbf84d9e048cae9528."}, {"url": "https://api.github.com/repos/apache/hive/commits/ed4e757638981b03d0240edbf84d9e048cae9528", "message": "HIVE-18063: Make CommandProcessorResponse an exception instead of a return class (Zoltan Haindrich, reviewed by Ashutosh Chauhan)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/66c013305ef04dbd2ebcfe2d36a5a69597923fb2", "message": "HIVE-18092 : Fix exception on tables handled by HBaseHandler if columnsstats are auto-gathered (Zoltan Haindrich via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/8b106e1488f3cb1dee03414e416ff020cb3bfb50", "message": "HIVE-17651: TableScanOperator might miss vectorization on flag (Zoltan Haindrich, reviewed by Jesus Camacho Rodriguez)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/de78ddb7746624912f093c54a50f5b3f6a0dd876", "message": "HIVE-17934 : Merging Statistics are promoted to COMPLETE (most of the time) (Zoltan Haindrich via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/3bb46de8cb20a2116c439fa9dbd9c4e56c041706", "message": "HIVE-18057 : remove PostExecute / PreExecute hook support (Zoltan Haindrich via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/f7f635d6e1b2e676139e1c2f68bbfb1715086a3f", "message": "HIVE-18012 : fix ct_noperm_loc test (Zoltan Haindrich via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/0e54f51a3bd38331ea1a1274cd133010c1444e78", "message": "HIVE-6590 : Hive does not work properly with boolean partition columns (wrong results and inserts to incorrect HDFS path) (Zoltan Haindrich via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/a0b94bc10db77f842c15b082513fc49990ad8d56", "message": "HIVE-16827: addendum: update bucketmapjoin7.q.out for spark"}, {"url": "https://api.github.com/repos/apache/hive/commits/3566873363de9c195984ebcc3825849bc8f9c8b5", "message": "HIVE-17767: addendum; change back order of warning messages"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/158", "title": "HIVE-16214 Service client experiment", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "abstractdog": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/1af7e2b41780161fc62c158a3d8443e98a31f816", "message": "HIVE-18445: qtests: auto_join25.q fails permanently (Laszlo Bodor via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/425271896a834b3eb695a0cfd23334d08565bc9a", "message": "HIVE-18061: q.outs: be more selective with masking hdfs paths (Laszlo Bodor via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/5d4559bbdb5fb36450b67841d0da4fa2fecdeeea", "message": "HIVE-18309: qtests: smb_mapjoin_19.q breaks bucketsortoptimize_insert_2.q (Laszlo Bodor via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/19c2b7225e1c072cba5c1a93329f70059e7e063d", "message": "HIVE-18314: qtests: semijoin_hint.q breaks hybridgrace_hashjoin_2.q (Laszlo Bodor via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vineetgarg02": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/a59cb886ba17aa1f2600f80405989bf38f00289a", "message": "HIVE-18490 : Query with EXISTS and NOT EXISTS with non-equi predicate can produce wrong result (Vineet Garg, reviewed by Jesus Camacho Rodriguez)"}, {"url": "https://api.github.com/repos/apache/hive/commits/d62a038a1ca84037b7b17ce989954a93d1a76a25", "message": "HIVE-18462 : Explain formatted for queries with map join has columnExprMap with unformatted column name (Vineet Garg, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/a91e0fc2a2dfcfa76db96d300065f425c7756c0a", "message": "Revert \"HIVE-186462 : Explain formatted for queries with map join has columnExprMap with unformatted column name (Vineet Garg, reviewed by Ashutosh Chauhan)\"\n\nThis reverts commit 20dec1c3fd4d01810be06c12ea0774e5ba1e4419."}, {"url": "https://api.github.com/repos/apache/hive/commits/20dec1c3fd4d01810be06c12ea0774e5ba1e4419", "message": "HIVE-186462 : Explain formatted for queries with map join has columnExprMap with unformatted column name (Vineet Garg, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/ca96613da5705c8dfd1a3269315551fee225444a", "message": "HIVE-18272: Fix check-style violations in subquery code (Vineet Garg,reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/8ab523b9612e81aec255051bcb2b4137419cfa3b", "message": "HIVE-18241: Query with LEFT SEMI JOIN producing wrong result (Vineet Garg, reviewed by Jesus Camacho Rodriguez)"}, {"url": "https://api.github.com/repos/apache/hive/commits/22e7bffa1c1a9be1e8503ebe0f7adf39060d1979", "message": "HIVE-18246 : Replace toString with getExprString in AbstractOperatorDesc::getColumnExprMapForExplain (Vineet Garg, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/b23c3299fb91954b7f100c003f07ee8c0a3f8353", "message": "HIVE-18173 : Improve plans for correlated subqueries with non-equi predicate (Vineet Garg, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/a988c159cc3e9f43211432eb035454ee6143b219", "message": "HIVE-18123 : Explain formatted improve column expression map display (Vineet Garg, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/a5c2e15c7cc125d8cda2ee3a8ed64c116ff6b755", "message": "HIVE-17898 : Explain plan output enhancement (Vineet Garg, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/7a24defc95b161418fd2bcb38fafee4f27875e56", "message": "HIVE-16828 : With CBO enabled, Query on partitioned views throws IndexOutOfBoundException (Vineet Garg, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/c2be4b717d46713ce2ecd5ee8cd4a57a0fdec770", "message": "HIVE-18008 : Add optimization rule to remove gby from right side of left semi-join (Vineet Garg, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/fde194e3c65fcd0bdc8ac6fcfbf78186031f90c9", "message": "Revert \"HIVE-18008 : Add optimization rule to remove gby from right side of left semi-join (Vineet Garg, reviewed by Ashutosh Chauhan)\"\n\nThis reverts commit ff3b327d322b04916e019fcec75d3fbd48e26bae."}, {"url": "https://api.github.com/repos/apache/hive/commits/ff3b327d322b04916e019fcec75d3fbd48e26bae", "message": "HIVE-18008 : Add optimization rule to remove gby from right side of left semi-join (Vineet Garg, reviewed by Ashutosh Chauhan)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jcamachor": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/41a5c06597cfdc29324d3181d4c5e89a93b0aded", "message": "HIVE-18482: Copy-paste error in the RelOptHiveTable (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/80e6f7b0f13c134642763b0a692c3abbf3ffe106", "message": "HIVE-18386 : Create dummy materialized views registry and make it configurable (Jesus Camacho Rodriguez via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/a45becb1602573cae32673e32161b1a3aa10a86b", "message": "HIVE-18473: Infer timezone information correctly in DruidSerde (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/ac247817f441eba5daf5b07cc831347d1c762ee4", "message": "HIVE-18416: Initial support for TABLE function (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan) (addendum II)\n\n* Fix dangling tests"}, {"url": "https://api.github.com/repos/apache/hive/commits/b1cdbc60d6ef856b852afe1aa44bd3520d5fb84a", "message": "HIVE-18416: Initial support for TABLE function (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan) (addendum)"}, {"url": "https://api.github.com/repos/apache/hive/commits/7e64114ddca5c07a0c4ac332c1b34b534cc2e9ed", "message": "HIVE-18416: Initial support for TABLE function (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/a7c41ba2afb36398134a824bc461a50499114feb", "message": "HIVE-14498: Freshness period for query rewriting using materialized views (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan) (addendum)"}, {"url": "https://api.github.com/repos/apache/hive/commits/57d909c34b07ed17cd88bbc6e0d5204e57c8b58f", "message": "HIVE-14498: Freshness period for query rewriting using materialized views (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/96a409e1c6ec846eeb6c72b50bed60790ccc1836", "message": "HIVE-18361: Extend shared work optimizer to reuse computation beyond work boundaries (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/3aaddb2d291a3d1b1d73f1b0adb38186cf9be3d0", "message": "HIVE-18053: Support different table types for MVs (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/7ea263cbe20eab04a813d270fc8cddc9ad80e7dc", "message": "HIVE-18250: CBO gets turned off with duplicates in RR error (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/095e6bf8988a03875bc9340b2ab82d5d13c4cb3c", "message": "HIVE-18068: Upgrade to Calcite 1.15 (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/a9e9d60000f542cd2244825752b0a5fa6546770f", "message": "HIVE-14487: Add REBUILD statement for materialized views (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/254e2169ceb5e90808487066a8b012577ea42826", "message": "HIVE-15018: ALTER rewriting flag in materialized view (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/0fe78115a49a9902bf69cc4ac036761cdbcb8860", "message": "HIVE-17717: Enable rule to push post-aggregations into Druid (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/cc62c91996e846bdb2dd128263fada27c5ad84fb", "message": "HIVE-18069: MetaStoreDirectSql to get tables has misplaced comma (Jesus Camacho Rodriguez, reviewed by Aihua Xu) (addendum)"}, {"url": "https://api.github.com/repos/apache/hive/commits/fcb0fe9da9bc9d12fc1c4641e1a4da47ee0fd373", "message": "HIVE-18069: MetaStoreDirectSql to get tables has misplaced comma (Jesus Camacho Rodriguez, reviewed by Aihua Xu)"}, {"url": "https://api.github.com/repos/apache/hive/commits/edde2d916497de9270884dcf663328ad99a918d8", "message": "HIVE-14495: Add SHOW MATERIALIZED VIEWS statement (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/e3168cfc867a534d82e4fd6deef1ba4c4c5ed7c1", "message": "HIVE-18046: Metastore: default IS_REWRITE_ENABLED=false instead of NULL (Jesus Camacho Rodriguez, reviewed by Sergey Shelukhin)"}, {"url": "https://api.github.com/repos/apache/hive/commits/69e38e8b288ad2170b9aa13b2d7c1d7752782500", "message": "HIVE-15436: Enhancing metastore APIs to retrieve only materialized views (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/b62d365102002c9478b0c752c98f72e54ac216f2", "message": "HIVE-18001: InvalidObjectException while creating Primary Key constraint on partition key column (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/175", "title": "HIVE-16575: Support for 'UNIQUE' and 'NOT NULL' constraints", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/107", "title": "HIVE-14474: Create datasource in Druid from Hive", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/245396492", "body": "@ashutoshc , last commit addresses your comments. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/245396492/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/271639597", "body": "@b-slim , sure, that is what I was pointing out. It should be in the method insertCommit in HiveMetaStoreClient.java.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/271639597/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/273527585", "body": "@nishantmonu51 , this was merged a while ago (https://issues.apache.org/jira/browse/HIVE-15277). If you detect issues, I would suggest opening new bugs for them.\r\n\r\nWrt lifecycle, I came across that issue while running tests. Fix is in https://issues.apache.org/jira/browse/HIVE-15614.\r\n\r\nThanks", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/273527585/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/331074627", "body": "@ashutoshc , I have addressed the comments in the first PR (sorry, I did not realized and I have pushed -f instead of creating a new commit on top of the old one).", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/331074627/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/76047716", "body": "You are right Julian, thanks\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/76047716/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/76060988", "body": "Yes, I did, also in https://github.com/jcamachor/calcite/tree/calcite-druid\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/76060988/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77797650", "body": "Exactly, I have added a comment.\n\nIn fact, in our code, as the input stream is read by ObjectMapper, and the feature AUTO_CLOSE_SOURCE is set to true by default, we do not need to do anything, by default it is closed once the full stream has been consumed or if there is an exception, etc.\n\nHowever, I have added calls to close if the Exception is caught at our level.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77797650/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77798446", "body": "Not really, I just did it because I had seen something similar before and I thought it was nice to support both interfaces since logic can be basically reused.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77798446/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77799375", "body": "Yes, probably you are right. I was doubting about storing always the address (whether given by the user at table creation time, or by the default configuration property), or storing it iff user gave it at table creation time. I guess I decided to not store it because changing it would be a huge pain (alter table for all Druid tables), but it is probably better as we will be certain of the source of broker address.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77799375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77802507", "body": "I think it was giving me some problems at runtime because there were some checks done about the path. I am going to double check it.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77802507/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77805024", "body": "I get this error if I just use a dummy path...\n\nDAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1\nFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1473247878706_0001_1_00, diagnostics=[Vertex vertex_1473247878706_0001_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: druid_table_1 initializer failed, vertex=vertex_1473247878706_0001_1_00 [Map 1], java.io.IOException: cannot find dir = / in pathToPartitionInfo: [file:/tmp/jcamachorodriguez/hive/warehouse/druid_table_1]\n\nI will not change it for the moment, I can study it further in a follow-up. In any case, it should be a single call per query.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77805024/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77805461", "body": "Added a comment. Basically, the Row object has different access paths depending on whether the result for the metric is a Float or a Long, thus we need to keep track.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77805461/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77807060", "body": "Good idea! I will also create a follow-up JIRA.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77807060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77826593", "body": "The actual type of the value object returned for each row might defer e.g. a long metric might be represented in some rows as a byte and in some others as an int, depending on the value length.\n\nIn any case, I realized that we can avoid the if-else using 'Number'. I have changed the code accordingly.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77826593/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77828508", "body": "This is a guard for special Druid types e.g. hyperUnique (http://druid.io/docs/0.9.1.1/querying/aggregations.html#hyperunique-aggregator).\n\nWe do not support doing anything special with them in Hive; however, those columns are there, and they can be actually read as normal dimensions e.g. with a select query. Thus, I think currently it is better to print the warning and just read them as String.\n\nI will add a comment to the code.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77828508/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77828827", "body": "Same as above.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77828827/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77829453", "body": "Will add TODO and create a follow-up JIRA.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77829453/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77861585", "body": "@ashutoshc, using array instead of map does not seem straightforward.\n\nThe map, except for GroupBy queries, is taken directly from the result of the Druid query as it is. The record reader just pulls the results from Druid, but it does not have additional information about the schema of the results e.g. order of the columns needed.\n\nI think with some re-engineering of the code, I could pass the column names to the RecordReader. However, that would just mean moving the work from the deserializer to the reader, as we would still need a look up in the Map in order to extract the columns and insert them in the array of the Writable object. Thus, two look-ups are needed: one at the reader, and one at the deserializer.\n\nI think the only gain would be that we do not store column names in every record. However (correct me if I am wrong) the deserializer is called after the record is produced by the reader, thus there is no network transfer? And those objects are already in memory? Hence, I am not sure if we should change it. What do you think?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77861585/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77862567", "body": "I was already doubting... You are absolutely right. I am going to rewrite the parts that are dependent in order to be able to remove this. Maybe I have to bring some constants and create them in Hive, but I still need think it makes sense to bring them instead of bringing the full dependencies.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77862567/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77863412", "body": "These are equivalent to FLOOR(date to TIMEUNIT). E.g. year_granularity(date) is equivalent to FLOOR(date to YEAR).\n\nIn turn, extract is equivalent to UDFs that are still present in Hive. E.g. year(date) is equivalent to EXTRACT(YEAR from date).\n\nBoth collections of UDFs will stay there; we just need to link them to the right syntax in the parser.\n\nAs part of the work to remove the dependency on Druid in ql, I will bring the code of the underlying logic for these UDFs from Druid to Hive (I have checked and they code is quite self-contained).\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77863412/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77864544", "body": "Good catch!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77864544/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77889217", "body": "Till we have a concrete plan on how to support multiple brokers, I have dropped this: now it is mandatory to specify the broker address in the configuration files.\n\nWe can tackle the specification of multiple Druid services/brokers in a follow-up.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77889217/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90846882", "body": "*style*. Please use new lines.", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90846882/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90846989", "body": "*style*. Please use new lines and use natural language for description.", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90846989/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90847152", "body": "*style*. Please use natural language for description.", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90847152/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90847358", "body": "Probably you should add a link to the wiki: https://cwiki.apache.org/confluence/display/Hive/Druid+Integration\r\nBtw, once the patch is pushed, the wiki should be updated with details about the indexing process.", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90847358/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90849874", "body": "Upgrading guava version is a major change since it might break compatibility with other applications built on top of Hive and thus deserves an issue on its own. Could you create a new issue describing the reason why you want to upgrade, strip the code related to this change from this patch, and submit a new patch to that issue? I am sure members from the community will chime in into that discussion. I guess if there is a lot of backlash, we might need to shade it for the moment (I think some other modules have done that in the past).", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90849874/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90850158", "body": "Since we do not use the storage directory provided by Hive anymore, but rather use our own path, we should revert these changes related to native storage in FileSinkOperator and Utilities classes.", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90850158/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90854194", "body": "All these properties should be moved to HiveConf and their descriptions added to that file. Otherwise, they are basically hidden from the final user.", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90854194/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90857432", "body": "It is OK to leave it and prevent the user from specifying 'LOCATION' when they use a CREATE EXTERNAL TABLE statement.", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90857432/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90858139", "body": "No need for the offensive language :)", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90858139/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 1, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90858281", "body": "Btw, I think we should never reach here if datasource name is null, since this code is reached after the initialization of the SerDe?", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90858281/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90858639", "body": "What about datasources discovery, i.e., sources that are already existing in Druid and we want to query them from Hive? Won't these changes break that path?", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90858639/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90858959", "body": "In which cases we would get an Exception here? We should at least print a warning message.", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90858959/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90859113", "body": "*style*. same line", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90859113/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90859287", "body": "No need for the offensive language :)", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90859287/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90860620", "body": "*style*", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90860620/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90861286", "body": "In this method, we should differentiate between dropping tables that 1) were created from Hive, and 2) sources that were in Druid and were discovered from Hive.\r\n\r\nFor 1), we can delete the data when we drop the table from Hive. However, for 2), we should rather remove the table from Hive, but not execute this logic that deletes data from Druid. This should be rather straightforward: since we enforce sources that are discovered from Hive to use the 'EXTERNAL' modifier for the table, we could just check whether the table is 'EXTERNAL' or not.", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90861286/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "xuefuz": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/9816cfb44ad91a8c2a030e540a703983862e4123", "message": "HIVE-17257: Hive should merge empty files (Chao via Xuefu)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ekoifman": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/727581d749306d5df3b6ab8c4adb21ef0fe31506", "message": "HIVE-18460 - Compactor doesn't pass Table properties to the Orc writer (Eugene Koifman, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/dfd7ea368dc8ea2ef071543c3d0cea9e05cd115a", "message": "HIVE-18419 - CliDriver loads different hive-site.xml into HiveConf and MetastoreConf (Eugene Koifman, reviewed by Alan Gates)"}, {"url": "https://api.github.com/repos/apache/hive/commits/fe3190d19fa1324a6835692e71f747e6cb37c7d1", "message": "HIVE-18429 - Compaction should handle a case when it produces no output (Eugene Koifman, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/42a2e11a1518a2797add821e53442c16f9f390b5", "message": "HIVE-18190 - Consider looking at ORC file schema rather than using _metadata_acid file (Eugene Koifman, reviewed by Alan Gates)"}, {"url": "https://api.github.com/repos/apache/hive/commits/da1c21544d7e47f4d78e04449bacc481701d7930", "message": "HIVE-18379 - ALTER TABLE authorization_part SET PROPERTIES (PARTITIONL_LEVEL_PRIVILEGE=TRUE); fails when authorization_part is MicroManaged table. (Steve Yeom, via Eugene Koifman)"}, {"url": "https://api.github.com/repos/apache/hive/commits/5b0d99383d667572d649cafa97dc2b8a41bf8a7f", "message": "HIVE-18294 - add switch to make acid table the default (Eugene Koifman, reviewed by Alan Gates)"}, {"url": "https://api.github.com/repos/apache/hive/commits/e2e1139db00eb05080a51098ef9cf7047986c4b8", "message": "HIVE-18293 - Hive is failing to compact tables contained within a folder that is not owned by identity running HiveMetaStore (Johannes Alberti via Eugene Koifman)"}, {"url": "https://api.github.com/repos/apache/hive/commits/3e930bfa3a870ea85e017b751a07e49dfed32f74", "message": "HIVE-18315 - update tests use non-acid tables (Eugene Koifman, reviewed by Jason Dere)"}, {"url": "https://api.github.com/repos/apache/hive/commits/21e18dea4fb3e6e9f9c70282eabdc04a0be569b2", "message": "HIVE-18317 - Improve error messages in TransactionalValidationListerner (Eugene Koifman, reviewed by Jason Dere)"}, {"url": "https://api.github.com/repos/apache/hive/commits/9dc02efae9b6273309eb287534cfb413c9633141", "message": "HIVE-18316 - HiveEndPoint should only work with full acid tables (Eugene Koifman, reviewed by Sergey Shelukhin)"}, {"url": "https://api.github.com/repos/apache/hive/commits/9efed65a5a139691b7862b2344f01d48ff02ea06", "message": "HIVE-18124 clean up isAcidTable() API vs isInsertOnlyTable() (Eugene Koifman, reviewed by Alan Gates)"}, {"url": "https://api.github.com/repos/apache/hive/commits/ad106f0c4378b10e9c6ef11116e1620bae689ba8", "message": "HIVE-18286 - java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.LongColumnVector (Eugene Koifman, reviewed by Alan Gates)"}, {"url": "https://api.github.com/repos/apache/hive/commits/b7be4acc05fb79033832422100715344e34376a7", "message": "HIVE-17710 LockManager should only lock Managed tables (Eugene Koifman, reviewed by Alan Gates)"}, {"url": "https://api.github.com/repos/apache/hive/commits/b38544f6933efd51f753567585ce3da170d63947", "message": "HIVE-18133 - Parametrize TestTxnNoBuckets wrt Vectorization (Eugene Koifman, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/31c1b719929298dc85510104ccbf5820a28be3ff", "message": "HIVE-18245 - clean up acid_vectorization_original.q (Eugene Koifman, reviewed by Jason Dere)"}, {"url": "https://api.github.com/repos/apache/hive/commits/508d7e6f269398a47147a697aecdbe546425679b", "message": "HIVE-17361 Support LOAD DATA for transactional tables (Eugene Koifman, reviewed by Alan Gates)"}, {"url": "https://api.github.com/repos/apache/hive/commits/624bfa61f584e00b37c83b2167ab941a6e0d5656", "message": "HIVE-17900 analyze stats on columns triggered by Compactor generates malformed SQL with > 1 partition column (Eugene Koifman, reviewed by Sergey Shelukhin)"}, {"url": "https://api.github.com/repos/apache/hive/commits/dbd2d35424da462040f1dd003f890e78db73ba92", "message": "HIVE-17076 : typo in itests/src/test/resources/testconfiguration.properties\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vihangk1": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/ef9d3ee97cee30725381013f4051790c432aa726", "message": "HIVE-18323 : Vectorization: add the support of timestamp in VectorizedPrimitiveColumnReader for parquet (Vihang Karajgaonkar, reviewed by Aihua Xu and Ferdinand Xu)"}, {"url": "https://api.github.com/repos/apache/hive/commits/798a17c679fc04e5a20b15675b6883f52a26ea57", "message": "HIVE-18461 : Fix precommit hive job (Vihang Karajgaonkar)"}, {"url": "https://api.github.com/repos/apache/hive/commits/73ea9f59596dc9e329dce2b07a031916d00a6c77", "message": "HIVE-16708 : Exception while renewing a Delegation Token (Vihang Karajgaonkar, reviewed by Aihua Xu)"}, {"url": "https://api.github.com/repos/apache/hive/commits/9a59592e5e483a9472771ec8cb0104764bdfcc9d", "message": "HIVE-17528 : Add more q-tests for Hive-on-Spark with Parquet vectorized reader (Ferdinand Xu, reviewed by Vihang Karajgaonkar)"}, {"url": "https://api.github.com/repos/apache/hive/commits/542b165c3f042f9d726d2ed831adfc5a89ce369f", "message": "HIVE-18147 : Tests can fail with java.net.BindException: Address already in use (Janaki Lahorani, reviewed by Andrew Sherman and Vihang Karajgaonkar)"}, {"url": "https://api.github.com/repos/apache/hive/commits/987d13099335a79ab2c3a85535d8164caad6bdb3", "message": "HIVE-16756 : Vectorization: LongColModuloLongColumn throws java.lang.ArithmeticException: / by zero (Vihang Karajgaonkar, reviewed by Matt McCline)"}, {"url": "https://api.github.com/repos/apache/hive/commits/4d32b8fc3e12b91434eaa816fef17a09e69cf414", "message": "HIVE-17942 : HiveAlterHandler not using conf from HMS Handler (Janaki Lahorani, reviewed by Vihang Karajgaonkar)"}, {"url": "https://api.github.com/repos/apache/hive/commits/0410bf17a361514f88774cf0545ec07271a26ab8", "message": "HIVE-17961 : NPE during initialization of VectorizedParquetRecordReader when input split is null (Vihang Karajgaonkar, reviewed by Ferdinand Xu)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/294", "title": "HIVE-17580 Remove dependency of get_fields_with_environment_context API to serde", "body": "This is an alternative approach to the solve the dependencies with serdes for get_fields HMS API. The earlier attempt for HIVE-17580 was very disruptive since it attempted to move TypeInfo, and various Type implementations to storage-api and also created another module called serde-api.\r\n\r\nThis patch is a lot more cleaner and less disruptive. Instead of moving TypeInfo, it creates similar classes in standalone-metastore. The PR is broken into multiple commits with descriptive commit messages.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/287", "title": "HIVE-17580 : Remove standalone-metastore's dependency with serdes", "body": "Removing the dependency on serdes for the metastore requires a series of changes. I have created multiple commits which hopefully would be easier to review. Each major commit has a descriptive commit message to give a high level idea of what the change is doing. There are still some bits which need to be completed but it would be good to a review.\r\n\r\nOverview of all the changes done:\r\n1. Creates a new module called serde-api under storage-api like discussed. Although I think we can keep it separate as well.\r\n2. Moved List, Map, Struct, Constant, Primitive, Union ObjectInspectors to serde-api\r\n3. Moved PrimitiveTypeInfo, PrimitiveTypeEntry and TypeInfo to serde-api.\r\n4. Moved TypeInfoParser, TypeInfoFactory to serde-api\r\n5. Added a new class which reading avro storage schema by copying the code from AvroSerde and AvroSerdeUtils. The parsing is done such that String value is first converted into TypeInfos and then into FieldSchemas bypassing the need for ObjectInspectors. In theory we could get rid of TypeInfos as well but that path was getting too difficult with lot of duplicate code between Hive and metastore.\r\n6. Introduces a default storage schema reader. I noticed that most of the serdes use the same logic to parse the metadata information. This code should be refactored to a common place instead of having many copies (one in standalone hms and another set in multiple serdes)\r\n7. Moved HiveChar, HiveVarchar, HiveCharWritable, HiveVarcharWritable to storage-api. I noticed that HiveDecimal is already in storage-api. It probably makes sense to move the other primitive types (timestamp, interval etc)to storage-api as well but it requires storage-api to be upgraded to Java 8.\r\n8. Adds a basic test for the schema reader. I plan to add more tests as this code is reviewed.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/357549214", "body": "> The storage-api as it existed before the standalone-metastore started was really the random collection of classes that ORC needed in order to be separate from Hive. Beyond that there was (as far as I can tell) no rhyme or reason to what put there. (This is why, for example, HiveDecimal is in storage-api but most of the other types are not.)\r\n\r\nThese are exactly my thoughts too. In fact I am thinking of starting a [DISCUSS] thread to really understand what goes into storage-api and what is logical organization of code in terms of types, object inspectors and serdes since I didn't find any particular pattern in it. Since there are some types in storage-api and some are not, I think we should move the type implementations to storage-api. At-least the primitive ones in my opinion. But I could not do it because the timestamp related types need Java 8 and storage-api is still at Java 7. Moving the timestamp types are needed to handle the serdes for timestamp parsing too (I have commented it currently).\r\n\r\n> I realize there are some Hive specific serdes (e.g. ThriftJDBCBinarySerDe) and there are some object inspectors I am not clear whether they are Hive specific or not (e.g. LazyHiveCharObjectInspector). Anything Hive specific shouldn't be moved.\r\n\r\nIn case of ObjectInspectors I tried to move the basic OIs (primitives and complex types only) to the serde-api. The idea was to move only the high level abstractions (interfaces) to the serde-api while keeping the other inspectors which extend and additional hive specific functionality into Hive. In the examples you gave above I would think LazyHiveCharObjectInspector belongs to Hive since it deals with Hive's implementation of the \"char\" type. If you have any OIs which you don't think should belong in serde-api currently or if there are ones which you think should be moved as well, please let me know and I can look at them as well.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/357549214/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/357802872", "body": "Hi Alan,\r\n\r\nI gave it some more thought and I think we need to clearly define what a standalone-metastore really is and hopefully it will help making us these decisions (rather than just to make it work). The code I have here \"works\" (I need to fix the issues with the test before I can claim that which I have atleast locally) but its probably not going to make it any easier for non-hive system to use metastore. If its not easy, I don't think people will adopt it.\r\n\r\nIf you think of metastore as a metadata repository which any application (mostly sql-systems as of now) use, it should be easy for an external application to introduce new types as well. But with the current approach, the TypeInfo which I tried to reuse resides in hive-serde-api, the type implementations are in the exec engine (could imagine something like Hive, Impala etc). Metastore should ideally not need to know anything about Type implementation (usage of PrimitiveEntry in metastore violates this principle).Ideally it should just need some metadata of the type itself like what is its string representation, what is its category (primitive, complex), is it parameterized (eg. varchar has length attribute), how do we validate the parameters (max values of precision, scale, length etc). But because PrimitiveEntry objects is one container which maps the String value, writable classname, type implementation classname together, it not really very useful for metastore. PrimitiveEntry should just be a util class which helps Hive to map one representation of type to its another representation (eg, maps its string value to type implementation classname). I think the TypeInfo or something equivalent should belong to metastore. In this change I tried to use the same TypeInfo by moving it to serde-api but may be we should move TypeInfo to metastore or create a new class called MetastoreTypeInfo which serves a similar purpose. Do you have thoughts on this?\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/357802872/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "jdere": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/01816fca2c2620f4f57a2b2a5c6ea404e1f0038f", "message": "HIVE-18430: Add new determinism category for runtime constants (current_date, current_timestamp) (Jason Dere, reviewed by Jesus Camacho Rodriguez)"}, {"url": "https://api.github.com/repos/apache/hive/commits/790976907070a2a737186543d81c71eb542a5337", "message": "HIVE-18385: mergejoin fails with java.lang.IllegalStateException (Jason Dere, reviewed by Deepak Jaiswal)"}, {"url": "https://api.github.com/repos/apache/hive/commits/60b850486a074dc556e9c06ecf09006c820ed929", "message": "HIVE-18306: Fix spark smb tests (Deepak Jaiswal, reviewed by Jason Dere)"}, {"url": "https://api.github.com/repos/apache/hive/commits/a96564cbc787ed8665c1bb6b3c3a0e9d8440b926", "message": "HIVE-18271: Druid Insert into fails with exception when committing files (Jason Dere, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/38405c1458cf2c6ee508fedf38581df1fc8c1f61", "message": "HIVE-18208: SMB Join : Fix the unit tests to run SMB Joins. (Deepak Jaiswal, reviewed by Jason Dere)"}, {"url": "https://api.github.com/repos/apache/hive/commits/5bbd864338244f2bf2eb58f074cb1f8b33357960", "message": "HIVE-14069 : update curator version to 2.12.0 (Jason Dere via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/8d39a0887618a3a4cb57b878095041a5db013177", "message": "HIVE-18151: LLAP external client: Better error message propagation during submission failures (Jason Dere, reviewed by Sergey Shelukhin)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vaibhavgumashta": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/456a65180dcb84f69f26b4c9b9265165ad16dfe4", "message": "HIVE-17495: CachedStore: prewarm improvement (avoid multiple sql calls to read partition column stats), refactoring and caching some aggregate stats (Vaibhav Gumashta reviewed by Daniel Dai)"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/106633313", "body": "Looks good, +1.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/106633313/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/170460083", "body": "+1. Looks good.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/170460083/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "winningsix": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/17abdb211c1b2b749fc7d8265d31e6c5987cea4b", "message": "HIVE-18411: Fix ArrayIndexOutOfBoundsException for VectorizedListColumnReader (Colin Ma, reviewed by Ferdinand Xu)"}, {"url": "https://api.github.com/repos/apache/hive/commits/a6fab1438865a113e98be1bb97fa0549d160a61d", "message": "HIVE-18159: Vectorization: Support Map type in MapWork (Colin Ma, reviewed by Ferdinand Xu)"}, {"url": "https://api.github.com/repos/apache/hive/commits/11227ebab390df10970fb8ef61f3e24421d6c66e", "message": "HIVE-18209: Fix API call in VectorizedListColumnReader to get value from BytesColumnVector (Colin Ma, reviewed by Ferdinand Xu)"}, {"url": "https://api.github.com/repos/apache/hive/commits/7acc4ce1bbae060d890494c1499938c1eda5f3b6", "message": "HIVE-18211: Support to read multiple level definition for Map type in Parquet file (Colin Ma, reviewed by Ferdinand Xu)"}, {"url": "https://api.github.com/repos/apache/hive/commits/3908209716769ee10c9e090b0d95747f9a2b07af", "message": "HIVE-18150: Upgrade Spark Version to 2.2.0 (Liyun Zhang, reviewed by Sahil Takiar)"}, {"url": "https://api.github.com/repos/apache/hive/commits/2efb7d38af639ee77f817f2ebf772314309174be", "message": "HIVE-17972: Implement Parquet vectorization reader for Map type (Colin Ma, reviewed by Ferdinand Xu)"}, {"url": "https://api.github.com/repos/apache/hive/commits/b8aa16ff6c2ec0185cd953a7854d6abda2306df7", "message": "HIVE-18043: Vectorization: Support List type in MapWork (Colin Ma, reviewed by Ferdinand Xu)"}, {"url": "https://api.github.com/repos/apache/hive/commits/39d46e8af5a3794f7395060b890f94ddc84516e7", "message": "HIVE-17931: Implement Parquet vectorization reader for Array type (Colin Ma, reviewed by Vihang and Ferdinand)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/251", "title": "HIVE-14836: Test the predicate pushing down support for Parquet vecto\u2026", "body": "\u2026rization read path\r\n\r\nAdd more unit test for Predicate pushing down for Parquet Vectorization.", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/165320380", "body": "Thanks @thejasmn for working on it. Just some minor issues. @sundapeng Can you take a look at this PR? Thank you!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/165320380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/250497759", "body": "@wagnermarkd @sunchao Thank you for your reviews. \nI'd like to add an option to disable nested column pruning by default because we may not support all data types in a single PR. To limit the impacts on other data format, let's make it configurable. And we may remove it in the future. Any thoughts about it?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/250497759/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/250629709", "body": "> It's up to the SerDe whether or not to leverage the information provided, right?\n\nCurrent solution did some changes on NeededCols. It will filter them if one of its subtype is included in groupPath.\n\n> Defaulting to off means that it will get less test coverage and there's more risk that it could be inadvertently reverted.\n\nOK, I will try to update the code working in this way.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/250629709/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/253379214", "body": "Hi @sahilTakiar , I think we may not be able to use the code in Spark directly since they're lazy materialized and we just keep the hierarchy: record -> column -> page.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/253379214/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/260225989", "body": "Thanks @dongjinleekr  for the contribution. Can you help create a ticket at [Jira page](https://issues.apache.org/jira/browse/HIVE/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel)?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/260225989/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/260227899", "body": "Thank you! Can you please update the title to \"HIVE-15191: Fix typo in conf/hive-env.sh.template\"? And attach file following the rule in [WIKI](https://cwiki.apache.org/confluence/display/Hive/HowToContribute#HowToContribute-CreatingaPatch) to trigger precommit test.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/260227899/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/81073893", "body": "Do you have any suggested name for this? I didn't come up with a better name yet. :)\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81073893/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81086356", "body": "This method is recursively checking whether a type is a subtype for the group type.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81086356/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81153175", "body": "@wagnermarkd  How about the name nestedColumnPath?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81153175/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81261654", "body": "I think the following case is valid according to the Hive wiki https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes-ComplexTypes\n\ngroupType: `s1: struct<s2: struct<f1:int, f2:string>>`\nsubtype: `f1:int`\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81261654/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82799689", "body": "The qtest doesn't ensure whether pruning take effects but to test whether it doesn't break functionality. UTs above are used to test whether the right configuration is set and we can use the configuration to do the pruning.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82799689/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84429652", "body": "This method is used for all select operator. We can add support for other operators as follow up.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84429652/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84430271", "body": "To avoid adding empty set.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84430271/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84433169", "body": "Can we do that? It shows error in my Intellij, I am using JDK7 as language level.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84433169/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84438394", "body": "Hmm, let us think of a way to handle this. Maybe some configurations to store the previous index.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84438394/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85050146", "body": "Can we keep it as final?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85050146/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85125289", "body": "hint: better originalTypeInfo?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85125289/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85126145", "body": "hint: better originalTypeInfo?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85126145/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85126333", "body": "row -> column\nnested struct fields -> nested column fields\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85126333/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85126982", "body": "hint: better isNested?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85126982/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85127607", "body": "No need to return index, we can simply return adjustedIndex. It will be the same as index when isRoot is true.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85127607/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85127931", "body": "I think we can change `adjustedIndex` to `readIndex` and `index` to `writeIndex`. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85127931/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85130461", "body": "2 space indent\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85130461/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85131626", "body": "Better PrunedStructTypeInfo \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85131626/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85132184", "body": "`String fieldName  = StringUtils.split(path, '.')[0];`\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85132184/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85135307", "body": "Is this method be used as follows?\n\n```\npath: a.b.c\ntype: a:struct<b:struct<c:string,d:double>,e:string>\nreturns: a:struct<b:struct<c:string>>\n```\n\nIf so, it may be problematic since we don't set the `select` field recursively for each leaf in L197. If I understand right, please add a test case to cover it as well.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85135307/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85258219", "body": "Can `prunedPaths` contain duplication?  If so, we can avoid prune the same path. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85258219/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85258580", "body": "Oh , it's my fault, I see `pruneFromSinglePath` was recursively revoked in L215. So it should work.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85258580/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85260602", "body": "In L81, the adjusted index of a column is the same as original index when no pruning happened. Will index be changed in other places?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85260602/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85472233", "body": "All `StructField`s have two indexes: `index` for original index and `ajustedIndex` for index after pruned. For those columns doesn't involved in pruned, original index is the same as adjusted index. How about refactoring index-> writeIndex and adjustedIndex -> writeIndex? We can always return `writeIndex` getting rid of the `isRoot`. \nAny thoughts about it?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85472233/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85656716", "body": "Yes, I mean adjustedIndex -> readIndex.\n\n> 1. column is not pruned: use original index\n> 2. column is pruned but is we are looking at the root level type: use original index\n> 3. column is pruned and we are looking at the nested level type: use adjusted index\n\nOK, let's keep it as `adjustedIndex` and use `isRoot` for the tag.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85656716/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82513942", "body": "isRepeating is false by default. We set it to true at initialization and if one value is not the same as others, it will change to false.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82513942/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82513997", "body": "This is required because when you fetch a batch of rows, it may reach the end of the current page and we need to load more.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82513997/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85684286", "body": "Do you mean something like keeping a map whose key is the value of `bytes` in `DecimalColumnVector` and the value is the object `HiveDecimalWritable`?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85684286/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86275448", "body": "It could be greater than maxDefLevel according to the definition of null in this [document](https://github.com/Parquet/parquet-mr/wiki/The-striping-and-assembly-algorithms-from-the-Dremel-paper). \n\n> When a field is not defined, the definition level will be smaller than the definition level of this field.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86275448/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86281096", "body": "Updated it by changing unite tests.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86281096/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86281227", "body": "OK, let us investigate it separately.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86281227/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86778367", "body": "Thank you for pointing this out. I am considering addressing this in a separate ticket. I need to refactor the test and current decimal support. If you're OK, I will file a new ticket to add decimal related qtest and unit tests in that Jira. But before that, I need to think about how to clean up the unit test a little bit.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86778367/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86778466", "body": "Please see my previous comment.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86778466/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86814118", "body": "The latest qtest failed due to this exception. I will update this qtest to pass the precommit.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86814118/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86991020", "body": "I  debug the Qtest and found it's using dictionary encoding because the number of row is very limited. Then the test is passed since it's supported already for dictionary code path.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86991020/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87417394", "body": "Good catch. For complex types related queries, they're not vectorized executed since no filter, no aggregations and just table scan . And explain to qtest to ensure vectorization is enabled.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87417394/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87539505", "body": "> can we add end-to-end test for the non-dictionary path?\n\nAdd qtest `parquet_types_non_dictionary_encoding_vectorization.q` to cover it.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87539505/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87746776", "body": "Thank you for pointing this out, let's remove it. And add it back if needed in the future.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87746776/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87823307", "body": "I am afraid not. `setRef` is used to construct a column vector from another vector. What's more, binary column vector is consisting of three parts: starts, lengths, bytes. We will retrieve binary array at index by getting the sub array beginning at the index starts[index] to starts[index]+lengths[index] from the bytes array.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87823307/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87823380", "body": "Please see my previous comments.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87823380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87010960", "body": "hint: can we merge all these three \"if\" clauses?  The same goes to `equals` method. You can break lines before \"||\" for better view.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87010960/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/88614715", "body": "hint: list -> set\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/88614715/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/88615204", "body": "b -> a\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/88615204/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/88615755", "body": "minor: fromString -> fromPath\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/88615755/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/88616003", "body": "extra space.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/88616003/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/88616679", "body": "Hint: it's most readable is to simply use google Guava after importing `com.google.common.collect.Sets`:\n\n```\nSet<String> StringSet = Sets.newHashSet(\"structCol.a\");\n```\n\nThe same as other test cases.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/88616679/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/88617484", "body": "You may need to update the JavaDoc.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/88617484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89025239", "body": "minor: hconf -> conf", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89025239/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89027878", "body": "hint: can we add JavaDoc to describe the key is lower-cased as a result?", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89027878/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89029145", "body": "We may just call ```lookupColumn``` once since we need to use the return value just below.", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89029145/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89921155", "body": "OK, let's keep it.", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89921155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90164131", "body": "It happens only when schema is corrupted. Addressing this by adding a check before this if block.", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90164131/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "deepeshk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/7942bc6c9a38e71ff3f8936508cc773a464b0d87", "message": "HIVE-18465: Hive metastore schema initialization failing on postgres (Deepesh Khandelwal, reviewed by Jesus Camacho Rodriguez)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alanfgates": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/d9801d9c6c406d5871147b80bc2e0359c3dbd085", "message": "HIVE-17982 Move metastore specific itests.  This closes #279.  (Alan Gates, reviewed by Peter Vary)"}, {"url": "https://api.github.com/repos/apache/hive/commits/12a33fd0d1f82422048af5a389671812bdf03c93", "message": "HIVE-17981 Create a set of builders for Thrift classes.  This closes #274.  (Alan Gates, reviewed by Peter Vary)"}, {"url": "https://api.github.com/repos/apache/hive/commits/d79c45951d0382dc42d5ce0c9474d994948522b7", "message": "HIVE-17980 Move HiveMetaStoreClient plus a few remaining classes.  This closes #272  (Alan Gates, reviewed by Daniel Dai)"}, {"url": "https://api.github.com/repos/apache/hive/commits/8fcc7f324c88eb33987e5c2c9de61d719167bbea", "message": "HIVE-17967 Move HiveMetaStore class.  This closes #270 (Alan Gates, reviewed by Thejas Nair)."}, {"url": "https://api.github.com/repos/apache/hive/commits/c1d9b2021dda554d85c9fc3417f3187e26dd0066", "message": "HIVE-18084 Upgrade checkstyle version to support lambdas (Adam Szita via Alan Gates)"}, {"url": "https://api.github.com/repos/apache/hive/commits/6558a68ec41b5dc6421a4e727b23bdd4db3a4c4c", "message": "HIVE-18085 Run checkstyle on storage-api module with proper configuration.  (Adam Szita via Alan Gates)"}, {"url": "https://api.github.com/repos/apache/hive/commits/67888cfe1fc70d8688b575ea4505c41f0f2caaa7", "message": "HIVE-17995 Run checkstyle on standalone-metastore module with proper configuration (Adam Szita via Alan Gates)"}, {"url": "https://api.github.com/repos/apache/hive/commits/d85d401aae1bcef38a3ef30677d8334235672cf1", "message": "HIVE-17997 Add rat plugin and configuration to standalone metastore pom (Adam Szita via Alan Gates)."}, {"url": "https://api.github.com/repos/apache/hive/commits/801198434dbad8894b08b4236fc131ce03698813", "message": "HIVE-17996 Fix ASF headers.  (Adam Szita via Alan Gates.)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/291", "title": "HIVE-17983 Make the standalone metastore generate tarballs etc.", "body": "See JIRA for full comments.", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/240831406", "body": "+1, changes look good to me.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/240831406/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/330756639", "body": "I should also comment about the change to AcidOpenTxnsCounterService.  Rather than have each thread type have its own ThreadPool my plan is to use the general ThreadPool.  I created a new RunnableConfigurable interface that all such threads should implement.  This simplifies the threads a bit.  ", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/330756639/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/332305708", "body": "As part of the latest change I moved TxnDbUtil to use a passed in Configuration file rather than generate a new one on its own every time.  This is more efficient and avoids errors where users have specifically set values in the conf file that they want reflected in the TxnStore interactions.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/332305708/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/339044433", "body": "As noted in the code comments, I laid out the motivation for the breaking changes at https://issues.apache.org/jira/browse/HIVE-17812?focusedCommentId=16204787&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16204787  \r\n\r\nTo re-emphasize, with or without the changes to LIstenerEvent, this will not be backwards compatible for server plugins because of the change from HiveConf to Configuration.  We can explore a shim for this that still lives in the metastore module.  \r\n\r\nAs for backporting to 2, I don't see how that would work.  By not changing the thrift API I believe we'll be able to guarantee that 2 clients work with the standalone server.  And it may be possible to write shims that allow 2 plugins to run against the standalone server.  But actually backporting to the 2 line would be too disruptive.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/339044433/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "sahilTakiar": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/87860fbca42b2477f504c8b1cefd43c865a2629c", "message": "HIVE-18214: Flaky test: TestSparkClient (Sahil Takiar, reviewed by Peter Vary)"}, {"url": "https://api.github.com/repos/apache/hive/commits/d0156565adec240735850bedc7729fa43cd05460", "message": "HIVE-17929: Use sessionId for HoS Remote Driver Client id (Sahil Takiar, reviewed by Rui Li)"}, {"url": "https://api.github.com/repos/apache/hive/commits/ed99182a274e7f7075c8746d7a85b60b5199339d", "message": "HIVE-18255: spark-client jar should be prefixed with hive- (Sahil Takiar, reviewed by Rui Li)"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/253270024", "body": "If a lot of these class are borrowed from Spark (https://github.com/apache/spark/tree/master/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet), then could we simply just use the Spark classes as they are to implement the `VectorizedParquetRecordReader`. It would be better than forking the code because any bug fixes Spark makes to this code can automatically be picked up in Hive. If we need to make any modifications to the code for Hive, we can just extend the existing classes.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/253270024/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "k0b3rIT": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/13c1bf9e903c502dbfe3f2fb10959ac1a64bc0ad", "message": "HIVE-18161: Remove hive.stats.atomic (Bertalan Kondrat via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/25dc2c4ac85c57f2717353a9a536f6db01b1c73c", "message": "HIVE-18185: update insert_values_orig_table_use_metadata.q.out (Bertalan Kondrat via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/63f2ec1902016537fcf83262bfcda1d604b236d8", "message": "HIVE-18036: Stats: Remove usage of clone() methods (Bertalan Kondrat via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/c673041127468e7a0e3a3442b5ee27f624542c5d", "message": "HIVE-18187: Add jamon generated-sources as source folder (Bertalan Kondrat via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/266d4bb9696db176e7b9d0d54e25b5e64ce86b1a", "message": "HIVE-18186: Fix wrong assertion in TestHiveMetaStoreAlterColumnPar test (Bertalan Kondrat reviewed by Vihang Karajgaonkar via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "anishek": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/22df53b6c223f03edee1a8c271a77e91d66bb2a1", "message": "HIVE-18341: Add repl load support for adding \"raw\" namespace for TDE with same encryption keys (Anishek Agarwal, reviewed Daniel Dai)"}, {"url": "https://api.github.com/repos/apache/hive/commits/fd4e222d54aba6298acd62b57406c83a3a14ba43", "message": "HIVE-18352: introduce a METADATAONLY option while doing REPL DUMP to allow integrations of other tools (Anishek Agarwal reviewed by Daniel Dai)"}, {"url": "https://api.github.com/repos/apache/hive/commits/bc2848a2605c53e47a84dfc94942f796227f7714", "message": "HIVE-17829: ArrayIndexOutOfBoundsException - HBASE-backed tables with Avro schema in Hive2 (Anishek Agarwal reviewed by Thejas Nair)"}, {"url": "https://api.github.com/repos/apache/hive/commits/67499f41a02c2f739a3a005a102c0ceda8d2eeba", "message": "HIVE-18090: acid heartbeat fails when metastore is connected via hadoop credential (Anishek Agarwal, reviewed by Eugene Koifman)"}, {"url": "https://api.github.com/repos/apache/hive/commits/25a6f4cf30345f47deeab84fe657f709c8a6168a", "message": "HIVE-17615: Task.executeTask has to be thread safe for parallel execution (Anishek Agarwal reviewed by Daniel Dai)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/289", "title": "HIVE-18341: Add repl load support for adding \"raw\" namespace for TDE with same encryption keys", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/286", "title": "HIVE-18352: introduce a METADATAONLY option while doing REPL DUMP to allow integrations of other tools", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/283", "title": "HIVE-17829: ArrayIndexOutOfBoundsException - HBASE-backed tables with Avro schema in Hive2", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/263", "title": "HIVE-17830: dbnotification fails to work with rdbms other than postgres", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/262", "title": "HIVE-17825: Socket not closed when trying to read files to copy over in replication from metadata", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/259", "title": "HIVE-17615 : Task.executeTask has to be thread safe for parallel execution", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/246", "title": "HIVE-17426: Execution framework in hive to run tasks in parallel other than MR Tasks", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/240", "title": "HIVE-17410 : repl load task during subsequent DAG generation does notstart from the last partition processed", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/237", "title": "HIVE-16886: HMS log notifications may have duplicated event IDs if multiple HMS are running concurrently", "body": "\u2026ltiple HMS are running concurrently", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/270336219", "body": "Incorrect BUG number", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/270336219/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/270860096", "body": "closed this as i had created wrong branch to track this change going to open another pull request for this from correct branch.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/270860096/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "daijyc": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/970d8c9680d89708c4007de214fdc72b77deb7da", "message": "HIVE-18299: DbNotificationListener fail on mysql with \"select for update\" (Daniel Dai, reviewed by Anishek Agarwal)"}, {"url": "https://api.github.com/repos/apache/hive/commits/c0734ac91bdd3e81b17d93fcfeddd4503430eea8", "message": "HIVE-18298: Fix TestReplicationScenarios.testConstraints (Daniel Dai, reviewed by Sankar Hariappan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/f1698b62c9ed9286caad53de9c1cf669e2a38864", "message": "HIVE-18056: CachedStore: Have a whitelist/blacklist config to allow selective caching of tables/partitions and allow read while prewarming (Vaibhav Gumashta, Daniel Dai, reviewed by Thejas Nair, Sergey Shelukhin)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/236", "title": "HIVE-17366: Constraint replication in bootstrap", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/219", "title": "HIVE-15705: Event replication for constraints", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/173", "title": "HIVE-16520: Cache hive metadata in metastore", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lirui-apache": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/35ea45a1e1a7b700df2984a234f444c4a65ccb0d", "message": "HIVE-18148: NPE in SparkDynamicPartitionPruningResolver (Rui reviewed by Liyun Zhang and Sahil Takiar)"}, {"url": "https://api.github.com/repos/apache/hive/commits/00212e0301868c9cd37a1d469dcf6c578344dd46", "message": "HIVE-18041: Add SORT_QUERY_RESULTS to subquery_multi (Rui Li via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/8ced3bc7c595be7700088a4487363b6151e6c3d2", "message": "HIVE-18111: Fix temp path for Spark DPP sink (Rui reviewed by Sahil)"}, {"url": "https://api.github.com/repos/apache/hive/commits/966d2b303b473e46a2877c13a58497b98c0896d5", "message": "HIVE-17964: HoS: some spark configs doesn't require re-creating a session (Rui reviewed by Xuefu)"}, {"url": "https://api.github.com/repos/apache/hive/commits/3bfcfdde0c0be2aab1afdf5b1bc71fdcc9e77360", "message": "HIVE-17976: HoS: don't set output collector if there's no data to process (Rui reviewed by Xuefu)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "PrabhuJoseph": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/d69601c24c04e6a065c2a0bcbe10935877aeb9c8", "message": "HIVE-18353 : CompactorMR should call jobclient.close() to trigger cleanup (Prabhu Joseph via Thejas Nair)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dvoros": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/30bc6cf8bec08c139b920a63a78d03a7fcc30809", "message": "HIVE-18414 : upgrade to tez-0.9.1 (Daniel Voros via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/338411718ed4a73c1182d5e717230bfc33855d12", "message": "HIVE-17988: Replace patch utility usage with git apply in ptest (Daniel Voros reviewed by Sergio Pe\u00f1a via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/303676755", "body": "Hey there! I've run the tests from HIVE-15834 on this and got two failures. Both of these come from the different handling of adding Collections to a JSONObject. With the original library we got:\r\n\r\n```\r\n{\r\n  \"input_partitions\": [\r\n    {\r\n      \"partitionName\": \"partition-name-mock\"\r\n    }\r\n  ],\r\n  \"input_tables\": [\r\n    {\r\n      \"tablename\": \"table-name-mock\",\r\n      \"tabletype\": \"EXTERNAL_TABLE\"\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nAnd with the new one:\r\n\r\n```\r\n{\r\n  \"input_tables\": \"[{tablename=table-name-mock, tabletype=EXTERNAL_TABLE}]\",\r\n  \"input_partitions\": \"[{partitionName=partition-name-mock}]\"\r\n}\r\n```\r\n\r\nI think the solution is to create JSONArrays and add those instead of creating Lists of Maps that we don't use for anything else. See https://github.com/omalley/hive/pull/1", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/303676755/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "evilsunny": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/77360d6cfbcef2b905bf6500374f2f23d5bb2803", "message": "HIVE-13000 : Hive returns useless parsing error (Alina Abramova via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "djhwx": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/841081a9c32ad5e0a4db41ec72c9668194a09f3f", "message": "HIVE-18335: Vectorization : Check bounds of array before the allocation in VectorMapJoinFastBytesHashTable (Deepak Jaiswal, reviewed by Matt McCline)"}, {"url": "https://api.github.com/repos/apache/hive/commits/525cfa7be544b948229af4c2c0d0b8d0c021fbba", "message": "HIVE-18157: Vectorization : Insert in bucketed table is broken with vectorization (Deepak Jaiswal, reviewed by Matt McCline"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/20446807", "body": "The build is failing in tests. Can you please revert this back to unblock others and check this in which fixed?", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/20446807/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "miklosgergely": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/eca6b89458be22abae2529ae549894cb1ba75a6d", "message": "HIVE-18365 : netty-all jar is not present in the llap tarball (Miklos Gergely via Gopal V)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "b-slim": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/035eca39fc0bd7aa0d9c1809a26e000ac52978d0", "message": "HIVE-18331 : Renew the Kerberos ticket used by Druid Query runner (Slim Bouguerra via Sergey Shelukhin)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/2a5ba5c9c6db65018355bcfb301e422e7c5d275f", "message": "HIVE-15393 : Update Guava version (Slim Bouguerra via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/1eef7e54daf0a8433ebcf25842d510ad7a2dd091", "message": "HIVE-18254 : Use proper AVG Calcite primitive instead of Other_FUNCTION (Slim Bougerra via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/1b3711b33cdce33688eabcc715880d2134242692", "message": "HIVE-18196 : Druid Mini Cluster to run Qtests integrations tests. (Slim Bouguerra via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/249", "title": "[HIVE-17523] Fix insert into bug", "body": "https://issues.apache.org/jira/browse/HIVE-17523", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/139", "title": "[HIVE-15727] Adding pre insert work and block insert intot statement", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/271445158", "body": "@jcamachor i have addressed all the comments please check this out.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/271445158/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/271639048", "body": "@jcamachor I AM not against adding it, but what i am saying if we do add it to the interface will need to add the call for it in hive core it self otherwise it will not make sense right ? ", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/271639048/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/275201083", "body": "Merged", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/275201083/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/275201353", "body": "merged", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/275201353/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/275201403", "body": "merged", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/275201403/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/275201465", "body": "merged", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/275201465/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/278372359", "body": "@jcamachor most of the change of this PR is re-writing the implementation from `long` to `DateTime`, i might be missing something but not sure what is exactly fixing. It will be nice if you layout a test scenario that breaks the old code and get fixed by this patch.  ", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/278372359/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "sankarh": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/b4b06ac1568490200f817cd7f1855b9a81df6bf9", "message": "HIVE-17897: Repl load in bootstrap phase fails when partitions have whitespace (Thejas Nair, reviewed by Sankar Hariappan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/ad5bcb150619764a0c9fccc42f056321ed18cca6", "message": "HIVE-18031: Support replication for Alter Database operation (Sankar Hariappan, reviewed by Anishek Agarwal)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/290", "title": "HIVE-18192: Introduce WriteID per table rather than using global transaction ID", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/280", "title": "HIVE-18031: Support replication for Alter Database operation", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/279203725", "body": "@vaibhavgumashta committed this pull request by applying the patch submitted in the JIRA ticket. So closing the pull request here.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/279203725/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/279243746", "body": "@thejasmn \r\nI made the changes and the code is pushed to my local branch and available in this pull request. Please review the same.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/279243746/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/279663258", "body": "Already committed to trunk.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/279663258/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/281260997", "body": "Already committed to master.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/281260997/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/297983596", "body": "Already committed to master.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/297983596/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/298831328", "body": "Already committed to master!", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/298831328/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/298831486", "body": "Already committed to master!", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/298831486/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/298831556", "body": "Already committed to master!", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/298831556/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/301243356", "body": "Already committed to master.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/301243356/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/301243371", "body": "Already committed to master.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/301243371/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/302136700", "body": "Already committed to master.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/302136700/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/303445276", "body": "Already merged to master.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/303445276/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/304355502", "body": "Already committed to master.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/304355502/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/306735954", "body": "Already merged to master.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/306735954/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/307298502", "body": "Already committed to master.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/307298502/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/307307746", "body": "Already committed to master.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/307307746/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/308995070", "body": "I thought additionally during repl load , we were going to update the parent object with the event id every time we apply a event\r\n\r\nFor this comment, we will do it as part of another JIRA BUG-81396 (No apache jira yet) which focus on failure case handling. Updating object state after event makes sense for failure handling only. So, appropriate to handle there.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/308995070/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/311944137", "body": "Already committed to master.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/311944137/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/315170422", "body": "Already committed to master", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/315170422/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/318629652", "body": "Already committed to master.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/318629652/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/318629697", "body": "Already committed to master.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/318629697/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/321503989", "body": "Already committed to master", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/321503989/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/322695183", "body": "Already committed to master.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/322695183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/322695508", "body": "Already committed to master.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/322695508/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/325885445", "body": "Already committed to master", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/325885445/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/326382103", "body": "Already committed to master", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/326382103/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/326474239", "body": "Already committed to master.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/326474239/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/327980819", "body": "Already committed to master", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/327980819/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/329193462", "body": "Already pushed to master.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/329193462/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "mmccline": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/0ecf649119e4979f1162ffe5dd27ed9444d3a337", "message": "HIVE-17994: Vectorization: Serialization bottlenecked on irrelevant hashmap lookup (Matt McCline, reviewed by Teddy Choi)"}, {"url": "https://api.github.com/repos/apache/hive/commits/f52e8b4ba38f2a1141650d99efb12c923cee7cd0", "message": "HIVE-18258: Vectorization: Reduce-Side GROUP BY MERGEPARTIAL with duplicate columns is broken (Matt McCline, reviewed by Teddy Choi)"}, {"url": "https://api.github.com/repos/apache/hive/commits/1320d2b31a358f319d8eeb4aa4f781d47bb3f4b4", "message": "HIVE-18191: Vectorization: Add validation of TableScanOperator (gather statistics) back (Matt McCline, reviewed by Teddy Choi)"}, {"url": "https://api.github.com/repos/apache/hive/commits/65cd866eb72b14d68e19178b20a5d8d3410c602a", "message": "HIVE-18146: Vectorization: VectorMapJoinOperator Decimal64ColumnVector key/value cast bug (Matt McCline, reviewed by Teddy Choi)"}, {"url": "https://api.github.com/repos/apache/hive/commits/c58435bf8bdd29fc3544bae27d7965d8665dff38", "message": "HIVE-18077: Vectorization: Add string conversion case for UDFToDouble (Matt McCline, reviewed by Teddy Choi)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bartash": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/b7ac74a730b13e167631a1d82f27eb4003bcbb61", "message": "HIVE-18310: Test vector_reduce_groupby_duplicate_cols.q is misspelled in testconfiguration.properties (Andrew Sherman via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/646ccce8ea3e8c944be164f86dbd5d3428bdbc44", "message": "HIVE-18054: Make Lineage work with concurrent queries on a Session (Andrew Sherman, reviewed by Sahil Takiar)"}, {"url": "https://api.github.com/repos/apache/hive/commits/33d527f257137c421e8d362c5cf15d8e8fe26599", "message": "HIVE-18127: Do not strip '--' comments from shell commands issued from CliDriver (Andrew Sherman, reviewed by Sahil Takiar)"}, {"url": "https://api.github.com/repos/apache/hive/commits/d9924ab3e285536f7e2cc15ecbea36a78c59c66d", "message": "HIVE-18136: WorkloadManagerMxBean is missing the Apache license header (Andrew Sherman via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sunchao": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/14df3b0212306a0a2d60176c26f710378037a5a1", "message": "HIVE-18283: Better error message and error code for HoS exceptions (Chao Sun, reviewed by Xuefu Zhang and Andrew Sherman)"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/250557209", "body": "@winningsix Yes, I think adding a configuration sounds good to me.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/250557209/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/263684033", "body": "The reason for making column/field name lower case is because Hive schema's are case-insensitive, so even if a nested col path is `S.X` it could corresponds to field `x` of struct `s`. To handle this, we convert all paths to lower case and then compare with schemas.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/263684033/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/80996412", "body": "Maybe some comments on what this method is doing? also perhaps more intuitive to call it 'isSubType'.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80996412/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80996890", "body": "Maybe this case can be combined with the first case? also add space after 'if'.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80996890/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80996934", "body": "space after '}'\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80996934/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80997926", "body": "Do we need to handle the case when 'subtype' is not a direct children of 'groupType'?\n(e.g., groupType = s1.s2.s3 while subtype = s3)\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80997926/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81036421", "body": "This change (and a few below) seems unrelated - maybe should be skipped from the commit.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81036421/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81038966", "body": "better call it 'getPrunedNestedColumns'.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81038966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81042855", "body": "Maybe some comments on what this field is for? (what's the key, what's the value, etc).\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81042855/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81042884", "body": "Some comments on this method would be helpful. Especially, please explain what is a group path.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81042884/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81042972", "body": "Seems 'groupPathsList' is always the last one, is this intentional?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81042972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81043040", "body": "Update the comments.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81043040/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81043378", "body": "Remove the comments.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81043378/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81047847", "body": "+1 on unit test.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81047847/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81048033", "body": "will 't' always be group type here\uff1f\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81048033/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81048825", "body": "why only add when 'res' is non-empty?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81048825/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81211118", "body": "Yeah I know that. I'm thinking about this case:\ngroupType: `struct<struct<f1:int, f2:string>>`\nsubtype: `f1:int`\nThis seems not be able to handle that correctly.\nBut first, is this case possible?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81211118/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82736796", "body": "attribution -> attribute?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82736796/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82736860", "body": "colIndex -> colIndexes?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82736860/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82742413", "body": "Remove star import.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82742413/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82743129", "body": "this comment is not consistent with the code.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82743129/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82744381", "body": "How do you know whether the nested columns are pruned from this test?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82744381/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82745776", "body": "Does the current code handle merging fields like `select a.b.c, a.b`? if so, maybe add a test case for that.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82745776/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82915454", "body": "Hmm.. this is not what I mean. I was thinking the case: `Arrays.asList(\"structCol.subStructCol\", \"structCol.subStructCol.b\")`. Does this make sense?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/82915454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83161795", "body": "better to use @BeforeClass instead of the static block\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83161795/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83171693", "body": "attribution -> attribute?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83171693/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83172809", "body": "(Just to my understanding ;))\nWhen this case will happen? can you give a sample query?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83172809/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83173476", "body": "Maybe we should first compare type & type name and then do to the case analysis?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83173476/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83173629", "body": "better add the type name to the exception message\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83173629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83174395", "body": "(Also for my understanding) when this case will happen? can you give an example?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83174395/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83340138", "body": "Perhaps this function is not necessary? we can just add the logic in line 383 above.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83340138/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83340265", "body": "No need to use +\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83340265/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83342387", "body": "also, better use @BeforeClass.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83342387/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83780926", "body": "The result `ts` could be empty, and if you try to build group type with that it will throw exception\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83780926/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83889961", "body": "Seems this not only applies to the select \\* statements. Can you update the comment?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83889961/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83902276", "body": "This is not necessary and is not correct - it may even though NPE for some cases.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83902276/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83902640", "body": "Why need to check empty? \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83902640/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83906286", "body": "Do we need to call `genNestedColPaths` twice? can we store the result somewhere?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83906286/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83906416", "body": "There're so many `(Operator<? extends OperatorDesc>) nd`. Just use `scanOp`.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83906416/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83907376", "body": "Is this comment still relevant?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83907376/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83918210", "body": "Seems this check is not necessary since `colNames` is a set.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83918210/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83924137", "body": "(minor suggestion) I think this func along with `projectLeafTypes` can be combined into one to make it more clear:\n\n`private static GroupType buildProjectedGroupType(GroupType originalType,\n      List<String> groupPaths, String currentPath) {\n    ....\n}\n`\n\nPerhaps return null if none of the group fields get included.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83924137/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83959594", "body": "This doesn't look right - what if the input query is `select f(s.a, s.b.c, ...) from tbl`? no pruning will be done for this case.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83959594/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83959865", "body": "As discussed offline, we may just use ExprNodeFieldDesc to retrieve the path.\nI think we may not even need to do the dfs and matching, simply just recursively\nfind all the field name should be OK? of course, we need to be careful about the\nLIST/MAP/UNION types along the traversal.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83959865/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83972883", "body": "This may produce wrong results since it will mess up the index. For instance:\nif I do `select s.c from tbl` where `tbl` is of schema `a int, s struct<b:int, c:string>`, then the\nindex for `c` is changed from original 1 to 0 in the new group type.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83972883/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84511073", "body": "Same as before - you can remove the 'if' clause to make it simpler.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84511073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84551388", "body": "Please add some comments on what this is doing\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84551388/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84551410", "body": "This doesn't seem need to be public.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84551410/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84551520", "body": "'c' is always nonnull here.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84551520/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84560559", "body": "These cases may not be needed. How about:\n\n```\n    if (desc instanceof ExprNodeColumnDesc) {\n      String f = ((ExprNodeColumnDesc) desc).getColumn();\n      String p = pathToRoot.isEmpty() ? f : f + \".\" + pathToRoot;\n      paths.add(p);\n    } else if (desc instanceof ExprNodeFieldDesc) {\n      String f = ((ExprNodeFieldDesc) desc).getFieldName();\n      String p = pathToRoot.isEmpty() ? f : f + \".\" + pathToRoot;\n      getNestedColsFromExprNodeDesc(((ExprNodeFieldDesc) desc).getDesc(), p, paths);\n    } else {\n      List<ExprNodeDesc> children = desc.getChildren();\n      if (children == null) {\n        return;\n      }\n      for (ExprNodeDesc c : children) {\n        getNestedColsFromExprNodeDesc(c, pathToRoot, paths);\n      }\n    }\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84560559/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84616820", "body": "I know it's \"to avoid adding empty set\", what I mean is this check may not be necessary.\nYou can just have `groups.addAll(gp);`, right? to make it simpler.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84616820/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85171725", "body": "I don't think this will work. We change `adjustedIndex` without looking at the `isRoot`, so even root-level inspectors have the adjusted index. But when inspecting data, they should still use the original index.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85171725/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85173292", "body": "This may not be precise since `index` is still used for read (for root-level).\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85173292/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85182181", "body": "Hmm.. I don't know about you mean. Why we need to set `select` recursively? Can you give a counter-example?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85182181/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85183677", "body": "I'm not sure -here we've already have the 'index' which can be used to retrieve the field name. And also `split` is more costly.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85183677/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85373598", "body": "Yes, when there's no pruning then we should still use the original index. This is the only place `StructFieldImpl` is initialized so I don't think the index is changed in any other places.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85373598/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85562424", "body": "Hmm, you mean adjustedIndex ->readIndex or something?\nAnd you mean always return `writeIndex` for those columns that are not pruned, so `isRoot` is not needed? but `isRoot` is used for those pruned columns as well. There are 3 cases:\n1. column is not pruned: use original index\n2. column is pruned but is we are looking at the root level type: use original index\n3. column is pruned and we are looking at the nested level type: use adjusted index\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85562424/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80389601", "body": "Seems this import is not used?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80389601/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80389629", "body": "Add some comments for this class. Also, seems part of this code is borrowed from Spark - maybe add reference and indicate what changes we made here.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80389629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80389633", "body": "why these needs to be thread local?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80389633/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80389661", "body": "Do we need to address https://issues.apache.org/jira/browse/SPARK-14217? seems the code is different there.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80389661/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80389755", "body": "Add comments on this class and also reference to Spark\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80389755/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80389758", "body": "Add comments on the class and reference to Spark.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80389758/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80389762", "body": "Add comments for the interface and reference to Spark.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80389762/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81063808", "body": "maybe just use the one in DataWritableReadSupport?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81063808/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81063840", "body": "Maybe make the ones in DataWritableReadSupport to be public and use them? or extract them into a util class.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81063840/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81064000", "body": "add an empty line.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81064000/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81064139", "body": "shouldn't we throw exception here?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81064139/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81064174", "body": "remove extra space before 'new'\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81064174/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81064294", "body": "maybe move this method to the bottom? it's generally suggested to have public methods in the top and protected/private methods in the bottom.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81064294/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81064428", "body": "This looks exactly the same as `ParquetRecordReaderWrapper#getSplit`, same for the `setFilter` below. Maybe we could create a base class for these two classes and put the functionality there?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81064428/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81064645", "body": "These few methods also look the same as the corresponding ones in `DataWritableReadSupport`. Better put into a utility class.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81064645/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81064876", "body": "This seems not need to be public.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81064876/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81065098", "body": "add a TODO?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81065098/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81065106", "body": "add a TODO?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81065106/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81065308", "body": "Not sure if we need this field. The batch is passed in in `next` method. For `createValue` we can just create a new batch and return it.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81065308/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81065383", "body": "nit: add braces\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81065383/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81065391", "body": "nit: add braces\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81065391/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81065402", "body": "nit: add braces.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81065402/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81065966", "body": "hmm. why set `isRepeating` to true?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81065966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81066544", "body": "Maybe better to move these iterator classes to `VectorizedColumnReader`? since they are used there and not here in this class.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81066544/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81066617", "body": "these can be private.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81066617/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81066814", "body": "This and a few following methods seem to be from `NanoTimeUtils`. Better to reuse the methods.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81066814/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81067104", "body": "seems not necessary to have this as a field.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81067104/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81068069", "body": "`readXX` methods do not need to take `dataColumn` and `maxDefLevel` as a parameter - they are fields.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81068069/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81068118", "body": "This and the `readXX` methods below can be private.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81068118/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81068147", "body": "Do we need to call `consume` again here? we've called before entering this method, and this is perf critical path.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81068147/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81068863", "body": "not sure about this, but might be better to replace this with `c.isRepeating = c.isRepeating && (c.vector[0] == c.vector[rowId])` is better since the former has branching cost.\n\nAlso, space after ')'\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81068863/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81068961", "body": "`readBinaries`?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/81068961/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83342767", "body": "It's confusing - it has 'Abstract' in name but is not an abstract class.\nChange to 'ParquetRecordReaderBase', or make it abstract.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/83342767/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84376226", "body": "Remove unused import\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/84376226/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85567217", "body": "Please not to include unrelated changes in the patch.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85567217/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85567262", "body": "Same as before - do not to include unrelated changes in the patch.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85567262/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85575360", "body": "Should we throw exception here if it's not supported?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85575360/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85614791", "body": "I still don't think you need to call `consume()` here. To my understanding, each iteration in L143 reads at most what's left in the current page, and will never cross the page boundary and read the next page. The `left` here is the smaller value of the number of values left in the current page, or the number of remaining values need to be read in the current batch. Thought?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85614791/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85666135", "body": "Init a new `HiveDecimalWritable` for each row doesn't sound good, especially in this hot path. Anyway we can reuse the objects?\n\nAlso, no need to call `getScale` again.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85666135/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85666432", "body": "Seems `subarray` also does copying. Can we avoid that?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85666432/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85666455", "body": "Replace `dictionaryIds.noNulls ? true : false` with `dictionaryIds.noNulls`\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85666455/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85666527", "body": "This is not used anywhere\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85666527/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85666568", "body": "'partial' -> 'part'\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85666568/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85666584", "body": "These two fields can be private.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85666584/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85666702", "body": "Remove?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/85666702/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86261637", "body": "partial -> part.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86261637/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86261940", "body": "nit: Seems you don't really need the `consume` function as it is only called here. \nAnd you don't need to  compute `leftInPage` without the call.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86261940/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86262056", "body": "Not sure if `definitionLevel` will be greater than `maxDefLevel`. Maybe change `>=` to `=`?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86262056/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86262779", "body": "Why in this constructor `rbCtx` is initialized in a different way than the first constructor?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86262779/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86265836", "body": "I think this piece of code can be improved. For instance, I'm not sure whether `rowGroupOffsets` will ever be null anymore. Also, we don't need to read footer again and do filtering as this is already done on the client side.\nYou don't have to fix this in the current commit but perhaps add a TODO here.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86265836/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86266054", "body": "Not so relevant to this commit, but could you add a TODO here to push the block filtering to the server side?\nUse the `readFooter(Configuration configuration, Path file, MetadataFilter filter)` API\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86266054/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86266297", "body": "Don't call `initRowBatch` here.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86266297/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86266390", "body": "Why it's testing only?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86266390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86266435", "body": "Don't print stack trace. Throw exception and/or log the error message.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86266435/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86266538", "body": "We can check this at the beginning to avoid unnecessary batch reset \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86266538/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86267506", "body": "It's generally better to put public methods before private ones. For easier reading. Can you rearrange the order?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86267506/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86572163", "body": "nit: Just one TODO is enough\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86572163/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86572915", "body": "nit: move this line to the above 'if' clause so don't have to always compute twice.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86572915/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86575659", "body": "nit: space before 'else'\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86575659/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86575723", "body": "nit: extra blank line\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86575723/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86580255", "body": "Is it necessary to add test for Decimal as well?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86580255/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86580825", "body": "Should we also test decimal?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86580825/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86580941", "body": "Hmm, why timestamp can be read even though we throw exception before?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86580941/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86581382", "body": "add license header\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86581382/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86928779", "body": "Sounds good. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86928779/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87350032", "body": "can we add end-to-end test for the non-dictionary path?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87350032/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87351527", "body": "Also I'm surprised that even complex types such as array and map can also be processed in the Qfile. Are these also handled by the dictionary?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87351527/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87542359", "body": "I'm wondering if we can use `setRef` here to avoid copying.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87542359/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87542583", "body": "Perhaps we should consider removing this. How useful it is for having `isRepeating` on a binary data? This could affect performance badly. Seems ORC doesn't set this.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87542583/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87615492", "body": "We could consider using `setRef` here as well.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87615492/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87865247", "body": "Sorry I don't quite understand. You mean it can only construct a column vector from another vector? why? From the API it doesn't look like so.\nAlso why the vector/start/length separation is an issue?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/87865247/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89906730", "body": "this is what is used before (I think it indicates hadoop configuration) so I didn't change it.", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89906730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89829072", "body": "License header?", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89829072/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89829477", "body": "Add some comment on the `readBatch` method? Also, from the method signature it seems it should not be restricted to only Parquet. How about `VectorizedColumnReader`?", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89829477/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89829617", "body": "Is this really necessary? this is the same as `VectorizedParquetColumnReader`.", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89829617/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89829796", "body": "Extra line", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89829796/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89830344", "body": "What if the path length is smaller than `depth`? Will this crash?", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89830344/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89830891", "body": "What if `fieldReaders.size()` is not equal to `fieldTypes.size()`. Can this be handled?", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89830891/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89831787", "body": "If this is a primitive column reader, why it should read complex types?", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/89831787/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90301980", "body": "Remove unused imports.", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90301980/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90302295", "body": "Better to improve this message, e.g., include the specific type involved.", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90302295/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90302428", "body": "NO break?", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90302428/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90303143", "body": "`>=` ?", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90303143/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90303478", "body": "nit: can we put this into the same line above? for easier reading.", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90303478/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90305601", "body": "is it possible that `indexColumnsWanted` could be empty?", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90305601/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90310650", "body": "make this private?", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90310650/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90311749", "body": "I think there's a difference between null struct versus struct with null fields. Seems this treat the two cases as the same. Do we need to differentiate them?", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90311749/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90311940", "body": "space before {", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90311940/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90312251", "body": "I think we talked about testing reading decimal. Should we add in this patch?", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/90312251/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/91415309", "body": "nit: rename this to `VectorizedStructColumnReader`? to be consistent with `VectorizedColumnReader` and `VectorizedPrimitiveColumnReader`.", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/91415309/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/91416023", "body": "nit: mark this as final?", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/91416023/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/91416221", "body": "should we set `structColumnVector.noNulls` as well?", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/91416221/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/91451693", "body": "Hmm.. why this needs to be in the inner loop. Can you just do:\r\n`structColumnVector.noNulls = (i == 0) ? vectors[i].noNulls : structColumnVector.noNulls && vectors[i].noNulls;`?", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/91451693/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ashutoshc": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/1d5303523b7fdc3cc4c915957272d4192000b019", "message": "HIVE-18251 : Loosen restriction for some checks (Ashutosh Chauhan via Jesus Camacho Rodriguez)"}, {"url": "https://api.github.com/repos/apache/hive/commits/b5ba82738337284d5a73bfddce11beb2c88c3ce2", "message": "HIVE-18100 : Some tests time out"}, {"url": "https://api.github.com/repos/apache/hive/commits/de278cfb1488c04c6c60f9686ae173a5f02ec815", "message": "HIVE-18089 : Update golden files (addendum)"}, {"url": "https://api.github.com/repos/apache/hive/commits/24d6aaaabee1cc9e7b842d12cdee2ea192d8194d", "message": "HIVE-14678 : Hive-on-MR deprecation warning  is not diplayed when engine is set to capital letter 'MR' (Sergey Shelukhin via Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/a9b5d794ca0576be662e7e3ec30e2547a7781375", "message": "HIVE-18089 : Update golden files for few tests"}, {"url": "https://api.github.com/repos/apache/hive/commits/afa9ffee8ae308c39ece2fa76897a304acd1c1ca", "message": "HIVE-18067 : Remove extraneous golden files"}, {"url": "https://api.github.com/repos/apache/hive/commits/812d75718eacec4d086e987fcf1e575a8f5bbd9b", "message": "HIVE-18010 : Update hbase version (Ashutosh Chauhan via Zoltan Haindrich)"}, {"url": "https://api.github.com/repos/apache/hive/commits/6f4388f4d328dce0016523ab2f1fdc2c27426b04", "message": "HIVE-18007 : Address maven warnings (Ashutosh Chauhan via Alan Gates)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/338747492", "body": "Michael, can you please add your patch here so that we get a CI run on it https://cwiki.apache.org/confluence/display/Hive/Hive+PreCommit+Patch+Testing", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/338747492/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/77758450", "body": "Can you improve description of this config? Is it the address used in create table statement if address is not specified in table properties?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77758450/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77758503", "body": "Can you add a comment on why we need to shade. That way when versions align we may get rid of shading.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77758503/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77758653", "body": "Comment: Caller is responsible for stream cleanup?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77758653/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77758742", "body": "Any reason for implementing mapreduce.InputFormat. Hive uses mapred.inputformat\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77758742/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77758932", "body": "This may connect to FS, which may be a slow operation. Can this be a dummy path?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77758932/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77759195", "body": "It will be good to add a comment why different query types warrant different record readers?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77759195/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77759267", "body": "I dont think we should ever rely on default address from config. Broker address should be present in metadata.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77759267/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77759532", "body": "Comment about need of these converters?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77759532/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77759674", "body": "Comment regarding dimensions, aggregators and post aggregators.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77759674/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77759782", "body": "Nice comments!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77759782/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77759847", "body": "May want to leave a TODO to generate vector row batches here so that vectorized execution may get triggered.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77759847/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77760224", "body": "This branch in inner loop may be expensive. Lets get rid of it.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77760224/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77760336", "body": "Why is there a mismatch between actual object type and type obtained from inspector. Such if-else is expensive, it will be good to avoid this.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77760336/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77760384", "body": "This should be an error condition, otherwise may give wrong results.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77760384/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77760424", "body": "Better to throw exception, then assume string.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77760424/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77760465", "body": "Exception.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77760465/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77760632", "body": "Using array might be a better choice than map. No need to store names for each column for each row. Also, hash lookup will be slower than array access.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77760632/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77760865", "body": " Ideally this should not happen as this leaks storage handler specific logic in ql which breaks abstraction. Add a comment for why we are doing this.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77760865/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77760916", "body": "now it really should :)\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77760916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77760955", "body": "Comment: these are short lived and will be replaced by extract soon?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77760955/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77761256", "body": "Comment for magic constant: 0.1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77761256/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77761448", "body": "Comment: To be removed once we move to Calcite 1.9\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77761448/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77761526", "body": "Comment: To be removed once we move to calcite 1.9+\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77761526/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77761745", "body": "Comment: To be removed after calcite 1.9+\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77761745/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77761870", "body": "Divide also seems incorrect.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77761870/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "vineshcpaul": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/4d2dc35131ba87e1d100525eb5830fc14878a8ab", "message": "HIVE-18232 : Add dfs-init.sh in package (Vinesh Paul via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "harishjp": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/a1f54df470537fb2a8a80855bf244d757bfabb9c", "message": "HIVE-18210: create resource plan allows duplicates (Harish Jaiprakash reviewed by Sergey Shelukhin)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "daijy": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/1968a9d458b7ba3fb61c47084ed19253cd2e18e8", "message": "HIVE-18189: Order by position does not work when cbo is disabled (Daniel Dai reviewed by Ashutosh Chauhan)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "colinmjj": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/8ac91e7317eebb3f36c12b84fa2b447de827dffb", "message": "HIVE-18207: Fix the test failure for TestCliDriver#vector_complex_join (Colin Ma via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "t3rmin4t0r": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/d041cc441e614e2e9a12645d9775803c6359bfad", "message": "HIVE-18213: Tests: YARN Minicluster times out if the disks are >90% full (Gopal V, reviewed by Prasanth Jayachandran)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ghajos": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/2a89e83c2f9c7160617618910c99178bb5ef7bc5", "message": "HIVE-17620: Use the default MR scratch directory (HDFS) in the only case when hive.blobstore.optimizations.enabled=true AND isFinalJob=true (Gergely Haj\u00f3s reviewed by Rajesh Balamohan via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hagleitn": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/af6f80ab541856ba6ceb7771f425f9168516812c", "message": "HIVE-18198: TablePropertyEnrichmentOptimizer.java is missing the Apache license header (Deepesh Khandelwal via Gunther Hagleitner)"}, {"url": "https://api.github.com/repos/apache/hive/commits/4218629de715df43a1778de03f85e41bc682b1a8", "message": "HIVE-18195: Hive schema broken on postgres (Deepesh Khandelwal, reviewed by Sergey Shelukhin)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/181", "title": "HIVE-1010: Implement INFORMATION_SCHEMA in Hive (Gunther Hagleitner)", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ayousufi": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/f87315a79296f4715beb9d45be291b74addae940", "message": "HIVE-14560: Support exchange partition between s3 and hdfs tables (Abdullah Yousufi via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rbalamohan": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/d5f552528bb45f1419c0c86a5ec76cf2664a57b9", "message": "HIVE-15504 : ArrayIndexOutOfBoundsException in GenericUDFTrunc::initialize (Rajesh Balamohan via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/7150f9c83b8ad0a9bf7ea47fa905dd01b0f26d21", "message": "HIVE-16406 : Remove unwanted interning when creating PartitionDesc (Rajesh Balamohan via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wagnermarkd": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/c9a67786483edaf7480cc73cdec2e492a7f16f46", "message": "HIVE-15739 : Incorrect exception message in PartExprEvalUtils (Mark Wagner via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/250625143", "body": "@winningsix It's up to the SerDe whether or not to leverage the information provided, right? What's the concern in placing the additional info in the configuration? Parquet can leverage it immediately, while other file formats can be updated to use it (or not) at their own pace.\n\nDefaulting to off means that it will get less test coverage and there's more risk that it could be inadvertently reverted.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/250625143/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/80876154", "body": "\"Group\" is a uniquely Parquet terminology. Would you mind replacing it with the Hive lingo?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80876154/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80878262", "body": "This test ensures that Parquet SerDe is unbroken. If someone makes a change in the future that inadvertently causes the projection info to be inaccurate or incomplete (e.g. if it were to include all nested members of  a top level column), we might not find out about it until a new release is deployed to clusters and people notice worse performance. Can you add unit tests to ensure the right projection is being pushed down?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/80878262/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "BELUGABEHR": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/82cb3d57abf2705069d82807e52177fdb41ff5ca", "message": "HIVE-17911 : org.apache.hadoop.hive.metastore.ObjectStore - Tune Up (Beluga Behr via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/bff9da2cc03da848189c7266ee57069dde3fe668", "message": "HIVE-18016 : org.apache.hadoop.hive.ql.util.ResourceDownloader - Review (Beluga Behr via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kellyzly": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/5104f5d33013daa22b88bca5b92d24155f770d08", "message": "HIVE-17973: Fix small bug in multi_insert_union_src.q (Liyun Zhang reviewed by Rui)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Tartarus0zm": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/295", "title": "HIVE-15631 When the Hive client is started, the sessionid is printed from the console.", "body": "When the Hive client is started, the sessionid is printed from the console.", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/359357629", "body": "a new pull request will be create", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/359357629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dosoft": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/292", "title": "HIVE-17331: Use Path instead of String as key type of the pathToAliases", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/234", "title": "HIVE-17314: Removed obsolete code", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/230", "title": "HIVE-17313: Fixed 'case fall-through'", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/229", "title": "HIVE-17311: Fixed numeric overflow, added test", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/224", "title": "HIVE-17260: Fixed typo (non-thrown exception)", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/222", "title": "HIVE-17258: Fixed typos", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/195", "title": "HIVE-16925: Add isSlowStart as parameter for the setAutoReduce method", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "msydoron": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/288", "title": "HIVE-18423", "body": "Added full support for jdbc external tables in hive.\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "omalley": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/285", "title": "HIVE-16480 (ORC-285) Empty vector batches of floats or doubles gets", "body": "EOFException.\r\n\r\nSigned-off-by: Owen O'Malley <omalley@apache.org>", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/77", "title": "HIVE-9660 Add length to ORC indexes so that the reader knows how much to read.", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/157602738", "body": "This has been committed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/157602738/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/241074499", "body": "We should probably test with java7 also.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/241074499/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/270699301", "body": "I did this as part of HIVE-15419.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/270699301/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/274216269", "body": "This version uses ORC 1.3.0.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/274216269/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/280145269", "body": "Ok, since the calling code will just allocate an array the size it wants, I made a static final with the empty array.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/280145269/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/328193155", "body": "+1 if the tests pass.\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/328193155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/17354974", "body": "But it recreates target/tmp in the next line. Which test assumes that files are pre-located there?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17354974/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/44687005", "body": "I did port all of the TestOrcFile to the vectorized writer in the new file TestVectorOrcFile. Does that address your concerns?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44687005/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45020685", "body": "This patch doesn't actually change the write path. I've done this in HIVE-12055 including adding a new writer version.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45020685/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46189185", "body": "The CompressionCodec isn't really an API class, since the user isn't able to add their own implementations (other than the external LZO implementation). I guess I can move it over, if you think it is important. In general LLAP is accessing ORC beneath the API level.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46189185/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46200588", "body": "The DirectDecompressor API was added in Hadoop 2.3. I'd rather go ahead and make it a real shim so that we can support older Hadoop versions.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46200588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60961527", "body": "I'll add a comment, but it does clone the FileHandle so that they can be closed separately.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60961527/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60963369", "body": "Ok, I've updated both Char and Varchar with the check.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60963369/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63047229", "body": "For now we should stick with what Hive supports. I copied this code directly from Hive's HadoopShims.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63047229/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63048104", "body": "I'm not sure what you think is misaligned, but I simplified this code.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63048104/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63048219", "body": "I copied this code from Hive's HadoopShims, but yes we should close the stream.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63048219/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63056675", "body": "Ok, the file stream is the same one as the DefaultDataReader, but that one is also not closed. I'll make the zerocopyshim closeable.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63056675/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "amrk7s": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/284", "title": "HIVE-18338 Exposing asynchronous execution through hive-jdbc client", "body": "**Problem statement**\r\n\r\nHive JDBC currently exposes 2 methods related to asynchronous execution\r\n**executeAsync()** - to trigger a query execution and return immediately.\r\n**waitForOperationToComplete()** - which waits till the current execution is complete **blocking the user thread**. \r\n\r\nThis has one problem\r\n- If the client process goes down, there is no way to resume queries although hive server is completely asynchronous. \r\n\r\n**Proposal**\r\n\r\nIf operation handle could be exposed, we can latch on to an active execution of a query.\r\n\r\n**Code changes**\r\n\r\nOperation handle is exposed. So client can keep a copy.\r\nlatchSync() and latchAsync() methods take an operation handle and try to latch on to the current execution in hive server if present", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/353875927", "body": "Travis CI seem to have failed? Is this a problem? Please let me know.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/353875927/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "mattk42": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/278", "title": "HIVE-18295 - Add ability to ignore invalid values in JSON SerDe ", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ymwdalex": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/273", "title": "HIVE-18130: Update table path to storage parameters when alter a table", "body": "When an managed table is created by Spark, table path information is not only store in `location` field (first figure), but also in `parameters.path` fields (second figure).\r\n<img width=\"445\" alt=\"location\" src=\"https://user-images.githubusercontent.com/1458656/33123050-466c6bdc-cf79-11e7-8b53-85ad7dc83f17.png\">\r\n<img width=\"677\" alt=\"storage_parameter\" src=\"https://user-images.githubusercontent.com/1458656/33123051-469576da-cf79-11e7-9c6c-9793a28f8389.png\">\r\n\r\nWhen hive alter a table, `storage parameter` is ignored. Then, spark cannot access the table anymore because spark use the parameter.\r\n\r\nIn this PR, when altering a table, the field `storageDescription.parameters.path` is also updated, if `path` key exists\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hejian991": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/269", "title": "fix small bug: when disable partition statistics in hive-site.xml, bu\u2026", "body": "\u2026t it does not work.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ishitbatra": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/268", "title": "hive installation link", "body": "hive link given", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thejasmn": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/266", "title": "Hive-17897", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/171", "title": "HIVE-16497 : FileUtils. isActionPermittedForFileHierarchy, isOwnerOfF\u2026", "body": "\u2026ileHierarchy file system operations should be impersonated", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/148", "title": "HIVE-15900", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/88", "title": "HIVE-13879", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/79", "title": "Hive 13867", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/71", "title": "HIVE-13491 - print thread dumps", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/69", "title": "HIVE-13418", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/67", "title": "Hive 13169", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/64", "title": "HIVE-12660", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/59", "title": "Hive 12722", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/58", "title": "HIVE-12698 : introduce HiveAuthorizationTranslator interface for isolating authori\u2026", "body": "\u2026zation impls from hive internal classes\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/51", "title": "HIVE-12261", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/43", "title": "HIVE-11134 - HS2 should log open session failure", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/40", "title": "HIVE-10843", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/38", "title": "Hive 10689", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/34", "title": "HIVE-10578 : update sql standard authorization configuration whitelist", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/22", "title": "Hive 8384", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/165341268", "body": "addressing the comments\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/165341268/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/165342518", "body": "Updated patch addresses additional comments\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/165342518/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "kanna14243": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/265", "title": "Update TBinarySortableProtocol.java", "body": "writeTextBytes doesn't respect the start parameter. It'll work only for cases where start = 0. Fixing it so that it'll work for any value of start.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rmartin-rp": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/232", "title": "Update 039-HIVE-12274.oracle.sql", "body": "You cannot modify a column from VARCHAR2(4000) to CLOB directly. You need to add a new column and drop the old one or recreate the table. I chose to recreate the table because it's more clean.\r\n\r\nhttps://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:1770086700346491686\r\n\r\nAlso, if you do something like:\r\nALTER TABLE COLUMNS_V2 MODIFY (COLUMN_NAME VARCHAR(767) NOT NULL);\r\nAnd the column is already NOT NULL, the ALTER TABLE fails.  I removed 2 like this.\r\n\r\nI hope it helps.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jason000zhang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/208", "title": "Cdh5 1.1.0 5.12.0", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mlorek": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/204", "title": "HIVE-17038 - DateParser fix", "body": "", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/314733813", "body": "this relates to:\r\n\r\nhttps://issues.apache.org/jira/browse/HIVE-17038\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/314733813/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "chitin": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/203", "title": "HIVE-17077: Hive should raise StringIndexOutOfBoundsException when LPAD/RPAD len character's value is negative number", "body": "[HIVE-17077] Hive should raise StringIndexOutOfBoundsException when LPAD/RPAD len character's value is negative number\r\n- return null when len character's value is negative number\r\n\r\nhttps://issues.apache.org/jira/browse/HIVE-17077", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Balakrishnan-Zapr": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/197", "title": "Custom Thrift Json deserializer for Hive", "body": "Custom Thrift Json deserializer to deserialize the Json records serialized by ThriftJsonProtocol.\r\n**Steps :**\r\n    1. ThriftJsonDeserializer class to deserialize the thrift json in hive.\r\n    2. Jackson serializer - Object mapper is used for converting the thrift jsons to the class object\r\n\r\n**Usage :** \r\nSET ROW FORMAT SERDE  'org.apache.hadoop.hive.serde2.thrift.ThriftJsonDeserializer'\r\nSET SERDEPROPERTIES (\r\n 'serialization.class'='contract class',\r\n 'serialization.format'='org.apache.thrift.protocol.TSimpleJSONProtocol' -- To get the schema for tables.\r\n )\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "txhsj": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/183", "title": "HIVE-16645 Commands.java has missed the catch statement and has some \u2026", "body": "In commands.java, the catch statement is missing and the Resultset statement is not closed.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/pulls/169", "title": "HIVE-15442:Driver.java has a redundancy code", "body": "Driver.java has a redundancy code about \"explain output\", i think the if statement \" if (conf.getBoolVar(ConfVars.HIVE_LOG_EXPLAIN_OUTPUT))\" has a repeat judge with the above statement.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kevinsimard": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/164", "title": "Use setNull when null value is passed to preparedstatement setter", "body": "Not sure if I should open a ticket first but PreparedStatement setters are failing when a null value is passed (the setString in my case). I changed the other ones as well so they would have the same behavior.\r\n\r\nA workaround solution is to use the setObject instead of setString but the wrong setter might be used depending on the value's type which I want to avoid.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "macalinao": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/157", "title": "[HIVE-16184] Thrift cleanup", "body": "Remove use of optionals in union; declare TProgressUpdateResp before its usage\r\n\r\nSee: https://issues.apache.org/jira/browse/HIVE-16184", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gyisgood": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/154", "title": "Avoiding leaked tcp link to metastore", "body": "Using Hive.getWithFastCheck in Task.getHive will cause leaked tcp link to metastore when executing\r\nSQL", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hellopower": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/150", "title": "HIVE-15848:count or sum distinct incorrect when hive.optimize.reducededuplication set to true", "body": "when I run command  'schematool -dbType mysql -initSchema -verbose'.\r\nerror:\r\n You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ') ENGINE=InnoDB DEFAULT CHARSET=latin1' at line 1", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/pulls/76", "title": "HIVE-13745: NullPointerException when current_date is used in mapreduce", "body": "NullPointerException when current_date is used in mapreduce.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "apivovarov": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/140", "title": "HIVE-15746: Fix default delimiter2 value in str_to_map UDF description", "body": "https://issues.apache.org/jira/browse/HIVE-15746", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/48", "title": "HIVE-11593 Add aes_encrypt and aes_decrypt UDFs", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/116139013", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/116139013/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "abbaspour": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/133", "title": "[HIVE-15690] Enable webhcat to run JDBC connection for Hive DDL queries", "body": "This is a change in `HcatDelegator` to run Hive DDL queries over **JDBC** connection in contrast to slow `hcat` command.\r\n\r\nMotivation is basically speed. The way `HcatDelegator` launches new `hcat` scripts per call makes it unsuitable for interactive REST calls. \r\n\r\nThis change speeds up /ddl queries from normally 10-20 sec down to few milliseconds. No connection pooling is in place to make the RP small but that can be added anytime. \r\n\r\nAlso being JDBC connection, this is pretty secure and compatible with all access policies define in Hive server2. User does not have visibility over other databases (which used to be the case in `hcat` mode.)\r\n\r\nTo switch to JDBC mode simply add this configuration to **webhcat-site.xml**\r\n\r\n```xml\r\n<property>\r\n      <name>templeton.ddl.mode</name>\r\n      <value>jdbc</value>\r\n</property>\r\n<property>\r\n      <name>hive.jdbc.url</name>\r\n      <value>jdbc:hive2://server:port</value>\r\n</property>\r\n```\r\n\r\nFor secure environments we also need these attributes in webhcat-site.xml configuration:\r\n\r\n```xml\r\n    <property>\r\n      <name>hive.server2.kerberos.keytab</name>\r\n      <value>/etc/security/keytabs/hive.service.keytab</value>\r\n    </property>\r\n\r\n    <property>\r\n      <name>hive.server2.kerberos.principal</name>\r\n      <value>hive/_HOST@{REALM}</value>\r\n    </property>\r\n```\r\n\r\nThis change uses Hive DDL JSON output so that should be allowed in **hiveserver2-site.xml**\r\n\r\n```xml\r\n    <property>\r\n      <name>hive.security.authorization.sqlstd.confwhitelist.append</name>\r\n      <value>hive.ddl.output.format</value>\r\n    </property>\r\n```\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lifove": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/126", "title": "HIVE-15497: Fix an unthrown SerDeException", "body": "https://issues.apache.org/jira/browse/HIVE-15497", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "subahugu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/123", "title": "HIVE-15424: Not calling rollbackCreateTable(), when create table fail\u2026", "body": "\u2026s with already exists exception.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/pulls/122", "title": "HIVE-15423: Allowing Hive to reverse map IP from hostname for partiti\u2026", "body": "\u2026on info", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "melode11": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/118", "title": "fix stripe size smaller than expected", "body": "I saw there's a Jira item: https://issues.apache.org/jira/browse/HIVE-13232 moved the compressed = null out of if block. But that is not seem to be a complete fix. To calculate the right value, we cannot use all bytes that a reserved, but should use bytes are filled. Thus I change the capacity() to position().\r\n```java  \r\npublic void flush() throws IOException {\r\n    spill();\r\n    if (compressed != null && compressed.position() != 0) {\r\n      compressed.flip();\r\n      receiver.output(compressed);\r\n//Should move compress = null out of if block, (already been moved out in 2.1.0 code)\r\n//otherwise its capacity will count for all following stripes even it is not used by them.\r\n      compressed = null;\r\n}\r\n    uncompressedBytes = 0;\r\n    compressedBytes = 0;\r\n    overflow = null;\r\n    current = null;\r\n  }\r\n```", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "buom": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/117", "title": "Wrong the location of table directory", "body": "Wrong the location of table directory.\r\n\r\nSee: \r\nhttps://github.com/apache/hive/blob/master/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatOutputFormat.java#L120\r\nhttps://github.com/apache/hive/blob/master/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatOutputFormat.java#L163\r\nhttps://github.com/apache/hive/blob/master/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FosterStorageHandler.java#L168\r\nhttps://github.com/apache/hive/blob/master/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FosterStorageHandler.java#L170\r\nhttps://github.com/apache/hive/blob/master/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FosterStorageHandler.java#L206\r\nhttps://github.com/apache/hive/blob/master/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FosterStorageHandler.java#L208", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "clojurians-org": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/111", "title": "update to spark 2.0 yarn mode", "body": "change default yarn mode from yarn-cluster to yarn\r\npass yarn property startWith yarn only.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cvaliente": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/101", "title": "HIVE-14759 remove GenericUDF from funcName instead of first 10 chars", "body": "remove \"GenericUDF\" from the function Name instead of first 10 chars which causes IndexOutOfBounds when my Classname is less than ten chars.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bonnetb": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/100", "title": "HIVE-14660 : ArrayIndexOutOfBounds on delete", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "szador": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/96", "title": "HIVE-14483", "body": "Fix java.lang.ArrayIndexOutOfBoundsException org.apache.orc.impl.TreeReaderFactory$BytesColumnVectorUtil.commonReadByteArrays\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/261583563", "body": "Fix Version/s: 1.3.0, 2.2.0, 2.1.1, 2.0.2\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/261583563/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dark0dave": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/94", "title": "updates to udfregexextract to help users understand issues", "body": "Slight change to RegExpExtract function in hive should be able to catch errors when thrown to inform users of their mistakes.\n\nAdded an travis file to ensure builds correctly. Named correctly now.\n\nUpdated pom file.\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/235252930", "body": "named travis file wrong\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/235252930/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "utf7": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/90", "title": "fix the import database_name.table_name from path", "body": "detail :\nuse test;\ncreate table a(id int,name string);\nexport table a to '/tmp/a';\ndrop table a;\nimport table test.a from '/tmp/a';\nhive> import table test.a from '/tmp/a';\nFailed with exception Invalid table name test.test.a\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask\n\nwhen use import database_name.table_name from ,\nthe tblDesc.getTableName() return database_name.table_name ,not table_name\nTable table = new Table(dbname,database_name.table_name) will return the table's name is dbname.database_name.table_name\ncorrect table name should be test.a,not   test.test.a\n\nwe can fix this:\n  String[] dbTableName =Utilities.getDbTableName(dbname,tblDesc.getTableName());\n   Table table = new Table(dbTableName[0], dbTableName[1]);\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/pulls/89", "title": "Update TaskFactory.java", "body": "getAndIncrementId() method use new Integer() ,Integer.valueOf() is a better method\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zhongdeyin": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/85", "title": "[HIVE-14160]Optimized group by within distinct data skew proplem", "body": "Jira issue: https://issues.apache.org/jira/browse/HIVE-14160\n\nModify GroupByOperator.endGroup() method, while keysCurrentGroup.size() is greater than a threshold(hive.distinct.setsize.max), will new a HashSet instead of clear set. Option hive.distinct.newset.max is the Maximum for getting new hashset, beyond this threshold,will still execute hashset.clear(). This will ensure task stability. User can set these two client parameters themselves.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stonewell": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/80", "title": "add serde type protobuf for single column deserialize", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jiangxt2": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/78", "title": "HIVE-13877 Fixed the problem with Hplsql UDF doesn't work in Hive Cli", "body": "Hive cli will throw \"Error evaluating hplsql\" exception when i use the hplsql udf like \"SELECT hplsql('hello[:1]', columnName) FROM tableName\". So, this request exists to fix that problem.\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/222101183", "body": "Is it my fault? How can i fix it? thx my bro. :)\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222101183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/222108475", "body": "@dmtolpeko Hi, I created a JIRA for this problem.\nThe url is https://issues.apache.org/jira/browse/HIVE-13877\nWhat should I do next?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222108475/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/222111588", "body": "@dmtolpeko OK, I GOT IT. Thanks very much. I'll wait for your response.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222111588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "timrobertson100": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/74", "title": "HIVE-13539: HiveHFileOutputFormat searching the wrong directory for H\u2026", "body": "I believe this is a fix for https://issues.apache.org/jira/browse/HIVE-13539\n\nWhen there are several reducers (or speculative execution) there becomes multiple output dircetories for the task attempts.  Previous behaviour threw exception incorrectly as it assumed multiple HFiles.\n\nHere, I am attempting to start the descending directory search from the task attempt folder and not from the higher directory for the table.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sundapeng": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/73", "title": "HIVE-13545: Add GLOBAL Type to Entity", "body": "", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chutium": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/66", "title": "[HIVE-13170] Implements interface HiveOutputFormat<K,V>", "body": "HiveAccumuloTableOutputFormat as OutputFormat in an hive storage handler, should implement HiveOutputFormat<K,V> interface as well to ensure compatibility\n\nref. https://issues.apache.org/jira/browse/HIVE-13170\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/191775846", "body": "thanks for reviewing, to reproduce the issue, you can create a spark context with following jars:\naccumulo-core-1.6.0.jar\naccumulo-fate-1.6.0.jar\naccumulo-start-1.6.0.jar\naccumulo-trace-1.6.0.jar\nhive-accumulo-handler-1.2.1.jar\n\nthen run some hive query like:\n\n```\nset accumulo.instance.name=instance_name;\nset accumulo.zookeepers=zk_host:2181;\nset accumulo.user.name=user;\nset accumulo.user.pass=pass;\n\nCREATE TABLE testtable(rowid STRING, value STRING)\nSTORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'\nWITH SERDEPROPERTIES('accumulo.columns.mapping' = ':rowid,colf:colq');\n\nINSERT OVERWRITE TABLE testtable SELECT col1, col2 FROM some_hive_table;\n```\n\nthen will get error `java.lang.ClassCastException` says `HiveAccumuloTableOutputFormat cannot be cast to org.apache.hadoop.hive.ql.io.HiveOutputFormat`\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/191775846/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/54878969", "body": "yes... seems like RecordWriter is unnecessary, will check this soon\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54878969/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54881651", "body": "noticed and wondering as well, but it works in our trials... the mutations will be written into the target table, that defined in hive query -_- my assume is the table name is already set in JobConf, so we do not need to specify table name for each write operation.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54881651/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "kasjain": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/65", "title": "HIVE-12943: Fixing USE DEFAULT by adding the 'default' keyword to grammer", "body": "Added the \"default\" keyword to the grammer files HiveParser.g, HiveLexer.g and Identifier.g\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gbraccialli": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/63", "title": "HIVE-11741: Add a new hook to run before query parse/compile", "body": "HIVE-11741: Add a new hook to run before query parse/compile\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kirill-vlasov": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/62", "title": "Null pointers should not be dereferenced", "body": "This pull request is focused on resolving occurrences of Sonar rule squid:S2259 - Null pointers should not be dereferenced\nYou can find more information about the issue here:\nhttps://dev.eclipse.org/sonar/coding_rules#q=squid:S2259\nPlease let me know if you have any questions.\nKirill Vlasov\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jasonliaoxiaoge": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/61", "title": "shuffle the metastore uris, so the request can be balanced to metastores", "body": "Currently, HiveMetaStoreClient connect to the first metastore uri defaultly If we have multi metastore uris. So the client only send request to the first metastore, the metastore will be the bottleneck to process the client request.\n\nI add the logic to shuffle metastore uris, so that the request can be balanced to all of the metastores.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Cazen": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/57", "title": "Merge pull request #1 from apache/master", "body": "Update\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "technmsg": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/50", "title": "HIVE-12002: correct typo in HiveMetaStore INFO log", "body": "Corrected minor typo in HiveMetaStore logging.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zhichao-li": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/47", "title": "HIVE-11334 : fix substring_index for multiple chars delim", "body": "", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/123561233", "body": "cc @apivovarov \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/123561233/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "navis": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/41", "title": "HIVE-10890 Provide implementable engine selector", "body": "Now hive supports three kind of engines. It would be good to have an automatic engine selector without setting explicitly engine for execution.\n\nAdded two configurations\n\n> hive.execution.engine.selector\n> hive.execution.engine.selector.param\n\nTest implementation is included as EngineSelector$SimpleSelector, which supports simple grammar something like,\n\n> spark = $total < 10gb && $num_aliases < 4, tez\n\nwhich means if total input length is less than 10gb and number of aliases is less than 4, use spark, else tez\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "FanTn": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/37", "title": "Fix bug when OrcFileStripeMergeRecordReader get nextStripe", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "crazxy": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/35", "title": "[HCatalog]: The column size property of rcfile header should be coded in decimal instead of octal", "body": "The column size property of rcfile header \n\n``` java\nHiveConf.ConfVars.HIVE_RCFILE_COLUMN_NUMBER_CONF\n```\n\nshould be decimal instead of octal.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "saucam": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/32", "title": "HIVE-10296: Remove cast since it might result in exceptions when filter conditions are pushed into joins in postgres", "body": "Try to drop a partition from hive:\nALTER TABLE f__edr_bin_source__900_sub_id DROP IF EXISTS PARTITION ( exporttimestamp=1427824800, timestamp=1427824800)\nThis triggers a query on the metastore like this :\n\"select \"PARTITIONS\".\"PART_ID\" from \"PARTITIONS\" inner join \"TBLS\" on \"PARTITIONS\".\"TBL_ID\" = \"TBLS\".\"TBL_ID\" and \"TBLS\".\"TBL_NAME\" = ? inner join \"DBS\" on \"TBLS\".\"DB_ID\" = \"DBS\".\"DB_ID\" and \"DBS\".\"NAME\" = ? inner join \"PARTITION_KEY_VALS\" \"FILTER0\" on \"FILTER0\".\"PART_ID\" = \"PARTITIONS\".\"PART_ID\" and \"FILTER0\".\"INTEGER_IDX\" = 0 inner join \"PARTITION_KEY_VALS\" \"FILTER1\" on \"FILTER1\".\"PART_ID\" = \"PARTITIONS\".\"PART_ID\" and \"FILTER1\".\"INTEGER_IDX\" = 1 where ( (((case when \"TBLS\".\"TBL_NAME\" = ? and \"DBS\".\"NAME\" = ? then cast(\"FILTER0\".\"PART_KEY_VAL\" as decimal(21,0)) else null end) = ?) and ((case when \"TBLS\".\"TBL_NAME\" = ? and \"DBS\".\"NAME\" = ? then cast(\"FILTER1\".\"PART_KEY_VAL\" as decimal(21,0)) else null end) = ?)) )\"\nIn some cases, when the internal tables in postgres (metastore) have some amount of data, the query plan pushes the condition down into the join.\nNow because of DERBY-6358 , case when clause is used before the cast, but in this case , cast is evaluated before condition being evaluated. So in case we have different tables partitioned on string and integer columns, cast exception is observed!\n15/04/06 08:41:20 ERROR metastore.ObjectStore: Direct SQL failed, falling back to ORM \njavax.jdo.JDODataStoreException: Error executing SQL query \"select \"PARTITIONS\".\"PART_ID\" from \"PARTITIONS\" inner join \"TBLS\" on \"PARTITIONS\".\"TBL_ID\" = \"TBLS\".\"TBL_ID\" and \"TBLS\".\"TBL_NAME\" = ? inner join \"DBS\" on \"TBLS\".\"DB_ID\" = \"DBS\".\"DB_ID\" and \"DBS\".\"NAME\" = ? inner join \"PARTITION_KEY_VALS\" \"FILTER0\" on \"FILTER0\".\"PART_ID\" = \"PARTITIONS\".\"PART_ID\" and \"FILTER0\".\"INTEGER_IDX\" = 0 inner join \"PARTITION_KEY_VALS\" \"FILTER1\" on \"FILTER1\".\"PART_ID\" = \"PARTITIONS\".\"PART_ID\" and \"FILTER1\".\"INTEGER_IDX\" = 1 where ( (((case when \"TBLS\".\"TBL_NAME\" = ? and \"DBS\".\"NAME\" = ? then cast(\"FILTER0\".\"PART_KEY_VAL\" as decimal(21,0)) else null end) = ?) and ((case when \"TBLS\".\"TBL_NAME\" = ? and \"DBS\".\"NAME\" = ? then cast(\"FILTER1\".\"PART_KEY_VAL\" as decimal(21,0)) else null end) = ?)) )\". \nat org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451) \nat org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321) \nat org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:300) \nat org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:211) \nat org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1915) \nat org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1909) \nat org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2208) \nat org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:1909) \nat org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:1882)\norg.postgresql.util.PSQLException: ERROR: invalid input syntax for type numeric: \"_DEFAULT_BINSRC_\"\n15/04/06 08:41:20 INFO metastore.ObjectStore: JDO filter pushdown cannot be used: Filtering is supported only on partition keys of type string \n15/04/06 08:41:20 ERROR metastore.ObjectStore: \njavax.jdo.JDOException: Exception thrown when executing query \nat org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:596) \nat org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:275) \nat org.apache.hadoop.hive.metastore.ObjectStore.getPartitionNamesNoTxn(ObjectStore.java:1700) \nat org.apache.hadoop.hive.metastore.ObjectStore.getPartitionNamesPrunedByExprNoTxn(ObjectStore.java:2003) \nat org.apache.hadoop.hive.metastore.ObjectStore.access$400(ObjectStore.java:146) \nat org.apache.hadoop.hive.metastore.ObjectStore$3.getJdoResult(ObjectStore.java:1937) \nat org.apache.hadoop.hive.metastore.ObjectStore$3.getJdoResult(ObjectStore.java:1909) \nat org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2214) \nat org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:1909) \nat org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:1882)\n\nThis PR removes the cast, applied, altogether, as a workaround\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/pulls/25", "title": "HIVE-9028: Enhance the hive parser to accept tuples in where in clause filter", "body": "Currently, the hive parser will only accept a list of values in the where in clause and the filter is applied only on a single column. Enhanced it to accept filter on multiple columns.\nSo current support is for queries like :\nSelect \\* from table where c1 in (value1,value2,...value n);\nAdded support in the parser for queries like :\nSelect \\* from table where (c1,c2,... cn) in ((value1,value2...value n), (value1' , value2' ... ,value n').... )\n\nHave added support for these queries in spark sql\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Jeffrio": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/31", "title": "Update HiveDatabaseMetaData.java change the identifierQuoteString", "body": "according to this jira https://issues.apache.org/jira/browse/HIVE-6013\nhive use the backstick as the quotestring\nso, I think the getIdentifierQuoteString() function should return the backstick rather than the space\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dingguitao": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/30", "title": "allow Chinese characters in json keys", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "solzy": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/27", "title": "Branch 0.14", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "martinschaef": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/23", "title": "Minor bugs found with static analysis", "body": "I'm testing a static analysis tool, here are some of its findings.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "codingtony": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/20", "title": "HIVE 2304 : for hive2", "body": "Fix for HivePreparedStatement for the hive2 driver.\n\nApplied the same setObject() code that fixed  HIVE 2304 for hive1 driver.\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/49483235", "body": "This is the fix  release 0.13.1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/49483235/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "kozanitis": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/19", "title": "Add case sensitivity support to Parquet Serde.", "body": "This is a small modification to ParquetHiveSerde to accommodate case sensitive fields through a serde property. \n\nAn example use case in hive:\n   CREATE EXTERNAL TABLE IF NOT EXISTS T(fieldName1 string, fieldname2 int, fieldNAME3 string)\n   ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.SensitiveParquetHiveSerDe'\n   WITH SERDEPROPERTIES (\"casesensitive\"=\"fieldName1,fieldname2,fieldNAME3)\n   STORED AS INPUTFORMAT 'parquet.hive.DeprecatedParquetInputFormat'\n   OUTPUTFORMAT 'parquet.hive.DeprecatedParquetOutputFormat'\n   LOCATION '...';\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thaparraj": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/18", "title": "Branch 0.12", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xelzmm": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/15", "title": "fix bug when extract a list of list with list", "body": "line 250 should be out of the for-loop\n\nFor example \njson = '{\"h\":[1, [2, 3], {\"i\": 0}, [{\"p\": 11}, {\"p\": 12}, {\"pp\": 13}]}'\nget_json_object(json, '$.h[*][0]') should return back the first node(if exists)  of every childrenof '$.h'\nwhich specifically should be \n    [2,{\"p\":11}] \nbut hive returns only\n    2\n\nbecause when hive pick the node '2' out, the tmp_jsonList will change to a list only contains one node '2':\n    [2]\nthen it was assigned to variable jsonList, in the next loop, value of i would be 2 which is greater than the size(always 1) of jsonList, then the loop broke out.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lakshmi83": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/14", "title": "Branch 0.13", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zhenlohuang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/12", "title": "HIVE-5596:hive-default.xml.template is invalid", "body": "Fixed HIVE-5596:hive-default.xml.template is invalid.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rajeshbnagaraju": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/10", "title": "Branch 0.8 r2", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mattyb149": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/9", "title": "[HIVE-4806]: Added implementations of JDBC API methods to Hive and Hive 2", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "virajb": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/8", "title": "Intial Patch for HIVE-4331", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kmosley": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/5", "title": "HIVE-3236.1.patch.txt", "body": "This branch contains the patch from https://issues.apache.org/jira/browse/HIVE-3236\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "slycoder": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/2", "title": "HIVE-1360.  It has been a long-standing request for UDFs to be able to", "body": "access parameter values.  This not only enables significant performance\nimprovement possibilities, it also allows for fundamentally richer\nbehavior, such as allowing the output type of a UDF to depend on its\ninputs.\n\nThe strategy in this diff is to introduce the notion of a\nConstantObjectInspector, like a regular ObjectInspector except that it\nencapsulates a constant value and knows what this constant value is.\nThese COIs are created through a factory method by ExprNodeConstantDesc\nduring plan generation hence UDFs will be able to capture these constant\nvalues during the initialize phase.  Furthermore, because these\nConstantObjectInspectors are simply subinterfaces of ObjectInspector,\nUDFs which are not \"constant-aware\" receive ObjectInspectors which\nalso implement the same interfaces they are used to, so no special\nhandling needs to be done for existing UDFs.\n\nAn example UDF which uses this new functionality is also included in\nthis diff.  NAMED_STRUCT is like STRUCT except that it also allows users\nto specify the _names_ of the fields of the struct, something previously\nnot possible because the names of the fields must be known at compile\ntime.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jarcec": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/17135868", "body": "Hi Viraj,\nunfortunately we can't accept pull requests due to licensing issues. You need to attach your patch to the JIRA in order to enable Hive community to get it in.\n\nJarcec\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/17135868/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "brockn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/47619540", "body": "See https://issues.apache.org/jira/browse/HIVE-7303\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/47619540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "pavel-sakun": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/47620553", "body": "Closing this one in favour of HIVE-7303\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/47620553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "slavag-altiscale": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/52104547", "body": "please do not merge for now till we can get a release out.  thanks\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/52104547/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "srowen": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/66122047", "body": "PS I found this from your cross-posted question about Kendall coefficient on Hive. What is this? PRs aren't for questions and this is some arbitrary branch of commits you've opened.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66122047/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "rdblue": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/66392346", "body": "Hi @dguy, I've been working on this problem recently in [HIVE-8909](https://issues.apache.org/jira/browse/HIVE-8909). That's already been merged, but I'm wondering if there's a case that we've missed that you fix here?\n\nIt looks like this allows the object inspector to work with the extra level of nesting that is included by the deserializer or detect if it isn't there. For HIVE-8909, we opted to keep the extra level of nesting just so we wouldn't need to change the object inspector code. We will need to follow up with a commit that removes the added layer from both. Did you come across a case where the added layer isn't there?\n\nAlso, I don't think Hive uses pull requests, so the best way to follow up is to open an issue in the JIRA issue tracker: https://issues.apache.org/jira/browse/HIVE\n\nThanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66392346/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/66649972", "body": "Hi Damian, thanks for following up and letting us know it's working!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66649972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dguy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/66427937", "body": "Hi Ryan,\n\nThanks for getting in touch. We are writing avro/parquet data-sets from\nvarious Map/Reduce jobs - we then create external hive tables for these\ndata-sets. The issue we are seeing is that AvroSchemaConverter\nhttps://github.com/apache/incubator-parquet-mr/blob/master/parquet-avro/src/main/java/parquet/avro/AvroSchemaConverter.java\nconverts avro array types like this:\n\n{\n     \"name\" : \"myarray\",\n     \"type\" : {\n       \"type\" : \"array\",\n       \"items\" : \"int\"\n }\n\ninto Parquet:\nrequired group myarray (LIST) {\n       repeated int32 array;\n }\n\nYou can see an example here\nhttps://github.com/apache/incubator-parquet-mr/blob/master/parquet-avro/src/test/java/parquet/avro/TestAvroSchemaConverter.java\n\nWhere as hive is expecting something like:\nrequired group myarray  {\n       repeated group bag {\n          optional int32 array;\n       }\n }\n\nWithout this change our external tables with array types currently won't\ndeserialize. So my patch was an attempt to make it work for both cases i\nknow about. I'll have to have a look through your patch and see if it\nhelps.\n\nThanks for the pointer.\nCheers,\nDamian\n\nOn 10 December 2014 at 01:50, Ryan Blue notifications@github.com wrote:\n\n> Hi @dguy https://github.com/dguy, I've been working on this problem\n> recently in [HIVE-8909|https://issues.apache.org/jira/browse/HIVE-8909].\n> That's already been merged, but I'm wondering if there's a case that we've\n> missed that you fix here?\n> \n> It looks like this allows the object inspector to work with the extra\n> level of nesting that is included by the deserializer or detect if it isn't\n> there. For HIVE-8909, we opted to keep the extra level of nesting just so\n> we wouldn't need to change the object inspector code. We will need to\n> follow up with a commit that removes the added layer from both. Did you\n> come across a case where the added layer isn't there?\n> \n> Also, I don't think Hive uses pull requests, so the best way to follow up\n> is to open an issue in the JIRA issue tracker:\n> https://issues.apache.org/jira/browse/HIVE\n> \n> Thanks!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/hive/pull/26#issuecomment-66392346.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66427937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/66442730", "body": "Hi,\nI've verified it works fine with your fix. Thanks for pointing me to this.\nCheers,\nDamian\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66442730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "swarnim87": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/105753655", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/105753655/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/31102308", "body": "Finish javadoc\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/31102308/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "Lewuathe": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/153306402", "body": "Sorry for mistake.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/153306402/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "joshelser": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/189366513", "body": "Thanks for submitting a PR for this, @chutium. I left you some comments.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/189366513/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/54267867", "body": "Isn't the `RecordWriter<Text, Mutation>` duplicative since `AccumuloRecordWriter` already implements that?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54267867/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54268048", "body": "This seems wrong. For the AccumuloRecordWriter, the `Text` can signify an Accumulo table to which this update should be written. Where is this `Writable` coming from?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54268048/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "gliptak": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/217591832", "body": "I might pick this up at a later date.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/217591832/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dmtolpeko": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/222104191", "body": "Can you create a Hive JIRA for this problem? Github is just a mirror\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222104191/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/222110901", "body": "I will review your changes and create a patch. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222110901/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "mbalassi": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/242540267", "body": "Thanks for shepherding the PR, @prasanthj.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/242540267/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/242546177", "body": "Sure thing.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/242546177/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "vishurkamble": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/255540045", "body": "Can somebody tell me, is UDF running in hive or not?  I tried some code fix but no luck. Still getting same error: PFB\n2016-10-22 09:49:54,311 WARN  [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.\n\nLogging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\nAdded [/home/cloudera/hplsql-0.3.17/hplsql.jar] to class path\nAdded resources: [/home/cloudera/hplsql-0.3.17/hplsql.jar]\nAdded [/home/cloudera/hplsql-0.3.17/antlr-runtime-4.5.jar] to class path\nAdded resources: [/home/cloudera/hplsql-0.3.17/antlr-runtime-4.5.jar]\nAdded resources: [/home/cloudera/hplsql-0.3.17/hplsql-site.xml]\nAdded resources: [/home/cloudera/hplsql-0.3.17/.hplsqlrc]\nOK\nTime taken: 1.294 seconds\nOK\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/lib/parquet/lib/parquet-hadoop-bundle-1.5.0-cdh5.8.0.jar!/shaded/parquet/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/lib/parquet/lib/parquet-format-2.1.0-cdh5.8.0.jar!/shaded/parquet/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/lib/parquet/lib/parquet-pig-bundle-1.5.0-cdh5.8.0.jar!/shaded/parquet/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/lib/hive/lib/hive-exec-1.1.0-cdh5.8.0.jar!/shaded/parquet/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/lib/hive/lib/hive-jdbc-1.1.0-cdh5.8.0-standalone.jar!/shaded/parquet/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [shaded.parquet.org.slf4j.helpers.NOPLoggerFactory]\nConfiguration file: file:/home/cloudera/hplsql-0.3.17/hplsql-site.xml\nParser tree: (program (block (stmt (expr_stmt (expr (expr_atom (ident hello))))) (stmt (expr_stmt (expr (expr_atom (ident [:1])))))))\nINLCUDE CONTENT .hplsqlrc (non-empty)\nLn:1 CREATE FUNCTION hello\nFailed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating hplsql\nTime taken: 1.129 seconds\nWARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.\nWARN: Please see http://www.slf4j.org/codes.html#release for an explanation.\nOct 22, 2016 9:50:12 AM WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\nOct 22, 2016 9:50:12 AM INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 53 records.\nOct 22, 2016 9:50:12 AM INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block\nOct 22, 2016 9:50:12 AM INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 33 ms. row count = 53\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/255540045/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dongjinleekr": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/260227633", "body": "No problem - this Pull Request solves [HIVE-15191](https://issues.apache.org/jira/browse/HIVE-15191).\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/260227633/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/260231316", "body": "Thanks for your kind guidance. I just submitted the patch following the guide.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/260231316/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "lidialiker": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/261580375", "body": "any chance this is getting merged? I'm seeing this exception as well when using ORC file format in Pig with OrcStorage.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/261580375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/261585741", "body": "@szador thanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/261585741/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "zeroflag": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/278032357", "body": "someone has fixed it since than", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/278032357/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "wangyum": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/338444975", "body": "Pushed to master", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/338444975/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "pvillard31": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/342746732", "body": "Merged with 945404273f330f6d1d316465baf540c7fec04d3a.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/342746732/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "Aegeaner": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/346938572", "body": "already merged to master branch.", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/346938572/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "johannes-altiscale": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/356024825", "body": "https://issues.apache.org/jira/browse/HIVE-18293 committed to master", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/356024825/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "chandulal": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/359227460", "body": "solving merge conflicts ", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/359227460/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "yoni": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/2375698", "body": "This change breaks development on OS X. Not sure if anyone else is having the same issues I am, but I can't seem to work around this. `git status` constantly reports changes have been made:\n\n```\n$ git status\n# On branch trunk\n# Changes not staged for commit:\n#   (use \"git add <file>...\" to update what will be committed)\n#   (use \"git checkout -- <file>...\" to discard changes in working directory)\n#\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/StandardUnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/UnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/ListTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/MapTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/PrimitiveTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/StructTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoFactory.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java\n#   modified:   serde/src/test/org/apache/hadoop/hive/serde2/thrift_test/CreateSequenceFile.java\n#\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n$ git reset --hard\nHEAD is now at af0162d HIVE-446 Implement TRUNCATE\n$ git status\n# On branch trunk\n# Changes not staged for commit:\n#   (use \"git add <file>...\" to update what will be committed)\n#   (use \"git checkout -- <file>...\" to discard changes in working directory)\n#\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/StandardUnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/UnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/ListTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/MapTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/PrimitiveTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/StructTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoFactory.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java\n#   modified:   serde/src/test/org/apache/hadoop/hive/serde2/thrift_test/CreateSequenceFile.java\n#\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/2375698/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "sameeragarwal": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/3868174", "body": "Shouldn't this be: \n\n``` java\nmyagg.variance += varianceFieldOI.get(partialVariance)\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/3868174/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "ksumit": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/5973358", "body": "shouldn't this be 0.13.0 for this branch? i tried compiling the current tip, it's broken and changing it to 0.13.0 works just fine.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/5973358/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "HZMengYue": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/8795892", "body": "hi,i checkout a spark branch today,and after built it,derby-10.11.1.1.jar in hive dir lib,but when i run hive shell ,like  ./hive --auxpath /home/hispark/spark-1.2.0/assembly/target/scala-2.10/spark-assembly-1.2.0-hadoop2.4.0.jar,it give me follow error:and i think u had fixed this problem.\nLogging initialized using configuration in file:/home/hispark/apache-hive-0.15.0-SNAPSHOT-bin/conf/hive-log4j.properties\nException in thread \"main\" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:449)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:634)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:578)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1481)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:64)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:74)\n        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2674)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2693)\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:430)\n        ... 7 more\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1479)\n        ... 12 more\nCaused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory\nNestedThrowables:\njava.lang.reflect.InvocationTargetException\n        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n        at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n        at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n        at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:341)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:370)\n        at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:267)\n        at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:234)\n        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\n        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:572)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:550)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:603)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:441)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5598)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:182)\n        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)\n        ... 17 more\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n        at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)\n        at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282)\n        at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240)\n        at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n        at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n        at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n        at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n        ... 46 more\nCaused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.derby.jdbc.EmbeddedDriver\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccess\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/8795892/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "sershe": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/9465637", "body": "should be gone\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/9465637/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/comments/9465752", "body": "this is wrong\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/9465752/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "jalajthanaki": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/11626965", "body": "I want to ask you ,Is there full text index kind of functionality available in Apache Hive for searching, matching & ranking keywords of the text?\n\nIf any functionality will be there then let me know.\n\nThanks,\nJalaj T\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/11626965/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "merlin-zaraza": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/14273837", "body": "Ok, initialize method is deprecated. How to use GenericUDTF now? (e.g. check for arguments)\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/14273837/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "walla2sl": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/14312112", "body": "I'm having the same issue. How is GenericUDTF usable now? After trying to use a UTDF I created, I keep getting \"Error while compiling statement: FAILED: IllegalStateException Should not be called directly\". \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/14312112/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/comments/14312367", "body": "I opened https://issues.apache.org/jira/browse/HIVE-12377\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/14312367/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "phoenixhadoop": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/15241484", "body": "I am using hive-2.0.0-SNAPSHOT\nwhen starting metastore I meet the ERROR: ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.\n\nwhen  I configure :export HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS -Dlog4j.configurationFile=hive-log4j2.xml \",the ERROR disappear.\n\nHow can I do?\n\nThank you very much.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/15241484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/comments/15244734", "body": "Hi,prasanthj \nNow we use hive-2.0.0-SNAPSHOT,we meet an error when starting the service metastore .\nThe error is :ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.\nIf we configure the variable HADOOP_CLIENT_OPTS  as\nexport HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS -Dlog4j.configurationFile=hive-log4j2.xml \" ,the error disappear,but entering hive cli slower.\nPlease help me how can avoid the ERROR.\nThank you very much.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/15244734/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "prongs": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/17350037", "body": "This is deleting `target/tmp/`causing all subsequent tests to fail. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17350037/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/comments/17363722", "body": "All of them!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17363722/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/comments/17363749", "body": "Check this: https://github.com/apache/hive/blob/master/pom.xml#L914. Then this: https://github.com/apache/hive/blob/master/pom.xml#L991\n\nFirst one sets up target/tmp/conf and second one is adding that to classpath. If the directory gets deleted, subsequent tests that create a `new HiveConf()` don't get to read `target/tmp/conf/hive-site.xml` and there are some test specific properties there. You can't just delete the directory and recreate. This should have been caught in review. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17363749/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "goyalr41": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/17575099", "body": "Hi,\n\nJust wanted to know how to access this function now? Should I flatten the map returned by new function getMapRedStats().\n\nThanks,\nRaman\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17575099/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "nguyenhoan": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/20889577", "body": "Hi @rbalamohan \r\nWe are a team of researchers from Iowa State, The University of Texs at Dallas and Oregon State University, USA. We are investigating common/repeated code changes.\r\nWe have four short questions regarding the change to this statement of the commit.\r\n\r\nQuestions:\r\n\r\nQ1- Is the change at these lines similar to another change from before? (yes, no, not sure)\r\n\r\nQ2- Can you briefly describe the change and why you made it? (for example, checking parameter before calling the method to avoid a Null Pointer Exception)\r\n\r\nQ3- Can you give it a name? (for example, Null Check)\r\n\r\nQ4- Would you like to have this change automated by a tool? (Yes, No, Already automated)\r\n\r\nThe data collected from the answers will never be associated with you or your project. Our questions are about recurring code changes from the developer community, not about personal information. All the data is merged across recurring changes from GitHub repositories. We will publish aggregated data from the trends of the whole community. \r\nWe have a long tradition of developing refactoring tools and contributing them freely to the Eclipse, Netbeans, Android Studio under their respective FLOSS licenses. For example, look at some of our recently released refactoring tools: http://refactoring.info/tools/ \r\n\r\nThank you,\r\nHoan Nguyen https://sites.google.com/site/nguyenanhhoan/\r\nMichael Hilton http://web.engr.oregonstate.edu/~hiltonm/\r\nTien Nguyen http://www.utdallas.edu/~tien.n.nguyen/\r\nDanny Dig http://eecs.oregonstate.edu/people/dig-danny\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/20889577/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "kiddingbaby": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/26133483", "body": "The property **hive.server.read.socket.timeout** not found in Wiki: Hive Configuration, can I use it in hive v1.2.1?", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/26133483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "julianhyde": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/75933141", "body": "Should be `switch (type)`?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/75933141/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/76060575", "body": "Ah, and now I see that I wrote that line before you copy-pasted it. Please fix it anyway!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/76060575/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77081655", "body": "s/accross/across/\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77081655/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77081708", "body": "s/uniformely/uniformly/\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/77081708/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "nishantmonu51": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/76488332", "body": "Druid also supports Jackson Smile format, If possible consider using that for better serde performance. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/76488332/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "erwa": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/86447007", "body": "I'm trying to backport this patch to Hive 1.1.0 and was wondering whether this change is required. With the change, some qtests, like `vectorization_part_project`, fail because `setVal()` throws a NPE because the `buffer` field in `BytesColumnVector` is `null`. Without this change, the qtest succeeds. Just wanted to know if there was some issue that this change was needed to fix.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86447007/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86467779", "body": "I figured out what was causing the NPE. HIVE-9937 (committed in Hive 1.2.0) added a line `vc.init();` to `VectorizedRowBatch.reset()`, which was needed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/86467779/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}}}}