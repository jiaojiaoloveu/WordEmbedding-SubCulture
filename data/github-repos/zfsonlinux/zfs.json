{"_default": {"1": {"voidzero": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7076", "title": "zfs iostat: zfs send does not seem to affect bandwidth rates", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Funtoo Linux (Gentoo derivative)\r\nLinux Kernel                 | 4.14.13\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7-release\r\nSPL Version                  | 0.7-release\r\nGCC version                  | 7.1.0\r\n\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\nI am running a `zfs send | zfs receive` command, from and to local disks. In the output of `zpool iostat -v -Td`, the write speed on the receiving disk shows 102M. But the sending disk shows a read of 2.58MB total.\r\n\r\nThe receiving vdev is a single disk. The sending vdev is a raidz1 with four disks. Each disk shows approx. 666K, so the total read speed is 2.58MB give or take.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7076/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7069", "title": "Compilation error kernel 4.14.13 and zfs 0.7-release", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Funtoo Linux (Gentoo derivative)\r\nLinux Kernel                 | 4.14.13\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7-release\r\nSPL Version                  | 0.7-release\r\nGCC version                  | 7.1.0\r\n\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\nCompilation output:\r\n\r\n```\r\nfs/zfs/zfs/metaslab.c:1052:2: error: positional initialization of field in \u2018struct\u2019 declared with \u2018designated_init\u2019 attribute [-Werror=designated-init]\r\n  metaslab_rt_create,\r\n  ^~~~~~~~~~~~~~~~~~\r\nfs/zfs/zfs/metaslab.c:1052:2: note: (near initialization for \u2018metaslab_rt_ops\u2019)\r\nfs/zfs/zfs/metaslab.c:1052:2: error: invalid initializer\r\nfs/zfs/zfs/metaslab.c:1052:2: note: (near initialization for \u2018metaslab_rt_ops.<anonymous>\u2019)\r\nfs/zfs/zfs/metaslab.c:1053:2: error: positional initialization of field in \u2018struct\u2019 declared with \u2018designated_init\u2019 attribute [-Werror=designated-init]\r\n  metaslab_rt_destroy,\r\n  ^~~~~~~~~~~~~~~~~~~\r\nfs/zfs/zfs/metaslab.c:1053:2: note: (near initialization for \u2018metaslab_rt_ops\u2019)\r\nfs/zfs/zfs/metaslab.c:1053:2: error: initialization from incompatible pointer type [-Werror=incompatible-pointer-types]\r\nfs/zfs/zfs/metaslab.c:1053:2: note: (near initialization for \u2018metaslab_rt_ops.rtop_add\u2019)\r\nfs/zfs/zfs/metaslab.c:1054:2: error: positional initialization of field in \u2018struct\u2019 declared with \u2018designated_init\u2019 attribute [-Werror=designated-init]\r\n  metaslab_rt_add,\r\n  ^~~~~~~~~~~~~~~\r\nfs/zfs/zfs/metaslab.c:1054:2: note: (near initialization for \u2018metaslab_rt_ops\u2019)\r\nfs/zfs/zfs/metaslab.c:1055:2: error: positional initialization of field in \u2018struct\u2019 declared with \u2018designated_init\u2019 attribute [-Werror=designated-init]\r\n  metaslab_rt_remove,\r\n  ^~~~~~~~~~~~~~~~~~\r\nfs/zfs/zfs/metaslab.c:1055:2: note: (near initialization for \u2018metaslab_rt_ops\u2019)\r\nfs/zfs/zfs/metaslab.c:1055:2: error: initialization from incompatible pointer type [-Werror=incompatible-pointer-types]\r\nfs/zfs/zfs/metaslab.c:1055:2: note: (near initialization for \u2018metaslab_rt_ops.rtop_destroy\u2019)\r\nfs/zfs/zfs/metaslab.c:1056:2: error: positional initialization of field in \u2018struct\u2019 declared with \u2018designated_init\u2019 attribute [-Werror=designated-init]\r\n  metaslab_rt_vacate\r\n  ^~~~~~~~~~~~~~~~~~\r\nfs/zfs/zfs/metaslab.c:1056:2: note: (near initialization for \u2018metaslab_rt_ops\u2019)\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7069/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "array42": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7075", "title": "scrub found mirrored file with checksum error on both copies", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Fedora\r\nDistribution Version    |  26\r\nLinux Kernel                 |  4.14.11-200.fc26.x86_6\r\nArchitecture                 |  x86_64\r\nZFS Version                  |  0.7.5-1 (zfs, 0.7.5, 4.14.11-200.fc26.x86_64, x86_64: installed)\r\nSPL Version                  |  0.7.5-1 (spl, 0.7.5, 4.14.11-200.fc26.x86_64, x86_64: installed)\r\n\r\n### Describe the problem you're observing\r\nI ran a scrub. A file turned out to have uncorrectable checksum errors. The error is listed to be in a snapshot. So does this also mean the same file from another snapshot is also defect?\r\n\r\nAt the same time I wonder how that can happen. The drives are HGST NAS drives, quite new. The machine has ECC memory. I just don't expect that both copies of a mirrored file contain a checksum error. That is the only error it found.\r\n\r\nAlso I did transfer the file in question with rsync and checksum check. So at that point it did verify the file after the transfer, so I was confident that the file is readable and everything should be well and just good as usual.\r\n\r\nSo.. is there an error with the checksum calculation itself? Impossible? Possible?\r\n### Describe how to reproduce the problem\r\nNo idea how to reproduce.\r\n### Include any warning/errors/backtraces from the system logs", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7075/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7073", "title": "selinux messages, invalid context, relabel inode or filesystem in question?", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Fedora\r\nDistribution Version    |  26\r\nLinux Kernel                 |  4.14.11-200.fc26.x86_6\r\nArchitecture                 |  x86_64\r\nZFS Version                  |  0.7.5-1 (zfs, 0.7.5, 4.14.11-200.fc26.x86_64, x86_64: installed)\r\nSPL Version                  |  0.7.5-1 (spl, 0.7.5, 4.14.11-200.fc26.x86_64, x86_64: installed)\r\n\r\n### Describe the problem you're observing\r\nsetroubleshootd is running at 100% CPU\r\ndmesg shows lots of messages like this, with different inodes and context messages, where context looks like file content or random bytes:\r\n```\r\nSELinux: inode=139911 on dev=zfs was found to have an invalid context=\\xe9\\xca\\xd2\\xcbx\\x94U.  This indicates you may need to relabel the inode or the filesystem in question.\r\n```\r\n### Describe how to reproduce the problem\r\nThe machine had a power loss, then just imported the pool back after a reboot. These errors started to show up. Did not do anything special. At the moment there is a scrub running, also some files are being copied to the pool. So what action should be taken?\r\n### Include any warning/errors/backtraces from the system logs", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6902", "title": "dkms modules not built automatically for Fedora update (or upgrade from 25 to 26)", "body": "Because of this Issue I did not upgrade to Fedora 27: https://github.com/zfsonlinux/zfs/issues/6814\r\n\r\nThe Upgrade from 25 to Version 26 did install the correct Version of zfs, but it did not build the dkms modules. I remember that I had to rebuild them once already manually after a normal upgrade.\r\n\r\nSome commands if you face the same situation:\r\ndkms status # nothing listed\r\ndkms autoinstall # did nothing\r\n(dkms remove ... if an old module exists in case of a normal update)\r\ndkms add -m spl -v 0.7.3\r\ndkms add -m zfs -v 0.7.3\r\ndkms install -m spl -v 0.7.3\r\ndkms install -m zfs -v 0.7.3", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "darrenfreeman": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7074", "title": "\"fg: no job control\" error on installing zfs .deb", "body": "Type                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian\r\nDistribution Version    | 9.3\r\nLinux Kernel                 | 4.9.65-3+deb9u2\r\nArchitecture                 | x86-64\r\nZFS Version                  | latest git\r\nSPL Version                  | latest git\r\n\r\nI've only recently switched to a .deb based system, so I'm sorry if I'm overlooking the obvious!\r\n\r\nI'm currently getting errors on running apt-get install:\r\n\r\n```\r\nroot@box:~# apt-get install edac-util\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nE: The package zfs needs to be reinstalled, but I can't find an archive for it.\r\n```\r\n\r\nI do remember ignoring some errors, so let's try this again:\r\n\r\n```\r\nroot@box:~# dpkg -i /opt/zfs/pkgs/zfs/zfs_0.7.0-255_amd64.deb \r\n(Reading database ... 192027 files and directories currently installed.)\r\nPreparing to unpack .../zfs/zfs_0.7.0-255_amd64.deb ...\r\nUnpacking zfs (0.7.0-255) over (0.7.0-255) ...\r\n/var/lib/dpkg/info/zfs.postrm: line 2: fg: no job control\r\ndpkg: warning: subprocess old post-removal script returned error exit status 1\r\ndpkg: trying script from the new package instead ...\r\n/var/lib/dpkg/tmp.ci/postrm: line 2: fg: no job control\r\ndpkg: error processing archive /opt/zfs/pkgs/zfs/zfs_0.7.0-255_amd64.deb (--install):\r\n subprocess new post-removal script returned error exit status 1\r\n/var/lib/dpkg/tmp.ci/postrm: line 2: fg: no job control\r\ndpkg: error while cleaning up:\r\n subprocess new post-removal script returned error exit status 1\r\nProcessing triggers for man-db (2.7.6.1-2) ...\r\nErrors were encountered while processing:\r\n /opt/zfs/pkgs/zfs/zfs_0.7.0-255_amd64.deb\r\n```\r\n\r\nAnd it's dead again. apt-get install fails again. What should I try next?", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7074/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7070", "title": "Silent drive failures with kernel debug messages", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian\r\nDistribution Version    | 9.3\r\nLinux Kernel                 | 4.9.0-5-amd64 #1 SMP Debian 4.9.65-3+deb9u2\r\nArchitecture                 | x86-64\r\nZFS Version                  | 0.7.5 (from tarball)\r\nSPL Version                  | 0.7.5 (from tarball)\r\n\r\n### Describe the problem you're observing\r\n\r\nI've moved to a new LSI2008 SAS controller, which I think is more reliable than the beehive of different cards I had before. However now I'm getting checksum errors and quite significant amounts of data needing to be resilvered, and it looks like one of the SSDs in the mirror is dropping out for days, before being readded on a reboot + zfs import. (Due to broken init scripts, zfs import is needed on every boot.)\r\n\r\nBut how can this be? Surely a drive falling off the controller shouldn't show as ONLINE, and no errors, during multiple scrubs. I scrub every day. Yet after a reboot, I've got nearly 100 GB of data to resilver.\r\n\r\nThis has happened enough times now, for me to think that internally, zfs is failing this drive and not considering it during future scrubs, which is why nothing ever needs repairing. But it's not presenting this to the user via zpool status. The drive is marked as ONLINE with no errors. Scrubs happen daily and nothing seems to be amiss. Then you reboot and zpool import, which clears the failed status, and wow, 40% of my dataset needs resilvering.\r\n\r\nI moved from an older git version about the same age as 0.7.2, to 0.7.5, at the same time as migrating the hardware. But the amount of data needing to be resilvered after this migration was such that it still looks like I had a silent drive failure for days or even weeks prior to the reboot required for migration to new hardware.\r\n\r\nWell, now the drive has been marked as UNAVAILABLE with checksum errors:\r\n\r\n```\r\n  pool: ssd\r\n state: DEGRADED\r\nstatus: One or more devices could not be used because the label is missing or\r\n        invalid.  Sufficient replicas exist for the pool to continue\r\n        functioning in a degraded state.\r\naction: Replace the device using 'zpool replace'.\r\n   see: http://zfsonlinux.org/msg/ZFS-8000-4J\r\n  scan: scrub repaired 0B in 0h53m with 0 errors on Sat Jan 20 08:08:30 2018\r\nconfig:\r\n\r\n        NAME        STATE     READ WRITE CKSUM\r\n        ssd         DEGRADED     0     0     0\r\n          mirror-0  DEGRADED     0     0     0\r\n            ssd1    UNAVAIL      0     0    14  corrupted data\r\n            ssd0    ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n```\r\n\r\nAt this point, I inspected /var/log/messages, and indeed this drive has thrown up a bunch of errors within the driver for the SAS card. About 15 minutes of very similar errors that look like the first few lines, followed by this:\r\n\r\n```\r\nJan 20 00:44:54 xxx kernel: [20008.105603] sd 0:0:2:0: [sdc] tag#3 FAILED Result: hostbyte=DID_SOFT_ERROR driverbyte=DRIVER_OK\r\nJan 20 00:44:54 xxx kernel: [20008.105608] sd 0:0:2:0: [sdc] tag#3 CDB: Write(10) 2a 00 32 71 17 88 00 00 10 00\r\nJan 20 00:44:54 xxx kernel: [20008.105636] sd 0:0:2:0: [sdc] tag#5 FAILED Result: hostbyte=DID_SOFT_ERROR driverbyte=DRIVER_OK\r\nJan 20 00:44:54 xxx kernel: [20008.105638] sd 0:0:2:0: [sdc] tag#5 CDB: Write(10) 2a 00 01 94 55 e0 00 00 08 00\r\nJan 20 00:44:55 xxx kernel: [20008.355524] mpt2sas_cm0: log_info(0x31120303): originator(PL), code(0x12), sub_code(0x0303)\r\nJan 20 00:44:55 xxx kernel: [20008.355530] sd 0:0:2:0: [sdc] tag#0 FAILED Result: hostbyte=DID_SOFT_ERROR driverbyte=DRIVER_OK\r\nJan 20 00:44:55 xxx kernel: [20008.355536] sd 0:0:2:0: [sdc] tag#0 CDB: Write(10) 2a 00 14 47 80 88 00 00 30 00\r\nJan 20 00:44:55 xxx kernel: [20008.607517] mpt2sas_cm0: log_info(0x31120303): originator(PL), code(0x12), sub_code(0x0303)\r\nJan 20 00:44:55 xxx kernel: [20008.607526] mpt2sas_cm0: log_info(0x31120303): originator(PL), code(0x12), sub_code(0x0303)\r\nJan 20 00:44:56 xxx kernel: [20009.358550] mpt2sas_cm0: log_info(0x31120303): originator(PL), code(0x12), sub_code(0x0303)\r\nJan 20 00:44:56 xxx kernel: [20009.358553] mpt2sas_cm0: log_info(0x31120303): originator(PL), code(0x12), sub_code(0x0303)\r\nJan 20 00:44:56 xxx kernel: [20009.558502] mpt2sas_cm0: log_info(0x31120303): originator(PL), code(0x12), sub_code(0x0303)\r\nJan 20 00:44:56 xxx kernel: [20009.558505] mpt2sas_cm0: log_info(0x31120303): originator(PL), code(0x12), sub_code(0x0303)\r\nJan 20 00:55:21 xxx kernel: [20634.996104] usb 4-2: USB disconnect, device number 2\r\nJan 20 04:15:12 xxx kernel: [32625.632170] z_zvol          D    0 30925      2 0x00000000\r\nJan 20 04:15:12 xxx kernel: [32625.632176]  ffffa0479ac48c00 0000000000000000 ffffa0466d46d200 ffffa047dfad8940\r\nJan 20 04:15:12 xxx kernel: [32625.632178]  ffffa047bbc401c0 ffffb6a309da3ca0 ffffffffb1a02923 ffffffffb1a04b5e\r\nJan 20 04:15:12 xxx kernel: [32625.632180]  0000000000000000 ffffa047dfad8940 ffffa047b5fa50d8 ffffa0466d46d200\r\nJan 20 04:15:12 xxx kernel: [32625.632183] Call Trace:\r\nJan 20 04:15:12 xxx kernel: [32625.632193]  [<ffffffffb1a02923>] ? __schedule+0x233/0x6d0\r\nJan 20 04:15:12 xxx kernel: [32625.632195]  [<ffffffffb1a04b5e>] ? mutex_lock+0xe/0x30\r\nJan 20 04:15:12 xxx kernel: [32625.632196]  [<ffffffffb1a02df2>] ? schedule+0x32/0x80\r\nJan 20 04:15:12 xxx kernel: [32625.632216]  [<ffffffffc0a795e6>] ? taskq_wait_outstanding+0x86/0xd0 [spl]\r\nJan 20 04:15:12 xxx kernel: [32625.632225]  [<ffffffffb14b86c0>] ? prepare_to_wait_event+0xf0/0xf0\r\nJan 20 04:15:12 xxx kernel: [32625.632357]  [<ffffffffc0c2824b>] ? zvol_task_cb+0x1fb/0x550 [zfs]\r\nJan 20 04:15:12 xxx kernel: [32625.632359]  [<ffffffffb1a0292b>] ? __schedule+0x23b/0x6d0\r\nJan 20 04:15:12 xxx kernel: [32625.632364]  [<ffffffffc0a7a2b6>] ? taskq_thread+0x286/0x460 [spl]\r\nJan 20 04:15:12 xxx kernel: [32625.632367]  [<ffffffffb14a10e0>] ? wake_up_q+0x70/0x70\r\nJan 20 04:15:12 xxx kernel: [32625.632372]  [<ffffffffc0a7a030>] ? task_done+0x90/0x90 [spl]\r\nJan 20 04:15:12 xxx kernel: [32625.632375]  [<ffffffffb1495ea7>] ? kthread+0xd7/0xf0\r\nJan 20 04:15:12 xxx kernel: [32625.632376]  [<ffffffffb1495dd0>] ? kthread_park+0x60/0x60\r\nJan 20 04:15:12 xxx kernel: [32625.632379]  [<ffffffffb1a07911>] ? ret_from_fork+0x41/0x50\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7070/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jhyeon": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7071", "title": "Suspend to disk does not work with zfs/spl 0.7.5", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Funtoo\r\nDistribution Version    | 11 Jan 2018\r\nLinux Kernel                 | 4.14.14\r\nArchitecture                 | Kaby Lake\r\nZFS Version                  | 0.7.5 from repo\r\nSPL Version                  | 0.7.5 from repo\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nAfter upgrading zfs/spl from 0.6.5.11 to 0.7.5, userland software suspend to disk stopped working.  The messages from `s2disk` on the console can be\r\n```\r\nLooking for splash system... none\r\ns2disk: Snapshotting system\r\ns2disk: System snapshot ready. Preparing to write\r\ns2disk: Image size: 2192764 kilobytes\r\ns2disk: Free swap: 64611752 kilobytes\r\ns2disk: Saving 548191 image data pages (press backspace to abort) ... 31%\r\n```\r\nThe progress in the last line increases up to 30% or so where `s2disk` fails to hibernate and returns.  A similar thing could happen with zfs/spl 0.6.5.11 (+ kernel 4.14.14) but `s2disk` completed hibernation if I repeated it a few more times.  With zfs/spl 0.7.5, the progress always stops around 30% even if I run `s2disk` multiple times in a row.  `echo 3 > /proc/sys/vm/drop_caches` does not help.\r\n\r\n### Describe how to reproduce the problem\r\n\r\n1. Fill the RAM to some extent by running several applications\r\n1. Run `s2disk`\r\n\r\nIf the RAM usage is very low e.g. just after booting the system, `s2disk` can happen to succeed.  The following is the output of `free` after an `s2disk` failure:\r\n```\r\n              total        used        free      shared  buff/cache   available\r\nMem:       32727300     1276112    31395968       30428       55220    31183392\r\nSwap:      67108860     2678104    64430756\r\n```\r\nMy `/etc/suspend.conf ` reads\r\n```\r\nsnapshot device = /dev/snapshot\r\nresume device = /dev/disk/by-label/swap\r\ncompress = y\r\nearly writeout = y\r\nsplash = y\r\nthreads = y\r\n```\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\nPart of `dmesg` output even though it contains no warning:\r\n```\r\n[  321.958683] Syncing filesystems ... \r\n[  327.083264] done.\r\n[  327.083266] Freezing user space processes ... (elapsed 0.001 seconds) done.\r\n[  327.085016] OOM killer disabled.\r\n[  327.085154] PM: Marking nosave pages: [mem 0x00000000-0x00000fff]\r\n[  327.085154] PM: Marking nosave pages: [mem 0x00058000-0x00058fff]\r\n[  327.085155] PM: Marking nosave pages: [mem 0x0008c000-0x000fffff]\r\n[  327.085156] PM: Marking nosave pages: [mem 0x65033000-0x65034fff]\r\n[  327.085157] PM: Marking nosave pages: [mem 0x6e972000-0x6fffefff]\r\n[  327.085211] PM: Marking nosave pages: [mem 0x70000000-0xffffffff]\r\n[  327.086056] PM: Basic memory bitmaps created\r\n[  327.086129] PM: Preallocating image memory... done (allocated 569280 pages)\r\n[  329.441452] PM: Allocated 2277120 kbytes in 2.35 seconds (968.98 MB/s)\r\n[  329.441453] Freezing remaining freezable tasks ... (elapsed 0.001 seconds) done.\r\n[  329.443328] wlan0: deauthenticating from a0:63:91:b1:fc:e0 by local choice (Reason: 3=DEAUTH_LEAVING)\r\n[  329.916974] ACPI: Preparing to enter system sleep state S4\r\n[  330.143422] ACPI: EC: event blocked\r\n[  330.143423] ACPI: EC: EC stopped\r\n[  330.143423] PM: Saving platform NVS memory\r\n[  330.143976] Disabling non-boot CPUs ...\r\n[  330.171136] smpboot: CPU 1 is now offline\r\n[  330.194461] smpboot: CPU 2 is now offline\r\n[  330.227780] smpboot: CPU 3 is now offline\r\n[  330.229875] PM: Creating hibernation image:\r\n[  330.541140] PM: Need to copy 547121 pages\r\n[  330.541141] PM: Normal pages needed: 547121 + 1024, available pages: 7779736\r\n[  331.100595] PM: Hibernation image created (547121 pages copied)\r\n[  330.230591] PM: Restoring platform NVS memory\r\n[  330.230786] ACPI: EC: EC started\r\n[  330.231760] Enabling non-boot CPUs ...\r\n[  330.231779] x86: Booting SMP configuration:\r\n[  330.231779] smpboot: Booting Node 0 Processor 1 APIC 0x2\r\n[  330.232340]  cache: parent cpu1 should not be sleeping\r\n[  330.232416] CPU1 is up\r\n[  330.232424] smpboot: Booting Node 0 Processor 2 APIC 0x1\r\n[  330.233076]  cache: parent cpu2 should not be sleeping\r\n[  330.233156] CPU2 is up\r\n[  330.233165] smpboot: Booting Node 0 Processor 3 APIC 0x3\r\n[  330.233757]  cache: parent cpu3 should not be sleeping\r\n[  330.233827] CPU3 is up\r\n[  330.236749] ACPI: Waking up from system sleep state S4\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7071/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tcaputi": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7066", "title": "dnode slots are never freed until the containing block is evicted", "body": "While writing #6864 @ahrens and I discovered an issue with dnode slots and zfs sends. The current code in `dnode_hold_impl()` sets up `dnode_handle_t`'s for each slot in a dnode block. However, these handles are not fully released when the dnode is destroyed; this only happens when the dnode block's dbuf is evicted. For the most part, this doesn't matter since normal filesystem datasets rarely reuse an old object number, zvols never create new objects after being set up, and `zfs recv` will call `dnode_reallocate()` to reuse an existing dnode if it needs to do so. However, in addition to code cleanliness concerns and the fact that this requires a workaround in #6864, we determined that this could be a problem when reusing multiple-slot dnodes as well:\r\n\r\nIf a zfs send stream includes instructions to reallocate a 2-slot dnode into 2 smaller dnodes (for instance), the second one cannot be reallocated since it's handle will never be unset from `DN_SLOT_INTERIOR` (until the containing dnode block is evicted). This is probably extremely rare since this is another special case on top of the previous prerequisites, but should be corrected.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7066/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6939", "title": "Some test cases do not pass with real disks ", "body": "The following tests seem to always fail when asking zfs-tests to use real disks instead of the default loop devices:\r\n\r\n- zdb_003_pos, zdb_004_pos, zdb_005_pos\r\n- zpool_create_001_pos, zpool_create_002_pos\r\n- inuse_005_pos, inuse_008_pos, inuse_009_pos\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 16.04\r\nLinux Kernel                 |  4.4.0-96-generic\r\nArchitecture                 |  x86_64\r\nZFS Version                  | 0.7.3\r\nSPL Version                  | 0.7.3\r\n\r\n### Describe the problem you're observing\r\n\r\nThe tests listed above seem to fail consistently on raw disks. \r\n\r\n### Describe how to reproduce the problem\r\n\r\nRun `zfs-tests.sh` while specifying real disks via the `$DISKS` environment variable.\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6939/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6898", "title": "DRR_BEGIN payload should use NV_ENCODE_XDR", "body": "### Describe the problem you're observing\r\nWhile working on verifying the on-disk changes in #6864, I spent some time testing sends from big endian to little endian systems and discovered that (in several cases) they were broken. The source of the problem seems to be that dmu_send.c does not pack the `DRR_BEGIN` payload in XDR format. As a result, the receive side cannot interpret this payload and returns `EOPNOTSUP`. This is easy enough to fix with the following patch:\r\n\r\n```\r\ndiff --git a/module/zfs/dmu_send.c b/module/zfs/dmu_send.c\r\nindex 76b9781..b16d9dd 100644\r\n--- a/module/zfs/dmu_send.c\r\n+++ b/module/zfs/dmu_send.c\r\n@@ -1125,7 +1125,8 @@ dmu_send_impl(void *tag, dsl_pool_t *dp, dsl_dataset_t *to_ds,\r\n                        fnvlist_add_nvlist(nvl, \"crypt_keydata\", keynvl);\r\n                }\r\n \r\n-               payload = fnvlist_pack(nvl, &payload_len);\r\n+               VERIFY0(nvlist_pack(nvl, (char **)&payload, &payload_len,\r\n+                   NV_ENCODE_XDR, KM_SLEEP));\r\n                drr->drr_payloadlen = payload_len;\r\n                fnvlist_free(keynvl);\r\n                fnvlist_free(nvl);\r\n```\r\n\r\nThe only issue is that this technically constitutes an ABI / format change. The good news, however, is that even older code should be able to understand this change since the `nvlist_unpack()` function reads the encoding.\r\n\r\n### Describe how to reproduce the problem\r\nAttempt a resumed or raw send from a big endian system to a little endian one (or visa versa)\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6898/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6845", "title": "Encrypted Indirect BPs erroneously MAC byteorder and compression bits", "body": "While looking into #6806 I discovered 2 small errors with the on-disk format for encrypted datasets that present problems with regards to raw sends. Indirect BPs include a checksum-of-MACs of a few fields in all of the BPs below. The way this is supposed to work is that the checksum-of-MACs only protects fields which can be preserved when doing a raw `zfs send -w`. However, the bug is that compression and byte order are included in these MACs, which is not portable to other systems.\r\n\r\nOn its own, this wouldn't be a big problem. We could simply adjust the on-disk format so that it overrides the real values with LZ4 compression and little endian byte order in all cases, since these 2 values are by far the mostly commonly used in production. This would mean virtually nobody would notice the on-disk format \"change\". Unfortunately, there is another much less serious bug where indirect dnode blocks are not getting compressed. The way that these 2 bugs interact would require us to always disable compression for encrypted indirect dnode blocks which could have a significant performance impact.\r\n\r\nI am currently working on a patch to correct this issue, although it will almost definitely require breaking existing pools that are using encryption. I am creating this ticket to help people watch the progress on this issue and to try to address any concerns they may have.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6845/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6188", "title": "Processes hang in uninterruptible sleep when trying to read flock'd file.", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Ubuntu\r\nDistribution Version    |  16.04\r\nLinux Kernel                 |  4.4.0-24-generic\r\nArchitecture                 |  x86_64\r\nZFS Version                  |  0.6.5.8\r\nSPL Version                  |  0.6.5.8\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nThe system running zfs was reported to me when it was observed that there were several processes hanging that were all non-responsive and unkillable. After looking around a little bit, it seemed that the system had several processes that were hanging in uninterruptible sleep trying to `cat` a file on the zpool that is managed under an advisory lock. Looking at htop, I noticed that the `txg_quiesce` thread was persistently using about 80% CPU. Other processes were also using a lot of CPU power, but as of now it is unclear if that is related to this bug. When I took a look at the txg history in `/proc`, I noticed that transaction groups were flying by at a rate of tens per second. During all of this, the disks were not under any significant load (almost completely idle).\r\n\r\n### Describe how to reproduce the problem\r\nAt this time I do not have specific commands to replicate this.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\nI was able to get the kern.log output of `echo t > /proc/sysrq-trigger`, but this is a bit big to paste here and I was unable to find anything particularly interesting there. The system was rebooted before I had the chance to collect more data.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6188/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6141", "title": "ASSERT panic in zpool_upgrade_004_pos.ksh", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | \r\nDistribution Version    | \r\nLinux Kernel                 | \r\nArchitecture                 | \r\nZFS Version                  | \r\nSPL Version                  | \r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nzpool_upgrade_004_pos.ksh spuriously triggers an ASSERT  on buildbot.\r\n\r\n### Describe how to reproduce the problem\r\nProblem can be reproduced via automated buildbot testing occasionally and by running the `zpool_upgrade` zfs-tests in a loop if you're lucky.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n```\r\n[ 8477.027346] VERIFY3(offset + size <= sm->sm_start + sm->sm_size) failed (6755399466238464 <= 25690112)\r\n[ 8477.032642] PANIC at space_map.c:115:space_map_load()\r\n[ 8477.035452] Showing stack for process 3322\r\n[ 8477.035456] CPU: 1 PID: 3322 Comm: z_wr_iss Tainted: P           OE   4.9.2-kmemleak #1\r\n[ 8477.035457] Hardware name: Xen HVM domU, BIOS 4.2.amazon 02/16/2017\r\n[ 8477.035459]  ffffae1dcf03f958 ffffffff98619156 ffffffffc1052519 0000000000000073\r\n[ 8477.035464]  ffffae1dcf03f968 ffffffffc0c65802 ffffae1dcf03faf0 ffffffffc0c65ad9\r\n[ 8477.035468]  ffff9556dcc07dc0 ffffae1d00000030 ffffae1dcf03fb00 ffffae1dcf03faa0\r\n[ 8477.035472] Call Trace:\r\n[ 8477.035480]  [<ffffffff98619156>] dump_stack+0xb0/0xea\r\n[ 8477.035495]  [<ffffffffc0c65802>] spl_dumpstack+0x62/0x70 [spl]\r\n[ 8477.035503]  [<ffffffffc0c65ad9>] spl_panic+0xd9/0x120 [spl]\r\n[ 8477.035509]  [<ffffffff98c9e387>] ? ftrace_call+0x5/0x34\r\n[ 8477.035518]  [<ffffffffc0c65a05>] ? spl_panic+0x5/0x120 [spl]\r\n[ 8477.035635]  [<ffffffffc0ec7986>] space_map_load+0x3d6/0x7e0 [zfs]\r\n[ 8477.035713]  [<ffffffffc0e9190b>] metaslab_load+0x7b/0x280 [zfs]\r\n[ 8477.035789]  [<ffffffffc0e91e78>] metaslab_activate+0x128/0x1e0 [zfs]\r\n[ 8477.035864]  [<ffffffffc0e95b22>] metaslab_alloc+0x952/0x1f10 [zfs]\r\n[ 8477.035945]  [<ffffffffc0f58b37>] zio_dva_allocate+0x217/0xc70 [zfs]\r\n[ 8477.035949]  [<ffffffff98c9e387>] ? ftrace_call+0x5/0x34\r\n[ 8477.035958]  [<ffffffffc0c6aa42>] ? tsd_hash_search.isra.0+0x52/0xf0 [spl]\r\n[ 8477.036037]  [<ffffffffc0f58925>] ? zio_dva_allocate+0x5/0xc70 [zfs]\r\n[ 8477.036046]  [<ffffffffc0c60290>] ? taskq_member+0x20/0x40 [spl]\r\n[ 8477.036146]  [<ffffffffc0f4fea9>] zio_execute+0x149/0x3f0 [zfs]\r\n[ 8477.036156]  [<ffffffffc0c617ed>] taskq_thread+0x36d/0x760 [spl]\r\n[ 8477.036161]  [<ffffffff9810bc80>] ? try_to_wake_up+0x680/0x680\r\n[ 8477.036169]  [<ffffffffc0c61480>] ? taskq_thread_spawn+0x80/0x80 [spl]\r\n[ 8477.036172]  [<ffffffff980fabd9>] kthread+0x119/0x150\r\n[ 8477.036175]  [<ffffffff980faac0>] ? kthread_flush_work+0x1f0/0x1f0\r\n[ 8477.036177]  [<ffffffff98c9ccf5>] ret_from_fork+0x25/0x30\r\n```\r\n\r\nThis refers to this bit of code in `space_map_load()`:\r\n```\r\n\tfor (entry = entry_map; entry < entry_map_end; entry++) {\r\n\t\tuint64_t e = *entry;\r\n\t\tuint64_t offset, size;\r\n\r\n\t\tif (SM_DEBUG_DECODE(e))\t\t/* Skip debug entries */\r\n\t\t\tcontinue;\r\n\r\n\t\toffset = (SM_OFFSET_DECODE(e) << sm->sm_shift) +\r\n\t\t    sm->sm_start;\r\n\t\tsize = SM_RUN_DECODE(e) << sm->sm_shift;\r\n\r\n\t\tVERIFY0(P2PHASE(offset, 1ULL << sm->sm_shift));\r\n\t\tVERIFY0(P2PHASE(size, 1ULL << sm->sm_shift));\r\n\t\tVERIFY3U(offset, >=, sm->sm_start);\r\n\t\tVERIFY3U(offset + size, <=, sm->sm_start + sm->sm_size); /* PANIC here */\r\n\t\tif (SM_TYPE_DECODE(e) == maptype) {\r\n\t\t\tVERIFY3U(range_tree_space(rt) + size, <=,\r\n\t\t\t    sm->sm_size);\r\n\t\t\trange_tree_add(rt, offset, size);\r\n\t\t} else {\r\n\t\t\trange_tree_remove(rt, offset, size);\r\n\t\t}\r\n\t}\r\n```\r\n\r\n\r\nI dug into the stack trace a little bit and I have some info that may be useful although I could not identify the root cause. The `VERIFY3U` is checking to make sure this spacemap segment is not going beyond the spacemap's upper bound. The endpoint of the segment is shown as `6755399466238464` which is a lot larger than `25690112`.\r\n\r\nIn binary` 6755399466238464 = 0b1000000000000000000000000001100000000100001000000000`, and if you remove the top bit the number becomes `25182720` which looks like a much more reasonable number. If I'm counting correctly, the top bit is number 51. So hopefully now debugging this should just be a matter of answering the question \"what is setting bit 51 during a zpool upgrade?\"\r\n\r\nThis could be related to #4034 as well.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6141/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6104", "title": "zfs_domount() performs double dmu_objset_disown() on error", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\nAll systems\r\n\r\n### Describe the problem you're observing\r\nThe `zfs_domount()` code path has some dubious error handling that will lead to a second call to `dmu_objset_disown()` in a few cases, most prominently if there is an error creating the root directory. The first disown happens in the the call to `zfs_umount()` and the second occurs in the `if (error)` block.\r\n\r\n### Describe how to reproduce the problem\r\nI experienced this while building out the encryption patch. A bug in my code caused the root directory to be unreadable on mount, which triggered an ASSERT in `dmu_objset_disown()` when trying to disown an already disowned dataset.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6104/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/44b61ea506212c287333e03d2cf8933216810800", "message": "Remove empty files accidentally added by a8b2e306 \n\nThis patch simply removes 2 empty files that were accidentally\r\nadded a part of the scrub priority patch.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Tom Caputi <tcaputi@datto.com>\r\nCloses #6990"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/a8b2e30685c9214ccfd0181977540e080340df4e", "message": "Support re-prioritizing asynchronous prefetches\n\nWhen sequential scrubs were merged, all calls to arc_read()\r\n(including prefetch IOs) were given ZIO_PRIORITY_ASYNC_READ.\r\nUnfortunately, this behaves badly with an existing issue where\r\nprefetch IOs cannot be re-prioritized after the issue. The\r\nresult is that synchronous reads end up in the same vdev_queue\r\nas the scrub IOs and can have (in some workloads) multiple\r\nseconds of latency.\r\n\r\nThis patch incorporates 2 changes. The first ensures that all\r\nscrub IOs are given ZIO_PRIORITY_SCRUB to allow the vdev_queue\r\ncode to differentiate between these I/Os and user prefetches.\r\nSecond, this patch introduces zio_change_priority() to provide\r\nthe missing capability to upgrade a zio's priority.\r\n\r\nReviewed by: George Wilson <george.wilson@delphix.com>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Tom Caputi <tcaputi@datto.com>\r\nCloses #6921 \r\nCloses #6926"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d4677269f286005768ae1a0fcd3389aa6015c4c7", "message": "Unbreak the scan status ABI\n\nWhen d4a72f23 was merged, pss_pass_issued was incorrectly\r\nadded to the middle of the pool_scan_stat_t structure\r\ninstead of the end. This patch simply corrects this issue.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Tom Caputi <tcaputi@datto.com>\r\nCloses #6909"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d4a72f23863382bdf6d0ae33196f5b5decbc48fd", "message": "Sequential scrub and resilvers\n\nCurrently, scrubs and resilvers can take an extremely\r\nlong time to complete. This is largely due to the fact\r\nthat zfs scans process pools in logical order, as\r\ndetermined by each block's bookmark. This makes sense\r\nfrom a simplicity perspective, but blocks in zfs are\r\noften scattered randomly across disks, particularly\r\ndue to zfs's copy-on-write mechanisms.\r\n\r\nThis patch improves performance by splitting scrubs\r\nand resilvers into a metadata scanning phase and an IO\r\nissuing phase. The metadata scan reads through the\r\nstructure of the pool and gathers an in-memory queue\r\nof I/Os, sorted by size and offset on disk. The issuing\r\nphase will then issue the scrub I/Os as sequentially as\r\npossible, greatly improving performance.\r\n\r\nThis patch also updates and cleans up some of the scan\r\ncode which has not been updated in several years.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nAuthored-by: Saso Kiselkov <saso.kiselkov@nexenta.com>\r\nAuthored-by: Alek Pinchuk <apinchuk@datto.com>\r\nAuthored-by: Tom Caputi <tcaputi@datto.com>\r\nSigned-off-by: Tom Caputi <tcaputi@datto.com>\r\nCloses #3625 \r\nCloses #6256"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/62df1bc813e6972130fffa91c6999a8e5ef80afd", "message": "Fix encryption root hierarchy issue\n\nAfter doing a recursive raw receive, zfs userspace performs\r\na final pass to adjust the encryption root hierarchy as\r\nneeded. Unfortunately, the FORCE_INHERIT ioctl had a bug\r\nwhich caused the encryption root to always be assigned to\r\nthe direct parent instead of the inheriting parent. This\r\npatch simply fixes this issue.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Alek Pinchuk <apinchuk@datto.com>\r\nSigned-off-by: Tom Caputi <tcaputi@datto.com>\r\nCloses #6847 \r\nCloses #6848"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/35df0bb5565c81ba52de430108b37eced9de0606", "message": "Fix ASSERT in dmu_free_long_object_raw()\n\nThis small patch fixes an issue where dmu_free_long_object_raw()\r\ncalls dnode_hold() after freeing the dnode a line above.\r\n\r\nReviewed-by: Jorgen Lundman <lundman@lundman.net>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Tom Caputi <tcaputi@datto.com>\r\nCloses #6766"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/9bae371ce69187f14e15129173ba0b138a965ada", "message": "Fix for #6714\n\nThis 2 line patch fixes a possible integer overflow reported by grsec.\n\nSigned-off-by: Tom Caputi <tcaputi@datto.com>"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/2637dda8f80dbd49dd0512c74108ff977dfb8c7b", "message": "Fix for #6706\n\nThis patch resolves an issue where raw sends would fail to send\nencryption parameters if the wrapping key was unloaded and reloaded\nbefore the data was sent and the dataset wass not an encryption root.\nThe code attempted to lookup the values from the wrapping key which\nwas not being initialized upon reload. This change forces the code to\nlookup the correct value from the encryption root's DSL Crypto Key.\nUnfortunately, this issue led to the on-disk DSL Crypto Key for some\nnon-encryption root datasets being left with zeroed out encryption\nparameters. However, this should not present a problem since these\nvalues are never looked at and are overrwritten upon changing keys.\n\nThis patch also fixes an issue where raw, resumable sends were not\nbeing cleaned up appropriately if an invalid DSL Crypto Key was\nreceived.\n\nSigned-off-by: Tom Caputi <tcaputi@datto.com>"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/b135b9f11ad15823d92f8ca3f40fcdd91690677d", "message": "Fix for #6703\n\nThis patch resolves an issue where spa_keystore_change_key_sync_impl()\nincorrectly recursed into clone DSL Directories while recursively\nrewrapping encryption keys. Clones share keys with their origins, so\nthis logic was incorrect.\n\nSigned-off-by: Tom Caputi <tcaputi@datto.com>"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/440a3eb939441a42ab5029e5e64498d802fa276b", "message": "Fixes for #6639\n\nSeveral issues were uncovered by running stress tests with zfs\nencryption and raw sends in particular. The issues and their\nassociated fixes are as follows:\n\n* arc_read_done() has the ability to chain several requests for\n  the same block of data via the arc_callback_t struct. In these\n  cases, the ARC would only use the first request's dsobj from\n  the bookmark to decrypt the data. This is problematic because\n  the first request might be a prefetch zio which is able to\n  handle the key not being loaded, while the second might use a\n  different key that it is sure will work. The fix here is to\n  pass the dsobj with each individual arc_callback_t so that each\n  request can attempt to decrypt the data separately.\n\n* DRR_FREE and DRR_FREEOBJECT records in a send file were not\n  having their transactions properly tagged as raw during raw\n  sends, which caused a panic when the dbuf code attempted to\n  decrypt these blocks.\n\n* traverse_prefetch_metadata() did not properly set\n  ZIO_FLAG_SPECULATIVE when issuing prefetch IOs.\n\n* Added a few asserts and code cleanups to ensure these issues\n  are more detectable in the future.\n\nSigned-off-by: Tom Caputi <tcaputi@datto.com>"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/4807c0badb130ae70cf6f0887b4be1648f217f1a", "message": "Encryption patch follow-up\n\n* PBKDF2 implementation changed to OpenSSL implementation.\n\n* HKDF implementation moved to its own file and tests\n  added to ensure correctness.\n\n* Removed libzfs's now unnecessary dependency on libzpool\n  and libicp.\n\n* Ztest can now create and test encrypted datasets. This is\n  currently disabled until issue #6526 is resolved, but\n  otherwise functions as advertised.\n\n* Several small bug fixes discovered after enabling ztest\n  to run on encrypted datasets.\n\n* Fixed coverity defects added by the encryption patch.\n\n* Updated man pages for encrypted send / receive behavior.\n\n* Fixed a bug where encrypted datasets could receive\n  DRR_WRITE_EMBEDDED records.\n\n* Minor code cleanups / consolidation.\n\nSigned-off-by: Tom Caputi <tcaputi@datto.com>"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/94d49e8f9bd2e58a783066327c84b7d3b605ac0f", "message": "Relax ASSERT for #6526\n\nThis patch resolves a minor issue where an ASSERT in\nmetaslab_passivate() that only applies to non weight-based\nmetaslabs was erroneously applied to all metaslabs.\n\nSigned-off-by: Tom Caputi <tcaputi@datto.com>"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/9b8407638da71ea9f4afb21375f991869f19811f", "message": "Send / Recv Fixes following b52563\n\nThis patch fixes several issues discovered after\r\nthe encryption patch was merged:\r\n\r\n* Fixed a bug where encrypted datasets could attempt\r\n  to receive embedded data records.\r\n\r\n* Fixed a bug where dirty records created by the recv\r\n  code wasn't properly setting the dr_raw flag.\r\n\r\n* Fixed a typo where a dmu_tx_commit() was changed to\r\n  dmu_tx_abort()\r\n\r\n* Fixed a few error handling bugs unrelated to the\r\n  encryption patch in dmu_recv_stream()\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Tom Caputi <tcaputi@datto.com>\r\nCloses #6512 \r\nCloses #6524 \r\nCloses #6545"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/b52563034230b35f0562b6f40ad1a00f02bd9a05", "message": "Native Encryption for ZFS on Linux\n\nThis change incorporates three major pieces:\r\n\r\nThe first change is a keystore that manages wrapping\r\nand encryption keys for encrypted datasets. These\r\ncommands mostly involve manipulating the new\r\nDSL Crypto Key ZAP Objects that live in the MOS. Each\r\nencrypted dataset has its own DSL Crypto Key that is\r\nprotected with a user's key. This level of indirection\r\nallows users to change their keys without re-encrypting\r\ntheir entire datasets. The change implements the new\r\nsubcommands \"zfs load-key\", \"zfs unload-key\" and\r\n\"zfs change-key\" which allow the user to manage their\r\nencryption keys and settings. In addition, several new\r\nflags and properties have been added to allow dataset\r\ncreation and to make mounting and unmounting more\r\nconvenient.\r\n\r\nThe second piece of this patch provides the ability to\r\nencrypt, decyrpt, and authenticate protected datasets.\r\nEach object set maintains a Merkel tree of Message\r\nAuthentication Codes that protect the lower layers,\r\nsimilarly to how checksums are maintained. This part\r\nimpacts the zio layer, which handles the actual\r\nencryption and generation of MACs, as well as the ARC\r\nand DMU, which need to be able to handle encrypted\r\nbuffers and protected data.\r\n\r\nThe last addition is the ability to do raw, encrypted\r\nsends and receives. The idea here is to send raw\r\nencrypted and compressed data and receive it exactly\r\nas is on a backup system. This means that the dataset\r\non the receiving system is protected using the same\r\nuser key that is in use on the sending side. By doing\r\nso, datasets can be efficiently backed up to an\r\nuntrusted system without fear of data being\r\ncompromised.\r\n\r\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Jorgen Lundman <lundman@lundman.net>\r\nSigned-off-by: Tom Caputi <tcaputi@datto.com>\r\nCloses #494 \r\nCloses #5769"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/314be68aa93a543f89d7c0137f25848e4c692653", "message": "Fixed VERIFY3_IMPL() bug from 682ce104\n\nWhen VERIFY3_IMPL() was adjusted in 682ce104, the values of\r\nthe operands were omitted from the variadic arguments list.\r\nThis patch simply corrects this.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Tony Hutter <hutter2@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Tom Caputi <tcaputi@datto.com>\r\nCloses #6343"}], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6864", "title": "Encryption and Raw Send Stability Improvements", "body": "The current on-disk format for encrypted datasets protects\r\nnot only the encrypted and authenticated blocks, but also\r\nthe order and interpretation of these blocks. In order to\r\nmake this work while maintaining the ability to do raw sends\r\nthe indirect bps maintain a secure checksum of all the MACs\r\nin the block below it, along with a few other fields that\r\ndetermine how the data is interpretted.\r\n\r\nUnfortunately, the current on-disk format erroniously\r\nincludes some fields which are not portable and thus cannot\r\nsupport raw sends. It is also not possible to easily work\r\naround this issue due to a separate and much smaller bug\r\nwhich causes indirect blocks for encrypted dnodes to not\r\nbe compressed, which conflicts with the previous bug. In\r\naddition, raw send streams do not currently include\r\ndn_maxblkid which is needed in order to ensure that we are\r\ncorrectly maintaining the portable objset MAC.\r\n\r\nThis patch zero's out the offending fields when computing the\r\nbp MAC (as they should have been) and registers an errata for\r\nthe on-disk format bug. We detect the errata by adding a\r\n\"version\" field to newly created DSL Crypto Keys. We allow\r\ndatasets without a version (version 0) to only be mounted for\r\nread so that they can easily be migrated. We also now include\r\ndn_maxblkid in raw send streams to ensure the MAC can be\r\nmaintained correctly.\r\n\r\nNote that this fix has not yet been finalized and should not be used until it is tested, reviewed, and merged unless you are ok with losing your data.\r\n\r\nSigned-off-by: Tom Caputi <tcaputi@datto.com>\r\n\r\n### How Has This Been Tested?\r\nI have added a new test for raw sends that essentially stresses as many edge cases as I could think of. In addition, I have manually tested that the recovery process laid out in https://github.com/zfsonlinux/zfsonlinux.github.com/pull/35 works as advertised, and that both old and new datasets function predictably.\r\n\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [x] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [ ] I have updated the documentation accordingly.\r\n- [x] I have read the **CONTRIBUTING** document.\r\n- [x] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [x] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "prometheanfire": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7065", "title": "slub_debug=P causes zfs to oops/panic", "body": "`CONFIG_SLUB_DEBUG=y` kernel, with the `slub_debug=P` option on the kernel command line will cause it.  In my specific use case this occurred with ZoL master as of about 18-01-2018-22:00.  I'm using the encryption on disk format update from https://github.com/zfsonlinux/zfs/pull/6864 .\r\n\r\nThe specific trigger for this was to load-key (and successfully input the key).\r\n\r\nHere's a picture I took of the backtrace.\r\n\r\nhttps://photos.app.goo.gl/EpbictIs5dAVc0p03 and another https://photos.app.goo.gl/Goxd2GN7ZRk7fEFx2", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7065/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/7da8f8d81bf1fadc2d9dff10f0435fe601e919fa", "message": "Run zfs load-key if needed in dracut\n\n'zfs load-key -a' will only be called if needed.  If a dataset not\r\nneeded for boot does not have its key loaded (home directories for\r\nexample) boot can still continue.\r\n\r\nzfs:AUTO was not working via dracut, so we still need the generator\r\nscript to do its thing.\r\n\r\nReviewed-by: Richard Yao <ryao@gentoo.org>\r\nReviewed-by: Manuel Amador (Rudd-O) <rudd-o@rudd-o.com>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: loli10K <ezomori.nozomu@gmail.com>\r\nSigned-off-by: Matthew Thode <mthode@mthode.org>\r\nCloses #6982 \r\nCloses #7004"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/c10cdcb55f81ea773486161b31bc91bb7b58b4c8", "message": "Fix copy-builtin to work with ASAN patch\n\nCommit fed90353 didn't fully update the copy-builtin script\r\nas needed to perform in-kernel builds.  Add the missing\r\noptions and flags.\r\n\r\nReviewed by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Matthew Thode <mthode@mthode.org>\r\nCloses #7033 \r\nCloses #7037"}], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/7008", "title": "DNM: make zfs-mount service work with encryption", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "danielkza": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7059", "title": "Scrub gets stuck, becomes unstoppable and locks up user processes in uninterruptible sleep", "body": "Type                    | Version/Name\r\n---                     | ---\r\nDistribution Name       | Fedora\r\nDistribution Version    | 27\r\nLinux Kernel            | 4.14.13\r\nArchitecture            | x86_64\r\nZFS Version             | 0.7.5\r\nSPL Version             | 0.7.5\r\n\r\n### Describe the problem you're observing\r\n\r\nAfter a routine scrub starting on the background, some programs seem stuck in uninterruptible IO due to ZFS. Attempting\r\nto pause or stop the scrub does not work - the `zpool` command hangs and also becomes unkillable.\r\n\r\nHere is the `/proc/PID/stack` of the stuck `zpool`:\r\n\r\n```\r\n[<ffffffffc11f1c23>] cv_wait_common+0x113/0x130 [spl]\r\n[<ffffffffc11f1c55>] __cv_wait+0x15/0x20 [spl]\r\n[<ffffffffc182972d>] txg_wait_synced+0xdd/0x120 [zfs]\r\n[<ffffffffc1801f36>] dsl_sync_task+0x176/0x260 [zfs]\r\n[<ffffffffc180041e>] dsl_scrub_set_pause_resume+0x3e/0x40 [zfs]\r\n[<ffffffffc181e511>] spa_scrub_pause_resume+0x31/0x60 [zfs]\r\n[<ffffffffc1858f85>] zfs_ioc_pool_scan+0xb5/0xc0 [zfs]\r\n[<ffffffffc18592d6>] zfsdev_ioctl+0x1d6/0x600 [zfs]\r\n[<ffffffff9429f575>] do_vfs_ioctl+0xa5/0x610\r\n[<ffffffff9429fb59>] SyS_ioctl+0x79/0x90\r\n[<ffffffff94a0008d>] entry_SYSCALL_64_fastpath+0x20/0x83\r\n```\r\n\r\nAnd of one of the stuck user processes:\r\n\r\n```\r\n[<ffffffff940d7746>] io_schedule+0x16/0x40\r\n[<ffffffffc11f1bb9>] cv_wait_common+0xa9/0x130 [spl]\r\n[<ffffffffc11f1c98>] __cv_wait_io+0x18/0x20 [spl]\r\n[<ffffffffc187f7f2>] zio_wait+0xf2/0x1b0 [zfs]\r\n[<ffffffffc17c38d3>] dbuf_read+0x6e3/0x910 [zfs]\r\n[<ffffffffc17c5c19>] __dbuf_hold_impl+0x549/0x600 [zfs]\r\n[<ffffffffc17c5d71>] dbuf_hold_impl+0xa1/0xd0 [zfs]\r\n[<ffffffffc17c5e33>] dbuf_hold+0x33/0x60 [zfs]\r\n[<ffffffffc17cf1cd>] dmu_buf_hold_noread+0x8d/0x100 [zfs]\r\n[<ffffffffc17cf26f>] dmu_buf_hold+0x2f/0x80 [zfs]\r\n[<ffffffffc1845a5e>] zap_lockdir+0x4e/0xb0 [zfs]\r\n[<ffffffffc1845c3a>] zap_cursor_retrieve+0x17a/0x2e0 [zfs]\r\n[<ffffffffc1869abc>] zfs_readdir+0x13c/0x460 [zfs]\r\n[<ffffffffc1886911>] zpl_iterate+0x51/0x80 [zfs]\r\n[<ffffffff9429fce0>] iterate_dir+0x170/0x1a0\r\n[<ffffffff942a046a>] SyS_getdents+0xaa/0x140\r\n[<ffffffff94a0008d>] entry_SYSCALL_64_fastpath+0x20/0x83\r\n[<ffffffffffffffff>] 0xffffffffffffffff\r\n```\r\n\r\nHere is the affected pool status:\r\n\r\n```\r\n  pool: daniel-pc-media\r\n state: ONLINE\r\n  scan: scrub in progress since Thu Jan 18 03:29:02 2018\r\n    102G scanned out of 2,45T at 2,60M/s, 263h46m to go\r\n    0B repaired, 4,06% done\r\nconfig:\r\n\r\n    NAME                                 STATE     READ WRITE CKSUM\r\n    daniel-pc-media                      ONLINE       0     0     0\r\n      mirror-0                           ONLINE       0     0     0\r\n        ata-ST4000DM000-1F2168_Z301QGEZ  ONLINE       0     0     0\r\n        ata-ST4000DM000-1F2168_Z301QGCM  ONLINE       0     0     0\r\n```\r\n\r\nThere seems to be no progress actually being made, as none of the counters advance (other than the expected ETA).\r\n\r\n### Describe how to reproduce the problem\r\n\r\nNot able to so far. I can provide more observations of the running system if it doesn't force me to restart by\r\nbecoming unstable/unusable.\r\n\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\nNothing of interest or related to ZFS is present in the kernel logs.\r\nThe problem *might* have been triggered by suspending and resuming the computer, but I was not monitoring the scrub before that, so I can't be sure.\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7059/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "makhomed": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7057", "title": "tasks txg_sync and zfs blocked for more than 120 seconds", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  CentOS Linux\r\nDistribution Version    | 7.4.1708\r\nLinux Kernel                 |  3.10.0-693.11.6.el7\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.5-1\r\nSPL Version                  | 0.7.5-1\r\n\r\nZFS installed from zfs-kmod repo, ```baseurl=http://download.zfsonlinux.org/epel/7.4/kmod/$basearch/```\r\n\r\n### Describe the problem you're observing\r\n\r\nMessages in /var/log/messages about blocked tasks.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nSorry, but I do not found way how to reproduce this bug.\r\nMay be stack trace will help to find root cause of this bug?\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n```\r\n\r\nJan 17 17:26:45 kvm-hardware-node kernel: INFO: task txg_sync:10906 blocked for more than 120 seconds.\r\nJan 17 17:26:45 kvm-hardware-node kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJan 17 17:26:45 kvm-hardware-node kernel: txg_sync        D ffff883f6a256eb0     0 10906      2 0x00000000\r\nJan 17 17:26:45 kvm-hardware-node kernel: Call Trace:\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc04ddf57>] ? taskq_dispatch_ent+0x57/0x170 [spl]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffff816ab6d9>] schedule+0x29/0x70\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffff816a90e9>] schedule_timeout+0x239/0x2c0\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc07ba30f>] ? zio_taskq_dispatch+0x8f/0xa0 [zfs]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc07ba352>] ? zio_issue_async+0x12/0x20 [zfs]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc07bebcc>] ? zio_nowait+0xbc/0x150 [zfs]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffff816aac5d>] io_schedule_timeout+0xad/0x130\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffff810b31a6>] ? prepare_to_wait_exclusive+0x56/0x90\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffff816aacf8>] io_schedule+0x18/0x20\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc04e24a2>] cv_wait_common+0xb2/0x150 [spl]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffff810b34b0>] ? wake_up_atomic_t+0x30/0x30\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc04e2598>] __cv_wait_io+0x18/0x20 [spl]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc07be49b>] zio_wait+0x10b/0x1b0 [zfs]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc07346cf>] dsl_pool_sync+0xbf/0x440 [zfs]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc07527c7>] spa_sync+0x437/0xdf0 [zfs]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffff810c6452>] ? default_wake_function+0x12/0x20\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffff810bf074>] ? __wake_up+0x44/0x50\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc0766a91>] txg_sync_thread+0x301/0x510 [zfs]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc0766790>] ? txg_fini+0x2a0/0x2a0 [zfs]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc04dcfa1>] thread_generic_wrapper+0x71/0x80 [spl]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc04dcf30>] ? __thread_exit+0x20/0x20 [spl]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffff810b252f>] kthread+0xcf/0xe0\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffff810b2460>] ? insert_kthread_work+0x40/0x40\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffff816b8798>] ret_from_fork+0x58/0x90\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffff810b2460>] ? insert_kthread_work+0x40/0x40\r\nJan 17 17:26:45 kvm-hardware-node kernel: INFO: task zfs:21118 blocked for more than 120 seconds.\r\nJan 17 17:26:45 kvm-hardware-node kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJan 17 17:26:45 kvm-hardware-node kernel: zfs             D ffff883f79a38000     0 21118   8250 0x00000080\r\nJan 17 17:26:45 kvm-hardware-node kernel: Call Trace:\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffff816ab6d9>] schedule+0x29/0x70\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc04e2515>] cv_wait_common+0x125/0x150 [spl]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffff810b34b0>] ? wake_up_atomic_t+0x30/0x30\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc04e2555>] __cv_wait+0x15/0x20 [spl]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc0765a2f>] txg_wait_synced+0xef/0x140 [zfs]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc0727d50>] ? dsl_dataset_snapshot_check_impl+0x210/0x210 [zfs]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc073d017>] dsl_sync_task+0x177/0x270 [zfs]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc07289d0>] ? dsl_dataset_snapshot_sync_impl+0x760/0x760 [zfs]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc0727d50>] ? dsl_dataset_snapshot_check_impl+0x210/0x210 [zfs]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc07289d0>] ? dsl_dataset_snapshot_sync_impl+0x760/0x760 [zfs]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc0728dc3>] dsl_dataset_snapshot+0x133/0x2e0 [zfs]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc0479157>] ? nvlist_remove_all+0x77/0xd0 [znvpair]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc0479655>] ? nvlist_add_common.part.51+0x325/0x430 [znvpair]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffff811df99c>] ? __kmalloc_node+0x5c/0x2b0\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc04db31d>] ? spl_kmem_alloc_impl+0xcd/0x170 [spl]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc04db31d>] ? spl_kmem_alloc_impl+0xcd/0x170 [spl]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc04db31d>] ? spl_kmem_alloc_impl+0xcd/0x170 [spl]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc0479fc2>] ? nvlist_lookup_common.part.71+0xa2/0xb0 [znvpair]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc0796868>] zfs_ioc_snapshot+0x348/0x3b0 [zfs]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffffc0798606>] zfsdev_ioctl+0x1d6/0x650 [zfs]\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffff8121710d>] do_vfs_ioctl+0x33d/0x540\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffff816b3801>] ? __do_page_fault+0x171/0x450\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffff812173b1>] SyS_ioctl+0xa1/0xc0\r\nJan 17 17:26:45 kvm-hardware-node kernel: [<ffffffff816b89fd>] system_call_fastpath+0x16/0x1b\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7057/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6886", "title": "ZED flood /var/log/messages with useless messages", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  CentOS\r\nDistribution Version    | 7.4.1708\r\nLinux Kernel                 | 3.10.0-693.5.2.el7.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.3-1\r\nSPL Version                  | 0.7.3-1\r\n\r\n### Describe the problem you're observing\r\n\r\nZED flood /var/log/messages with useless messages like this:\r\n\r\n```\r\n[...]\r\nzed: eid=14894 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14895 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14896 class=history_event pool_guid=0x2B391E161C2A5766\r\n[...]\r\n```\r\nthis is one-line useless information about events ```create snapshot```, ```destroy snapshot``` and so on.\r\n\r\nI prefer to see in /var/log/messages only important messages from ZED, \r\nbut there is no way to disable these ```class=history_event``` non-error and non-warning messages\r\nfrom ZED. \r\n\r\n### Describe how to reproduce the problem\r\n\r\n0. Install ZFS and enable ZED service\r\n1. Install and configure any tool for zfs snapshot automation\r\n2. See into /var/log/messages after some time\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\n```\r\n[...]\r\nzed: eid=14894 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14895 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14896 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14897 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14898 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14899 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14900 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14901 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14902 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14903 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14904 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14905 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14906 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14907 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14908 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14909 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14910 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14911 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14912 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14913 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14914 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14915 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14916 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14917 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14918 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14919 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14920 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14921 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14922 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14923 class=history_event pool_guid=0x2B391E161C2A5766\r\nzed: eid=14924 class=history_event pool_guid=0x2B391E161C2A5766\r\n[...]\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6886/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "beren12": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7056", "title": "Improve snapshot listing error message", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian\r\nDistribution Version    | 9\r\nLinux Kernel                 | 4.13.13-1~bpo9+1\r\nArchitecture                 | x64\r\nZFS Version                  | 0.7.4\r\nSPL Version                  | 0.7.4\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nlisting snapshots for a single dataset fails unless -r is used, but this is not mentioned in the error message. -r is not needed to list all snapshots, so it can be a confusing behavior.\r\n\r\n### Describe how to reproduce the problem\r\n\r\n```\r\nzfs list -t snap rpool\r\n```\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\n```\r\nzfs list -t snap rpool\r\ncannot open 'rpool': missing '@' delimiter in snapshot name\r\n```\r\n\r\nCould we amend the error message to also give a hint? Or possibly be consistent and list all snapshots without -r, just as giving no dataset does? Bookmarks might also need the same edit, ike here:\r\n\r\n```diff\r\n--- lib/libzfs/libzfs_dataset.c\t2018-01-17 10:07:12.178817043 -0500\r\n+++ lib/libzfs/libzfs_dataset.c.new\t2018-01-17 10:06:47.307290884 -0500\r\n@@ -175,7 +175,7 @@\r\n \tif (type == ZFS_TYPE_SNAPSHOT && strchr(path, '@') == NULL) {\r\n \t\tif (hdl != NULL)\r\n \t\t\tzfs_error_aux(hdl, dgettext(TEXT_DOMAIN,\r\n-\t\t\t    \"missing '@' delimiter in snapshot name\"));\r\n+\t\t\t    \"missing '@' delimiter in snapshot name, did you mean to use -r?\"));\r\n \t\treturn (0);\r\n \t}\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7056/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "behlendorf": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7052", "title": "zfs load-key double free", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS\r\nDistribution Version    | 7\r\nLinux Kernel                 | 3.10.0-693.11.6.1\r\nArchitecture                 | x86_64\r\nZFS Version                  | zfs-0.7.0-246-gd658b2c\r\nSPL Version                  | master\r\n\r\n### Describe the problem you're observing\r\n\r\nWhen zfs is built with `--enable-debug --enable-debuginfo` and an incorrect passphrase is provided to `zfs load-key` followed by an empty one a \"double free or leak\" is reported.  Observed during manual testing.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nAt build time `--enable-debug --enable-debuginfo`, then,\r\n\r\n```sh\r\n$ truncate -s 512M /var/tmp/vdev\r\n$ zpool create tank /var/tmp/vdev\r\n$ zfs create -o encryption=on -o keyformat=passphrase tank/fs\r\nEnter passphrase: password\r\nRe-enter passphrase: password\r\n$ zfs unload-key -a\r\n```\r\n\r\nReload the key giving the wrong password first \"password1\" which is correctly rejected.  Then just hit enter when prompted again.\r\n\r\n```sh\r\n$ zfs load-key -a\r\nEnter passphrase for 'tank/fs': password1\r\nKey load error: Incorrect key provided for 'tank/fs'.\r\nEnter passphrase for 'tank/fs': <empty>\r\nKey load error: Passphrase too short (min 8).\r\n*** Error in `cmd/zfs/.libs/lt-zfs': double free or corruption (fasttop): 0x000000000061f150 ***\r\n======= Backtrace: =========\r\n/lib64/libc.so.6(+0x7c619)[0x2aaaacc65619]\r\nlib/libzfs/.libs/libzfs.so.2(zfs_crypto_load_key+0xf3)[0x2aaaab109023]\r\ncmd/zfs/.libs/lt-zfs[0x406557]\r\ncmd/zfs/.libs/lt-zfs[0x405d41]\r\ncmd/zfs/.libs/lt-zfs[0x408298]\r\ncmd/zfs/.libs/lt-zfs[0x4051ef]\r\n/lib64/libc.so.6(__libc_start_main+0xf5)[0x2aaaacc0ac05]\r\ncmd/zfs/.libs/lt-zfs[0x405318]\r\n...\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7052/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7026", "title": "Test case history_004_pos", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | all\r\nDistribution Version    | all\r\nLinux Kernel                 | all\r\nArchitecture                 | all\r\nZFS Version                  | zfs-0.7.0-230-gb02beca\r\nSPL Version                  | 0.7\r\n\r\n### Describe the problem you're observing\r\n\r\nRarely observed failure of history_004_pos during automated testing.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nReproducible by the buildbot.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\n\r\nhttp://build.zfsonlinux.org/builders/Amazon%202%20x86_64%20Release%20%28TEST%29/builds/105/\r\n\r\n```\r\nTest: /usr/share/zfs/zfs-tests/tests/functional/history/history_004_pos (run as root) [00:04] [FAIL]\r\n02:51:58.52 ASSERTION: 'zpool history' can cope with simultaneous commands.\r\n02:52:01.35 umount: testpool/clone3: mountpoint not found\r\n02:52:01.35 cannot unmount 'testpool/clone3': umount failed\r\n02:52:01.55 cannot create 'testpool/clone3': dataset already exists\r\n02:52:01.62 cannot promote 'testpool/clone3': not a cloned filesystem\r\n02:52:01.66 cannot destroy 'testpool/testfs3': filesystem has children\r\n02:52:01.66 use '-r' to destroy the following datasets:\r\n02:52:01.66 testpool/testfs3@snap\r\n02:52:01.81 cannot create 'testpool/testfs3': dataset already exists\r\n02:52:01.92 cannot create snapshot 'testpool/testfs3@snap': dataset already exists\r\n02:52:02.69 The entries count error: entry_count=297  orig_count = 103\r\n02:52:02.69 NOTE: Performing test-fail callback (/usr/share/zfs/zfs-tests/callbacks/zfs_dbgmsg.ksh)\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7026/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6935", "title": "Test case vdev_zaps_004_pos", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | all\r\nDistribution Version    | all\r\nLinux Kernel                 | all\r\nArchitecture                 | all\r\nZFS Version                  | zfs-0.7.0-205-g1ce23dc\r\nSPL Version                  | 0.7\r\n\r\n### Describe the problem you're observing\r\n\r\nOccasionally observed failure of `vdev_zaps_004_pos` during automated testing.  This is caused by a problem with the test case which needs to be resolved before this test case can be re-enabed.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nReproducible by the buildbot.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\nhttp://build.zfsonlinux.org/builders/Ubuntu%2014.04%20i686%20%28TEST%29/builds/5046/steps/shell_8/logs/log\r\n\r\n```\r\nTest: /usr/share/zfs/zfs-tests/tests/functional/vdev_zaps/vdev_zaps_004_pos (run as root) [00:04] [FAIL]\r\n17:22:09.36 ASSERTION: Per-vdev ZAPs are transferred properly on attach/detach\r\n17:22:09.65 SUCCESS: zpool create -f testpool loop0\r\n17:22:13.02 SUCCESS: zpool attach testpool loop0 loop1\r\n17:22:13.65 \r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6935/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6680", "title": "Test case history_005_neg", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | all\r\nDistribution Version    | all\r\nLinux Kernel                 | all\r\nArchitecture                 | all\r\nZFS Version                  | 0.7.0-92-g3fd3e56\r\nSPL Version                  | 0.7\r\n\r\n### Describe the problem you're observing\r\n\r\nRarely observed failure of history_005_neg during automated testing.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nReproducible by the buildbot.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\nhttp://build.zfsonlinux.org/builders/Ubuntu%2017.04%20x86_64%20%28TEST%29/builds/959/steps/shell_8/logs/log\r\n\r\n```\r\nTest: /usr/share/zfs/zfs-tests/tests/functional/history/history_005_neg (run as root) [00:00] [FAIL]\r\n19:26:03.52 ASSERTION: Verify 'zpool get|history|list|status|iostat' will not be logged.\r\n19:26:03.53 SUCCESS: eval zpool history testpool >/tmp/old_history.15349\r\n19:26:03.58 SUCCESS: eval zpool history testpool >/tmp/new_history.15349\r\n19:26:03.58 302a303\r\n19:26:03.58 > 2017-09-25.19:26:03 zpool get all testpool\r\n19:26:03.58 \r\n19:26:03.58 ERROR: diff /tmp/old_history.15349 /tmp/new_history.15349 exited 1\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6680/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6145", "title": "Test case: zpool_destroy_001_pos", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Amazon Linux\r\nDistribution Version    | AMI release 2017.03\r\nLinux Kernel                 | 4.9.27-14.31.amzn1.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | zfs-0.7.0-rc4-17-ga32df59\r\nSPL Version                  | 0.7.0-rc4\r\n\r\n### Describe the problem you're observing\r\n\r\nReliable deadlock when running `zpool_destory_001_pos`.  This test case creates a zpool layered on a ZVOL which is known to be problematic.  Commits 5559ba094feff560abe00afd31ab99dd1f70698c and 07783588bcb513a3a1f4d995b5d4685a9cfc89e5 were designed to allow this and do appear to work reliably on other platforms, but not always.\r\n\r\n### Describe how to reproduce the problem\r\n\r\n* Boot an m3.large instance in EC2 running the latest Amazon Linux AMI.\r\n* Enable support for partitions on loopback devices and reboot.\r\n    `echo \"options loop max_part=15\" >/etc/modprobe.d/loop.conf`\r\n* Update the AMI and install the needed packages to build ZFS.\r\n* Build the spl and zfs source with debugging enabled (master branch)\r\n* Run `sudo ./scripts/zfs-helpers.sh -iv` to install the zfs udev rules.\r\n* Run `sudo ./scripts/zfs.sh` to load the zfs kernel modules.\r\n* Modify the  `zpool_destroy_001_pos.ksh` test case to run the original non-linux version of the test.\r\n\r\n```diff\r\ndiff --git a/tests/zfs-tests/tests/functional/cli_root/zpool_destroy/zpool_destroy_001_pos.ksh b/tests/zfs-tests/tests/functional/cli_root/zpool_destroy/zpool_destroy_001_pos.ksh\r\nindex 428765e..200ea76 100755\r\n--- a/tests/zfs-tests/tests/functional/cli_root/zpool_destroy/zpool_destroy_001_pos.ksh\r\n+++ b/tests/zfs-tests/tests/functional/cli_root/zpool_destroy/zpool_destroy_001_pos.ksh\r\n@@ -73,14 +73,14 @@ partition_disk $SLICE_SIZE $DISK 2\r\n \r\n create_pool \"$TESTPOOL\" \"${DISK}${SLICE_PREFIX}${SLICE0}\"\r\n \r\n-if is_linux; then\r\n-       # Layering a pool on a zvol can deadlock and isn't supported.\r\n-       create_pool \"$TESTPOOL2\" \"${DISK}${SLICE_PREFIX}${SLICE1}\"\r\n-else\r\n+#if is_linux; then\r\n+#      # Layering a pool on a zvol can deadlock and isn't supported.\r\n+#      create_pool \"$TESTPOOL2\" \"${DISK}${SLICE_PREFIX}${SLICE1}\"\r\n+#else\r\n        create_pool \"$TESTPOOL1\" \"${DISK}${SLICE_PREFIX}${SLICE1}\"\r\n        log_must zfs create -s -V $VOLSIZE $TESTPOOL1/$TESTVOL\r\n        create_pool \"$TESTPOOL2\" \"${ZVOL_DEVDIR}/$TESTPOOL1/$TESTVOL\"\r\n-fi\r\n+#fi\r\n \r\n typeset -i i=0\r\n```\r\n\r\n* Run `./scripts/zfs-tests.sh -vx -t ./tests/zfs-tests/tests/functional/cli_root/zpool_destroy/zpool_destroy_001_pos.ksh`.\r\n* System should deadlock immediately.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\n```\r\n[   79.571848] ZFS: Loaded module v0.7.0-rc4_17_ga32df59 (DEBUG mode), ZFS pool version 5000, ZFS filesystem version 5\r\n[   88.306058] loop: module loaded\r\n[   88.776354]  loop0: p1\r\n[   89.212604]  loop0: p1 p2\r\n[   89.596101] SPL: using hostid 0x00000000\r\n[   90.057163]  zd0: p1 p9\r\n[  123.868062] BUG: workqueue lockup - pool cpus=0 node=0 flags=0x0 nice=0 stuck for 33s!\r\n[  123.875947] Showing busy workqueues and worker pools:\r\n[  123.879551] workqueue events: flags=0x0\r\n[  123.882503]   pwq 0: cpus=0 node=0 flags=0x0 nice=0 active=2/256\r\n[  123.886495]     pending: vmstat_shepherd, push_to_pool\r\n[  123.894217] workqueue events_power_efficient: flags=0x80\r\n[  123.898104]   pwq 0: cpus=0 node=0 flags=0x0 nice=0 active=1/256\r\n[  123.902096]     pending: neigh_periodic_work\r\n[  123.922839] workqueue kblockd: flags=0x18\r\n[  123.926263]   pwq 1: cpus=0 node=0 flags=0x0 nice=-20 active=2/256\r\n[  123.930255]     pending: blk_mq_timeout_work, blk_mq_timeout_work\r\n[  123.939473] workqueue vmstat: flags=0xc\r\n[  123.942804]   pwq 0: cpus=0 node=0 flags=0x0 nice=0 active=1/256\r\n[  123.946789]     pending: vmstat_update\r\n[  123.953876] workqueue ipv6_addrconf: flags=0x40008\r\n[  123.957801]   pwq 0: cpus=0 node=0 flags=0x0 nice=0 active=1/1\r\n[  123.961792]     pending: addrconf_verify_work [ipv6]\r\n[  149.216056] INFO: rcu_sched self-detected stall on CPU\r\n[  149.216056] \t0-...: (14736 ticks this GP) idle=0a9/140000000000001/0 softirq=8042/8042 fqs=7126 \r\n[  149.216056] \t (t=14750 jiffies g=1102 c=1101 q=806)\r\n[  149.216056] Task dump for CPU 0:\r\n[  149.216056] blkid           R  running task        0  4497   3136 0x00000008\r\n[  149.216056]  ffff8801e7a03ce8 ffffffff8109d6a7 0000000000000000 0000000000000000\r\n[  149.216056]  ffff8801e7a03d00 ffffffff8109fff9 ffffffff81a54100 ffff8801e7a03d30\r\n[  149.216056]  ffffffff81173a40 ffff8801e7a18940 ffffffff81a54100 0000000000000000\r\n[  149.216056] Call Trace:\r\n[  149.216056]  <IRQ> \r\n[  149.216056]  [<ffffffff8109d6a7>] sched_show_task+0xd7/0x140\r\n[  149.216056]  [<ffffffff8109fff9>] dump_cpu_task+0x39/0x40\r\n[  149.216056]  [<ffffffff81173a40>] rcu_dump_cpu_stacks+0x80/0xbc\r\n[  149.216056]  [<ffffffff810d12bf>] rcu_check_callbacks+0x6ef/0x850\r\n[  149.216056]  [<ffffffff810a0a51>] ? account_system_time+0x81/0x110\r\n[  149.216056]  [<ffffffff810a0ce0>] ? account_process_tick+0x60/0x170\r\n[  149.216056]  [<ffffffff810e60b0>] ? tick_sched_do_timer+0x30/0x30\r\n[  149.216056]  [<ffffffff810d770f>] update_process_times+0x2f/0x60\r\n[  149.216056]  [<ffffffff810e5af6>] tick_sched_handle.isra.13+0x36/0x50\r\n[  149.216056]  [<ffffffff810e60ed>] tick_sched_timer+0x3d/0x70\r\n[  149.216056]  [<ffffffff810d8296>] __hrtimer_run_queues+0xd6/0x230\r\n[  149.216056]  [<ffffffff810d8718>] hrtimer_interrupt+0xa8/0x1a0\r\n[  149.216056]  [<ffffffff81021b6f>] xen_timer_interrupt+0x1f/0x30\r\n[  149.216056]  [<ffffffff810c41cc>] __handle_irq_event_percpu+0x3c/0x1a0\r\n[  149.216056]  [<ffffffff810c4353>] handle_irq_event_percpu+0x23/0x60\r\n[  149.216056]  [<ffffffff810c7f5a>] handle_percpu_irq+0x3a/0x50\r\n[  149.216056]  [<ffffffff810c3502>] generic_handle_irq+0x22/0x30\r\n[  149.216056]  [<ffffffff813972f8>] evtchn_2l_handle_events+0x238/0x240\r\n[  149.216056]  [<ffffffff81394b63>] __xen_evtchn_do_upcall+0x43/0x80\r\n[  149.216056]  [<ffffffff813968c0>] xen_evtchn_do_upcall+0x30/0x50\r\n[  149.216056]  [<ffffffff81534732>] xen_hvm_callback_vector+0x82/0x90\r\n[  149.216056]  <EOI> \r\n[  149.216056]  [<ffffffff81533400>] ? _raw_spin_lock+0x10/0x30\r\n[  149.216056]  [<ffffffffa056c4f6>] ? zvol_open+0xa6/0x3a0 [zfs]\r\n[  149.216056]  [<ffffffff81234334>] __blkdev_get+0xc4/0x420\r\n[  149.216056]  [<ffffffff8123391e>] ? bdget+0x3e/0x130\r\n[  149.216056]  [<ffffffff81234506>] __blkdev_get+0x296/0x420\r\n[  149.216056]  [<ffffffff8123483f>] blkdev_get+0x1af/0x300\r\n[  149.216056]  [<ffffffff81234a3b>] blkdev_open+0x5b/0x70\r\n[  149.216056]  [<ffffffff811f6f93>] do_dentry_open+0x213/0x310\r\n[  149.216056]  [<ffffffff812349e0>] ? blkdev_get_by_dev+0x50/0x50\r\n[  149.216056]  [<ffffffff811f82af>] vfs_open+0x4f/0x70\r\n[  149.216056]  [<ffffffff8120408b>] ? may_open+0x9b/0x100\r\n[  149.216056]  [<ffffffff81207459>] path_openat+0x529/0x1300\r\n[  149.216056]  [<ffffffff811b7402>] ? page_add_file_rmap+0x52/0x150\r\n[  149.216056]  [<ffffffff81176976>] ? filemap_map_pages+0x366/0x3a0\r\n[  149.216056]  [<ffffffff81209dce>] do_filp_open+0x7e/0xd0\r\n[  149.216056]  [<ffffffff8102a649>] ? __switch_to+0x1f9/0x5d0\r\n[  149.216056]  [<ffffffff81217bc0>] ? __alloc_fd+0xb0/0x170\r\n[  149.216056]  [<ffffffff811f8655>] do_sys_open+0x115/0x1f0\r\n[  149.216056]  [<ffffffff811f874e>] SyS_open+0x1e/0x20\r\n[  149.216056]  [<ffffffff81533677>] entry_SYSCALL_64_fastpath+0x1a/0xa9\r\n[  242.652110] INFO: task lt-zpool:4363 blocked for more than 120 seconds.\r\n[  242.658901]       Tainted: P           OE   4.9.27-14.31.amzn1.x86_64 #1\r\n[  242.665595] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[  242.673689] lt-zpool        D    0  4363   3372 0x00000000\r\n[  242.680394]  ffff8801e227e180 0000000000000000 ffff8801e662bb00 ffff8801d2b98000\r\n[  242.690713]  ffff8801e7a57bc0 ffffc90003b839f8 ffffffff8152ed4c ffffc90003b839c8\r\n[  242.700941]  ffffffff8152f45e ffffc90003b83a20 ffff8801d2b98000 ffff8801e5bb96dc\r\n[  242.711462] Call Trace:\r\n[  242.714602]  [<ffffffff8152ed4c>] ? __schedule+0x23c/0x680\r\n[  242.720233]  [<ffffffff8152f45e>] ? schedule_preempt_disabled+0xe/0x10\r\n[  242.726812]  [<ffffffff8152f1c6>] schedule+0x36/0x80\r\n[  242.732045]  [<ffffffff8152f45e>] schedule_preempt_disabled+0xe/0x10\r\n[  242.738617]  [<ffffffff81530cb5>] __mutex_lock_slowpath+0x95/0x110\r\n[  242.744922]  [<ffffffff81530d47>] mutex_lock+0x17/0x27\r\n[  242.750449]  [<ffffffff812342cf>] __blkdev_get+0x5f/0x420\r\n[  242.756244]  [<ffffffff8123488f>] blkdev_get+0x1ff/0x300\r\n[  242.761600]  [<ffffffff8121a2c4>] ? mntput+0x24/0x40\r\n[  242.766930]  [<ffffffff8120356e>] ? path_put+0x1e/0x30\r\n[  242.772304]  [<ffffffff81234bc3>] blkdev_get_by_path+0x53/0x90\r\n[  242.778509]  [<ffffffffa0501f24>] vdev_disk_open+0x3b4/0x420 [zfs]\r\n[  242.784856]  [<ffffffff812152ca>] ? iput+0x8a/0x200\r\n[  242.790043]  [<ffffffffa04feaae>] vdev_open+0x12e/0x730 [zfs]\r\n[  242.796170]  [<ffffffffa04ff105>] vdev_open_children+0x55/0x170 [zfs]\r\n[  242.802863]  [<ffffffffa0511540>] vdev_root_open+0x50/0x110 [zfs]\r\n[  242.809212]  [<ffffffffa04feaae>] vdev_open+0x12e/0x730 [zfs]\r\n[  242.815298]  [<ffffffffa04ff282>] vdev_create+0x22/0xa0 [zfs]\r\n[  242.821367]  [<ffffffffa03f449c>] ? zfs_allocatable_devs+0x5c/0x80 [zcommon]\r\n[  242.828408]  [<ffffffffa04e75f5>] spa_create+0x435/0xaa0 [zfs]\r\n[  242.834551]  [<ffffffffa037e725>] ? nvlist_add_uint64+0x35/0x40 [znvpair]\r\n[  242.841280]  [<ffffffffa052ab61>] ? zfs_fill_zplprops_impl+0x231/0x410 [zfs]\r\n[  242.848309]  [<ffffffffa053014e>] zfs_ioc_pool_create+0x12e/0x230 [zfs]\r\n[  242.855010]  [<ffffffffa0307eff>] ? strdup+0x3f/0x60 [spl]\r\n[  242.860858]  [<ffffffffa0530e39>] zfsdev_ioctl+0x4d9/0x600 [zfs]\r\n[  242.867149]  [<ffffffff8120ccd6>] do_vfs_ioctl+0x96/0x5b0\r\n[  242.872660]  [<ffffffff8106100a>] ? __do_page_fault+0x24a/0x4a0\r\n[  242.878770]  [<ffffffff8120d269>] SyS_ioctl+0x79/0x90\r\n[  242.884182]  [<ffffffff81533677>] entry_SYSCALL_64_fastpath+0x1a/0xa9\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6145/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6136", "title": "Test case: threadsappend_001_pos", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | all\r\nDistribution Version    | all\r\nLinux Kernel                 | all\r\nArchitecture                 | all\r\nZFS Version                  | 0.7.0-rc4\r\nSPL Version                  | 0.7.0-rc4\r\n\r\n### Describe the problem you're observing\r\n\r\nOccasional failure of test case threadsappend_001_pos.  This looks like a legitimate bug caused by concurrent appends to a file.  A brief investigation suggests the problem may be that `ppos` isn't updated under the range lock during append.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nReproducible about 5% of the time by running this test case repeatedly in a loop.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\nhttp://build.zfsonlinux.org/builders/CentOS%206.7%20x86_64%20%28TEST%29/builds/5519/steps/shell_9/logs/log\r\n\r\n```\r\nTest: /usr/share/zfs/zfs-tests/tests/functional/threadsappend/threadsappend_001_pos (run as root) [00:00] [FAIL]\r\n02:49:07.28 ASSERTION: Ensure multiple threads performing write appends to the same ZFS file succeed\r\n02:49:07.30 SUCCESS: threadsappend /var/tmp/testdir/testfile-threadsappend\r\n...\r\n02:49:07.36 'The length of /var/tmp/testdir/testfile-threadsappend' doesn't equal 1310720.\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6136/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6120", "title": "Trace points Build Failure", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS\r\nDistribution Version    | 7.3.1611\r\nLinux Kernel                 | 3.10.0-514.16.1.el7.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | zfs-0.7.0-rc4\r\nSPL Version                  | spl-0.7.0-rc4\r\n\r\n### Describe the problem you're observing\r\n\r\nChanging the license in from CDDL to GPL in the top level META file results in a build failure when compiling support for trace points.  Building [ZFS directly in to a more recent kernel](http://build.zfsonlinux.org/builders/Kernel.org%20Built-in%20x86_64%20%28BUILD%29/builds/12931) does not appear to cause this build issue.  \r\n### Describe how to reproduce the problem\r\n\r\n```\r\n> sed -i -- 's/CDDL/GPL/g' META\r\n> sh autogen.sh\r\n> ./configure\r\n> make\r\n```\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\n```\r\nIn file included from ./arch/x86/include/asm/spinlock.h:4:0,\r\n                 from include/linux/spinlock.h:87,\r\n                 from include/linux/seqlock.h:35,\r\n                 from include/linux/time.h:5,\r\n                 from include/linux/stat.h:18,\r\n                 from include/linux/module.h:10,\r\n                 from /home/behlendo/src/git/spl/include/sys/sysmacros.h:28,\r\n                 from /home/behlendo/src/git/spl/include/sys/types.h:29,\r\n                 from /home/behlendo/src/git/zfs/include/sys/zfs_context.h:34,\r\n                 from /home/behlendo/src/git/zfs/include/sys/multilist.h:22,\r\n                 from /home/behlendo/src/git/zfs/module/zfs/trace.c:26:\r\ninclude/linux/jump_label.h:120:40: error: initializer element is not constant\r\n #define STATIC_KEY_INIT_FALSE ((struct static_key) \\\r\n                                        ^\r\ninclude/linux/tracepoint.h:210:24: note: in expansion of macro \u2018STATIC_KEY_INIT_FALSE\u2019\r\n   { __tpstrtab_##name, STATIC_KEY_INIT_FALSE, reg, unreg, NULL };\\\r\n                        ^\r\ninclude/linux/tracepoint.h:216:2: note: in expansion of macro \u2018DEFINE_TRACE_FN\u2019\r\n  DEFINE_TRACE_FN(name, NULL, NULL);\r\n  ^\r\ninclude/trace/define_trace.h:45:2: note: in expansion of macro \u2018DEFINE_TRACE\u2019\r\n  DEFINE_TRACE(name)\r\n  ^\r\n/home/behlendo/src/git/zfs/include/sys/trace_dbgmsg.h:73:1: note: in expansion of macro \u2018DEFINE_EVENT\u2019\r\n DEFINE_EVENT(zfs_dprintf_class, name, \\\r\n ^\r\n/home/behlendo/src/git/zfs/include/sys/trace_dbgmsg.h:78:1: note: in expansion of macro \u2018DEFINE_DPRINTF_EVENT\u2019\r\n DEFINE_DPRINTF_EVENT(zfs_zfs__dprintf);\r\n ^\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6120/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6112", "title": "Test case: zpool_upgrade_007_pos", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | all\r\nDistribution Version    | all\r\nLinux Kernel                 | all\r\nArchitecture                 | all\r\nZFS Version                  | 0.7.0-rc4\r\nSPL Version                  | 0.7.0-rc4\r\n\r\n### Describe the problem you're observing\r\n\r\nOccasional failure of test case zpool_upgrade_007_pos\r\n\r\n### Describe how to reproduce the problem\r\n\r\nReproducible occasionally by the automated testing and locally.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\n```\r\n[ 8394.810713] VERIFY3(newds == os->os_dsl_dataset) failed (ffff88000d383000 == ffff88004d5da800)\r\n[ 8394.813104] PANIC at dmu_objset.c:726:dmu_objset_refresh_ownership()\r\n[ 8394.813929] Showing stack for process 24000\r\n[ 8394.813939] CPU: 2 PID: 24000 Comm: lt-zfs Tainted: P           OE  ------------   3.10.0-514.16.1.el7.x86_64 #1\r\n[ 8394.813941] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Ubuntu-1.8.2-1ubuntu1 04/01/2014\r\n[ 8394.813943]  ffffffffa0904c16 00000000ce97c6ab ffff88004ddafa10 ffffffff81686ac3\r\n[ 8394.813947]  ffff88004ddafa20 ffffffffa0687e34 ffff88004ddafba8 ffffffffa0687eff\r\n[ 8394.813950]  0000000000000000 0000000000000030 ffff88004ddafbb8 ffff88004ddafb58\r\n[ 8394.813954] Call Trace:\r\n[ 8394.813979]  [<ffffffff81686ac3>] dump_stack+0x19/0x1b\r\n[ 8394.814004]  [<ffffffffa0687e34>] spl_dumpstack+0x44/0x50 [spl]\r\n[ 8394.814011]  [<ffffffffa0687eff>] spl_panic+0xbf/0xf0 [spl]\r\n[ 8394.814020]  [<ffffffff81698eb1>] ? ftrace_call+0x5/0x2f\r\n[ 8394.814024]  [<ffffffff8168e305>] ? _raw_spin_unlock+0x5/0x30\r\n[ 8394.814028]  [<ffffffff8168c3e5>] ? _cond_resched+0x5/0x50\r\n[ 8394.814121]  [<ffffffffa07d4714>] dmu_objset_refresh_ownership+0x104/0x1e0 [zfs]\r\n[ 8394.814136]  [<ffffffff810b5a55>] ? up_write+0x5/0x20\r\n[ 8394.814140]  [<ffffffff8168c3e5>] ? _cond_resched+0x5/0x50\r\n[ 8394.814143]  [<ffffffff8168b312>] ? down_write+0x12/0x30\r\n[ 8394.814146]  [<ffffffff81698eb1>] ? ftrace_call+0x5/0x2f\r\n[ 8394.814169]  [<ffffffffa07d0d94>] ? dmu_objset_evict_dbufs+0x194/0x1b0 [zfs]\r\n[ 8394.814175]  [<ffffffffa067fe0a>] ? spl_kmem_free+0x2a/0x40 [spl]\r\n[ 8394.814198]  [<ffffffffa07d4615>] ? dmu_objset_refresh_ownership+0x5/0x1e0 [zfs]\r\n[ 8394.814251]  [<ffffffffa084e850>] zfs_ioc_userspace_upgrade+0xf0/0x120 [zfs]\r\n[ 8394.814282]  [<ffffffffa084e765>] ? zfs_ioc_userspace_upgrade+0x5/0x120 [zfs]\r\n[ 8394.814314]  [<ffffffffa0852ce8>] zfs_prop_set_special+0x338/0x550 [zfs]\r\n[ 8394.814362]  [<ffffffffa0854d66>] zfs_set_prop_nvlist+0x136/0x3a0 [zfs]\r\n[ 8394.814394]  [<ffffffffa085653f>] zfs_ioc_set_prop+0x10f/0x150 [zfs]\r\n[ 8394.814428]  [<ffffffffa0856435>] ? zfs_ioc_set_prop+0x5/0x150 [zfs]\r\n[ 8394.814471]  [<ffffffffa0853d56>] zfsdev_ioctl+0x506/0x550 [zfs]\r\n[ 8394.814499]  [<ffffffffa0853850>] ? pool_status_check.part.24+0xb0/0xb0 [zfs]\r\n[ 8394.814516]  [<ffffffff81212555>] do_vfs_ioctl+0x2d5/0x4b0\r\n[ 8394.814520]  [<ffffffff812127d1>] SyS_ioctl+0xa1/0xc0\r\n[ 8394.814526]  [<ffffffff81697189>] system_call_fastpath+0x16/0x1b\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6112/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6101", "title": "Test case: zfs_destroy_005_neg", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | all\r\nDistribution Version    | all\r\nLinux Kernel                 | all\r\nArchitecture                 | all\r\nZFS Version                  | 0.7.0-rc4\r\nSPL Version                  | 0.7.0-rc4\r\n\r\n### Describe the problem you're observing\r\n\r\nOccasional failure of test case zfs_destroy_005_neg.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nReproducible occasionally by the automated testing.  Mostly commonly by the kmemleak builder.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\nhttp://build.zfsonlinux.org/builders/Ubuntu%2016.04%20x86_64%20Kmemleak%20%28TEST%29/builds/1642/steps/shell_9/logs/log\r\n\r\n```\r\nTest: /usr/share/zfs/zfs-tests/tests/functional/cli_root/zfs_destroy/zfs_destroy_005_neg (run as root) [00:05] [FAIL]\r\n00:45:32.16 ASSERTION: Separately verify 'zfs destroy -f|-r|-rf|-R|-rR <dataset>' will  fail in different conditions.\r\n00:45:32.30 SUCCESS: zfs create testpool/testctr\r\n00:45:32.47 SUCCESS: zfs create testpool/testctr/testfs\r\n00:45:32.60 SUCCESS: zfs create -V 150m testpool/testctr/testvol\r\n00:45:35.29 NOTE: SUCCESS: newfs /dev/zvol/testpool/testctr/testvol>/dev/null\r\n00:45:35.30 SUCCESS: mkdir /var/tmp/testdir1\r\n00:45:35.38 SUCCESS: mount /dev/zvol/testpool/testctr/testvol /var/tmp/testdir1\r\n00:45:35.44 SUCCESS: zfs snapshot testpool/testctr/testfs@testsnap\r\n00:45:35.48 SUCCESS: zfs snapshot testpool/testctr/testvol@testsnap\r\n00:45:35.51 SUCCESS: zfs destroy -f testpool/testctr exited 1\r\n00:45:35.53 SUCCESS: zfs destroy -f testpool/testctr/testfs exited 1\r\n00:45:35.54 SUCCESS: zfs destroy -f testpool/testctr/testvol exited 1\r\n00:45:35.76 SUCCESS: zfs clone testpool/testctr/testfs@testsnap testpool/testfsclone\r\n00:45:35.85 SUCCESS: zfs clone testpool/testctr/testvol@testsnap testpool/testvolclone\r\n00:45:35.91 SUCCESS: zfs destroy -r testpool/testctr exited 1\r\n00:45:35.97 SUCCESS: zfs destroy -rf testpool/testctr exited 1\r\n00:45:35.99 SUCCESS: zfs destroy -r testpool/testctr/testfs exited 1\r\n00:45:36.01 SUCCESS: zfs destroy -rf testpool/testctr/testfs exited 1\r\n00:45:36.05 SUCCESS: zfs destroy -r testpool/testctr/testvol exited 1\r\n00:45:36.09 SUCCESS: zfs destroy -rf testpool/testctr/testvol exited 1\r\n00:45:36.10 NOTE: mkbusy /testpool/testctr/testfs/testfile0 (pidlist: 27630)\r\n00:45:36.23 SUCCESS: zfs destroy -R testpool/testctr exited 1\r\n00:45:36.25 SUCCESS: zfs destroy -rR testpool/testctr exited 1\r\n00:45:36.26 SUCCESS: datasetexists testpool/testctr\r\n00:45:36.28 SUCCESS: datasetexists testpool/testctr/testfs\r\n00:45:36.29 SUCCESS: datasetexists testpool/testctr/testvol\r\n00:45:36.32 SUCCESS: datasetnonexists testpool/testctr/testvol@testsnap\r\n00:45:36.33 SUCCESS: datasetnonexists testpool/testvolclone\r\n00:45:36.34 SUCCESS: datasetexists testpool/testctr/testfs@testsnap\r\n00:45:36.36 SUCCESS: datasetexists testpool/testfsclone\r\n00:45:36.46 SUCCESS: zfs snapshot testpool/testctr/testvol@testsnap\r\n00:45:36.61 SUCCESS: zfs clone testpool/testctr/testvol@testsnap testpool/testvolclone\r\n00:45:36.79 SUCCESS: zfs destroy -R testpool/testctr/testfs exited 1\r\n00:45:36.81 SUCCESS: zfs destroy -rR testpool/testctr/testfs exited 1\r\n00:45:36.83 SUCCESS: datasetexists testpool/testctr\r\n00:45:36.84 SUCCESS: datasetexists testpool/testctr/testfs\r\n00:45:36.86 SUCCESS: datasetexists testpool/testctr/testvol\r\n00:45:36.87 SUCCESS: datasetexists testpool/testctr/testvol@testsnap\r\n00:45:36.90 SUCCESS: datasetexists testpool/testvolclone\r\n00:45:36.91 SUCCESS: datasetnonexists testpool/testctr/testfs@testsnap\r\n00:45:36.92 SUCCESS: datasetnonexists testpool/testfsclone\r\n00:45:36.92 SUCCESS: kill 27630\r\n00:45:36.95 SUCCESS: pgrep -fl mkbusy exited 1\r\n00:45:37.05 SUCCESS: zfs snapshot testpool/testctr/testfs@testsnap\r\n00:45:37.23 SUCCESS: zfs clone testpool/testctr/testfs@testsnap testpool/testfsclone\r\n00:45:37.25 NOTE: mkbusy /var/tmp/testdir1/testfile0 (pidlist: 27964)\r\n00:45:37.40 SUCCESS: zfs destroy -R testpool/testctr exited 1\r\n00:45:37.42 SUCCESS: zfs destroy -rR testpool/testctr exited 1\r\n00:45:37.43 SUCCESS: datasetexists testpool/testctr\r\n00:45:37.45 SUCCESS: datasetexists testpool/testctr/testvol\r\n00:45:37.46 SUCCESS: datasetnonexists testpool/testctr/testvol@testsnap\r\n00:45:37.47 SUCCESS: datasetnonexists testpool/testvolclone\r\n00:45:37.49 SUCCESS: datasetexists testpool/testctr/testfs\r\n00:45:37.51 SUCCESS: datasetexists testpool/testctr/testfs@testsnap\r\n00:45:37.52 SUCCESS: datasetexists testpool/testfsclone\r\n00:45:37.63 SUCCESS: zfs snapshot testpool/testctr/testvol@testsnap\r\n00:45:37.77 SUCCESS: zfs clone testpool/testctr/testvol@testsnap testpool/testvolclone\r\n00:45:37.81 SUCCESS: zfs destroy -R testpool/testctr/testvol exited 1\r\n00:45:37.84 SUCCESS: zfs destroy -rR testpool/testctr/testvol exited 1\r\n00:45:37.87 SUCCESS: datasetexists testpool/testctr\r\n00:45:37.88 SUCCESS: datasetexists testpool/testctr/testvol\r\n00:45:37.89 SUCCESS: datasetexists testpool/testctr/testfs\r\n00:45:37.91 SUCCESS: datasetexists testpool/testctr/testfs@testsnap\r\n00:45:37.92 SUCCESS: datasetexists testpool/testfsclone\r\n00:45:37.93 \r\n00:45:37.94 ERROR: datasetnonexists testpool/testctr/testvol@testsnap exited 1\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6101/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6100", "title": "Test case: zfs_destroy_001_pos", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | all\r\nDistribution Version    | all\r\nLinux Kernel                 | all\r\nArchitecture                 | all\r\nZFS Version                  | 0.7.0-rc4\r\nSPL Version                  | 0.7.0-rc4\r\n\r\n### Describe the problem you're observing\r\n\r\nOccasional failure of test case zfs_destroy_001_pos due to an EBUSY error.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nReproducible occasionally by the automated testing.  Mostly commonly by the kmemleak builder.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\nhttp://build.zfsonlinux.org/builders/Ubuntu%2016.04%20x86_64%20Kmemleak%20%28TEST%29/builds/1640/steps/shell_9/logs/log\r\n\r\n```\r\nTest: /usr/share/zfs/zfs-tests/tests/functional/cli_root/zfs_destroy/zfs_destroy_001_pos (run as root) [01:03] [FAIL]\r\n22:57:12.93 ASSERTION: 'zfs destroy -r|-R|-f|-rf|-Rf <fs|ctr|vol|snap>' should  recursively destroy all children.\r\n22:57:12.93 NOTE: Starting test: zfs destroy -r testpool/testctr\r\n22:57:13.07 SUCCESS: zfs create testpool/testctr\r\n22:57:13.24 SUCCESS: zfs create testpool/testctr/testfs\r\n22:57:13.36 SUCCESS: zfs create -V 150m testpool/testctr/testvol\r\n22:57:16.09 NOTE: SUCCESS: newfs /dev/zvol/testpool/testctr/testvol>/dev/null\r\n22:57:16.10 SUCCESS: mkdir /var/tmp/testdir1\r\n22:57:16.19 SUCCESS: mount /dev/zvol/testpool/testctr/testvol /var/tmp/testdir1\r\n22:57:16.24 SUCCESS: zfs snapshot testpool/testctr/testfs@testsnap\r\n22:57:16.28 SUCCESS: zfs snapshot testpool/testctr/testvol@testsnap\r\n22:57:16.32 SUCCESS: pgrep -fl mkbusy exited 1\r\n22:57:16.37 SUCCESS: umount -f /var/tmp/testdir1\r\n22:57:16.81 SUCCESS: zfs destroy -r testpool/testctr\r\n22:57:16.82 SUCCESS: datasetnonexists testpool/testctr\r\n22:57:16.83 SUCCESS: datasetnonexists testpool/testctr/testfs\r\n22:57:16.84 SUCCESS: datasetnonexists testpool/testctr/testvol\r\n22:57:16.85 SUCCESS: datasetnonexists testpool/testctr/testfs@testsnap\r\n22:57:16.86 SUCCESS: datasetnonexists testpool/testctr/testvol@testsnap\r\n22:57:16.86 NOTE: 'zfs destroy -r testpool/testctr' passed.\r\n22:57:16.86 NOTE: Starting test: zfs destroy -R testpool/testctr\r\n22:57:17.01 SUCCESS: zfs create testpool/testctr\r\n22:57:17.17 SUCCESS: zfs create testpool/testctr/testfs\r\n22:57:17.30 SUCCESS: zfs create -V 150m testpool/testctr/testvol\r\n22:57:20.03 NOTE: SUCCESS: newfs /dev/zvol/testpool/testctr/testvol>/dev/null\r\n22:57:20.12 SUCCESS: mount /dev/zvol/testpool/testctr/testvol /var/tmp/testdir1\r\n22:57:20.17 SUCCESS: zfs snapshot testpool/testctr/testfs@testsnap\r\n22:57:20.21 SUCCESS: zfs snapshot testpool/testctr/testvol@testsnap\r\n22:57:20.34 SUCCESS: zfs clone testpool/testctr/testfs@testsnap testpool/testfsclone\r\n22:57:20.43 SUCCESS: zfs clone testpool/testctr/testvol@testsnap testpool/testvolclone\r\n22:57:20.49 SUCCESS: pgrep -fl mkbusy exited 1\r\n22:57:20.56 SUCCESS: umount -f /var/tmp/testdir1\r\n22:57:21.12 SUCCESS: zfs destroy -R testpool/testctr\r\n22:57:21.14 SUCCESS: datasetnonexists testpool/testctr\r\n22:57:21.15 SUCCESS: datasetnonexists testpool/testctr/testfs\r\n22:57:21.16 SUCCESS: datasetnonexists testpool/testctr/testvol\r\n22:57:21.17 SUCCESS: datasetnonexists testpool/testctr/testfs@testsnap\r\n22:57:21.18 SUCCESS: datasetnonexists testpool/testctr/testvol@testsnap\r\n22:57:21.19 SUCCESS: datasetnonexists testpool/testfsclone\r\n22:57:21.20 SUCCESS: datasetnonexists testpool/testvolclone\r\n22:57:21.21 NOTE: 'zfs destroy -R testpool/testctr' passed.\r\n22:57:21.21 NOTE: Starting test: zfs destroy -f testpool/testctr\r\n22:57:21.21 NOTE: UNSUPPORTED: '-f ' is only available for  leaf FS.\r\n22:57:21.21 NOTE: Starting test: zfs destroy -rf testpool/testctr\r\n22:57:21.35 SUCCESS: zfs create testpool/testctr\r\n22:57:21.52 SUCCESS: zfs create testpool/testctr/testfs\r\n22:57:21.66 SUCCESS: zfs create -V 150m testpool/testctr/testvol\r\n22:57:24.41 NOTE: SUCCESS: newfs /dev/zvol/testpool/testctr/testvol>/dev/null\r\n22:57:24.50 SUCCESS: mount /dev/zvol/testpool/testctr/testvol /var/tmp/testdir1\r\n22:57:24.56 SUCCESS: zfs snapshot testpool/testctr/testfs@testsnap\r\n22:57:24.60 SUCCESS: zfs snapshot testpool/testctr/testvol@testsnap\r\n22:57:24.62 NOTE: mkbusy /testpool/testctr/testfs/testfile0  (pidlist:  18912)\r\n22:57:24.67 SUCCESS: zfs destroy -rR testpool/testctr exited 1\r\n22:57:24.68 SUCCESS: kill -TERM 18912\r\n22:57:24.71 SUCCESS: pgrep -fl mkbusy exited 1\r\n22:57:24.77 SUCCESS: umount -f /var/tmp/testdir1\r\n22:57:25.22 SUCCESS: zfs destroy -rf testpool/testctr\r\n22:57:25.24 SUCCESS: datasetnonexists testpool/testctr\r\n22:57:25.25 SUCCESS: datasetnonexists testpool/testctr/testfs\r\n22:57:25.25 SUCCESS: datasetnonexists testpool/testctr/testvol\r\n22:57:25.27 SUCCESS: datasetnonexists testpool/testctr/testfs@testsnap\r\n22:57:25.27 SUCCESS: datasetnonexists testpool/testctr/testvol@testsnap\r\n22:57:25.28 NOTE: 'zfs destroy -rf testpool/testctr' passed.\r\n22:57:25.28 NOTE: Starting test: zfs destroy -Rf testpool/testctr\r\n22:57:25.42 SUCCESS: zfs create testpool/testctr\r\n22:57:25.60 SUCCESS: zfs create testpool/testctr/testfs\r\n22:57:25.74 SUCCESS: zfs create -V 150m testpool/testctr/testvol\r\n22:57:28.47 NOTE: SUCCESS: newfs /dev/zvol/testpool/testctr/testvol>/dev/null\r\n22:57:28.56 SUCCESS: mount /dev/zvol/testpool/testctr/testvol /var/tmp/testdir1\r\n22:57:28.62 SUCCESS: zfs snapshot testpool/testctr/testfs@testsnap\r\n22:57:28.66 SUCCESS: zfs snapshot testpool/testctr/testvol@testsnap\r\n22:57:28.83 SUCCESS: zfs clone testpool/testctr/testfs@testsnap testpool/testfsclone\r\n22:57:28.93 SUCCESS: zfs clone testpool/testctr/testvol@testsnap testpool/testvolclone\r\n22:57:28.95 NOTE: mkbusy /testpool/testctr/testfs/testfile0  (pidlist:  19690)\r\n22:57:28.99 SUCCESS: zfs destroy -rR testpool/testctr exited 1\r\n22:57:29.01 SUCCESS: kill -TERM 19690\r\n22:57:29.05 SUCCESS: pgrep -fl mkbusy exited 1\r\n22:57:29.12 SUCCESS: umount -f /var/tmp/testdir1\r\n22:57:29.75 SUCCESS: zfs destroy -Rf testpool/testctr\r\n22:57:29.77 SUCCESS: datasetnonexists testpool/testctr\r\n22:57:29.78 SUCCESS: datasetnonexists testpool/testctr/testfs\r\n22:57:29.79 SUCCESS: datasetnonexists testpool/testctr/testvol\r\n22:57:29.80 SUCCESS: datasetnonexists testpool/testctr/testfs@testsnap\r\n22:57:29.81 SUCCESS: datasetnonexists testpool/testctr/testvol@testsnap\r\n22:57:29.82 SUCCESS: datasetnonexists testpool/testfsclone\r\n22:57:29.83 SUCCESS: datasetnonexists testpool/testvolclone\r\n22:57:29.84 NOTE: 'zfs destroy -Rf testpool/testctr' passed.\r\n22:57:29.84 NOTE: Starting test: zfs destroy -r testpool/testctr/testfs\r\n22:57:29.98 SUCCESS: zfs create testpool/testctr\r\n22:57:30.15 SUCCESS: zfs create testpool/testctr/testfs\r\n22:57:30.29 SUCCESS: zfs create -V 150m testpool/testctr/testvol\r\n22:57:33.04 NOTE: SUCCESS: newfs /dev/zvol/testpool/testctr/testvol>/dev/null\r\n22:57:33.13 SUCCESS: mount /dev/zvol/testpool/testctr/testvol /var/tmp/testdir1\r\n22:57:33.18 SUCCESS: zfs snapshot testpool/testctr/testfs@testsnap\r\n22:57:33.23 SUCCESS: zfs snapshot testpool/testctr/testvol@testsnap\r\n22:57:33.26 SUCCESS: pgrep -fl mkbusy exited 1\r\n22:57:33.32 SUCCESS: umount -f /var/tmp/testdir1\r\n22:57:33.47 SUCCESS: zfs destroy -r testpool/testctr/testfs\r\n22:57:33.49 SUCCESS: datasetexists testpool/testctr\r\n22:57:33.51 SUCCESS: datasetexists testpool/testctr/testvol\r\n22:57:33.53 SUCCESS: datasetnonexists testpool/testctr/testfs\r\n22:57:33.55 SUCCESS: datasetexists testpool/testctr/testvol@testsnap\r\n22:57:33.56 SUCCESS: datasetnonexists testpool/testctr/testfs@testsnap\r\n22:57:33.56 NOTE: 'zfs destroy -r testpool/testctr/testfs' passed.\r\n22:57:33.56 NOTE: Starting test: zfs destroy -R testpool/testctr/testfs\r\n22:57:33.83 SUCCESS: zfs destroy -Rf testpool/testctr\r\n22:57:33.97 SUCCESS: zfs create testpool/testctr\r\n22:57:34.14 SUCCESS: zfs create testpool/testctr/testfs\r\n22:57:34.28 SUCCESS: zfs create -V 150m testpool/testctr/testvol\r\n22:57:37.05 NOTE: SUCCESS: newfs /dev/zvol/testpool/testctr/testvol>/dev/null\r\n22:57:37.14 SUCCESS: mount /dev/zvol/testpool/testctr/testvol /var/tmp/testdir1\r\n22:57:37.19 SUCCESS: zfs snapshot testpool/testctr/testfs@testsnap\r\n22:57:37.24 SUCCESS: zfs snapshot testpool/testctr/testvol@testsnap\r\n22:57:37.42 SUCCESS: zfs clone testpool/testctr/testfs@testsnap testpool/testfsclone\r\n22:57:37.51 SUCCESS: zfs clone testpool/testctr/testvol@testsnap testpool/testvolclone\r\n22:57:37.57 SUCCESS: pgrep -fl mkbusy exited 1\r\n22:57:37.63 SUCCESS: umount -f /var/tmp/testdir1\r\n22:57:37.87 SUCCESS: zfs destroy -R testpool/testctr/testfs\r\n22:57:37.89 SUCCESS: datasetexists testpool/testctr\r\n22:57:37.92 SUCCESS: datasetexists testpool/testctr/testvol\r\n22:57:37.93 SUCCESS: datasetnonexists testpool/testctr/testfs\r\n22:57:37.96 SUCCESS: datasetexists testpool/testctr/testvol@testsnap\r\n22:57:37.97 SUCCESS: datasetnonexists testpool/testctr/testfs@testsnap\r\n22:57:37.99 SUCCESS: datasetexists testpool/testvolclone\r\n22:57:38.00 SUCCESS: datasetnonexists testpool/testfsclone\r\n22:57:38.01 NOTE: 'zfs destroy -R testpool/testctr/testfs' passed.\r\n22:57:38.01 NOTE: Starting test: zfs destroy -f testpool/testctr/testfs\r\n22:57:38.36 SUCCESS: zfs destroy -Rf testpool/testctr\r\n22:57:38.51 SUCCESS: zfs create testpool/testctr\r\n22:57:38.68 SUCCESS: zfs create testpool/testctr/testfs\r\n22:57:38.81 SUCCESS: zfs create -V 150m testpool/testctr/testvol\r\n22:57:41.58 NOTE: SUCCESS: newfs /dev/zvol/testpool/testctr/testvol>/dev/null\r\n22:57:41.67 SUCCESS: mount /dev/zvol/testpool/testctr/testvol /var/tmp/testdir1\r\n22:57:41.68 NOTE: mkbusy /testpool/testctr/testfs/testfile0  (pidlist:  21996)\r\n22:57:41.70 SUCCESS: zfs destroy -rR testpool/testctr/testfs exited 1\r\n22:57:41.71 SUCCESS: kill -TERM 21996\r\n22:57:41.74 SUCCESS: pgrep -fl mkbusy exited 1\r\n22:57:41.80 SUCCESS: umount -f /var/tmp/testdir1\r\n22:57:41.91 SUCCESS: zfs destroy -f testpool/testctr/testfs\r\n22:57:41.93 SUCCESS: datasetexists testpool/testctr\r\n22:57:41.96 SUCCESS: datasetexists testpool/testctr/testvol\r\n22:57:41.97 SUCCESS: datasetnonexists testpool/testctr/testfs\r\n22:57:41.97 NOTE: 'zfs destroy -f testpool/testctr/testfs' passed.\r\n22:57:41.97 NOTE: Starting test: zfs destroy -rf testpool/testctr/testfs\r\n22:57:42.20 SUCCESS: zfs destroy -Rf testpool/testctr\r\n22:57:42.35 SUCCESS: zfs create testpool/testctr\r\n22:57:42.52 SUCCESS: zfs create testpool/testctr/testfs\r\n22:57:42.67 SUCCESS: zfs create -V 150m testpool/testctr/testvol\r\n22:57:45.46 NOTE: SUCCESS: newfs /dev/zvol/testpool/testctr/testvol>/dev/null\r\n22:57:45.55 SUCCESS: mount /dev/zvol/testpool/testctr/testvol /var/tmp/testdir1\r\n22:57:45.60 SUCCESS: zfs snapshot testpool/testctr/testfs@testsnap\r\n22:57:45.64 SUCCESS: zfs snapshot testpool/testctr/testvol@testsnap\r\n22:57:45.66 NOTE: mkbusy /testpool/testctr/testfs/testfile0  (pidlist:  22627)\r\n22:57:45.71 SUCCESS: zfs destroy -rR testpool/testctr/testfs exited 1\r\n22:57:45.72 SUCCESS: kill -TERM 22627\r\n22:57:45.75 SUCCESS: pgrep -fl mkbusy exited 1\r\n22:57:45.81 SUCCESS: umount -f /var/tmp/testdir1\r\n22:57:45.91 SUCCESS: zfs destroy -rf testpool/testctr/testfs\r\n22:57:45.93 SUCCESS: datasetexists testpool/testctr\r\n22:57:45.96 SUCCESS: datasetexists testpool/testctr/testvol\r\n22:57:45.97 SUCCESS: datasetnonexists testpool/testctr/testfs\r\n22:57:45.99 SUCCESS: datasetexists testpool/testctr/testvol@testsnap\r\n22:57:46.00 SUCCESS: datasetnonexists testpool/testctr/testfs@testsnap\r\n22:57:46.00 NOTE: 'zfs destroy -rf testpool/testctr/testfs' passed.\r\n22:57:46.00 NOTE: Starting test: zfs destroy -Rf testpool/testctr/testfs\r\n22:57:46.30 SUCCESS: zfs destroy -Rf testpool/testctr\r\n22:57:46.44 SUCCESS: zfs create testpool/testctr\r\n22:57:46.62 SUCCESS: zfs create testpool/testctr/testfs\r\n22:57:46.80 SUCCESS: zfs create -V 150m testpool/testctr/testvol\r\n22:57:49.53 NOTE: SUCCESS: newfs /dev/zvol/testpool/testctr/testvol>/dev/null\r\n22:57:49.62 SUCCESS: mount /dev/zvol/testpool/testctr/testvol /var/tmp/testdir1\r\n22:57:49.67 SUCCESS: zfs snapshot testpool/testctr/testfs@testsnap\r\n22:57:49.72 SUCCESS: zfs snapshot testpool/testctr/testvol@testsnap\r\n22:57:49.89 SUCCESS: zfs clone testpool/testctr/testfs@testsnap testpool/testfsclone\r\n22:57:49.99 SUCCESS: zfs clone testpool/testctr/testvol@testsnap testpool/testvolclone\r\n22:57:50.02 NOTE: mkbusy /testpool/testctr/testfs/testfile0  (pidlist:  23413)\r\n22:57:50.19 SUCCESS: zfs destroy -rR testpool/testctr/testfs exited 1\r\n22:57:50.20 SUCCESS: kill -TERM 23413\r\n22:57:50.22 SUCCESS: pgrep -fl mkbusy exited 1\r\n22:57:50.30 SUCCESS: umount -f /var/tmp/testdir1\r\n22:57:50.41 SUCCESS: zfs destroy -Rf testpool/testctr/testfs\r\n22:57:50.43 SUCCESS: datasetexists testpool/testctr\r\n22:57:50.45 SUCCESS: datasetexists testpool/testctr/testvol\r\n22:57:50.47 SUCCESS: datasetnonexists testpool/testctr/testfs\r\n22:57:50.49 SUCCESS: datasetexists testpool/testctr/testvol@testsnap\r\n22:57:50.50 SUCCESS: datasetnonexists testpool/testctr/testfs@testsnap\r\n22:57:50.52 SUCCESS: datasetexists testpool/testvolclone\r\n22:57:50.54 SUCCESS: datasetnonexists testpool/testfsclone\r\n22:57:50.54 NOTE: 'zfs destroy -Rf testpool/testctr/testfs' passed.\r\n22:57:50.54 NOTE: Starting test: zfs destroy -r testpool/testctr/testvol\r\n22:57:50.88 SUCCESS: zfs destroy -Rf testpool/testctr\r\n22:57:51.02 SUCCESS: zfs create testpool/testctr\r\n22:57:51.19 SUCCESS: zfs create testpool/testctr/testfs\r\n22:57:51.32 SUCCESS: zfs create -V 150m testpool/testctr/testvol\r\n22:57:54.10 NOTE: SUCCESS: newfs /dev/zvol/testpool/testctr/testvol>/dev/null\r\n22:57:54.19 SUCCESS: mount /dev/zvol/testpool/testctr/testvol /var/tmp/testdir1\r\n22:57:54.27 SUCCESS: zfs snapshot testpool/testctr/testfs@testsnap\r\n22:57:54.32 SUCCESS: zfs snapshot testpool/testctr/testvol@testsnap\r\n22:57:54.35 SUCCESS: pgrep -fl mkbusy exited 1\r\n22:57:54.39 SUCCESS: umount -f /var/tmp/testdir1\r\n22:57:54.63 SUCCESS: zfs destroy -r testpool/testctr/testvol\r\n22:57:54.65 SUCCESS: datasetexists testpool/testctr\r\n22:57:54.66 SUCCESS: datasetexists testpool/testctr/testfs\r\n22:57:54.68 SUCCESS: datasetexists testpool/testctr/testfs@testsnap\r\n22:57:54.69 SUCCESS: datasetnonexists testpool/testctr/testvol\r\n22:57:54.70 SUCCESS: datasetnonexists testpool/testctr/testvol@testsnap\r\n22:57:54.70 NOTE: 'zfs destroy -r testpool/testctr/testvol' passed.\r\n22:57:54.70 NOTE: Starting test: zfs destroy -R testpool/testctr/testvol\r\n22:57:54.98 SUCCESS: zfs destroy -Rf testpool/testctr\r\n22:57:55.12 SUCCESS: zfs create testpool/testctr\r\n22:57:55.29 SUCCESS: zfs create testpool/testctr/testfs\r\n22:57:55.43 SUCCESS: zfs create -V 150m testpool/testctr/testvol\r\n22:57:58.22 NOTE: SUCCESS: newfs /dev/zvol/testpool/testctr/testvol>/dev/null\r\n22:57:58.31 SUCCESS: mount /dev/zvol/testpool/testctr/testvol /var/tmp/testdir1\r\n22:57:58.37 SUCCESS: zfs snapshot testpool/testctr/testfs@testsnap\r\n22:57:58.44 SUCCESS: zfs snapshot testpool/testctr/testvol@testsnap\r\n22:57:58.59 SUCCESS: zfs clone testpool/testctr/testfs@testsnap testpool/testfsclone\r\n22:57:58.73 SUCCESS: zfs clone testpool/testctr/testvol@testsnap testpool/testvolclone\r\n22:57:58.78 SUCCESS: pgrep -fl mkbusy exited 1\r\n22:57:58.85 SUCCESS: umount -f /var/tmp/testdir1\r\n22:57:59.10 SUCCESS: zfs destroy -R testpool/testctr/testvol\r\n22:57:59.12 SUCCESS: datasetexists testpool/testctr\r\n22:57:59.14 SUCCESS: datasetexists testpool/testctr/testfs\r\n22:57:59.15 SUCCESS: datasetexists testpool/testctr/testfs@testsnap\r\n22:57:59.16 SUCCESS: datasetnonexists testpool/testctr/testvol\r\n22:57:59.18 SUCCESS: datasetnonexists testpool/testctr/testvol@testsnap\r\n22:57:59.19 SUCCESS: datasetexists testpool/testfsclone\r\n22:57:59.20 SUCCESS: datasetnonexists testpool/testvolclone\r\n22:57:59.20 NOTE: 'zfs destroy -R testpool/testctr/testvol' passed.\r\n22:57:59.20 NOTE: Starting test: zfs destroy -f testpool/testctr/testvol\r\n22:57:59.20 NOTE: UNSUPPORTED: '-f ' is only available for  leaf FS.\r\n22:57:59.20 NOTE: Starting test: zfs destroy -rf testpool/testctr/testvol\r\n22:57:59.55 SUCCESS: zfs destroy -Rf testpool/testctr\r\n22:57:59.69 SUCCESS: zfs create testpool/testctr\r\n22:57:59.86 SUCCESS: zfs create testpool/testctr/testfs\r\n22:58:00.02 SUCCESS: zfs create -V 150m testpool/testctr/testvol\r\n22:58:02.75 NOTE: SUCCESS: newfs /dev/zvol/testpool/testctr/testvol>/dev/null\r\n22:58:02.84 SUCCESS: mount /dev/zvol/testpool/testctr/testvol /var/tmp/testdir1\r\n22:58:02.90 SUCCESS: zfs snapshot testpool/testctr/testfs@testsnap\r\n22:58:02.94 SUCCESS: zfs snapshot testpool/testctr/testvol@testsnap\r\n22:58:02.95 NOTE: mkbusy /var/tmp/testdir1/testfile0  (pidlist:  25668)\r\n22:58:03.01 SUCCESS: zfs destroy -rR testpool/testctr/testvol exited 1\r\n22:58:03.02 SUCCESS: kill -TERM 25668\r\n22:58:03.05 SUCCESS: pgrep -fl mkbusy exited 1\r\n22:58:03.10 SUCCESS: umount -f /var/tmp/testdir1\r\n22:58:03.25 SUCCESS: zfs destroy -rf testpool/testctr/testvol\r\n22:58:03.27 SUCCESS: datasetexists testpool/testctr\r\n22:58:03.28 SUCCESS: datasetexists testpool/testctr/testfs\r\n22:58:03.30 SUCCESS: datasetexists testpool/testctr/testfs@testsnap\r\n22:58:03.31 SUCCESS: datasetnonexists testpool/testctr/testvol\r\n22:58:03.32 SUCCESS: datasetnonexists testpool/testctr/testvol@testsnap\r\n22:58:03.32 NOTE: 'zfs destroy -rf testpool/testctr/testvol' passed.\r\n22:58:03.32 NOTE: Starting test: zfs destroy -Rf testpool/testctr/testvol\r\n22:58:03.58 SUCCESS: zfs destroy -Rf testpool/testctr\r\n22:58:03.73 SUCCESS: zfs create testpool/testctr\r\n22:58:03.90 SUCCESS: zfs create testpool/testctr/testfs\r\n22:58:04.03 SUCCESS: zfs create -V 150m testpool/testctr/testvol\r\n22:58:06.79 NOTE: SUCCESS: newfs /dev/zvol/testpool/testctr/testvol>/dev/null\r\n22:58:06.88 SUCCESS: mount /dev/zvol/testpool/testctr/testvol /var/tmp/testdir1\r\n22:58:06.94 SUCCESS: zfs snapshot testpool/testctr/testfs@testsnap\r\n22:58:06.98 SUCCESS: zfs snapshot testpool/testctr/testvol@testsnap\r\n22:58:07.11 SUCCESS: zfs clone testpool/testctr/testfs@testsnap testpool/testfsclone\r\n22:58:07.21 SUCCESS: zfs clone testpool/testctr/testvol@testsnap testpool/testvolclone\r\n22:58:07.21 NOTE: mkbusy /var/tmp/testdir1/testfile0  (pidlist:  26437)\r\n22:58:07.26 SUCCESS: zfs destroy -rR testpool/testctr/testvol exited 1\r\n22:58:07.28 SUCCESS: kill -TERM 26437\r\n22:58:07.32 SUCCESS: pgrep -fl mkbusy exited 1\r\n22:58:07.37 SUCCESS: umount -f /var/tmp/testdir1\r\n22:58:07.62 SUCCESS: zfs destroy -Rf testpool/testctr/testvol\r\n22:58:07.64 SUCCESS: datasetexists testpool/testctr\r\n22:58:07.65 SUCCESS: datasetexists testpool/testctr/testfs\r\n22:58:07.67 SUCCESS: datasetexists testpool/testctr/testfs@testsnap\r\n22:58:07.68 SUCCESS: datasetnonexists testpool/testctr/testvol\r\n22:58:07.69 SUCCESS: datasetnonexists testpool/testctr/testvol@testsnap\r\n22:58:07.71 SUCCESS: datasetexists testpool/testfsclone\r\n22:58:07.72 SUCCESS: datasetnonexists testpool/testvolclone\r\n22:58:07.72 NOTE: 'zfs destroy -Rf testpool/testctr/testvol' passed.\r\n22:58:07.72 NOTE: Starting test: zfs destroy -r testpool/testctr/testfs@testsnap\r\n22:58:08.07 SUCCESS: zfs destroy -Rf testpool/testctr\r\n22:58:08.22 SUCCESS: zfs create testpool/testctr\r\n22:58:08.39 SUCCESS: zfs create testpool/testctr/testfs\r\n22:58:08.52 SUCCESS: zfs create -V 150m testpool/testctr/testvol\r\n22:58:11.26 NOTE: SUCCESS: newfs /dev/zvol/testpool/testctr/testvol>/dev/null\r\n22:58:11.35 SUCCESS: mount /dev/zvol/testpool/testctr/testvol /var/tmp/testdir1\r\n22:58:11.43 SUCCESS: zfs snapshot testpool/testctr/testfs@testsnap\r\n22:58:11.50 SUCCESS: zfs snapshot testpool/testctr/testvol@testsnap\r\n22:58:11.52 SUCCESS: pgrep -fl mkbusy exited 1\r\n22:58:11.58 SUCCESS: umount -f /var/tmp/testdir1\r\n22:58:11.64 SUCCESS: zfs destroy -r testpool/testctr/testfs@testsnap\r\n22:58:11.67 SUCCESS: datasetexists testpool/testctr\r\n22:58:11.68 SUCCESS: datasetexists testpool/testctr/testfs\r\n22:58:11.71 SUCCESS: datasetexists testpool/testctr/testvol\r\n22:58:11.73 SUCCESS: datasetexists testpool/testctr/testvol@testsnap\r\n22:58:11.74 SUCCESS: datasetnonexists testpool/testctr/testfs@testsnap\r\n22:58:11.74 NOTE: 'zfs destroy -r testpool/testctr/testfs@testsnap' passed.\r\n22:58:11.74 NOTE: Starting test: zfs destroy -R testpool/testctr/testfs@testsnap\r\n22:58:12.12 SUCCESS: zfs destroy -Rf testpool/testctr\r\n22:58:12.26 SUCCESS: zfs create testpool/testctr\r\n22:58:12.43 SUCCESS: zfs create testpool/testctr/testfs\r\n22:58:12.56 SUCCESS: zfs create -V 150m testpool/testctr/testvol\r\n22:58:15.35 NOTE: SUCCESS: newfs /dev/zvol/testpool/testctr/testvol>/dev/null\r\n22:58:15.44 SUCCESS: mount /dev/zvol/testpool/testctr/testvol /var/tmp/testdir1\r\n22:58:15.52 SUCCESS: zfs snapshot testpool/testctr/testfs@testsnap\r\n22:58:15.56 SUCCESS: zfs snapshot testpool/testctr/testvol@testsnap\r\n22:58:15.73 SUCCESS: zfs clone testpool/testctr/testfs@testsnap testpool/testfsclone\r\n22:58:15.83 SUCCESS: zfs clone testpool/testctr/testvol@testsnap testpool/testvolclone\r\n22:58:15.90 SUCCESS: pgrep -fl mkbusy exited 1\r\n22:58:15.92 SUCCESS: umount -f /var/tmp/testdir1\r\n22:58:15.97 cannot destroy 'testpool/testfsclone': dataset is busy\r\n22:58:15.97 ERROR: zfs destroy -R testpool/testctr/testfs@testsnap exited 1\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6100/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6087", "title": "Test case: send-c_volume (32-bit)", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | All\r\nDistribution Version    | \r\nLinux Kernel                 | All\r\nArchitecture                 | i686\r\nZFS Version                  | 0.7.0-rc3\r\nSPL Version                  | 0.7.0-rc3\r\n\r\n### Describe the problem you're observing\r\n\r\nThe recently added `send-c_volume` test case fails occasionally on 32-bit systems.  The failure is the result of the stream size not matching the expected dataset size.  However, the behavior here isn't consistent and the test case often passes.\r\n\r\n```\r\nTest: tests/functional/rsend/send-c_volume (run as root) [00:08] [FAIL]\r\n```\r\n### Describe how to reproduce the problem\r\n\r\nThe issue is more likely to manifest itself in the buildbot test environment.\r\n\r\nhttp://build.zfsonlinux.org/builders/Ubuntu%2014.04%20i686%20%28TEST%29/builds/1894/steps/shell_9/logs/log\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\n```\r\nTest: /usr/share/zfs/zfs-tests/tests/functional/rsend/send-c_volume (run as root) [00:08] [FAIL]\r\n02:08:40.70 ASSERTION: Verify compressed send works with volumes\r\n02:08:41.27 SUCCESS: zfs create -V 256m -o compress=lz4 testpool/newvol\r\n02:08:45.89 SUCCESS: dd if=/var/tmp/backdir-rsend/file.0 of=/dev/zvol/testpool/newvol bs=1024k\r\n02:08:46.02 SUCCESS: zfs snapshot testpool/newvol@snap\r\n02:08:46.18 SUCCESS: eval zfs send -c testpool/newvol@snap >/var/tmp/backdir-rsend/full\r\n02:08:46.58 SUCCESS: eval zfs recv -d testpool2 </var/tmp/backdir-rsend/full\r\n02:08:46.67 NOTE: Comparing 3218944 and 3147776 given 90% (calculated: 97.78%)\r\n02:08:46.75 NOTE: Comparing 3218944 and 3147776 given 90% (calculated: 97.78%)\r\n02:08:46.96 SUCCESS: dd seek=8 if=/var/tmp/backdir-rsend/file.1 of=/dev/zvol/testpool/newvol bs=1024k\r\n02:08:47.80 SUCCESS: zfs snapshot testpool/newvol@snap2\r\n02:08:47.99 SUCCESS: eval zfs send -c -i snap testpool/newvol@snap2 >/var/tmp/backdir-rsend/inc\r\n02:08:48.33 SUCCESS: eval zfs recv -d testpool2 </var/tmp/backdir-rsend/inc\r\n02:08:48.45 NOTE: Comparing 3206656 and 921600 given 90% (calculated: 28.74%)\r\n02:08:48.46 NOTE: Performing test-fail callback (/usr/share/zfs/zfs-tests/callbacks/zfs_dbgmsg.ksh)\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6087/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6066", "title": "Test case: rsend_008_pos", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | all\r\nDistribution Version    | all\r\nLinux Kernel                 | all\r\nArchitecture                 | all\r\nZFS Version                  | 0.7.0-rc3\r\n\r\nSPL Version                  | 0.7.0-rc3\r\n\r\n### Describe the problem you're observing\r\n\r\nEasily reproduced failure of rsend_008_pos during automated testing.  This same failure is known to occur on other OpenZFS platforms.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nReproducible by the buildbot and locally by running the test case.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\n```\r\nTest: /home/behlendo/src/git/zfs/tests/zfs-tests/tests/functional/rsend/rsend_008_pos (run as root) [00:00] [FAIL]\r\n14:40:35.56 ASSERTION: Changes made by 'zfs promote' can be properly received.\r\n14:40:35.62 SUCCESS: zfs promote testpool/pclone\r\n14:40:35.68 SUCCESS: zfs promote testpool/testfs/fs1/fclone\r\n14:40:35.75 SUCCESS: zfs promote testpool/testfs/vclone\r\n14:40:35.90 lt-zfs: libzfs_sendrecv.c:1506: dump_filesystems: Assertion `progress' failed. /home/behlendo/src/git/zfs/tests/zfs-tests/tests/functional/rsend/rsend_008_pos.ksh[89]: log_must[68]: log_pos[254]: eval: line 1: 14925: Abort\r\n14:40:35.90 ERROR: eval zfs send -R testpool@final > /var/tmp/backdir-rsend/pool-final-R exited 262\r\n14:40:35.90 NOTE: Performing test-fail callback (/home/behlendo/src/git/zfs/tests/zfs-tests/callbacks/zfs_dbgmsg.ksh)\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6066/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/31864e3d8c9fc762d4c30324d9a061f4ed009446", "message": "OpenZFS 8652 - Tautological comparisons with ZPROP_INVAL\n\nusr/src/uts/common/sys/fs/zfs.h\r\n\tChange ZPROP_INVAL and ZPROP_CONT from macros to enum values.  Clang\r\n\tand GCC both prefer to use unsigned ints to store enums.  That was\r\n\tcausing tautological comparison warnings (and likely eliminating\r\n\terror handling code at compile time) whenever a zfs_prop_t or\r\n\tzpool_prop_t was compared to ZPROP_INVAL or ZPROP_CONT.  Making the\r\n\terror flags be explicity enum values forces the enum types to be\r\n\tsigned.\r\n\r\n\tZPROP_INVAL was also compared against two different enum types.  I\r\n\thad to change its name to ZPOOL_PROP_INVAL whenever its compared to\r\n\ta zpool_prop_t.  There are still some places where ZPROP_INVAL or\r\n\tZPROP_CONT is compared to a plain int, in code that doesn't know\r\n\twhether the int is storing a zfs_prop_t or a zpool_prop_t.\r\n\r\nusr/src/uts/common/fs/zfs/spa.c\r\n\ts/ZPROP_INVAL/ZPOOL_PROP_INVAL/\r\n\r\nAuthored by: Alan Somers <asomers@gmail.com>\r\nApproved by: Gordon Ross <gwr@nexenta.com>\r\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\r\nReviewed by: Igor Kozhukhov <igor@dilos.org>\r\nReviewed by: George Melikov <mail@gmelikov.ru>\r\nPorted-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\n\r\nOpenZFS-issue: https://www.illumos.org/issues/8652\r\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/c2de80dc74\r\nCloses #7061"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/1574c73bd0680cf3141e26627191120daba3fe8d", "message": "OpenZFS 8641 - \"zpool clear\" and \"zinject\" don't work on \"spare\" or \"replacing\" vdevs\n\nAdd \"spare\" and \"replacing\" to the list of interior vdev types in\r\nzpool_vdev_is_interior(), alongside the existing \"mirror\" and \"raidz\".\r\nThis fixes running \"zinject -d\" and \"zpool clear\" on spare and replacing\r\nvdevs.\r\n\r\nAuthored by: Alan Somers <asomers@gmail.com>\r\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\r\nReviewed by: George Melikov <mail@gmelikov.ru>\r\nApproved by: Gordon Ross <gwr@nexenta.com>\r\nPorted-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\n\r\nOpenZFS-issue: https://www.illumos.org/issues/8641\r\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/9a36801382\r\nCloses #7060"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/3da3488e6339ff2dc5c7f3da8c8a0c552d018d68", "message": "Fix shellcheck v0.4.6 warnings\n\nResolve new warnings reported after upgrading to shellcheck\r\nversion 0.4.6.  This patch contains no functional changes.\r\n\r\n* egrep is non-standard and deprecated. Use grep -E instead. [SC2196]\r\n* Check exit code directly with e.g. 'if mycmd;', not indirectly\r\n  with $?.  [SC2181]  Suppressed.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #7040"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/e1a0850c3570ae53df5779bc656f17b98b86f160", "message": "Force ztest to always use /dev/urandom\n\nFor ztest, which is solely for testing, using a pseudo random\r\nis entirely reasonable.  Using /dev/urandom ensures the system\r\nentropy pool doesn't get depleted thus stalling the testing.\r\nThis is a particular problem when testing in VMs.\r\n\r\nReviewed-by: Tim Chase <tim@chase2k.com>\r\nReviewed by: Thomas Caputi <tcaputi@datto.com>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #7017 \r\nCloses #7036"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/fed90353d799acbc5e81b0dfadc6d649b0f2e8b5", "message": "Support -fsanitize=address with --enable-asan\n\nWhen --enable-asan is provided to configure then build all user\r\nspace components with fsanitize=address.  For kernel support\r\nuse the Linux KASAN feature instead.\r\n\r\nhttps://github.com/google/sanitizers/wiki/AddressSanitizer\r\n\r\nWhen using gcc version 4.8 any test case which intentionally\r\ngenerates a core dump will fail when using --enable-asan.\r\nThe default behavior is to disable core dumps and only newer\r\nversions allow this behavior to be controled at run time with\r\nthe ASAN_OPTIONS environment variable.\r\n\r\nAdditionally, this patch includes some build system cleanup.\r\n\r\n* Rules.am updated to set the minimum AM_CFLAGS, AM_CPPFLAGS,\r\n  and AM_LDFLAGS.  Any additional flags should be added on a\r\n  per-Makefile basic.  The --enable-debug and --enable-asan\r\n  options apply to all user space binaries and libraries.\r\n\r\n* Compiler checks consolidated in always-compiler-options.m4\r\n  and renamed for consistency.\r\n\r\n* -fstack-check compiler flag was removed, this functionality\r\n  is provided by asan when configured with --enable-asan.\r\n\r\n* Split DEBUG_CFLAGS in to DEBUG_CFLAGS, DEBUG_CPPFLAGS, and\r\n  DEBUG_LDFLAGS.\r\n\r\n* Moved default kernel build flags in to module/Makefile.in and\r\n  split in to ZFS_MODULE_CFLAGS and ZFS_MODULE_CPPFLAGS.  These\r\n  flags are set with the standard ccflags-y kbuild mechanism.\r\n\r\n* -Wframe-larger-than checks applied only to binaries or\r\n  libraries which include source files which are built in\r\n  both user space and kernel space.  This restriction is\r\n  relaxed for user space only utilities.\r\n\r\n* -Wno-unused-but-set-variable applied only to libzfs and\r\n  libzpool.  The remaining warnings are the result of an\r\n  ASSERT using a variable when is always declared.\r\n\r\n* -D_POSIX_PTHREAD_SEMANTICS and -D__EXTENSIONS__ dropped\r\n  because they are Solaris specific and thus not needed.\r\n\r\n* Ensure $GDB is defined as gdb by default in zloop.sh.\r\n\r\nSigned-off-by: DHE <git@dehacked.net>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #7027"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/7e7f5132779a04da0070cf6e6ffd8e9b5f7692de", "message": "Disable history_004_pos\n\nOccasionally observed failure of history_004_pos due to the test\r\ncase not being 100% reliable.  In order to prevent false positives\r\ndisable this test case until it can be made reliable.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nIssue #7026 \r\nCloses #7028"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/bfe27ace0de64838d50ff351396423a481de6c84", "message": "Fix unused variable warnings\n\nResolved unused variable warnings observed after restricting\n-Wno-unused-but-set-variable to only libzfs and libzpool.\n\nReviewed-by: DHE <git@dehacked.net>\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\nCloses #6941"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/06401e42221d2f5130065caf70f8276ba4d19acd", "message": "Fix ztest_verify_dnode_bt() test case\n\nIn ztest_verify_dnode_bt the ztest_object_lock must be held in\norder to safely verify the unused bonus space.\n\nReviewed-by: DHE <git@dehacked.net>\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\nCloses #6941"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/b02becaa00aef3d25b30588bf49affbf1e9a84a4", "message": "Reduce codecov PR comments\n\nAttempt to reduce the number of comments posted by codecov\r\nto PR requests.  Based on the codecov documenation setting\r\n\"require_changes=yes\" and \"behavior=once\" should result in\r\na single comment under most circumstances.\r\n\r\nhttps://docs.codecov.io/v4.3.6/docs/pull-request-comments\r\n\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nIssue #7022 \r\nCloses #7025"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/0873bb6337452e3e028e40f5dad945b30deab185", "message": "Fix ARC hit rate\n\nWhen the compressed ARC feature was added in commit d3c2ae1\r\nthe method of reference counting in the ARC was modified.  As\r\npart of this accounting change the arc_buf_add_ref() function\r\nwas removed entirely.\r\n\r\nThis would have be fine but the arc_buf_add_ref() function\r\nserved a second undocumented purpose of updating the ARC access\r\ninformation when taking a hold on a dbuf.  Without this logic\r\nin place a cached dbuf would not migrate its associated\r\narc_buf_hdr_t to the MFU list.  This would negatively impact\r\nthe ARC hit rate, particularly on systems with a small ARC.\r\n\r\nThis change reinstates the missing call to arc_access() from\r\ndbuf_hold() by implementing a new arc_buf_access() function.\r\n\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Tony Hutter <hutter2@llnl.gov>\r\nReviewed-by: Tim Chase <tim@chase2k.com>\r\nReviewed by: George Wilson <george.wilson@delphix.com>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6171 \r\nCloses #6852 \r\nCloses #6989"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/bbffb59efc49baba1d131d497202e8cda0068c3d", "message": "Fix multihost stale cache file import\n\nWhen the multihost property is enabled it should be impossible to\r\nimport an active pool even using the force (-f) option.  This patch\r\nprevents a forced import from succeeding when importing with a\r\nstale cache file.\r\n\r\nThe root cause of the problem is that the kernel modules trusted\r\nthe hostid provided in configuration.  This is always correct when\r\nthe configuration is generated by scanning for the pool.  However,\r\nwhen using an existing cache file the hostid could be stale which\r\nwould result in the activity check being skipped.\r\n\r\nResolve the issue by always using the hostid read from the label\r\nconfiguration where the best uberblock was found.\r\n\r\nReviewed-by: Olaf Faaland <faaland1@llnl.gov>\r\nReviewed-by: Tony Hutter <hutter2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6933 \r\nCloses #6971"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/516c09d0d59585157e0804649aac7a675515fa7c", "message": "Remove lib/libspl/include/sys/frame.h\n\nThe functionality provided by this header is not required by any\r\nof the ZFS user space code.  Minimal functionality was provided\r\nin commit c28a677 which added include/sys/frame.h.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6960 \r\nCloses #6972"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/3ab3166347b84c6004002f9a6d06f7a87fe1cd4a", "message": "Disable vdev_zaps_004_pos\n\nOccasionally observed failure of vdev_zaps_004_pos due to the test\r\ncase not being 100% reliable.  In order to prevent false positives\r\ndisable this test case until it can be made reliable.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nIssue #6935\r\nCloses #6936"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/c28a67733c68b5540968136a7aca9633146afaf9", "message": "Suppress incorrect objtool warnings\n\nSuppress incorrect warnings from versions of objtool which are not\r\naware of x86 EVEX prefix instructions used for AVX512.\r\n\r\n  module/zfs/vdev_raidz_math_avx512bw.o: warning:\r\n  objtool: <func+offset>: can't find jump dest instruction at .text\r\n\r\nReviewed-by: Don Brady <don.brady@delphix.com>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6928"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/0c415a93d286ced3c552b671228e7d5d6c49d472", "message": "Disable create-o_ashift\n\nOccasionally observed failure of create-o_ashift due to the test\r\ncase not being 100% reliable.  In order to prevent false positives\r\ndisable this test case until it can be made reliable.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: loli10K <ezomori.nozomu@gmail.com>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nIssue #6924\r\nCloses #6925"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/7b3407003fde9eb78ea8ce5ce9165cef7e4795f3", "message": "Fix NFS sticky bit permission denied error\n\nWhen zfs_sticky_remove_access() was originally adapted for Linux\r\na typo was made which altered the intended behavior.  As described\r\nin the block comment, the intended behavior is that permission\r\nshould be granted when the entry is a regular file and you have\r\nwrite access.  That is, S_ISREG should have been used instead of\r\nS_ISDIR.\r\n\r\nRestricting permission to regular files made good sense for older\r\nsystems where setting the bit on executable files would instruct\r\nthe system to save the program's text segment on the swap device.\r\n\r\nOn modern systems this behavior has been replaced by the sticky\r\nbit acting as a restricted deletion flag and the plain file\r\nrestriction has been relaxed.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6889 \r\nCloses #6910"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/ea39f75f64ff72e30900a36e00632f180f5f6676", "message": "Fix 'zpool create|add' replication level check\n\nWhen the pool configuration contains a hole due to a previous device\r\nremoval ignore this top level vdev.  Failure to do so will result in\r\nthe current configuration being assessed to have a non-uniform\r\nreplication level and the expected warning will be disabled.\r\n\r\nThe zpool_add_010_pos test case was extended to cover this scenario.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6907 \r\nCloses #6911"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/72841b9fd957a392bb621393685b06dc042d4523", "message": "Preserve itx alloc size for zio_data_buf_free()\n\nUsing zio_data_buf_alloc() to allocate the itx's may be unsafe\r\nbecause the itx->itx_lr.lrc_reclen field is not constant from\r\nallocation to free.  Using a different itx->itx_lr.lrc_reclen\r\nsize in zio_data_buf_free() can result in the allocation being\r\nreturned to the wrong kmem cache.\r\n\r\nThis issue can be avoided entirely by storing the allocation size\r\nin itx->itx_size and using that for zio_data_buf_free().\r\n\r\nReviewed by: Prakash Surya <prakash.surya@delphix.com>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6912"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/94183a9d8a1133ff0d29666a86f84c24f2c4083c", "message": "Update for cppcheck v1.80\n\nResolve new warnings and errors from cppcheck v1.80.\r\n\r\n* [lib/libshare/libshare.c:543]: (warning)\r\n  Possible null pointer dereference: protocol\r\n* [lib/libzfs/libzfs_dataset.c:2323]: (warning)\r\n  Possible null pointer dereference: srctype\r\n* [lib/libzfs/libzfs_import.c:318]: (error)\r\n  Uninitialized variable: link\r\n* [module/zfs/abd.c:353]: (error) Uninitialized variable: sg\r\n* [module/zfs/abd.c:353]: (error) Uninitialized variable: i\r\n* [module/zfs/abd.c:385]: (error) Uninitialized variable: sg\r\n* [module/zfs/abd.c:385]: (error) Uninitialized variable: i\r\n* [module/zfs/abd.c:553]: (error) Uninitialized variable: i\r\n* [module/zfs/abd.c:553]: (error) Uninitialized variable: sg\r\n* [module/zfs/abd.c:763]: (error) Uninitialized variable: i\r\n* [module/zfs/abd.c:763]: (error) Uninitialized variable: sg\r\n* [module/zfs/abd.c:305]: (error) Uninitialized variable: tmp_page\r\n* [module/zfs/zpl_xattr.c:342]: (warning)\r\n   Possible null pointer dereference: value\r\n* [module/zfs/zvol.c:208]: (error) Uninitialized variable: p\r\n\r\nConvert the following suppression to inline.\r\n\r\n* [module/zfs/zfs_vnops.c:840]: (error)\r\n  Possible null pointer dereference: aiov\r\n\r\nExclude HAVE_UIO_ZEROCOPY and HAVE_DNLC from analysis since\r\nthese macro's will never be defined until this functionality\r\nis implemented.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6879"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/454365bbaacc153f98d2a3adaf33b13a6183d45d", "message": "Fix dirty check in dmu_offset_next()\n\nThe correct way to determine if a dnode is dirty is to check\r\nif any of the dn->dn_dirty_link's are active.  Relying solely\r\non the dn->dn_dirtyctx can result in the dnode being mistakenly\r\nreported as clean.\r\n\r\nReviewed-by: Chunwei Chen <tuxoko@gmail.com>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #3125 \r\nCloses #6867"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/13589da974e4e808b16d7dd280744277bb8d079b", "message": "Disable automatic dependencies in zfs-test package\n\nAll of the ZTS test scripts specify /bin/ksh as the interpreter.\r\nUnfortunately, as of Fedora 27 only /usr/bin/ksh is provided by\r\nthe package manager.  Rather than change all the scripts to\r\naccommodate the latest Fedora disable automatic dependencies\r\nfor the zfs-test package.  Functionally this will not cause\r\nany problems since /bin is a symlink to /usr/bin.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6868"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/71788d91f432e3b633fa26375ec13265882c9e3f", "message": "Disable zvol_ENOSPC_001_pos on 32-bit systems\n\nOccasionally observed failure of zvol_ENOSPC_001_pos due to the\r\ntest case taking too long to complete.  Disable the test case until\r\nit can be improved.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nIssue #5848 \r\nCloses #6862"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d8fdfc2d657b2926cf8d7ceb9675ff0df7265858", "message": "OpenZFS 8607 - variable set but not used\n\nReviewed by: Yuri Pankov <yuripv@gmx.com>\r\nReviewed by: Igor Kozhukhov <igor@dilos.org>\r\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\r\nApproved by: Robert Mustacchi <rm@joyent.com>\r\nAuthored by: Toomas Soome <tsoome@me.com>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nPorted-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\n\r\nOpenZFS-issue: https://www.illumos.org/issues/8607\r\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/b852c2f5\r\nCloses #6842"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/12954494a1bfe4c115332049cca1704d865f72d0", "message": "Disable automatic dependencies in DKMS package\n\nBy default additional dependencies are generated automatically for\r\npackages.  This is normally a good thing because it helps ensure\r\nthings just work.  It doesn't make sense for the DKMS package which\r\nrequires minimal dependencies that can be easily listed.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: loli10K <ezomori.nozomu@gmail.com>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6467 \r\nCloses #6835"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/34c2b3680bd705c3eabe32442c0d3c9ca82628b5", "message": "Initramfs fixes\n\n* initramfs: Fix inconsistent whitespace\r\n* initramfs: Fix a spelling error\r\n* initramfs: Set elevator=noop on the rpool's disks\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Richard Laager <rlaager@wiktel.com>\r\nCloses #6807"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/c9427c4696a244bc0d1bdecc37be320bb57ce54d", "message": "Add scan.coverity.com badge to README\n\nInclude the scan.coverity.com status badge in the top level README.\r\n\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6801"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/f4ae39a19da8a5756cc1287a426e1c8a62eeaaac", "message": "Fix status command options in zpool(8)\n\nThe 'zpool status' command supports the -P option for printing full\r\npath names.  It does not support the -p parsable option for printing\r\nexact values.\r\n    \r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: loli10K <ezomori.nozomu@gmail.com>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6792 \r\nCloses #6794"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/867959b5887c5211c520ad10ef8d12990a6d79fa", "message": "OpenZFS 8081 - Compiler warnings in zdb\n\nFix compiler warnings in zdb.  With these changes, FreeBSD can compile\r\nzdb with all compiler warnings enabled save -Wunused-parameter.\r\n\r\nusr/src/cmd/zdb/zdb.c\r\nusr/src/cmd/zdb/zdb_il.c\r\nusr/src/uts/common/fs/zfs/sys/sa.h\r\nusr/src/uts/common/fs/zfs/sys/spa.h\r\n\tFix numerous warnings, including:\r\n\t* const-correctness\r\n\t* shadowing global definitions\r\n\t* signed vs unsigned comparisons\r\n\t* missing prototypes, or missing static declarations\r\n\t* unused variables and functions\r\n\t* Unreadable array initializations\r\n\t* Missing struct initializers\r\n\r\nusr/src/cmd/zdb/zdb.h\r\n\tAdd a header file to declare common symbols\r\n\r\nusr/src/lib/libzpool/common/sys/zfs_context.h\r\nusr/src/uts/common/fs/zfs/arc.c\r\nusr/src/uts/common/fs/zfs/dbuf.c\r\nusr/src/uts/common/fs/zfs/spa.c\r\nusr/src/uts/common/fs/zfs/txg.c\r\n\tAdd a function prototype for zk_thread_create, and ensure that every\r\n\tcallback supplied to this function actually matches the prototype.\r\n\r\nusr/src/cmd/ztest/ztest.c\r\nusr/src/uts/common/fs/zfs/sys/zil.h\r\nusr/src/uts/common/fs/zfs/zfs_replay.c\r\nusr/src/uts/common/fs/zfs/zvol.c\r\n\tAdd a function prototype for zil_replay_func_t, and ensure that\r\n\tevery function of this type actually matches the prototype.\r\n\r\nusr/src/uts/common/fs/zfs/sys/refcount.h\r\n\tChange FTAG so it discards any constness of __func__, necessary\r\n\tsince existing APIs expect it passed as void *.\r\n\r\nPorting Notes:\r\n- Many of these fixes have already been applied to Linux.  For\r\n  consistency the OpenZFS version of a change was applied if the\r\n  warning was addressed in an equivalent but different fashion.\r\n\r\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\r\nReviewed by: Prakash Surya <prakash.surya@delphix.com>\r\nAuthored by: Alan Somers <asomers@gmail.com>\r\nApproved by: Richard Lowe <richlowe@richlowe.net>\r\nPorted-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\n\r\nOpenZFS-issue: https://www.illumos.org/issues/8081\r\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/843abe1b8a\r\nCloses #6787"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/a032ac4b3819408b2e17085224290b6a762de79a", "message": "OpenZFS 8558, 8602 - lwp_create() returns EAGAIN\n\n8558 lwp_create() returns EAGAIN on system with more than 80K ZFS filesystems\r\n\r\nOn a system with more than 80K ZFS filesystems, we've seen cases\r\nwhere lwp_create() will start to fail by returning EAGAIN. The\r\nproblem being, for each of those 80K ZFS filesystems, a taskq will\r\nbe created for each dataset as part of the ZIL for each dataset.\r\n\r\nPorting Notes:\r\n- The new nomem taskq kstat was dropped.\r\n- Added module options and documentation for new tunings\r\n  zfs_zil_clean_taskq_nthr_pct, zfs_zil_clean_taskq_minalloc,\r\n  zfs_zil_clean_taskq_maxalloc, and zfs_sync_taskq_batch_pct.\r\n\r\nReviewed by: George Wilson <george.wilson@delphix.com>\r\nReviewed by: Sebastien Roy <sebastien.roy@delphix.com>\r\nApproved by: Robert Mustacchi <rm@joyent.com>\r\nAuthored by: Prakash Surya <prakash.surya@delphix.com>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Chris Dunlop <chris@onthe.net.au>\r\nPorted-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\n\r\nOpenZFS-issue: https://www.illumos.org/issues/8558\r\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/216d772\r\n\r\n8602 remove unused \"dp_early_sync_tasks\" field from \"dsl_pool\" structure\r\n\r\nReviewed by: Serapheim Dimitropoulos <serapheim@delphix.com>\r\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\r\nApproved by: Robert Mustacchi <rm@joyent.com>\r\nAuthored by: Prakash Surya <prakash.surya@delphix.com>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Chris Dunlop <chris@onthe.net.au>\r\nPorted-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\n\r\nOpenZFS-issue: https://www.illumos.org/issues/8602\r\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/2bcb545\r\nCloses #6779"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d5e024cba215ddbe433658e2d19e611eab33e2c9", "message": "Emit history events for 'zpool create'\n\nHistory commands and events were being suppressed for the\r\n'zpool create' command since the history object did not\r\nyet exist.  Create the object earlier so this history\r\ndoesn't get lost.\r\n\r\nSplit the pool_destroy event in to pool_destroy and\r\npool_export so they may be distinguished.\r\n\r\nUpdated events_001_pos and events_002_pos test cases.  They\r\nnow check for the expected history events and were reworked\r\nto be more reliable.\r\n\r\nReviewed-by: Nathaniel Clark <nathaniel.l.clark@intel.com>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6712 \r\nCloses #6486"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/bbf1ad67cd0d6f3bc1e22a8a0c7448c15409d007", "message": "Remove vn_rename and vn_remove dependency\n\nThe only place vn_rename and vn_remove are used is when writing\r\nout an updated pool configuration file.  By truncating the file\r\ninstead of renaming and removing it we can avoid having to implement\r\nthese interfaces entirely.  Functionally an empty cache file is\r\ntreated the same as a missing cache file.  This is particularly\r\nadvantageous because the Linux kernel has never provided a way\r\nto reliably implement vn_rename and vn_remove.\r\n\r\nThe cachefile_004_pos.ksh test case was updated to understand\r\nthat an empty cache file is the same as a missing one.\r\n\r\nThe zfs-import-* systemd service files were not updated to use\r\nConditionFileNotEmpty in place of ConditionPathExists.  This\r\nmeans that after exporting all pools and rebooting new pools\r\nwill not the scanned for on the next boot.  This small change\r\nshould not impact normal usage since pools are not exported\r\nas part of a normal shutdown.\r\n\r\nDocumentation was updated accordingly.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Arkadiusz Buba\u0142a <arkadiusz.bubala@open-e.com>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses zfsonlinux/spl#648 \r\nCloses #6753"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/ca9b8e8797dbb7109a052bd3d7316ef27081531c", "message": "Update codecov.io behavior\n\nUpdate the codecov.yml included in the repository to behave as\r\noriginally intended.  This can be refined as needed.\r\n\r\n* Always post coverage results to the GitHub PR after two builds\r\n  have been uploaded.  This is the normal case since there will\r\n  be a build uploaded for both kernel and user coverage results.\r\n\r\n* Adjust red -> yellow -> green coloring in the web interface.\r\n  Due to the number of unlikely error conditions which are hard\r\n  to force consider 90% coverage an excellent level of coverage.\r\n\r\n* Allow a 1% variance in coverage between test runs.  This is\r\n  approximately 10x larger than the typical variance observed\r\n  which leaves us a reasonable margin to prevent false positives.\r\n\r\n* Always post a new smaller comment to PRs which does not include\r\n  a file list.  Old coverage reports are removed.\r\n\r\nReviewed by: Prakash Surya <prakash.surya@delphix.com>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6765"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/aea899a6fabbd8d0daf25a23ba114804b022529b", "message": "Increase default zloop.sh vdev size\n\nThe default 128M vdev size used by zloop.sh isn't always large\r\nenough and can result in ENOSPC failures which suspend the pool.\r\nIncrease the default size to 512M and provide a -s option which\r\ncan be used to specify an alternate size.\r\n\r\nThis does increase the free space requirements to run zloop.sh.\r\nHowever, since the vdevs are sparse 4x the space is not required.\r\n\r\nReviewed-by: Don Brady <don.brady@delphix.com>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6758"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/21a932b83c3e0c6fe5f42f874fd3664e67f374c7", "message": "Post-Encryption Followup\n\nThis PR includes fixes for bugs and documentation issues found \r\nafter the encryption patch was merged and general code improvements \r\nfor long-term maintainability.\r\n\r\nReviewed-by: Jorgen Lundman <lundman@lundman.net>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Tom Caputi <tcaputi@datto.com>\r\nIssue #6526\r\nCloses #6639\r\nCloses #6703\r\nCloese #6706\r\nCloses #6714\r\nCloses #6595"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/e0922b0421697294583804f00a67d10a77ecd6c6", "message": "Fixes for SPARC support\n\nThe current code base almost compiles on SPARC, but a few fixes are\r\nrequired for the code to compile (and work efficiently). Code in this \r\nPR comes from OpenZFS project which was initially dropped when porting\r\nthe crypto framework.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Pengcheng Xu <i@jsteward.moe>\r\nCloses #6733 \r\nCloses #6738 \r\nCloses #6750"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/29e07af5ae77e0ddf9ccfb77684f9713627b2ceb", "message": "Fix chattr/cleanup failure\n\nThe chattr cleanup step may fail to delete the user if there is still\r\nan active process running as that user.  Retry the userdel when this\r\noccurs to eliminate spurious false positves.\r\n\r\n  ERROR: userdel quser1 exited 8\r\n  userdel: user quser1 is currently used by process 26814\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6749"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/70f02287f86db33950eba9ceeb4f4c07c23131e0", "message": "Fix ARC behavior on 32-bit systems\n\nWith the addition of the ABD changes consumption of the virtual\r\naddress space has been greatly reduced.  This exposed an issue on\r\nCONFIG_HIGHMEM systems where free memory was being calculated\r\nincorrectly.  Functionally this didn't cause any major problems\r\nprior to ABD because a lack of available virtual address space\r\nwas used as an indicator of low memory.\r\n\r\nThis patch makes the following changes to address the issue and\r\nin the process realigns the code further with OpenZFS.  There\r\nare no substantive changes in behavior for 64-bit systems.\r\n\r\n* Added CONFIG_HIGHMEM case to the arc_all_memory() and\r\n  arc_free_memory() functions to only consider low memory pages\r\n  on CONFIG_HIGHMEM systems.\r\n\r\n* The arc_free_memory() function was updated to return bytes\r\n  instead of pages to be consistent with the other helper\r\n  functions.  In user space we make up some reasonable values\r\n  since currently only testing is performed in this context.\r\n\r\n* Adds three new values to the arcstats kstat to provide visibility\r\n  in to the ARC's assessment of the memory situation:\r\n  memory_all_bytes, memory_free_bytes, and memory_available_bytes.\r\n\r\n* Added kmem_reap() call to arc_available_memory() for 32-bit\r\n  builds to realign code with OpenZFS.\r\n\r\n* Reduced size of test file in /async_destroy_001_pos.ksh to\r\n  speed up test case.  Multiple txgs are still required.\r\n\r\n* Move vdevs used by zpool_clear_001_pos and zpool_upgrade_002_pos\r\n  to TEST_BASE_DIR location to speed up test cases.\r\n\r\nReviewed-by: David Quigley <david.quigley@intel.com>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #5352\r\nCloses #6734"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/57f4ef2e819670f8b28760b32745e0729f8d80d7", "message": "Fix abdstats kstat on 32-bit systems\n\nWhen decrementing the struct_size and scatter_chunk_waste kstats\r\nthe value needs to be cast to an int on 32-bit systems.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6721"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/c11f1004d19dd74e4be8869d211639413293dea0", "message": "Remove dead code from AVL tree\n\nThe avl_update_* functions are never used by ZFS and are therefore\r\nbeing removed.  They're barely even used in Illumos.  Additionally,\r\nsimplify avl_add() by using a VERIFY which produces exactly the same\r\nbehavior under Linux.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6716"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/7a6acb31b7a5bcc27a9b0313e46c411d2af353db", "message": "Fix \"--enable-code-coverage\" debug build\n\nWhen --enable-code-coverage is provided it should not result\r\nin NDEBUG being defined.  This is controlled by --enable-debug.\r\n\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6674"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/bb2773b358561145457670f2feb8c7c85b861711", "message": "Update codecov.yml\n\nUpdate the codecov.yml to make the following functional changes.\r\n\r\n* Do not require the CI testing to pass before posting results.\r\n* Set red-yellow-green coverage percent from 50%-100%\r\n* Allow a 1% drop in coverage to still be considered a pass.\r\n* Reduce the size of the comment posted to the issue.\r\n\r\nAdditionally, the top level README.markdown has been updated\r\nto include the codecov.io badge and the project summary reworded.\r\n\r\nReviewed-by: Prakash Surya <prakash.surya@delphix.com>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6669"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/4ce3c45a5e30a6ee698ea60be381c774050093ed", "message": "Increase default arc_c_min\n\nIncrease the default arc_c_min value to which whichever is larger,\r\neither 32M or 1/32 of total system memory.  This is advantageous for\r\nsystems with more than 1G of memory where performance issues may\r\noccur when the ARC is allowed to collapse below a minimum size.\r\nAt the same time we want to use the bare minimum value which is\r\nstill functional so the filesystem can be used in very low memory\r\nenvironments.\r\n\r\nReviewed-by: Tim Chase <tim@chase2k.com>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6659"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/848259c10f08694fd57c005aeb5ca8d724f046b6", "message": "Export symbol dmu_tx_mark_netfree()\n\nThis symbol is needed by Lustre for the same reason it was needed\r\nby the ZPL.  It should have been exported when the original patch\r\nwas merged.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Tony Hutter <hutter2@llnl.gov>\r\nReviewed-by: Alex Zhuravlev <bzzz@whamcloud.com>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6660"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/8e2dddab421e7131b7bd7eadd517fb36b59b2ddd", "message": "ZTS fix slog_replay_volume.ksh failure\n\nThe slog_replay_volume.ksh test case will fail when the pool is\r\nlayered on files in a filesystem which does not support discard.\r\nAvoid this issue by creating the pool using DISKS which will\r\neither be loopback device or real disk.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6654"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/a35b4cc8cca6eed5e93d54aa9ca2c72273a8943b", "message": "ZTS fix events_002_pos.sh failure\n\nFix spurious events_002_pos failures by waiting longer before\r\ngrabbing the log to check for the resilver_finish event.  It\r\nwould be better to rework this logic to wait only as long as\r\nneeded rather than a fixed timeout.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6651"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d9ec8b9b2a3880a1a2b68e57a25833486c3b164d", "message": "Add configure option to enable gcov analysis\n\n* Add configure option to enable gcov analysis.\r\n* Includes a few minor ctime fixes.\r\n* Add codecov.yml configuration.\r\n\r\nReviewed-by: Prakash Surya <prakash.surya@delphix.com>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6642"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/5c214ae318cbca37285ec38e6a2044a7002d31e9", "message": "Fix volume WR_INDIRECT log replay\n\nThe portion of the zvol_replay_write() handler responsible for\r\nreplaying indirect log records for some reason never existed.\r\nAs a result indirect log records were not being correctly replayed.\r\n\r\nThis went largely unnoticed since the majority of zvol log records\r\nwere of the type WR_COPIED or WR_NEED_COPY prior to OpenZFS 7578.\r\n\r\nThis patch updates zvol_replay_write() to correctly handle these\r\nlog records and adds a new test case which verifies volume replay\r\nto prevent any regression.  The existing test case which verified\r\nreplay on filesystem was renamed slog_replay_fs.ksh for clarity.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: loli10K <ezomori.nozomu@gmail.com>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6603 \r\nCloses #6615"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/e0dd0a32a8c8db725be673153b09bee0ca9adcf2", "message": "Revert \"Handle new dnode size in incremental...\"\n\nThis reverts commit 65dcb0f67a4d72ee4e1e534703db5caacf1ec85f until\na comprehensive fix is finalized.  The stricter interior dnode\ndetection in 4c5b89f59e4e5c8f5b4680040118ebde09598bbe and the new\ntest case added by this patch revealed a issue with resizing\ndnodes when receiving an incremental backup stream.\n\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\nIssue #6576"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/e771de534ff88a72b0531a85abc04ffb4333da53", "message": "Trim new line from zfs_vdev_scheduler\n\nAdd a helper function to trim the tailing new line.  While we're\r\nhere use this new hook to immediately apply the new scheduler.\r\n\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #3356 \r\nCloses #6573"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/133a5c6598ddc858f5b7ecedaf1364fcfe2e477f", "message": "zimport.sh: Allow custom pool create options\n\nAllow custom options to be passed to 'zpool create` when creating\r\na new pool.\r\n\r\nNormally zimport.sh is intented to prevent accidentally introduced\r\nincompatibilities so we want the default behavior.  However, when\r\nintroducing a known incompatibility with a feature flag we need a\r\nway to disable the feature.  By adding a line like the following\r\nto the commit message the feature can be disabled allowing the\r\npool to be compatibile with older versions.\r\n\r\nTEST_ZIMPORT_CREATE_OPTIONS=\"-o feature@encryption=disabled\"\r\n\r\n* Additionally fix /dev/nul -> /dev/null typo and minor white space\r\n  formating issues.\r\n\r\n* Updated fail function to print a message and exit with 1 for use\r\n  by the buildbot.\r\n\r\n* Silence warnings when zlib_inflate / zlib_default modules don't\r\n  exist.  This can happen when they're build in to the kernel.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6520"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/c8f9061fc714696a53cf4d14a4567f0a83dbf862", "message": "Retire legacy test infrastructure\n\n* Removed zpios kmod, utility, headers and man page.\r\n\r\n* Removed unused scripts zpios-profile/*, zpios-test/*,\r\n  zpool-config/*, smb.sh, zpios-sanity.sh, zpios-survey.sh,\r\n  zpios.sh, and zpool-create.sh.\r\n\r\n* Removed zfs-script-config.sh.in.  When building 'make' generates\r\n  a common.sh with in-tree path information from the common.sh.in\r\n  template.  This file and sourced by the test scripts and used\r\n  for in-tree testing, it is not included in the packages.  When\r\n  building packages 'make install' uses the same template to\r\n  create a new common.sh which is appropriate for the packaging.\r\n\r\n* Removed unused functions/variables from scripts/common.sh.in.\r\n  Only minimal path information and configuration environment\r\n  variables remain.\r\n\r\n* Removed unused scripts from scripts/ directory.\r\n\r\n* Remaining shell scripts in the scripts directory updated to\r\n  cleanly pass shellcheck and added to checked scripts.\r\n\r\n* Renamed tests/test-runner/cmd/ to tests/test-runner/bin/ to\r\n  match install location name.\r\n\r\n* Removed last traces of the --enable-debug-dmu-tx configure\r\n  options which was retired some time ago.\r\n\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6509"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/70322be8dc160f003fa95e034462ad625b325568", "message": "Fix ZTS grow_pool/setup\n\nThe addition of the large_dnode_008_pos test case, which runs\r\nright before this one, exposed some racy behavior in grow_pool\r\nsetup.sh on the Ubuntu kmemleak builder.  Before creating\r\npartitions on a device destroying any existing ones.\r\n\r\n  ERROR: set_partition 1  100mb loop0 exited 1\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6499 \r\nCloses #6516"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/c25b8f99f8dcbe898b81728e6a9dab107df4fc0b", "message": "Simplify threads, mutexs, cvs and rwlocks\n\n* Simplify threads, mutexs, cvs and rwlocks\r\n\r\n* Update the zk_thread_create() function to use the same trick\r\n  as Illumos.  Specifically, cast the new pthread_t to a void\r\n  pointer and return that as the kthread_t *.  This avoids the\r\n  issues associated with managing a wrapper structure and is\r\n  safe as long as the callers never attempt to dereference it.\r\n\r\n* Update all function prototypes passed to pthread_create() to\r\n  match the expected prototype.  We were getting away this with\r\n  before since the function were explicitly cast.\r\n\r\n* Replaced direct zk_thread_create() calls with thread_create()\r\n  for code consistency.  All consumers of libzpool now use the\r\n  proper wrappers.\r\n\r\n* The mutex_held() calls were converted to MUTEX_HELD().\r\n\r\n* Removed all mutex_owner() calls and retired the interface.\r\n  Instead use MUTEX_HELD() which provides the same information\r\n  and allows the implementation details to be hidden.  In this\r\n  case the use of the pthread_equals() function.\r\n\r\n* The kthread_t, kmutex_t, krwlock_t, and krwlock_t types had\r\n  any non essential fields removed.  In the case of kthread_t\r\n  and kcondvar_t they could be directly typedef'd to pthread_t\r\n  and pthread_cond_t respectively.\r\n\r\n* Removed all extra ASSERTS from the thread, mutex, rwlock, and\r\n  cv wrapper functions.  In practice, pthreads already provides\r\n  the vast majority of checks as long as we check the return\r\n  code.  Removing this code from our wrappers help readability.\r\n\r\n* Added TS_JOINABLE state flag to pass to request a joinable rather\r\n  than detached thread.  This isn't a standard thread_create() state\r\n  but it's the least invasive way to pass this information and is\r\n  only used by ztest.\r\n\r\nTEST_ZTEST_TIMEOUT=3600\r\n\r\nChunwei Chen <tuxoko@gmail.com>\r\nReviewed-by: Tom Caputi <tcaputi@datto.com>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #4547 \r\nCloses #5503 \r\nCloses #5523 \r\nCloses #6377 \r\nCloses #6495"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/46364cb2f35545a7fc915df9593b719a94c43a83", "message": "Add libtpool (thread pools)\n\nOpenZFS provides a library called tpool which implements thread\r\npools for user space applications.  Porting this library means\r\nthe zpool utility no longer needs to borrow the kernel mutex and\r\ntaskq interfaces from libzpool.  This code was updated to use\r\nthe tpool library which behaves in a very similar fashion.\r\n\r\nPorting libtpool was relatively straight forward and minimal\r\nmodifications were needed.  The core changes were:\r\n\r\n* Fully convert the library to use pthreads.\r\n* Updated signal handling.\r\n* lmalloc/lfree converted to calloc/free\r\n* Implemented portable pthread_attr_clone() function.\r\n\r\nFinally, update the build system such that libzpool.so is no\r\nlonger linked in to zfs(8), zpool(8), etc.  All that is required\r\nis libzfs to which the zcommon soures were added (which is the way\r\nit always should have been).  Removing the libzpool dependency\r\nresulted in several build issues which needed to be resolved.\r\n\r\n* Moved zfeature support to module/zcommon/zfeature_common.c\r\n* Moved ratelimiting to to module/zfs/zfs_ratelimit.c\r\n* Moved get_system_hostid() to lib/libspl/gethostid.c\r\n* Removed use of cmn_err() in zcommon source\r\n* Removed dprintf_setup() call from zpool_main.c and zfs_main.c\r\n* Removed highbit() and lowbit()\r\n* Removed unnecessary library dependencies from Makefiles\r\n* Removed fletcher-4 kstat in user space\r\n* Added sha2 support explicitly to libzfs\r\n* Added highbit64() and lowbit64() to zpool_util.c\r\n\r\nReviewed-by: Tony Hutter <hutter2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6442"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/9631681b75336ec6265d8fa5cecb353687c1f373", "message": "Fix dnode allocation race\n\nWhen performing concurrent object allocations using the new\r\nmulti-threaded allocator and large dnodes it's possible to\r\nallocate overlapping large dnodes.\r\n\r\nThis case should have been handled by detecting an error\r\nreturned by dnode_hold_impl().  But that logic only checked\r\nthe returned dnp was not-NULL, and the dnp variable was not\r\nreset to NULL when retrying.  Resolve this issue by properly\r\nchecking the return value of dnode_hold_impl().\r\n\r\nAdditionally, it was possible that dnode_hold_impl() would\r\nmisreport a dnode as free when it was in fact in use.  This\r\ncould occurs for two reasons:\r\n\r\n* The per-slot zrl_lock must be held over the entire critical\r\n  section which includes the alloc/free until the new dnode\r\n  is assigned to children_dnodes.  Additionally, all of the\r\n  zrl_lock's in the range must be held to protect moving\r\n  dnodes.\r\n\r\n* The dn->dn_ot_type cannot be solely relied upon to check\r\n  the type.  When allocating a new dnode its type will be\r\n  DMU_OT_NONE after dnode_create().  Only latter when\r\n  dnode_allocate() is called will it transition to the new\r\n  type.  This means there's a window when allocating where\r\n  it can mistaken for a free dnode.\r\n\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Ned Bass <bass6@llnl.gov>\r\nReviewed-by: Tony Hutter <hutter2@llnl.gov>\r\nReviewed-by: Olaf Faaland <faaland1@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6414 \r\nCloses #6439"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/1e0565d10a5f75bd15104df3db3264554e8b2dd1", "message": " Fix aarch64 build\n\nAdd aarch64 to the list of architecture which do not sanitize the\r\nLDFLAGS from the environment.  See fb963d33 for details.\r\n\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6424"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/ccad64314ab55963c1a0731ff62c8cf6c6976f28", "message": "Tag zfs-0.7.0\n\nMETA file and changelog updated.\n\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/9ff13dbe921c7177faee3f10c832e88bded39920", "message": "Fix zpool-features.5 indentation\n\nThe userobj_accounting feature described in the zpool-features.5\r\nman page was incorrectly indented.  Fix it.\r\n\r\nReviewed-by: Tony Hutter <hutter2@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6402"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/3f759c0c736d3a636614406ac0d6f9335819d6e9", "message": "Fix 'zpool clear' on suspended pools\n\n'zpool clear' should be able to resume I/O on suspended, but otherwise\r\nhealthy, pools.\r\n\r\n4a283c7 accidentally introduced a new code path where we call\r\ntxg_wait_synced() on the suspended pool before we had the chance to\r\nresume I/O via zio_resume(): this results in the 'zpool clear'\r\ncommand hanging indefinitely, waiting for a TXG that cannot be synced.\r\n\r\nFix this by avoiding the call to txg_wait_synced().\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: loli10K <ezomori.nozomu@gmail.com>\r\nReviewed-by: Tony Hutter <hutter2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6399"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/36ba27e9e07b35340ba388e6624e65995595ed92", "message": "Linux 4.13 compat: bio->bi_status and blk_status_t\n\nCommit torvalds/linux@4e4cbee9.  The bio->bi_error field was\r\nreplaced with bio->bi_status which is an enum that describes\r\nall possible error types.\r\n\r\nReviewed-by: Chunwei Chen <david.chen@osnexus.com>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nCloses #6351"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/ff1cb6bf26cd7d16c6f846ea9199e734025c6d6d", "message": "Tag 0.7.0-rc5\n\nFifth release candidate.\n\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\n`"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/cc9c6bcb7341cf37ccd4dfc939ec2abd4872e4bb", "message": "Fix vdev_probe() call outside SCL_STATE_ALL lock\n\nWhen an IO fails then zio_vdev_io_done() can call vdev_probe()\nto determine the health of the vdev.  This is safe as long as\nthe original zio was submitted with zio_wait() and holds the\nSCL_STATE_ALL lock over the operation.\n\nIf zio_no_wait() was used then the done callback will submit\nthe probe IO outside the SCL_STATE_ALL lock and hit this\nASSERT in zio_create()\n\n  ASSERT(!vd || spa_config_held(spa, SCL_STATE_ALL, RW_READER));\n\nResolve the issue by only allowing vdev_probe() to be called\nwhen there's a waiter indicating the caller is using zio_wait().\nThis assumes that caller is still holding SCL_STATE_ALL.\n\nThis issue isn't MMP specific but was surfaced when testing.\nWithout this patch it can be reproduced by running:\n\n  zpool set multihost on <pool>\n  zinject -d <vdev> -e io -T write -f 50 <pool> -L uber\n\nReviewed-by: Olaf Faaland <faaland1@llnl.gov>\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\nSigned-off-by: Don Brady <don.brady@intel.com>\nCloses #745\nCloses #6279"}], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6999", "title": "Extend deadman logic", "body": "### Description\r\n\r\nThe intent of this patch is extend the existing deadman code such that it's flexible enough to be used by both ztest and on production systems.  The proposed changes include:\r\n\r\n* Added a new `zfs_deadman_failmode` module option which is used to dynamically control the behavior of the deadman.  It's loosely modeled after, but independant from, the pool failmode property.  It can be set to wait, continue, or panic.\r\n\r\n    * wait     - Wait for the \"hung\" I/O (default)\r\n    * continue - Attempt to recover from a \"hung\" I/O\r\n    * panic    - Panic the system\r\n\r\n* Added a new `zfs_deadman_ziotime_ms` module option which is analogous to zfs_deadman_synctime_ms` except instead of applying to a pool TXG sync it applies to zio_wait().  A   default value of 300s is used to define a \"hung\" zio.\r\n\r\n* The ztest deadman thread has been re-enabled by default, aligned with the upstream OpenZFS code, and then extended to terminate the process when it takes significantly longer to complete than expected.\r\n\r\n* The -G option was added to ztest to print the internal debug log when a fatal error is encountered.  This same option was previously added to zdb in commit fa603f82.  Update zloop.sh to unconditionally pass -G to obtain additional debugging.\r\n\r\n* The FM_EREPORT_ZFS_DELAY event which was previously posted when the deadman detect a \"hung\" pool has been replaced by a new dedicated FM_EREPORT_ZFS_DEADMAN event.\r\n\r\n* The proposed recovery logic attempts to restart a \"hung\"  zio by calling zio_interrupt() on any outstanding leaf zios.  We may want to further restrict this to zios in either the  ZIO_STAGE_VDEV_IO_START or ZIO_STAGE_VDEV_IO_DONE stages.  Calling zio_interrupt() is expected to only be useful for cases when an IO has been submitted to the physical device\r\n  but for some reasonable the completion callback hasn't been called by the lower layers.  This shouldn't be possible but  has been observed and may be caused by kernel/driver bugs.\r\n\r\n* The 'zfs_deadman_synctime_ms' default value was reduced from 1000s to 600s.\r\n\r\n* Depending on how ztest fails there may be no cache file to move.  This should not be considered fatal, collect the logs which are available and carry on.\r\n\r\n### Motivation and Context\r\n\r\nAdd some of the needed infrastructure to make it possible to root cause `ztest` \"hangs\" observed during automated testing.  With this change applied at least basic debugging information will be collected for any \"hangs\".  This change can be further augmented with improvements to the debugging infrastructure.\r\n\r\nIssue #6901.\r\n\r\n### How Has This Been Tested?\r\n\r\nLocally by running `zloop.sh` in-tree for approximated 4 days.  Over this time period the deadman behaved as expected and properly terminated `ztest` when it appeared to be hung.  Further analysis of the debug logs and cores obtained is still needed.  The expectation is they will provide some statistical insight in the most often observed failures.\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Performance enhancement (non-breaking change which improves efficiency)\r\n- [x] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [x] I have updated the documentation accordingly.\r\n- [x] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [x] All new and existing tests passed.\r\n- [x] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "OWNER"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/147559", "body": "Thanks, I hadn't noticed the rendering issue.  Fixed by commit bbf3a3575c0b5795d3e4ddc27523258dc61ffa88.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/147559/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/195492", "body": "I'm not particularly happy with all this grubbing around in /dev/ either for 'zpool import', but for the moment I view it as a short term solution.  The longer term solution, which is well under way, is to be tightly integrated with libblkid.  There has been a patch submitted upstream and accepted by the maintainers to correctly identify a disk which belongs to a zfs pool.  Once a version of libblkid with this change filters back in to the distributions we can simply consult libblkid for the list of zfs devices and avoid checking /dev/.  In fact all the code on the zfs side is already in place for this.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/195492/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/282533", "body": "- Removed Makefile-sample, with the full integration in the build system it isn't needed.\n- Added all autogen.sh products (Makefile.in, configure) using the following versions of the utils.  Using the same versions of the tools minimizes how much change there is in the autogen products and makes it easier to review.\n  \n  autoconf (GNU Autoconf) 2.63\n  automake (GNU automake) 1.11.1\n  ltmain.sh (GNU libtool) 2.2.6b\n- Added the CDDL header to zvol_id_main.c, including correctly attributing the source.\n- Minor stray whitespace cleanup.\n- Update kmem_free() in zvol_remove_minors() to match  kmem_zalloc()'s use of MAXNAMELEN.  If we fail to do the the memory account code will flag this is a memory leak.  It's critical to ensure you alloc/free both use the same size for the buffer.\n- Add <sys/stat.h> header in zvol_id_main.c, without it my build was failing on RHEL6.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/282533/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/282739", "body": "Sorry!  I've force updated my branch to include the Makefile.in... in and the process obliterated the previous review comments.  We need to figure out how to handle this best, I'd really like to be landing one nice concise commit to fix an issue.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/282739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/383680", "body": "The zvol will be created with unique /dev/zdN names and then the /dev/zvol/pool/dataset links are created with udev rules.  This is exactly how normal block devices work such as /dev/sda with /dev/disk/by-_/_ links created with udev.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/383680/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/384675", "body": "This behavior is forced by the Linux kernel.  The special device files created under /dev/\\* has certain limitations including a maximum name length and certain reserved characters.  To avoid these limitations the standard solution is to create simple unique names at the top level /dev/\\* and symlink them with udev.  That's why all persistent storage devices work this way.  As you say you should never use these top level devices because their names may change.  This is equally true for /dev/sda, /dev/hda, and /dev/zd1.  The above comment is the code was simply an example test case and does not show a real usage scenario.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/384675/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/409945", "body": "I'm pretty sure dbuf_hold_impl() is called in other contexts.  I've love to revert this too but it's going to take more convincing that this is safe...  but that's for pointing it out, I'd actually forgotten about this particular hack!\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/409945/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/512022", "body": "I didn't either at the time or I would have added it to the original patch.  I only noticed later when it annoyed me.  :)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/512022/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/623636", "body": "Drat, I didn't test dash just sh.  Do you have a few minutes to propose a clean fix?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/623636/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/633128", "body": "Which fix do you prefer for debian/ubuntu.  Using bash or pulling /usr/bin/printf in to the initramfs... or both.  Presumably there are already other consumers of bash in the ubuntu initramfs.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/633128/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/635709", "body": "Alright, I'm going to leave it as is for now.  If you cook up a fix which works for Debian please let me know and I'll include it.  I will we tagging an 0.6.0-rc6 in the next day or two.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/635709/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/656247", "body": "I believe this behavior was modeled after what the lvm2 packages do.  Now I'm not a packaging guru (for any distro) so I'm not 100% sure if that's right or wrong but that's the history.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/656247/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/1531073", "body": "Probably over a year ago by now, the oldest supported kernel (i.e. kernel I test) is 2.6.26 which is what's used in Debian Lenny.  Supporting anything older just wasn't practical since there where some major API changes for the Posix layer needed.  Some people still do use ZFS with RHEL 5 but they have to install a newer kernel.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/1531073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/1832874", "body": "They are \"completely harmless\" in the sense that they are understood, expected to occur periodically, and as such are handled safely.  Thus I feel they should be suppressed to ensure they never mask any more interesting unexpected failures which get logged to the console.\n\nAs mentioned previously, the only fix for this is to never do this sort of thing.  That will address the root of the problem, but it's a ways off.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/1832874/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2223304", "body": "It does indeed.  Although perhaps a better variable name could have been used.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2223304/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2504347", "body": "There is but in the context of the lookup I believe we're safe since we're here either because the dentry isn't cached or the revalidate failed.  Still a good thought.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2504347/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/3524891", "body": "@ryao I felt it would be cleaner to just stick with using the unsigned version for now which has been implemented for quite some time.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/3524891/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/3528150", "body": "Unfortunately the Linux kernel doesn't support variable stack sizes when creating a thread.  Because of this numerous code path in the zfs code were reworked to save as much stack space as possible.  This code however was left as is to remain consistent with the upstream.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/3528150/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/3595460", "body": "Great, I'll get it merged for 0.6.2.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/3595460/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/4007121", "body": "@devZer0 Yes, yes it would.  However, we really didn't have the resources to get that parser done and tested for the 0.6.2 tag.  If someone wants to tackle that bit of work I think it's a good idea.  Or even better this sort of thing could be pursued with the upstream kernel developers, other filesystems will suffer from similar issues so it would be ideal if this was done generically.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/4007121/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/4530781", "body": "@dajhorn When the libzfs_core library was merged, which happened post 0.6.2 being tagged, it incorrectly set the library version to 1:1:0.  We wanted to roll it back to 1:0:0 for the initial version of the library which will first appear in 0.6.3.  There was a span of about 18 commits when it was 1:1:0.  So this shouldn't impact updates from 0.6.2 to 0.6.3.\n\na6ce1ea Fix libzfs_core changes to follow GNU libtool guidelines\n31fc193 Generate libraries with correct DT_NEEDED entries\n1db7b9b Fix libblkid support\n65ee05a Update detach section of zpool(8)\n40a806d Export symbols dsl_pool_config_{enter,exit}\n222b948 Fix memory leak false positive in log_internal()\n3549721 Update drive database\n36342b1 Export addition dsl_prop_\\* symbols\n8769db3 Allocate the ioctl \"output\" nvlist with KM_PUSHPAGE.\nc532223 Fix several new KM_SLEEP warnings\ncbfa294 Fix spa_deadman() TQ_SLEEP warning\nf9f3f1e Removing unneeded mutex for reading vq_pending_tree size\n77831e1 Reduce the stack usage of dsl_dataset_remove_clones_key\n34d5a5f Fix zpl_mknod() return values\n17897ce Fix uninitialized variables\nb83e3e4 Stop runtime pointer modifications in autotools checks\n4cf652e Fix dmu_objset_find_dp() KM_SLEEP warning\n13fe019 Illumos #3464\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/4530781/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/5342158", "body": "Indeed,  @lundman thanks for the extra set of eyes on this change.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/5342158/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/5950306", "body": "I love seeing patches which simplify the code and fix bugs.  This looks good to me.  In particular, I like the fact that it pulls the error logging out of what should be a generic library function.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/5950306/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6057966", "body": "I believe pool import _should_ occur after the block device are available due to the following in the service files.\n\n```\nRequires=systemd-udev-settle.service\nAfter=systemd-udev-settle.service\n```\n\n> The patch also doesn't seem to contemplate support for root on ZFS.\n\nThat's true, but this is definitely a step in the right direction for most installations.  I'm all for building on this to properly handle the root on ZFS case.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6057966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6057994", "body": "I agree it would be nice to support both of those use cases.  My suggestion would be to open a new issue for each of those features and those most familiar with systemd can hash out the right way to add that functionality.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6057994/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6815842", "body": "This definitely doesn't resolve the root problem of what caused the DVA to be damaged/incorrect in the first place.  I've opened #2426 so we don't loose track of running the root cause to ground for this so we can revert this change.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6815842/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/7020836", "body": "@CMCDragonkai relatime=[on|off] will have no effect unless atime=on\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/7020836/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/7038580", "body": "Yeah, this has been a problem in to past as well.  I've wondered if we shouldn't just blacklist strdup() and strfree() so they never used.  Sadly a a fair bit of code uses them...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/7038580/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/7196089", "body": "That's definitely true.  Then comment could have been better here we should update it.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/7196089/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/7702488", "body": "Yes, it should assuming innodb only requires AIO support.  If it also requires something like Direct IO which isn't yet supported then it won't.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/7702488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/8107721", "body": "There shouldn't be a $ in front of `source_tree`.  This was merged to master so we'll need to fix it there, but I did catch this before it ended up in the stable tag.  (to be pushed shortly).\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/8107721/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/8261237", "body": "@Rudd-O Yes, this doesn't completely address that issue, but along with d94fd5f0def2fbcb720647acac79ae75e2b9fa3b which is now in master forcing the import should no longer be required.  Although, it would be nice to get some independent confirmation of that.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/8261237/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/9036604", "body": "I'll ask around a bit at the upcoming Linux foundation collaboration summit.  Many of the key kernel developers attend so it'll be a good place to determine if this is something which can be changed.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/9036604/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/9173237", "body": "When I originally set this up it wasn't entirely clear to me that we should be setting this for filesystem which spanned multiple block devices.  At the time nothing else in Linux filesystem stack did this, everything always layered on top of something like md or lvm which presented a single block device.  It was also only used by NFS export so it wasn't critical early on.\n\nThat said we should probably revisit this decision.  It looks like btrfs does set this flag and it only really only means that `.s_dev` is set in the super block.  We are setting that these days so I'd expect that setting this flag should just work.  Have you tried enabling it?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/9173237/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/9319150", "body": "Yes, but let's consider it future proofing, although I agree the error handling here could be neater.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/9319150/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10290779", "body": "Nice catch, yes this shouldn't appear in the `zfs receive` section which already takes a `-e` argument.  Could you open a pull request to drop this hunk.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10290779/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10502943", "body": "Thanks!  Yes, yes it is.  I'll make a quick fix.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10502943/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10503156", "body": "I've pushed 74aa2ba259e61512bd029c9e8f857f0611a80bbd with the fix.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10503156/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10970634", "body": "`zfs` should be removed, and `zfs-mount`, `zfs-share`, should be added.\n\nMy plan is to spend the next few hours working through this pull request and testing it out on a CentOS 6 system so expect significant additional feedback.  Or in other words give be a little time to make all my comments before refreshing this.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10970634/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10971633", "body": "Using a shared `zfs-common` file for the distribution specific bits is something I'm normally all for, but I'm not sure it's worth the trouble in this case.  At this point we just need to maintain init scripts for lsb style systems and for CentOS/RHEL6.  Everything else at this point has moved to using systemd.\n\nSince we already have the infrastructure to install different init scripts what about install having a `zfs-mount.lsb` and a `zfs-mount.redhat`.  That would simplify testing greatly since every change wouldn't need to be tested on both platforms.  In some ways this might even make a `zfs-common.lsb` and `zfs-common.redhat` file more useful since it could clearly include the right common bits for that platform.\n\nA good example of one thing this would help with is the logging.  As I'm sure you're aware the way the lsb standard wants you to log things and they way Redhat wants it done are annoyingly different.  I see you've provided some wrappers to handle this but the resulted scripts would be easier to read if we didn't have to add this additional layer of abstraction.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10971633/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10971660", "body": "This is an excellent example of something which would be simpler with separate scripts.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10971660/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10971718", "body": "On a CentOS/RHEL system this file would be expected to be installed in `/etc/sysconfig/zfs`.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10971718/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10971752", "body": "Why are the lines wrapped at  about 60-65 character?  It looks like these comments could be cleaned up some.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10971752/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972306", "body": "In fact, it looks like many of these comments should be updated.  It's clear they assume that upstart or systemd is going to be part of the init process somehow.  Since currently this config file is just for the init scripts that's clearly not going to be the case.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972306/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972381", "body": "Sensible defaults\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972381/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972425", "body": "Why does this LOCKDIR differ from that in the defaults file?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972425/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972564", "body": "Why is this check needed?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972564/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972784", "body": "Oh, I see.  This is one of many checks which were brought in from the Fedora init scripts.  Unless this is functionality is needed on Debian for some reason I'd be strongly included to drop all of this.  Since Fedora moved to systemd many releases ago this code has been unused for a long time.  I would say that all that is strictly needed can be found in the existing zfs.redhat.in script.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972784/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972797", "body": "This hunk should definitely be dropped.  ZFS has been added to the default SELinux policies and works just fine.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972797/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972822", "body": "Why sleep?  Is this is ensure the /dev/zfs has been created?  If so that's worth a comment.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972822/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972844", "body": "This hunk would be good to drop unless you need to to support some specific supported configuration.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972844/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972939", "body": "This function looks to be unused.  Which is probably good because I'm not sure what the `mountinfo` command is or does.  \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972939/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972971", "body": "A possibly unused function if you determine that you can drop the /etc/mtab hunk above.  Same for the following ones.  This is quite a lot of complexity it would be nice to be able to drop,\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972971/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973346", "body": "Have you considered splitting this in to `zfs-import` and `zfs-mount` scripts.  This would allow the pool import and mount to be performed at different times during the boot.  One case where this is would be useful are legacy mount points as described in issue #3300.  This would allow the pool to be imported before /etc/fstab is processed.  Then slightly latter in the boot zfs-mount could be run to add the remaining zfs managed mounts.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973346/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973410", "body": "Another good reason to have a `.lsb` and `.redhat` version of these scripts is that it's entirely possible, maybe even likely, that we'd want these scripts to run at slightly different times during the boot.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973410/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973489", "body": "This is presumably used in the additional initrd patches?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973489/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973491", "body": "This is a handy bit of functionality.  \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973491/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973553", "body": "Wouldn't this fstab problem just solve itself by splitting the init script as described above?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973596", "body": "This is another good reason I hadn't thought of for splitting the script.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973596/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973717", "body": "`etc/init.d/default` should be added to the EXTRA_DIST and the Makefile update so it's installed in the correct location.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973717/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973810", "body": "Ahh good point.  Although at the moment that's definitely not true and I can see it causing confusion.  Perhaps it makes sense to keep this file sysv specific until/if the systemd bits use it.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973863", "body": "Then the question I guess is that something we still want to support / file useful.  I'm OK either way as long as we leave a little comment.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973863/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973915", "body": "I would _think_ but haven't verified that a `udevadm settle` after the modprobe would ensure we always wait long enough, but not longer.  Alternately, you could borrow the `wait_udev` function from `scripts/common.sh` and change this too `wait_udev /dev/zfs`.  That's a little better than just a sleep.\n\n```\nwait_udev() {\n        local DEVICE=$1\n        local DELAY=$2\n        local COUNT=0\n\n        udev_trigger\n        while [ ! -e ${DEVICE} ]; do\n                if [ ${COUNT} -gt ${DELAY} ]; then\n                        return 1\n                fi\n\n                let COUNT=${COUNT}+1\n                sleep 1\n        done\n\n        return 0\n}\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973915/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974070", "body": "I believe this was originally added to handle certain configurations where ZFS was used as the root filesystem on Fedora.  However, this script hasn't been used for that in a long time now so I've no idea if it will work as intended.  How about we drop it for now to keep things simple.  We can always add it back trivially if we discover a need for it.  It will be on the git commit history forever.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974070/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974465", "body": "It's not the ideal, but it's probably more practical and maintainable.  Particularly if we limit this two just the two variants proposed.  If they were combined I'd always be worried about accidentally breaking something unless it was tested on both platforms.  Plus this allows for:\n- Different default start/kill levels which could easily differs between Debian and Redhat.\n- Avoids the use of complicated logging wrappers.\n- Distribution specific customization can live in one place, the zfs-common script.\n\nThat said, if you really don't like this plan we can try working through this with them all in the same script and see how ugly it gets.  :)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974465/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974499", "body": "Right neither the `default` script nor any of the init scripts get added to the `make dist` tarball.  Adding all of the `.in` files to EXTRA_DIST will make sure they're included.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974499/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974574", "body": "Portability is good.  But what exactly are we trying to be portable with?  On the Fedora/Redhat side of things I can almost guarantee this would be dead code.  Fedora is all in on systemd, that ship has sailed.  RHEL6 and earlier and will need these init scripts for years to come, but the current scripts don't do this and it's never come up as an issue.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974574/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974592", "body": "It _may_ be unavoidable.  To my knowledge the distributions don't have to agree at all about when something should run.  \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974592/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974693", "body": "But if the pool is already imported much earlier in the boot, possibly in the initramfs, then we shouldn't need any special handling for this.  A normal Linux `mount -a` command would be able to mount any non-zfs filesystems on zvols or legacy-style zfs datasets.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974693/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974718", "body": "Fair point.  Clearly 5 are too many.  But perhaps 2 are a more reasonable number.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974718/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974852", "body": "> There are A LOT of distributions out there\u2026\n\nYes there are!  And to some degree it's a fools errand to even try to be portable with all of them.  Unless we're actively testing the scripts on those platforms there's little reason to expect they're going to work.  Distributions can be far too different.\n\nIn guess in part I view these scripts as a reasonable starting point for end distribution maintainers to use.  And the two major families of init scripts are either Debian-style of Redhat-style.  I somewhat expect they'll get modified a little downstream, at least until those changes can be integrated.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974852/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974949", "body": "OK.  Well how about this.  Go ahead and refresh this with markups we've discussed and keep it as just the **one** version.  Then when you're ready I'll give it another spin under CentOS 6 and see what issue I encounter in practice and have to work though.  If it turns out in practice I've been making a mountain out of the molehill and the discrepancies are minor we'll keep it as one.  If it's possible that's may preference as well, I'm just a bit concerned it won't be.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974949/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10975565", "body": "This script should have its permissions changed to 0775 like the others.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10975565/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10975638", "body": "Renaming this to just `zfs` so it's installed as `/etc/sysconfig/zfs` or `/etc/default/zfs` would be good.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10975638/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10975700", "body": "This should be as follows to ensure the right files are part of `make dist` and `make install` installs the init scripts.  (not 100% tested so double check it)\n\n``` diff\n-init_SCRIPTS = zfs-common zfs-mount zfs-share zfs-zed zfs-import\n-EXTRA_DIST = $(top_srcdir)/etc/init.d/zfs\n+init_SCRIPTS = zfs-common zfs-import zfs-mount zfs-share zfs-zed\n+EXTRA_DIST = \\\n+       $(top_srcdir)/etc/init.d/zfs \\\n+       $(top_srcdir)/etc/init.d/zfs-common.in \\\n+       $(top_srcdir)/etc/init.d/zfs-import \\\n+       $(top_srcdir)/etc/init.d/zfs-mount \\\n+       $(top_srcdir)/etc/init.d/zfs-share \\\n+       $(top_srcdir)/etc/init.d/zfs-zed\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10975700/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10975725", "body": "This should actually be as follows.  Where 30 is how many seconds you're willing to wait for before failing and returning and error.\n\n```\n    wait_udev /dev/zfs 30\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10975725/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10975788", "body": "NFS_SRV doesn't appear to be used anywhere in the proposed scripts.  Since this isn't, and shouldn't be, a hard dependency for ZFS let's drop this hunk.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10975788/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10992444", "body": "It's probably 1000x longer than needed.  :)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10992444/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11029164", "body": "@ryao do you happen to know if illumos has an equivalent?  If so then adding a wrapper might be better.  As always we can revisit this if/when it becomes and issue.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11029164/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11051517", "body": "The GNU coding standards don't appear to have anything to say about a `lockstatedir` directory.  Unlike, `runstatedir` which was added to accommodate systemd.  As much as don't like hard coding things, it appears that the right thing to do here is assume `/var/lock` which is right for all platforms I know of.  Otherwise this may expand to `/run/lock` which doesn't exist in CentOS6.\n\nhttps://www.gnu.org/prep/standards/html_node/Directory-Variables.html\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11051517/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11051545", "body": "As commented above, \"/var/lock\" is more correct and should at worst be a symlink to `/run/lock`.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11051545/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11052774", "body": "Me either, but it seems like the right thing here.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11052774/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053034", "body": "mkdir -p \"$LOCKDIR\"\n\nAdd the `-p` in case intermediate directories need to be created and quote $LOCKDIR in case it contains white space.  Both unlikely, but safer.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053034/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053092", "body": "This should be left to the caller since it assumes the caller has globally defined `$servicename`.  This is true for the current scripts but goes a little beyond what a library call should be assuming.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053092/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053128", "body": "FWIW this will probably _not_ be true for the next tag.  Once the ABD patches are merged we'll be past the worst of the 32-bit issues.  But if you want to leave it here for now that's OK.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053128/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053262", "body": "This check is inverted, should `||` not `&&`\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053262/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053316", "body": "Use 'break' not 'return' or only a single pool will ever be imported.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053316/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053320", "body": "Use 'break' not 'return' or only a single pool will ever be imported.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053320/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053323", "body": "Use 'break' not 'return' or only a single pool will ever be imported.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053323/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053356", "body": "[style] Let's try to mind the 80 character limit here and elsewhere.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053356/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053459", "body": "A trailing space needs to be added after the ':'.  Although, I'd suggest dropping the ';' and unifying the output with the (using cache file) message below.  I.e.\n\n```\nImporting ZFS pool tank2 using /dev/disk/by-id\nImporting ZFS pools using cache file\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053459/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053622", "body": "This `zfs_log_end_msg` is redundant and must be removed, it will occur after the `break`.  \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053622/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053753", "body": "I'd suggest dropping the chunk of this that unloads the zfs module.  If we've had any failures along the way this won't succeed, if the ZED is still running this won't succeed, if the end user compiled ZFS in to their kernel this won't succeed.  Plus in the end it isn't that valuable, at best you'll reclaim a little memory on your system.\n\nHowever, if you do feel a strong need to keep this then use `modprobe -r` to ensure dependent modules like zcommon and the spl are also unloaded.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053753/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053858", "body": "Let's be symmetric with the import log messages\n\n```\n                zfs_log_begin_msg \"Exporting ZFS pool $pool\" \n                \"$ZPOOL\" export $pool\n                RET=$?\n                zfs_log_end_msg $RET\n```\n\n```\n[behlendo@ovirt-guest-220 zfs]$ sudo /etc/init.d/zfs-import start\nImporting ZFS pool tank2 using: /dev/disk/by-id            [  OK  ]\nImporting ZFS pool tank1 using: /dev/disk/by-id            [  OK  ]\nImporting ZFS pool tank3 using: /dev/disk/by-id            [  OK  ]\n\n[behlendo@ovirt-guest-220 zfs]$ sudo /etc/init.d/zfs-import stop\nExporting ZFS pool tank1                                   [  OK  ]\nExporting ZFS pool tank2                                   [  OK  ]\nExporting ZFS pool tank3                                   [  OK  ]\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053858/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053880", "body": "Remove this message when logging each pool export as suggested.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053880/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053882", "body": "Remove this message when logging each pool export as suggested.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053882/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053915", "body": "Should be prefixed with `[ \"$VERBOSE_MOUNT\" == 'yes' ]` to match `do_start()`.  Or simply removed.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11053915/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11054017", "body": "I don't see a ton of utility in this, I'd just drop these log message entirely and the user of VERBOSE_MOUNT here.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11054017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11054296", "body": "I still don't see a strong need for maintaining this MTAB/FSTAB logic after splitting the init scripts in to zfs-import and zfs-mount.  I've verified that at least under CentOS 6 the /etc/fstab will be processed after the `zpool import` and therefore any filesystems created on zvols with be mounted properly with the usual mechanisms.\n\nI'd really like to drop all this complexity if possible.  What's the remaining legitimate use case for keeping it?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11054296/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11054341", "body": "I'd also like to point out that this code in fact doesn't actually work when I tested it.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11054341/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11054344", "body": "Should be '||' not '&&'\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11054344/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11054363", "body": "Then maybe add a no-op 'status' target like 'zfs-share'.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11054363/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11054368", "body": "Should be '||' not '&&'\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11054368/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11071239", "body": "It's OK for importing but not exporting?  My concern with the incremental progress is that it could easily exceed the 60 character limit for status.  What about just `Exporting ZFS pools` and no incremental status?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11071239/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11071305", "body": "No `-r` option, that is old.  OK, well then if you want to keep it will need to be moved to a library function much like `unload_modules()` in scripts/common.sh.in.  Unloading just the zfs.ko module won't be sufficient for allow for a new version to be loaded.  You must remove the entire stack zfs, zcommon, znvpair, spl, zavl, zunicode.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11071305/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11071395", "body": "It sounds like this might be fairly distribution specific then.  I tried this exact same test under CentOS6 and it was clear that non-root filesystems were always mounted after the import so it just worked.  Is this not the case under Debian?\n\nWhat if we just keep the updated code but make this functionality optional with a setting in the config file.  We can leave it disabled by default until those use cases surface.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11071395/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11071724", "body": "I could get on board with something like this, where there's a dot per pool.  This could optionally be extended to one line per-pool for debugging when `VERBOSE_IMPORT=on is set`.\n\n```\nImporting ZFS pools...                           [OK]\nExporting ZFS pools...                           [OK]\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11071724/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11072632", "body": "Enabling the script with `chkconfig` appears to cause it located very early in the boot.  This appears to work but I haven't dug to deeply to determine if it's exactly right.\n\n```\nS01zfs-import\nS02zfs-mount\nS30zfs-share\nK50zfs-zed\n```\n\nI've just started looking over the latest patched so I'll start a new round of review comments there.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11072632/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11072874", "body": "My understanding is that chkconfig will override the passed values in favor of the LSB-style stanza if it exists.  Plus it will honor the listed Requires.  That said, I'm not an expert here but from from the man page.\n\n```\n       chkconfig  also supports LSB-style init stanzas, and will apply them in\n       preference to \"chkconfig:\" lines where available.  A LSB  stanza  looks\n       like:\n       ### BEGIN INIT INFO\n       # Provides: foo\n       # Required-Start: bar\n       # Defalt-Start: 2 3 4 5\n       # Default-Stop: 0 1 6\n       # Description: Foo init script\n       ### END INIT INFO\n```\n\nI've also added the following lines to my /etc/fstab for testing and they do get mounted properly on boot.\n\n```\n/dev/tank2/ext4     /mnt/ext4       ext4    defaults    0 0\ntank1/zfs-filesystem    /mnt/zfs                zfs     defaults        0 0\n```\n\n```\n$ df -h\nFilesystem            Size  Used Avail Use% Mounted on\n/dev/mapper/VolGroup-lv_root\n                       62G   23G   36G  39% /\ntmpfs                 3.9G     0  3.9G   0% /dev/shm\n/dev/vda1             477M  117M  335M  26% /boot\n/dev/zd0             1008M  1.3M  956M   1% /mnt/ext4\ntank1/zfs-filesystem   31G     0   31G   0% /mnt/zfs\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11072874/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11072935", "body": "Actually, I half take it back.  They appear to have mounted properly but only due to a retry.  The following error were reported at boot.  I'll have to dig a little deeper.\n\n```\nRemounting root filesystem in read-write mode:             [  OK  ]\nMounting local filesystems:  mount: special device /dev/tank2/ext4 does not exist\nfilesystem 'tank1/zfs-filesystem' cannot be mounted, unable to open the dataset\n                                                           [FAILED]\nEnabling /etc/fstab swaps:                                 [  OK  ]\nEntering non-interactive startup\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11072935/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11073264", "body": "OK, it does not look like this will work out as I'd hoped on CentOS6.  There's no way that I can see to run `zfs-import` prior to the local mounts.  All the lvm/dm block device code happens in the `rc.sysinit` script prior to pivoting on to the root filesystem.  Bummer. \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11073264/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11223747", "body": "I wish there was a way to annotate the commit message but since there isn't I'll comment here.\n\n[style] If you use all '*'s rather that '+'s that's valid Github markdown and it will properly render the bullets.\n[style] Also git wants there to be a line of whitespace after the summary and before the commit message body.\n\nI don't think there's any need to credit me for the ZED init script refresh.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11223747/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11223798", "body": "s/script was/scripts were/\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11223798/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11223835", "body": "`[ -r /sys/module/zfs/version ] && return 0 || return 1` would cleanly handle having ZFS modules or having it built in to the kernel.  It's a more generic check for is ZFS kernel level functionality available.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11223835/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11223854", "body": "It's not clear to be this is actually wrong.  It's tempting to drop this comment which isn't particularly helpful.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11223854/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11223865", "body": "OK, I live with the leading `(` since it doesn't actually seem to matter.  As long as it's consistent.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11223865/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11223876", "body": "Still should drop the trailing `>&2` and add `[ -n \"$1\" ] && echo \"Error: Unknown command $1.\"`\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11223876/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11223902", "body": "Sorry, I guess I wasn't clear.  I think the official correct style is.\n\n```\ncase \"$1\" in\n        start)\n                do_start\n                ;;\n       <etc>\n       *)\nesac\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11223902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11247699", "body": "Assuming we keep this, all errors except ENOENT should be treated as fatal.  There's no reason to block for other failure modes.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11247699/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11271187", "body": "I placed them in common.init.in because we've tried to keep all the compatibility code abstracted away from the init script itself.  And while there is only one daemon today it's not inconceivable that there could be others in the future.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11271187/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11271229", "body": "Good catch, I'm a bit surprised that was missed.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11271229/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11271336", "body": "It was placed here to avoid replicating this code several times in the common script.  But I'm not at all opposed to moving it if the general feeling is that is clearer, I went back and forth on this several times.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11271336/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11271359", "body": "I wondered about that.  In practice it doesn't seem to be required, but it definitely seems like a good idea.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11271359/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11271408", "body": "This appears to be optional, but I agree it's a good idea.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11271408/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11271469", "body": "The example I happened to be looking at did this, although it appears most init scripts done so yes we should drop the `/dev/null` bit.  And updating it as you suggested does seem to be more standard.  I see a few exceptions in /etc/init.d/ but they're clearly oddballs.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11271469/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11271512", "body": "They're provided by the same package so it _very_ likely, although there's no harm in being extra careful.  We could consider drop this hunk entirely since the tools have been unified in to udevadm for quite some time now.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11271512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11271546", "body": "Pool names can contain whitespace.  It's rare but should be handled.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11271546/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11271572", "body": "The more careful the review the better!  Keep it  up @dun!\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11271572/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11254", "body": "This file should be readded exactly for the reasons mentioned in the file.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11254/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11255", "body": "Yes, please remove them.  They are currently unused hooks and they cause compile errors on older platforms.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11255/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11259", "body": "A version of this file was already added to the dracut subdirectory.  If you want to make changes/rewrite it that's fine but let's just keep one copy of it around with the other dracut code.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11259/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/600507", "body": "Then this should be vap->va_gid? \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/600507/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/600513", "body": "Yes, that seems the most correct.  However, up until now we've relied pretty much exclusively on the VFS layer for this sort of permission checking and enforcement.  Delegations should probably rely on crgetuid/crgetgid and the ACLs and attribute code should use crgetfsuid/crgetfsguid. \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/600513/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/600529", "body": "Sorry, yes I just missed the zfs side changes you had proposed.  I was a bit rushed on Friday when looking at this originally.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/600529/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/600535", "body": "By all means, if you could sort this out that would be ideal from my point of view.  Frankly, your going to be able to give this a lot more careful thought than me in the near future.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/600535/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/1518013", "body": "It's a typo.  It was supposed to be 'not uncommon', and I'd prefer not to change it to 'common' since I don't have any hard data on what fraction of drives behave this way.  Let's go with the suggested '...because drives are sometimes known to misreport...'.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/1518013/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/2811405", "body": "I'll get it in the merge.  Testing went well so I should be able to get the SPL and ZFS patch in today.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/2811405/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4281524", "body": "Not strictly because we're just checking the value and we don;t really care about the exact result but I'd keep it anyway.  I'd rather have it be strictly correct than try to prematurely optimize this.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4281524/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/9855126", "body": "Your right, this looks like a leak in the case you describe.  It's also possible in the other two early returns.  This function really shouldn't return a void type but instead return an error code so it's clear if the nvlist must be cleaned up.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/9855126/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/9855181", "body": "Yes, it would be better to either not all NULL to be set as a valid callback, or to handle the NULL properly.\n\nSince both of these issues aren't directly related to these proposed changes could you open up an issue for each of them so we don't loose track of making these improvements.  Better yet, propose a patch as well!\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/9855181/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/9675800", "body": "That's a good thought.  I suspect we may end up preserving this functionality just in case we ever need it again.  I'll rename the variable to `spa_errata` and refactor the commit when I update this branch.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/9675800/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25618962", "body": "@yarikoptic the key bit of information you need to be aware of is that large parts of ZoL are compiled twice when you build it.  The first time it build the core ZFS code in user space for ztest and the user space utilities.  The second time it builds it as kernel modules.  This means we have two mutex implementations one for user space and one for the kernel.\n\nThis commit only ensures that this flag is understood in user space so everything builds.  As you correctly pointed out it doesn't actually fix anything, but that's fine because in user space this deadlock can't occur.  It's the counter part SPL patches where the fix is, that's the kernel mutex implementation.  Those patches were updated so they disable part of the kernel's direct reclaim path while the mutex is held.  This prevents the deadlock from being possible.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25618962/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/27073402", "body": "I had the same thought and went back and forth between `ne_labels` and `ne_nr_labels`.  `ne_nr_labels` is a little clearer but I didn't like the look of the extra `_`.  We could just add a comment to the struct.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/27073402/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/27074110", "body": "There are definitely a few ways to structure this.  After rewriting this 3 times I opted for the `if-continue` blocks which I think are more readable because they allow a comment to be clearly placed above each block.  Putting everything in an `if / else if / else if` or `if (a || b || c)` block probably means I'd need to write a single big comment above the whole thing explaining all the case.  Which would be fine too, although I agree that last continue is a little odd.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/27074110/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/27074283", "body": "Good point.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/27074283/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/27074384", "body": "Good catch!  Yes, that's exactly what I meant.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/27074384/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/29080422", "body": "Yes your of course right, thanks,\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/29080422/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30648696", "body": "I actually intentionally dropped it because we're only going to be spinning briefly.  But I can add it back, it does appear to be POSIX so there aren't compatibility concerns.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30648696/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30656927", "body": "Yes, yes it should be.  I'll fix it by reverting this hunk so it matches upstream.  I clearly just replaced all this calls without fully inspecting them.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30656927/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30657001", "body": "I oped for this to be consistent with the existing `libzfs_error_action()` and `libzfs_error_description()` functions.  Consistency is good.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30657001/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30665291", "body": "That's embarrassing, and exactly what I deserve for rushing this small tweak to the original patch.  Originally it didn't have a minimum bound.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30665291/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30665483", "body": "I can get on board with this.  Originally @ryao has structured this as a `do-while` which normally I like to avoid since I think it hurts readability.  But in this case it does makes good sense.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30665483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30743384", "body": "Rather than sneak that in here.  Let's tackle that in a separate patch because why that's desirable is going to need a fair bit of explanation and it would be a change in existing behavior.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30743384/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30556074", "body": "My concern would be that it's wasteful of cpu for a highly concurrent boot process.  For init scripts I completely agree this is a non issue, but for something like systemd I wonder if that's the case.  \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30556074/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30645625", "body": "I'm proposing #3431 which takes a slightly different approach but should address all the concerns raised.  The updated patch will busy-wait for up to 10ms before falling back to polling for by default 10 seconds.  It also includes some other related cleanup.  See the commit message for full details and let me know if this is an approach you can live with.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30645625/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38353246", "body": "Yes, I'll remove it.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38353246/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38355382", "body": "Right, a race is still possible here since a lock isn't held over the ismount check and subsequent unmount.  However, if this happens there are no negative consequences.  The check is there mainly to just keep this window as small as possible.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38355382/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "wphilips": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7050", "title": "zfs-dracut boot failure with out of date zpool.cache - zfs_force not working", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Fedora \r\nDistribution Version    |  26 \r\nLinux Kernel                 |  any (e.g., 4.14.6-200.fc26.x86_64)\r\nArchitecture                 |  x86_64\r\nZFS Version                  |   v0.7.5-1\r\nSPL Version                  |  v0.7.5-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nI have several systems with root and boot on zfs. The systems boot with grub initramfs \r\nis generated by dracut with zfs-dracut-0.7.5-1.fc26.x86_64\r\n\r\nThe problem occurs whenever significant changes are made to the zpools attached to the\r\nsystem, or even when adding empty disks.  Very often, dracut enters the emergency shell\r\nbecause it cannot import the pools based on the zpool.cache file. Even adding zfs_force \r\nas a kernel option does not work. E.g., I tried:\r\n\r\nlinux16 /boot/@/vmlinuz-4.14.13-200.fc26.x86_64 root=zfs:ssd/fc26 boot=ssd ro rd_NO_PLYMOUTH audit=0 zfs_force=1\r\n\r\n\r\nThe reason seems to be that the zpoool.cache file does not reflect the current (changed) configuration of the system. It is not clear why zfs.force  or zfs_force does not work.\r\n\r\n\r\nHere are 2 use cases:\r\n\r\n1. to defragment  the pool on which the zfs root is installed, I attach a new disk, create a new\r\nzpool on it, copy all the data, remove the old disk, reboot and change some grub parameters so\r\nthat it boots the new bool. Before the reboot, zpool.cache refers to the old pool on the old disk.\r\nRunning 'dracut -f ...' will therefore copy the \"old\" zpool.cache into initamfs. After boot, the disks\r\nhave changed and this zpool.cache is outdated. \r\n\r\n2. in a system with 3 rpools, I remove one of the disks which contains a non-essential \r\nrpool (after exporting it). I then add two new empty disks. The system boots into the dracut \r\nshell even though the root pool has not changed. The now missing, but non-essential pool\r\nprevents a normal boot.\r\n\r\n\r\nIt is possible to somewhat prevent these problems by removing zpool.cache, then running\r\ndracut and then rebooting. In this case, often dracut still enters the emergency shell claiming\r\nthat the pool(s) are in use in another system, but by force importing them in the dracut shell\r\nand rebooting it is possible to boot the system. Then it is possible to recreate zpool.cache, \r\nand rerun dracut to create a working system. Alternatively, one can continue to use the initramfs\r\nwith the missing zpool.cache.\r\n\r\nIt is probably also possible to create a zpool.cache file for the future new configuration, but it probably \r\ninvolves deleting the current one and it is easy to make a mistake.\r\n\r\nIn any case, make a simple mistake or  forget to take these  \"preventive\" measures \r\nand you end up with a system which will always enter the dracut emergency shell with \r\nno way to recover (except if you have e.g., a usb boot disk with zfs at hand. Even then\r\nit is really hard to recover).\r\n\r\nIn the good old days it also use to be  possible to fix problems in the dracut shell and then continue to boot. These days, systemd prevents this from working (probably related to the message \"transaction is destructive\")\r\n\r\nWhile fixing the zfs_force option would help, adding a configuration option to dracut to never \r\ncreate zfs.cache and/or adding a kernel command line option to ignore zpool.cache might \r\nalso help.\r\n \r\n\r\nPS. Even better would be to fix dracut or systemd so that a boot can continue after fixing problems \r\nin dracut. For instance, in the emergency shell you would remove the zpool.cache file and \r\nthen type some command to continue boot. However, that is probably a more general (non zfsonlinux)\r\nissue.\r\n\r\n\r\n\r\n\r\n\r\n\r\n### Describe how to reproduce the problem\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7050/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6386", "title": "zfs-dracut cannot boot zfsroot on fedora 26", "body": "### System information\r\nDistribution Name       Fedora \r\nDistribution Version    26 \r\nLinux Kernel                  4.11.10-300.fc26.x86_64 #1 SMP \r\nArchitecture                 x86_64\r\nZFS Version                0.6.5.11-1  \r\nSPL Version                 0.6.5.11-1\r\n\r\n### Describe the problem you're observing\r\ndracut does not boot a zfs root, which is installed in the filesystem zfs/ROOT/fedora\r\nInstead, the dracut emergency shell appears. With some debugging I found that \r\nzfs-parse recognizes the root parameter, but mount.zfs module is never called. As a \r\nresult, systemd fails on \"Mounting sysroot\"\r\n\r\n### Describe how to reproduce the problem\r\n1. install fedora 26 on ext4 root (In reality I first copied an existing installation with a fedora22 zfsroot to the ext4. Then I upgraded fedora. This works from ext4, with all zfs filesystems being accessible.)\r\n\r\nzfs was installed as follows\r\nsudo dkms autoinstall -m zfs -k 4.11.10-300.fc26.x86_64                         \r\nsudo dracut  --force /boot/initramfs-4.11.10-300.fc26.x86_64.img  4.11.10-300.fc26.x86_64                                                                      \r\n \r\n2. copy the ext4 root filesystem to zfs/ROOT/fedora which is in a pool on another partition\r\n\r\n3. adjust the grub command line. I did that manually (too many problems otherwise)\r\n\r\n4. boot. The dracut/kernel command line is                                                             \r\nBOOT_IMAGE=/vmlinuz-4.11.10-300.fc26.x86_64 root=zfs:zfs/ROOT/fedora ro audit=0\r\n vconsole.font=latarcyrheb-sun16 LANG=en_US.UTF-8 audit=0 rd.debug=1   \r\n\r\n5. dracut drops to the emergency shell, allowing me to save rdsosreport.txt\r\nIn the emergency shell, it is clear that the zpool is not imported. \r\nA simple \"zpool import zfs\" imports the pool and mounts all filesystems except root\r\n\r\nThe following commands sheds more light on the systemd part:\r\nsystemctl status sysroot.mount\r\n```\r\nsysroot.mount - /sysroot\r\n   Loaded: loaded (/proc/cmdline; generated; vendor preset: enabled)\r\n   Active: failed (Result: exit-code) since Fri 2017-07-21 20:28:31 UTC; 41s ago\r\n    Where: /sysroot\r\n     What: zfs:zfs/ROOT/fedora\r\n     Docs: man:fstab(5)\r\n           man:systemd-fstab-generator(8)\r\n  Process: 448 ExecMount=/usr/bin/mount zfs:zfs/ROOT/fedora /sysroot -o ro (code=exited, status=32)\r\n\r\nJul 21 20:28:31 duckman.mynet systemd[1]: Mounting /sysroot...\r\nJul 21 20:28:31 duckman.mynet systemd[1]: sysroot.mount: Mount process exited, code=exited status=32\r\nJul 21 20:28:31 duckman.mynet systemd[1]: Failed to mount /sysroot.\r\nJul 21 20:28:31 duckman.mynet systemd[1]: sysroot.mount: Unit entered failed state.\r\n````\r\nFrom rdosreport.txt, it is clear that the dracut module mount.zfs is never called by systemd (at least \r\nbefore the error occurs), which explains why the zpool is not imported\r\n\r\nAdditional info:\r\nsudo zpool get bootfs\r\n```\r\nNAME  PROPERTY  VALUE   SOURCE\r\nzfs   bootfs    -       default\r\n```\r\nsudo zfs get mountpoint zfs/ROOT/fedora\r\n```\r\nNAME             PROPERTY\r\n    VALUE       SOURCE\r\nzfs/ROOT/fedora  mountpoint  /           local\r\n```\r\n\r\n[rdsosreport.txt](https://github.com/zfsonlinux/zfs/files/1166716/rdsosreport.txt)\r\n\r\n[sysroot_mount.txt](https://github.com/zfsonlinux/zfs/files/1166718/sysroot_mount.txt)\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6386/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jhammond-intel": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7045", "title": "pool suspension due to delayed MMP writes needs a better error message", "body": "### System information\r\nDistribution Name       | *\r\nDistribution Version    | *\r\nLinux Kernel                 | * \r\nArchitecture                 | *\r\nZFS Version                  | 0.7.5\r\nSPL Version                  | 0.7.5\r\n\r\nThis is related to Lustre issue https://jira.hpdd.intel.com/browse/LU-9845.\r\n\r\nWhen an MMP thread suspends a pool because \"no MMP write has succeeded in over mmp_interval * mmp_fail_intervals nanoseconds\" the only message we see on the console is \"WARNING: Pool 'blahblah' has encountered an uncorrectable I/O failure and has been suspended.\" This is not really informative enough and probably a bit misleading. We encountered these mysteriously suspended pool in our test clusters and were only able to attribute this to MMP by setting the pool failure mode to panic.\r\n\r\nI was able to easily reproduce using the Lustre backed zfs setup (VM has hostid set and 2 vCPUs, pool has MMP enabled) using the following:\r\n```\r\nm:~# export FSTYPE=zfs\r\nm:~# bash $LUSTRE/tests/llmount.sh\r\n...\r\nm:~# cat /sys/module/zfs/parameters/zfs_multihost_interval \r\n1000\r\nm:~# echo 100 > /sys/module/zfs/parameters/zfs_multihost_interval # set mmp interval to 100ms\r\nm:~# chrt -f 20 dd if=/dev/zero of=/dev/null &\r\nm:~# chrt -f 20 dd if=/dev/zero of=/dev/null &\r\n```\r\n\r\nI think we should probably keep the message from `zio_suspend()` as is but add a suitable message to `mmp_thread()` before calling `zio_suspend()`.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7045/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/51d1b58ef3467c3a9711c65458f93063dd17354f", "message": "Emit an error message before MMP suspends pool\n\nIn mmp_thread(), emit an MMP specific error message before calling\r\nzio_suspend() so that the administrator will understand why the pool\r\nis being suspended.\r\n\r\nReviewed-by: Olaf Faaland <faaland1@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: John L. Hammond <john.hammond@intel.com>\r\nCloses #7048"}], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/7068", "title": "MMP delay logic changes", "body": "<!--- Provide a general summary of your changes in the Title above -->\r\n\r\n<!---\r\nDocumentation on ZFS Buildbot options can be found at\r\nhttps://github.com/zfsonlinux/zfs/wiki/Buildbot-Options\r\n-->\r\n\r\n### Description\r\nRFC MMP delay logic changes from @adilger. These are based on the discussion on https://github.com/zfsonlinux/zfs/issues/7045 and https://github.com/zfsonlinux/zfs/pull/7048. (I have not tested these but I plan to.)\r\n\r\n### Motivation and Context\r\n<!--- Why is this change required? What problem does it solve? -->\r\n<!--- If it fixes an open issue, please link to the issue here. -->\r\n\r\n### How Has This Been Tested?\r\n<!--- Please describe in detail how you tested your changes. -->\r\n<!--- Include details of your testing environment, and the tests you ran to -->\r\n<!--- see how your change affects other areas of the code, etc. -->\r\n<!--- If your change is a performance enhancement, please provide benchmarks here. -->\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [ ] I have updated the documentation accordingly.\r\n- [x] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [ ] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "samuelbernardo": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7042", "title": "BUG: soft lockup - CPU# stuck for 22s! [z_wr_iss]", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Gentoo\r\nDistribution Version    | -\r\nLinux Kernel                 | 4.14.12\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.5\r\nSPL Version                  | 0.7.5\r\n\r\n\r\n### Describe the problem you're observing\r\n\r\nzfs thread lock after some intensive IO. It allows to continue to access data, but all writes won't be commited to disk, since reboot needs ctrl+shift+sysreq reisub. It remains locked after trying soft reboot, and the only solution is a forced reboot with sysreq.\r\nThe zfs lock is registered systematically after some intensive IO on each OS reboot.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nThis is the configuration of zfs volume that has the deadlock (using deduplication and lz4 compression):\r\n\r\nNAME   SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT\r\nzfs  21.8T  5.69T  16.1T         -     6%    26%  1.07x  ONLINE  -\r\n  raidz1  10.9T  2.85T  8.03T         -     6%    26%\r\n    ata-TOSHIBA_DT01ACA300_Z5RS6H0KS      -      -      -         -      -      -\r\n    ata-TOSHIBA_HDWD130_678KR5JAS      -      -      -         -      -      -\r\n    ata-TOSHIBA_DT01ACA300_16QUEEEKS      -      -      -         -      -      -\r\n    ata-TOSHIBA_HDWD130_678KPYLAS      -      -      -         -      -      -\r\n  raidz1  10.9T  2.85T  8.03T         -     6%    26%\r\n    ata-TOSHIBA_HDWD130_678KTDUAS      -      -      -         -      -      -\r\n    ata-TOSHIBA_DT01ACA300_16QUDE3KS      -      -      -         -      -      -\r\n    ata-WDC_WD40EZRX-75SPEB0_WD-WCC4E2YAA98J-part6      -      -      -         -      -      -\r\n    ata-TOSHIBA_HDWD130_678KPP7AS      -      -      -         -      -      -\r\ncache      -      -      -         -      -      -\r\n  sdc   699G  79.7M   699G         -     0%     0%\r\n  sde   699G  78.2M   699G         -     0%     0%\r\n\r\n  pool: zfs\r\n state: ONLINE\r\n  scan: resilvered 75.5G in 0h38m with 0 errors on Mon Oct 16 03:45:53 2017\r\nconfig:\r\n        NAME                                                STATE     READ WRITE CKSUM\r\n        zfs                                                 ONLINE       0     0     0\r\n          raidz1-0                                          ONLINE       0     0     0\r\n            ata-TOSHIBA_DT01ACA300_Z5RS6H0KS                ONLINE       0     0     0\r\n            ata-TOSHIBA_HDWD130_678KR5JAS                   ONLINE       0     0     0\r\n            ata-TOSHIBA_DT01ACA300_16QUEEEKS                ONLINE       0     0     0\r\n            ata-TOSHIBA_HDWD130_678KPYLAS                   ONLINE       0     0     0\r\n          raidz1-1                                          ONLINE       0     0     0\r\n            ata-TOSHIBA_HDWD130_678KTDUAS                   ONLINE       0     0     0\r\n            ata-TOSHIBA_DT01ACA300_16QUDE3KS                ONLINE       0     0     0\r\n            ata-WDC_WD40EZRX-75SPEB0_WD-WCC4E2YAA98J-part6  ONLINE       0     0     0\r\n            ata-TOSHIBA_HDWD130_678KPP7AS                   ONLINE       0     0     0\r\n        cache\r\n          sdc                                               ONLINE       0     0     0\r\n          sde                                               ONLINE       0     0     0\r\n\r\ncapacity  |   operations  |   bandwidth  |  total_wait   |  disk_wait  |  syncq_wait  |  asyncq_wait | scrub\r\n\r\npool |  alloc |  free |  read | write |  read | write |  read | write |  read | write  | read | write |  read | write |  wait \r\n  --- |   --- |   --- |   --- |  --- |   --- |  --- |   --- |  --- |   --- |  ---  |  --- |  --- |   --- |  --- |   --- \r\nzfs     |    5.69T | 16.1T  |   81 |   109 |  500K |  950K |   4us  |  1us  |  4us | 543ns | 187ns  |  2ns  |  1us |   1us | 723ns\r\n\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n```\r\nJan 10 21:57:11 x99 kernel: INFO: rcu_sched detected expedited stalls on CPUs/tasks: { 9-... } 218109 jiffies s: 401 root: 0x200/.\r\nJan 10 21:57:11 x99 kernel: blocking rcu_node structures:\r\nJan 10 21:57:11 x99 kernel: Task dump for CPU 9:\r\nJan 10 21:57:11 x99 kernel: z_wr_iss        R  running task    12256   749      2 0x80000008\r\nJan 10 21:57:11 x99 kernel: Call Trace:\r\nJan 10 21:57:11 x99 kernel:  ? arc_buf_info+0xcc7/0xf80 [zfs]\r\nJan 10 21:57:11 x99 kernel:  ? mutex_lock+0x9/0x30\r\nJan 10 21:57:11 x99 kernel:  ? dbuf_rele_and_unlock+0x4cb/0x540 [zfs]\r\nJan 10 21:57:11 x99 kernel:  ? mutex_lock+0x9/0x30\r\nJan 10 21:57:11 x99 kernel:  ? mutex_lock+0x9/0x30\r\nJan 10 21:57:11 x99 kernel:  ? zio_worst_error+0x60f/0x1250 [zfs]\r\nJan 10 21:57:11 x99 kernel:  ? zio_wait+0x113/0x160 [zfs]\r\nJan 10 21:57:11 x99 kernel:  ? dbuf_read+0x617/0xd80 [zfs]\r\nJan 10 21:57:11 x99 kernel:  ? __kmalloc_node+0x27d/0x290\r\nJan 10 21:57:11 x99 kernel:  ? spl_kmem_zalloc+0x85/0x150 [spl]\r\nJan 10 21:57:11 x99 kernel:  ? zap_leaf_lookup+0x6d/0x130 [zfs]\r\nJan 10 21:57:11 x99 kernel:  ? fzap_length+0x48/0x90 [zfs]\r\nJan 10 21:57:11 x99 kernel:  ? zap_name_alloc_uint64+0x50/0x60 [zfs]\r\nJan 10 21:57:11 x99 kernel:  ? zap_length_uint64+0x74/0x230 [zfs]\r\nJan 10 21:57:11 x99 kernel:  ? ddt_walk+0x31b/0x450 [zfs]\r\nJan 10 21:57:11 x99 kernel:  ? ddt_lookup+0xb4/0x190 [zfs]\r\nJan 10 21:57:11 x99 kernel:  ? zio_checksum_compute+0x15d/0x2a0 [zfs]\r\nJan 10 21:57:11 x99 kernel:  ? spl_kmem_cache_alloc+0x5b/0xb10 [spl]\r\nJan 10 21:57:11 x99 kernel:  ? __kmalloc_node+0x27d/0x290\r\nJan 10 21:57:11 x99 kernel:  ? spl_kmem_alloc+0x8e/0x160 [spl]\r\nJan 10 21:57:11 x99 kernel:  ? spl_kmem_alloc+0x8e/0x160 [spl]\r\nJan 10 21:57:11 x99 kernel:  ? zio_flush+0x867/0xde0 [zfs]\r\nJan 10 21:57:11 x99 kernel:  ? zio_push_transform+0x662/0xbe0 [zfs]\r\nJan 10 21:57:11 x99 kernel:  ? zio_execute+0x7c/0x430 [zfs]\r\nJan 10 21:57:11 x99 kernel:  ? taskq_dispatch_delay+0x51f/0x950 [spl]\r\nJan 10 21:57:11 x99 kernel:  ? wake_up_q+0x70/0x70\r\nJan 10 21:57:11 x99 kernel:  ? zio_interrupt+0x1030/0x1030 [zfs]\r\nJan 10 21:57:11 x99 kernel:  ? kthread+0xf7/0x130\r\nJan 10 21:57:11 x99 kernel:  ? taskq_dispatch_delay+0x2c0/0x950 [spl]\r\nJan 10 21:57:11 x99 kernel:  ? kthread_create_on_node+0x40/0x40\r\nJan 10 21:57:11 x99 kernel:  ? do_group_exit+0x35/0xa0\r\nJan 10 21:57:11 x99 kernel:  ? ret_from_fork+0x1f/0x30\r\nJan 10 21:57:13 x99 kernel: watchdog: BUG: soft lockup - CPU#9 stuck for 22s! [z_wr_iss:749]\r\nJan 10 21:57:13 x99 kernel: Modules linked in: nvidia_uvm(PO) zfs(PO) zunicode(PO) zavl(PO) icp(PO) zcommon(PO) znvpair(PO) spl(O) nv>\r\nJan 10 21:57:13 x99 kernel: CPU: 9 PID: 749 Comm: z_wr_iss Tainted: P        W  O L  4.14.12-gentoox99 #1\r\nJan 10 21:57:13 x99 kernel: Hardware name: ASUS All Series/X99-S, BIOS 3402 08/18/2016\r\nJan 10 21:57:13 x99 kernel: task: ffff880fef778e00 task.stack: ffffc9000a2dc000\r\nJan 10 21:57:13 x99 kernel: RIP: 0010:zap_leaf_lookup+0x92/0x130 [zfs]\r\nJan 10 21:57:13 x99 kernel: RSP: 0018:ffffc9000a2dfa40 EFLAGS: 00000213 ORIG_RAX: ffffffffffffff10\r\nJan 10 21:57:13 x99 kernel: RAX: 0000000000000000 RBX: ffff880d59b78130 RCX: 0000000000000007\r\nJan 10 21:57:13 x99 kernel: RDX: 000000000000000c RSI: ffff880d59b78000 RDI: 5d7f96b690640000\r\nJan 10 21:57:13 x99 kernel: RBP: ffffc9000a2dfa88 R08: 00000000002ebfcb R09: ffff880de2c53800\r\nJan 10 21:57:13 x99 kernel: R10: ffff880de2c53800 R11: ffff880fe497a000 R12: ffff880de2c53800\r\nJan 10 21:57:13 x99 kernel: R13: ffff880c575f3600 R14: ffff880d59b78132 R15: 0000000000000001\r\nJan 10 21:57:13 x99 kernel: FS:  0000000000000000(0000) GS:ffff880fff440000(0000) knlGS:0000000000000000\r\nJan 10 21:57:13 x99 kernel: CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nJan 10 21:57:13 x99 kernel: CR2: 00007f767108b850 CR3: 0000000004823006 CR4: 00000000001606e0\r\nJan 10 21:57:13 x99 kernel: Call Trace:\r\nJan 10 21:57:13 x99 kernel:  fzap_length+0x48/0x90 [zfs]\r\nJan 10 21:57:13 x99 kernel:  ? zap_name_alloc_uint64+0x50/0x60 [zfs]\r\nJan 10 21:57:13 x99 kernel:  zap_length_uint64+0x74/0x230 [zfs]\r\nJan 10 21:57:13 x99 kernel:  ddt_walk+0x31b/0x450 [zfs]\r\nJan 10 21:57:13 x99 kernel:  ddt_lookup+0xb4/0x190 [zfs]\r\nJan 10 21:57:13 x99 kernel:  ? zio_checksum_compute+0x15d/0x2a0 [zfs]\r\nJan 10 21:57:13 x99 kernel:  ? spl_kmem_cache_alloc+0x5b/0xb10 [spl]\r\nJan 10 21:57:13 x99 kernel:  ? __kmalloc_node+0x27d/0x290\r\nJan 10 21:57:13 x99 kernel:  ? spl_kmem_alloc+0x8e/0x160 [spl]\r\nJan 10 21:57:13 x99 kernel:  ? spl_kmem_alloc+0x8e/0x160 [spl]\r\nJan 10 21:57:13 x99 kernel:  zio_flush+0x867/0xde0 [zfs]\r\nJan 10 21:57:13 x99 kernel:  ? zio_push_transform+0x662/0xbe0 [zfs]\r\nJan 10 21:57:13 x99 kernel:  zio_execute+0x7c/0x430 [zfs]\r\nJan 10 21:57:13 x99 kernel:  taskq_dispatch_delay+0x51f/0x950 [spl]\r\nJan 10 21:57:13 x99 kernel:  ? wake_up_q+0x70/0x70\r\nJan 10 21:57:13 x99 kernel:  ? zio_interrupt+0x1030/0x1030 [zfs]\r\nJan 10 21:57:13 x99 kernel:  kthread+0xf7/0x130\r\nJan 10 21:57:13 x99 kernel:  ? taskq_dispatch_delay+0x2c0/0x950 [spl]\r\nJan 10 21:57:13 x99 kernel:  ? kthread_create_on_node+0x40/0x40\r\nJan 10 21:57:13 x99 kernel:  ? do_group_exit+0x35/0xa0\r\nJan 10 21:57:13 x99 kernel:  ret_from_fork+0x1f/0x30\r\nJan 10 21:57:13 x99 kernel: Code: eb 29 0f b7 43 02 4c 8d 73 02 66 83 f8 ff 74 7f 49 8b 8c 24 d8 00 00 00 41 8b 94 24 d0 00 00 00 49 >\r\nJan 10 21:57:18 x99 systemd[1]: Received SIGINT.\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7042/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6976", "title": "PANIC: zfs: allocating allocated segment", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Gentoo\r\nDistribution Version    | profile user customized\r\nLinux Kernel                 | 4.9.69\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.3\r\nSPL Version                  | 0.7.3\r\n\r\n\r\n### Describe the problem you're observing\r\n\r\nKernel panic after starting OS with a force reboot and repete same operation that crashed the system before. I was doing a system upgrade and when calculating dependencies for the upgrade, kernel panic appeared.\r\n\r\nAfter kernel panic system seems to be working normally without doing any action.\r\n```\r\nzpool status -x\r\nall pools are healthy\r\n```\r\n```\r\nzpool status -v\r\n  pool: zfs\r\n state: ONLINE\r\nstatus: Some supported features are not enabled on the pool. The pool can\r\n        still be used, but some features are unavailable.\r\naction: Enable all features using 'zpool upgrade'. Once this is done,\r\n        the pool may no longer be accessible by software that does not support\r\n        the features. See zpool-features(5) for details.\r\n  scan: none requested\r\nconfig:\r\n\r\n        NAME                                                STATE     READ WRITE CKSUM\r\n        zfs                                                 ONLINE       0     0     0\r\n          raidz1-0                                          ONLINE       0     0     0\r\n            ata-TOSHIBA_DT01ACA300_Z5RS6H0KS                ONLINE       0     0     0\r\n            ata-TOSHIBA_HDWD130_678KR5JAS                   ONLINE       0     0     0\r\n            ata-TOSHIBA_DT01ACA300_16QUEEEKS                ONLINE       0     0     0\r\n            ata-TOSHIBA_HDWD130_678KPYLAS                   ONLINE       0     0     0\r\n          raidz1-1                                          ONLINE       0     0     0\r\n            ata-TOSHIBA_HDWD130_678KTDUAS                   ONLINE       0     0     0\r\n            ata-TOSHIBA_DT01ACA300_16QUDE3KS                ONLINE       0     0     0\r\n            ata-WDC_WD40EZRX-75SPEB0_WD-WCC4E2YAA98J-part6  ONLINE       0     0     0\r\n            ata-TOSHIBA_HDWD130_678KPP7AS                   ONLINE       0     0     0\r\n        cache\r\n          sdc                                               ONLINE       0     0     0\r\n          sde                                               ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n```\r\n\r\n### Describe how to reproduce the problem\r\n\r\nThis is the configuration of zfs volume that had the bug (using deduplication and lz4 compression):\r\n\r\nNAME   SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT\r\nzfs  21.8T  5.69T  16.1T         -     6%    26%  1.07x  ONLINE  -\r\n  raidz1  10.9T  2.85T  8.03T         -     6%    26%\r\n    ata-TOSHIBA_DT01ACA300_Z5RS6H0KS      -      -      -         -      -      -\r\n    ata-TOSHIBA_HDWD130_678KR5JAS      -      -      -         -      -      -\r\n    ata-TOSHIBA_DT01ACA300_16QUEEEKS      -      -      -         -      -      -\r\n    ata-TOSHIBA_HDWD130_678KPYLAS      -      -      -         -      -      -\r\n  raidz1  10.9T  2.85T  8.03T         -     6%    26%\r\n    ata-TOSHIBA_HDWD130_678KTDUAS      -      -      -         -      -      -\r\n    ata-TOSHIBA_DT01ACA300_16QUDE3KS      -      -      -         -      -      -\r\n    ata-WDC_WD40EZRX-75SPEB0_WD-WCC4E2YAA98J-part6      -      -      -         -      -      -\r\n    ata-TOSHIBA_HDWD130_678KPP7AS      -      -      -         -      -      -\r\ncache      -      -      -         -      -      -\r\n  sdc   699G  79.7M   699G         -     0%     0%\r\n  sde   699G  78.2M   699G         -     0%     0%\r\n\r\ncapacity  |   operations  |   bandwidth  |  total_wait   |  disk_wait  |  syncq_wait  |  asyncq_wait | scrub\r\n\r\npool |  alloc |  free |  read | write |  read | write |  read | write |  read | write  | read | write |  read | write |  wait \r\n  --- |   --- |   --- |   --- |  --- |   --- |  --- |   --- |  --- |   --- |  ---  |  --- |  --- |   --- |  --- |   --- \r\nzfs     |    5.69T | 16.1T  |   81 |   109 |  500K |  950K |   4us  |  1us  |  4us | 543ns | 187ns  |  2ns  |  1us |   1us | 723ns\r\n\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n```\r\n[  335.068216] PANIC: zfs: allocating allocated segment(offset=10858229538816 size=8192)\r\n[  335.068221] Showing stack for process 2019\r\n[  335.068223] CPU: 4 PID: 2019 Comm: txg_sync Tainted: P        W  O    4.9.69-gentoox99 #1\r\n[  335.068224] Hardware name: ASUS All Series/X99-S, BIOS 3402 08/18/2016\r\n[  335.068226]  ffffc9002eff3a48 ffffffff81790587 0000000000000003 ffff880fdcae7800\r\n[  335.068228]  ffffc9002eff3a58 ffffffffa0edf1bd ffffc9002eff3b78 ffffffffa0edf302\r\n[  335.068230]  ffff880ff47eb0c0 6c6c61203a73667a 20676e697461636f 657461636f6c6c61\r\n[  335.068232] Call Trace:\r\n[  335.068238]  [<ffffffff81790587>] dump_stack+0x4d/0x66\r\n[  335.068242]  [<ffffffffa0edf1bd>] spl_dumpstack+0x3d/0x40 [spl]\r\n[  335.068245]  [<ffffffffa0edf302>] vcmn_err+0x52/0xf0 [spl]\r\n[  335.068247]  [<ffffffff8122e27c>] ? __slab_free+0xbc/0x300\r\n[  335.068249]  [<ffffffff8122d97e>] ? kmem_cache_alloc+0x12e/0x1c0\r\n[  335.068251]  [<ffffffffa0eda8be>] ? spl_kmem_cache_alloc+0x5e/0xae0 [spl]\r\n[  335.068252]  [<ffffffff8122e25c>] ? __slab_free+0x9c/0x300\r\n[  335.068255]  [<ffffffffa0dc8616>] ? avl_insert+0xb6/0xd0 [zavl]\r\n[  335.068273]  [<ffffffffa138bcba>] zfs_panic_recover+0x5a/0x60 [zfs]\r\n[  335.068286]  [<ffffffffa13740d7>] range_tree_add+0x167/0x2a0 [zfs]\r\n[  335.068298]  [<ffffffffa1373f70>] ? range_tree_destroy+0x60/0x60 [zfs]\r\n[  335.068300]  [<ffffffff8122e67b>] ? kmem_cache_free+0x1bb/0x1e0\r\n[  335.068311]  [<ffffffffa1373f70>] ? range_tree_destroy+0x60/0x60 [zfs]\r\n[  335.068321]  [<ffffffffa1373f70>] ? range_tree_destroy+0x60/0x60 [zfs]\r\n[  335.068331]  [<ffffffffa1374658>] range_tree_vacate+0x48/0xc0 [zfs]\r\n[  335.068341]  [<ffffffffa1373f70>] ? range_tree_destroy+0x60/0x60 [zfs]\r\n[  335.068354]  [<ffffffffa136ff04>] metaslab_sync_done+0x114/0x6c0 [zfs]\r\n[  335.068369]  [<ffffffffa13938b9>] vdev_sync_done+0x39/0x70 [zfs]\r\n[  335.068382]  [<ffffffffa137d7e3>] spa_sync+0x603/0xfd0 [zfs]\r\n[  335.068384]  [<ffffffff81117d4d>] ? default_wake_function+0xd/0x10\r\n[  335.068402]  [<ffffffffa139032c>] txg_rele_to_sync+0x73c/0x8d0 [zfs]\r\n[  335.068415]  [<ffffffffa1390050>] ? txg_rele_to_sync+0x460/0x8d0 [zfs]\r\n[  335.068417]  [<ffffffffa0edc260>] ? __thread_exit+0x20/0xa0 [spl]\r\n[  335.068419]  [<ffffffffa0edc2cd>] __thread_exit+0x8d/0xa0 [spl]\r\n[  335.068421]  [<ffffffff8110d232>] kthread+0xd2/0xf0\r\n[  335.068423]  [<ffffffff8110d160>] ? kthread_park+0x60/0x60\r\n[  335.068424]  [<ffffffff8110d160>] ? kthread_park+0x60/0x60\r\n[  335.068427]  [<ffffffff81fdb112>] ret_from_fork+0x22/0x30\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6976/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6874", "title": "PANIC: blkptr at ffff880eb6742800 DVA 0 has invalid VDEV 4294967295", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Gentoo\r\nDistribution Version    | -\r\nLinux Kernel                 | 4.9.61\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.3\r\nSPL Version                  | 0.7.3\r\n\r\n\r\n### Describe the problem you're observing\r\n\r\nKernel panic after some intensive IO. One thread seams to enter in deadlock, reviewing thread activity in htop.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nThis is the configuration of zfs volume that has the deadlock (using deduplication and lz4 compression):\r\n\r\nNAME   SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT\r\nzfs  21.8T  5.69T  16.1T         -     6%    26%  1.07x  ONLINE  -\r\n  raidz1  10.9T  2.85T  8.03T         -     6%    26%\r\n    ata-TOSHIBA_DT01ACA300_Z5RS6H0KS      -      -      -         -      -      -\r\n    ata-TOSHIBA_HDWD130_678KR5JAS      -      -      -         -      -      -\r\n    ata-TOSHIBA_DT01ACA300_16QUEEEKS      -      -      -         -      -      -\r\n    ata-TOSHIBA_HDWD130_678KPYLAS      -      -      -         -      -      -\r\n  raidz1  10.9T  2.85T  8.03T         -     6%    26%\r\n    ata-TOSHIBA_HDWD130_678KTDUAS      -      -      -         -      -      -\r\n    ata-TOSHIBA_DT01ACA300_16QUDE3KS      -      -      -         -      -      -\r\n    ata-WDC_WD40EZRX-75SPEB0_WD-WCC4E2YAA98J-part6      -      -      -         -      -      -\r\n    ata-TOSHIBA_HDWD130_678KPP7AS      -      -      -         -      -      -\r\ncache      -      -      -         -      -      -\r\n  sdc   699G  79.7M   699G         -     0%     0%\r\n  sde   699G  78.2M   699G         -     0%     0%\r\n\r\n  pool: zfs\r\n state: ONLINE\r\n  scan: resilvered 75.5G in 0h38m with 0 errors on Mon Oct 16 03:45:53 2017\r\nconfig:\r\n        NAME                                                STATE     READ WRITE CKSUM\r\n        zfs                                                 ONLINE       0     0     0\r\n          raidz1-0                                          ONLINE       0     0     0\r\n            ata-TOSHIBA_DT01ACA300_Z5RS6H0KS                ONLINE       0     0     0\r\n            ata-TOSHIBA_HDWD130_678KR5JAS                   ONLINE       0     0     0\r\n            ata-TOSHIBA_DT01ACA300_16QUEEEKS                ONLINE       0     0     0\r\n            ata-TOSHIBA_HDWD130_678KPYLAS                   ONLINE       0     0     0\r\n          raidz1-1                                          ONLINE       0     0     0\r\n            ata-TOSHIBA_HDWD130_678KTDUAS                   ONLINE       0     0     0\r\n            ata-TOSHIBA_DT01ACA300_16QUDE3KS                ONLINE       0     0     0\r\n            ata-WDC_WD40EZRX-75SPEB0_WD-WCC4E2YAA98J-part6  ONLINE       0     0     0\r\n            ata-TOSHIBA_HDWD130_678KPP7AS                   ONLINE       0     0     0\r\n        cache\r\n          sdc                                               ONLINE       0     0     0\r\n          sde                                               ONLINE       0     0     0\r\n\r\ncapacity  |   operations  |   bandwidth  |  total_wait   |  disk_wait  |  syncq_wait  |  asyncq_wait | scrub\r\n\r\npool |  alloc |  free |  read | write |  read | write |  read | write |  read | write  | read | write |  read | write |  wait \r\n  --- |   --- |   --- |   --- |  --- |   --- |  --- |   --- |  --- |   --- |  ---  |  --- |  --- |   --- |  --- |   --- \r\nzfs     |    5.69T | 16.1T  |   81 |   109 |  500K |  950K |   4us  |  1us  |  4us | 543ns | 187ns  |  2ns  |  1us |   1us | 723ns\r\n\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n```\r\n[ 6305.806334] PANIC: blkptr at ffff880eb6742800 DVA 0 has invalid VDEV 4294967295\r\n[ 6305.806337] Showing stack for process 744\r\n[ 6305.806338] CPU: 1 PID: 744 Comm: z_wr_iss Tainted: P        W  O    4.9.61-gentoox99 #3\r\n[ 6305.806339] Hardware name: ASUS All Series/X99-S, BIOS 3402 08/18/2016\r\n[ 6305.806340]  ffffc9000bba75b0 ffffffff8178f198 0000000000000003 ffff880eb6742800\r\n[ 6305.806341]  ffffc9000bba75c0 ffffffffa0ed020d ffffc9000bba76e0 ffffffffa0ed0352\r\n[ 6305.806342]  0000001000000010 61207274706b6c62 3838666666662074 3832343736626530\r\n[ 6305.806344] Call Trace:\r\n[ 6305.806348]  [<ffffffff8178f198>] dump_stack+0x4d/0x65\r\n[ 6305.806350]  [<ffffffffa0ed020d>] spl_dumpstack+0x3d/0x40 [spl]\r\n[ 6305.806352]  [<ffffffffa0ed0352>] vcmn_err+0x52/0xf0 [spl]\r\n[ 6305.806355]  [<ffffffff8122e58a>] ? kmem_cache_alloc+0x17a/0x1d0\r\n[ 6305.806356]  [<ffffffffa0ecb8ce>] ? spl_kmem_cache_alloc+0x5e/0xa40 [spl]\r\n[ 6305.806357]  [<ffffffffa0ecb8ce>] ? spl_kmem_cache_alloc+0x5e/0xa40 [spl]\r\n[ 6305.806359]  [<ffffffffa0ecb8ce>] ? spl_kmem_cache_alloc+0x5e/0xa40 [spl]\r\n[ 6305.806360]  [<ffffffffa0ecb8ce>] ? spl_kmem_cache_alloc+0x5e/0xa40 [spl]\r\n[ 6305.806361]  [<ffffffffa0ecb8ce>] ? spl_kmem_cache_alloc+0x5e/0xa40 [spl]\r\n[ 6305.806376]  [<ffffffffa0fca727>] zfs_panic_recover+0x57/0x60 [zfs]\r\n[ 6305.806381]  [<ffffffffa0f652e0>] ? arc_read+0xa40/0x3320 [zfs]\r\n[ 6305.806389]  [<ffffffffa101f3fb>] zfs_blkptr_verify+0x30b/0x360 [zfs]\r\n[ 6305.806390]  [<ffffffffa0ecb96d>] ? spl_kmem_cache_alloc+0xfd/0xa40 [spl]\r\n[ 6305.806397]  [<ffffffffa101f47f>] zio_read+0x2f/0x180 [zfs]\r\n[ 6305.806398]  [<ffffffff8122f6fa>] ? __kmalloc_node+0x14a/0x2d0\r\n[ 6305.806400]  [<ffffffffa0ecb00e>] ? spl_kmem_zalloc+0x9e/0x180 [spl]\r\n[ 6305.806401]  [<ffffffffa0ecb00e>] ? spl_kmem_zalloc+0x9e/0x180 [spl]\r\n[ 6305.806406]  [<ffffffffa0f652e0>] ? arc_read+0xa40/0x3320 [zfs]\r\n[ 6305.806410]  [<ffffffffa0f64e23>] arc_read+0x583/0x3320 [zfs]\r\n[ 6305.806415]  [<ffffffffa0f6efb0>] ? dbuf_rele_and_unlock+0x480/0x560 [zfs]\r\n[ 6305.806423]  [<ffffffffa0f8ceaa>] ? dnode_block_freed+0xba/0x160 [zfs]\r\n[ 6305.806428]  [<ffffffffa0f6de5a>] dbuf_read+0x7fa/0xe20 [zfs]\r\n[ 6305.806434]  [<ffffffffa0f78f7b>] dmu_buf_hold_by_dnode+0x4b/0x80 [zfs]\r\n[ 6305.806445]  [<ffffffffa0fe37f4>] raidz_will_scalar_work+0x20e4/0x2780 [zfs]\r\n[ 6305.806453]  [<ffffffffa0fe3a43>] raidz_will_scalar_work+0x2333/0x2780 [zfs]\r\n[ 6305.806461]  [<ffffffffa0fe4cbd>] fzap_length+0x2d/0x90 [zfs]\r\n[ 6305.806470]  [<ffffffffa0fe8cb8>] ? zap_name_alloc_uint64+0x58/0x70 [zfs]\r\n[ 6305.806479]  [<ffffffffa0fe9707>] zap_length_uint64+0x77/0x240 [zfs]\r\n[ 6305.806485]  [<ffffffffa0f76610>] ddt_walk+0x370/0x4d0 [zfs]\r\n[ 6305.806488]  [<ffffffffa0cef95d>] ? SHA2Update+0x1ad/0x1f0 [icp]\r\n[ 6305.806494]  [<ffffffffa0f7484f>] ddt_entry_compare+0x82f/0x840 [zfs]\r\n[ 6305.806499]  [<ffffffffa0f75566>] ddt_lookup+0xc6/0x1c0 [zfs]\r\n[ 6305.806510]  [<ffffffffa0fb74d5>] ? abd_checksum_SHA256+0x55/0xa0 [zfs]\r\n[ 6305.806518]  [<ffffffffa10239ce>] ? zio_checksum_compute+0x17e/0x2d0 [zfs]\r\n[ 6305.806520]  [<ffffffffa0ecc835>] ? spl_kmem_cache_free+0x115/0x1a0 [spl]\r\n[ 6305.806522]  [<ffffffff81fd500d>] ? mutex_lock+0xd/0x30\r\n[ 6305.806529]  [<ffffffffa102106d>] zio_flush+0x8ed/0xed0 [zfs]\r\n[ 6305.806530]  [<ffffffffa0ecc835>] ? spl_kmem_cache_free+0x115/0x1a0 [spl]\r\n[ 6305.806538]  [<ffffffffa101dc0a>] ? zio_push_transform+0x3fa/0xbc0 [zfs]\r\n[ 6305.806544]  [<ffffffffa101d1a5>] zio_execute+0x85/0x450 [zfs]\r\n[ 6305.806546]  [<ffffffffa0ece396>] taskq_cancel_id+0x336/0x760 [spl]\r\n[ 6305.806548]  [<ffffffff81117b30>] ? wake_up_q+0x70/0x70\r\n[ 6305.806550]  [<ffffffffa0ece160>] ? taskq_cancel_id+0x100/0x760 [spl]\r\n[ 6305.806551]  [<ffffffff8110d092>] kthread+0xd2/0xf0\r\n[ 6305.806553]  [<ffffffff8110cfc0>] ? kthread_park+0x60/0x60\r\n[ 6305.806554]  [<ffffffff81fd7b12>] ret_from_fork+0x22/0x30\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6874/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ltz3317": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7038", "title": "zfs sync hang ", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  centos 7.2 \uff0csync hang\r\nDistribution Version    | \r\nLinux Kernel                 | 3.10.0-327.13.1.el7.x86_64 \r\nArchitecture                 | \r\nZFS Version                  | v0.7.5-1\r\nSPL Version                  |  v0.7.5-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nsync hang \uff0cmysql hang\uff0ckworker cpu 100\r\n### Describe how to reproduce the problem\r\nhigh frequency  create/destroy/clone\r\n### Include any warning/errors/backtraces from the system logs\r\ndmsg:\r\n[  189.990968] Adjusting tsc more than 11% (8039035 vs 7759471)\r\n[ 2522.644734] INFO: task mysqld:6608 blocked for more than 120 seconds.\r\n[ 2522.644790] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 2522.644854] mysqld          D 0000000000000000     0  6608   5602 0x00000000\r\n[ 2522.644860]  ffff880feb913da0 0000000000000086 ffff881006eac500 ffff880feb913fd8\r\n[ 2522.644865]  ffff880feb913fd8 ffff880feb913fd8 ffff881006eac500 ffff880fffc8a180\r\n[ 2522.644869]  ffff880fffc8a000 ffff880fffc8a188 ffff880fffc8a028 0000000000000000\r\n[ 2522.644873] Call Trace:\r\n[ 2522.644882]  [<ffffffff8163ae49>] schedule+0x29/0x70\r\n[ 2522.644906]  [<ffffffffa04744d5>] cv_wait_common+0x125/0x150 [spl]\r\n[ 2522.644911]  [<ffffffff810a6ae0>] ? wake_up_atomic_t+0x30/0x30\r\n[ 2522.644922]  [<ffffffffa0474515>] __cv_wait+0x15/0x20 [spl]\r\n[ 2522.644995]  [<ffffffffa062bd7b>] zil_commit.part.12+0x8b/0x830 [zfs]\r\n[ 2522.645006]  [<ffffffffa046d037>] ? spl_kmem_alloc+0xc7/0x170 [spl]\r\n[ 2522.645017]  [<ffffffffa0475cb4>] ? tsd_set+0x324/0x500 [spl]\r\n[ 2522.645022]  [<ffffffff81639082>] ? mutex_lock+0x12/0x2f\r\n[ 2522.645075]  [<ffffffffa062c537>] zil_commit+0x17/0x20 [zfs]\r\n[ 2522.645128]  [<ffffffffa0621a27>] zfs_fsync+0x77/0xf0 [zfs]\r\n[ 2522.645179]  [<ffffffffa0639665>] zpl_fsync+0x65/0x90 [zfs]\r\n[ 2522.645186]  [<ffffffff8120fa45>] do_fsync+0x65/0xa0\r\n[ 2522.645191]  [<ffffffff8120fd10>] SyS_fsync+0x10/0x20\r\n[ 2522.645196]  [<ffffffff81645ec9>] system_call_fastpath+0x16/0x1b\r\n[ 2522.645202] INFO: task mysqld:7514 blocked for more than 120 seconds.\r\n[ 2522.645245] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 2522.645296] mysqld          D ffff880fb554bda0     0  7514   5602 0x00000000\r\n[ 2522.645299]  ffff880fb554bd30 0000000000000086 ffff880ff4c8f300 ffff880fb554bfd8\r\n[ 2522.645303]  ffff880fb554bfd8 ffff880fb554bfd8 ffff880ff4c8f300 ffff880fb554bdb0\r\n[ 2522.645307]  ffff88107ff8dec0 0000000000000002 ffffffff811f9420 ffff880fb554bda0\r\n[ 2522.645312] Call Trace:\r\n[ 2522.645316]  [<ffffffff811f9420>] ? unlock_two_nondirectories+0x60/0x60\r\n[ 2522.645321]  [<ffffffff8163ae49>] schedule+0x29/0x70\r\n[ 2522.645324]  [<ffffffff811f942e>] inode_wait+0xe/0x20\r\n[ 2522.645328]  [<ffffffff81638cc0>] __wait_on_bit+0x60/0x90\r\n[ 2522.645335]  [<ffffffff812083bf>] __inode_wait_for_writeback+0xaf/0xf0\r\n[ 2522.645339]  [<ffffffff810a6b60>] ? wake_atomic_t_function+0x40/0x40\r\n[ 2522.645345]  [<ffffffff8120b996>] inode_wait_for_writeback+0x26/0x40\r\n[ 2522.645348]  [<ffffffff811fa135>] evict+0x95/0x170\r\n[ 2522.645351]  [<ffffffff811fa985>] iput+0xf5/0x180\r\n[ 2522.645357]  [<ffffffff811ef41e>] do_unlinkat+0x1ae/0x2b0\r\n[ 2522.645362]  [<ffffffff811e3fe1>] ? SyS_readlinkat+0xd1/0x140\r\n[ 2522.645367]  [<ffffffff811f0426>] SyS_unlink+0x16/0x20\r\n[ 2522.645371]  [<ffffffff81645ec9>] system_call_fastpath+0x16/0x1b\r\n[ 2522.645402] INFO: task sync:2653 blocked for more than 120 seconds.\r\n[ 2522.645443] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 2522.645494] sync            D ffffffff8120f8c0     0  2653   1455 0x00000000\r\n[ 2522.645498]  ffff880ffedd7d50 0000000000000082 ffff880f01449700 ffff880ffedd7fd8\r\n[ 2522.645502]  ffff880ffedd7fd8 ffff880ffedd7fd8 ffff880f01449700 ffff880ffedd7e80\r\n[ 2522.645506]  ffff880ffedd7e88 7fffffffffffffff ffff880f01449700 ffffffff8120f8c0\r\n[ 2522.645509] Call Trace:\r\n[ 2522.645515]  [<ffffffff8120f8c0>] ? generic_write_sync+0x60/0x60\r\n[ 2522.645519]  [<ffffffff8163ae49>] schedule+0x29/0x70\r\n[ 2522.645523]  [<ffffffff81638b39>] schedule_timeout+0x209/0x2d0\r\n[ 2522.645527]  [<ffffffff8109b426>] ? __queue_work+0x136/0x320\r\n[ 2522.645530]  [<ffffffff8109b6da>] ? __queue_delayed_work+0xaa/0x1a0\r\n[ 2522.645534]  [<ffffffff8109ba41>] ? try_to_grab_pending+0xb1/0x160\r\n[ 2522.645538]  [<ffffffff8120f8c0>] ? generic_write_sync+0x60/0x60\r\n[ 2522.645543]  [<ffffffff8163b216>] wait_for_completion+0x116/0x170\r\n[ 2522.645548]  [<ffffffff810b8c30>] ? wake_up_state+0x20/0x20\r\n[ 2522.645552]  [<ffffffff81208987>] sync_inodes_sb+0xb7/0x1e0\r\n[ 2522.645557]  [<ffffffff8120f8c0>] ? generic_write_sync+0x60/0x60\r\n[ 2522.645561]  [<ffffffff8120f8d9>] sync_inodes_one_sb+0x19/0x20\r\n[ 2522.645565]  [<ffffffff811e2182>] iterate_supers+0xb2/0x110\r\n[ 2522.645570]  [<ffffffff8120fb94>] sys_sync+0x44/0xb0\r\n[ 2522.645575]  [<ffffffff81645ec9>] system_call_fastpath+0x16/0x1b\r\n[ 2642.739175] INFO: task mysqld:6608 blocked for more than 120 seconds.\r\n[ 2642.739228] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 2642.739284] mysqld          D 0000000000000000     0  6608   5602 0x00000000\r\n[ 2642.739290]  ffff880feb913da0 0000000000000086 ffff881006eac500 ffff880feb913fd8\r\n[ 2642.739296]  ffff880feb913fd8 ffff880feb913fd8 ffff881006eac500 ffff880fffc8a180\r\n[ 2642.739300]  ffff880fffc8a000 ffff880fffc8a188 ffff880fffc8a028 0000000000000000\r\n[ 2642.739305] Call Trace:\r\n[ 2642.739315]  [<ffffffff8163ae49>] schedule+0x29/0x70\r\n[ 2642.739340]  [<ffffffffa04744d5>] cv_wait_common+0x125/0x150 [spl]\r\n[ 2642.739345]  [<ffffffff810a6ae0>] ? wake_up_atomic_t+0x30/0x30\r\n[ 2642.739357]  [<ffffffffa0474515>] __cv_wait+0x15/0x20 [spl]\r\n[ 2642.739434]  [<ffffffffa062bd7b>] zil_commit.part.12+0x8b/0x830 [zfs]\r\n[ 2642.739447]  [<ffffffffa046d037>] ? spl_kmem_alloc+0xc7/0x170 [spl]\r\n[ 2642.739460]  [<ffffffffa0475cb4>] ? tsd_set+0x324/0x500 [spl]\r\n[ 2642.739466]  [<ffffffff81639082>] ? mutex_lock+0x12/0x2f\r\n[ 2642.739526]  [<ffffffffa062c537>] zil_commit+0x17/0x20 [zfs]\r\n[ 2642.739587]  [<ffffffffa0621a27>] zfs_fsync+0x77/0xf0 [zfs]\r\n[ 2642.739647]  [<ffffffffa0639665>] zpl_fsync+0x65/0x90 [zfs]\r\n[ 2642.739654]  [<ffffffff8120fa45>] do_fsync+0x65/0xa0\r\n[ 2642.739659]  [<ffffffff8120fd10>] SyS_fsync+0x10/0x20\r\n[ 2642.739665]  [<ffffffff81645ec9>] system_call_fastpath+0x16/0x1b\r\n[ 2642.739670] INFO: task mysqld:7514 blocked for more than 120 seconds.\r\n[ 2642.739727] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 2642.739793] mysqld          D ffff880fb554bda0     0  7514   5602 0x00000000\r\n[ 2642.739798]  ffff880fb554bd30 0000000000000086 ffff880ff4c8f300 ffff880fb554bfd8\r\n[ 2642.739803]  ffff880fb554bfd8 ffff880fb554bfd8 ffff880ff4c8f300 ffff880fb554bdb0\r\n[ 2642.739808]  ffff88107ff8dec0 0000000000000002 ffffffff811f9420 ffff880fb554bda0\r\n[ 2642.739812] Call Trace:\r\n[ 2642.739818]  [<ffffffff811f9420>] ? unlock_two_nondirectories+0x60/0x60\r\n[ 2642.739823]  [<ffffffff8163ae49>] schedule+0x29/0x70\r\n[ 2642.739826]  [<ffffffff811f942e>] inode_wait+0xe/0x20\r\n[ 2642.739831]  [<ffffffff81638cc0>] __wait_on_bit+0x60/0x90\r\n[ 2642.739839]  [<ffffffff812083bf>] __inode_wait_for_writeback+0xaf/0xf0\r\n[ 2642.739843]  [<ffffffff810a6b60>] ? wake_atomic_t_function+0x40/0x40\r\n[ 2642.739850]  [<ffffffff8120b996>] inode_wait_for_writeback+0x26/0x40\r\n[ 2642.739854]  [<ffffffff811fa135>] evict+0x95/0x170\r\n[ 2642.739857]  [<ffffffff811fa985>] iput+0xf5/0x180\r\n[ 2642.739862]  [<ffffffff811ef41e>] do_unlinkat+0x1ae/0x2b0\r\n[ 2642.739868]  [<ffffffff811e3fe1>] ? SyS_readlinkat+0xd1/0x140\r\n[ 2642.739873]  [<ffffffff811f0426>] SyS_unlink+0x16/0x20\r\n[ 2642.739878]  [<ffffffff81645ec9>] system_call_fastpath+0x16/0x1b\r\n[ 2642.739897] INFO: task sync:2653 blocked for more than 120 seconds.\r\n[ 2642.739951] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 2642.740017] sync            D ffffffff8120f8c0     0  2653   1455 0x00000000\r\n[ 2642.740021]  ffff880ffedd7d50 0000000000000082 ffff880f01449700 ffff880ffedd7fd8\r\n[ 2642.740026]  ffff880ffedd7fd8 ffff880ffedd7fd8 ffff880f01449700 ffff880ffedd7e80\r\n[ 2642.740031]  ffff880ffedd7e88 7fffffffffffffff ffff880f01449700 ffffffff8120f8c0\r\n[ 2642.740036] Call Trace:\r\n[ 2642.740042]  [<ffffffff8120f8c0>] ? generic_write_sync+0x60/0x60\r\n[ 2642.740047]  [<ffffffff8163ae49>] schedule+0x29/0x70\r\n[ 2642.740051]  [<ffffffff81638b39>] schedule_timeout+0x209/0x2d0\r\n[ 2642.740056]  [<ffffffff8109b426>] ? __queue_work+0x136/0x320\r\n[ 2642.740060]  [<ffffffff8109b6da>] ? __queue_delayed_work+0xaa/0x1a0\r\n[ 2642.740064]  [<ffffffff8109ba41>] ? try_to_grab_pending+0xb1/0x160\r\n[ 2642.740069]  [<ffffffff8120f8c0>] ? generic_write_sync+0x60/0x60\r\n[ 2642.740093]  [<ffffffff8163b216>] wait_for_completion+0x116/0x170\r\n[ 2642.740100]  [<ffffffff810b8c30>] ? wake_up_state+0x20/0x20\r\n[ 2642.740105]  [<ffffffff81208987>] sync_inodes_sb+0xb7/0x1e0\r\n[ 2642.740110]  [<ffffffff8120f8c0>] ? generic_write_sync+0x60/0x60\r\n[ 2642.740116]  [<ffffffff8120f8d9>] sync_inodes_one_sb+0x19/0x20\r\n[ 2642.740121]  [<ffffffff811e2182>] iterate_supers+0xb2/0x110\r\n[ 2642.740127]  [<ffffffff8120fb94>] sys_sync+0x44/0xb0\r\n[ 2642.740134]  [<ffffffff81645ec9>] system_call_fastpath+0x16/0x1b\r\n[ 2762.834495] INFO: task mysqld:6608 blocked for more than 120 seconds.\r\n[ 2762.834547] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 2762.834604] mysqld          D 0000000000000000     0  6608   5602 0x00000000\r\n[ 2762.834609]  ffff880feb913da0 0000000000000086 ffff881006eac500 ffff880feb913fd8\r\n[ 2762.834615]  ffff880feb913fd8 ffff880feb913fd8 ffff881006eac500 ffff880fffc8a180\r\n[ 2762.834619]  ffff880fffc8a000 ffff880fffc8a188 ffff880fffc8a028 0000000000000000\r\n[ 2762.834624] Call Trace:\r\n[ 2762.834634]  [<ffffffff8163ae49>] schedule+0x29/0x70\r\n[ 2762.834659]  [<ffffffffa04744d5>] cv_wait_common+0x125/0x150 [spl]\r\n[ 2762.834665]  [<ffffffff810a6ae0>] ? wake_up_atomic_t+0x30/0x30\r\n[ 2762.834677]  [<ffffffffa0474515>] __cv_wait+0x15/0x20 [spl]\r\n[ 2762.834748]  [<ffffffffa062bd7b>] zil_commit.part.12+0x8b/0x830 [zfs]\r\n[ 2762.834761]  [<ffffffffa046d037>] ? spl_kmem_alloc+0xc7/0x170 [spl]\r\n[ 2762.834774]  [<ffffffffa0475cb4>] ? tsd_set+0x324/0x500 [spl]\r\n[ 2762.834779]  [<ffffffff81639082>] ? mutex_lock+0x12/0x2f\r\n[ 2762.834843]  [<ffffffffa062c537>] zil_commit+0x17/0x20 [zfs]\r\n[ 2762.834908]  [<ffffffffa0621a27>] zfs_fsync+0x77/0xf0 [zfs]\r\n[ 2762.834971]  [<ffffffffa0639665>] zpl_fsync+0x65/0x90 [zfs]\r\n[ 2762.834978]  [<ffffffff8120fa45>] do_fsync+0x65/0xa0\r\n[ 2762.834983]  [<ffffffff8120fd10>] SyS_fsync+0x10/0x20\r\n[ 2762.834989]  [<ffffffff81645ec9>] system_call_fastpath+0x16/0x1b\r\n[ 2762.834994] INFO: task mysqld:7514 blocked for more than 120 seconds.\r\n[ 2762.835051] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 2762.835118] mysqld          D ffff880fb554bda0     0  7514   5602 0x00000000\r\n[ 2762.835122]  ffff880fb554bd30 0000000000000086 ffff880ff4c8f300 ffff880fb554bfd8\r\n[ 2762.835127]  ffff880fb554bfd8 ffff880fb554bfd8 ffff880ff4c8f300 ffff880fb554bdb0\r\n[ 2762.835132]  ffff88107ff8dec0 0000000000000002 ffffffff811f9420 ffff880fb554bda0\r\n[ 2762.835137] Call Trace:\r\n[ 2762.835143]  [<ffffffff811f9420>] ? unlock_two_nondirectories+0x60/0x60\r\n[ 2762.835148]  [<ffffffff8163ae49>] schedule+0x29/0x70\r\n[ 2762.835151]  [<ffffffff811f942e>] inode_wait+0xe/0x20\r\n[ 2762.835156]  [<ffffffff81638cc0>] __wait_on_bit+0x60/0x90\r\n[ 2762.835164]  [<ffffffff812083bf>] __inode_wait_for_writeback+0xaf/0xf0\r\n[ 2762.835168]  [<ffffffff810a6b60>] ? wake_atomic_t_function+0x40/0x40\r\n[ 2762.835175]  [<ffffffff8120b996>] inode_wait_for_writeback+0x26/0x40\r\n[ 2762.835179]  [<ffffffff811fa135>] evict+0x95/0x170\r\n[ 2762.835183]  [<ffffffff811fa985>] iput+0xf5/0x180\r\n[ 2762.835188]  [<ffffffff811ef41e>] do_unlinkat+0x1ae/0x2b0\r\n[ 2762.835195]  [<ffffffff811e3fe1>] ? SyS_readlinkat+0xd1/0x140\r\n[ 2762.835199]  [<ffffffff811f0426>] SyS_unlink+0x16/0x20\r\n[ 2762.835205]  [<ffffffff81645ec9>] system_call_fastpath+0x16/0x1b\r\n[ 2762.835223] INFO: task sync:2653 blocked for more than 120 seconds.\r\n[ 2762.835277] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 2762.835343] sync            D ffffffff8120f8c0     0  2653   1455 0x00000000\r\n[ 2762.835348]  ffff880ffedd7d50 0000000000000082 ffff880f01449700 ffff880ffedd7fd8\r\n[ 2762.835352]  ffff880ffedd7fd8 ffff880ffedd7fd8 ffff880f01449700 ffff880ffedd7e80\r\n[ 2762.835357]  ffff880ffedd7e88 7fffffffffffffff ffff880f01449700 ffffffff8120f8c0\r\n[ 2762.835362] Call Trace:\r\n[ 2762.835369]  [<ffffffff8120f8c0>] ? generic_write_sync+0x60/0x60\r\n[ 2762.835374]  [<ffffffff8163ae49>] schedule+0x29/0x70\r\n[ 2762.835378]  [<ffffffff81638b39>] schedule_timeout+0x209/0x2d0\r\n[ 2762.835382]  [<ffffffff8109b426>] ? __queue_work+0x136/0x320\r\n[ 2762.835386]  [<ffffffff8109b6da>] ? __queue_delayed_work+0xaa/0x1a0\r\n[ 2762.835390]  [<ffffffff8109ba41>] ? try_to_grab_pending+0xb1/0x160\r\n[ 2762.835396]  [<ffffffff8120f8c0>] ? generic_write_sync+0x60/0x60\r\n[ 2762.835409]  [<ffffffff8163b216>] wait_for_completion+0x116/0x170\r\n[ 2762.835417]  [<ffffffff810b8c30>] ? wake_up_state+0x20/0x20\r\n[ 2762.835422]  [<ffffffff81208987>] sync_inodes_sb+0xb7/0x1e0\r\n[ 2762.835427]  [<ffffffff8120f8c0>] ? generic_write_sync+0x60/0x60\r\n[ 2762.835433]  [<ffffffff8120f8d9>] sync_inodes_one_sb+0x19/0x20\r\n[ 2762.835438]  [<ffffffff811e2182>] iterate_supers+0xb2/0x110\r\n[ 2762.835443]  [<ffffffff8120fb94>] sys_sync+0x44/0xb0\r\n[ 2762.835451]  [<ffffffff81645ec9>] system_call_fastpath+0x16/0x1b\r\n[ 2882.929813] INFO: task mysqld:6608 blocked for more than 120 seconds.\r\n[ 2882.929861] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 2882.929913] mysqld          D 0000000000000000     0  6608   5602 0x00000000\r\n[ 2882.929919]  ffff880feb913da0 0000000000000086 ffff881006eac500 ffff880feb913fd8\r\n[ 2882.929924]  ffff880feb913fd8 ffff880feb913fd8 ffff881006eac500 ffff880fffc8a180\r\n[ 2882.929928]  ffff880fffc8a000 ffff880fffc8a188 ffff880fffc8a028 0000000000000000\r\n[ 2882.929932] Call Trace:\r\n[ 2882.929942]  [<ffffffff8163ae49>] schedule+0x29/0x70\r\n[ 2882.929967]  [<ffffffffa04744d5>] cv_wait_common+0x125/0x150 [spl]\r\n[ 2882.929972]  [<ffffffff810a6ae0>] ? wake_up_atomic_t+0x30/0x30\r\n[ 2882.929983]  [<ffffffffa0474515>] __cv_wait+0x15/0x20 [spl]\r\n[ 2882.930049]  [<ffffffffa062bd7b>] zil_commit.part.12+0x8b/0x830 [zfs]\r\n[ 2882.930059]  [<ffffffffa046d037>] ? spl_kmem_alloc+0xc7/0x170 [spl]\r\n[ 2882.930070]  [<ffffffffa0475cb4>] ? tsd_set+0x324/0x500 [spl]\r\n[ 2882.930075]  [<ffffffff81639082>] ? mutex_lock+0x12/0x2f\r\n[ 2882.930129]  [<ffffffffa062c537>] zil_commit+0x17/0x20 [zfs]\r\n[ 2882.930181]  [<ffffffffa0621a27>] zfs_fsync+0x77/0xf0 [zfs]\r\n[ 2882.930232]  [<ffffffffa0639665>] zpl_fsync+0x65/0x90 [zfs]\r\n[ 2882.930238]  [<ffffffff8120fa45>] do_fsync+0x65/0xa0\r\n[ 2882.930243]  [<ffffffff8120fd10>] SyS_fsync+0x10/0x20\r\n[ 2882.930248]  [<ffffffff81645ec9>] system_call_fastpath+0x16/0x1b\r\n[ 4802.367346] perf interrupt took too long (2507 > 2500), lowering kernel.perf_event_max_sample_rate to 50000\r\n\r\nsync process stack:\r\n[<ffffffff81208987>] sync_inodes_sb+0xb7/0x1e0\r\n[<ffffffff8120f8d9>] sync_inodes_one_sb+0x19/0x20\r\n[<ffffffff811e2182>] iterate_supers+0xb2/0x110\r\n[<ffffffff8120fb94>] sys_sync+0x44/0xb0\r\n[<ffffffff81645ec9>] system_call_fastpath+0x16/0x1b\r\n[<ffffffffffffffff>] 0xffffffffffffffff\r\n\r\nkworker process stack:\r\n cat /proc/3445/stack \r\n[<ffffffffa058feb5>] dmu_zfetch+0x455/0x4f0 [zfs]\r\n[<ffffffffa05711ea>] dbuf_read+0x8ea/0x9f0 [zfs]\r\n[<ffffffffa0591246>] dnode_hold_impl+0xc6/0xc30 [zfs]\r\n[<ffffffffffffffff>] 0xffffffffffffffff\r\n\r\ncat /proc/3445/stack \r\n[<ffffffffffffffff>] 0xffffffffffffffff\r\n[root@ifcos ~]# cat /proc/3445/stack \r\n[<ffffffffa062879c>] zfs_zget+0xfc/0x250 [zfs]\r\n[<ffffffffa0623db7>] zfs_get_data+0x57/0x2d0 [zfs]\r\n[<ffffffffa062c10c>] zil_commit.part.12+0x41c/0x830 [zfs]\r\n[<ffffffffa062c537>] zil_commit+0x17/0x20 [zfs]\r\n[<ffffffffa06394b6>] zpl_writepages+0xd6/0x170 [zfs]\r\n[<ffffffff811759fe>] do_writepages+0x1e/0x40\r\n[<ffffffff812084e0>] __writeback_single_inode+0x40/0x220\r\n[<ffffffff81208f4e>] writeback_sb_inodes+0x25e/0x420\r\n[<ffffffff8120988f>] wb_writeback+0xff/0x2f0\r\n[<ffffffff8120bac5>] bdi_writeback_workfn+0x115/0x460\r\n[<ffffffff8109d5fb>] process_one_work+0x17b/0x470\r\n[<ffffffff8109e3cb>] worker_thread+0x11b/0x400\r\n[<ffffffff810a5aef>] kthread+0xcf/0xe0\r\n[<ffffffff81645e18>] ret_from_fork+0x58/0x90\r\n[<ffffffffffffffff>] 0xffffffffffffffff\r\n\r\n cat /proc/3445/stack \r\n[<ffffffffa058feb5>] dmu_zfetch+0x455/0x4f0 [zfs]\r\n[<ffffffffa0572fd5>] __dbuf_hold_impl+0x135/0x5a0 [zfs]\r\n[<ffffffffffffffff>] 0xffffffffffffffff\r\n\r\n cat /proc/3445/stack \r\n[<ffffffffa056fa69>] dbuf_find+0x1c9/0x1d0 [zfs]\r\n[<ffffffffa0572ee2>] __dbuf_hold_impl+0x42/0x5a0 [zfs]\r\n[<ffffffffffffffff>] 0xffffffffffffffff\r\n\r\n cat /proc/3445/stack \r\n[<ffffffffa058feb5>] dmu_zfetch+0x455/0x4f0 [zfs]\r\n[<ffffffffa0571056>] dbuf_read+0x756/0x9f0 [zfs]\r\n[<ffffffffa0591246>] dnode_hold_impl+0xc6/0xc30 [zfs]\r\n\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7038/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sempervictus": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7035", "title": "Consider adding mitigations for speculative execution related concerns", "body": "GCC should get retpoline support soon, and Intel seems to be proposing kernel code with barriers to speculative execution - https://patchwork.ozlabs.org/cover/856316/. ZFS is already pretty unhappy from KPTI, but since there's a good deal of user controlled data going into it, this might be worth investigating.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7035/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6888", "title": "ZVOL ops hanging", "body": "### System information\r\n<!--  add version after \"|\" character -->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Arch\r\nDistribution Version    | N/A\r\nLinux Kernel                 | 4.9.62\r\nArchitecture                 | x86-64\r\nZFS Version                  | master + all patches and changes\r\nSPL Version                  | UNKNOWN\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nWhile running a dd if=ssd of=raidz2/zvol bs=64M i'm getting consistent crashes and hangs which look like:\r\n```\r\n[ 1107.239763] INFO: task zvol:514 blocked for more than 120 seconds.\r\n[ 1107.261785]       Tainted: P           OE   4.9.62 #1\r\n[ 1107.274064] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1107.299938] zvol            D    0   514      2 0x00000000\r\n[ 1107.299947]  ffffffff810a88e0 0000000000000000 ffff8817ec930000 ffff880ff76e0000\r\n[ 1107.299956]  ffff880fffc96ac0 ffffc9000d73bae8 ffffffff81806d4a ffffffff813d2e03\r\n[ 1107.299963]  ffff880fffc96ac0 ffffc9000d73bb10 ffff880ff76e0000 ffff881ff75f7a40\r\n[ 1107.299975] Call Trace:\r\n[ 1107.299990]  [<ffffffff810a88e0>] ? switched_to_idle+0x20/0x20\r\n[ 1107.299999]  [<ffffffff81806d4a>] ? __schedule+0x24a/0x6e0\r\n[ 1107.300004]  [<ffffffff813d2e03>] ? __list_add+0x33/0x60\r\n[ 1107.300010]  [<ffffffff81807224>] schedule+0x44/0x90\r\n[ 1107.300024]  [<ffffffffa01f8fd4>] cv_wait_common+0x144/0x160 [spl]\r\n[ 1107.300030]  [<ffffffff810bc910>] ? prepare_to_wait_event+0x110/0x110\r\n[ 1107.300043]  [<ffffffffa01f900f>] __cv_wait+0x1f/0x30 [spl]\r\n[ 1107.300126]  [<ffffffffa07b0533>] txg_wait_open+0xb3/0x100 [zfs]\r\n[ 1107.300185]  [<ffffffffa075b640>] dmu_tx_wait+0x380/0x390 [zfs]\r\n[ 1107.300244]  [<ffffffffa075b833>] dmu_tx_assign+0x93/0x4b0 [zfs]\r\n[ 1107.300294]  [<ffffffffa0819fdd>] zvol_write+0x14d/0x560 [zfs]\r\n[ 1107.300303]  [<ffffffffa01f125c>] ? spl_kmem_free+0x2c/0x40 [spl]\r\n[ 1107.300312]  [<ffffffffa01f4ca1>] taskq_thread+0x2b1/0x4d0 [spl]\r\n[ 1107.300324]  [<ffffffff810a01e0>] ? wake_up_q+0x90/0x90\r\n[ 1107.300333]  [<ffffffffa01f49f0>] ? task_done+0xa0/0xa0 [spl]\r\n[ 1107.300340]  [<ffffffff81095e4d>] kthread+0xfd/0x120\r\n[ 1107.300346]  [<ffffffff81095d50>] ? kthread_parkme+0x40/0x40\r\n[ 1107.300352]  [<ffffffff8180c667>] ret_from_fork+0x37/0x50\r\n```\r\nPreventing the operation from completing and requiring a full system reboot. I pulled all patches and changes, just built from master, and still seeing the issue.\r\nThis is a very common stack trace (or similar to many others i've seen over the years), and i'd love to never see it again... :)", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6888/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6856", "title": "Broken ZIL (during heavy testing of #6566)", "body": "While trying to diagnose a DVA error seen in ztest on some hosts, i seem to have broken a production pool's intent log. The pool is a raidz consisting of 5 SSDs on dm-crypt - a pretty much universal setup around here. During one of the crashes seen yesterday, something bad happened to the ZIL such that import fails crashing the system spectacularly:\r\n```\r\n[Sun Nov 12 01:51:55 2017] NMI watchdog: BUG: soft lockup - CPU#0 stuck for 22s! [dmu_objset_find:6907]\r\n[Sun Nov 12 01:51:55 2017] Modules linked in: serpent_sse2_x86_64 serpent_generic lrw glue_helper ablk_helper cryptd jc42 nls_iso8859_1 nls_cp437 vfat fat ext4 crc16 jbd2 fscrypto mbcache iTCO_wdt iTCO_vendor_support gpio_ich ppdev ttm drm_kms_helper intel_powerclamp drm coretemp intel_cstate input_leds evdev i2c_algo_bit fb_sys_fops led_class joydev syscopyarea mousedev mac_hid sysfillrect usbmouse usbkbd sysimgblt psmouse uio_pdrv_genirq pcspkr uio i2c_i801 parport_pc i2c_smbus ioatdma i7core_edac parport button edac_core shpchp i5500_temp ipmi_ssif dca lpc_ich acpi_cpufreq sch_fq_codel 8021q garp mrp iscsi_scst scst_local scst dlm dm_thin_pool dm_persistent_data dm_bio_prison dm_bufio libcrc32c crc32c_generic kvm_intel kvm irqbypass ipmi_devintf ipmi_si ipmi_msghandler br_netfilter bridge stp llc ip_tables x_tables\r\n[Sun Nov 12 01:51:55 2017]  zfs(POE) zunicode(POE) zcommon(POE) znvpair(POE) zavl(POE) icp(POE) spl(OE) zlib_deflate xts gf128mul algif_skcipher af_alg dm_crypt dm_mod sd_mod mlx4_en hid_generic usbhid hid uas usb_storage uhci_hcd serio_raw atkbd libps2 crc32c_intel mlx4_core ahci devlink libahci ehci_pci ehci_hcd usbcore e1000e mpt3sas usb_common ptp raid_class scsi_transport_sas pps_core i8042 serio fjes\r\n[Sun Nov 12 01:51:55 2017] CPU: 0 PID: 6907 Comm: dmu_objset_find Tainted: P           OEL  4.9.61 #1\r\n[Sun Nov 12 01:51:55 2017] Hardware name: Intel Thurley/Greencity, BIOS GEMAV200 01/28/2011\r\n[Sun Nov 12 01:51:55 2017] task: ffff8807eb65ee00 task.stack: ffffc9003e910000\r\n[Sun Nov 12 01:51:55 2017] RIP: 0010:[<ffffffffa0626649>]  [<ffffffffa0626649>] zil_claim_log_record+0x19/0x60 [zfs]\r\n[Sun Nov 12 01:51:55 2017] RSP: 0000:ffffc9003e913a38  EFLAGS: 00000246\r\n[Sun Nov 12 01:51:55 2017] RAX: 0000000000000000 RBX: ffffc90036d86efe RCX: 0000000002305788\r\n[Sun Nov 12 01:51:55 2017] RDX: 0000000000000000 RSI: ffffc90036d86efe RDI: ffff880813d56800\r\n[Sun Nov 12 01:51:55 2017] RBP: ffffc9003e913a58 R08: ffff8807e0324b58 R09: 00000001802e001c\r\n[Sun Nov 12 01:51:55 2017] R10: 0100000000000000 R11: 0100000000000000 R12: ffff880813d56800\r\n[Sun Nov 12 01:51:55 2017] R13: 0000000000000000 R14: 0000000002305788 R15: ffffffffffffffff\r\n[Sun Nov 12 01:51:55 2017] FS:  0000000000000000(0000) GS:ffff88081fa00000(0000) knlGS:0000000000000000\r\n[Sun Nov 12 01:51:55 2017] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[Sun Nov 12 01:51:55 2017] CR2: 00000061b0b41000 CR3: 0000000001d5a000 CR4: 00000000000006f0\r\n[Sun Nov 12 01:51:55 2017] Stack:\r\n[Sun Nov 12 01:51:55 2017]  0000000000000000 ffffffffa0626630 ffffc90036d86efe 00000002540ee182\r\n[Sun Nov 12 01:51:55 2017]  ffffc9003e913c68 ffffffffa06280b8 ffffc90001800190 ffffc9003e913aec\r\n[Sun Nov 12 01:51:55 2017]  ffffc9003e913af8 ffffffffa0626630 ffffc90037d97000 0000000000000000\r\n[Sun Nov 12 01:51:55 2017] Call Trace:\r\n[Sun Nov 12 01:51:55 2017]  [<ffffffffa0626630>] ? zil_claim_log_block+0x90/0x90 [zfs]\r\n[Sun Nov 12 01:51:55 2017]  [<ffffffffa06280b8>] zil_parse+0x688/0x910 [zfs]\r\n[Sun Nov 12 01:51:55 2017]  [<ffffffffa0626630>] ? zil_claim_log_block+0x90/0x90 [zfs]\r\n[Sun Nov 12 01:51:55 2017]  [<ffffffffa06265a0>] ? zil_replaying+0x70/0x70 [zfs]\r\n[Sun Nov 12 01:51:55 2017]  [<ffffffffa0628d91>] zil_check_log_chain+0xe1/0x1a0 [zfs]\r\n[Sun Nov 12 01:51:55 2017]  [<ffffffffa0574e11>] dmu_objset_find_dp_impl+0x151/0x400 [zfs]\r\n[Sun Nov 12 01:51:55 2017]  [<ffffffffa05750e6>] dmu_objset_find_dp_cb+0x26/0x40 [zfs]\r\n[Sun Nov 12 01:51:55 2017]  [<ffffffffa014cca1>] taskq_thread+0x2b1/0x4d0 [spl]\r\n[Sun Nov 12 01:51:55 2017]  [<ffffffff810a0160>] ? wake_up_q+0x90/0x90\r\n[Sun Nov 12 01:51:55 2017]  [<ffffffffa014c9f0>] ? task_done+0xa0/0xa0 [spl]\r\n[Sun Nov 12 01:51:55 2017]  [<ffffffff81095ddd>] kthread+0xfd/0x120\r\n[Sun Nov 12 01:51:55 2017]  [<ffffffff81095ce0>] ? kthread_parkme+0x40/0x40\r\n[Sun Nov 12 01:51:55 2017]  [<ffffffff8180c5e7>] ret_from_fork+0x37/0x50\r\n```\r\nTried to import with different revisions of ZFS compatible with the feature flags on that pool (luckily not a crypto pool), all with the same result. The -m and -F flags also give nada. Google searches for ignoring the ZIL lead to SLOG-related issues, though i seem to recall that someone had a patch allowing import dropping the current ZIL state to avoid this sort of mess.\r\n\r\nI'm pushing a restore from just before this all started back to another pool in the system, but would be nice to not be missing a couple of days of delta.\r\n@dweeezil, @prakashsurya, @behlendorf: do you folks happen to recall if and where such a ZIL-drop patch would be? Probably something we want to doc and push up in the search results if it exists.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6856/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6789", "title": "ZFS Reports Incorrect/Opinionated Space Info with Dedup Working", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Arch\r\nDistribution Version    |  4.9 (rolling)\r\nLinux Kernel                 | 4.9.x with some changes\r\nArchitecture                 | x64\r\nZFS Version                  | master - 0.7.0-1 + DMU prefetch, removal of lock tracking, and write path optimization PRs\r\nSPL Version                  | master - 0.7.0-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nA 40G pool with ~10G used and ~30G free, shows at the VFS/ZPL tier as having 67G of data with 27 to spare:\r\n```\r\n[root@tl-anc00 ~]# zfs list\r\nNAME            USED  AVAIL  REFER  MOUNTPOINT\r\ntl-nc00        67.2G  27.1G    23K  none\r\ntl-nc00/snap   64.6G  27.1G  64.0G  /var/snap\r\ntl-nc00/snapd  2.58G  27.1G   792M  /var/lib/snapd\r\n[root@tl-anc00 ~]# zpool list\r\nNAME      SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT\r\ntl-nc00  39.8G  9.68G  30.1G         -    48%    24%  7.39x  ONLINE  -\r\n[root@tl-anc00 ~]# modinfo zfs | head -2\r\nfilename:       /lib/modules/4.9.53-1-sv/extra/zfs/zfs/zfs.ko\r\nversion:        0.7.0-1\r\n```\r\nThe dataset compressratio is 1.2. If the math on this is _dataSize * dedupRatio * 1/compressRatio_ then _9.68 * 7.39 * 0.83_ comes out to 59.4, not 67.2. Either way, showing the used and especially the available space in a dataset based on dedup computation at the pool level seems like it could lead to serious overestimation of available capacity by someone looking at an aggregate display of available space through some DCIM/devops toolset when determining where to place an entirely new, potentially encrypted (not a good candidate for dedup or compression) dataset. Same goes for things like ElasticSearch which would normally stop a node at a certain threshold as reported by the VFS, but could jump right past it if the new data doesnt compress and dedup like prior data did. At the VFS tier, this white lie is opaque, and ENOSPC on such systems can be less than fun to deal with, especially if the original delay in response leads to other CoW failures in the pool/system.\r\nIs this intended by design, or a trickle-down effect of pool-level vs dataset-level operations (dedup vs compression for instance)? Is there appetite to change this behavior, or is it either desired this way or too burdensome to implement as a breaking change?", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6789/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6447", "title": "Attempting to export pool with its opaquely underlying media removed breaks ZFS systemwide", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Arch\r\nDistribution Version    | Current\r\nLinux Kernel                 | 4.9.39\r\nArchitecture                 | x64\r\nZFS Version                  | master + crypto\r\nSPL Version                  | master + crypto\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nWhen calling zpool export on a pool which has lost its backing media (in this case a thumb drive with a dm-crypt volume on it, with the volume being the sole backing VDEV for the pool), the command blocks, subsequently blocking all invocations of _zpool_ systemwide and producing this stack trace:\r\n```\r\nAug 02 14:57:23 unknown kernel: INFO: task zpool:7041 blocked for more than 120 seconds.\r\nAug 02 14:57:23 unknown kernel:       Tainted: P           OE   4.9.39-1-sv #1\r\nAug 02 14:57:23 unknown kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nAug 02 14:57:23 unknown kernel: zpool           D    0  7041   6464 0x00000084\r\nAug 02 14:57:23 unknown kernel:  ffffffff810a93a0 0000000000000000 ffff880835b6c000 ffff8806b1ac4d00\r\nAug 02 14:57:23 unknown kernel:  ffff88083f2d6b40 ffffc90061803b90 ffffffff81837dda ffffffff814008e3\r\nAug 02 14:57:23 unknown kernel:  ffff88083ac34d00 ffffc90061803bb8 ffff8806b1ac4d00 ffff88052f5a4230\r\nAug 02 14:57:23 unknown kernel: Call Trace:\r\nAug 02 14:57:23 unknown kernel:  [<ffffffff810a93a0>] ? switched_to_idle+0x20/0x20\r\nAug 02 14:57:23 unknown kernel:  [<ffffffff81837dda>] ? __schedule+0x24a/0x6e0\r\nAug 02 14:57:23 unknown kernel:  [<ffffffff814008e3>] ? __list_add+0x33/0x60\r\nAug 02 14:57:23 unknown kernel:  [<ffffffff818382b4>] schedule+0x44/0x90\r\nAug 02 14:57:23 unknown kernel:  [<ffffffffa0e8c139>] cv_wait_common+0x129/0x140 [spl]\r\nAug 02 14:57:23 unknown kernel:  [<ffffffff810bd3d0>] ? prepare_to_wait_event+0x110/0x110\r\nAug 02 14:57:23 unknown kernel:  [<ffffffffa0e8c16f>] __cv_wait+0x1f/0x30 [spl]\r\nAug 02 14:57:23 unknown kernel:  [<ffffffffa124c3d0>] txg_wait_synced+0xd0/0x110 [zfs]\r\nAug 02 14:57:23 unknown kernel:  [<ffffffffa123b148>] spa_export_common.part.22+0x2e8/0x3b0 [zfs]\r\nAug 02 14:57:23 unknown kernel:  [<ffffffff811d7a73>] ? kfree+0x183/0x1c0\r\nAug 02 14:57:23 unknown kernel:  [<ffffffffa123b297>] spa_export+0x47/0x60 [zfs]\r\nAug 02 14:57:23 unknown kernel:  [<ffffffffa127ca82>] zfs_ioc_pool_export+0x32/0x40 [zfs]\r\nAug 02 14:57:23 unknown kernel:  [<ffffffffa127b089>] zfsdev_ioctl+0x229/0x510 [zfs]\r\nAug 02 14:57:23 unknown kernel:  [<ffffffff8121b590>] do_vfs_ioctl+0xc0/0x7d0\r\nAug 02 14:57:23 unknown kernel:  [<ffffffff810013e9>] ? syscall_trace_enter+0x129/0x2b0\r\nAug 02 14:57:23 unknown kernel:  [<ffffffff8121bd2c>] sys_ioctl+0x8c/0xa0\r\nAug 02 14:57:23 unknown kernel:  [<ffffffff8100191f>] do_syscall_64+0x7f/0x1b0\r\nAug 02 14:57:23 unknown kernel:  [<ffffffff8183d465>] entry_SYSCALL64_slow_path+0x25/0x25\r\n```\r\nFrom a naive reading of the stack trace, looks like spa_export_common doesn't deal with this sort of failure. If the ioctls are serialized, then hanging like this would logically block all further invocations.\r\n\r\nThe crypt volume which acts as the VDEV is still \"logically present\" in the system, and cannot be dropped while it has consumers (the reference held by the zpool vdev). Unless spa_export_common can be taught to deal with faulty but not failed disks (those accepting IOs but returning nothing), i'm not sure this can be addressed in the export code.\r\n\r\nThe use case will be legitimate for a while yet. While ZFS crypto can address this to a degree, it stores metadata in the clear, and isn't ready in terms of integration with init systems for decryption at boot, user managers for encrypted homes, crypttab, or grub. DM-crypt aside, i could see this happening with NFS-exported files backing pools (virtualization for instance), or with caching tiers losing their backing in all sorts of applications.\r\n\r\nAll that in mind, is there an alternate path by which we can permit the ioctl interface to accept commands to other zpools/functions while we have a hung pool? Can we \"unhang\" pools somehow once this sort of condition has been created?", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6447/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/24187613", "body": "I think this is supposed to be #endif for REQ_SECURE, and GCC seems to agree that this is a problem. Strange thing is, the compile-time error only popped up during the DKMS build on one host (several others built, which is now slightly concerning). Everything runs the same kernel, not sure why this one tripped it.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/24187613/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/24735064", "body": "Are we sure that _spa_writeable(vd->vdev_spa)_ returns true in all contexts here? I'm getting the suspicion that #3094 is connected to this or #2784, and this line may explain the pool importing itself sans space map and corrupting all future space maps by writing data during the period it is imported this way without updating the space maps.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/24735064/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "abraunegg": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7034", "title": "Missing parameter descriptions in ZFS-Module-Parameters man page", "body": "The following zfs module parameters are missing from the zfs-module-parameters man page (updated 28th Oct 2017) using ZFS 0.7.5:\r\n\r\n* dbuf_cache_hiwater_pct\r\n* dbuf_cache_lowater_pct\r\n* dbuf_cache_max_bytes\r\n* dbuf_cache_max_shift\r\n* dmu_object_alloc_chunk_shift\r\n* send_holes_without_birth_time\r\n* zfs_abd_scatter_enabled\r\n* zfs_abd_scatter_max_order\r\n* zfs_compressed_arc_enabled\r\n* zfs_sync_taskq_batch_pct\r\n\r\nHappy to create a documentation patch for the man pages if someone can send me the a description of what the module parameter is, what the default should be and what valid options are if it is being changed.\r\n\r\nBest regards,\r\n\r\nAlex", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7034/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/ca85d690974c4c1e12f1bd5eadfa47806ba01f89", "message": "Update zfs module parameters man5\n\nUpdate zfs module parameters man5 with missing parameter details\r\nfor multiple tunings.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Alex Braunegg <alex.braunegg@gmail.com>\r\nCloses #6785"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rincebrain": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7024", "title": "zfs send -R | zfs recv can fail in the middle due to a snapshot being taken", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian\r\nDistribution Version    | 9.3\r\nLinux Kernel                 | 4.9.0-4-amd64\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.7.3-3\r\nSPL Version                  | 0.7.3-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing + how to reproduce the problem\r\nWhen doing a long-running `zfs send -R foo/bar/baz@ten | zfs recv -ds newfoo`, an automated utility helpfully took a recursive snapshot on newfoo, and zfs recv abruptly died with `cannot receive incremental stream: kernel modules must be upgraded to receive this stream.` with newfoo/bar/baz having completed snapshots one, ..., seven and throwing that error on attempting to resume.\r\n\r\nI would have expected the in-progress receiving dataset(s) to have remained immutable until the receives were completed, but apparently this isn't the case.\r\n\r\nDestroying the errant snapshot on newfoo/bar/baz allowed the zfs send to proceed like nothing ever happened.\r\n\r\nSince there's already a number of bugs suggesting that this message should be broken out and detailed further (#6547, #6574), this bug is mostly about the fact that nothing is stopping you from shooting yourself in the foot and not being able to discover why without making dramatic leaps or (presumably) reading zfs/dbgmsg.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7024/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6335", "title": "zfs diff outputs grossly incorrect results", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian\r\nDistribution Version    | 9.0\r\nLinux Kernel                 | 4.9.0-3-amd64\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.9\r\nSPL Version                  | 0.6.5.9\r\n\r\n### Describe the problem you're observing\r\nI have a temporary directory for incoming data that has nightly snapshots.\r\n\r\nPer zfs list -t all -r...\r\n```\r\nmonolith/unsorted/incoming@periodic-20161212   488K      -   579G  -\r\nmonolith/unsorted/incoming@periodic-20161213   720K      -   407G  -\r\n```\r\n\r\nBut per zfs diff...\r\n```\r\n# zfs diff monolith/unsorted/incoming@periodic-20161212 monolith/unsorted/incoming@periodic-20161213\r\nM       /monolith/unsorted/incoming/\r\nUnable to determine path or stats for object 23125 in monolith/unsorted/incoming@periodic-20161212: Invalid argument\r\n(The object with inum 23125 is an 18k textfile, the COPYING from a udev-0.5.5 tree. It is not 100+ GB)\r\n\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6335/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6180", "title": "zpool history often missing attach/detach events", "body": "I'm running 0.6.5.8 on Debian 4.7.0-0.bpo.1-amd64, at the moment.\r\n\r\nI have a pool which was originally created on 0.6.5.X well over a year ago, and currently looks like this:\r\n\r\n```\r\n  pool: monolith\r\n state: ONLINE\r\n  scan: scrub repaired 0 in 25h54m with 0 errors on Mon May 15 02:18:34 2017\r\nconfig:\r\n\r\n        NAME                                     STATE     READ WRITE CKSUM\r\n        monolith                                 ONLINE       0     0     0\r\n          mirror-0                               ONLINE       0     0     0\r\n            ata-TOSHIBA_MD04ACA500_xxx  ONLINE       0     0     0\r\n            ata-TOSHIBA_MD04ACA500_xxx  ONLINE       0     0     0\r\n            ata-TOSHIBA_HDWE150_xxx     ONLINE       0     0     0\r\n          mirror-1                               ONLINE       0     0     0\r\n            ata-TOSHIBA_MD04ACA500_xxx  ONLINE       0     0     0\r\n            ata-TOSHIBA_HDWE150_xxx     ONLINE       0     0     0\r\n          mirror-2                               ONLINE       0     0     0\r\n            ata-TOSHIBA_MD04ACA500_xxx  ONLINE       0     0     0\r\n            ata-TOSHIBA_HDWE150_xxx     ONLINE       0     0     0\r\n          mirror-3                               ONLINE       0     0     0\r\n            ata-TOSHIBA_HDWE150_xxx     ONLINE       0     0     0\r\n            ata-TOSHIBA_MD04ACA500_xxx  ONLINE       0     0     0\r\n          mirror-4                               ONLINE       0     0     0\r\n            ata-TOSHIBA_MD04ACA500_xxx  ONLINE       0     0     0\r\n            ata-TOSHIBA_HDWE150_xxx     ONLINE       0     0     0\r\n```\r\n\r\nSo, 5 vdevs, 4 of them 2-way mirrors and one 3-way, for a total of 11 disks.\r\n\r\nLet's look at the history.\r\n\r\n```\r\n$ sudo zpool history monolith | egrep '(zpool create|attach|detach|add)'\r\n2015-10-19.22:10:49 zpool create -f dinolith -o ashift=12 sdk sdb sdi sdc sde\r\n2017-02-13.02:42:51 zpool detach monolith ata-TOSHIBA_MD04ACA500_xxx\r\n2017-02-13.02:43:11 zpool attach monolith ata-HGST_HDN724040ALE640_xxx ata-TOSHIBA_MD04ACA500_xxx\r\n2017-02-13.17:44:11 zpool detach monolith ata-HGST_HDN724040ALE640_xxx\r\n2017-02-13.21:24:00 zpool attach monolith -f ata-HGST_HDN724040ALE640_xxx ata-TOSHIBA_HDWE150_xxx\r\n2017-02-14.15:00:11 zpool detach monolith ata-HGST_HDN724040ALE640_xxx\r\n2017-02-20.02:09:53 zpool attach -f monolith ata-TOSHIBA_MD04ACA500_xxx ata-TOSHIBA_HDWE150_xxx\r\n```\r\n\r\nSo according to zpool history, the pool was created with 5 simple disks, then a pattern of 3 instances of detach then attach were run.\r\n\r\nAccording to this, I should have no more than 5 disks in the pool at the moment, with a period of having 0 disks in one of the vdevs.\r\n\r\nI'm not sure what should be occurring here - was there a point in the past where attach/detach was not included in zpool history, since corrected? I see in #1998 mention of attach and detach in zpool history's output, so it was expected at least that far back...", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6180/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6135", "title": "Inconsistent vdev size with identical disks", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Debian\r\nDistribution Version    |  8.7 (Jessie)\r\nLinux Kernel                 | 4.7.0-0.bpo.1-amd64\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.6.5.8\r\nSPL Version                  | 0.6.5.8\r\n\r\n### Describe the problem you're observing\r\nI have a pool, originally created from 5 disks, e.g. zpool create monolith sda sdb sdc sdd sde, and then turned into a stripe of mirrors using 5 zpool attaches.\r\n\r\nSince then, I've also replaced all the originally involved disks with larger-capacity disks, primarily of two models - the Toshiba HDWE150 and MD04ACA500.\r\n\r\nRecently, I noticed the following when examining zpool list -v:\r\n```\r\n  mirror  4.53T  4.32T   218G         -    40%    95%\r\n    ata-TOSHIBA_MD04ACA500_xxx      -      -      -         -      -      -\r\n    ata-TOSHIBA_MD04ACA500_xxx      -      -      -         -      -      -\r\n    ata-TOSHIBA_HDWE150_xxx      -      -      -         -      -      -\r\n  mirror  4.53T  4.32T   212G         -    40%    95%\r\n    ata-TOSHIBA_MD04ACA500_xxx      -      -      -         -      -      -\r\n    ata-TOSHIBA_HDWE150_xxx      -      -      -         -      -      -\r\n  mirror  4.53T  4.17T   370G         -    33%    92%\r\n    ata-TOSHIBA_MD04ACA500_xxx      -      -      -         -      -      -\r\n    ata-TOSHIBA_HDWE150_xxx      -      -      -         -      -      -\r\n  mirror  4.55T  4.41T   137G         -    46%    97%\r\n    ata-TOSHIBA_HDWE150_xxx      -      -      -         -      -      -\r\n    ata-TOSHIBA_MD04ACA500_xxx      -      -      -         -      -      -\r\n  mirror  4.55T  4.27T   284G         -    39%    93%\r\n    ata-TOSHIBA_MD04ACA500_xxx      -      -      -         -      -      -\r\n    ata-TOSHIBA_HDWE150_xxx      -      -      -         -      -      -\r\n```\r\n\r\nThat is, three of the vdevs report 4.53T total capacity, and two report 4.55T - yet all 5 are composed of identical disks, reporting the same available capacity down to the byte, and all 5 were only created by handing ZFS whole disks without partitioning.\r\n\r\nAll but one of them report identical partitioning (the first disk in the fifth vdev has a 4096b longer part1 than all the others).\r\n\r\n### Describe how to reproduce the problem\r\nUnknown?\r\n\r\nI've tried zpool online -e with reckless abandon, but no changes have occurred, and autoexpand=on anyway, so it should have happened on last import if it were going to do anything.\r\n\r\nI'm going to try 0.6.5.9 on next reboot (which I suppose I'll schedule Real Soon Now specifically for this), but I didn't see anything in the changelog that might touch this, so I thought I might as well file it now.\r\n\r\n(Also, ashift=12 on all 5 vdevs.)", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6135/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "h1z1": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7015", "title": "Impact of Intel bug (tm) on ZFS", "body": "Maybe the wrong avenue for this but no doubt like others watching the events of the last few days unfold, I've been asked to comment on the impact to ZFS in our environment.   I'm in a rather odd position as I don't really have the hardware to duplicate an entire production silo, running 4.15.x kernel.   I do however know at least 4.14 will bite us as per #6929.  \r\n\r\nHas anyone tested or confirmed what the impact of this will be going forward?  Would rather not duplicate effort if it's already being addressed.\r\n\r\nThanks", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7015/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6885", "title": "Feature request:  Detect and warn slow disk performance", "body": "Wasn't really sure how to bring this up.  Short version : assuming it's not already possible, it would be quite helpful if ZFS detected when a disk or vdev fell below a consistent metric like access time.  \r\n\r\nSlightly longer version and why this came up is I have run into cases where individual disks silently fail, both soft and hard.   Vendors of course have their \"NAS\" versions however I've run into this WITH THOSE DISKS.   I've also run into this where Samsung SSD's (the 840 and 850 for example), will start to crawl due to well known firmware bugs especially on the  840s.  However regardless of firmware or power management bugs, the underlining issue is vendor neutral.\r\n\r\nThis really came to light last week when I had two 10TB Segate Ironwolf pro drives in a three-way mirror go bad in 4 hours.   There were no SMART errors and certainly no errors bubbling up to the kernel.   Users didn't really notice anything as most of their data is hot in either cache or L2 .  Despite the number of systems that should have caught it, ZFS only did through checksums (something I'll be forever thankful for).   Yet testing those disks with a simple dd, would have shown they were operating at under 10M/s.\r\n\r\nIs any of this already in place outside individual implementations?  I'm not really looking to replace vendors analytic engines for example as they serve a different purpose.\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6885/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sanjeevbagewadi": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7011", "title": "With \"casesensitivity=mixed\" hitting an assert in ZAP code", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | CentOS release 6.8 (Final)\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS\r\nDistribution Version    | 6.8\r\nLinux Kernel                 | 4.4.14-1.el6\r\nArchitecture                 | x86\r\nZFS Version                  | 0.7.1-1\r\nSPL Version                  |  0.7.1-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nWith casesensitivity=mixed was running the following test :\r\nroot@NTNX-10-5-137-31-A-FSVM:/home/nutanix# cat names.py\r\n#!/usr/bin/python\r\nimport itertools\r\ns=\"abcdefghijklmnopqrstuvwxyz\"\r\nlength = len(s)\r\nnames = map(''.join, itertools.product(*zip(s.upper(), s.lower())))\r\nfor name in names:\r\n    print name\r\n\r\nroot@NTNX-10-5-137-31-A-FSVM:/home/nutanix# ./names.py | while read file\r\n> do\r\n> touch /test/fs2/dir1/$file\r\n> done\r\n\r\nAnd hit the following panic \r\n-- snip --\r\n[    1.068019] VERIFY(!RW_LOCK_HELD(&l->l_rwlock)) failed\r\n[    1.068077] PANIC at zap.c:407:zap_leaf_evict_sync()\r\n[    1.068113] Showing stack for process 67625\r\n[    1.068116] CPU: 0 PID: 67625 Comm: touch Tainted: P           OE   4.4.14-1.el6.nutanix.10272016.x86_64 #1\r\n[    1.068117] Hardware name: Nutanix AHV, BIOS seabios-1.7.5-11.el6 04/01/2014\r\n[    1.068122]  0000000000000000 ffff88015312b2a8 ffffffff81319ae3 0000000000000001\r\n[    1.068125]  0000000100480b8b ffff88015312b2f8 ffffffffa0b5a9c0 ffff88015312b2b8\r\n[    1.068127]  ffffffffa09918c4 ffff88015312b458 ffffffffa0991aeb 0000000000000040\r\n[    1.068129] Call Trace:\r\n[    1.068136]  [<ffffffff81319ae3>] dump_stack+0x67/0x94\r\n[    1.068146]  [<ffffffffa09918c4>] spl_dumpstack+0x44/0x50 [spl]\r\n[    1.068150]  [<ffffffffa0991aeb>] spl_panic+0xcb/0xe0 [spl]\r\n[    1.068153]  [<ffffffff8132a483>] ? __sg_free_table+0x63/0x90\r\n[    1.068157]  [<ffffffff811e447e>] ? kmem_cache_free+0x1ee/0x210\r\n[    1.068160]  [<ffffffffa098d477>] ? spl_kmem_cache_free+0x117/0x140 [spl]\r\n[    1.068200]  [<ffffffffa0a46ecc>] ? arc_hdr_destroy+0x17c/0x1d0 [zfs]\r\n[    1.068231]  [<ffffffffa0acf457>] zap_leaf_evict_sync+0x57/0x60 [zfs]\r\n[    1.068248]  [<ffffffffa0a4d575>] dbuf_evict_user+0x45/0x70 [zfs]\r\n[    1.068265]  [<ffffffffa0a4f95f>] dbuf_destroy+0x4f/0x330 [zfs]\r\n[    1.068282]  [<ffffffffa0a4f561>] dbuf_rele_and_unlock+0x221/0x3e0 [zfs]\r\n[    1.068313]  [<ffffffffa0ad514f>] ? zap_lockdir+0x7f/0xa0 [zfs]\r\n[    1.068344]  [<ffffffffa0ad15e6>] ? zap_grow_ptrtbl+0x186/0x1a0 [zfs]\r\n[    1.068361]  [<ffffffffa0a4f900>] dbuf_rele+0x40/0x50 [zfs]\r\n[    1.068394]  [<ffffffffa0a4fd1e>] dmu_buf_rele+0xe/0x10 [zfs]\r\n[    1.068427]  [<ffffffffa0acf3dd>] zap_put_leaf+0x3d/0x60 [zfs]\r\n[    1.068460]  [<ffffffffa0ad16c7>] zap_put_leaf_maybe_grow_ptrtbl+0xc7/0x130 [zfs]\r\n[    1.068492]  [<ffffffffa0ad1be8>] fzap_add_cd+0xd8/0x130 [zfs]\r\n[    1.068541]  [<ffffffffa0ad4ce4>] mzap_upgrade+0x194/0x210 [zfs]\r\n[    1.068593]  [<ffffffffa0ad4fba>] zap_lockdir_impl+0x25a/0x370 [zfs]\r\n[    1.068628]  [<ffffffffa0ad514f>] zap_lockdir+0x7f/0xa0 [zfs]\r\n[    1.068664]  [<ffffffffa0ad695b>] zap_add+0x5b/0xa0 [zfs]\r\n[    1.068668]  [<ffffffff810ca871>] ? __raw_callee_save___pv_queued_spin_unlock+0x11/0x20\r\n[    1.068703]  [<ffffffffa0adfcbf>] zfs_link_create+0x37f/0x520 [zfs]\r\n[    1.068761]  [<ffffffffa0b00b2a>] zfs_create+0x62a/0x810 [zfs]\r\n[    1.068764]  [<ffffffff811e76f6>] ? __kmalloc_node+0x1f6/0x2b0\r\n[    1.068798]  [<ffffffffa0b19bf2>] zpl_create+0xb2/0x160 [zfs]\r\n[    1.068802]  [<ffffffff81210424>] vfs_create+0xd4/0x100\r\n[    1.068804]  [<ffffffff8120dc4d>] ? lookup_real+0x1d/0x60\r\n[    1.068806]  [<ffffffff812111e3>] lookup_open+0x173/0x1a0\r\n[    1.068808]  [<ffffffff812138d9>] do_last+0x299/0x760\r\n[    1.068811]  [<ffffffff812056d7>] ? get_empty_filp+0xd7/0x1c0\r\n[    1.068813]  [<ffffffff81213e1c>] path_openat+0x7c/0x140\r\n[    1.068832]  [<ffffffff811b53c2>] ? __pte_alloc+0xe2/0x190\r\n[    1.068834]  [<ffffffff81213f65>] do_filp_open+0x85/0xe0\r\n[    1.068836]  [<ffffffff8120eade>] ? getname_flags+0xce/0x1f0\r\n[    1.068838]  [<ffffffff8120311a>] do_sys_open+0x11a/0x220\r\n[    1.068842]  [<ffffffff81003513>] ? syscall_trace_enter_phase1+0x133/0x150\r\n[    1.068844]  [<ffffffff8120325e>] SyS_open+0x1e/0x20\r\n[    1.068850]  [<ffffffff816cf76e>] entry_SYSCALL_64_fastpath+0x12/0x71\r\n-- snip --\r\n\r\n### Describe how to reproduce the problem\r\n\r\nThe following are the steps\\:\r\n- Create zfs dataset with casesensitivity=mixed\r\n- Run the above listed code.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7011/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6649", "title": "\"zpool export <zpoolname>\" on a faulted zpool hangs and blocks other zpool commands after that", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | centos\r\nDistribution Version    | CentOS release 6.8 (Final)\r\nLinux Kernel                 | 4.4.14-1.el6\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.1\r\nSPL Version                  | 0.7.1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nHad a zpool (with failmode=wait) has entered a degraded/faulted state due to IO failures.\r\nIssued a \"zpool export\" and that blocked as below :\r\n\r\ncrash> bt 0xffff8801e79e9540\r\nPID: 5478   TASK: ffff8801e79e9540  CPU: 1   COMMAND: \"zpool\"\r\n #0 [ffff88032e1a7bf0] __schedule at ffffffff816cb514\r\n #1 [ffff88032e1a7ca0] schedule at ffffffff816cbc10\r\n #2 [ffff88032e1a7cc0] cv_wait_common at ffffffffa07cf845 [spl]\r\n #3 [ffff88032e1a7d40] __cv_wait at ffffffffa07cf8d5 [spl]\r\n #4 [ffff88032e1a7d50] txg_wait_synced at ffffffffa08f3919 [zfs]\r\n #5 [ffff88032e1a7da0] spa_export_common at ffffffffa08e3dc0 [zfs]\r\n #6 [ffff88032e1a7e00] spa_export at ffffffffa08e407b [zfs]\r\n #7 [ffff88032e1a7e10] zfs_ioc_pool_export at ffffffffa0924d7f [zfs]\r\n #8 [ffff88032e1a7e40] zfsdev_ioctl at ffffffffa09277d4 [zfs]\r\n #9 [ffff88032e1a7eb0] do_vfs_ioctl at ffffffff81216072\r\n#10 [ffff88032e1a7f00] sys_ioctl at ffffffff81216402\r\n#11 [ffff88032e1a7f50] entry_SYSCALL_64_fastpath at ffffffff816cf76e\r\n    RIP: 00007f40aaff1a77  RSP: 00007fff29addca8  RFLAGS: 00000246\r\n    RAX: ffffffffffffffda  RBX: 0000000000c491a0  RCX: 00007f40aaff1a77\r\n    RDX: 00007fff29addcc0  RSI: 0000000000005a03  RDI: 0000000000000003\r\n    RBP: 00007fff29adda70   R8: 6338383337336261   R9: 3566353033323238\r\n    R10: 00007fff29adda30  R11: 0000000000000246  R12: 0000000000000006\r\n    R13: 00007fff29addb50  R14: 0000000000000000  R15: 0000000000000000\r\n    ORIG_RAX: 0000000000000010  CS: 0033  SS: 002b\r\n\r\nUnfortunately, the spa_sync() will not complete because, the vdisk is faulted. And until\r\na \"zpool online\" is issued, it will not progress. However, the \"zpool export\" is holding the\r\nspa_namespace_lock in WRITE mode and hence other command will block as below :\r\n\r\ncrash> bt 5582\r\nPID: 5582   TASK: ffff88008ffc5500  CPU: 2   COMMAND: \"zpool\"\r\n #0 [ffff8802a7e1fb80] __schedule at ffffffff816cb514\r\n #1 [ffff8802a7e1fc30] schedule at ffffffff816cbc10\r\n #2 [ffff8802a7e1fc50] schedule_preempt_disabled at ffffffff816cbe4e\r\n #3 [ffff8802a7e1fc60] __mutex_lock_slowpath at ffffffff816cd440\r\n #4 [ffff8802a7e1fd00] mutex_lock at ffffffff816cd4f3\r\n #5 [ffff8802a7e1fd20] spa_open_common at ffffffffa08e64a3 [zfs]\r\n #6 [ffff8802a7e1fda0] spa_get_stats at ffffffffa08e6909 [zfs]\r\n #7 [ffff8802a7e1fe00] zfs_ioc_pool_stats at ffffffffa0924c11 [zfs]\r\n #8 [ffff8802a7e1fe40] zfsdev_ioctl at ffffffffa09277d4 [zfs]\r\n #9 [ffff8802a7e1feb0] do_vfs_ioctl at ffffffff81216072\r\n#10 [ffff8802a7e1ff00] sys_ioctl at ffffffff81216402\r\n#11 [ffff8802a7e1ff50] entry_SYSCALL_64_fastpath at ffffffff816cf76e\r\n    RIP: 00007fd4730c5a77  RSP: 00007fffa8889408  RFLAGS: 00000202\r\n    RAX: ffffffffffffffda  RBX: 00007fd473373120  RCX: 00007fd4730c5a77\r\n    RDX: 00007fffa8889430  RSI: 0000000000005a05  RDI: 0000000000000004\r\n    RBP: 0000000000772f80   R8: 0000000000000008   R9: 0000000001e00000\r\n    R10: 00007fffa8889190  R11: 0000000000000202  R12: 0000000000020090\r\n    R13: 0000000000772f70  R14: 0000000000010000  R15: 00007fd473373120\r\n    ORIG_RAX: 0000000000000010  CS: 0033  SS: 002b\r\n\r\nAnd hence, all zpool commands will block on spa_namespace_lock.\r\n\r\nProbably, it is better for spa_export_common() to wait on txg_wait_synced() without holding the spa_namespace_lock.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nHere are the steps to reproduce the problem :\r\n- inject IO faults using zinject \r\n   zinject -a -d /dev/sdz -e io zpool-1\r\n- Trigger some IO to the zpool\r\n- Wait for the zpool to get into degraded state\r\n- Try to export the zpool : zpool export zpool-1\r\n   This command will hang.\r\nAt this point all other zpool command (e.g.zpool list, zpool status) will hang.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6649/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6530", "title": "Wrong TAG used by by mmp_write_uberblock() for spa_config_enter()", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Centos\r\nDistribution Version    | \r\nLinux Kernel                 | 4.4.14-1.el6\r\nArchitecture                 |  x86_64\r\nZFS Version                  |  0.7.1-1\r\nSPL Version                  | 0.7.1-3_gce8260d\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nSaw the following panic with debug bits while running zfs-testsuite \r\n```\r\n-- snip --\r\n[    2.668699] Kernel panic - not syncing: No such hold ffffffffa0a017a1 on refcount ffff88028da65a38\r\n[    2.668892] CPU: 0 PID: 24257 Comm: z_null_int Tainted: P           OE   4.4.14-1.el6.nutanix.10272016.x86_64 #1\r\n[    2.669018] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 09/21/2015\r\n[    2.669161]  0000000000000000 ffff88027d27fb88 ffffffff81319ae3 00000000ffffffff\r\n[    2.669303]  0000000000000038 ffffffffa0a45d08 0000000000000001 ffff88027d27fc08\r\n[    2.669403]  ffffffff811825c9 0000000000000018 ffff88027d27fc18 ffff88027d27fbb8\r\n[    2.669502] Call Trace:\r\n[    2.669547]  [<ffffffff81319ae3>] dump_stack+0x67/0x94\r\n[    2.669607]  [<ffffffff811825c9>] panic+0xc1/0x219\r\n[    2.669664]  [<ffffffff816cd4de>] ? mutex_lock+0xe/0x40\r\n[    2.669793]  [<ffffffffa09180ee>] refcount_remove_many+0x20e/0x240 [zfs]\r\n[    2.669897]  [<ffffffffa0918136>] refcount_remove+0x16/0x20 [zfs]\r\n[    2.669999]  [<ffffffffa093843f>] spa_config_exit+0xaf/0x1e0 [zfs]\r\n[    2.670099]  [<ffffffffa09153e5>] mmp_write_done+0xc5/0x1a0 [zfs]\r\n[    2.670207]  [<ffffffffa09b2bb9>] zio_done+0x709/0x16b0 [zfs]\r\n[    2.670272]  [<ffffffff816cbba9>] ? _cond_resched+0x9/0x30\r\n[    2.670336]  [<ffffffff816ccf19>] ? mutex_unlock+0x9/0x20\r\n[    2.670398]  [<ffffffff816ccf19>] ? mutex_unlock+0x9/0x20\r\n[    2.670519]  [<ffffffffa09aade5>] ? zio_wait_for_children+0xc5/0x180 [zfs]\r\n[    2.670637]  [<ffffffffa09ad11d>] zio_execute+0xed/0x2e0 [zfs]\r\n[    2.670711]  [<ffffffffa07cd76c>] taskq_thread+0x2bc/0x4b0 [spl]\r\n[    2.670797]  [<ffffffff810ad830>] ? try_to_wake_up+0x270/0x270\r\n[    2.670863]  [<ffffffff810a2462>] ? __kthread_parkme+0x12/0x80\r\n[    2.670930]  [<ffffffffa07cd4b0>] ? param_set_taskq_kick+0xf0/0xf0 [spl]\r\n[    2.671000]  [<ffffffff810a263c>] kthread+0xcc/0xf0\r\n[    2.671058]  [<ffffffff810aa5e1>] ? finish_task_switch+0x21/0x270\r\n[    2.671124]  [<ffffffff810abd4e>] ? schedule_tail+0x1e/0xc0\r\n[    2.671186]  [<ffffffff810a2570>] ? kthread_freezable_should_stop+0x70/0x70\r\n[    2.671258]  [<ffffffff816cfacf>] ret_from_fork+0x3f/0x70\r\n[    2.671320]  [<ffffffff810a2570>] ? kthread_freezable_should_stop+0x70/0x70\r\n-- snip --\r\n```\r\n\r\n### Describe how to reproduce the problem\r\nRun the  mmp setup test of zfs-testsuite.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6530/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/21df134f4cb1c1e05eb89992b71573843df62b27", "message": "zio_dva_throttle_done() should allow zinjected ZIO\n\nIf fault injection is enabled, the ZIO_FLAG_IO_RETRY could be set by\r\nzio_handle_device_injection() to generate the FMA events and update\r\nstats. Hence, ignore the flag and process such zios.\r\n\r\nA better fix would be to add another flag in the zio_t to indicate that\r\nthe zio is failed because of a zinject rule. However, considering the\r\nfact that we do this in debug bits, we could do with the crude check\r\nusing the global flag zio_injection_enabled which is set to 1 when\r\nzinject records are added.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Sanjeev Bagewadi <sanjeev.bagewadi@gmail.com>\r\nCloses #6383 \r\nCloses #6384"}], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/7054", "title": "Handle zap_add() failures in \"casesensitivity=mixed\" mode.", "body": "With \"casesensitivity=mixed\", zap_add() could fail when the number of\r\nfiles/directories with the same name (varying in case) exceed the\r\ncapacity of the leaf node of a Fatzap. This results in a ASSERT()\r\nfailure as zfs_link_create() does not expect zap_add() to fail. The fix\r\nis to handle these failures and rollback the transactions.\r\n\r\nSigned-off-by: Sanjeev Bagewadi <sanjeev.bagewadi@gmail.com>\r\n\r\n<!--- Provide a general summary of your changes in the Title above -->\r\n\r\n<!---\r\nDocumentation on ZFS Buildbot options can be found at\r\nhttps://github.com/zfsonlinux/zfs/wiki/Buildbot-Options\r\n-->\r\n\r\n### Description\r\n<!--- Describe your changes in detail -->\r\nFor a dataset with \"casesensitivity=mixed\", when a large number of files/directories\r\nwith same name (varying only in case e.g: ABCD, ABCd, ABcD and so on) are created\r\nzap_add() could fail. With mixed mode zap_add() normalises the names before the hash\r\nis computed. And all the names would generate the same hash and land in the same leaf.\r\nWhen the number of entries exceed the capacity of the leaf-block, zap_add() tries to split\r\nthe leaf-block which fails as well and zap_add() fails. This trips an ASSERT in zfs_link_create()\r\nas it does not expect zap_add() to fail.\r\n\r\nThe fix does the following :\r\n- fzap_add_cd() : Handle the case when zap_expand_leaf() fails with ENOSPC and bailout\r\n   without calling zap_put_leaf_maybe_grow_ptrtbl(). \r\n- zap_add_impl() : When adding to a micro-zap check if the total number of entries\r\n  with colliding/same hash value can fit into fatzap-leaf-block. This is important because, if/when\r\n  the microzap needs to be upgraded to fatzap, all the entries with the same hash would need to\r\n  fit into the same leaf-block (16K). If the number of such entries donot fit fail the zap_add().\r\n  \r\n   The routine mze_canfit_fzap_leaf() today assumes the MZAP_NAME_LEN for every entry.\r\n   This is erring on the safer side but, ends up accommodating lesser number (127) of entries\r\n    with same hash value in microzap. We could find out the size of name of every mze and that\r\n    would be accurate. But, it is expensive to compute the length every time. Alternatively, we\r\n    could compute the length of each entry and cache it. I felt that the amount of code needed\r\n    for this is not worth the gain. I am open to changing it if necessary.\r\n\r\n- zfs_link_create() : Move the call to zap_add() to the beginning and in case of a failure\r\n  return. This ensures that we can bailout easily before making any other modifications to\r\n  the parent-zap or the child-dnode. Keeps the code simpler.\r\n- ZPL interfaces (zfs_create(), zfs_mkdir(), zfs_symlink()) : Handle the failure of zfs_link_create()\r\n  and rollback the operation.\r\n\r\nWith these changes a call to create a file could fail with ENOSPC. Not the best error value.\r\nBut, this is the closest I found. Any alternate suggestions are welcome.\r\n\r\n### Motivation and Context\r\n<!--- Why is this change required? What problem does it solve? -->\r\n<!--- If it fixes an open issue, please link to the issue here. -->\r\n\r\nWith \"casesensitivity=mixed\" it is easy to panic the node with a simple test case\r\nas described in https://github.com/zfsonlinux/zfs/issues/7011\r\n\r\n### How Has This Been Tested?\r\n<!--- Please describe in detail how you tested your changes. -->\r\n<!--- Include details of your testing environment, and the tests you ran to -->\r\n<!--- see how your change affects other areas of the code, etc. -->\r\n<!--- If your change is a performance enhancement, please provide benchmarks here. -->\r\nThe following tests were run : \r\n- zfs-testsuite\r\n- ztest\r\n- Unit-test described in the #7011 \r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [ ] I have updated the documentation accordingly.\r\n- [x] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [x] All new and existing tests passed.\r\n- [x] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "woffs": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7003", "title": "autoreplace = on, but spare not automatically activated on drive error", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian\r\nDistribution Version    | Stretch (9)\r\nLinux Kernel                 | 4.9.51\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.3\r\nSPL Version                  | 0.7.3\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n \r\nA disk was failing, zed reported errors ...\r\n\r\n```\r\nZFS has detected that a device was removed.\r\n\r\n impact: Fault tolerance of the pool may be compromised.\r\n    eid: 71656\r\n  class: statechange\r\n  state: REMOVED\r\n   host: inferno\r\n   time: 2017-12-29 19:57:19+0100\r\n  vpath: /dev/disk/by-vdev/E2-part1\r\n  vguid: 0x8F23CA44FDEBE82C\r\n   pool: 0x83BBC476EFE065A2\r\n```\r\n\r\n```\r\nThe number of I/O errors associated with a ZFS device exceeded\r\nacceptable levels. ZFS has marked the device as faulted.\r\n\r\n impact: Fault tolerance of the pool may be compromised.\r\n    eid: 71662\r\n  class: statechange\r\n  state: FAULTED\r\n   host: inferno\r\n   time: 2017-12-29 19:57:19+0100\r\n  vpath: /dev/disk/by-vdev/E2-part1\r\n  vguid: 0x8F23CA44FDEBE82C\r\n   pool: 0x83BBC476EFE065A2\r\n```\r\n\r\n... but the spare was not activated automatically, although the autoreplace property was set to `on`.\r\n\r\n```\r\n        inferno# zpool status\r\n  [...]\r\n                pool: torx\r\n         state: DEGRADED\r\n        status: One or more devices are faulted in response to persistent errors.\r\n                Sufficient replicas exist for the pool to continue functioning in a\r\n                degraded state.\r\n        action: Replace the faulted device, or use 'zpool clear' to mark the device\r\n                repaired.\r\n                scan: scrub repaired 0B in 188h32m with 0 errors on Sun Dec 17 20:56:54 2017\r\n        config:\r\n\r\n                NAME        STATE     READ WRITE CKSUM\r\n                torx        DEGRADED     0     0     0\r\n                        raidz2-0  DEGRADED     0     0     0\r\n                                E0      ONLINE       0     0     0\r\n                                E1      ONLINE       0     0     0\r\n                                E2      FAULTED      0     0     0  too many errors\r\n                                E3      ONLINE       0     0     0\r\n                                E4      ONLINE       0     0     0\r\n                                E5      ONLINE       0     0     0\r\n                                E6      ONLINE       0     0     0\r\n                                E7      ONLINE       0     0     0\r\n                                E8      ONLINE       0     0     0\r\n                                E9      ONLINE       0     0     0\r\n                spares\r\n                        EA        AVAIL\r\n\r\n        errors: No known data errors\r\n```\r\n\r\nAfter manually issuing `zpool replace torx E2 EA` the resilver to the spare started.\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7003/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6920", "title": "zfs diff sometimes very slow", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian\r\nDistribution Version    | 9 (Stretch)\r\nLinux Kernel                 | 4.9.51\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.3\r\nSPL Version                  | 0.7.3\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\n`zfs diff -HF pool/snap1 pool/snap2` normally is moderate fast (some minutes), but sometimes takes very long (several days) while producing very little but constant disk I/O and hogging just 1 CPU. It takes place on datasets with many files, but only sometimes and unpredictable.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nThe datasets hold backups and get filled via rsync daily and are snapshotted daily. Then `zfs diff` is run between the snapshots to get file lists of changed files.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\nperf top -U\r\n```\r\nSamples: 18K of event 'cycles', Event count (approx.): 8972483612\r\nOverhead  Shared O  Symbol\r\n  20.73%  [kernel]  [k] zap_leaf_lookup_closest\r\n  10.26%  [kernel]  [k] memmove\r\n   9.72%  [kernel]  [k] zap_leaf_array_read\r\n   7.43%  [kernel]  [k] up_read\r\n   7.43%  [kernel]  [k] down_read\r\n   5.91%  [kernel]  [k] fzap_cursor_retrieve\r\n   3.22%  [kernel]  [k] zap_entry_read\r\n```\r\n\r\n$ egrep ^(ar)?c|^size|^p\\> /proc/spl/kstat/zfs/arcstats\r\n```\r\np                               4    25342114344\r\nc                               4    50688182272\r\nc_min                           4    6442450944\r\nc_max                           4    50688182272\r\nsize                            4    50506368440\r\ncompressed_size                 4    30213347328\r\narc_no_grow                     4    0\r\narc_tempreserve                 4    0\r\narc_loaned_bytes                4    0\r\narc_prune                       4    376973175\r\narc_meta_used                   4    23148165448\r\narc_meta_limit                  4    38016136704\r\narc_dnode_limit                 4    6442450944\r\narc_meta_max                    4    28417261944\r\narc_meta_min                    4    6442450944\r\narc_need_free                   4    0\r\narc_sys_free                    4    1584005696\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6920/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "krichter722": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7002", "title": "\"VERIFY3(range_tree_space(rt) == space) failed\" after I/O freeze", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 17.10\r\nLinux Kernel                 | 4.13.0-21-generic\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.7.0-227_g823d48bfb\r\nSPL Version                  | 0.7.0-22_gc9821f1c\r\n\r\n### Describe the problem you're observing\r\nMy pool consisting of 1 HDD vdev and 1 SSD cache and 1 SSD log device experienced an I/O freeze under heavy load including heavy dedup action (parallel checkout and building of Firefox on docker images) where all commands doing I/O on the pool switched to uninterruptible state and no I/O occured anymore according to `iotop`.\r\n\r\nAfter starting the machine again I'm no longer able to import the pool because the `zpool import` command never returns and after a few seconds of reading a few 100 MB the I/O stops and the stack below is printed in `dmesg`.\r\n\r\nA readonly import is possible. `zfs set mountpoint=none data/docker` fails due to `internal error: out of memory` immediately without any noticable memory issues.\r\n\r\n### Describe how to reproduce the problem\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nThe I/O freeze was caused by\r\n\r\n```\r\n[27969.280956] VERIFY3(range_tree_space(rt) == space) failed (6922371072 == 6922383360)\r\n[27969.280960] PANIC at space_map.c:127:space_map_load()\r\n[27969.280961] Showing stack for process 13639\r\n[27969.280963] CPU: 5 PID: 13639 Comm: z_wr_iss Tainted: P        W  OE   4.13.0-21-generic #24-Ubuntu\r\n[27969.280964] Hardware name: LENOVO 20221/INVALID, BIOS 71CN51WW(V1.21) 07/12/2013\r\n[27969.280964] Call Trace:\r\n[27969.280970]  dump_stack+0x63/0x8b\r\n[27969.280979]  spl_dumpstack+0x42/0x50 [spl]\r\n[27969.280982]  spl_panic+0xc8/0x110 [spl]\r\n[27969.280985]  ? kmem_cache_free+0x197/0x1c0\r\n[27969.280988]  ? avl_add+0x65/0xb0 [zavl]\r\n[27969.281027]  ? rt_avl_add+0x11/0x20 [zfs]\r\n[27969.281054]  ? range_tree_add_impl+0x2f5/0x440 [zfs]\r\n[27969.281078]  ? dnode_rele+0x39/0x40 [zfs]\r\n[27969.281108]  space_map_load+0x470/0x4f0 [zfs]\r\n[27969.281109]  ? avl_nearest+0x2b/0x30 [zavl]\r\n[27969.281136]  metaslab_load+0x36/0xf0 [zfs]\r\n[27969.281162]  metaslab_activate+0x93/0xc0 [zfs]\r\n[27969.281186]  metaslab_alloc+0x4b9/0x1170 [zfs]\r\n[27969.281217]  zio_dva_allocate+0xac/0x630 [zfs]\r\n[27969.281245]  ? zio_execute+0x8a/0xf0 [zfs]\r\n[27969.281274]  ? vdev_config_sync+0x180/0x180 [zfs]\r\n[27969.281301]  ? vdev_mirror_io_start+0xa4/0x180 [zfs]\r\n[27969.281305]  ? tsd_hash_search.isra.3+0x47/0xa0 [spl]\r\n[27969.281308]  ? tsd_get_by_thread+0x2e/0x40 [spl]\r\n[27969.281311]  ? taskq_member+0x18/0x30 [spl]\r\n[27969.281340]  zio_execute+0x8a/0xf0 [zfs]\r\n[27969.281343]  taskq_thread+0x2aa/0x4d0 [spl]\r\n[27969.281345]  ? wake_up_q+0x80/0x80\r\n[27969.281373]  ? zio_reexecute+0x3e0/0x3e0 [zfs]\r\n[27969.281375]  kthread+0x125/0x140\r\n[27969.281378]  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\n[27969.281379]  ? kthread_create_on_node+0x70/0x70\r\n[27969.281382]  ret_from_fork+0x25/0x30\r\n```\r\nwhich I captured before having to shutdown the machine with the power button. After every reboot the import fails due to\r\n\r\n```\r\n[  274.685568]  dump_stack+0x63/0x8b\r\n[  274.685575]  spl_dumpstack+0x42/0x50 [spl]\r\n[  274.685578]  spl_panic+0xc8/0x110 [spl]\r\n[  274.685581]  ? kmem_cache_free+0x197/0x1c0\r\n[  274.685583]  ? avl_add+0x65/0xb0 [zavl]\r\n[  274.685619]  ? rt_avl_add+0x11/0x20 [zfs]\r\n[  274.685645]  ? range_tree_add_impl+0x2f5/0x440 [zfs]\r\n[  274.685667]  ? dnode_rele+0x39/0x40 [zfs]\r\n[  274.685694]  space_map_load+0x470/0x4f0 [zfs]\r\n[  274.685720]  metaslab_load+0x36/0xf0 [zfs]\r\n[  274.685743]  metaslab_activate+0x93/0xc0 [zfs]\r\n[  274.685766]  metaslab_alloc+0x4b9/0x1170 [zfs]\r\n[  274.685794]  zio_dva_allocate+0xac/0x630 [zfs]\r\n[  274.685795]  ? mutex_lock+0x12/0x40\r\n[  274.685799]  ? tsd_hash_search.isra.3+0x47/0xa0 [spl]\r\n[  274.685802]  ? tsd_get_by_thread+0x2e/0x40 [spl]\r\n[  274.685805]  ? taskq_member+0x18/0x30 [spl]\r\n[  274.685832]  zio_execute+0x8a/0xf0 [zfs]\r\n[  274.685835]  taskq_thread+0x2aa/0x4d0 [spl]\r\n[  274.685837]  ? wake_up_q+0x80/0x80\r\n[  274.685864]  ? zio_reexecute+0x3e0/0x3e0 [zfs]\r\n[  274.685865]  kthread+0x125/0x140\r\n[  274.685869]  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\n[  274.685870]  ? kthread_create_on_node+0x70/0x70\r\n[  274.685871]  ret_from_fork+0x25/0x30\r\n```\r\nalso after a successful readonly import.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/7002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6836", "title": "Add option to wait for zpool scrub/run zpool scrub in foreground until completion", "body": "### Describe the problem you're observing\r\nThere's no way to wait for `zpool scrub` by running the process in the foreground and make it return only after the scrub has been completed. This makes the use of the command unnecessarily difficult in maintenance scripts (e.g. cron scripts) for which one might want to generate a result email for and send it.\r\n\r\nI know about ways to achieve that with zed or parse the output of `zpool status`, but I consider them workarounds for this easier and more intuitive approach.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6836/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6808", "title": "unable to handle kernel NULL pointer dereference at zfs_ace_fuid_size+0x5/0x60 [zfs]", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 17.10\r\nLinux Kernel                 | 4.13.0-16-generic\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.7.0-152_gf4ae39a19\r\nSPL Version                  | 0.7.0-20_g35a44fc\r\n\r\n### Describe the problem you're observing\r\nIntensive I/O during deletion of several 10000 files in `nautilus` is interrupted and doesn't continue while `iotop` shows 0 read/write activity.\r\n\r\n### Describe how to reproduce the problem\r\nProduce a lot of I/O on a pool with 1 HDD partition and 1 SSD cache and 1 SSD log partition and wait for the issue to happen (happens randomly).\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n`dmesg` contains\r\n\r\n```\r\n[20894.125706] BUG: unable to handle kernel NULL pointer dereference at           (null)\r\n[20894.125781] IP: zfs_ace_fuid_size+0x5/0x60 [zfs]\r\n[20894.125797] PGD 0 \r\n[20894.125798] P4D 0 \r\n\r\n[20894.125821] Oops: 0000 [#1] SMP\r\n[20894.125833] Modules linked in: rfcomm msr veth nf_conntrack_netlink nfnetlink xfrm_user xfrm_algo xt_CHECKSUM iptable_mangle xt_addrtype ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_nat_ipv4 nf_conntrack_ipv4 nf_defrag_ipv4 xt_conntrack nf_nat nf_conntrack libcrc32c br_netfilter ipt_REJECT nf_reject_ipv4 xt_tcpudp bridge stp llc ebtable_filter ebtables ip6table_filter ip6_tables snd_hda_codec_hdmi snd_hda_codec_realtek snd_hda_codec_generic snd_hda_intel snd_hda_codec snd_hda_core snd_hwdep snd_pcm snd_timer snd soundcore iptable_filter pci_stub vboxpci(OE) vboxnetadp(OE) vboxnetflt(OE) vboxdrv(OE) bnep bbswitch(OE) cdc_ether usbnet r8152 rtsx_usb_ms memstick btusb btrtl btbcm btintel bluetooth ecdh_generic binfmt_misc hid_multitouch uvcvideo videobuf2_vmalloc videobuf2_memops videobuf2_v4l2\r\n[20894.126055]  videobuf2_core videodev media zfs(POE) zunicode(POE) zavl(POE) icp(POE) nls_iso8859_1 zcommon(POE) znvpair(POE) spl(OE) intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd glue_helper cryptd intel_cstate intel_rapl_perf joydev input_leds serio_raw arc4 iwldvm ideapad_laptop sparse_keymap wmi mac80211 iwlwifi cfg80211 mac_hid lpc_ich mei_me mei shpchp ib_iser rdma_cm iw_cm ib_cm ib_core iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi coda parport_pc ppdev lp parport sunrpc ip_tables x_tables autofs4 btrfs xor raid6_pq rtsx_usb_sdmmc rtsx_usb hid_generic usbhid hid psmouse r8169 mii i915 video ahci i2c_algo_bit libahci drm_kms_helper syscopyarea sysfillrect sysimgblt\r\n[20894.126284]  fb_sys_fops drm [last unloaded: nvidia]\r\n[20894.127431] CPU: 1 PID: 17157 Comm: pool Tainted: P           OE   4.13.0-16-generic #19-Ubuntu\r\n[20894.128580] Hardware name: LENOVO 20221/INVALID, BIOS 71CN51WW(V1.21) 07/12/2013\r\n[20894.129729] task: ffff9a84c1a69740 task.stack: ffffa874083f4000\r\n[20894.130840] RIP: 0010:zfs_ace_fuid_size+0x5/0x60 [zfs]\r\n[20894.131906] RSP: 0018:ffffa874083f7bc8 EFLAGS: 00010293\r\n[20894.133007] RAX: ffffffffc0f64640 RBX: 0000000000000000 RCX: ffffa874083f7c5c\r\n[20894.134082] RDX: ffffa874083f7c70 RSI: 0000000000000000 RDI: 0000000000000000\r\n[20894.135145] RBP: ffffa874083f7c10 R08: ffffa874083f7c5a R09: ffffa874083f7c58\r\n[20894.136241] R10: ffff9a85f7ac1340 R11: ffff9a829f85de00 R12: ffff9a829f85de00\r\n[20894.137301] R13: ffff9a828a1e8e40 R14: ffffa874083f7c58 R15: ffffa874083f7c5a\r\n[20894.138384] FS:  00007efd197fc700(0000) GS:ffff9a86bf240000(0000) knlGS:0000000000000000\r\n[20894.139467] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[20894.140558] CR2: 0000000000000000 CR3: 000000038bbe1000 CR4: 00000000001406e0\r\n[20894.141669] Call Trace:\r\n[20894.142772]  ? zfs_acl_next_ace+0x5b/0x140 [zfs]\r\n[20894.143849]  zfs_zaccess_aces_check+0xd5/0x380 [zfs]\r\n[20894.144939]  zfs_zaccess_common+0xda/0x1f0 [zfs]\r\n[20894.145555] audit: type=1400 audit(1509591916.615:120571): apparmor=\"ALLOWED\" operation=\"recvmsg\" profile=\"system_i2p//null-/usr/lib/jvm/java-9-openjdk-amd64/bin/java\" pid=12323 comm=55445052656365697665722031 lport=19076 family=\"inet6\" sock_type=\"dgram\" protocol=17 requested_mask=\"receive\" denied_mask=\"receive\"\r\n[20894.148270]  zfs_zaccess_delete+0xdc/0x1b0 [zfs]\r\n[20894.149364]  zfs_remove+0x277/0x940 [zfs]\r\n[20894.150400]  zpl_unlink+0x66/0xb0 [zfs]\r\n[20894.151408]  vfs_unlink+0x111/0x1c0\r\n[20894.152543]  ? apparmor_path_unlink+0x1b/0x20\r\n[20894.153635]  do_unlinkat+0x2b1/0x310\r\n[20894.154729]  SyS_unlink+0x16/0x20\r\n[20894.155752]  entry_SYSCALL_64_fastpath+0x1e/0xa9\r\n[20894.156793] RIP: 0033:0x7efde31f5bc7\r\n[20894.157793] RSP: 002b:00007efd197fb528 EFLAGS: 00000202 ORIG_RAX: 0000000000000057\r\n[20894.158840] RAX: ffffffffffffffda RBX: 000000000000002c RCX: 00007efde31f5bc7\r\n[20894.159792] RDX: 00007efd197fb580 RSI: 000055be0d7d6ce0 RDI: 00007efd1033c890\r\n[20894.160761] RBP: 0000000000000000 R08: 00007efdc8011758 R09: 00007efdc8011760\r\n[20894.161691] R10: 000055be0c8136b8 R11: 0000000000000202 R12: 00007efd197fb238\r\n[20894.162621] R13: 00007efd1001f700 R14: 00007efde59a2200 R15: 0000000000000000\r\n[20894.163563] Code: 48 89 e5 66 25 40 70 66 3d 00 10 0f 94 c1 66 3d 40 20 0f 94 c2 08 d1 75 0a 66 3d 00 40 74 04 48 89 77 08 5d c3 90 0f 1f 44 00 00 <0f> b7 07 55 48 89 e5 66 83 f8 01 76 13 83 e8 05 66 83 f8 04 48 \r\n[20894.165709] RIP: zfs_ace_fuid_size+0x5/0x60 [zfs] RSP: ffffa874083f7bc8\r\n[20894.166747] CR2: 0000000000000000\r\n[20894.171544] ---[ end trace a246b172f23abaa3 ]---\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6808/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6596", "title": "I/O processing for multiple processes stops suddenly under light load", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 17.04\r\nLinux Kernel                 | 4.10.0-33-generic\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.7.0-58_gcf7684bc8\r\nSPL Version                  | 0.7.0-12_g9df96926\r\n\r\n### Describe the problem you're observing\r\nI/O processing for multiple processes stops suddenly under light load whereas other processes aren't effected immediately, but can become affected over the time. Some processes like the VLC media player don't seem to be affected or can deal with the low streaming rate from the filesystem because of their internal proceedures. The situation recovers after approx. 1 hour if the system is left idle and programs which could cause further I/O are requested to be closed (which also takes very long time). Killing processes with SIGKILL doesn't fix the problem immediately, but seems to accelerate the recovery (not tried systematically). New processes partly suffer the same symptoms (e.g. `modinfo zfs`). The behaviour of `txg_sync` and `z_fr_iss_[n]` threads is very similar as in #5867 (the difference make the stacks and the time it takes to recover).\r\n\r\nMy pool consists of 1 1.8 TB partition on a 2TB HDD and a 170 GB cache partition on a 250 GB SSD.\r\n\r\n### Describe how to reproduce the problem\r\nNavigating in a large `git` repository like https://github.com/WebKit/webkit.git (e.g. `git reset --hard [some 1000s commit before HEAD]`) seems to start the trouble all of the times tried.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n`dmesg`:\r\n\r\n```\r\n[43019.211193] INFO: task txg_sync:13110 blocked for more than 120 seconds.\r\n[43019.211198]       Tainted: P           OE   4.10.0-33-generic #37-Ubuntu\r\n[43019.211200] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[43019.211202] txg_sync        D    0 13110      2 0x00000000\r\n[43019.211205] Call Trace:\r\n[43019.211212]  __schedule+0x233/0x6f0\r\n[43019.211214]  schedule+0x36/0x80\r\n[43019.211216]  schedule_timeout+0x22a/0x3f0\r\n[43019.211275]  ? zio_issue_async+0x12/0x20 [zfs]\r\n[43019.211323]  ? zio_nowait+0xb6/0x150 [zfs]\r\n[43019.211327]  ? ktime_get+0x41/0xb0\r\n[43019.211329]  io_schedule_timeout+0xa4/0x110\r\n[43019.211338]  cv_wait_common+0xbc/0x140 [spl]\r\n[43019.211342]  ? wake_atomic_t_function+0x60/0x60\r\n[43019.211348]  __cv_wait_io+0x18/0x20 [spl]\r\n[43019.211397]  zio_wait+0xfd/0x1b0 [zfs]\r\n[43019.211442]  dsl_pool_sync+0xb8/0x440 [zfs]\r\n[43019.211489]  spa_sync+0x42d/0xdc0 [zfs]\r\n[43019.211493]  ? __wake_up+0x44/0x50\r\n[43019.211539]  txg_sync_thread+0x2e2/0x4b0 [zfs]\r\n[43019.211587]  ? txg_quiesce_thread+0x3f0/0x3f0 [zfs]\r\n[43019.211593]  thread_generic_wrapper+0x72/0x80 [spl]\r\n[43019.211597]  kthread+0x109/0x140\r\n[43019.211602]  ? __thread_exit+0x20/0x20 [spl]\r\n[43019.211605]  ? kthread_create_on_node+0x60/0x60\r\n[43019.211608]  ret_from_fork+0x2c/0x40\r\n[43019.211660] INFO: task DOM Worker:7225 blocked for more than 120 seconds.\r\n[43019.211663]       Tainted: P           OE   4.10.0-33-generic #37-Ubuntu\r\n[43019.211665] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[43019.211667] DOM Worker      D    0  7225  17712 0x00000000\r\n[43019.211670] Call Trace:\r\n[43019.211673]  __schedule+0x233/0x6f0\r\n[43019.211674]  schedule+0x36/0x80\r\n[43019.211681]  cv_wait_common+0x128/0x140 [spl]\r\n[43019.211683]  ? wake_atomic_t_function+0x60/0x60\r\n[43019.211688]  __cv_wait+0x15/0x20 [spl]\r\n[43019.211735]  txg_wait_open+0xb8/0x100 [zfs]\r\n[43019.211774]  dmu_tx_wait+0x389/0x3a0 [zfs]\r\n[43019.211823]  zfs_create+0x317/0x870 [zfs]\r\n[43019.211871]  zpl_create+0xbf/0x170 [zfs]\r\n[43019.211875]  path_openat+0x13a1/0x14f0\r\n[43019.211877]  ? get_futex_key+0x332/0x430\r\n[43019.211879]  do_filp_open+0x91/0x100\r\n[43019.211882]  ? hashlen_string+0xa0/0xa0\r\n[43019.211884]  ? __alloc_fd+0x46/0x170\r\n[43019.211887]  do_sys_open+0x135/0x280\r\n[43019.211890]  SyS_open+0x1e/0x20\r\n[43019.211892]  entry_SYSCALL_64_fastpath+0x1e/0xad\r\n[43019.211894] RIP: 0033:0x7ff008860d8d\r\n[43019.211895] RSP: 002b:00007fefd44cc2d0 EFLAGS: 00000293 ORIG_RAX: 0000000000000002\r\n[43019.211897] RAX: ffffffffffffffda RBX: 0000000000000020 RCX: 00007ff008860d8d\r\n[43019.211898] RDX: 0000000000000180 RSI: 0000000000000641 RDI: 00007fefb1edf4c0\r\n[43019.211899] RBP: 00007ff007800210 R08: 0000000000000000 R09: 0000000000000000\r\n[43019.211900] R10: 00007fefd44cc230 R11: 0000000000000293 R12: 00007ff007800040\r\n[43019.211901] R13: 0000000000000000 R14: 00007fefc5300920 R15: 00007fefc5330000\r\n[43019.211918] INFO: task mozStorage #4:24210 blocked for more than 120 seconds.\r\n[43019.211921]       Tainted: P           OE   4.10.0-33-generic #37-Ubuntu\r\n[43019.211922] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[43019.211923] mozStorage #4   D    0 24210  17712 0x00000000\r\n[43019.211925] Call Trace:\r\n[43019.211928]  __schedule+0x233/0x6f0\r\n[43019.211929]  schedule+0x36/0x80\r\n[43019.211931]  schedule_timeout+0x22a/0x3f0\r\n[43019.211938]  ? taskq_dispatch_ent+0x55/0x170 [spl]\r\n[43019.211984]  ? zio_reexecute+0x3d0/0x3d0 [zfs]\r\n[43019.211987]  ? ktime_get+0x41/0xb0\r\n[43019.211989]  io_schedule_timeout+0xa4/0x110\r\n[43019.211995]  cv_wait_common+0xbc/0x140 [spl]\r\n[43019.211998]  ? wake_atomic_t_function+0x60/0x60\r\n[43019.212003]  __cv_wait_io+0x18/0x20 [spl]\r\n[43019.212049]  zio_wait+0xfd/0x1b0 [zfs]\r\n[43019.212095]  zil_commit.part.14+0x4d6/0x8a0 [zfs]\r\n[43019.212143]  zil_commit+0x17/0x20 [zfs]\r\n[43019.212190]  zfs_fsync+0x77/0xf0 [zfs]\r\n[43019.212236]  zpl_fsync+0x68/0xa0 [zfs]\r\n[43019.212239]  vfs_fsync_range+0x4b/0xb0\r\n[43019.212241]  do_fsync+0x3d/0x70\r\n[43019.212243]  SyS_fsync+0x10/0x20\r\n[43019.212246]  entry_SYSCALL_64_fastpath+0x1e/0xad\r\n[43019.212247] RIP: 0033:0x7efd0c83cc3d\r\n[43019.212248] RSP: 002b:00007efcc6192570 EFLAGS: 00000293 ORIG_RAX: 000000000000004a\r\n[43019.212250] RAX: ffffffffffffffda RBX: 00007efcdb5f7100 RCX: 00007efd0c83cc3d\r\n[43019.212250] RDX: 0000000000000000 RSI: 0000000000000002 RDI: 000000000000002f\r\n[43019.212251] RBP: 00007efcdb5f7148 R08: 0000000000000000 R09: 0000000000000002\r\n[43019.212252] R10: 002da21ef0031274 R11: 0000000000000293 R12: 00007efced300000\r\n[43019.212253] R13: 0000000000000000 R14: 00007efcc61926a0 R15: 00007efcc418b038\r\n[43019.212257] INFO: task git:25372 blocked for more than 120 seconds.\r\n[43019.212260]       Tainted: P           OE   4.10.0-33-generic #37-Ubuntu\r\n[43019.212261] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[43019.212263] git             D    0 25372  25236 0x00000004\r\n[43019.212265] Call Trace:\r\n[43019.212267]  __schedule+0x233/0x6f0\r\n[43019.212269]  schedule+0x36/0x80\r\n[43019.212276]  cv_wait_common+0x128/0x140 [spl]\r\n[43019.212278]  ? wake_atomic_t_function+0x60/0x60\r\n[43019.212284]  __cv_wait+0x15/0x20 [spl]\r\n[43019.212330]  txg_wait_open+0xb8/0x100 [zfs]\r\n[43019.212370]  dmu_tx_wait+0x389/0x3a0 [zfs]\r\n[43019.212408]  dmu_tx_assign+0x8b/0x490 [zfs]\r\n[43019.212454]  zfs_write+0x635/0xdf0 [zfs]\r\n[43019.212457]  ? mutex_lock+0x12/0x40\r\n[43019.212500]  ? rrw_exit+0x5a/0x150 [zfs]\r\n[43019.212503]  ? dput+0x40/0x270\r\n[43019.212550]  zpl_write_common_iovec+0x8c/0xe0 [zfs]\r\n[43019.212595]  zpl_iter_write+0xb7/0xf0 [zfs]\r\n[43019.212598]  new_sync_write+0xd5/0x130\r\n[43019.212600]  __vfs_write+0x26/0x40\r\n[43019.212601]  vfs_write+0xb5/0x1a0\r\n[43019.212603]  SyS_write+0x55/0xc0\r\n[43019.212606]  entry_SYSCALL_64_fastpath+0x1e/0xad\r\n[43019.212607] RIP: 0033:0x7f33ad0ad670\r\n[43019.212608] RSP: 002b:00007ffece76ca48 EFLAGS: 00000246 ORIG_RAX: 0000000000000001\r\n[43019.212610] RAX: ffffffffffffffda RBX: 0000561df9526c00 RCX: 00007f33ad0ad670\r\n[43019.212611] RDX: 0000000000004000 RSI: 00007ffece76cae0 RDI: 0000000000000004\r\n[43019.212612] RBP: 00007ffece770e40 R08: 00007ffece76ca70 R09: 0000561df93081b8\r\n[43019.212613] R10: 00007f33ace8fb58 R11: 0000000000000246 R12: 0000561dfa94ba80\r\n[43019.212614] R13: 0000000000000000 R14: 0000561df9526c44 R15: 0000000000008000\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6596/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6431", "title": "Add systemd setup to \"Ubuntu ZFS mountall FAQ and troubleshooting\" wiki page", "body": "### Describe the problem you're observing\r\nThe [Ubuntu ZFS mountall FAQ and troubleshooting](https://github.com/zfsonlinux/zfs/wiki/Ubuntu-ZFS-mountall-FAQ-and-troubleshooting) wiki page mentions the configuration involving the deprecated Ubuntu PPA and `upstart` which is no longer the default the init manager in 17.04 (maybe earlier). It'd be appreciated if that'd be differentiated in the wiki. I'd be appreciated as well if information for a source installation (discouraged for beginners, but still interesting) would be provided or linked.\r\n\r\nI'm looking for information\r\n\r\n  * how to deal with the built-in kernel modules (after installing from source, I'm deleting `/lib/modules/[version]/kernel/zfs` and running `depmod -a`, but that might be wrong and only work by accident).\r\n  * how to make `systemd` units depend on mountpoints (`xy.mount` only seems available after the mount happened with `zfs mount`; maybe there's a better dependency).\r\n\r\nIf information for `systemd` can be found for other OS they should be separated or be linked without too much redundancy.\r\n\r\nPlease note that this is not a support request - I'd like to see this fixed for anybody instead of just me because I think it concerns a large audience and up-to-date information make a good impression on users.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6431/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "redzhang1990": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6995", "title": "Can ZFS support numa binding?", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Redhat\r\nDistribution Version    | 7.4\r\nLinux Kernel                 | 4.11.0\r\nArchitecture                 | ARM\r\nZFS Version                  | 0.7.1\r\nSPL Version                  | 0.7.1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nCan the ZFS support or willing support numa binding?\r\n### Describe how to reproduce the problem\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\n```\r\n-->\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6995/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "fejesjoco": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6994", "title": "Documentation of ACLs should be fixed", "body": "There are issues in the manpage of zfs(8).\r\n\r\naclinherit talks about ACEs, acltype talks about ACLs, this is inconsistent.\r\n\r\naclinherit doesn't mention what kind of ACEs it's talking about. Since neither regular file permission bits not POSIX ACLs have write_acl/write_owner, this must be NFSv4. So that should be mentioned here explicitly.\r\n\r\nacltype has two values. Again this doesn't say what it's talking about and one can only guess. Does \"off\" turn off both NFSv4 and POSIX ACLs, or just POSIX? Does \"posixacl\" enable both NFSv4 and POSIX, or only POSIX? I can even read it in a way that I can either have POSIX ACLs or no ACLs, which would mean NFSv4 ACLs are not even supported under Linux.\r\n\r\nThe source code has many mentions of an aclmode property but this is not documented anywhere.\r\n\r\nSince the document talks about multiple ACL types, it might be worth mentioning if regular file permission bits work as usual or not (this is especially interesting across dataset mount boundaries).\r\n\r\nIf you can confirm these points, I can volunteer to send a PR.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6994/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dechamps": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6988", "title": "zil_itx_needcopy_bytes kstat counter is corrupted", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian\r\nDistribution Version    | Unstable\r\nLinux Kernel                 | 4.13.0-1-amd64\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.7.3-3\r\nSPL Version                  | 0.7.3-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\n```\r\n$ cat /proc/spl/kstat/zfs/zil\r\n15 1 0x01 13 624 31503034653 382758011634377\r\nname                            type data\r\nzil_commit_count                4    197902\r\nzil_commit_writer_count         4    197884\r\nzil_itx_count                   4    611431070\r\nzil_itx_indirect_count          4    0\r\nzil_itx_indirect_bytes          4    0\r\nzil_itx_copied_count            4    0\r\nzil_itx_copied_bytes            4    0\r\nzil_itx_needcopy_count          4    611266365\r\nzil_itx_needcopy_bytes          4    18446744072731425348\r\nzil_itx_metaslab_normal_count   4    0\r\nzil_itx_metaslab_normal_bytes   4    0\r\nzil_itx_metaslab_slog_count     4    1169526\r\nzil_itx_metaslab_slog_bytes     4    140983216376\r\n```\r\n\r\nThe `zil_itx_needcopy_bytes` counter is blatantly wrong - I'm pretty sure I did not write 16 exabytes of data in that pool :) Its value is quite close to `UINT64_MAX`, which suggests some kind of overflow or memory corruption.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nNot sure. However I can tell that it started after I did a system upgrade, which included the following version changes:\r\n\r\n- Kernel: 4.12 \u2192 4.13\r\n- SPL: 0.6.5 \u2192 0.7.3\r\n- ZFS: 0.6.5 \u2192 0.7.3\r\n\r\nFor this reason I suspect this might be a regression introduced between SPL/ZFS 0.6.5 and SPL/ZFS 0.7.3.\r\n\r\nThis issue might seem benign, but in my case it's really not because it prevents [Prometheus Node exporter](https://github.com/prometheus/node_exporter) from exporting ZFS metrics correctly. Here's the log message from the node exporter in an attempt to make this issue easier to search for:\r\n\r\n```\r\ntime=\"2017-12-20T22:32:39Z\" level=error msg=\"ERROR: zfs collector failed after 0.000693s: could not parse expected integer value for \\\"kstat.zfs.misc.zil.zil_itx_needcopy_bytes\\\"\" source=\"node_exporter.go:95\"\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6988/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2502562", "body": "The vdev's align is not always 512 bytes. If you use `ashift=12` it will be 4096 bytes, for example. That's the reason why this is useful.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2502562/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/374490", "body": "Hmmm\u2026 in fact this line is supposed to be in pull request #384. Seems like I seriously screwed up while doing the merges: all my pull requests show the same commits. What a mess\u2026 git is new to me, I guess this was bound to happen. There should be only one commit in this pull request: 90e1b2108f3b8fd3d2b92bdaa4775fe2321cffa3, so if you're just interested in ZVOL synchronicity, you should check it out. I'm not sure how to fix this, I guess I'll have to recreate the pull requests.\n\nFYI, in the context of #384, this comment means that maybe the discard operation should be added to the log. This is not very important since losing discard operations cannot result in data corruption.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/374490/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/374498", "body": "That's what I did, basically; the problem is, when updating my master branch from upstream I guessed it would be a good idea to also update individual pull request branches from my master branch. Alas, it was a very bad idea, because my master branch add commits from all my pull requests, so by merging master into each pull request, I merged all commits from all pull requests into each pull request, hence the mess. In the future I'll just let my pull requests alone when I'm done with them.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/374498/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/374511", "body": "I never commited anything to my master branch, just merges. I wrote the pull request's code into the appropriate pull request branches, as I should. The issue is, I was merging master into my pull requests without realizing what I was doing. The solution is to stop doing that. I just emailed Brian so that we decide what to do about the already messed up pull requests.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/374511/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ScaMar": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6985", "title": "\"space map refcount mismatch\" on never used zpool after reboot", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Ubuntu\r\nDistribution Version    |  LTS 16.04.03\r\nLinux Kernel                 |  4.4.0-104-generic\r\nArchitecture                 |  x86_64\r\nZFS Version                  |  0.6.5.6\r\nSPL Version                  |  0.6.5.6\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nHi all,\r\non my live zpool i've found the \"space map refcount mismatch\" (error? warning?).\r\nBecause the pool wasn't too big, i've copied data on an external storage, so i've destroyes the zpool then i've recreated it.\r\nI've execute \"zdb <pool>\" and it was ok. After the first reboot (no data were copied/created on zpool), i've executed \"zdb <pool>\" again and i've found the message \"space map refcount mismatch\".\r\n\r\nI've destroyed - recreated the pool several times, always i got the \"space map refcount mismatch\".\r\nEach time before reboot i've tried something like \"zfs unmount <pool>\".\r\nI think the message is related to the first zpool.cache creation.\r\nAfter i've deleted the zpool.cache, the error was not present after the reboot.\r\n\r\nSo my problem are these lines in the zdb output:\r\n```\r\nspace map refcount mismatch: expected 11 != actual 5\r\n```\r\n\r\n### Describe how to reproduce the problem\r\nInstall Ubuntu 16.04. Update it. Create a zpool. Reboot.\r\n\r\n### Questions, considerations, any suggestions?\r\nAbout such issue, i have some questions:\r\n\r\n1) Is it something i need to worry about?\r\n2) Is there a way to recalculate/rebuild space map?\r\n\r\nThere are similar issues about such message someone wrote \"It is a problem in claiming empty space\", some other wrote \"This situation may lead to data corruption\", \"Ignore it if the delta beetwen refcount and space is fixed (if not?)\".\r\nMay we have a clear/human about the consequencies of this error / warning?\r\n\r\nThe only think i know, and sincerily i don't understand a single line (my fault, i'm not a coder), this is the line of code where the counts are compared:\r\n```\r\nstatic int\r\nverify_spacemap_refcounts(spa_t *spa)\r\n{\r\n\tuint64_t expected_refcount = 0;\r\n\tuint64_t actual_refcount;\r\n\r\n\t(void) feature_get_refcount(spa,\r\n\t    &spa_feature_table[SPA_FEATURE_SPACEMAP_HISTOGRAM],\r\n\t    &expected_refcount);\r\n\tactual_refcount = get_dtl_refcount(spa->spa_root_vdev);\r\n\tactual_refcount += get_metaslab_refcount(spa->spa_root_vdev);\r\n\r\n\tif (expected_refcount != actual_refcount) {\r\n\t\t(void) printf(\"space map refcount mismatch: expected %lld != \"\r\n\t\t    \"actual %lld\\n\",\r\n\t\t    (longlong_t)expected_refcount,\r\n\t\t    (longlong_t)actual_refcount);\r\n\t\treturn (2);\r\n\t}\r\n\treturn (0);\r\n}\r\n```\r\nPlease let me know if i must worry about this, so i can evaluate other ways to achieve my personal storage:\r\n1) linux with btrfs\r\n2) OpenIndiana/FreeBSD with ZFS\r\n3) Old but stable md / lvm / xfs (i will risk the cosmic ray bitrotter...)\r\n\r\nThank you,\r\nMarco\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n\r\n\r\n[zdbout.newcreated.txt](https://github.com/zfsonlinux/zfs/files/1571863/zdbout.newcreated.txt)\r\n[zdbout.firstreboot.txt](https://github.com/zfsonlinux/zfs/files/1571864/zdbout.firstreboot.txt)\r\n```\r\nHistory for 'magazzino':\r\n2017-12-19.13:28:20 zpool create magazzino mirror /dev/disk/by-id/ata-WDC_WD40EFRX-68N32N0_WD-WCC7K0KA3UYK /dev/disk/by-id/ata-WDC_WD40EFRX-68N32N0_WD-WCC7K7NF5E20\r\n2017-12-19.13:28:21 zpool add magazzino mirror /dev/disk/by-id/ata-WDC_WD30EFRX-68EUZN0_WD-WCC4N7UUHR2X /dev/disk/by-id/ata-WDC_WD30EFRX-68EUZN0_WD-WCC4N3CHVS8X\r\n2017-12-19.13:28:22 zpool add magazzino cache /dev/disk/by-id/ata-KINGSTON_SUV400S37120G_50026B77720127CB-part5 /dev/disk/by-id/ata-KINGSTON_SUV400S37120G_50026B776B0175F0-part5\r\n2017-12-19.13:28:22 zpool add magazzino log mirror /dev/disk/by-id/ata-KINGSTON_SUV400S37120G_50026B77720127CB-part6 /dev/disk/by-id/ata-KINGSTON_SUV400S37120G_50026B776B0175F0-part6\r\n2017-12-19.13:28:22 zfs create magazzino/video\r\n2017-12-19.13:28:22 zfs create magazzino/foto\r\n2017-12-19.13:28:23 zfs create magazzino/owncloud\r\n2017-12-19.13:28:29 zpool scrub magazzino\r\n```\r\n--> deleted /etc/zfs/zpool.cache\r\n--> reboot\r\n```\r\n2017-12-19.13:31:24 zpool import -d /dev/disk/by-id -aN\r\n```\r\n--> new reboot withouth deleting zpool.cache\r\n```\r\n2017-12-19.13:37:03 zpool import -c /etc/zfs/zpool.cache -aN\r\n```\r\n--> another reboot withouth deleting zpool.cache\r\n```\r\n2017-12-19.13:41:01 zpool import -c /etc/zfs/zpool.cache -aN\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n ", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6985/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "samis": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6984", "title": "zpool property 'freeing' partially stuck", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Gentoo\r\nDistribution Version    | Profile 17.0\r\nLinux Kernel                 | 4.14.5-gentoo\r\nArchitecture                 | X86_64\r\nZFS Version                  | 0.7.0-211_g4e9b1569\r\nSPL Version                  | 0.7.0-21_ged19bcc\r\n\r\n\r\n### Describe the problem you're observing\r\nI recently decided to clean up and delete two unused sparse zvols. After this, the freeing property increased as expected and did initially decrease. However, it's almost 24 hours (and two reboots) later and the property is still reporting the exact same value. \r\n\r\nAs a test, I filled a zvol with 1G of /dev/urandom and then destroyed it. The property increased from it's original value of 14353956864 to 14605664256 but shortly afterwards the data was freed and the value was back to 14353956864. This is similar to #5808 but both the scenario and the behaviour appear to be different, as neither zvol was ever used for NFS purposes.\r\n### Describe how to reproduce the problem\r\nI have not yet reproduced this beyond the above test. I can't be certain that the freeing value was correct before, but the timing seems right for this issue.\r\n### Include any warning/errors/backtraces from the system logs\r\nSo far there have been no warnings, errors or backtraces created as a result of this problem. \r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6984/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zielony360": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6979", "title": "Increasing zfs_vdev_aggregation_limit with zvols", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian\r\nDistribution Version    | 7\r\nLinux Kernel                 | 4.0.4\r\nArchitecture                 |x86_64 \r\nZFS Version                  | 0.6.5.7\r\nSPL Version                  | 0.6.5.7\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nHello,\r\n\r\nwe have pool consisting 5 x 10-disk raidz2 group and using zvols with volblocksize=128k, shared through LIO FC target. Due to the fact that zfs_vdev_aggregation_limit is limited to 128k too, writes on singular disk are very small, like 13 kB average, what causes performance issues. Unfortunately, there is no large blocks for zvols to prevent it.\r\n\r\nMy questions are:\r\n1. Can I somehow safely increase zfs_vdev_aggregation_limit to 512 kB with existing configuration?\r\n2. If not, how can I migrate to large blocks datasets using the same pool? I mean I would like to create datasets, set recordsize=512k on them, but also raise zfs_vdev_aggregation_limit to make it sensible. For some time zvols will coexist with datasets during such migration.\r\n\r\n### Describe how to reproduce the problem\r\nCreate a pool with wide raidz2 vdevs and use zvols.\r\n\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6979/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ThomDietrich": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6975", "title": "Different sector size error message wrong or misleading", "body": "Hello everyone.\r\nThe optimal sector size error related to a device replacement is misleading or simply wrong. As you can see below, the error presented doesn't make much sense and is not helpful. What is the alleged \"optimal sector size\", how does a user need to handle that issue? I'd suggest to improve the message as did [another user already](https://github.com/zfsonlinux/zfs/pull/2427#issuecomment-321887912).\r\n\r\n----\r\n\r\nMy case - help would also be appreciated:\r\n\r\nI am in the process of replacing a hard drive when faced with this error:\r\n\r\n```shell\r\n $ sudo zpool replace tank 12849889439322109762 ata-ST2000DM006-2DM164_Z4Z90YHR\r\ncannot replace 12849889439322109762 with ata-ST2000DM006-2DM164_Z4Z90YHR: new device has a different optimal sector size; use the option '-o ashift=N' to override the optimal size\r\n\r\n $ sudo zpool replace -o ashift=12 tank 12849889439322109762 ata-ST2000DM006-2DM164_Z4Z90YHR\r\ncannot replace 12849889439322109762 with ata-ST2000DM006-2DM164_Z4Z90YHR: new device has a different optimal sector size; use the option '-o ashift=N' to override the optimal size\r\n```\r\n\r\n\r\n```shell\r\n $ sudo zpool status\r\n  pool: tank\r\n state: DEGRADED\r\nconfig:\r\n\r\n        NAME                                    STATE     READ WRITE CKSUM\r\n        tank                                    DEGRADED     0     0     0\r\n          raidz1-0                              ONLINE       0     0     0\r\n            ... REDACTED\r\n          raidz1-1                              DEGRADED     0     0     0\r\n            12849889439322109762                UNAVAIL      0     0     0  was /dev/disk/by-id/ata-SAMSUNG_HD204UI_S2H7J1CZ907120-part1\r\n            ata-SAMSUNG_HD204UI_S2H7J90B321120  ONLINE       0     0     0\r\n            ata-SAMSUNG_HD204UI_S2H7J9JB902323  ONLINE       0     0     0\r\n            ata-SAMSUNG_HD204UI_S2H7J9JB902334  ONLINE       0     0     0\r\n          raidz1-2                              ONLINE       0     0     0\r\n            ... REDACTED\r\n        spares\r\n          ata-ST2000DM006-2DM164_Z4Z90YHR       AVAIL\r\n\r\nerrors: No known data errors\r\n```\r\n\r\n(I've tried both with a spare and an external drive with the same effect)\r\n\r\nOther than issues I found on the matter, I payed attention to the 4K issue (I hope) back when the pool was created by specifically setting it up with `ashift=12`, as you can see from zdb:\r\n\r\n```shell\r\n $ sudo zdb tank\r\n...\r\nashift: 12\r\n...\r\n```\r\n\r\nThe obvious difference I can see is, that the new drives are indeed 4K devices, while the old ones weren't.\r\n\r\nOld drive:\r\n\r\n```\r\n $ sudo hdparm -I /dev/disk/by-id/ata-ST2000DL003-9VT166_6YD1HK6W\r\n        Logical/Physical Sector size:           512 bytes\r\n```\r\n\r\nNew drive:\r\n```\r\n $ sudo hdparm -I /dev/disk/by-id/ata-ST2000DM006-2DM164_Z4Z90YHR\r\n        Logical  Sector size:                   512 bytes\r\n        Physical Sector size:                  4096 bytes\r\n        Logical Sector-0 offset:                  0 bytes\r\n```\r\n\r\nWhat can I do about that? Thanks for you work and help!\r\n\r\n\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Ubuntu\r\nDistribution Version    |  Ubuntu 14.04.5 LTS\r\nLinux Kernel                 |  3.13.0-137-generic\r\nArchitecture                 | x86_64\r\nZFS Version                  |  0.6.5.11-1~trusty\r\nSPL Version                  |  0.6.5.11-1~trusty\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6975/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "itcrowdsource": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6966", "title": "ZFS mount/unmount process stuck", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Ubuntu\r\nDistribution Version    | 16.04 / Xenial\r\nLinux Kernel                 | 4.4.0-101-generic\r\nArchitecture                 | x86-64 fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts cpufreq\r\nZFS Version                  | version:        0.6.5.6-0ubuntu16\r\nSPL Version                  | version:        0.6.5.6-0ubuntu4\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nSpecifically were encountering issues within LXD. The ZFS mount point seems to be locked after restarting a LXC container that uses ZFS. This doesnt seem to be a LXD-related problem but rather a bug within ZFS.\r\n\r\nLXD log:\r\naction=shutdown created=2017-11-24T15:01:04+0000 ephemeral=false lvl=info msg=\"Shutting down container\" name=c2 t=2017-12-15T05:44:10+0000 timeout=-1s used=2017-11-24T15:06:55+0000\r\naction=shutdown created=2017-11-24T15:01:04+0000 ephemeral=false lvl=info msg=\"Shut down container\" name=c2 t=2017-12-15T05:44:40+0000 timeout=-1s used=2017-11-24T15:06:55+0000\r\nlvl=warn msg=\"Unable to update backup.yaml at this time.\" name=c2 t=2017-12-15T05:44:40+0000\r\nlvl=warn msg=\"ZFS returned EBUSY while \\\"/var/snap/lxd/common/lxd/storage-pools/lxd/containers/c2\\\" is actually not a mountpoint.\" t=2017-12-15T05:44:50+0000\r\nlvl=warn msg=\"Unable to update backup.yaml at this time.\" name=c2 t=2017-12-15T05:44:50+0000\r\n\r\n### Describe how to reproduce the problem\r\n1. lxc restart NAME\r\n2. Container is stuck in the shutdown process\r\n3. After a while the container isnt running anymore\r\n4. Unable to start the container\r\n\r\nhttps://github.com/lxc/lxd/issues/4104\r\n\r\nProbably also related to:\r\nhttps://github.com/zfsonlinux/zfs/issues/5796\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cwedgwood": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6958", "title": "SEEK_DATA / SEEK_HOLE do not work as expected with RO if there are pending atime updates", "body": "Type                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | \r\nDistribution Version    | \r\nLinux Kernel                 | 4.14.x\r\nArchitecture                 | amd64\r\nZFS Version                  | ba88ef6bb\r\nSPL Version                  | ed19bcc\r\n\r\n### Describe the problem you're observing\r\n\r\nWhen using SEEK_DATA & SEEK_HOLE to map a file it works robustly if the file is not being updated (open elsewhere for write).\r\n\r\nWhen it is open elsewhere for write and being updated, it's common to get wrong results, often a single data segment from [0, len]\r\n\r\n### Describe how to reproduce the problem\r\n\r\nCreate a sparse file, verify SEEK_DATA/SEEK_HOLE are working...\r\n\r\n*read* the file (anywhere) and SEEK_DATA/SEEK_HOLE no long function until the atime update from the previous access has flushed\r\n\r\nthere is a (weak) argument for this in the case of pending file content updates; but i feel it's even weaker again if it's just causes by pending atime from RO access", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6958/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6880", "title": "dbuf_evict / txg_sync hang", "body": "linux 4.9.61\r\nzfs d4a72f23863382bdf6d0ae33196f5b5decbc48fd (origin/master as of a few hours ago)\r\n\r\n~~~\r\ndbuf_evict      D12200   508      2 0x00000000\r\n ffff880c2425b880 00000000ffffffff ffff88098d3ada00 ffff88061f113880\r\n ffff880c3fd97900 ffffc9000deebd48 ffffffff816ad811 ffff8805e0830938\r\n 00ffc9000deebd30 ffff880c3fd97900 ffffc9000deebd70 ffff88061f113880\r\nCall Trace:\r\n [<ffffffff816ad811>] ? __schedule+0x221/0x6f0\r\n [<ffffffff816add16>] schedule+0x36/0x80\r\n [<ffffffff816adfae>] schedule_preempt_disabled+0xe/0x10\r\n [<ffffffff816af7f0>] __mutex_lock_slowpath+0x90/0x100\r\n [<ffffffff816af877>] mutex_lock+0x17/0x30\r\n [<ffffffffa1293907>] dbuf_destroy+0x157/0x740 [zfs]\r\n [<ffffffffa1293fcb>] dbuf_evict_one+0xdb/0x160 [zfs]\r\n [<ffffffffa12941f3>] dbuf_evict_thread+0x1a3/0x330 [zfs]\r\n [<ffffffffa1294050>] ? dbuf_evict_one+0x160/0x160 [zfs]\r\n [<ffffffffa0f5c440>] ? __thread_exit+0x20/0x20 [spl]\r\n [<ffffffffa0f5c4bb>] thread_generic_wrapper+0x7b/0xc0 [spl]\r\n [<ffffffff810a5ae7>] kthread+0xd7/0xf0\r\n [<ffffffff810a5a10>] ? kthread_park+0x60/0x60\r\n [<ffffffff816b2a47>] ret_from_fork+0x27/0x40\r\n\r\ntxg_sync        D10456  3551      2 0x00000000\r\n ffff880c241ed4c0 00000000ffffffff ffff8806225e0000 ffff88061b871c40\r\n ffff880c3fcd7900 ffffc90028ba79a8 ffffffff816ad811 000000000003ee90\r\n 00ff8805f845ae18 ffff880c3fcd7900 ffffffff811d39da ffff88061b871c40\r\nCall Trace:\r\n [<ffffffff816ad811>] ? __schedule+0x221/0x6f0\r\n [<ffffffff811d39da>] ? kmem_cache_free+0x1ba/0x1e0\r\n [<ffffffff816add16>] schedule+0x36/0x80\r\n [<ffffffff816adfae>] schedule_preempt_disabled+0xe/0x10\r\n [<ffffffff816af7f0>] __mutex_lock_slowpath+0x90/0x100\r\n [<ffffffff816af877>] mutex_lock+0x17/0x30\r\n [<ffffffffa1293907>] dbuf_destroy+0x157/0x740 [zfs]\r\n [<ffffffffa1293fcb>] dbuf_evict_one+0xdb/0x160 [zfs]\r\n [<ffffffffa129344a>] dbuf_rele_and_unlock+0x50a/0x540 [zfs]\r\n [<ffffffffa0f58e5a>] ? spl_kmem_free+0x2a/0x40 [spl]\r\n [<ffffffffa1352f5b>] ? __dprintf+0xfb/0x140 [zfs]\r\n [<ffffffffa129377b>] dbuf_rele+0x4b/0x80 [zfs]\r\n [<ffffffffa129817e>] dmu_buf_rele+0xe/0x10 [zfs]\r\n [<ffffffffa13427be>] zap_put_leaf+0x6e/0x90 [zfs]\r\n [<ffffffffa1343445>] fzap_lookup+0x125/0x130 [zfs]\r\n [<ffffffffa1348c1c>] zap_lookup_impl+0xcc/0x1d0 [zfs]\r\n [<ffffffffa1349570>] zap_lookup_norm+0x80/0xb0 [zfs]\r\n [<ffffffffa134961b>] zap_contains+0x3b/0x50 [zfs]\r\n [<ffffffffa130d25a>] spa_sync+0x96a/0x1240 [zfs]\r\n [<ffffffffa13257f6>] txg_sync_thread+0x2c6/0x4f0 [zfs]\r\n [<ffffffffa1325530>] ? txg_quiesce_thread+0x4f0/0x4f0 [zfs]\r\n [<ffffffffa0f5c440>] ? __thread_exit+0x20/0x20 [spl]\r\n [<ffffffffa0f5c4bb>] thread_generic_wrapper+0x7b/0xc0 [spl]\r\n [<ffffffff810a5ae7>] kthread+0xd7/0xf0\r\n [<ffffffff810a5a10>] ? kthread_park+0x60/0x60\r\n [<ffffffff816b2a47>] ret_from_fork+0x27/0x40\r\n~~~\r\n\r\n@behlendorf if this happens again (i'm going to reboot i guess) how ideally do i find out who is holding the mutex?", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6880/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Nasf-Fan": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6957", "title": "zfs_setattr miss to handle directory-based extended attributes", "body": "```\r\n$ touch /tank/shark/{f1,f2,f3}\r\n$ zfs userspace tank/shark\r\nTYPE     NAME   USED  QUOTA  OBJUSED  OBJQUOTA\r\nPOSIX User  root    2K   none        4      none\r\n$ setfattr -n trusted.foo -v bar /tank/shark/*\r\n$ zfs userspace tank/shark\r\nTYPE     NAME   USED  QUOTA  OBJUSED  OBJQUOTA\r\nPOSIX User  root  6.50K   none       10      none\r\n$ chown 123.123 /tank/shark/*\r\n$ zfs userspace tank/shark\r\nTYPE     NAME   USED  QUOTA  OBJUSED  OBJQUOTA\r\nPOSIX User  root  3.50K   none        4      none\r\nPOSIX User  123      3K   none        6      none\r\n```\r\n\r\nSo three xattr objects have not been handled by chown. Similar trouble for chgrp & groupspace, and the on-coming project quota (#6290 ).", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6957/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6948", "title": "Remove specified entry from ZAP directly without lookup internally", "body": "Currently, zap_remove()/zap_remove_by_dnode() needs to lookup the specified index (or name) to locate the entry internally with the directory locked, then remove the found entry. In fact, for most of cases, the caller has already performed lookup before calling zap_remove()/zap_remove_by_dnode(). But related lookup result cannot be used by the zap_remove()/zap_remove_by_dnode().\r\n\r\nI am wondering whether or not we can extend related API (and locks) to allow lookup the entry outside and directly use the lookup result for removing, that will save some lookup overhead. That will improve unlink/delete performance, especially for large directory.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6948/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6290", "title": "Project Quota on ZFS", "body": "Project quota is a new ZFS system space/object usage accounting\r\nand enforcement mechanism. Similar as user/group quota, project\r\nquota is another dimension of system quota. It bases on the new\r\nobject attribute - project ID.\r\n\r\nProject ID is a numerical value to indicate to which project an\r\nobject belongs. An object only can belong to one project though\r\nyou (the object owner or privileged user) can change the object\r\nproject ID that can be set/modified via 'chattr -p' explicitly,\r\nor inherited from its parent object when created if such parent\r\nhas the project inherit flag (via 'chattr +P').\r\n\r\nBy accounting the spaces/objects belong to the same project, we\r\ncan know how many spaces/objects used by the project. And if we\r\nset the upper limit then we can control the spaces/objects that\r\nare consumed by such project. It is useful when multiple groups\r\nand users cooperate for the same project, or when an user/group\r\nneeds to participate in multiple projects.\r\n\r\nSupport the following commands and functionalities:\r\n\r\nzfs set projectquota@project\r\nzfs set projectobjquota@project\r\n\r\nzfs get projectquota@project\r\nzfs get projectobjquota@project\r\nzfs get projectused@project\r\nzfs get projectobjused@project\r\n\r\nzfs projectspace\r\n\r\nzfs allow projectquota\r\nzfs allow projectobjquota\r\nzfs allow projectused\r\nzfs allow projectobjused\r\n\r\nzfs unallow projectquota\r\nzfs unallow projectobjquota\r\nzfs unallow projectused\r\nzfs unallow projectobjused\r\n\r\nchattr +/-P\r\nchattr -p project_id\r\nlsattr -p\r\n\r\nSigned-off-by: Fan Yong <fan.yong@intel.com>\r\nChange-Id: Ib4f0544602e03fb61fd46a849d7ba51a6005693c\r\n\r\n<!--- Provide a general summary of your changes in the Title above -->\r\n\r\n<!---\r\nDocumentation on ZFS Buildbot options can be found at\r\nhttps://github.com/zfsonlinux/zfs/wiki/Buildbot-Options\r\n-->\r\n\r\n### Description\r\n<!--- Describe your changes in detail -->\r\n\r\n### Motivation and Context\r\n<!--- Why is this change required? What problem does it solve? -->\r\n<!--- If it fixes an open issue, please link to the issue here. -->\r\n\r\n### How Has This Been Tested?\r\n<!--- Please describe in detail how you tested your changes. -->\r\n<!--- Include details of your testing environment, and the tests you ran to -->\r\n<!--- see how your change affects other areas of the code, etc. -->\r\n<!--- If your change is a performance enhancement, please provide benchmarks here. -->\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [x] I have updated the documentation accordingly.\r\n- [x] I have read the **CONTRIBUTING** document.\r\n- [x] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [x] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "woquflux": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6952", "title": "High cpu usage of dbuf_evict for small random reads", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Redhat\r\nDistribution Version    | 7.2\r\nLinux Kernel                 | 3.10.0-327.el7.x86_64\r\nArchitecture                 | \r\nZFS Version                  | 0.6.4.2\r\nSPL Version                  |  0.6.4.2\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nTerrible Performance with SSD x 18 (Intel S3500)\r\n\r\n### Describe how to reproduce the problem\r\nCPU usage of rngd is 100%\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n![image](https://user-images.githubusercontent.com/30719174/33937415-62c584fa-e03e-11e7-8e6c-31ae6a1c1f4e.png)\r\n\r\n![image](https://user-images.githubusercontent.com/30719174/33937339-05621756-e03e-11e7-99ef-a4c2fa4583ae.png)\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6952/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6555", "title": "Random Write amplification 2~3x\uff1f", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Redhat\r\nDistribution Version    | 7.2\r\nLinux Kernel                 | 3.10.0-327.el7.x86_64\r\nArchitecture                 | \r\nZFS Version                  | 0.6.4.2\r\nSPL Version                  | 0.6.4.2\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nWrite amplification 2~3x when fio test\r\n\r\nThese screenshots show details:\r\n\r\n![qq20170824-160731](https://user-images.githubusercontent.com/30719174/29656643-9bff15c8-88e7-11e7-9eba-8134f73be7f1.png)\r\n![addb81a0-d7db-45b0-937f-e156bfd6cf2b](https://user-images.githubusercontent.com/30719174/29656653-a4c967e4-88e7-11e7-960f-7b84ed359c70.png)\r\n\r\n```\r\n$zfs get copies qbackup\r\nNAME     PROPERTY  VALUE   SOURCE\r\nqbackup  copies    1       default\r\n```\r\n\r\n### Describe how to reproduce the problem\r\nZfs config\r\n```\r\n$cat /sys/module/zfs/parameters/zfs_vdev_aggregation_limit\r\n8192\r\n$cat /sys/module/zfs/parameters/zfs_dirty_data_max\r\n214748364\r\n\r\nzpool create test sdd  # 60G Intel ssd\r\nzfs create -V 10G test/test1\r\nOR\r\nzpool create qbackup sfd0 # 3.2TB Flashcard\r\nzfs create -V 100G qbackup/test1\r\n```\r\n\r\nFio config\r\n```\r\n[global]\r\nnumjobs=1\r\nrw=randwrite\r\nbs=8k\r\nruntime=60\r\nioengine=libaio\r\ndirect=1\r\niodepth=1\r\nname=jicki\r\ngroup_reporting\r\n[file1]\r\nfilename=/dev/qbackup/test1\r\n```\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6555/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jgottula": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6950", "title": "Linux 4.13+: Warnings from objtool while compiling the 'icp' kernel module: \"unsupported stack pointer realignment\" (in SHA1/SHA2 functions)", "body": "I don't know how important these build-time warnings actually are, but I've typed up a full report here anyway.\r\n\r\n## The warnings\r\n\r\nI was building the latest zfs master, 4e9b156960562373e005798575a3fbc6d66e32ff, for Linux 4.14.5, on x86_64 today.\r\n\r\nHere's what came up during the build process:\r\n```\r\n/home/build/zfs/zfs-20171212-latest/archzfs/packages/linux/zfs-linux-git/src/zfs/module/icp/asm-x86_64/sha1/sha1-x86_64.o: warning: objtool: sha1_block_data_order()+0x11: unsupported stack pointer realignment\r\n/home/build/zfs/zfs-20171212-latest/archzfs/packages/linux/zfs-linux-git/src/zfs/module/icp/asm-x86_64/sha2/sha256_impl.o: warning: objtool: SHA256TransformBlocks()+0x19: unsupported stack pointer realignment\r\n/home/build/zfs/zfs-20171212-latest/archzfs/packages/linux/zfs-linux-git/src/zfs/module/icp/asm-x86_64/sha2/sha512_impl.o: warning: objtool: SHA512TransformBlocks()+0x1c: unsupported stack pointer realignment\r\n```\r\nThose warnings correspond to these particular instructions from the objdump'd object files:\r\n```\r\n0000000000000000 <sha1_block_data_order>:\r\n      ...\r\n      11:       48 83 e4 c0             and    rsp,0xffffffffffffffc0\r\n      ...\r\n```\r\n```\r\n0000000000000000 <SHA256TransformBlocks>:\r\n      ...\r\n      19:       48 83 e4 c0             and    rsp,0xffffffffffffffc0\r\n      ...\r\n```\r\n```\r\n0000000000000000 <SHA512TransformBlocks>:\r\n      ...\r\n      1c:       48 83 e4 c0             and    rsp,0xffffffffffffffc0\r\n      ...\r\n```\r\n\r\n## The recent update to objtool \r\n\r\nIt looks like this warning will only appear when compiling for Linux 4.13 or later, since the objtool patchset that added new warnings (including the \"unsupported stack pointer realignment\" one) was merged into the mainline kernel in 4.13.\r\n\r\nHere's some additional info on that objtool patchset:\r\n- LKML: [objtool: stack validation 2.0](https://lkml.org/lkml/2017/6/1/38)\r\n- Patchwork: [objtool: stack validation 2.0](https://patchwork.kernel.org/patch/9814577/)\r\n- Commit: [baa41469a7b992c1e3db2a39854219cc7442e48f](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit?id=baa41469a7b992c1e3db2a39854219cc7442e48f)\r\n\r\n[Here's the specific part of the objtool code that's complaining.](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/tools/objtool/check.c?id=baa41469a7b992c1e3db2a39854219cc7442e48f#n1101)\r\n[And here's some internal documentation regarding what it's complaining about.](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/tools/objtool/check.c?id=baa41469a7b992c1e3db2a39854219cc7442e48f#n961)\r\n\r\n## The ZoL source lines being complained about\r\n\r\nHere are the relevant assembly code source file lines in ZoL:\r\n<!--\r\n- [module/icp/asm-x86_64/sha1/sha1-x86_64.S line 79](https://github.com/zfsonlinux/zfs/blob/4e9b156960562373e005798575a3fbc6d66e32ff/module/icp/asm-x86_64/sha1/sha1-x86_64.S#L79)\r\n- [module/icp/asm-x86_64/sha2/sha256_impl.S line 96](https://github.com/zfsonlinux/zfs/blob/4e9b156960562373e005798575a3fbc6d66e32ff/module/icp/asm-x86_64/sha2/sha256_impl.S#L96)\r\n- [module/icp/asm-x86_64/sha2/sha512_impl.S line 97](https://github.com/zfsonlinux/zfs/blob/4e9b156960562373e005798575a3fbc6d66e32ff/module/icp/asm-x86_64/sha2/sha512_impl.S#L97)\r\n-->\r\nhttps://github.com/zfsonlinux/zfs/blob/4e9b156960562373e005798575a3fbc6d66e32ff/module/icp/asm-x86_64/sha1/sha1-x86_64.S#L79\r\nhttps://github.com/zfsonlinux/zfs/blob/4e9b156960562373e005798575a3fbc6d66e32ff/module/icp/asm-x86_64/sha2/sha256_impl.S#L96\r\nhttps://github.com/zfsonlinux/zfs/blob/4e9b156960562373e005798575a3fbc6d66e32ff/module/icp/asm-x86_64/sha2/sha512_impl.S#L97\r\n\r\n## 32-bit\r\n\r\nAs far as I can tell, i386 builds of zfs won't emit this warning, as from what I can see, the asm implementations of the three SHA functions in question are only used for the kernel module on x86_64, and everything else just uses the C implementations.\r\n\r\nBut I don't have an i386 Linux box/VM ready-at-hand to do a quick build and actually confirm for sure that these warnings don't happen there.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6950/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6881", "title": "VERIFY3(0 == remove_reference(hdr, ((void *)0), tag)) failed (0 == 1)", "body": "Skip down a couple sections if you don't care about any of this backstory; it might be relevant to the problem, or it might not, I don't know.\r\n\r\n## Background information\r\n\r\nI have a ZFS filesystem containing disk image files which I update every week and then take a snapshot, so that I have a weekly backup history stored up.\r\n\r\nI had some disk corruption on this pool's 4-disk raidz1 vdev during a period in which I was running it on only 3 disks, so `zpool status -v` knows of a couple of snapshots that contain corrupted data. I'm fairly certain that there are also a few more snapshots affected than just what `zpool status -v` reports, due to shared blocks between the snapshots.\r\n\r\nBasically, this pool is barely hanging on, with *at least* one *very marginal* drive. I've spent over 3 months and numerous hours now attempting to get all of my data off of this doomed pool via `zfs send`/`zfs recv`, something that is *still* not fully possible due to `EINVAL`-related problems with `zfs recv` that I've described in other issue reports here.\r\n\r\nI also cannot really \"simply swap out the bad disks\", because whenever I attempt a `zfs replace`, it gets stuck at a certain point, never completes, and never detaches the old device. (Oh and this pool now also spends its time perpetually resilvering over and over, apparently due to the stuck-replacement problem. Or some other bug. I don't know.)\r\n\r\nHere's what this wonderful pool looks like:\r\n```\r\n  pool: pool\r\n state: DEGRADED\r\nstatus: One or more devices is currently being resilvered.  The pool will\r\n        continue to function, possibly in a degraded state.\r\naction: Wait for the resilver to complete.\r\n  scan: resilver in progress since Wed Nov 15 03:43:32 2017\r\n        7.94T scanned out of 10.5T at 37.2M/s, 20h10m to go\r\n        118G resilvered, 75.52% done\r\nconfig:\r\n\r\n        NAME                        STATE     READ WRITE CKSUM\r\n        pool                        DEGRADED     0     0    22\r\n          raidz1-0                  DEGRADED     0     0    50\r\n            replacing-0             DEGRADED     0     0     2\r\n              JGPool1               ONLINE       0     0     0\r\n              14172011574593492267  UNAVAIL      0     0     0  was /dev/disk/by-partlabel/JGPoolY\r\n            JGPoolX                 ONLINE       1     0     0  (resilvering)\r\n            replacing-2             DEGRADED     0     0 78.4M\r\n              6328974533181069670   UNAVAIL      0     0     0  was /dev/disk/by-partlabel/JGPool3\r\n              JGPoolZ               ONLINE       0     0     0  (resilvering)\r\n            JGPool4                 ONLINE       0     0     0\r\n        logs\r\n          mirror-1                  ONLINE       0     0     0\r\n            ZIL_Samsung             ONLINE       0     0     0\r\n            ZIL_Corsair             ONLINE       0     0     0\r\n        cache\r\n          L2ARC_Samsung             ONLINE       0     0     0\r\n          L2ARC_Corsair             ONLINE       0     0     0\r\n```\r\n\r\n## What I was doing when the panics happened\r\n\r\nI've resigned myself to the inevitability that I'll never be able to `zfs send`/`zfs recv` my data off of this pool and onto a new one before my drives have fully disintegrated into scrap. Since this collection of historical backups relies extremely heavily on CoW space savings between snapshots (it's about 2.77 TiB compressed, but would be well over 7x larger otherwise), I decided to write up some scripts and manually port over the data to the new pool in a manner that maintains the CoW space savings by only writing the changed parts.\r\n\r\nThe details of that aren't relevant; what is relevant here is that as a precondition to doing this manual transfer process, I ran a bunch of `md5sum` processes with GNU `parallel` to get hashes of each disk image at each snapshot. Most of this went fine.\r\n\r\nFor reference, `zpool status -v` knows of data corruption in snapshots `20170327` and `20170529`.\r\n\r\nThe `md5sum` process that attempted to read the disk image in the `20170327` snapshot error'd out with `EIO`, as expected.\r\n\r\nHowever, the `md5sum` processes that attempted to read the disk image in the `20170515`, `20170522`, `20170529`, and `20170605` snapshots got stuck in state `D` (uninterruptible sleep) forever, assumedly due to the task thread panics that are the subject of this report.\r\n\r\nInterestingly, the system is still entirely usable (i.e. other I/O initiated after the problem doesn't get stuck); but those `md5sum` processes are forever stuck in state `D`, and the couple of `z_rd_int_7` and `z_rd_int_0` kernel threads that had the panics are now idle, doing nothing.\r\n\r\n## The task thread panics themselves\r\n\r\nTwo panics, separated by approximately 2 seconds.\r\n\r\n```\r\nNov 15 10:02:41 jgpc kernel: VERIFY3(0 == remove_reference(hdr, ((void *)0), tag)) failed (0 == 1)\r\nNov 15 10:02:41 jgpc kernel: PANIC at arc.c:3748:arc_buf_destroy()\r\nNov 15 10:02:41 jgpc kernel: Showing stack for process 1033\r\nNov 15 10:02:41 jgpc kernel: CPU: 2 PID: 1033 Comm: z_rd_int_7 Tainted: P      D    O    4.13.6-1 #1\r\nNov 15 10:02:41 jgpc kernel: Hardware name: To be filled by O.E.M. To be filled by O.E.M./M5A99X EVO, BIOS 1708 04/10/2013\r\nNov 15 10:02:41 jgpc kernel: Call Trace:\r\nNov 15 10:02:41 jgpc kernel:  dump_stack+0x62/0x7a\r\nNov 15 10:02:41 jgpc kernel:  spl_dumpstack+0x55/0x60 [spl]\r\nNov 15 10:02:41 jgpc kernel:  spl_panic+0xd2/0x120 [spl]\r\nNov 15 10:02:41 jgpc kernel:  ? debug_smp_processor_id+0x2a/0x30\r\nNov 15 10:02:41 jgpc kernel:  ? spl_kmem_cache_alloc+0xc1/0x710 [spl]\r\nNov 15 10:02:41 jgpc kernel:  ? spl_kmem_cache_alloc+0x70/0x710 [spl]\r\nNov 15 10:02:41 jgpc kernel:  ? buf_cons+0x84/0x90 [zfs]\r\nNov 15 10:02:41 jgpc kernel:  arc_buf_destroy+0x143/0x150 [zfs]\r\nNov 15 10:02:41 jgpc kernel:  dbuf_read_done+0x99/0xf0 [zfs]\r\nNov 15 10:02:41 jgpc kernel:  arc_read_done+0x1e9/0x470 [zfs]\r\nNov 15 10:02:41 jgpc kernel:  zio_done+0x420/0xe48 [zfs]\r\nNov 15 10:02:41 jgpc kernel:  zio_execute+0x9d/0xf8 [zfs]\r\nNov 15 10:02:41 jgpc kernel:  taskq_thread+0x299/0x4b8 [spl]\r\nNov 15 10:02:41 jgpc kernel:  ? wake_up_q+0x90/0x90\r\nNov 15 10:02:41 jgpc kernel:  ? zio_reexecute+0x3a0/0x3a0 [zfs]\r\nNov 15 10:02:41 jgpc kernel:  kthread+0x134/0x150\r\nNov 15 10:02:41 jgpc kernel:  ? taskq_thread_should_stop+0x88/0x88 [spl]\r\nNov 15 10:02:41 jgpc kernel:  ? kthread_create_on_node+0x50/0x50\r\nNov 15 10:02:41 jgpc kernel:  ret_from_fork+0x22/0x30\r\n\r\nNov 15 10:02:43 jgpc kernel: VERIFY3(0 == remove_reference(hdr, ((void *)0), tag)) failed (0 == 2)\r\nNov 15 10:02:43 jgpc kernel: PANIC at arc.c:3748:arc_buf_destroy()\r\nNov 15 10:02:43 jgpc kernel: Showing stack for process 946\r\nNov 15 10:02:43 jgpc kernel: CPU: 2 PID: 946 Comm: z_rd_int_0 Tainted: P      D    O    4.13.6-1 #1\r\nNov 15 10:02:43 jgpc kernel: Hardware name: To be filled by O.E.M. To be filled by O.E.M./M5A99X EVO, BIOS 1708 04/10/2013\r\nNov 15 10:02:43 jgpc kernel: Call Trace:\r\nNov 15 10:02:43 jgpc kernel:  dump_stack+0x62/0x7a\r\nNov 15 10:02:43 jgpc kernel:  spl_dumpstack+0x55/0x60 [spl]\r\nNov 15 10:02:43 jgpc kernel:  spl_panic+0xd2/0x120 [spl]\r\nNov 15 10:02:43 jgpc kernel:  ? debug_smp_processor_id+0x2a/0x30\r\nNov 15 10:02:43 jgpc kernel:  ? spl_kmem_cache_alloc+0xc1/0x710 [spl]\r\nNov 15 10:02:43 jgpc kernel:  ? spl_kmem_cache_alloc+0x70/0x710 [spl]\r\nNov 15 10:02:43 jgpc kernel:  ? buf_cons+0x84/0x90 [zfs]\r\nNov 15 10:02:43 jgpc kernel:  arc_buf_destroy+0x143/0x150 [zfs]\r\nNov 15 10:02:43 jgpc kernel:  dbuf_read_done+0x99/0xf0 [zfs]\r\nNov 15 10:02:43 jgpc kernel:  arc_read_done+0x1e9/0x470 [zfs]\r\nNov 15 10:02:43 jgpc kernel:  zio_done+0x420/0xe48 [zfs]\r\nNov 15 10:02:43 jgpc kernel:  zio_execute+0x9d/0xf8 [zfs]\r\nNov 15 10:02:43 jgpc kernel:  taskq_thread+0x299/0x4b8 [spl]\r\nNov 15 10:02:43 jgpc kernel:  ? wake_up_q+0x90/0x90\r\nNov 15 10:02:43 jgpc kernel:  ? zio_reexecute+0x3a0/0x3a0 [zfs]\r\nNov 15 10:02:43 jgpc kernel:  kthread+0x134/0x150\r\nNov 15 10:02:43 jgpc kernel:  ? taskq_thread_should_stop+0x88/0x88 [spl]\r\nNov 15 10:02:43 jgpc kernel:  ? kthread_create_on_node+0x50/0x50\r\nNov 15 10:02:43 jgpc kernel:  ret_from_fork+0x22/0x30\r\n```\r\n\r\n## The stuck `md5sum` processes\r\n\r\nAll of the stuck `md5sum` processes have the following stack:\r\n```\r\n# cat /proc/$STUCK_PID/stack\r\n[<ffffffff8109b6a9>] io_schedule+0x29/0x50\r\n[<ffffffffa062fbc8>] cv_wait_common+0xb0/0x120 [spl]\r\n[<ffffffffa062fcc3>] __cv_wait_io+0x2b/0x30 [spl]\r\n[<ffffffffa09ceff2>] zio_wait+0xf2/0x198 [zfs]\r\n[<ffffffffa0912847>] dmu_buf_hold_array_by_dnode+0x167/0x480 [zfs]\r\n[<ffffffffa0913f39>] dmu_read_uio_dnode+0x49/0xf0 [zfs]\r\n[<ffffffffa0914038>] dmu_read_uio_dbuf+0x58/0x78 [zfs]\r\n[<ffffffffa09b6f85>] zfs_read+0x135/0x3f0 [zfs]\r\n[<ffffffffa09d8f30>] zpl_read_common_iovec+0x80/0xc0 [zfs]\r\n[<ffffffffa09d97c9>] zpl_iter_read+0xa9/0xf0 [zfs]\r\n[<ffffffff81222e3b>] __vfs_read+0xcb/0x128\r\n[<ffffffff81224152>] vfs_read+0xa2/0x148\r\n[<ffffffff8122586d>] SyS_read+0x55/0xb8\r\n[<ffffffff8164aaa4>] entry_SYSCALL_64_fastpath+0x17/0x98\r\n[<ffffffffffffffff>] 0xffffffffffffffff\r\n```\r\n\r\n## System information\r\n\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Arch Linux\r\nDistribution Version    | n/a\r\nLinux Kernel                 | 4.13.6\r\nArchitecture                 | x86_64\r\nZFS Version                  | zfsonlinux/zfs@cdc15a76045fa70743fb95a1fd450229e2b73fd3\r\nSPL Version                  | zfsonlinux/spl@0cefc9dbcd9de8a35c51e172edabf2f2ecf15f92", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6881/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ggzengel": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6947", "title": "New feature: drop DDT entries", "body": "In #5904 I have a very big DDT even when I disable dedup on all file systems.\r\nI want to get rid of the DDT entries.\r\n\r\nI think it's best to have an import option which resets the DDT.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6947/reactions", "total_count": 1, "+1": 0, "-1": 1, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "twarberg": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6945", "title": "kernel NULL pointer dereference when using L2arc", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 16.04 LTS\r\nLinux Kernel                 | 4.10.0-42-generic\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.3\r\nSPL Version                  | 0.7.3\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nWe're running on Google Cloud and is trying to use an local NVMe SSD for L2arc but run into the crash after 7-12 days. We've seen it on 2 different systems a MySQL and a Postgres server. It happened on the Postgres server with both 0.7.1 and 0.7.2 with about a weeks interval (2x on 0.7.1 and once on 0.7.2). Unfortunately we do not have console logs for those which is why we haven't reported it until now. We decided to give 0.7.3 a try after upgrading the MySQL server and got a crash after 10 days.\r\nBoth systems are busy 24/7 with the Postgres being write heavy and the MySQL read heavy. Postgres is at least 2x as disk active.\r\nThe Postgres server has been running stable without L2arc for months.\r\n\r\nCommon settings\r\n* Ubuntu 16.04 LTS\r\n* lz4 compression\r\n* recordsize=16K\r\n* atime=off\r\n* local NVMe SSD for L2arc\r\n* SSD backed disk volumes\r\n\r\n### Describe how to reproduce the problem\r\nRun a busy MySQL or Postgres on Google Cloud instance with local NVMe SSD L2arc for 7-12 days.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n```\r\n[896673.398682] BUG: unable to handle kernel NULL pointer dereference at 0000000000000018\r\n[896673.406756] IP: kmem_cache_alloc+0x77/0x1b0\r\n[896673.411138] PGD b1f993067\r\n[896673.411139] PUD 6d76be067\r\n[896673.414041] PMD 0\r\n[896673.416944]\r\n[896673.420828] Oops: 0000 [#1] SMP\r\n[896673.424157] Modules linked in: ufs msdos xfs tcp_diag inet_diag binfmt_misc zfs(POE) zunicode(POE) zavl(PO) icp(POE) zcommon(POE) znvpair(POE) spl(OE) ip6table_filter ip6_tables iptable_filter ip_tables x_tables ppdev input_leds parport_pc parport pvpanic serio_raw ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi autofs4 btrfs raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c raid1 raid0 multipath linear crct10dif_pclmul crc32_pclmul ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd glue_helper cryptd psmouse nvme virtio_net nvme_core virtio_scsi\r\n[896673.481944] CPU: 5 PID: 19051 Comm: arc_reclaim Tainted: P           OE   4.10.0-40-generic #44~16.04.1-Ubuntu\r\n[896673.492129] Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\r\n[896673.501532] task: ffff92dd83781d40 task.stack: ffffbb4507004000\r\n[896673.507640] RIP: 0010:kmem_cache_alloc+0x77/0x1b0\r\n[896673.512528] RSP: 0018:ffffbb4507007c38 EFLAGS: 00010206\r\n[896673.517938] RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000090eebb2a\r\n[896673.525258] RDX: 0000000090eebb29 RSI: 0000000001404220 RDI: 000000000001c6a0\r\n[896673.532579] RBP: ffffbb4507007c68 R08: ffff92ddbfd5c6a0 R09: 0000000000000018\r\n[896673.539899] R10: ffff92dd60bac7b0 R11: 0000000000000000 R12: 0000000001404220\r\n[896673.547223] R13: ffffffffc06a12b2 R14: ffff92dd8b0037c0 R15: ffff92dd8b0037c0\r\n[896673.554550] FS:  0000000000000000(0000) GS:ffff92ddbfd40000(0000) knlGS:0000000000000000\r\n[896673.562825] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[896673.568757] CR2: 0000000000000018 CR3: 00000006c8c6d000 CR4: 00000000001406e0\r\n[896673.576078] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\n[896673.583398] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\r\n[896673.590719] Call Trace:\r\n[896673.593366]  spl_kmem_cache_alloc+0x72/0x7d0 [spl]\r\n[896673.598348]  ? kmem_cache_free+0x1cd/0x1e0\r\n[896673.602675]  ? arc_state_multilist_index_func+0x4c/0x60 [zfs]\r\n[896673.608630]  arc_hdr_realloc+0x31/0x270 [zfs]\r\n[896673.613217]  arc_evict_state+0x4fb/0x870 [zfs]\r\n[896673.617867]  arc_adjust+0x4af/0x690 [zfs]\r\n[896673.622068]  ? kvm_clock_get_cycles+0x1e/0x20\r\n[896673.626637]  arc_reclaim_thread+0xab/0x280 [zfs]\r\n[896673.631458]  ? arc_shrink+0xb0/0xb0 [zfs]\r\n[896673.635666]  thread_generic_wrapper+0x72/0x80 [spl]\r\n[896673.640733]  kthread+0x109/0x140\r\n[896673.644149]  ? __thread_exit+0x20/0x20 [spl]\r\n[896673.648607]  ? kthread_create_on_node+0x60/0x60\r\n[896673.653334]  ret_from_fork+0x2c/0x40\r\n[896673.657096] Code: 08 65 4c 03 05 2b e1 be 61 49 83 78 10 00 4d 8b 08 0f 84 f8 00 00 00 4d 85 c9 0f 84 ef 00 00 00 49 63 47 20 48 8d 4a 01 49 8b 3f <49> 8b 1c 01 4c 89 c8 65 48 0f c7 0f 0f 94 c0 84 c0 74 bb 49 63\r\n[896673.676170] RIP: kmem_cache_alloc+0x77/0x1b0 RSP: ffffbb4507007c38\r\n[896673.682537] CR2: 0000000000000018\r\n[896673.686040] ---[ end trace 4de68d0227766242 ]---\r\n\r\n[896674.140835] BUG: unable to handle kernel NULL pointer dereference at 0000000000000018\r\n[896674.149026] IP: kmem_cache_alloc_trace+0x7b/0x1c0\r\n[896674.153915] PGD b1f993067\r\n[896674.153915] PUD 6d76be067\r\n[896674.156816] PMD 0\r\n[896674.159809]\r\n[896674.163680] Oops: 0000 [#2] SMP\r\n[896674.167006] Modules linked in: ufs msdos xfs tcp_diag inet_diag binfmt_misc zfs(POE) zunicode(POE) zavl(PO) icp(POE) zcommon(POE) znvpair(POE) spl(OE) ip6table_filter ip6_tables iptable_filter ip_tables x_tables ppdev input_leds parport_pc parport pvpanic serio_raw ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi autofs4 btrfs raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c raid1 raid0 multipath linear crct10dif_pclmul crc32_pclmul ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd glue_helper cryptd psmouse nvme virtio_net nvme_core virtio_scsi\r\n[896674.224935] CPU: 5 PID: 25120 Comm: kthreadd Tainted: P      D    OE   4.10.0-40-generic #44~16.04.1-Ubuntu\r\n[896674.234862] Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\r\n[896674.244267] task: ffff92d5aefaba80 task.stack: ffffbb450f680000\r\n[896674.250376] RIP: 0010:kmem_cache_alloc_trace+0x7b/0x1c0\r\n[896674.255785] RSP: 0018:ffffbb450f683eb8 EFLAGS: 00010206\r\n[896674.261195] RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000090eebb2a\r\n[896674.268513] RDX: 0000000090eebb29 RSI: 00000000014000c0 RDI: 000000000001c6a0\r\n[896674.275844] RBP: ffffbb450f683ef8 R08: ffff92ddbfd5c6a0 R09: ffff92dd8b0037c0\r\n[896674.283165] R10: 0000000000000018 R11: 0000000000000000 R12: 00000000014000c0\r\n[896674.290488] R13: ffffffff9e2a8d26 R14: ffff92d603fc8ba0 R15: ffff92dd8b0037c0\r\n[896674.297807] FS:  0000000000000000(0000) GS:ffff92ddbfd40000(0000) knlGS:0000000000000000\r\n[896674.306082] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[896674.312014] CR2: 0000000000000018 CR3: 00000006c8c6d000 CR4: 00000000001406e0\r\n[896674.319335] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\n[896674.326665] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\r\n[896674.333985] Call Trace:\r\n[896674.336624]  ? kthread_create_on_node+0x60/0x60\r\n[896674.341340]  kthread+0x46/0x140\r\n[896674.344673]  ? taskq_cancel_id+0x130/0x130 [spl]\r\n[896674.349475]  ? kthread_create_on_node+0x60/0x60\r\n[896674.354194]  ret_from_fork+0x2c/0x40\r\n[896674.357957] Code: 08 65 4c 03 05 77 e3 be 61 49 83 78 10 00 4d 8b 10 0f 84 f0 00 00 00 4d 85 d2 0f 84 e7 00 00 00 49 63 41 20 48 8d 4a 01 49 8b 39 <49> 8b 1c 02 4c 89 d0 65 48 0f c7 0f 0f 94 c0 84 c0 74 bb 49 63\r\n[896674.377023] RIP: kmem_cache_alloc_trace+0x7b/0x1c0 RSP: ffffbb450f683eb8\r\n[896674.383910] CR2: 0000000000000018\r\n[896674.387412] ---[ end trace 4de68d0227766243 ]---\r\n\r\n[896678.440616] BUG: unable to handle kernel NULL pointer dereference at 0000000000000018\r\n[896678.448691] IP: __kmalloc+0xbc/0x200\r\n[896678.452453] PGD 0\r\n[896678.452454]\r\n[896678.456322] Oops: 0000 [#3] SMP\r\n[896678.459646] Modules linked in: ufs msdos xfs tcp_diag inet_diag binfmt_misc zfs(POE) zunicode(POE) zavl(PO) icp(POE) zcommon(POE) znvpair(POE) spl(OE) ip6table_filter ip6_tables iptable_filter ip_tables x_tables ppdev input_leds parport_pc parport pvpanic serio_raw ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi autofs4 btrfs raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c raid1 raid0 multipath linear crct10dif_pclmul crc32_pclmul ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd glue_helper cryptd psmouse nvme virtio_net nvme_core virtio_scsi\r\n[896678.517434] CPU: 5 PID: 889 Comm: z_wr_iss Tainted: P      D    OE   4.10.0-40-generic #44~16.04.1-Ubuntu\r\n[896678.527186] Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\r\n[896678.536591] task: ffff92dd6014d7c0 task.stack: ffffbb450f4a4000\r\n[896678.542701] RIP: 0010:__kmalloc+0xbc/0x200\r\n[896678.546984] RSP: 0018:ffffbb450f4a7be8 EFLAGS: 00010206\r\n[896678.552393] RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000090eebb2a\r\n[896678.559712] RDX: 0000000090eebb29 RSI: 0000000000000000 RDI: 000000000001c6a0\r\n[896678.567032] RBP: ffffbb450f4a7c20 R08: ffff92ddbfd5c6a0 R09: ffff92dd8b0037c0\r\n[896678.574353] R10: 0000000000000018 R11: fffff2ae44c880a0 R12: 0000000001400200\r\n[896678.581671] R13: 0000000000000060 R14: ffffffff9e65bc59 R15: ffff92dd8b0037c0\r\n[896678.588989] FS:  0000000000000000(0000) GS:ffff92ddbfd40000(0000) knlGS:0000000000000000\r\n[896678.597263] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[896678.603194] CR2: 0000000000000018 CR3: 000000097f609000 CR4: 00000000001406e0\r\n[896678.610514] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\n[896678.617834] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\r\n[896678.625153] Call Trace:\r\n[896678.627791]  sg_kmalloc+0x19/0x30\r\n[896678.631294]  __sg_alloc_table+0xfd/0x160\r\n[896678.635402]  ? sg_free_table+0x70/0x70\r\n[896678.639337]  sg_alloc_table+0x22/0x90\r\n[896678.643230]  abd_alloc+0x230/0x470 [zfs]\r\n[896678.647354]  arc_hdr_alloc_pabd+0xe7/0xf0 [zfs]\r\n[896678.652085]  arc_write_ready+0x135/0x2f0 [zfs]\r\n[896678.657080]  ? pick_next_task_fair+0x108/0x4d0\r\n[896678.661712]  ? mutex_lock+0x12/0x40\r\n[896678.665447]  zio_ready+0x65/0x460 [zfs]\r\n[896678.669475]  ? tsd_get_by_thread+0x2e/0x40 [spl]\r\n[896678.674279]  ? taskq_member+0x18/0x30 [spl]\r\n[896678.678683]  zio_execute+0x8a/0xe0 [zfs]\r\n[896678.682793]  taskq_thread+0x260/0x460 [spl]\r\n[896678.687162]  ? wake_up_q+0x70/0x70\r\n[896678.690750]  kthread+0x109/0x140\r\n[896678.694166]  ? taskq_cancel_id+0x130/0x130 [spl]\r\n[896678.698971]  ? kthread_create_on_node+0x60/0x60\r\n[896678.703688]  ret_from_fork+0x2c/0x40\r\n[896678.707451] Code: 08 65 4c 03 05 56 cd be 61 49 83 78 10 00 4d 8b 10 0f 84 d5 00 00 00 4d 85 d2 0f 84 cc 00 00 00 49 63 41 20 48 8d 4a 01 49 8b 39 <49> 8b 1c 02 4c 89 d0 65 48 0f c7 0f 0f 94 c0 84 c0 74 bb 49 63\r\n[896678.726517] RIP: __kmalloc+0xbc/0x200 RSP: ffffbb450f4a7be8\r\n[896678.732273] CR2: 0000000000000018\r\n[896678.735774] ---[ end trace 4de68d0227766244 ]---\r\n\r\n[896683.127883] BUG: unable to handle kernel NULL pointer dereference at 0000000000000018\r\n[896683.135948] IP: kmem_cache_alloc_node_trace+0xd7/0x1d0\r\n[896683.141267] PGD 91ddb7067\r\n[896683.141268] PUD b7aa63067\r\n[896683.144157] PMD 0\r\n[896683.147045]\r\n[896683.150912] Oops: 0000 [#4] SMP\r\n[896683.154236] Modules linked in: ufs msdos xfs tcp_diag inet_diag binfmt_misc zfs(POE) zunicode(POE) zavl(PO) icp(POE) zcommon(POE) znvpair(POE) spl(OE) ip6table_filter ip6_tables iptable_filter ip_tables x_tables ppdev input_leds parport_pc parport pvpanic serio_raw ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi autofs4 btrfs raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c raid1 raid0 multipath linear crct10dif_pclmul crc32_pclmul ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd glue_helper cryptd psmouse nvme virtio_net nvme_core virtio_scsi\r\n[896683.212018] CPU: 5 PID: 17027 Comm: mysqld Tainted: P      D    OE   4.10.0-40-generic #44~16.04.1-Ubuntu\r\n[896683.221770] Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\r\n[896683.231172] task: ffff92dcc9abd7c0 task.stack: ffffbb452dd00000\r\n[896683.237283] RIP: 0010:kmem_cache_alloc_node_trace+0xd7/0x1d0\r\n[896683.243124] RSP: 0018:ffffbb452dd03c28 EFLAGS: 00010246\r\n[896683.248534] RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000090eebb2a\r\n[896683.255852] RDX: 0000000090eebb29 RSI: 00000000014000c0 RDI: 000000000001c6a0\r\n[896683.263170] RBP: ffffbb452dd03c70 R08: ffff92ddbfd5c6a0 R09: ffff92dd8b0037c0\r\n[896683.270487] R10: 0000000000000018 R11: ffff92dcc9abd7c0 R12: 00000000014000c0\r\n[896683.277805] R13: 00000000ffffffff R14: ffffffff9e3f9d2d R15: ffff92dd8b0037c0\r\n[896683.285123] FS:  00007f8398596740(0000) GS:ffff92ddbfd40000(0000) knlGS:0000000000000000\r\n[896683.293404] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[896683.299333] CR2: 0000000000000018 CR3: 000000063100a000 CR4: 00000000001406e0\r\n[896683.306659] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\n[896683.313976] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\r\n[896683.321294] Call Trace:\r\n[896683.323932]  alloc_vmap_area+0x8d/0x380\r\n[896683.327958]  __get_vm_area_node+0xb4/0x140\r\n[896683.332237]  __vmalloc_node_range+0x73/0x280\r\n[896683.336691]  ? _do_fork+0xe7/0x3f0\r\n[896683.340277]  ? copy_process.part.34+0x11f/0x1c20\r\n[896683.345089]  copy_process.part.34+0x61b/0x1c20\r\n[896683.349715]  ? _do_fork+0xe7/0x3f0\r\n[896683.353298]  ? dput+0x34/0x250\r\n[896683.356536]  _do_fork+0xe7/0x3f0\r\n[896683.359953]  ? ____fput+0xe/0x10\r\n[896683.363369]  ? task_work_run+0x83/0xa0\r\n[896683.367304]  SyS_clone+0x19/0x20\r\n[896683.370717]  do_syscall_64+0x5b/0xc0\r\n[896683.374478]  entry_SYSCALL64_slow_path+0x25/0x25\r\n[896683.379281] RIP: 0033:0x7f83960e03a1\r\n[896683.383043] RSP: 002b:00007ffd47e8b288 EFLAGS: 00000202 ORIG_RAX: 0000000000000038\r\n[896683.390798] RAX: ffffffffffffffda RBX: 00007f7fcc73f700 RCX: 00007f83960e03a1\r\n[896683.398119] RDX: 00007f7fcc73f9d0 RSI: 00007f7fcc73efb0 RDI: 00000000003d0f00\r\n[896683.405445] RBP: 0000000001da4a40 R08: 00007f7fcc73f700 R09: 00007f7fcc73f700\r\n[896683.412776] R10: 00007f7fcc73f9d0 R11: 0000000000000202 R12: 0000000000000000\r\n[896683.420106] R13: 00007ffd47e8b33f R14: 0000000000041000 R15: 00000000041ff5b0\r\n[896683.427437] Code: 49 63 51 1c 4c 89 d7 31 f6 4c 89 4d c0 4c 89 55 c8 e8 ae be 23 00 4c 8b 4d c0 4c 8b 55 c8 eb 33 49 63 41 20 48 8d 4a 01 49 8b 39 <49> 8b 1c 02 4c 89 d0 65 48 0f c7 0f 0f 94 c0 84 c0 0f 84 61 ff\r\n[896683.446529] RIP: kmem_cache_alloc_node_trace+0xd7/0x1d0 RSP: ffffbb452dd03c28\r\n[896683.453863] CR2: 0000000000000018\r\n[896683.457480] ---[ end trace 4de68d0227766245 ]---\r\n\r\n[896686.675733] BUG: unable to handle kernel NULL pointer dereference at 0000000000000018\r\n[896686.683803] IP: __kmalloc+0xbc/0x200\r\n[896686.687566] PGD b1f993067\r\n[896686.687567] PUD 6d76be067\r\n[896686.690460] PMD 0\r\n[896686.693351]\r\n[896686.697226] Oops: 0000 [#5] SMP\r\n[896686.700552] Modules linked in: ufs msdos xfs tcp_diag inet_diag binfmt_misc zfs(POE) zunicode(POE) zavl(PO) icp(POE) zcommon(POE) znvpair(POE) spl(OE) ip6table_filter ip6_tables iptable_filter ip_tables x_tables ppdev input_leds parport_pc parport pvpanic serio_raw ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi autofs4 btrfs raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c raid1 raid0 multipath linear crct10dif_pclmul crc32_pclmul ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd glue_helper cryptd psmouse nvme virtio_net nvme_core virtio_scsi\r\n[896686.758339] CPU: 5 PID: 1792 Comm: thread.rb:70 Tainted: P      D    OE   4.10.0-40-generic #44~16.04.1-Ubuntu\r\n[896686.768531] Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\r\n[896686.777935] task: ffff92dc81210000 task.stack: ffffbb45116fc000\r\n[896686.784043] RIP: 0010:__kmalloc+0xbc/0x200\r\n[896686.788323] RSP: 0018:ffffbb45116ffbe0 EFLAGS: 00010206\r\n[896686.793732] RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000090eebb2a\r\n[896686.801053] RDX: 0000000090eebb29 RSI: 0000000000000000 RDI: 000000000001c6a0\r\n[896686.808382] RBP: ffffbb45116ffc18 R08: ffff92ddbfd5c6a0 R09: ffff92dd8b0037c0\r\n[896686.815705] R10: 0000000000000018 R11: 0000000074656e64 R12: 00000000014080c0\r\n[896686.823035] R13: 0000000000000044 R14: ffffffff9e4d407e R15: ffff92dd8b0037c0\r\n[896686.830614] FS:  00007fd8677ff700(0000) GS:ffff92ddbfd40000(0000) knlGS:0000000000000000\r\n[896686.838886] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[896686.844815] CR2: 0000000000000018 CR3: 00000006c8c6d000 CR4: 00000000001406e0\r\n[896686.852139] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\n[896686.859458] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\r\n[896686.866778] Call Trace:\r\n[896686.869414]  ? ext4fs_dirhash+0xc2/0x2b0\r\n[896686.873525]  ext4_htree_store_dirent+0x3e/0x120\r\n[896686.878242]  htree_dirblock_to_tree+0xf3/0x290\r\n[896686.882871]  ? dput+0x34/0x250\r\n[896686.886110]  ext4_htree_fill_tree+0xb5/0x320\r\n[896686.890566]  ? kmem_cache_alloc_trace+0xdb/0x1c0\r\n[896686.895368]  ext4_readdir+0x701/0xa20\r\n[896686.899218]  iterate_dir+0x172/0x1a0\r\n[896686.902977]  SyS_getdents+0x99/0x120\r\n[896686.906738]  ? fillonedir+0x100/0x100\r\n[896686.910588]  entry_SYSCALL_64_fastpath+0x1e/0xad\r\n[896686.915391] RIP: 0033:0x7fd8793258eb\r\n[896686.919151] RSP: 002b:00007fd8677fc9c0 EFLAGS: 00000206 ORIG_RAX: 000000000000004e\r\n[896686.926906] RAX: ffffffffffffffda RBX: 0000000000000007 RCX: 00007fd8793258eb\r\n[896686.934228] RDX: 0000000000008000 RSI: 00007fd8674a5870 RDI: 0000000000000107\r\n[896686.941545] RBP: 00007fd878e21000 R08: 0000000000000000 R09: 0000000000000001\r\n[896686.948865] R10: 0000000000000000 R11: 0000000000000206 R12: 00007fd878e21010\r\n[896686.956183] R13: 00000000000004ed R14: 00007fd8677ff630 R15: 0000000000000002\r\n[896686.963503] Code: 08 65 4c 03 05 56 cd be 61 49 83 78 10 00 4d 8b 10 0f 84 d5 00 00 00 4d 85 d2 0f 84 cc 00 00 00 49 63 41 20 48 8d 4a 01 49 8b 39 <49> 8b 1c 02 4c 89 d0 65 48 0f c7 0f 0f 94 c0 84 c0 74 bb 49 63\r\n[896686.982568] RIP: __kmalloc+0xbc/0x200 RSP: ffffbb45116ffbe0\r\n[896686.988324] CR2: 0000000000000018\r\n[896686.991913] ---[ end trace 4de68d0227766246 ]---\r\n\r\n[896925.519989] BUG: unable to handle kernel NULL pointer dereference at 0000000000000018\r\n[896925.528065] IP: __kmalloc+0xbc/0x200\r\n[896925.531828] PGD b1f993067\r\n[896925.531830] PUD 6d76be067\r\n[896925.534721] PMD 0\r\n[896925.537613]\r\n[896925.541492] Oops: 0000 [#6] SMP\r\n[896925.544825] Modules linked in: ufs msdos xfs tcp_diag inet_diag binfmt_misc zfs(POE) zunicode(POE) zavl(PO) icp(POE) zcommon(POE) znvpair(POE) spl(OE) ip6table_filter ip6_tables iptable_filter ip_tables x_tables ppdev input_leds parport_pc parport pvpanic serio_raw ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi autofs4 btrfs raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c raid1 raid0 multipath linear crct10dif_pclmul crc32_pclmul ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd glue_helper cryptd psmouse nvme virtio_net nvme_core virtio_scsi\r\n[896925.602626] CPU: 5 PID: 649 Comm: jbd2/sda1-8 Tainted: P      D    OE   4.10.0-40-generic #44~16.04.1-Ubuntu\r\n[896925.612639] Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\r\n[896925.622579] task: ffff92dd7fd68000 task.stack: ffffbb4506f84000\r\n[896925.628697] RIP: 0010:__kmalloc+0xbc/0x200\r\n[896925.632984] RSP: 0018:ffffbb4506f878f8 EFLAGS: 00010206\r\n[896925.638395] RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000090eebb2a\r\n[896925.645723] RDX: 0000000090eebb29 RSI: 0000000000000000 RDI: 000000000001c6a0\r\n[896925.653044] RBP: ffffbb4506f87930 R08: ffff92ddbfd5c6a0 R09: ffff92dd8b0037c0\r\n[896925.660365] R10: 0000000000000018 R11: 0000000000000000 R12: 0000000001408040\r\n[896925.667686] R13: 0000000000000060 R14: ffffffff9e509751 R15: ffff92dd8b0037c0\r\n[896925.675008] FS:  0000000000000000(0000) GS:ffff92ddbfd40000(0000) knlGS:0000000000000000\r\n[896925.683285] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[896925.689215] CR2: 0000000000000018 CR3: 00000006c8c6d000 CR4: 00000000001406e0\r\n[896925.696542] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\n[896925.703870] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\r\n[896925.711196] Call Trace:\r\n[896925.713839]  ext4_find_extent+0x1f1/0x2f0\r\n[896925.718034]  ext4_ext_map_blocks+0x78/0x1c30\r\n[896925.722492]  ? kvm_sched_clock_read+0x1e/0x30\r\n[896925.728426]  ? pick_next_task_fair+0x35f/0x4d0\r\n[896925.734446]  ? __switch_to+0x23c/0x530\r\n[896925.739773]  ext4_map_blocks+0x40a/0x5f0\r\n[896925.744492]  ? bit_wait+0x60/0x60\r\n[896925.748864]  _ext4_get_block+0x92/0x100\r\n[896925.754461]  ? __slab_free+0x9a/0x2d0\r\n[896925.759700]  ext4_get_block+0x16/0x20\r\n[896925.764610]  generic_block_bmap+0x4e/0x70\r\n[896925.769069]  ? kmem_cache_free+0x1cd/0x1e0\r\n[896925.773351]  ext4_bmap+0x7d/0xe0\r\n[896925.776765]  bmap+0x1c/0x30\r\n[896925.779744]  jbd2_journal_bmap+0x2b/0x80\r\n[896925.783855]  jbd2_journal_next_log_block+0x6b/0x80\r\n[896925.788831]  jbd2_journal_get_descriptor_buffer+0x38/0xe0\r\n[896925.794416]  jbd2_journal_commit_transaction+0x9da/0x17f0\r\n[896925.800004]  ? update_curr+0xf3/0x180\r\n[896925.803852]  ? dequeue_task_fair+0x4ee/0xb20\r\n[896925.808309]  ? try_to_del_timer_sync+0x5a/0x80\r\n[896925.812940]  kjournald2+0xca/0x250\r\n[896925.816531]  ? wake_atomic_t_function+0x60/0x60\r\n[896925.821250]  kthread+0x109/0x140\r\n[896925.824674]  ? commit_timeout+0x10/0x10\r\n[896925.828744]  ? kthread_create_on_node+0x60/0x60\r\n[896925.833468]  ret_from_fork+0x2c/0x40\r\n[896925.837229] Code: 08 65 4c 03 05 56 cd be 61 49 83 78 10 00 4d 8b 10 0f 84 d5 00 00 00 4d 85 d2 0f 84 cc 00 00 00 49 63 41 20 48 8d 4a 01 49 8b 39 <49> 8b 1c 02 4c 89 d0 65 48 0f c7 0f 0f 94 c0 84 c0 74 bb 49 63\r\n[896925.856296] RIP: __kmalloc+0xbc/0x200 RSP: ffffbb4506f878f8\r\n[896925.862062] CR2: 0000000000000018\r\n[896925.865561] ---[ end trace 4de68d0227766247 ]---\r\n\r\n[897265.894190] BUG: unable to handle kernel NULL pointer dereference at 0000000000000018\r\n[897265.902259] IP: kmem_cache_alloc_node_trace+0xd7/0x1d0\r\n[897265.907582] PGD cf2fad067\r\n[897265.907583] PUD cf32f5067\r\n[897265.910474] PMD 0\r\n[897265.913369]\r\n[897265.917249] Oops: 0000 [#7] SMP\r\n[897265.920616] Modules linked in: ufs msdos xfs tcp_diag inet_diag binfmt_misc zfs(POE) zunicode(POE) zavl(PO) icp(POE) zcommon(POE) znvpair(POE) spl(OE) ip6table_filter ip6_tables iptable_filter ip_tables x_tables ppdev input_leds parport_pc parport pvpanic serio_raw ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi autofs4 btrfs raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c raid1 raid0 multipath linear crct10dif_pclmul crc32_pclmul ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd glue_helper cryptd psmouse nvme virtio_net nvme_core virtio_scsi\r\n[897265.978416] CPU: 5 PID: 1982 Comm: google_accounts Tainted: P      D    OE   4.10.0-40-generic #44~16.04.1-Ubuntu\r\n[897265.988882] Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\r\n[897265.998286] task: ffff92dd6b85d7c0 task.stack: ffffbb45077d8000\r\n[897266.004399] RIP: 0010:kmem_cache_alloc_node_trace+0xd7/0x1d0\r\n[897266.010244] RSP: 0018:ffffbb45077dbc28 EFLAGS: 00010246\r\n[897266.015656] RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000090eebb2a\r\n[897266.022980] RDX: 0000000090eebb29 RSI: 00000000014000c0 RDI: 000000000001c6a0\r\n[897266.030304] RBP: ffffbb45077dbc70 R08: ffff92ddbfd5c6a0 R09: ffff92dd8b0037c0\r\n[897266.037628] R10: 0000000000000018 R11: ffff92dd6b85d7c0 R12: 00000000014000c0\r\n[897266.044947] R13: 00000000ffffffff R14: ffffffff9e3f9d2d R15: ffff92dd8b0037c0\r\n[897266.052272] FS:  00007faa3c321700(0000) GS:ffff92ddbfd40000(0000) knlGS:0000000000000000\r\n[897266.060544] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[897266.066475] CR2: 0000000000000018 CR3: 0000000cf2d8d000 CR4: 00000000001406e0\r\n[897266.073797] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\n[897266.081115] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\r\n[897266.088433] Call Trace:\r\n[897266.091084]  alloc_vmap_area+0x8d/0x380\r\n[897266.095110]  __get_vm_area_node+0xb4/0x140\r\n[897266.099397]  __vmalloc_node_range+0x73/0x280\r\n[897266.103865]  ? _do_fork+0xe7/0x3f0\r\n[897266.107454]  ? copy_process.part.34+0x11f/0x1c20\r\n[897266.112256]  copy_process.part.34+0x61b/0x1c20\r\n[897266.116886]  ? _do_fork+0xe7/0x3f0\r\n[897266.120476]  ? do_wp_page+0x109/0x5d0\r\n[897266.124322]  ? handle_mm_fault+0x86b/0x1270\r\n[897266.128694]  ? kmem_cache_alloc+0xd7/0x1b0\r\n[897266.132978]  _do_fork+0xe7/0x3f0\r\n[897266.136396]  ? __do_page_fault+0x265/0x4e0\r\n[897266.140675]  SyS_clone+0x19/0x20\r\n[897266.144113]  do_syscall_64+0x5b/0xc0\r\n[897266.147897]  entry_SYSCALL64_slow_path+0x25/0x25\r\n[897266.152700] RIP: 0033:0x7faa3bbed41a\r\n[897266.156486] RSP: 002b:00007ffcadee05b0 EFLAGS: 00000246 ORIG_RAX: 0000000000000038\r\n[897266.164246] RAX: ffffffffffffffda RBX: 00007ffcadee05b0 RCX: 00007faa3bbed41a\r\n[897266.171571] RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000001200011\r\n[897266.178893] RBP: 00007ffcadee0600 R08: 00000000000007be R09: 00007faa3c321700\r\n[897266.186212] R10: 00007faa3c3219d0 R11: 0000000000000246 R12: 00000000000007be\r\n[897266.193531] R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000000\r\n[897266.200851] Code: 49 63 51 1c 4c 89 d7 31 f6 4c 89 4d c0 4c 89 55 c8 e8 ae be 23 00 4c 8b 4d c0 4c 8b 55 c8 eb 33 49 63 41 20 48 8d 4a 01 49 8b 39 <49> 8b 1c 02 4c 89 d0 65 48 0f c7 0f 0f 94 c0 84 c0 0f 84 61 ff\r\n[897266.219914] RIP: kmem_cache_alloc_node_trace+0xd7/0x1d0 RSP: ffffbb45077dbc28\r\n[897266.227244] CR2: 0000000000000018\r\n[897266.230844] ---[ end trace 4de68d0227766248 ]---\r\n\r\n[897266.237052] BUG: unable to handle kernel NULL pointer dereference at 0000000000000018\r\n[897266.245200] IP: kmem_cache_alloc_trace+0x7b/0x1c0\r\n[897266.250094] PGD 0\r\n[897266.250095]\r\n[897266.253978] Oops: 0000 [#8] SMP\r\n[897266.257310] Modules linked in: ufs msdos xfs tcp_diag inet_diag binfmt_misc zfs(POE) zunicode(POE) zavl(PO) icp(POE) zcommon(POE) znvpair(POE) spl(OE) ip6table_filter ip6_tables iptable_filter ip_tables x_tables ppdev input_leds parport_pc parport pvpanic serio_raw ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi autofs4 btrfs raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c raid1 raid0 multipath linear crct10dif_pclmul crc32_pclmul ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd glue_helper cryptd psmouse nvme virtio_net nvme_core virtio_scsi\r\n[897266.316492] CPU: 5 PID: 4102 Comm: kworker/5:2 Tainted: P      D    OE   4.10.0-40-generic #44~16.04.1-Ubuntu\r\n[897266.326596] Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\r\n[897266.336013] Workqueue: events cgroup_release_agent\r\n[897266.341002] task: ffff92d1282aba80 task.stack: ffffbb451b728000\r\n[897266.348506] RIP: 0010:kmem_cache_alloc_trace+0x7b/0x1c0\r\n[897266.353922] RSP: 0018:ffffbb451b72bd80 EFLAGS: 00010206\r\n[897266.360638] RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000090eebb2a\r\n[897266.367976] RDX: 0000000090eebb29 RSI: 00000000014080c0 RDI: 000000000001c6a0\r\n[897266.375305] RBP: ffffbb451b72bdc0 R08: ffff92ddbfd5c6a0 R09: ffff92dd8b0037c0\r\n[897266.384011] R10: 0000000000000018 R11: 000000000001d100 R12: 00000000014080c0\r\n[897266.391334] R13: ffffffff9e29f05e R14: 0000000000000001 R15: ffff92dd8b0037c0\r\n[897266.398655] FS:  0000000000000000(0000) GS:ffff92ddbfd40000(0000) knlGS:0000000000000000\r\n[897266.406932] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[897266.412865] CR2: 0000000000000018 CR3: 000000097f609000 CR4: 00000000001406e0\r\n[897266.420189] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\n[897266.427511] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\r\n[897266.434834] Call Trace:\r\n[897266.437478]  call_usermodehelper+0x3e/0xb0\r\n[897266.441771]  cgroup_release_agent+0x136/0x140\r\n[897266.447703]  process_one_work+0x16b/0x4a0\r\n[897266.451903]  worker_thread+0x4b/0x500\r\n[897266.457057]  kthread+0x109/0x140\r\n[897266.460473]  ? process_one_work+0x4a0/0x4a0\r\n[897266.464845]  ? kthread_create_on_node+0x60/0x60\r\n[897266.469567]  ret_from_fork+0x2c/0x40\r\n[897266.473331] Code: 08 65 4c 03 05 77 e3 be 61 49 83 78 10 00 4d 8b 10 0f 84 f0 00 00 00 4d 85 d2 0f 84 e7 00 00 00 49 63 41 20 48 8d 4a 01 49 8b 39 <49> 8b 1c 02 4c 89 d0 65 48 0f c7 0f 0f 94 c0 84 c0 74 bb 49 63\r\n[897266.493776] RIP: kmem_cache_alloc_trace+0x7b/0x1c0 RSP: ffffbb451b72bd80\r\n[897266.500665] CR2: 0000000000000018\r\n[897266.505472] ---[ end trace 4de68d0227766249 ]---\r\n\r\n[897570.084437] BUG: unable to handle kernel NULL pointer dereference at 0000000000000018\r\n[897570.092505] IP: __kmalloc+0xbc/0x200\r\n[897570.096266] PGD 0\r\n[897570.096266]\r\n[897570.100141] Oops: 0000 [#9] SMP\r\n[897570.103467] Modules linked in: ufs msdos xfs tcp_diag inet_diag binfmt_misc zfs(POE) zunicode(POE) zavl(PO) icp(POE) zcommon(POE) znvpair(POE) spl(OE) ip6table_filter ip6_tables iptable_filter ip_tables x_tables ppdev input_leds parport_pc parport pvpanic serio_raw ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi autofs4 btrfs raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c raid1 raid0 multipath linear crct10dif_pclmul crc32_pclmul ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd glue_helper cryptd psmouse nvme virtio_net nvme_core virtio_scsi\r\n[897570.161259] CPU: 5 PID: 25246 Comm: release-upgrade Tainted: P      D    OE   4.10.0-40-generic #44~16.04.1-Ubuntu\r\n[897570.171790] Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\r\n[897570.181190] task: ffff92dd746fd7c0 task.stack: ffffbb45100b4000\r\n[897570.187295] RIP: 0010:__kmalloc+0xbc/0x200\r\n[897570.191574] RSP: 0018:ffffbb45100b77f8 EFLAGS: 00010206\r\n[897570.196988] RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000090eebb2a\r\n[897570.204308] RDX: 0000000090eebb29 RSI: 0000000000000000 RDI: 000000000001c6a0\r\n[897570.211626] RBP: ffffbb45100b7830 R08: ffff92ddbfd5c6a0 R09: ffff92dd8b0037c0\r\n[897570.218943] R10: 0000000000000018 R11: 0000000000000040 R12: 0000000001408040\r\n[897570.226265] R13: 0000000000000060 R14: ffffffff9e509751 R15: ffff92dd8b0037c0\r\n[897570.233584] FS:  00007fa86cd10700(0000) GS:ffff92ddbfd40000(0000) knlGS:0000000000000000\r\n[897570.241856] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[897570.247786] CR2: 0000000000000018 CR3: 0000000b39b53000 CR4: 00000000001406e0\r\n[897570.255104] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\n[897570.262420] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\r\n[897570.269737] Call Trace:\r\n[897570.272376]  ext4_find_extent+0x1f1/0x2f0\r\n[897570.276571]  ext4_ext_map_blocks+0x78/0x1c30\r\n[897570.281027]  ? count_shadow_nodes+0xa0/0xa0\r\n[897570.285393]  ? __radix_tree_replace+0xf7/0x100\r\n[897570.290019]  ? mem_cgroup_commit_charge+0x7e/0x4d0\r\n[897570.295000]  ? page_cache_tree_insert+0xad/0x110\r\n[897570.299803]  ext4_map_blocks+0x40a/0x5f0\r\n[897570.303913]  ext4_mpage_readpages+0x365/0x9b0\r\n[897570.308453]  ext4_readpages+0x36/0x40\r\n[897570.312302]  __do_page_cache_readahead+0x19a/0x270\r\n[897570.317277]  ondemand_readahead+0x178/0x2a0\r\n[897570.321645]  page_cache_sync_readahead+0x2e/0x50\r\n[897570.326447]  generic_file_read_iter+0x6ab/0x900\r\n[897570.331161]  ext4_file_read_iter+0x37/0xb0\r\n[897570.335441]  new_sync_read+0xd0/0x120\r\n[897570.339287]  __vfs_read+0x26/0x40\r\n[897570.342785]  vfs_read+0x93/0x130\r\n[897570.346199]  prepare_binprm+0x10c/0x1f0\r\n[897570.350218]  do_execveat_common.isra.39+0x4b4/0x7a0\r\n[897570.355277]  SyS_execve+0x3a/0x50\r\n[897570.358779]  do_syscall_64+0x5b/0xc0\r\n[897570.362540]  entry_SYSCALL64_slow_path+0x25/0x25\r\n[897570.367340] RIP: 0033:0x7fa86c7f8777\r\n[897570.371096] RSP: 002b:00007fff21470028 EFLAGS: 00000246 ORIG_RAX: 000000000000003b\r\n[897570.378848] RAX: ffffffffffffffda RBX: 00005630d473db50 RCX: 00007fa86c7f8777\r\n[897570.386162] RDX: 00005630d473db38 RSI: 00005630d473db10 RDI: 00005630d473db50\r\n[897570.393478] RBP: 00005630d473db10 R08: 000000000000000e R09: 0000000000000001\r\n[897570.400794] R10: 0000000000000001 R11: 0000000000000246 R12: 00005630d473db38\r\n[897570.408109] R13: 0000000000000008 R14: 0400002000000001 R15: 0000000000000001\r\n[897570.415425] Code: 08 65 4c 03 05 56 cd be 61 49 83 78 10 00 4d 8b 10 0f 84 d5 00 00 00 4d 85 d2 0f 84 cc 00 00 00 49 63 41 20 48 8d 4a 01 49 8b 39 <49> 8b 1c 02 4c 89 d0 65 48 0f c7 0f 0f 94 c0 84 c0 74 bb 49 63\r\n[897570.434481] RIP: __kmalloc+0xbc/0x200 RSP: ffffbb45100b77f8\r\n[897570.440233] CR2: 0000000000000018\r\n[897570.443822] ---[ end trace 4de68d022776624a ]---\r\n\r\n[897570.504707] BUG: unable to handle kernel NULL pointer dereference at 0000000000000018\r\n[897570.513683] IP: __kmalloc+0xbc/0x200\r\n[897570.517447] PGD 0\r\n[897570.517448]\r\n[897570.521326] Oops: 0000 [#10] SMP\r\n[897570.524742] Modules linked in: ufs msdos xfs tcp_diag inet_diag binfmt_misc zfs(POE) zunicode(POE) zavl(PO) icp(POE) zcommon(POE) znvpair(POE) spl(OE) ip6table_filter ip6_tables iptable_filter ip_tables x_tables ppdev input_leds parport_pc parport pvpanic serio_raw ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi autofs4 btrfs raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c raid1 raid0 multipath linear crct10dif_pclmul crc32_pclmul ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd glue_helper cryptd psmouse nvme virtio_net nvme_core virtio_scsi\r\n[897570.583884] CPU: 5 PID: 25259 Comm: update-motd-fsc Tainted: P      D    OE   4.10.0-40-generic #44~16.04.1-Ubuntu\r\n[897570.594419] Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\r\n[897570.603825] task: ffff92dad81d57c0 task.stack: ffffbb45100e4000\r\n[897570.609933] RIP: 0010:__kmalloc+0xbc/0x200\r\n[897570.614225] RSP: 0018:ffffbb45100e77f8 EFLAGS: 00010206\r\n[897570.620941] RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000090eebb2a\r\n[897570.628274] RDX: 0000000090eebb29 RSI: 0000000000000000 RDI: 000000000001c6a0\r\n[897570.635599] RBP: ffffbb45100e7830 R08: ffff92ddbfd5c6a0 R09: ffff92dd8b0037c0\r\n[897570.644308] R10: 0000000000000018 R11: 0000000000000000 R12: 0000000001408040\r\n[897570.651717] R13: 0000000000000060 R14: ffffffff9e509751 R15: ffff92dd8b0037c0\r\n[897570.660347] FS:  00007f6a9f2a2700(0000) GS:ffff92ddbfd40000(0000) knlGS:0000000000000000\r\n[897570.668624] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[897570.675854] CR2: 0000000000000018 CR3: 0000000bb23d0000 CR4: 00000000001406e0\r\n[897570.683177] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\n[897570.690498] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\r\n[897570.699117] Call Trace:\r\n[897570.701756]  ext4_find_extent+0x1f1/0x2f0\r\n[897570.705950]  ext4_ext_map_blocks+0x78/0x1c30\r\n[897570.711822]  ? list_lru_del+0x59/0x120\r\n[897570.717145]  ? __radix_tree_replace+0x4e/0x100\r\n[897570.721781]  ? mem_cgroup_commit_charge+0x7e/0x4d0\r\n[897570.726766]  ? page_cache_tree_insert+0xad/0x110\r\n[897570.731580]  ext4_map_blocks+0x40a/0x5f0\r\n[897570.735701]  ext4_mpage_readpages+0x365/0x9b0\r\n[897570.741631]  ext4_readpages+0x36/0x40\r\n[897570.745486]  __do_page_cache_readahead+0x19a/0x270\r\n[897570.750465]  ondemand_readahead+0x178/0x2a0\r\n[897570.754839]  page_cache_sync_readahead+0x2e/0x50\r\n[897570.759644]  generic_file_read_iter+0x6ab/0x900\r\n[897570.764362]  ext4_file_read_iter+0x37/0xb0\r\n[897570.768646]  new_sync_read+0xd0/0x120\r\n[897570.772498]  __vfs_read+0x26/0x40\r\n[897570.776016]  vfs_read+0x93/0x130\r\n[897570.779442]  prepare_binprm+0x10c/0x1f0\r\n[897570.784854]  do_execveat_common.isra.39+0x4b4/0x7a0\r\n[897570.789924]  SyS_execve+0x3a/0x50\r\n[897570.794814]  do_syscall_64+0x5b/0xc0\r\n[897570.798581]  entry_SYSCALL64_slow_path+0x25/0x25\r\n[897570.803388] RIP: 0033:0x7f6a9ed8a777\r\n[897570.807153] RSP: 002b:00007ffd8daeebc8 EFLAGS: 00000246 ORIG_RAX: 000000000000003b\r\n[897570.814910] RAX: ffffffffffffffda RBX: 00005577a5c9be98 RCX: 00007f6a9ed8a777\r\n[897570.822244] RDX: 00005577a5c9be80 RSI: 00005577a5c9be40 RDI: 00005577a5c9be98\r\n[897570.829567] RBP: 00005577a5c9be40 R08: 000000000000000f R09: 0000000000000001\r\n[897570.836891] R10: 0000000000000001 R11: 0000000000000246 R12: 00005577a5c9be80\r\n[897570.844213] R13: 0000000000000005 R14: 0400002000000001 R15: 0000000000000001\r\n[897570.851539] Code: 08 65 4c 03 05 56 cd be 61 49 83 78 10 00 4d 8b 10 0f 84 d5 00 00 00 4d 85 d2 0f 84 cc 00 00 00 49 63 41 20 48 8d 4a 01 49 8b 39 <49> 8b 1c 02 4c 89 d0 65 48 0f c7 0f 0f 94 c0 84 c0 74 bb 49 63\r\n[897570.871997] RIP: __kmalloc+0xbc/0x200 RSP: ffffbb45100e77f8\r\n[897570.877759] CR2: 0000000000000018\r\n[897570.881328] ---[ end trace 4de68d022776624b ]---\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6945/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "G-man88": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6944", "title": "Zpool import taints kernel", "body": "Distribution Name   Artful Aardvark    | \r\nDistribution Version 17.10   | \r\nLinux Kernel   4.13.0-19-generic               | \r\nArchitecture                 | \r\nZFS Version  0.6.5.11-1ubuntu3          | \r\nSPL Version  0.6.5.11-1ubuntu1               | \r\n\r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\nI have an issue where my zpool taints my kernel. I was running on 17.04 because my Nas is running on Ryzen. I got it working for the most part without issue for a few months when yesterday it started to freeze so I rebooted the system, and it hanged for a while. I then manually turned it off but the zpool didn't mount so I shut it down again and I got Tainted: P error and it would hang. I then decided to upgrade to Ubuntu 17.10, but I'm getting the same issue The error I'm getting below It keeps repeating that error and just hangs.  Forgive me for not being well versed in how to submit these issues. If there is anything else you need please let me know and I'll post it. \r\n\r\n```\r\ntask txg_sync:2323 blocked for more than 120 seconds \r\n      Tainted: P                0          4.13.0-19-generic #22-Ubuntu\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6944/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "DurvalMenezes": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6938", "title": "Documentation error: conflicting information on the zpool manpage regarding removing log devices", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nZFS Version                  | 0.7.3-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Problem description \r\n\"man zpool\" shows conflicting information; please compare: \r\n\r\n       zpool remove pool device ...\r\n           Removes  the  specified  device  from the pool. This command currently only supports\r\n           removing hot spares, cache, **and log devices.** \r\n\r\nAnd \r\n\r\n       Intent Log\r\n        (...)\r\n        Log  devices can be **added, replaced, attached, detached, and imported** and exported as part\r\n        of the larger pool. Mirrored log devices can be removed by specifying the top-level mirror for the log.\r\n\r\nPlease note that the first paragraph states that log devices can be removed (with the `zpool remove` command) while the 2nd part (specific to the log devices) doesn't say it can, except if the log is mirrored (and in that case, I believe the right command would be `zpool detach`, not `zpool remove`, and therefore it should not be called \"removing\"). \r\n\r\n### Steps to reproduce\r\n\r\nJust install ZoL and issue a `man zpool`, and search for the above paragraphs. \r\n\r\nNOTE: this issue has been discussed in the zfs-discuss mailing list and there was consensus that it indeed is a bug to be reported, please see here: http://list.zfsonlinux.org/pipermail/zfs-discuss/2017-December/030031.html\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6938/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6813", "title": "After replacing a disk from the pool, every import starts a resilver?!", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues\r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking\r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n  \r\n### System information\r\n<!--  add version after \"|\" character -->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Devuan GNU/Linux (Debian systemd-less fork)\r\nDistribution Version    | 1.0 (jessie)\r\nLinux Kernel                 | 4.9.0-0.bpo.3-amd64 (from Debian Backports)\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.9-2~bpo8+1 (also from  Debian Backports)\r\nSPL Version                  | 0.6.5.9-1~bpo8+1 (ditto)\r\n<!--\r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version\r\n-->\r\n  \r\n### Describe the problem you're observing\r\nEvery time I import this pool (after having exported it previously with no issues), ZoL immediately starts a resilver. Waiting for the resilver to finish and then exporting and reimporting the pool again results in a new resilver, always with the same device being resilvered (ZFS_ARCHIVE_003B2 on the output below), this repeats at every import ad nauseam (I've gone through that 6 times already).\r\n\r\nI tried detaching this device from its mirror vdev, then \"zpool labelclear\"ing it, then attaching it back up and waiting for the resilver to finish. To no avail: when I export and reimport the pool back, again ZoL starts a new resilvering.\r\n\r\nFWIW, this disk (ZFS_ARCHIVE_003B2) was previously \"zpool replaced\" into the pool in order to replace a previous disk which was presenting hard read errors; the replace never finished, even after waiting hours, it still showed \"replacing\". I was able to export the pool and then detach the previous device and\r\nattach the new one in its place. Could this \"never finished replace\" be the source of this issue? Exporting and reimporting the pool *without* this device\r\n(ie, after detaching and before reattaching it) does *not* start a resilver in the pool, so definitely the problem is connected to this device.\r\n\r\nAlso, this disk size is 3TB, while the previous disk it replaced, as well as its pair in the mirror (ZFS_ARCHIVE_003A2 below) are both 2TB disks.\r\nAgain, could this be the cause of the problem?\r\n\r\nThe \"3 data errors\"shown at the end of the \"zpool status\" commands below are ancient (more than 1 year ago), from the time the pool was used to copy files off an ECC-less machine which later was diagnosed with RAM issues (the pool has been, since before the current issue, connected to another machine, which has ECC RAM). \r\n\r\nAs a final note, all the disk vdevs in this pool are actually LUKS devices running on top of the physical disks -- I've been doing this since forever and I'm pretty sure this has nothing to do with the problem.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nJust export and reimport the pool, eg:\r\n```\r\n  zpool status pool2015011801\r\n                      pool: pool2015011801\r\n                     state: ONLINE\r\n                    status: One or more devices has experienced an error resulting in data\r\n                            corruption.  Applications may be affected.\r\n                    action: Restore the file in question if possible.  Otherwise restore the\r\n                            entire pool from backup.\r\n                       see: http://zfsonlinux.org/msg/ZFS-8000-8A\r\n                      scan: resilvered 1.21T in 9h4m with 3 errors on Thu Nov  2 14:13:49 2017\r\n                    config:\r\n                            NAME                   STATE     READ WRITE CKSUM\r\n                            pool2015011801         ONLINE       0     0     3\r\n                              mirror-0             ONLINE       0     0     0\r\n                                ZFS_ARCHIVE_003A1  ONLINE       0     0     0\r\n                                ZFS_ARCHIVE_003B1  ONLINE       0     0     0\r\n                              mirror-1             ONLINE       0     0     6\r\n                                ZFS_ARCHIVE_003A2  ONLINE       0     0     6\r\n                                ZFS_ARCHIVE_003B2  ONLINE       0     0     6\r\n                    errors: 3 data errors, use '-v' for a list\r\n\r\n                zpool export pool2015011801\r\n\r\n                zpool import -d /dev/mapper\r\n                       pool: pool2015011801\r\n                         id: 7358349082112742918\r\n                      state: ONLINE\r\n                     status: The pool is formatted using a legacy on-disk version.\r\n                     action: The pool can be imported using its name or numeric identifier, though\r\n                            some features will not be available without an explicit 'zpool upgrade'.\r\n                     config:\r\n                            pool2015011801         ONLINE\r\n                              mirror-0             ONLINE\r\n                                ZFS_ARCHIVE_003A1  ONLINE\r\n                                ZFS_ARCHIVE_003B1  ONLINE\r\n                              mirror-1             ONLINE\r\n                                ZFS_ARCHIVE_003A2  ONLINE\r\n                                ZFS_ARCHIVE_003B2  ONLINE\r\n\r\n                zpool import -d /dev/mapper pool2015011801\r\n\r\n                zpool status pool2015011801\r\n                      pool: pool2015011801\r\n                     state: ONLINE\r\n                    status: One or more devices is currently being resilvered.  The pool will\r\n                            continue to function, possibly in a degraded state.\r\n                    action: Wait for the resilver to complete.\r\n                      scan: resilver in progress since Thu Nov  2 14:39:32 2017\r\n                        336M scanned out of 4.31T at 14.6M/s, 85h53m to go\r\n                        185M resilvered, 0.01% done\r\n                    config:\r\n                            NAME                   STATE     READ WRITE CKSUM\r\n                            pool2015011801         ONLINE       0     0     0\r\n                              mirror-0             ONLINE       0     0     0\r\n                                ZFS_ARCHIVE_003A1  ONLINE       0     0     0\r\n                                ZFS_ARCHIVE_003B1  ONLINE       0     0     0\r\n                              mirror-1             ONLINE       0     0     0\r\n                                ZFS_ARCHIVE_003A2  ONLINE       0     0     0\r\n                                ZFS_ARCHIVE_003B2  ONLINE       0     0     0  (resilvering)\r\n                    errors: 3 data errors, use '-v' for a list\r\n```\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\nSyslog records nothing relevant, only zed logging the resilver right after every \"zpool import\":\r\n\r\n```\r\nNov  2 14:39:28 redacted zed[4149]: Finished \"all-syslog.sh\" eid=63 pid=14120 exit=0\r\nNov  2 14:39:32 redacted zed[4149]: Invoking \"all-syslog.sh\" eid=64 pid=14135\r\nNov  2 14:39:32 redacted zed: eid=64 class=resilver.start pool=pool2015011801\r\nNov  2 14:39:32 redacted zed[4149]: Finished \"all-syslog.sh\" eid=64 pid=14135 exit=0\r\n```\r\n\r\n\r\n<!--\r\n*IMPORTANT* - Please mark logs and text output from terminal commands\r\nor else Github will not display them correctly.\r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6813/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6677", "title": "\"zpool remove\" on a log device being silently ignored", "body": "Hello, \r\n\r\nAs my original issue (#4067) was closed by a repo collaborator, I can't reopen it; so I'm opening this new one. \r\n\r\nJust to be clear, this issue is pretty much alive; the commands and output below are from Springdale EL6 (RHEL 6 clone) running kernel 4.1.12 (based off Oracle's UEK package) with ZoL 0.7.1-1: \r\n\r\n```\r\n[root@REDACTED ~]# zpool status\r\n  pool: pool01 \r\n state: ONLINE \r\nstatus: The pool is formatted using a legacy on-disk format.  The pool can\r\n        still be used, but some features are unavailable.\r\naction: Upgrade the pool using 'zpool upgrade'.  Once this is done, the\r\n        pool will no longer be accessible on software that does not support\r\n        feature flags.\r\n  scan: scrub repaired 0 in 27h7m with 0 errors on Sat Sep 23 14:06:22 2017\r\nconfig:\r\n\r\n        NAME                                               STATE     READ WRITE CKSUM\r\n        pool01                                             ONLINE       0     0     0\r\n          mirror-0                                         ONLINE       0     0     0\r\n            ata-HGST_HTS541010A9E680_REDACTED-part2  ONLINE       0     0     0\r\n            ata-HGST_HTS721010A9E630_REDACTED-part2  ONLINE       0     0     0\r\n        logs\r\n          ata-M4-CT256M4SSD3_REDACTED-part6    ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n\r\n[root@REDACTED ~]# zpool remove pool01 ata-M4-CT256M4SSD3_REDACTED-part6\r\n\r\n[root@REDACTED ~]# zpool status\r\n  pool: pool01\r\n state: ONLINE\r\nstatus: The pool is formatted using a legacy on-disk format.  The pool can\r\n        still be used, but some features are unavailable.\r\naction: Upgrade the pool using 'zpool upgrade'.  Once this is done, the\r\n        pool will no longer be accessible on software that does not support\r\n        feature flags.\r\n  scan: scrub repaired 0 in 27h7m with 0 errors on Sat Sep 23 14:06:22 2017\r\n\r\n        NAME                                               STATE     READ WRITE CKSUM\r\n        pool01                                             ONLINE       0     0     0\r\n          mirror-0                                         ONLINE       0     0     0\r\n            ata-HGST_HTS541010A9E680_REDACTED-part2  ONLINE       0     0     0\r\n            ata-HGST_HTS721010A9E630_REDACTED-part2  ONLINE       0     0     0\r\n        logs   \r\n          ata-M4-CT256M4SSD3_REDACTED-part6    ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n```\r\nPlease let me know if you need anything else, and I would appreciate being contacted before closing this again as \"stale\" with no further request to (or input from) me.\r\n\r\nThanks in advance, \r\n-- Durval.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6677/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Ufynjy": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6937", "title": "zfs behavior in case disks was deleted from system", "body": "I got case \"task txg_sync:17913 blocked for more than 120 seconds\" when pool's disks was deleted from system.\r\n\r\nfilename:       /lib/modules/3.19.0-58-generic/updates/dkms/zfs.ko\r\nversion:        0.6.5.11-1~trusty\r\nlicense:        CDDL\r\nauthor:         OpenZFS on Linux\r\ndescription:    ZFS\r\nsrcversion:     B9DC860740C2225772C1B4F\r\n\r\n`#zpool reopen test`\r\n`cannot reopen 'test': pool I/O is currently suspended`\r\n\r\nzpool can yet return info (no change of states for pool and disks)\r\n` #zpool status -P -v`\r\n`pool: test`\r\n`state: ONLINE`\r\n`status: One or more devices are faulted in response to IO failures.`\r\n`action: Make sure the affected devices are connected, then run 'zpool clear'.`\r\n`see: http://zfsonlinux.org/msg/ZFS-8000-JQ`\r\n`scan: none requested`\r\n`config:`\r\n\r\n`NAME                                              STATE     READ WRITE CKSUM`\r\n `   test                                              ONLINE       0    16     0`\r\n `     mirror-0                                        ONLINE       0    32     0`\r\n`       /dev/disk/by-id/scsi-35000cca0801eb30c-part1  ONLINE       3    32     0`\r\n`       /dev/disk/by-id/scsi-35000cca0801e6f20-part1  ONLINE       3    32     0`\r\n\r\n`errors: List of errors unavailable (insufficient privileges)`\r\n\r\nbut zfs hangs on create zvol (no checks and returns error before try to do, this output repeated some times in log every 2 minutes)\r\n\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.826762] Call Trace:\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.826775] [<ffffffff817ba349>] schedule+0x29/0x70\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.826823] [<ffffffffc0247afd>] cv_wait_common+0xcd/0x100 [spl]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.826834] [<ffffffff810b66a0>] ? prepare_to_wait_event+0x110/0x110\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.826847] [<ffffffffc0247b45>] __cv_wait+0x15/0x20 [spl]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.826947] [<ffffffffc0448abf>] txg_wait_synced+0xdf/0x120 [zfs]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.826997] [<ffffffffc0405269>] ? dmu_tx_commit+0x199/0x1d0 [zfs]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.827045] [<ffffffffc03fc180>] ? dmu_objset_clone_sync+0x280/0x280 [zfs]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.827099] [<ffffffffc0425ca7>] dsl_sync_task+0x177/0x270 [zfs]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.827144] [<ffffffffc03fbd30>] ? dmu_objset_clone+0x50/0x50 [zfs]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.827189] [<ffffffffc03fc180>] ? dmu_objset_clone_sync+0x280/0x280 [zfs]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.827232] [<ffffffffc03fbd30>] ? dmu_objset_clone+0x50/0x50 [zfs]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.827303] [<ffffffffc049dcd0>] ? zvol_is_zvol+0x40/0x40 [zfs]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.827348] [<ffffffffc03fbcd4>] dmu_objset_create+0x54/0x60 [zfs]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.827416] [<ffffffffc049dcd0>] ? zvol_is_zvol+0x40/0x40 [zfs]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.827486] [<ffffffffc0472b30>] zfs_ioc_create+0x110/0x260 [zfs]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.827553] [<ffffffffc0470703>] zfsdev_ioctl+0x213/0x500 [zfs]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.827562] [<ffffffff812028c8>] do_vfs_ioctl+0x2f8/0x510\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.827567] [<ffffffff81202b61>] SyS_ioctl+0x81/0xa0\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.827573] [<ffffffff817be78d>] system_call_fastpath+0x16/0x1b\r\n2017-10-18 13:20:34 0 3 localhost kernel:[ 9245.827666] INFO: task txg_sync:17913 blocked for more than 120 seconds.\r\n2017-10-18 13:20:34 0 3 localhost kernel:[ 9245.828450] Tainted: P OE 3.19.0-58-generic #64~14.04.1-Ubuntu\r\n2017-10-18 13:20:34 0 3 localhost kernel:[ 9245.829259] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n2017-10-18 13:20:34 0 6 localhost kernel:[ 9245.830108] txg_sync D ffff881057c8bb88 0 17913 2 0x00000000\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830113] ffff881057c8bb88 ffff88085b326bf0 0000000000013e80 ffff881057c8bfd8\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830117] 0000000000013e80 ffff88105c2e5850 ffff88085b326bf0 ffff88103b822a40\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830121] ffff88107fc94778 ffff88103b822a40 ffff88103b822a78 ffff88103b822a68\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830125] Call Trace:\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830131] [<ffffffff817ba650>] io_schedule+0xa0/0x130\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830149] [<ffffffffc0247ac8>] cv_wait_common+0x98/0x100 [spl]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830154] [<ffffffff810b66a0>] ? prepare_to_wait_event+0x110/0x110\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830167] [<ffffffffc0247b88>] __cv_wait_io+0x18/0x20 [spl]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830236] [<ffffffffc0491f23>] zio_wait+0xc3/0x150 [zfs]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830291] [<ffffffffc041e1d1>] dsl_pool_sync+0xb1/0x480 [zfs]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830352] [<ffffffffc0437de8>] spa_sync+0x378/0xb30 [zfs]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830358] [<ffffffff810b5ed8>] ? __wake_up_common+0x58/0x90\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830365] [<ffffffff8101eeb9>] ? read_tsc+0x9/0x10\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830427] [<ffffffffc0449755>] txg_sync_thread+0x3b5/0x610 [zfs]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830485] [<ffffffffc04493a0>] ? txg_fini+0x2d0/0x2d0 [zfs]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830500] [<ffffffffc0242ea1>] thread_generic_wrapper+0x71/0x80 [spl]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830511] [<ffffffffc0242e30>] ? __thread_exit+0x20/0x20 [spl]\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830521] [<ffffffff81094db2>] kthread+0xd2/0xf0\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830526] [<ffffffff81094ce0>] ? kthread_create_on_node+0x1c0/0x1c0\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830532] [<ffffffff817be6d8>] ret_from_fork+0x58/0x90\r\n2017-10-18 13:20:34 0 4 localhost kernel:[ 9245.830536] [<ffffffff81094ce0>] ? kthread_create_on_node+0x1c0/0x1c0", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Redsandro": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6934", "title": ".deb installation not enabling automatic imports", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Linux Mint 18.3 AKA Ubuntu 16.04.3\r\nDistribution Version    | Sylvia / Xenial\r\nLinux Kernel                 | 4.13\r\nArchitecture                 | x86_64\r\nZFS Version                  | ~0.7.0-17\r\nSPL Version                  | Multiple between 0.7.0-rc2 and current master 0.7.0-xxx\r\n\r\n### Describe the problem you're observing\r\n\r\nI've compiled (`make deb`) and installed multiple versions from git/master on multiple machines. Some Debian machines and some Ubuntu machines.\r\n\r\nNow unless I'm mistaken, I feel like the first couple of installs (Debian) everything worked out of the box. But lately, when testing on Ubuntu machines, I notice that no pool is being imported at startup. I have to do this manually.\r\n\r\nI feel like either:\r\n\r\n* The installation is no longer doing certain steps; or\r\n* The documentation doesn't mention certain manual steps that need to be taken; or\r\n* I'm making mistakes.\r\n\r\n#### Workaround\r\n\r\nAfter some googling around I read about steps like the following need to be taken:\r\n\r\n```\r\nsystemctl unmask zfs-*\r\nsystemctl enable zfs-import\r\nsystemctl enable zfs-mount\r\nsystemctl enable zfs-share\r\nsystemctl enable zfs-zed\r\nupdate-rc.d zfs-import enable\r\nupdate-rc.d zfs-mount enable\r\nupdate-rc.d zfs-share enable\r\nupdate-rc.d zfs-zed enable\r\n```\r\n\r\nI'm reasonably sure I didn't have to do that before. What changed? Is it 'me' or 'you'?", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6934/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wenerme": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6930", "title": "zfs-zed init script will cause dead loop in openrc", "body": "https://github.com/OpenRC/openrc/issues/168#issuecomment-349870167\r\n\r\n```bash\r\n\tif type status_of_proc > /dev/null 2>&1 ; then\r\n\t\t# LSB functions\r\n\t\tstatus_of_proc \"$DAEMON_NAME\" \"$DAEMON_BIN\"\r\n\t\treturn $?\r\n\telif type status > /dev/null 2>&1 ; then\r\n\t\t# Fedora/RedHat functions\r\n\t\tstatus -p \"$PIDFILE\" \"$DAEMON_NAME\"\r\n\t\treturn $?\r\n\telse\r\n\t\t# Unsupported\r\n\t\treturn 3\r\n\tfi\r\n```\r\n\r\nthere is no status_of_proc or status in openrc, status is alias of do_status, this will cause dead loop, then segfault.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6930/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "24x7ict": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6923", "title": "invalid backup stream when having multiple datasets", "body": "### System information\r\n\r\nWe have multiple servers with this issue, from Debian 8 to 9 with old and new versions of ZFS.\r\n\r\nZFS version:        0.6.5.7-8-jessie and higher\r\nSPL version:        0.6.5.7-5-jessie and higher\r\n\r\nDebian 8 and 9, different kernels\r\n\r\nWe use the following back-up routine on all servers:\r\n\r\nEvery server has a single ZPOOL that is replicated to a standby back-up server AND a off-site back-up server. This works without any problems.\r\n\r\nWhenever we expand the server with a second ZPOOL and apply the same back-up routine, we constantly get \"invalid backup stream\" errors.\r\n\r\n**So working situation:**\r\nZFS storage server\r\nFirst ZPOOL \r\n-> ZFS snapshot replica to standby server\r\nzfs send hdd/images@test1 hdd/images@test2 | ssh standby-server zfs receive data/backup-hdd\r\n-> ZFS snapshot replica to off-site server\r\nzfs send hdd/images@test1 hdd/images@test2 | ssh offsite-server zfs receive data/backup-hdd\r\n\r\n**Non-working situation:**\r\nZFS storage server\r\nFirst ZPOOL \r\n-> ZFS snapshot replica to standby server\r\nzfs send hdd/images@test1 hdd/images@test2 | ssh standby-server zfs receive data/backup-hdd\r\n-> ZFS snapshot replica to off-site server\r\nzfs send hdd/images@test1 hdd/images@test2 | ssh offsite-server zfs receive data/backup-hdd\r\nSecond ZPOOL \r\n-> ZFS snapshot replica to standby server\r\nzfs send ssd/images@test1 ssd/images@test2 | ssh standby-server zfs receive data/backup-ssd\r\n-> ZFS snapshot replica to off-site server\r\nzfs send ssd/images@test1 ssd/images@test2 | ssh offsite-server zfs receive data/backup-ssd\r\n\r\nWe schedule back-up every hour, so sometimes multiple are active.\r\n\r\nWe also have the invalid backup stream error when 2 ZFS servers are sending ZFS snapshots to each other every hour and sometimes overlap.\r\n\r\nSo ZFS server 1 <-> ZFS server 2\r\nZFS01: zfs send hdd/images@test1 hdd/images@test2 | ssh ZFS02 zfs receive hdd/backup-zfs01\r\nZFS02: zfs send hdd/images@test1 hdd/images@test2 | ssh ZFS01 zfs receive hdd/backup-zfs02\r\n\r\nI think it's a combination of the following:\r\n- 2 datasets on source server\r\n- Making a snapshot replica to a single backup server at the same time\r\n- Maybe fail because both zpools from source server go to same zpool on backup server (only datasets are different)?\r\n\r\nPlease let me know if this can be solved.\r\n\r\nI will try the following now and let you know the results:\r\n\r\nThe ZFS server with 2 ZPOOLs will back-up every ZPOOL to a different back-up server. So ZPOOL hdd will go to backup-server01 and ZPOOL ssd will go to backup-server02.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6923/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "PenegalECI": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6922", "title": "manpages: clarify which options allow comma-separated values", "body": "Type                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian\r\nDistribution Version    | Stretch\r\nLinux Kernel                 | 4.9.0\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.6.5.9-5\r\nSPL Version                  | 0.6.5.9-1\r\n\r\nAs I understand the manpages source code, the problem is still present in upstream.\r\n\r\n### Describe the problem you're observing\r\nThere are inconsistencies in the manpages about which options allow a comma-separated values list, and which do not allow this format.\r\n\r\n### Describe how to reproduce the problem\r\nRead the `zpool` manpage: the `zpool create` section header does not state that the `-o` option supports this format, and its `-o` subsection does not state it either. Nevertheless, its `-R` subsection does state that it is _equivalent to \"-o cachefile=none,altroot=root\"_.\r\n\r\nIt is likely that this problem lies at other places in the manpages, but I do not have enough experience in the `zfs` and `zpool` commands to find it by reading it, nor in manpages source to correct it. It is possible that this format is always accepted for `-o` switches, but, still, the manpages should reflect that possibility whenever possible.\r\n\r\nAs a side note, if this format is currently not accepted by the `-o` switch of some `zfs` or `zpool` commands, it could be useful to allow these commands to accept it, in order to uniformize both commands and manpages.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6922/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tjikkun": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6918", "title": "Directory access blocked in D state until reboot", "body": "### System information\r\n\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  CloudLinux Server\r\nDistribution Version    | 6.9\r\nLinux Kernel                 | Linux 2.6.32-673.26.1.lve1.4.27.el6.x86_64 #1 SMP Sun May 7 19:22:54 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.1-1\r\nSPL Version                  | 0.7.1-1\r\n\r\n### Describe the problem you're observing\r\nWe occasionally see on a \"random\" one of our servers that a \"random\" directory gets locked. All processes trying to access it from then on end in D state.\r\n\r\n### Describe how to reproduce the problem\r\nI have no certain way of reproducing yet. I just wait until one of our servers hits the issue.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\ncat /proc/$pid/stack of oldest in D state:\r\n```\r\n[<ffffffffa0281ef1>] cv_wait_common+0xb1/0x130 [spl]\r\n[<ffffffffa0281f88>] __cv_wait_io+0x18/0x20 [spl]\r\n[<ffffffffa03f85cb>] zio_wait+0xfb/0x180 [zfs]\r\n[<ffffffffa0337ab9>] dbuf_read+0x6e9/0x970 [zfs]\r\n[<ffffffffa0340cc8>] dmu_buf_hold_by_dnode+0x68/0x90 [zfs]\r\n[<ffffffffa03b5474>] zap_get_leaf_byblk+0x94/0x260 [zfs]\r\n[<ffffffffa03b59e8>] zap_deref_leaf+0xc8/0xe0 [zfs]\r\n[<ffffffffa03b5b20>] fzap_cursor_retrieve+0x120/0x270 [zfs]\r\n[<ffffffffa03bc4ab>] zap_cursor_retrieve+0x13b/0x2c0 [zfs]\r\n[<ffffffffa03e34ce>] zfs_readdir+0x16e/0x4d0 [zfs]\r\n[<ffffffffa03fdcd3>] zpl_readdir+0x73/0xb0 [zfs]\r\n[<ffffffff811d1530>] vfs_readdir+0xc0/0xe0\r\n[<ffffffff811d16b9>] sys_getdents+0x89/0xf0\r\n[<ffffffff8100b1a2>] system_call_fastpath+0x16/0x1b\r\n[<ffffffffffffffff>] 0xffffffffffffffff\r\n```\r\nsome other procs in D state:\r\n```\r\n[<ffffffff811c9ce3>] do_lookup+0x153/0x270\r\n[<ffffffff811cadd0>] __link_path_walk+0x9b0/0x1190\r\n[<ffffffff811cb81a>] path_walk+0x6a/0xe0\r\n[<ffffffff811cbbab>] filename_lookup+0x6b/0xc0\r\n[<ffffffff811cced9>] user_path_at+0x59/0xa0\r\n[<ffffffff811bf520>] vfs_fstatat+0x50/0xb0\r\n[<ffffffff811bf5bb>] vfs_stat+0x1b/0x20\r\n[<ffffffff811bf784>] sys_newstat+0x24/0x50\r\n[<ffffffff8100b1a2>] system_call_fastpath+0x16/0x1b\r\n[<ffffffffffffffff>] 0xffffffffffffffff\r\n\r\n[<ffffffff811c9ce3>] do_lookup+0x153/0x270\r\n[<ffffffff811cadd0>] __link_path_walk+0x9b0/0x1190\r\n[<ffffffff811cb81a>] path_walk+0x6a/0xe0\r\n[<ffffffff811cbbab>] filename_lookup+0x6b/0xc0\r\n[<ffffffff811cced9>] user_path_at+0x59/0xa0\r\n[<ffffffff811bf520>] vfs_fstatat+0x50/0xb0\r\n[<ffffffff811bf5bb>] vfs_stat+0x1b/0x20\r\n[<ffffffff811bf784>] sys_newstat+0x24/0x50\r\n[<ffffffff8100b1a2>] system_call_fastpath+0x16/0x1b\r\n[<ffffffffffffffff>] 0xffffffffffffffff\r\n\r\n[<ffffffff811d14f8>] vfs_readdir+0x88/0xe0\r\n[<ffffffff811d16b9>] sys_getdents+0x89/0xf0\r\n[<ffffffff8100b1a2>] system_call_fastpath+0x16/0x1b\r\n[<ffffffffffffffff>] 0xffffffffffffffff\r\n\r\n```\r\nI can get other info on request, a next time I can run diagnostic commands that you would like.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6918/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6612", "title": "Kernel panic on unmount: Dentry ffff881f3294a198{i=4,n=/}  still in use (1) [unmount of zfs zfs]", "body": "\r\n\r\n<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  CloudLinux (EL clone)\r\nDistribution Version    | 6.9\r\nLinux Kernel                 | 2.6.32-673.26.1.lve1.4.32.el6.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.1-1 (RPM zfs-0.7.1-1.el6.x86_64)\r\nSPL Version                  | 0.7.1-1 (RPM spl-0.7.1-1.el6.x86_64)\r\n\r\n### Describe the problem you're observing\r\nAfter our upgrade from 0.6.4.2 to 0.7.1 we occasionally see the below kernel panics.\r\n\r\n### Describe how to reproduce the problem\r\nI can't reproduce reliably yet, but it happens on unmounting, possibly only on unmounting of a cloned FS.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\n```\r\nSep  4 10:32:00 s220 [365122.312911] BUG: Dentry ffff881f3294a198{i=4,n=/} still in use (1) [unmount of zfs zfs]\r\nSep  4 10:32:00 s220 [365122.313188] ------------[ cut here ]------------\r\nSep  4 10:32:00 s220 [365122.313315] kernel BUG at fs/dcache.c:849!\r\nSep  4 10:32:00 s220 [365122.313440] invalid opcode: 0000 [#1] \r\nSep  4 10:32:00 s220 SMP \r\nSep  4 10:32:00 s220 \r\nSep  4 10:32:00 s220 [365122.313571] last sysfs file: /sys/devices/system/cpu/online\r\nSep  4 10:32:00 s220 [365122.313700] CPU 15 \r\nSep  4 10:32:00 s220 \r\nSep  4 10:32:00 s220 [365122.313707] Modules linked in:\r\nSep  4 10:32:00 s220 cls_fw\r\nSep  4 10:32:00 s220 sch_sfq\r\nSep  4 10:32:00 s220 sch_htb\r\nSep  4 10:32:00 s220 kcare\r\nSep  4 10:32:00 s220 (U)\r\nSep  4 10:32:00 s220 des_generic\r\nSep  4 10:32:00 s220 ecb\r\nSep  4 10:32:00 s220 md4\r\nSep  4 10:32:00 s220 nls_utf8\r\nSep  4 10:32:00 s220 cifs\r\nSep  4 10:32:00 s220 ip6t_LOG\r\nSep  4 10:32:00 s220 ip6t_REJECT\r\nSep  4 10:32:00 s220 nf_conntrack_ipv6\r\nSep  4 10:32:00 s220 nf_defrag_ipv6\r\nSep  4 10:32:00 s220 ip6table_filter\r\nSep  4 10:32:00 s220 xt_set\r\nSep  4 10:32:00 s220 ip_set_hash_ip\r\nSep  4 10:32:00 s220 ip_set_hash_net\r\nSep  4 10:32:00 s220 ip_set\r\nSep  4 10:32:00 s220 nfnetlink\r\nSep  4 10:32:00 s220 xt_owner\r\nSep  4 10:32:00 s220 nf_nat_ftp\r\nSep  4 10:32:00 s220 ipt_REDIRECT\r\nSep  4 10:32:00 s220 xt_conntrack\r\nSep  4 10:32:00 s220 nf_conntrack_ftp\r\nSep  4 10:32:00 s220 ipt_REJECT\r\nSep  4 10:32:00 s220 ipt_LOG\r\nSep  4 10:32:00 s220 xt_limit\r\nSep  4 10:32:00 s220 iptable_filter\r\nSep  4 10:32:00 s220 xt_multiport\r\nSep  4 10:32:00 s220 iptable_nat\r\nSep  4 10:32:00 s220 nf_nat\r\nSep  4 10:32:00 s220 nf_conntrack_ipv4\r\nSep  4 10:32:00 s220 nf_conntrack\r\nSep  4 10:32:00 s220 nf_defrag_ipv4\r\nSep  4 10:32:00 s220 netconsole\r\nSep  4 10:32:00 s220 configfs\r\nSep  4 10:32:00 s220 8021q\r\nSep  4 10:32:00 s220 garp\r\nSep  4 10:32:00 s220 stp\r\nSep  4 10:32:00 s220 llc\r\nSep  4 10:32:00 s220 bonding\r\nSep  4 10:32:00 s220 kmodlve\r\nSep  4 10:32:00 s220 (U)\r\nSep  4 10:32:00 s220 ip6table_mangle\r\nSep  4 10:32:00 s220 ip6_tables\r\nSep  4 10:32:00 s220 ipv6\r\nSep  4 10:32:00 s220 iptable_mangle\r\nSep  4 10:32:00 s220 xt_MARK\r\nSep  4 10:32:00 s220 ip_tables\r\nSep  4 10:32:00 s220 dm_round_robin\r\nSep  4 10:32:00 s220 dm_multipath\r\nSep  4 10:32:00 s220 iTCO_wdt\r\nSep  4 10:32:00 s220 iTCO_vendor_support\r\nSep  4 10:32:00 s220 serio_raw\r\nSep  4 10:32:00 s220 sb_edac\r\nSep  4 10:32:00 s220 edac_core\r\nSep  4 10:32:00 s220 zfs(P)\r\nSep  4 10:32:00 s220 (U)\r\nSep  4 10:32:00 s220 zcommon(P)\r\nSep  4 10:32:00 s220 (U)\r\nSep  4 10:32:00 s220 znvpair(P)\r\nSep  4 10:32:00 s220 (U)\r\nSep  4 10:32:00 s220 icp(P)\r\nSep  4 10:32:00 s220 (U)\r\nSep  4 10:32:00 s220 spl\r\nSep  4 10:32:00 s220 (U)\r\nSep  4 10:32:00 s220 zlib_deflate\r\nSep  4 10:32:00 s220 zavl(P)\r\nSep  4 10:32:00 s220 (U)\r\nSep  4 10:32:00 s220 zunicode(P)\r\nSep  4 10:32:00 s220 (U)\r\nSep  4 10:32:00 s220 ipmi_devintf\r\nSep  4 10:32:00 s220 ipmi_si\r\nSep  4 10:32:00 s220 ipmi_msghandler\r\nSep  4 10:32:00 s220 i2c_i801\r\nSep  4 10:32:00 s220 lpc_ich\r\nSep  4 10:32:00 s220 mfd_core\r\nSep  4 10:32:00 s220 joydev\r\nSep  4 10:32:00 s220 ioatdma\r\nSep  4 10:32:00 s220 ses\r\nSep  4 10:32:00 s220 enclosure\r\nSep  4 10:32:00 s220 sg\r\nSep  4 10:32:00 s220 acpi_pad\r\nSep  4 10:32:00 s220 ixgbe\r\nSep  4 10:32:00 s220 (U)\r\nSep  4 10:32:00 s220 igb\r\nSep  4 10:32:00 s220 dca\r\nSep  4 10:32:00 s220 i2c_algo_bit\r\nSep  4 10:32:00 s220 i2c_core\r\nSep  4 10:32:00 s220 ptp\r\nSep  4 10:32:00 s220 pps_core\r\nSep  4 10:32:00 s220 shpchp\r\nSep  4 10:32:00 s220 ext4\r\nSep  4 10:32:00 s220 jbd2\r\nSep  4 10:32:00 s220 mbcache\r\nSep  4 10:32:00 s220 raid1\r\nSep  4 10:32:00 s220 isci\r\nSep  4 10:32:00 s220 libsas\r\nSep  4 10:32:00 s220 sd_mod\r\nSep  4 10:32:00 s220 crc_t10dif\r\nSep  4 10:32:00 s220 mpt2sas\r\nSep  4 10:32:00 s220 scsi_transport_sas\r\nSep  4 10:32:00 s220 raid_class\r\nSep  4 10:32:00 s220 wmi\r\nSep  4 10:32:00 s220 dm_mirror\r\nSep  4 10:32:00 s220 dm_region_hash\r\nSep  4 10:32:00 s220 dm_log\r\nSep  4 10:32:00 s220 dm_mod\r\nSep  4 10:32:00 s220 [last unloaded: scsi_wait_scan]\r\nSep  4 10:32:00 s220 \r\nSep  4 10:32:00 s220 [365122.315936] \r\nSep  4 10:32:00 s220 [365122.316058] Pid: 768609, comm: umount veid: 0 Tainted: P           -- ------------    2.6.32-673.26.1.lve1.4.27.el6.x86_64 #1 042stab116_2\r\nSep  4 10:32:00 s220 Supermicro X9DRW-3LN4F+/X9DRW-3TF+\r\nSep  4 10:32:00 s220 /X9DRW-3LN4F+/X9DRW-3TF+\r\nSep  4 10:32:00 s220 \r\nSep  4 10:32:00 s220 [365122.316442] RIP: 0010:[<ffffffff811d5ce8>] \r\nSep  4 10:32:00 s220 [<ffffffff811d5ce8>] shrink_dcache_for_umount_subtree+0x298/0x2a0\r\nSep  4 10:32:00 s220 [365122.316702] RSP: 0018:ffff881092797dc8  EFLAGS: 00010292\r\nSep  4 10:32:00 s220 [365122.316830] RAX: 0000000000000062 RBX: ffff881f3294a198 RCX: 000000000000eba4\r\nSep  4 10:32:00 s220 [365122.317075] RDX: 0000000000000000 RSI: 0000000000000046 RDI: 0000000000000246\r\nSep  4 10:32:00 s220 [365122.317322] RBP: ffff881092797e08 R08: 0000000000000001 R09: 0000000000000000\r\nSep  4 10:32:00 s220 [365122.317572] R10: 000000002b300086 R11: ffff8818729c0090 R12: 0000000000000000\r\nSep  4 10:32:00 s220 [365122.317821] R13: ffffffff81a86380 R14: ffff882c4ed72c00 R15: ffff881f3294a208\r\nSep  4 10:32:00 s220 [365122.318066] FS:  00007fe23f5bb740(0000) GS:ffff88010c440000(0000) knlGS:0000000000000000\r\nSep  4 10:32:00 s220 [365122.318314] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nSep  4 10:32:00 s220 [365122.318443] CR2: 00007fe23ec368d5 CR3: 0000000f9db1e000 CR4: 00000000000407e0\r\nSep  4 10:32:00 s220 [365122.318686] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\nSep  4 10:32:00 s220 [365122.318930] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400\r\nSep  4 10:32:00 s220 [365122.319176] Process umount (pid: 768609, veid: 0, threadinfo ffff881092794000, task ffff881643de4ea0)\r\nSep  4 10:32:00 s220 [365122.319422] Stack:\r\nSep  4 10:32:00 s220 [365122.319542]  ffff882c4ed72e80\r\nSep  4 10:32:00 s220 0000000000000202\r\nSep  4 10:32:00 s220 ffff881092797e48\r\nSep  4 10:32:00 s220 ffff882c4ed72c00\r\nSep  4 10:32:00 s220 \r\nSep  4 10:32:00 s220 [365122.319679] <d>\r\nSep  4 10:32:00 s220 ffffffffa048d500\r\nSep  4 10:32:00 s220 ffffffff81c1dfc0\r\nSep  4 10:32:00 s220 ffff882c4ed72c00\r\nSep  4 10:32:00 s220 0000000000000000\r\nSep  4 10:32:00 s220 \r\nSep  4 10:32:00 s220 [365122.319936] <d>\r\nSep  4 10:32:00 s220 ffff881092797e28\r\nSep  4 10:32:00 s220 ffffffff811d5d26\r\nSep  4 10:32:00 s220 ffff88187fc78380\r\nSep  4 10:32:00 s220 ffff882c4ed72c00\r\nSep  4 10:32:00 s220 \r\nSep  4 10:32:00 s220 [365122.320308] Call Trace:\r\nSep  4 10:32:00 s220 [365122.320443]  [<ffffffff811d5d26>] shrink_dcache_for_umount+0x36/0x60\r\nSep  4 10:32:00 s220 [365122.320577]  [<ffffffff811bc133>] generic_shutdown_super+0x23/0x100\r\nSep  4 10:32:00 s220 [365122.320711]  [<ffffffff811bc2a0>] kill_anon_super+0x40/0x80\r\nSep  4 10:32:00 s220 [365122.320889]  [<ffffffffa0449e5e>] zpl_kill_sb+0x1e/0x30 [zfs]\r\nSep  4 10:32:00 s220 [365122.321020]  [<ffffffff811bc769>] deactivate_super+0x79/0xa0\r\nSep  4 10:32:00 s220 [365122.321152]  [<ffffffff811ded5f>] mntput_no_expire+0xbf/0x110\r\nSep  4 10:32:00 s220 [365122.321283]  [<ffffffff811df9e2>] sys_umount+0x82/0x3d0\r\nSep  4 10:32:00 s220 [365122.321418]  [<ffffffff8100b1a2>] system_call_fastpath+0x16/0x1b\r\nSep  4 10:32:00 s220 [365122.321551] Code: \r\nSep  4 10:32:00 s220 50 \r\nSep  4 10:32:00 s220 30 \r\nSep  4 10:32:00 s220 4c \r\nSep  4 10:32:00 s220 8b \r\nSep  4 10:32:00 s220 0a \r\nSep  4 10:32:00 s220 31 \r\nSep  4 10:32:00 s220 d2 \r\nSep  4 10:32:00 s220 48 \r\nSep  4 10:32:00 s220 85 \r\nSep  4 10:32:00 s220 f6 \r\nSep  4 10:32:00 s220 74 \r\nSep  4 10:32:00 s220 04 \r\nSep  4 10:32:00 s220 48 \r\nSep  4 10:32:00 s220 8b \r\nSep  4 10:32:00 s220 56 \r\nSep  4 10:32:00 s220 40 \r\nSep  4 10:32:00 s220 48 \r\nSep  4 10:32:00 s220 05 \r\nSep  4 10:32:00 s220 80 \r\nSep  4 10:32:00 s220 02 \r\nSep  4 10:32:00 s220 00 \r\nSep  4 10:32:00 s220 00 \r\nSep  4 10:32:00 s220 48 \r\nSep  4 10:32:00 s220 89 \r\nSep  4 10:32:00 s220 de \r\nSep  4 10:32:00 s220 48 \r\nSep  4 10:32:00 s220 c7 \r\nSep  4 10:32:00 s220 c7 \r\nSep  4 10:32:00 s220 c0 \r\nSep  4 10:32:00 s220 eb \r\nSep  4 10:32:00 s220 7d \r\nSep  4 10:32:00 s220 81 \r\nSep  4 10:32:00 s220 48 \r\nSep  4 10:32:00 s220 89 \r\nSep  4 10:32:00 s220 04 \r\nSep  4 10:32:00 s220 24 \r\nSep  4 10:32:00 s220 31 \r\nSep  4 10:32:00 s220 c0 \r\nSep  4 10:32:00 s220 e8 \r\nSep  4 10:32:00 s220 4e \r\nSep  4 10:32:00 s220 0d \r\nSep  4 10:32:00 s220 37 \r\nSep  4 10:32:00 s220 00 \r\nSep  4 10:32:00 s220 0b \r\nSep  4 10:32:00 s220 eb \r\nSep  4 10:32:00 s220 fe \r\nSep  4 10:32:00 s220 0f \r\nSep  4 10:32:00 s220 0b \r\nSep  4 10:32:00 s220 eb \r\nSep  4 10:32:00 s220 fe \r\nSep  4 10:32:00 s220 55 \r\nSep  4 10:32:00 s220 48 \r\nSep  4 10:32:00 s220 89 \r\nSep  4 10:32:00 s220 e5 \r\nSep  4 10:32:00 s220 53 \r\nSep  4 10:32:00 s220 48 \r\nSep  4 10:32:00 s220 83 \r\nSep  4 10:32:00 s220 ec \r\nSep  4 10:32:00 s220 08 \r\nSep  4 10:32:00 s220 0f \r\nSep  4 10:32:00 s220 1f \r\nSep  4 10:32:00 s220 44 \r\nSep  4 10:32:00 s220 00 \r\nSep  4 10:32:00 s220 \r\nSep  4 10:32:00 s220 [365122.322133] RIP \r\nSep  4 10:32:00 s220 [<ffffffff811d5ce8>] shrink_dcache_for_umount_subtree+0x298/0x2a0\r\nSep  4 10:32:00 s220 [365122.322382]  RSP <ffff881092797dc8>\r\n```\r\n```\r\nSep  5 13:55:19 s160 [1064298.449926] BUG: Dentry ffff88011c815438{i=4,n=/} still in use (1) [unmount of zfs zfs]\r\nSep  5 13:55:19 s160 [1064298.450228] ------------[ cut here ]------------\r\nSep  5 13:55:19 s160 [1064298.450361] kernel BUG at fs/dcache.c:849!\r\nSep  5 13:55:19 s160 [1064298.450492] invalid opcode: 0000 [#1] \r\nSep  5 13:55:19 s160 SMP \r\nSep  5 13:55:19 s160 \r\nSep  5 13:55:19 s160 [1064298.450634] last sysfs file: /sys/devices/system/cpu/online\r\nSep  5 13:55:19 s160 [1064298.450767] CPU 10 \r\nSep  5 13:55:19 s160 \r\nSep  5 13:55:19 s160 [1064298.450777] Modules linked in:\r\nSep  5 13:55:19 s160 cls_fw\r\nSep  5 13:55:19 s160 sch_sfq\r\nSep  5 13:55:19 s160 sch_htb\r\nSep  5 13:55:19 s160 kcare\r\nSep  5 13:55:19 s160 (U)\r\nSep  5 13:55:19 s160 netconsole\r\nSep  5 13:55:19 s160 configfs\r\nSep  5 13:55:19 s160 des_generic\r\nSep  5 13:55:19 s160 ecb\r\nSep  5 13:55:19 s160 md4\r\nSep  5 13:55:19 s160 nls_utf8\r\nSep  5 13:55:19 s160 cifs\r\nSep  5 13:55:19 s160 ip6t_REJECT\r\nSep  5 13:55:19 s160 ip_set_hash_ip\r\nSep  5 13:55:19 s160 xt_set\r\nSep  5 13:55:19 s160 ip_set_hash_net\r\nSep  5 13:55:19 s160 nf_conntrack_ipv6\r\nSep  5 13:55:19 s160 nf_defrag_ipv6\r\nSep  5 13:55:19 s160 ip6t_LOG\r\nSep  5 13:55:19 s160 ip_set\r\nSep  5 13:55:19 s160 nfnetlink\r\nSep  5 13:55:19 s160 ip6table_filter\r\nSep  5 13:55:19 s160 xt_owner\r\nSep  5 13:55:19 s160 nf_nat_ftp\r\nSep  5 13:55:19 s160 ipt_REDIRECT\r\nSep  5 13:55:19 s160 xt_conntrack\r\nSep  5 13:55:19 s160 nf_conntrack_ftp\r\nSep  5 13:55:19 s160 ipt_REJECT\r\nSep  5 13:55:19 s160 ipt_LOG\r\nSep  5 13:55:19 s160 xt_limit\r\nSep  5 13:55:19 s160 iptable_filter\r\nSep  5 13:55:19 s160 xt_multiport\r\nSep  5 13:55:19 s160 iptable_nat\r\nSep  5 13:55:19 s160 nf_nat\r\nSep  5 13:55:19 s160 nf_conntrack_ipv4\r\nSep  5 13:55:19 s160 nf_conntrack\r\nSep  5 13:55:19 s160 nf_defrag_ipv4\r\nSep  5 13:55:19 s160 8021q\r\nSep  5 13:55:19 s160 garp\r\nSep  5 13:55:19 s160 stp\r\nSep  5 13:55:19 s160 llc\r\nSep  5 13:55:19 s160 bonding\r\nSep  5 13:55:19 s160 kmodlve\r\nSep  5 13:55:19 s160 (U)\r\nSep  5 13:55:19 s160 ip6table_mangle\r\nSep  5 13:55:19 s160 ip6_tables\r\nSep  5 13:55:19 s160 ipv6\r\nSep  5 13:55:19 s160 iptable_mangle\r\nSep  5 13:55:19 s160 xt_MARK\r\nSep  5 13:55:19 s160 ip_tables\r\nSep  5 13:55:19 s160 ipmi_devintf\r\nSep  5 13:55:19 s160 ipmi_si\r\nSep  5 13:55:19 s160 ipmi_msghandler\r\nSep  5 13:55:19 s160 iTCO_wdt\r\nSep  5 13:55:19 s160 iTCO_vendor_support\r\nSep  5 13:55:19 s160 zfs(P)\r\nSep  5 13:55:19 s160 (U)\r\nSep  5 13:55:19 s160 zcommon(P)\r\nSep  5 13:55:19 s160 (U)\r\nSep  5 13:55:19 s160 znvpair(P)\r\nSep  5 13:55:19 s160 (U)\r\nSep  5 13:55:19 s160 icp(P)\r\nSep  5 13:55:19 s160 (U)\r\nSep  5 13:55:19 s160 spl\r\nSep  5 13:55:19 s160 (U)\r\nSep  5 13:55:19 s160 zlib_deflate\r\nSep  5 13:55:19 s160 zavl(P)\r\nSep  5 13:55:19 s160 (U)\r\nSep  5 13:55:19 s160 zunicode(P)\r\nSep  5 13:55:19 s160 (U)\r\nSep  5 13:55:19 s160 acpi_pad\r\nSep  5 13:55:19 s160 ixgbe\r\nSep  5 13:55:19 s160 (U)\r\nSep  5 13:55:19 s160 dca\r\nSep  5 13:55:19 s160 ptp\r\nSep  5 13:55:19 s160 pps_core\r\nSep  5 13:55:19 s160 sb_edac\r\nSep  5 13:55:19 s160 edac_core\r\nSep  5 13:55:19 s160 joydev\r\nSep  5 13:55:19 s160 i2c_i801\r\nSep  5 13:55:19 s160 i2c_core\r\nSep  5 13:55:19 s160 sg\r\nSep  5 13:55:19 s160 lpc_ich\r\nSep  5 13:55:19 s160 mfd_core\r\nSep  5 13:55:19 s160 shpchp\r\nSep  5 13:55:19 s160 ext4\r\nSep  5 13:55:19 s160 jbd2\r\nSep  5 13:55:19 s160 mbcache\r\nSep  5 13:55:19 s160 raid1\r\nSep  5 13:55:19 s160 sd_mod\r\nSep  5 13:55:19 s160 crc_t10dif\r\nSep  5 13:55:19 s160 mpt3sas\r\nSep  5 13:55:19 s160 raid_class\r\nSep  5 13:55:19 s160 isci\r\nSep  5 13:55:19 s160 libsas\r\nSep  5 13:55:19 s160 scsi_transport_sas\r\nSep  5 13:55:19 s160 ahci\r\nSep  5 13:55:19 s160 wmi\r\nSep  5 13:55:19 s160 dm_mirror\r\nSep  5 13:55:19 s160 dm_region_hash\r\nSep  5 13:55:19 s160 dm_log\r\nSep  5 13:55:19 s160 dm_mod\r\nSep  5 13:55:19 s160 [last unloaded: scsi_wait_scan]\r\nSep  5 13:55:19 s160 \r\nSep  5 13:55:19 s160 [1064298.453072] \r\nSep  5 13:55:19 s160 [1064298.453199] Pid: 133880, comm: umount veid: 0 Tainted: P           -- ------------    2.6.32-673.26.1.lve1.4.30.el6.x86_64 #1 042stab116_2\r\nSep  5 13:55:19 s160 Supermicro X9DRT-PT\r\nSep  5 13:55:19 s160 /X9DRT-PT\r\nSep  5 13:55:19 s160 \r\nSep  5 13:55:19 s160 [1064298.456442] RIP: 0010:[<ffffffff811d5dd8>] \r\nSep  5 13:55:19 s160 [<ffffffff811d5dd8>] shrink_dcache_for_umount_subtree+0x298/0x2a0\r\nSep  5 13:55:19 s160 [1064298.456712] RSP: 0018:ffff8817c37d7dc8  EFLAGS: 00010292\r\nSep  5 13:55:19 s160 [1064298.456844] RAX: 0000000000000063 RBX: ffff88011c815438 RCX: 000000000000423e\r\nSep  5 13:55:19 s160 [1064298.457095] RDX: 0000000000000000 RSI: 0000000000000046 RDI: 0000000000000246\r\nSep  5 13:55:19 s160 [1064298.457345] RBP: ffff8817c37d7e08 R08: 0000000000000001 R09: 0000000000000000\r\nSep  5 13:55:19 s160 [1064298.457598] R10: 000000002b300087 R11: ffff881878dc1090 R12: 0000000000000000\r\nSep  5 13:55:19 s160 [1064298.457848] R13: ffffffff81a86380 R14: ffff880bdd483c00 R15: ffff88011c8154a8\r\nSep  5 13:55:19 s160 [1064298.458100] FS:  00007f824d418740(0000) GS:ffff8810a0500000(0000) knlGS:0000000000000000\r\nSep  5 13:55:19 s160 [1064298.458351] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nSep  5 13:55:19 s160 [1064298.458482] CR2: 00007f824ca938d5 CR3: 0000001811235000 CR4: 00000000001407e0\r\nSep  5 13:55:19 s160 [1064298.458733] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\nSep  5 13:55:19 s160 [1064298.458984] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400\r\nSep  5 13:55:19 s160 [1064298.459235] Process umount (pid: 133880, veid: 0, threadinfo ffff8817c37d4000, task ffff881334890f20)\r\nSep  5 13:55:19 s160 [1064298.459489] Stack:\r\nSep  5 13:55:19 s160 [1064298.459614]  ffff880bdd483e80\r\nSep  5 13:55:19 s160 0000000000000202\r\nSep  5 13:55:19 s160 ffff8817c37d7e48\r\nSep  5 13:55:19 s160 ffff880bdd483c00\r\nSep  5 13:55:19 s160 \r\nSep  5 13:55:19 s160 [1064298.459760] <d>\r\nSep  5 13:55:19 s160 ffffffffa046e500\r\nSep  5 13:55:19 s160 ffffffff81c1dfc0\r\nSep  5 13:55:19 s160 ffff880bdd483c00\r\nSep  5 13:55:19 s160 0000000000000000\r\nSep  5 13:55:19 s160 \r\nSep  5 13:55:19 s160 [1064298.460031] <d>\r\nSep  5 13:55:19 s160 ffff8817c37d7e28\r\nSep  5 13:55:19 s160 ffffffff811d5e16\r\nSep  5 13:55:19 s160 ffff88107fc78380\r\nSep  5 13:55:19 s160 ffff880bdd483c00\r\nSep  5 13:55:19 s160 \r\nSep  5 13:55:19 s160 [1064298.460420] Call Trace:\r\nSep  5 13:55:19 s160 [1064298.460559]  [<ffffffff811d5e16>] shrink_dcache_for_umount+0x36/0x60\r\nSep  5 13:55:19 s160 [1064298.460697]  [<ffffffff811bc213>] generic_shutdown_super+0x23/0x100\r\nSep  5 13:55:19 s160 [1064298.460833]  [<ffffffff811bc380>] kill_anon_super+0x40/0x80\r\nSep  5 13:55:19 s160 [1064298.461036]  [<ffffffffa042ae5e>] zpl_kill_sb+0x1e/0x30 [zfs]\r\nSep  5 13:55:19 s160 [1064298.461176]  [<ffffffff811bc849>] deactivate_super+0x79/0xa0\r\nSep  5 13:55:19 s160 [1064298.461313]  [<ffffffff811dee4f>] mntput_no_expire+0xbf/0x110\r\nSep  5 13:55:19 s160 [1064298.461452]  [<ffffffff811dfad2>] sys_umount+0x82/0x3d0\r\nSep  5 13:55:19 s160 [1064298.461591]  [<ffffffff8100b1a2>] system_call_fastpath+0x16/0x1b\r\nSep  5 13:55:19 s160 [1064298.461726] Code: \r\nSep  5 13:55:19 s160 50 \r\nSep  5 13:55:19 s160 30 \r\nSep  5 13:55:19 s160 4c \r\nSep  5 13:55:19 s160 8b \r\nSep  5 13:55:19 s160 0a \r\nSep  5 13:55:19 s160 31 \r\nSep  5 13:55:19 s160 d2 \r\nSep  5 13:55:19 s160 48 \r\nSep  5 13:55:19 s160 85 \r\nSep  5 13:55:19 s160 f6 \r\nSep  5 13:55:19 s160 74 \r\nSep  5 13:55:19 s160 04 \r\nSep  5 13:55:19 s160 48 \r\nSep  5 13:55:19 s160 8b \r\nSep  5 13:55:19 s160 56 \r\nSep  5 13:55:19 s160 40 \r\nSep  5 13:55:19 s160 48 \r\nSep  5 13:55:19 s160 05 \r\nSep  5 13:55:19 s160 80 \r\nSep  5 13:55:19 s160 02 \r\nSep  5 13:55:19 s160 00 \r\nSep  5 13:55:19 s160 00 \r\nSep  5 13:55:19 s160 48 \r\nSep  5 13:55:19 s160 89 \r\nSep  5 13:55:19 s160 de \r\nSep  5 13:55:19 s160 48 \r\nSep  5 13:55:19 s160 c7 \r\nSep  5 13:55:19 s160 c7 \r\nSep  5 13:55:19 s160 00 \r\nSep  5 13:55:19 s160 ec \r\nSep  5 13:55:19 s160 7d \r\nSep  5 13:55:19 s160 81 \r\nSep  5 13:55:19 s160 48 \r\nSep  5 13:55:19 s160 89 \r\nSep  5 13:55:19 s160 04 \r\nSep  5 13:55:19 s160 24 \r\nSep  5 13:55:19 s160 31 \r\nSep  5 13:55:19 s160 c0 \r\nSep  5 13:55:19 s160 e8 \r\nSep  5 13:55:19 s160 4e \r\nSep  5 13:55:19 s160 0f \r\nSep  5 13:55:19 s160 37 \r\nSep  5 13:55:19 s160 00 \r\nSep  5 13:55:19 s160 0b \r\nSep  5 13:55:19 s160 eb \r\nSep  5 13:55:19 s160 fe \r\nSep  5 13:55:19 s160 0f \r\nSep  5 13:55:19 s160 0b \r\nSep  5 13:55:19 s160 eb \r\nSep  5 13:55:19 s160 fe \r\nSep  5 13:55:19 s160 55 \r\nSep  5 13:55:19 s160 48 \r\nSep  5 13:55:19 s160 89 \r\nSep  5 13:55:19 s160 e5 \r\nSep  5 13:55:19 s160 53 \r\nSep  5 13:55:19 s160 48 \r\nSep  5 13:55:19 s160 83 \r\nSep  5 13:55:19 s160 ec \r\nSep  5 13:55:19 s160 08 \r\nSep  5 13:55:19 s160 0f \r\nSep  5 13:55:19 s160 1f \r\nSep  5 13:55:19 s160 44 \r\nSep  5 13:55:19 s160 00 \r\nSep  5 13:55:19 s160 \r\nSep  5 13:55:19 s160 [1064298.462360] RIP \r\nSep  5 13:55:19 s160 [<ffffffff811d5dd8>] shrink_dcache_for_umount_subtree+0x298/0x2a0\r\nSep  5 13:55:19 s160 [1064298.462615]  RSP <ffff8817c37d7dc8>\r\nSep  5 13:55:19 s160 [1064298.463112] Tainting kernel with flag 0x7\r\nSep  5 13:55:19 s160 [1064298.463288] Pid: 133880, comm: umount veid: 0 Tainted: P           -- ------------    2.6.32-673.26.1.lve1.4.30.el6.x86_64 #1\r\nSep  5 13:55:19 s160 [1064298.463593] Call Trace:\r\nSep  5 13:55:19 s160 [1064298.463772]  [<ffffffff8107e6f1>] ? add_taint+0x71/0x80\r\nSep  5 13:55:19 s160 [1064298.463954]  [<ffffffff8154c324>] ? oops_end+0x54/0x100\r\nSep  5 13:55:19 s160 [1064298.464136]  [<ffffffff810116bb>] ? die+0x5b/0x90\r\nSep  5 13:55:19 s160 [1064298.464313]  [<ffffffff8154bb64>] ? do_trap+0xc4/0x160\r\nSep  5 13:55:19 s160 [1064298.464495]  [<ffffffff8100ced5>] ? do_invalid_op+0x95/0xb0\r\nSep  5 13:55:19 s160 [1064298.464676]  [<ffffffff811d5dd8>] ? shrink_dcache_for_umount_subtree+0x298/0x2a0\r\nSep  5 13:55:19 s160 [1064298.464980]  [<ffffffff8100c15b>] ? invalid_op+0x1b/0x20\r\nSep  5 13:55:19 s160 [1064298.465164]  [<ffffffff811d5dd8>] ? shrink_dcache_for_umount_subtree+0x298/0x2a0\r\nSep  5 13:55:19 s160 [1064298.465463]  [<ffffffff811d5dd8>] ? shrink_dcache_for_umount_subtree+0x298/0x2a0\r\nSep  5 13:55:19 s160 [1064298.465766]  [<ffffffff811d5e16>] ? shrink_dcache_for_umount+0x36/0x60\r\nSep  5 13:55:19 s160 [1064298.465948]  [<ffffffff811bc213>] ? generic_shutdown_super+0x23/0x100\r\nSep  5 13:55:19 s160 [1064298.466130]  [<ffffffff811bc380>] ? kill_anon_super+0x40/0x80\r\nSep  5 13:55:19 s160 [1064298.466343]  [<ffffffffa042ae5e>] ? zpl_kill_sb+0x1e/0x30 [zfs]\r\nSep  5 13:55:19 s160 [1064298.466526]  [<ffffffff811bc849>] ? deactivate_super+0x79/0xa0\r\nSep  5 13:55:19 s160 [1064298.466708]  [<ffffffff811dee4f>] ? mntput_no_expire+0xbf/0x110\r\nSep  5 13:55:19 s160 [1064298.466889]  [<ffffffff811dfad2>] ? sys_umount+0x82/0x3d0\r\nSep  5 13:55:19 s160 [1064298.467071]  [<ffffffff8100b1a2>] ? system_call_fastpath+0x16/0x1b\r\nSep  5 13:55:19 s160 [1064298.467270] ---[ end trace 04751bd1dc0315b5 ]---\r\nSep  5 13:55:19 s160 [1064298.467459] Kernel panic - not syncing: Fatal exception\r\nSep  5 13:55:19 s160 [1064298.467637] Pid: 133880, comm: umount veid: 0 Tainted: P      D    -- ------------    2.6.32-673.26.1.lve1.4.30.el6.x86_64 #1\r\nSep  5 13:55:19 s160 [1064298.467942] Call Trace:\r\nSep  5 13:55:19 s160 [1064298.468119]  [<ffffffff81546c5e>] ? panic+0xa7/0x16f\r\nSep  5 13:55:19 s160 [1064298.468301]  [<ffffffff8154c3b4>] ? oops_end+0xe4/0x100\r\nSep  5 13:55:19 s160 [1064298.468481]  [<ffffffff810116bb>] ? die+0x5b/0x90\r\nSep  5 13:55:19 s160 [1064298.468659]  [<ffffffff8154bb64>] ? do_trap+0xc4/0x160\r\nSep  5 13:55:19 s160 [1064298.468840]  [<ffffffff8100ced5>] ? do_invalid_op+0x95/0xb0\r\nSep  5 13:55:19 s160 [1064298.469020]  [<ffffffff811d5dd8>] ? shrink_dcache_for_umount_subtree+0x298/0x2a0\r\nSep  5 13:55:19 s160 [1064298.469324]  [<ffffffff8100c15b>] ? invalid_op+0x1b/0x20\r\nSep  5 13:55:19 s160 [1064298.469507]  [<ffffffff811d5dd8>] ? shrink_dcache_for_umount_subtree+0x298/0x2a0\r\nSep  5 13:55:19 s160 [1064298.469808]  [<ffffffff811d5dd8>] ? shrink_dcache_for_umount_subtree+0x298/0x2a0\r\nSep  5 13:55:19 s160 [1064298.470110]  [<ffffffff811d5e16>] ? shrink_dcache_for_umount+0x36/0x60\r\nSep  5 13:55:19 s160 [1064298.470291]  [<ffffffff811bc213>] ? generic_shutdown_super+0x23/0x100\r\nSep  5 13:55:19 s160 [1064298.470473]  [<ffffffff811bc380>] ? kill_anon_super+0x40/0x80\r\nSep  5 13:55:19 s160 [1064298.470686]  [<ffffffffa042ae5e>] ? zpl_kill_sb+0x1e/0x30 [zfs]\r\nSep  5 13:55:19 s160 [1064298.470869]  [<ffffffff811bc849>] ? deactivate_super+0x79/0xa0\r\nSep  5 13:55:19 s160 [1064298.471058]  [<ffffffff811dee4f>] ? mntput_no_expire+0xbf/0x110\r\nSep  5 13:55:19 s160 [1064298.471240]  [<ffffffff811dfad2>] ? sys_umount+0x82/0x3d0\r\nSep  5 13:55:19 s160 [1064298.471425]  [<ffffffff8100b1a2>] ? system_call_fastpath+0x16/0x1b\r\nSep  5 13:55:19 s160 [1064298.471626] Rebooting in 30 seconds..\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6612/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bud4": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6917", "title": "processes blocked in io operations state D", "body": "Type                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Ubuntu\r\nDistribution Version    |16.04.3 LTS\r\nLinux Kernel                 | 4.4.0-101-generic\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.6-0ubuntu16\r\nSPL Version                  | 0.6.5.6-0ubuntu4\r\n\r\n### Describe the problem you're observing\r\nOn mastrer zfs file-server whit about 1000 smbd connections  : \r\n**Many samba demons have stalled in state D. It was impossible to kill them. We kept lock of various files.**\r\n The only solution was to reboot the system. But the zfs fylesystems have not been cleanly unmounted\r\n### Describe how to reproduce the problem\r\nheavy load, maybe there was a temporary loss of the Ethernet link.\r\n### Include a warning/errors/backtraces from the system logs\r\n[kern.log](https://github.com/zfsonlinux/zfs/files/1522117/kern.log)\r\n\r\n\r\n'''\r\nov 29 01:14:41 zfs-cis kernel: [1664429.009800] INFO: task zfs:119150 blocked for more than 120 seconds.\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.010008]       Tainted: P           O    4.4.0-98-generic #121-Ubuntu\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.010242] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.010488] zfs             D ffff8804a15cfbf8     0 119150 116705 0x00000000\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.010498]  ffff8804a15cfbf8 0000000000000246 ffff882037597000 ffff8803f02bb800\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.010503]  ffff8804a15d0000 ffff88103236ba20 ffff88103236ba48 ffff88103236bae0\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.010507]  0000000000000000 ffff8804a15cfc10 ffffffff81840585 ffff88103236bad8\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.010511] Call Trace:\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.010522]  [<ffffffff81840585>] schedule+0x35/0x80\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.010540]  [<ffffffffc027ec3b>] cv_wait_common+0x10b/0x140 [spl]\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.010556]  [<ffffffff810c4410>] ? wake_atomic_t_function+0x60/0x60\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.010569]  [<ffffffffc027ec85>] __cv_wait+0x15/0x20 [spl]\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.010647]  [<ffffffffc063e2b5>] txg_wait_synced+0xe5/0x130 [zfs]\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.010705]  [<ffffffffc0619bf9>] dsl_sync_task+0x179/0x260 [zfs]\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.010774]  [<ffffffffc06972e0>] ? dsl_destroy_snapshot_check_impl+0x90/0x90 [zfs]\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.010840]  [<ffffffffc06964a0>] ? dsl_destroy_snapshot_sync_impl+0xb50/0xb50 [zfs]\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.010903]  [<ffffffffc06972e0>] ? dsl_destroy_snapshot_check_impl+0x90/0x90 [zfs]\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.010966]  [<ffffffffc06964a0>] ? dsl_destroy_snapshot_sync_impl+0xb50/0xb50 [zfs]\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.011026]  [<ffffffffc069748f>] dsl_destroy_snapshots_nvl+0x8f/0x100 [zfs]\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.011092]  [<ffffffffc0668be8>] zfs_ioc_destroy_snaps+0xa8/0xc0 [zfs]\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.011158]  [<ffffffffc0666504>] zfsdev_ioctl+0x244/0x4e0 [zfs]\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.011164]  [<ffffffff812245cf>] do_vfs_ioctl+0x29f/0x490\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.011169]  [<ffffffff8106b594>] ? __do_page_fault+0x1b4/0x400\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.011173]  [<ffffffff81224839>] SyS_ioctl+0x79/0x90\r\nNov 29 01:14:41 zfs-cis kernel: [1664429.011179]  [<ffffffff818446b2>] entry_SYSCALL_64_fastpath+0x16/0x71\r\n9 14:06:41 zfs-cis kernel: [1710753.133928] INFO: task zfs:153986 blocked for more than 120 seconds.\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.134136]       Tainted: P           O    4.4.0-98-generic #121-Ubuntu\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.134348] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.134597] zfs             D ffff8814924afad0     0 153986 153795 0x00000000\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.134603]  ffff8814924afad0 0000000000000246 ffff882037594600 ffff8807f1f10000\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.134608]  ffff8814924b0000 ffff88103236ba20 ffff88103236ba48 ffff88103236bae0\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.134611]  0000000000000000 ffff8814924afae8 ffffffff81840585 ffff88103236bad8\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.134615] Call Trace:\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.134626]  [<ffffffff81840585>] schedule+0x35/0x80\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.134645]  [<ffffffffc027ec3b>] cv_wait_common+0x10b/0x140 [spl]\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.134653]  [<ffffffff810c4410>] ? wake_atomic_t_function+0x60/0x60\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.134662]  [<ffffffffc027ec85>] __cv_wait+0x15/0x20 [spl]\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.134742]  [<ffffffffc063e2b5>] txg_wait_synced+0xe5/0x130 [zfs]\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.134794]  [<ffffffffc0619bf9>] dsl_sync_task+0x179/0x260 [zfs]\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.134863]  [<ffffffffc06980e0>] ? dsl_dataset_user_hold_sync+0x170/0x170 [zfs]\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.134933]  [<ffffffffc0697820>] ? dsl_dataset_user_hold_sync_one_impl+0x2c0/0x2c0 [zfs]\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.135004]  [<ffffffffc06980e0>] ? dsl_dataset_user_hold_sync+0x170/0x170 [zfs]\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.135073]  [<ffffffffc0697820>] ? dsl_dataset_user_hold_sync_one_impl+0x2c0/0x2c0 [zfs]\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.135142]  [<ffffffffc0697cb5>] dsl_dataset_user_release_impl+0x1c5/0x290 [zfs]\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.135212]  [<ffffffffc0697ab0>] ? dsl_dataset_user_release_sync+0x290/0x290 [zfs]\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.135220]  [<ffffffffc01d614b>] ? avl_find+0x5b/0xa0 [zavl]\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.135282]  [<ffffffffc0638062>] ? spa_lookup+0x62/0x80 [zfs]\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.135288]  [<ffffffff818424f2>] ? mutex_lock+0x12/0x30\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.135347]  [<ffffffffc062ff30>] ? spa_open_common+0x2d0/0x490 [zfs]\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.135352]  [<ffffffff810bad77>] ? should_numa_migrate_memory+0x57/0x130\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.135422]  [<ffffffffc0697e57>] dsl_dataset_user_release_onexit+0xd7/0x110 [zfs]\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.135492]  [<ffffffffc066a4e9>] zfs_onexit_destroy+0xa9/0x170 [zfs]\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.135562]  [<ffffffffc06649b0>] zfsdev_release+0x40/0x80 [zfs]\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.135567]  [<ffffffff81212474>] __fput+0xe4/0x220\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.135570]  [<ffffffff812125ee>] ____fput+0xe/0x10\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.135576]  [<ffffffff8109f0a1>] task_work_run+0x81/0xa0\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.135581]  [<ffffffff81003242>] exit_to_usermode_loop+0xc2/0xd0\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.135586]  [<ffffffff81003c6e>] syscall_return_slowpath+0x4e/0x60\r\nNov 29 14:06:41 zfs-cis kernel: [1710753.135591]  [<ffffffff81844810>] int_ret_from_sys_call+0x25/0x8f\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.144598] INFO: task zfs:153986 blocked for more than 120 seconds.\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.144810]       Tainted: P           O    4.4.0-98-generic #121-Ubuntu\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145022] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145270] zfs             D ffff8814924afad0     0 153986 153795 0x00000000\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145277]  ffff8814924afad0 0000000000000246 ffff882037594600 ffff8807f1f10000\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145281]  ffff8814924b0000 ffff88103236ba20 ffff88103236ba48 ffff88103236bae0\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145285]  0000000000000000 ffff8814924afae8 ffffffff81840585 ffff88103236bad8\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145289] Call Trace:\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145300]  [<ffffffff81840585>] schedule+0x35/0x80\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145319]  [<ffffffffc027ec3b>] cv_wait_common+0x10b/0x140 [spl]\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145327]  [<ffffffff810c4410>] ? wake_atomic_t_function+0x60/0x60\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145336]  [<ffffffffc027ec85>] __cv_wait+0x15/0x20 [spl]\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145408]  [<ffffffffc063e2b5>] txg_wait_synced+0xe5/0x130 [zfs]\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145462]  [<ffffffffc0619bf9>] dsl_sync_task+0x179/0x260 [zfs]\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145528]  [<ffffffffc06980e0>] ? dsl_dataset_user_hold_sync+0x170/0x170 [zfs]\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145599]  [<ffffffffc0697820>] ? dsl_dataset_user_hold_sync_one_impl+0x2c0/0x2c0 [zfs]\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145670]  [<ffffffffc06980e0>] ? dsl_dataset_user_hold_sync+0x170/0x170 [zfs]\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145741]  [<ffffffffc0697820>] ? dsl_dataset_user_hold_sync_one_impl+0x2c0/0x2c0 [zfs]\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145811]  [<ffffffffc0697cb5>] dsl_dataset_user_release_impl+0x1c5/0x290 [zfs]\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145881]  [<ffffffffc0697ab0>] ? dsl_dataset_user_release_sync+0x290/0x290 [zfs]\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145889]  [<ffffffffc01d614b>] ? avl_find+0x5b/0xa0 [zavl]\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145951]  [<ffffffffc0638062>] ? spa_lookup+0x62/0x80 [zfs]\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.145956]  [<ffffffff818424f2>] ? mutex_lock+0x12/0x30\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146016]  [<ffffffffc062ff30>] ? spa_open_common+0x2d0/0x490 [zfs]\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146021]  [<ffffffff810bad77>] ? should_numa_migrate_memory+0x57/0x130\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146091]  [<ffffffffc0697e57>] dsl_dataset_user_release_onexit+0xd7/0x110 [zfs]\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146162]  [<ffffffffc066a4e9>] zfs_onexit_destroy+0xa9/0x170 [zfs]\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146232]  [<ffffffffc06649b0>] zfsdev_release+0x40/0x80 [zfs]\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146237]  [<ffffffff81212474>] __fput+0xe4/0x220\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146240]  [<ffffffff812125ee>] ____fput+0xe/0x10\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146246]  [<ffffffff8109f0a1>] task_work_run+0x81/0xa0\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146251]  [<ffffffff81003242>] exit_to_usermode_loop+0xc2/0xd0\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146256]  [<ffffffff81003c6e>] syscall_return_slowpath+0x4e/0x60\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146262]  [<ffffffff81844810>] int_ret_from_sys_call+0x25/0x8f\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146270] INFO: task zpool:156254 blocked for more than 120 seconds.\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146478]       Tainted: P           O    4.4.0-98-generic #121-Ubuntu\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146690] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146937] zpool           D ffff88076f0bbb48     0 156254 156253 0x00000000\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146942]  ffff88076f0bbb48 0000000000000001 ffff880bfe46b800 ffff880bfe46aa00\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146946]  ffff88076f0bc000 ffffffffc07c27c4 ffff880bfe46aa00 00000000ffffffff\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146951]  ffffffffc07c27c8 ffff88076f0bbb60 ffffffff81840585 ffffffffc07c27c0\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146955] Call Trace:\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146960]  [<ffffffff81840585>] schedule+0x35/0x80\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146964]  [<ffffffff8184082e>] schedule_preempt_disabled+0xe/0x10\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146969]  [<ffffffff81842469>] __mutex_lock_slowpath+0xb9/0x130\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.146974]  [<ffffffff818424ff>] mutex_lock+0x1f/0x30\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.147045]  [<ffffffffc0668ea4>] zfsdev_open+0x24/0x120 [zfs]\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.147051]  [<ffffffff8152e081>] misc_open+0x141/0x160\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.147056]  [<ffffffff812150af>] chrdev_open+0xbf/0x1b0\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.147061]  [<ffffffff8120e1cf>] do_dentry_open+0x1ff/0x310\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.147064]  [<ffffffff81214ff0>] ? cdev_put+0x30/0x30\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.147068]  [<ffffffff8120f364>] vfs_open+0x54/0x80\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.147073]  [<ffffffff8121b0fb>] ? may_open+0x5b/0xf0\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.147077]  [<ffffffff8121f017>] path_openat+0x1b7/0x1330\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.147081]  [<ffffffff81221381>] do_filp_open+0x91/0x100\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.147085]  [<ffffffff8122edb6>] ? __alloc_fd+0x46/0x190\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.147090]  [<ffffffff8120f738>] do_sys_open+0x138/0x2a0\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.147096]  [<ffffffff8106b594>] ? __do_page_fault+0x1b4/0x400\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.147100]  [<ffffffff8120f8be>] SyS_open+0x1e/0x20\r\nNov 29 14:08:41 zfs-cis kernel: [1710873.147106]  [<ffffffff818446b2>] entry_SYSCALL_64_fastpath+0x16/0x71\r\nNov 29 15:54:41 zfs-cis kernel: [1717233.717192] INFO: task smbd:19081 blocked for more than 120 seconds.\r\n'''\r\n\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6917/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6914", "title": "trace during mount --not usable filesystem after mount ", "body": "Distribution Name       Ubuntu\r\nDistribution Version    Ubuntu 16.04.3 LTS\r\nLinux Kernel                4.4.0-101-generic #124-Ubuntu| \r\nArchitecture                 x86_64\r\nZFS Version                  0.6.5.6-0ubuntu16\r\nSPL Version                  |0.6.5.6-0ubuntu4 \r\n\r\ni have the following  trace  mounting zfs   . After mount  the filesystem is not usable . \r\nzfs list -t snapshot  -s creation  take morte than 50 minutes ...  \r\n\r\n\r\n```\r\n[  360.613907] INFO: task l2arc_feed:2281 blocked for more than 120 seconds.\r\n[  360.622413]       Tainted: P           O    4.4.0-101-generic #124-Ubuntu\r\n[  360.626960] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[  360.636032] l2arc_feed      D ffff88103022bd98     0  2281      2 0x00000000\r\n[  360.636036]  ffff88103022bd98 ffffffffc0bcb180 ffff881037a2aa00 ffff881030ecb800\r\n[  360.636038]  ffff88103022c000 ffffffffc0cbbc64 ffff881030ecb800 00000000ffffffff\r\n[  360.636039]  ffffffffc0cbbc68 ffff88103022bdb0 ffffffff818406d5 ffffffffc0cbbc60\r\n[  360.636041] Call Trace:\r\n[  360.636052]  [<ffffffff818406d5>] schedule+0x35/0x80\r\n[  360.636054]  [<ffffffff8184097e>] schedule_preempt_disabled+0xe/0x10\r\n[  360.636057]  [<ffffffff818425b9>] __mutex_lock_slowpath+0xb9/0x130\r\n[  360.636059]  [<ffffffff8184264f>] mutex_lock+0x1f/0x30\r\n[  360.636105]  [<ffffffffc0ad48a0>] l2arc_feed_thread+0x1b0/0x470 [zfs]\r\n[  360.636121]  [<ffffffffc0ad46f0>] ? l2arc_evict+0x350/0x350 [zfs]\r\n[  360.636129]  [<ffffffffc0325dc0>] ? __thread_exit+0x20/0x20 [spl]\r\n[  360.636132]  [<ffffffffc0325e31>] thread_generic_wrapper+0x71/0x80 [spl]\r\n[  360.636136]  [<ffffffff810a0c75>] kthread+0xe5/0x100\r\n[  360.636138]  [<ffffffff810a0b90>] ? kthread_create_on_node+0x1e0/0x1e0\r\n[  360.636141]  [<ffffffff81844b8f>] ret_from_fork+0x3f/0x70\r\n[  360.636142]  [<ffffffff810a0b90>] ? kthread_create_on_node+0x1e0/0x1e0\r\n[  360.636147] INFO: task zpool:6451 blocked for more than 120 seconds.\r\n[  360.640887]       Tainted: P           O    4.4.0-101-generic #124-Ubuntu\r\n[  360.645719] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[  360.655319] zpool           D ffff88201a7a3c30     0  6451      1 0x00000000\r\n[  360.655321]  ffff88201a7a3c30 0000000000000000 ffff8820374b8000 ffff88201a788e00\r\n[  360.655322]  ffff88201a7a4000 ffff8820367e2220 ffff8820367e2248 ffff8820367e22e0\r\n[  360.655324]  0000000000000000 ffff88201a7a3c48 ffffffff818406d5 ffff8820367e22d8\r\n[  360.655325] Call Trace:\r\n[  360.655327]  [<ffffffff818406d5>] schedule+0x35/0x80\r\n[  360.655332]  [<ffffffffc032ac3b>] cv_wait_common+0x10b/0x140 [spl]\r\n[  360.655336]  [<ffffffff810c4420>] ? wake_atomic_t_function+0x60/0x60\r\n[  360.655340]  [<ffffffffc032ac85>] __cv_wait+0x15/0x20 [spl]\r\n[  360.655366]  [<ffffffffc0b392b5>] txg_wait_synced+0xe5/0x130 [zfs]\r\n[  360.655390]  [<ffffffffc0b309fc>] spa_config_update+0x11c/0x170 [zfs]\r\n[  360.655414]  [<ffffffffc0b2cb4c>] spa_import+0x5cc/0x790 [zfs]\r\n[  360.655443]  [<ffffffffc0b5dd9a>] zfs_ioc_pool_import+0xfa/0x130 [zfs]\r\n[  360.655471]  [<ffffffffc0b6170b>] zfsdev_ioctl+0x44b/0x4e0 [zfs]\r\n[  360.655476]  [<ffffffff8122466f>] do_vfs_ioctl+0x29f/0x490\r\n[  360.655479]  [<ffffffff8106b594>] ? __do_page_fault+0x1b4/0x400\r\n[  360.655482]  [<ffffffff812248d9>] SyS_ioctl+0x79/0x90\r\n[  360.655484]  [<ffffffff818447f2>] entry_SYSCALL_64_fastpath+0x16/0x71\r\n[  360.655537] INFO: task txg_sync:8513 blocked for more than 120 seconds.\r\n[  360.660412]       Tainted: P           O    4.4.0-101-generic #124-Ubuntu\r\n[  360.665474] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[  360.675442] txg_sync        D ffff88202b6abaa8     0  8513      2 0x00000000\r\n[  360.675444]  ffff88202b6abaa8 ffff88202b6aba88 ffff8820374b9c00 ffff88200e669c00\r\n[  360.675445]  ffff88202b6ac000 ffff88203df16e00 7fffffffffffffff ffff882011919948\r\n[  360.675447]  0000000000000001 ffff88202b6abac0 ffffffff818406d5 0000000000000000\r\n[  360.675448] Call Trace:\r\n[  360.675450]  [<ffffffff818406d5>] schedule+0x35/0x80\r\n[  360.675452]  [<ffffffff81843825>] schedule_timeout+0x1b5/0x270\r\n[  360.675456]  [<ffffffff810ac642>] ? default_wake_function+0x12/0x20\r\n[  360.675458]  [<ffffffff810c3d62>] ? __wake_up_common+0x52/0x90\r\n[  360.675462]  [<ffffffff810f635b>] ? ktime_get+0x3b/0xb0\r\n[  360.675464]  [<ffffffff8183fc04>] io_schedule_timeout+0xa4/0x110\r\n[  360.675468]  [<ffffffffc032abec>] cv_wait_common+0xbc/0x140 [spl]\r\n[  360.675470]  [<ffffffff810c4420>] ? wake_atomic_t_function+0x60/0x60\r\n[  360.675474]  [<ffffffffc032acc8>] __cv_wait_io+0x18/0x20 [spl]\r\n[  360.675501]  [<ffffffffc0b832fe>] zio_wait+0x10e/0x1f0 [zfs]\r\n[  360.675523]  [<ffffffffc0b0ce66>] dsl_pool_sync+0x2c6/0x430 [zfs]\r\n[  360.675550]  [<ffffffffc0b285b6>] spa_sync+0x366/0xb30 [zfs]\r\n[  360.675551]  [<ffffffff810ac642>] ? default_wake_function+0x12/0x20\r\n[  360.675576]  [<ffffffffc0b39a4a>] txg_sync_thread+0x3ba/0x630 [zfs]\r\n[  360.675601]  [<ffffffffc0b39690>] ? txg_delay+0x180/0x180 [zfs]\r\n[  360.675606]  [<ffffffffc0325dc0>] ? __thread_exit+0x20/0x20 [spl]\r\n[  360.675609]  [<ffffffffc0325e31>] thread_generic_wrapper+0x71/0x80 [spl]\r\n[  360.675610]  [<ffffffff810a0c75>] kthread+0xe5/0x100\r\n[  360.675612]  [<ffffffff810a0b90>] ? kthread_create_on_node+0x1e0/0x1e0\r\n[  360.675614]  [<ffffffff81844b8f>] ret_from_fork+0x3f/0x70\r\n[  360.675615]  [<ffffffff810a0b90>] ? kthread_create_on_node+0x1e0/0x1e0\r\n[  480.685855] INFO: task l2arc_feed:2281 blocked for more than 120 seconds.\r\n[  480.696131]       Tainted: P           O    4.4.0-101-generic #124-Ubuntu\r\n[  480.701470] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[  480.711929] l2arc_feed      D ffff88103022bd98     0  2281      2 0x00000000\r\n[  480.711931]  ffff88103022bd98 ffffffffc0bcb180 ffff881037a2aa00 ffff881030ecb800\r\n[  480.711932]  ffff88103022c000 ffffffffc0cbbc64 ffff881030ecb800 00000000ffffffff\r\n[  480.711933]  ffffffffc0cbbc68 ffff88103022bdb0 ffffffff818406d5 ffffffffc0cbbc60\r\n[  480.711934] Call Trace:\r\n[  480.711944]  [<ffffffff818406d5>] schedule+0x35/0x80\r\n[  480.711946]  [<ffffffff8184097e>] schedule_preempt_disabled+0xe/0x10\r\n[  480.711949]  [<ffffffff818425b9>] __mutex_lock_slowpath+0xb9/0x130\r\n[  480.711950]  [<ffffffff8184264f>] mutex_lock+0x1f/0x30\r\n[  480.711998]  [<ffffffffc0ad48a0>] l2arc_feed_thread+0x1b0/0x470 [zfs]\r\n[  480.712016]  [<ffffffffc0ad46f0>] ? l2arc_evict+0x350/0x350 [zfs]\r\n[  480.712024]  [<ffffffffc0325dc0>] ? __thread_exit+0x20/0x20 [spl]\r\n[  480.712027]  [<ffffffffc0325e31>] thread_generic_wrapper+0x71/0x80 [spl]\r\n[  480.712031]  [<ffffffff810a0c75>] kthread+0xe5/0x100\r\n[  480.712032]  [<ffffffff810a0b90>] ? kthread_create_on_node+0x1e0/0x1e0\r\n[  480.712035]  [<ffffffff81844b8f>] ret_from_fork+0x3f/0x70\r\n[  480.712036]  [<ffffffff810a0b90>] ? kthread_create_on_node+0x1e0/0x1e0\r\n[  480.712042] INFO: task zpool:6451 blocked for more than 120 seconds.\r\n[  480.717357]       Tainted: P           O    4.4.0-101-generic #124-Ubuntu\r\n[  480.717357] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[  480.717359] zpool           D ffff88201a7a3c30     0  6451      1 0x00000000\r\n[  480.717360]  ffff88201a7a3c30 0000000000000000 ffff8820374b8000 ffff88201a788e00\r\n[  480.717360]  ffff88201a7a4000 ffff8820367e2220 ffff8820367e2248 ffff8820367e22e0\r\n[  480.717361]  0000000000000000 ffff88201a7a3c48 ffffffff818406d5 ffff8820367e22d8\r\n[  480.717362] Call Trace:\r\n[  480.717363]  [<ffffffff818406d5>] schedule+0x35/0x80\r\n[  480.717367]  [<ffffffffc032ac3b>] cv_wait_common+0x10b/0x140 [spl]\r\n[  480.717371]  [<ffffffff810c4420>] ? wake_atomic_t_function+0x60/0x60\r\n[  480.717374]  [<ffffffffc032ac85>] __cv_wait+0x15/0x20 [spl]\r\n[  480.717421]  [<ffffffffc0b392b5>] txg_wait_synced+0xe5/0x130 [zfs]\r\n[  480.717857]  [<ffffffffc0b309fc>] spa_config_update+0x11c/0x170 [zfs]\r\n[  480.718525]  [<ffffffffc0b2cb4c>] spa_import+0x5cc/0x790 [zfs]\r\n[  480.719065]  [<ffffffffc0b5dd9a>] zfs_ioc_pool_import+0xfa/0x130 [zfs]\r\n[  480.719826]  [<ffffffffc0b6170b>] zfsdev_ioctl+0x44b/0x4e0 [zfs]\r\n[  480.719849]  [<ffffffff8122466f>] do_vfs_ioctl+0x29f/0x490\r\n[  480.719868]  [<ffffffff8106b594>] ? __do_page_fault+0x1b4/0x400\r\n[  480.719869]  [<ffffffff812248d9>] SyS_ioctl+0x79/0x90\r\n[  480.719897]  [<ffffffff818447f2>] entry_SYSCALL_64_fastpath+0x16/0x71\r\n[  480.720267] INFO: task z_wr_iss:7864 blocked for more than 120 seconds.\r\n[  480.720268]       Tainted: P           O    4.4.0-101-generic #124-Ubuntu\r\n[  480.720268] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[  480.720285] z_wr_iss        D ffff88102fc37b00     0  7864      2 0x00000000\r\n[  480.720302]  ffff88102fc37b00 ffffffffc0b18dcb ffff881037a2aa00 ffff881032aac600\r\n[  480.720304]  ffff88102fc38000 ffff88200f09d800 ffff88200f09d828 ffff88200f09d840\r\n[  480.720305]  0000000000000000 ffff88102fc37b18 ffffffff818406d5 ffff88200f09d838\r\n[  480.720305] Call Trace:\r\n[  480.720752]  [<ffffffffc0b18dcb>] ? metaslab_rt_add+0x2b/0x60 [zfs]\r\n[  480.720761]  [<ffffffff818406d5>] schedule+0x35/0x80\r\n[  480.720945]  [<ffffffffc032ac3b>] cv_wait_common+0x10b/0x140 [spl]\r\n[  480.721009]  [<ffffffff810c4420>] ? wake_atomic_t_function+0x60/0x60\r\n[  480.721012]  [<ffffffffc032ac85>] __cv_wait+0x15/0x20 [spl]\r\n[  480.721226]  [<ffffffffc0b1a336>] metaslab_activate+0x46/0xb0 [zfs]\r\n[  480.721655]  [<ffffffffc0b1bbbc>] metaslab_alloc+0x59c/0xbe0 [zfs]\r\n[  480.721782]  [<ffffffffc0324c32>] ? spl_kmem_cache_alloc+0x72/0x7d0 [spl]\r\n[  480.722116]  [<ffffffff811eec6f>] ? kmem_cache_alloc+0x19f/0x1f0\r\n[  480.722724]  [<ffffffffc0b83844>] zio_dva_allocate+0x94/0x400 [zfs]\r\n[  480.722752]  [<ffffffffc0b7f8c9>] ? zio_push_transform+0x39/0x90 [zfs]\r\n[  480.722756]  [<ffffffffc032614e>] ? taskq_member+0x4e/0x70 [spl]\r\n[  480.723373]  [<ffffffffc0b81203>] zio_execute+0xc3/0x170 [zfs]\r\n[  480.723497]  [<ffffffffc0327115>] taskq_thread+0x225/0x410 [spl]\r\n[  480.723512]  [<ffffffff810ac630>] ? wake_up_q+0x70/0x70\r\n[  480.723545]  [<ffffffffc0326ef0>] ? taskq_cancel_id+0x140/0x140 [spl]\r\n[  480.723624]  [<ffffffff810a0c75>] kthread+0xe5/0x100\r\n[  480.723665]  [<ffffffff810a0b90>] ? kthread_create_on_node+0x1e0/0x1e0\r\n[  480.723825]  [<ffffffff81844b8f>] ret_from_fork+0x3f/0x70\r\n[  480.723826]  [<ffffffff810a0b90>] ? kthread_create_on_node+0x1e0/0x1e0\r\n[  480.724681] INFO: task txg_sync:8513 blocked for more than 120 seconds.\r\n[  480.724682]       Tainted: P           O    4.4.0-101-generic #124-Ubuntu\r\n[  480.724702] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[  480.724762] txg_sync        D ffff88202b6abaa8     0  8513      2 0x00000000\r\n[  480.724785]  ffff88202b6abaa8 ffff88202b6aba88 ffff8820374b9c00 ffff88200e669c00\r\n[  480.724789]  ffff88202b6ac000 ffff88203df16e00 7fffffffffffffff ffff882011919948\r\n[  480.724796]  0000000000000001 ffff88202b6abac0 ffffffff818406d5 0000000000000000\r\n[  480.724801] Call Trace:\r\n[  480.724825]  [<ffffffff818406d5>] schedule+0x35/0x80\r\n[  480.724856]  [<ffffffff81843825>] schedule_timeout+0x1b5/0x270\r\n[  480.724859]  [<ffffffff810ac642>] ? default_wake_function+0x12/0x20\r\n[  480.724931]  [<ffffffff810c3d62>] ? __wake_up_common+0x52/0x90\r\n[  480.724972]  [<ffffffff810f635b>] ? ktime_get+0x3b/0xb0\r\n[  480.724975]  [<ffffffff8183fc04>] io_schedule_timeout+0xa4/0x110\r\n[  480.725052]  [<ffffffffc032abec>] cv_wait_common+0xbc/0x140 [spl]\r\n[  480.725053]  [<ffffffff810c4420>] ? wake_atomic_t_function+0x60/0x60\r\n[  480.725067]  [<ffffffffc032acc8>] __cv_wait_io+0x18/0x20 [spl]\r\n[  480.725926]  [<ffffffffc0b832fe>] zio_wait+0x10e/0x1f0 [zfs]\r\n[  480.726363]  [<ffffffffc0b0ce66>] dsl_pool_sync+0x2c6/0x430 [zfs]\r\n[  480.726909]  [<ffffffffc0b285b6>] spa_sync+0x366/0xb30 [zfs]\r\n[  480.726933]  [<ffffffff810ac642>] ? default_wake_function+0x12/0x20\r\n[  480.727507]  [<ffffffffc0b39a4a>] txg_sync_thread+0x3ba/0x630 [zfs]\r\n[  480.728230]  [<ffffffffc0b39690>] ? txg_delay+0x180/0x180 [zfs]\r\n[  480.728317]  [<ffffffffc0325dc0>] ? __thread_exit+0x20/0x20 [spl]\r\n[  480.728320]  [<ffffffffc0325e31>] thread_generic_wrapper+0x71/0x80 [spl]\r\n[  480.728326]  [<ffffffff810a0c75>] kthread+0xe5/0x100\r\n[  480.728327]  [<ffffffff810a0b90>] ? kthread_create_on_node+0x1e0/0x1e0\r\n[  480.728351]  [<ffffffff81844b8f>] ret_from_fork+0x3f/0x70\r\n[  480.728352]  [<ffffffff810a0b90>] ? kthread_create_on_node+0x1e0/0x1e0\r\n[  720.749786] INFO: task metaslab_group_:8098 blocked for more than 120 seconds.\r\n[  720.761076]       Tainted: P           O    4.4.0-101-generic #124-Ubuntu\r\n[  720.766851] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[  720.778266] metaslab_group_ D ffff88103411bd50     0  8098      2 0x00000000\r\n[  720.778269]  ffff88103411bd50 ffff88103411bd80 ffff8820374bc600 ffff881032c7d400\r\n[  720.778272]  ffff88103411c000 ffff88200fc26c00 ffff88200fc26c28 ffff88200fc26c40\r\n[  720.778273]  0000000000000000 ffff88103411bd68 ffffffff818406d5 ffff88200fc26c38\r\n[  720.778275] Call Trace:\r\n[  720.778284]  [<ffffffff818406d5>] schedule+0x35/0x80\r\n[  720.778293]  [<ffffffffc032ac3b>] cv_wait_common+0x10b/0x140 [spl]\r\n[  720.778299]  [<ffffffff810c4420>] ? wake_atomic_t_function+0x60/0x60\r\n[  720.778305]  [<ffffffffc032ac85>] __cv_wait+0x15/0x20 [spl]\r\n[  720.778349]  [<ffffffffc0b1a275>] metaslab_preload+0x65/0xe0 [zfs]\r\n[  720.778353]  [<ffffffffc0327115>] taskq_thread+0x225/0x410 [spl]\r\n[  720.778355]  [<ffffffff810ac630>] ? wake_up_q+0x70/0x70\r\n[  720.778359]  [<ffffffffc0326ef0>] ? taskq_cancel_id+0x140/0x140 [spl]\r\n[  720.778362]  [<ffffffff810a0c75>] kthread+0xe5/0x100\r\n[  720.778363]  [<ffffffff810a0b90>] ? kthread_create_on_node+0x1e0/0x1e0\r\n[  720.778367]  [<ffffffff81844b8f>] ret_from_fork+0x3f/0x70\r\n[  720.778370]  [<ffffffff810a0b90>] ? kthread_create_on_node+0x1e0/0x1e0\r\n[  720.778371] INFO: task metaslab_group_:8100 blocked for more than 120 seconds.\r\n[  720.789790]       Tainted: P           O    4.4.0-101-generic #124-Ubuntu\r\n[  720.795569] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[  720.806984] metaslab_group_ D ffff8810331a3d50     0  8100      2 0x00000000\r\n[  720.806987]  ffff8810331a3d50 ffff8810331a3d80 ffff8820374baa00 ffff88202e375400\r\n[  720.807005]  ffff8810331a4000 ffff88200fc26c00 ffff88200fc26c28 ffff88200fc26c40\r\n[  720.807006]  0000000000000000 ffff8810331a3d68 ffffffff818406d5 ffff88200fc26c38\r\n[  720.807008] Call Trace:\r\n[  720.807010]  [<ffffffff818406d5>] schedule+0x35/0x80\r\n[  720.807015]  [<ffffffffc032ac3b>] cv_wait_common+0x10b/0x140 [spl]\r\n[  720.807017]  [<ffffffff810c4420>] ? wake_atomic_t_function+0x60/0x60\r\n[  720.807021]  [<ffffffffc032ac85>] __cv_wait+0x15/0x20 [spl]\r\n[  720.807046]  [<ffffffffc0b1a275>] metaslab_preload+0x65/0xe0 [zfs]\r\n[  720.807051]  [<ffffffffc0327115>] taskq_thread+0x225/0x410 [spl]\r\n[  720.807052]  [<ffffffff810ac630>] ? wake_up_q+0x70/0x70\r\n[  720.807056]  [<ffffffffc0326ef0>] ? taskq_cancel_id+0x140/0x140 [spl]\r\n[  720.807058]  [<ffffffff810a0c75>] kthread+0xe5/0x100\r\n[  720.807060]  [<ffffffff810a0b90>] ? kthread_create_on_node+0x1e0/0x1e0\r\n[  720.807062]  [<ffffffff81844b8f>] ret_from_fork+0x3f/0x70\r\n[  720.807064]  [<ffffffff810a0b90>] ? kthread_create_on_node+0x1e0/0x1e0\r\n\r\n\r\nstrace of process  zfs list -t snapshot  -s creation \r\n\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nbrk(0x185c000)                          = 0x185c000\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x15, 0x00), 0x7fffdb0e8ac0) = -1 ESRCH (No such process)\r\nioctl(3, _IOC(0, 0x5a, 0x14, 0x00), 0x7fffdb0ec0f0) = 0\r\nioctl(3, _IOC(0, 0x5a, 0x14, 0x00), 0x7fffdb0e8ac0) = -1 ESRCH (No such process)\r\n[kern.log](https://github.com/zfsonlinux/zfs/files/1521627/kern.log)\r\n[strace_zfs_list.log](https://github.com/zfsonlinux/zfs/files/1521628/strace_zfs_list.log)\r\n\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6914/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "phreaker0": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6916", "title": "recovery import (zpool import -F) issue with encrypted pool", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 16.04.3 LTS\r\nLinux Kernel                 | 4.14.0-pf4+\r\nArchitecture                 | x86_64\r\nZFS Version                  | v0.7.0-198_g56d8d8a\r\nSPL Version                  | v0.7.0-21_ged19bcc\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nI'm using an ZFS pool on an external USB disk for backup and also for data. Everything\r\nis encrypted by using a top level dataset with activated encryption (aes-256-gcm). Sometimes\r\nthe usb disk disconnects (because the cable loosens, ...), in that case the pool i/o will be suspended. After reconnecting the drive I can resume the pool by some zpool command (i think I always uses the zpool clear one). Until the last time it disconnected, i reconnected the drive and uses the command again to resume pool i/o, but it just hanged. After a day or so I rebooted the system (all the zfs/zpool commands were stuck at this point).\r\nI tried to import the zfs pool but it couldn't because of corrupted metadata and suggested recovery import to rollback the last transactions. Attempting to do so freezes the import command and nothing is happening after some panics in the log. (I tried with the zfs_recover =1 option)\r\n\r\nWhat worked is importing the pool in read only mount with recovery: zpool import -F -N -o readonly=on ext\r\nI don't care about the broken pool, as I could send all the snapshots of the data in readonly mode to another pool and most of the backup data too (one dataset snapshot send/recv failed to complete, I don't know why). But maybe there is an issue with recovery code in case of encryption?\r\n\r\nI always use the latest kernel and zfs master version on this machine (root filesystem is also on an ZFS SSD pool)\r\n\r\n### Describe how to reproduce the problem\r\n\r\nnot easily possible I guess\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\n```\r\nroot@token:~# zpool import\r\n   pool: ext\r\n     id: 15081949372965998973\r\n  state: FAULTED\r\n status: The pool metadata is corrupted.\r\n action: The pool cannot be imported due to damaged devices or data.\r\n        The pool may be active on another system, but can be imported using\r\n        the '-f' flag.\r\n   see: http://zfsonlinux.org/msg/ZFS-8000-72\r\n config:\r\n\r\n        ext                                      FAULTED  corrupted data\r\n          usb-ORICO_6518US3_0123456789ABCDE-0:0  ONLINE\r\nroot@token:~# zpool import ext\r\ncannot import 'ext': I/O error\r\n        Recovery is possible, but will result in some data loss.\r\n        Returning the pool to its state as of Mon 27 Nov 2017 09:05:11 CET\r\n        should correct the problem.  Approximately 61 seconds of data\r\n        must be discarded, irreversibly.  Recovery can be attempted\r\n        by executing 'zpool import -F ext'.  A scrub of the pool\r\n        is strongly recommended after recovery.\r\nroot@token:~# zpool import -N -F ext\r\n\r\nNov 30 11:48:25 token kernel: SPL: Loaded module v0.7.0-21_ged19bcc\r\nNov 30 11:48:26 token kernel: ZFS: Loaded module v0.7.0-198_g56d8d8a, ZFS pool version 5000, ZFS filesystem version 5\r\nNov 30 11:49:06 token kernel: ZFS: Unable to set \"noop\" scheduler for /dev/disk/by-id/usb-ORICO_6518US3_0123456789ABCDE-0:0-part1 (sdb): 256\r\nNov 30 11:49:06 token kernel: ZFS: Unable to set \"noop\" scheduler for /dev/disk/by-id/usb-ORICO_6518US3_0123456789ABCDE-0:0-part1 (sdb): 256\r\nNov 30 11:49:06 token kernel: ZFS: Unable to set \"noop\" scheduler for /dev/disk/by-id/usb-ORICO_6518US3_0123456789ABCDE-0:0-part1 (sdb): 256\r\nNov 30 11:49:06 token kernel: ZFS: Unable to set \"noop\" scheduler for /dev/disk/by-id/usb-ORICO_6518US3_0123456789ABCDE-0:0-part1 (sdb): 256\r\nNov 30 11:49:48 token kernel: ZFS: Unable to set \"noop\" scheduler for /dev/disk/by-id/usb-ORICO_6518US3_0123456789ABCDE-0:0-part1 (sdb): 256\r\nNov 30 11:49:48 token kernel: ZFS: Unable to set \"noop\" scheduler for /dev/disk/by-id/usb-ORICO_6518US3_0123456789ABCDE-0:0-part1 (sdb): 256\r\nNov 30 11:49:48 token kernel: ZFS: Unable to set \"noop\" scheduler for /dev/disk/by-id/usb-ORICO_6518US3_0123456789ABCDE-0:0-part1 (sdb): 256\r\nNov 30 11:49:48 token kernel: ZFS: Unable to set \"noop\" scheduler for /dev/disk/by-id/usb-ORICO_6518US3_0123456789ABCDE-0:0-part1 (sdb): 256\r\nNov 30 11:49:53 token kernel: VERIFY3(0 == spa_do_crypt_abd(B_TRUE, spa, zio->io_bookmark.zb_objset, bp, zio->io_txg, psize, zio->io_abd, eabd, iv, mac, salt, &no_crypt)) failed (0 == 2)\r\nNov 30 11:49:53 token kernel: VERIFY3(0 == spa_do_crypt_abd(B_TRUE, spa, zio->io_bookmark.zb_objset, bp, zio->io_txg, psize, zio->io_abd, eabd, iv, mac, salt, &no_crypt)) failed (0 == 2)\r\nNov 30 11:49:53 token kernel: PANIC at zio.c:3705:zio_encrypt()\r\nNov 30 11:49:53 token kernel: Showing stack for process 3705\r\nNov 30 11:49:53 token kernel: CPU: 0 PID: 3705 Comm: z_wr_iss Tainted: P           OE   4.14.0-pf4+ #1\r\nNov 30 11:49:53 token kernel: PANIC at zio.c:3705:zio_encrypt()\r\nNov 30 11:49:53 token kernel: Hardware name: Acer Aspire V5-572G/Dazzle_CX , BIOS V2.07 07/12/2013\r\nNov 30 11:49:53 token kernel: Call Trace:\r\nNov 30 11:49:53 token kernel: Showing stack for process 3704\r\nNov 30 11:49:53 token kernel:  dump_stack+0x63/0x8b\r\nNov 30 11:49:53 token kernel:  spl_dumpstack+0x42/0x50 [spl]\r\nNov 30 11:49:53 token kernel: VERIFY3(0 == spa_do_crypt_abd(B_TRUE, spa, zio->io_bookmark.zb_objset, bp, zio->io_txg, psize, zio->io_abd, eabd, iv, mac, salt, &no_crypt)) failed (0 == 2)\r\nNov 30 11:49:53 token kernel: PANIC at zio.c:3705:zio_encrypt()\r\nNov 30 11:49:53 token kernel: Showing stack for process 3703\r\nNov 30 11:49:53 token kernel:  spl_panic+0xc8/0x110 [spl]\r\nNov 30 11:49:53 token kernel:  ? spa_do_crypt_abd+0x80/0x2f0 [zfs]\r\nNov 30 11:49:53 token kernel:  zio_encrypt+0x5d0/0x6b0 [zfs]\r\nNov 30 11:49:53 token kernel:  ? zio_buf_free+0x58/0x60 [zfs]\r\nNov 30 11:49:53 token kernel:  ? zio_write_compress+0x496/0x6a0 [zfs]\r\nNov 30 11:49:53 token kernel:  ? __schedule+0x3ca/0x890\r\nNov 30 11:49:53 token kernel:  zio_execute+0x8a/0xf0 [zfs]\r\nNov 30 11:49:53 token kernel:  taskq_thread+0x2ab/0x4d0 [spl]\r\nNov 30 11:49:53 token kernel:  ? wake_up_q+0x70/0x70\r\nNov 30 11:49:53 token kernel:  ? zio_reexecute+0x3d0/0x3d0 [zfs]\r\nNov 30 11:49:53 token kernel:  kthread+0x108/0x140\r\nNov 30 11:49:53 token kernel:  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\nNov 30 11:49:53 token kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 30 11:49:53 token kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 30 11:49:53 token kernel:  ret_from_fork+0x25/0x30\r\nNov 30 11:49:53 token kernel: CPU: 1 PID: 3704 Comm: z_wr_iss Tainted: P           OE   4.14.0-pf4+ #1\r\nNov 30 11:49:53 token kernel: Hardware name: Acer Aspire V5-572G/Dazzle_CX , BIOS V2.07 07/12/2013\r\nNov 30 11:49:53 token kernel: Call Trace:\r\nNov 30 11:49:53 token kernel:  dump_stack+0x63/0x8b\r\nNov 30 11:49:53 token kernel:  spl_dumpstack+0x42/0x50 [spl]\r\nNov 30 11:49:53 token kernel:  spl_panic+0xc8/0x110 [spl]\r\nNov 30 11:49:53 token kernel:  ? spa_do_crypt_abd+0x80/0x2f0 [zfs]\r\nNov 30 11:49:53 token kernel:  zio_encrypt+0x5d0/0x6b0 [zfs]\r\nNov 30 11:49:53 token kernel:  ? zio_buf_free+0x58/0x60 [zfs]\r\nNov 30 11:49:53 token kernel:  ? zio_write_compress+0x496/0x6a0 [zfs]\r\nNov 30 11:49:53 token kernel:  ? __schedule+0x3ca/0x890\r\nNov 30 11:49:53 token kernel:  zio_execute+0x8a/0xf0 [zfs]\r\nNov 30 11:49:53 token kernel:  taskq_thread+0x2ab/0x4d0 [spl]\r\nNov 30 11:49:53 token kernel:  ? wake_up_q+0x70/0x70\r\nNov 30 11:49:53 token kernel:  ? zio_reexecute+0x3d0/0x3d0 [zfs]\r\nNov 30 11:49:53 token kernel:  kthread+0x108/0x140\r\nNov 30 11:49:53 token kernel:  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\nNov 30 11:49:53 token kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 30 11:49:53 token kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 30 11:49:53 token kernel:  ret_from_fork+0x25/0x30\r\nNov 30 11:49:53 token kernel: CPU: 2 PID: 3703 Comm: z_wr_iss Tainted: P           OE   4.14.0-pf4+ #1\r\nNov 30 11:49:53 token kernel: Hardware name: Acer Aspire V5-572G/Dazzle_CX , BIOS V2.07 07/12/2013\r\nNov 30 11:49:53 token kernel: Call Trace:\r\nNov 30 11:49:53 token kernel:  dump_stack+0x63/0x8b\r\nNov 30 11:49:53 token kernel:  spl_dumpstack+0x42/0x50 [spl]\r\nNov 30 11:49:53 token kernel:  spl_panic+0xc8/0x110 [spl]\r\nNov 30 11:49:53 token kernel:  ? spa_do_crypt_abd+0x80/0x2f0 [zfs]\r\nNov 30 11:49:53 token kernel:  zio_encrypt+0x5d0/0x6b0 [zfs]\r\nNov 30 11:49:53 token kernel:  ? zio_buf_free+0x58/0x60 [zfs]\r\nNov 30 11:49:53 token kernel:  ? zio_write_compress+0x496/0x6a0 [zfs]\r\nNov 30 11:49:53 token kernel:  ? __schedule+0x3ca/0x890\r\nNov 30 11:49:53 token kernel:  zio_execute+0x8a/0xf0 [zfs]\r\nNov 30 11:49:53 token kernel:  taskq_thread+0x2ab/0x4d0 [spl]\r\nNov 30 11:49:53 token kernel:  ? wake_up_q+0x70/0x70\r\nNov 30 11:49:53 token kernel:  ? zio_reexecute+0x3d0/0x3d0 [zfs]\r\nNov 30 11:49:53 token kernel:  kthread+0x108/0x140\r\nNov 30 11:49:53 token kernel:  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\nNov 30 11:49:53 token kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 30 11:49:53 token kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 30 11:49:53 token kernel:  ret_from_fork+0x25/0x30\r\nNov 30 11:53:37 token kernel: INFO: task zpool:3532 blocked for more than 120 seconds.\r\nNov 30 11:53:37 token kernel:       Tainted: P           OE   4.14.0-pf4+ #1\r\nNov 30 11:53:37 token kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nNov 30 11:53:37 token kernel: zpool           D    0  3532   2766 0x00000000\r\nNov 30 11:53:37 token kernel: Call Trace:\r\nNov 30 11:53:37 token kernel:  __schedule+0x3c2/0x890\r\nNov 30 11:53:37 token kernel:  schedule+0x36/0x80\r\nNov 30 11:53:37 token kernel:  cv_wait_common+0x101/0x140 [spl]\r\nNov 30 11:53:37 token kernel:  ? wait_woken+0x80/0x80\r\nNov 30 11:53:37 token kernel:  __cv_wait+0x15/0x20 [spl]\r\nNov 30 11:53:37 token kernel:  txg_wait_synced+0xdc/0x130 [zfs]\r\nNov 30 11:53:37 token kernel:  spa_load+0x1920/0x20d0 [zfs]\r\nNov 30 11:53:37 token kernel:  spa_load_best+0x182/0x280 [zfs]\r\nNov 30 11:53:37 token kernel:  spa_import+0x208/0x740 [zfs]\r\nNov 30 11:53:37 token kernel:  zfs_ioc_pool_import+0x130/0x140 [zfs]\r\nNov 30 11:53:37 token kernel:  zfsdev_ioctl+0x590/0x620 [zfs]\r\nNov 30 11:53:37 token kernel:  do_vfs_ioctl+0xa1/0x5e0\r\nNov 30 11:53:37 token kernel:  ? putname+0x53/0x60\r\nNov 30 11:53:37 token kernel:  SyS_ioctl+0x79/0x90\r\nNov 30 11:53:37 token kernel:  entry_SYSCALL_64_fastpath+0x1e/0xa9\r\nNov 30 11:53:37 token kernel: RIP: 0033:0x7fdf0bcdcf07\r\nNov 30 11:53:37 token kernel: RSP: 002b:00007fff01d9fbe8 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\r\nNov 30 11:53:37 token kernel: RAX: ffffffffffffffda RBX: 00000000017e7420 RCX: 00007fdf0bcdcf07\r\nNov 30 11:53:37 token kernel: RDX: 00007fff01d9fc50 RSI: 0000000000005a02 RDI: 0000000000000003\r\nNov 30 11:53:37 token kernel: RBP: 0000000000000000 R08: 00000000017f6230 R09: 00000000017f4de0\r\nNov 30 11:53:37 token kernel: R10: 000000000000027a R11: 0000000000000246 R12: 0000000000000000\r\nNov 30 11:53:37 token kernel: R13: 000000000000002d R14: 00000000017e7910 R15: 00000000017e7420\r\nNov 30 11:53:37 token kernel: INFO: task z_wr_iss:3703 blocked for more than 120 seconds.\r\nNov 30 11:53:37 token kernel:       Tainted: P           OE   4.14.0-pf4+ #1\r\nNov 30 11:53:37 token kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nNov 30 11:53:37 token kernel: z_wr_iss        D    0  3703      2 0x80000000\r\nNov 30 11:53:37 token kernel: Call Trace:\r\nNov 30 11:53:37 token kernel:  __schedule+0x3c2/0x890\r\nNov 30 11:53:37 token kernel:  schedule+0x36/0x80\r\nNov 30 11:53:37 token kernel:  spl_panic+0xfa/0x110 [spl]\r\nNov 30 11:53:37 token kernel:  ? spa_do_crypt_abd+0x80/0x2f0 [zfs]\r\nNov 30 11:53:37 token kernel:  zio_encrypt+0x5d0/0x6b0 [zfs]\r\nNov 30 11:53:37 token kernel:  ? zio_buf_free+0x58/0x60 [zfs]\r\nNov 30 11:53:37 token kernel:  ? zio_write_compress+0x496/0x6a0 [zfs]\r\nNov 30 11:53:37 token kernel:  ? __schedule+0x3ca/0x890\r\nNov 30 11:53:37 token kernel:  zio_execute+0x8a/0xf0 [zfs]\r\nNov 30 11:53:37 token kernel:  taskq_thread+0x2ab/0x4d0 [spl]\r\nNov 30 11:53:37 token kernel:  ? wake_up_q+0x70/0x70\r\nNov 30 11:53:37 token kernel:  ? zio_reexecute+0x3d0/0x3d0 [zfs]\r\nNov 30 11:53:37 token kernel:  kthread+0x108/0x140\r\nNov 30 11:53:37 token kernel:  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\nNov 30 11:53:37 token kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 30 11:53:37 token kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 30 11:53:37 token kernel:  ret_from_fork+0x25/0x30\r\nNov 30 11:53:37 token kernel: INFO: task z_wr_iss:3704 blocked for more than 120 seconds.\r\nNov 30 11:53:37 token kernel:       Tainted: P           OE   4.14.0-pf4+ #1\r\nNov 30 11:53:37 token kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nNov 30 11:53:37 token kernel: z_wr_iss        D    0  3704      2 0x80000000\r\nNov 30 11:53:37 token kernel: Call Trace:\r\nNov 30 11:53:37 token kernel:  __schedule+0x3c2/0x890\r\nNov 30 11:53:37 token kernel:  schedule+0x36/0x80\r\nNov 30 11:53:37 token kernel:  spl_panic+0xfa/0x110 [spl]\r\nNov 30 11:53:37 token kernel:  ? spa_do_crypt_abd+0x80/0x2f0 [zfs]\r\nNov 30 11:53:37 token kernel:  zio_encrypt+0x5d0/0x6b0 [zfs]\r\nNov 30 11:53:37 token kernel:  ? zio_buf_free+0x58/0x60 [zfs]\r\nNov 30 11:53:37 token kernel:  ? zio_write_compress+0x496/0x6a0 [zfs]\r\nNov 30 11:53:37 token kernel:  ? __schedule+0x3ca/0x890\r\nNov 30 11:53:37 token kernel:  zio_execute+0x8a/0xf0 [zfs]\r\nNov 30 11:53:37 token kernel:  taskq_thread+0x2ab/0x4d0 [spl]\r\nNov 30 11:53:37 token kernel:  ? wake_up_q+0x70/0x70\r\nNov 30 11:53:37 token kernel:  ? zio_reexecute+0x3d0/0x3d0 [zfs]\r\nNov 30 11:53:37 token kernel:  kthread+0x108/0x140\r\nNov 30 11:53:37 token kernel:  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\nNov 30 11:53:37 token kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 30 11:53:37 token kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 30 11:53:37 token kernel:  ret_from_fork+0x25/0x30\r\nNov 30 11:53:37 token kernel: INFO: task z_wr_iss:3705 blocked for more than 120 seconds.\r\nNov 30 11:53:37 token kernel:       Tainted: P           OE   4.14.0-pf4+ #1\r\nNov 30 11:53:37 token kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nNov 30 11:53:37 token kernel: z_wr_iss        D    0  3705      2 0x80000000\r\nNov 30 11:53:37 token kernel: Call Trace:\r\nNov 30 11:53:37 token kernel:  __schedule+0x3c2/0x890\r\nNov 30 11:53:37 token kernel:  schedule+0x36/0x80\r\nNov 30 11:53:37 token kernel:  spl_panic+0xfa/0x110 [spl]\r\nNov 30 11:53:37 token kernel:  ? spa_do_crypt_abd+0x80/0x2f0 [zfs]\r\nNov 30 11:53:37 token kernel:  zio_encrypt+0x5d0/0x6b0 [zfs]\r\nNov 30 11:53:37 token kernel:  ? zio_buf_free+0x58/0x60 [zfs]\r\nNov 30 11:53:37 token kernel:  ? zio_write_compress+0x496/0x6a0 [zfs]\r\nNov 30 11:53:37 token kernel:  ? __schedule+0x3ca/0x890\r\nNov 30 11:53:37 token kernel:  zio_execute+0x8a/0xf0 [zfs]\r\nNov 30 11:53:37 token kernel:  taskq_thread+0x2ab/0x4d0 [spl]\r\nNov 30 11:53:37 token kernel:  ? wake_up_q+0x70/0x70\r\nNov 30 11:53:37 token kernel:  ? zio_reexecute+0x3d0/0x3d0 [zfs]\r\nNov 30 11:53:37 token kernel:  kthread+0x108/0x140\r\nNov 30 11:53:37 token kernel:  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\nNov 30 11:53:37 token kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 30 11:53:37 token kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 30 11:53:37 token kernel:  ret_from_fork+0x25/0x30\r\nNov 30 11:53:37 token kernel: INFO: task txg_sync:3850 blocked for more than 120 seconds.\r\nNov 30 11:53:37 token kernel:       Tainted: P           OE   4.14.0-pf4+ #1\r\nNov 30 11:53:37 token kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nNov 30 11:53:37 token kernel: txg_sync        D    0  3850      2 0x80000000\r\nNov 30 11:53:37 token kernel: Call Trace:\r\nNov 30 11:53:37 token kernel:  __schedule+0x3c2/0x890\r\nNov 30 11:53:37 token kernel:  ? zio_nowait+0xb6/0x150 [zfs]\r\nNov 30 11:53:37 token kernel:  schedule+0x36/0x80\r\nNov 30 11:53:37 token kernel:  io_schedule+0x16/0x40\r\nNov 30 11:53:37 token kernel:  cv_wait_common+0xb2/0x140 [spl]\r\nNov 30 11:53:37 token kernel:  ? wait_woken+0x80/0x80\r\nNov 30 11:53:37 token kernel:  __cv_wait_io+0x18/0x20 [spl]\r\nNov 30 11:53:37 token kernel:  zio_wait+0xfd/0x1b0 [zfs]\r\nNov 30 11:53:37 token kernel:  dsl_pool_sync+0x1d1/0x420 [zfs]\r\nNov 30 11:53:37 token kernel:  spa_sync+0x41d/0xda0 [zfs]\r\nNov 30 11:53:37 token kernel:  txg_sync_thread+0x2d4/0x4a0 [zfs]\r\nNov 30 11:53:37 token kernel:  ? txg_delay+0x170/0x170 [zfs]\r\nNov 30 11:53:37 token kernel:  thread_generic_wrapper+0x72/0x80 [spl]\r\nNov 30 11:53:37 token kernel:  kthread+0x108/0x140\r\nNov 30 11:53:37 token kernel:  ? __thread_exit+0x20/0x20 [spl]\r\nNov 30 11:53:37 token kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 30 11:53:37 token kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 30 11:53:37 token kernel:  ret_from_fork+0x25/0x30\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6539", "title": "kmod package generation fails", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 16.04.3 LTS\r\nLinux Kernel                 | 4.12.0-pf7+\r\nArchitecture                 | x86_64\r\nZFS Version                  | master\r\nSPL Version                  | master\r\n\r\n### Describe the problem you're observing\r\n\r\nsince the native encryption patch (#5769) was merged kmod packages can't be generated any more.\r\n\r\n### Describe how to reproduce the problem\r\n\r\n$ cd zfs\r\n$ ./configure\r\n$ make pkg-utils pkg-kmod\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\nrelevant output from the make command:\r\n```\r\n  CCLD     many_fds\r\n../../../../../lib/libzfs/.libs/libzfs.so: undefined reference to `lzc_load_key'\r\n../../../../../lib/libzfs/.libs/libzfs.so: undefined reference to `lzc_change_key'\r\n../../../../../lib/libzfs/.libs/libzfs.so: undefined reference to `lzc_unload_key'\r\ncollect2: error: ld returned 1 exit status\r\nMakefile:622: recipe for target 'many_fds' failed\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6539/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "prakashsurya": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6901", "title": "Hang when running zloop - apparently due to \"zio_wait\"", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 17.04\r\nLinux Kernel                 | 4.10.0-19-generic\r\nArchitecture                 | x86_64\r\nZFS Version                  | zfsonlinux/zfs@94183a9d8a1133ff0d29666a86f84c24f2c4083c\r\nSPL Version                  | zfsonlinux/spl@ed19bccfb651843fa208232b3a2d3d22a4152bc8\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nWhen running `zloop` indefinitely, I'm seeing cases where `ztest` appears to hang. The first sign of trouble, is `zloop` will stop outputting new lines; e.g.:\r\n```\r\n$ sudo ./scripts/zloop.sh -f /var/tmp/zloop -c /var/tmp/zloop\r\ncore dump directory (/var/tmp/zloop) does not exist, creating it.\r\n11/22 23:57:33 /export/home/delphix/zfs/bin/ztest -VVVVV -m 2 -r 3 -R 2 -v 3 -a 12 -T 61 -P 15 -s 512m -f /var/tmp/zloop/zloop-run\r\n...\r\n11/23 09:38:46 /export/home/delphix/zfs/bin/ztest -VVVVV -m 1 -r 0 -R 3 -v 4 -a 9 -T 71 -P 10 -s 512m -f /var/tmp/zloop/zloop-run\r\n```\r\nIn this example, `zloop` ran for nearly 12 hours, kicking off hundreds of `ztest` invocations in the process (estimating, I didn't count), until this last `ztest` process hangs (it's been in this state for several days).\r\n\r\nThere's a single `ztest` process running on the system right now:\r\n```\r\n$ date\r\nMon Nov 27 22:26:06 UTC 2017\r\n\r\n$ pgrep ztest | xargs ps -f -p\r\nUID        PID  PPID  C STIME TTY      STAT   TIME CMD\r\nroot      2570  3102  0 Nov23 pts/1    S+     0:03 /export/home/delphix/zfs/cmd/ztest/.libs/ztest -VVVVV -m 1 -r 0 -R 3 -v\r\nroot     17724  2570  9 Nov23 pts/1    Sl+  629:30 /export/home/delphix/zfs/cmd/ztest/.libs/ztest\r\n\r\n$ pstree -p 2570 | head\r\nztest(2570)---ztest(17724)-+-{ztest}(17725)\r\n                           |-{ztest}(17726)\r\n                           |-{ztest}(17727)\r\n                           |-{ztest}(17728)\r\n                           |-{ztest}(17729)\r\n                           |-{ztest}(17730)\r\n                           |-{ztest}(17731)\r\n                           |-{ztest}(17732)\r\n                           |-{ztest}(17733)\r\n                           |-{ztest}(17734)\r\n```\r\n\r\nI then used `gcore` to get a core dump of the process:\r\n```\r\n$ sudo gcore 17724\r\n...\r\nSaved corefile core.17724\r\n```\r\n\r\nAnd then `gdb` to inspect the corefile, to see what's going on:\r\n```\r\n$ sudo gdb /export/home/delphix/zfs/cmd/ztest/.libs/ztest core.17724\r\n```\r\n\r\nThe \"main\" `ztest` thread appears to be waiting for all other threads to finish:\r\n```\r\n(gdb) bt\r\n#0  0x00007f83848f79dd in pthread_join (threadid=140202638337792, thread_return=0x0) at pthread_join.c:90\r\n#1  0x000055f212e27074 in ztest_run (zs=0x7f8385b574a0) at ztest.c:6543\r\n#2  0x000055f212e27c9a in main (argc=<optimized out>, argv=0x7ffe42869718) at ztest.c:7092\r\n```\r\n\r\nUsing `thread apply all where` in `gdb` to inspect the stacks of all threads, most stacks look benign, but I see one in `zio_wait`:\r\n```\r\nThread 575 (Thread 0x7f8378746700 (LWP 18780)):\r\n#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\r\n#1  0x00007f8384e585e4 in cv_wait (cv=cv@entry=0x7f82d413cbf0, mp=mp@entry=0x7f82d413cbc0) at kernel.c:328\r\n#2  0x00007f8384f97e7b in zio_wait (zio=0x7f82d413c810) at ../../module/zfs/zio.c:1891\r\n#3  0x00007f8384e737c8 in arc_read (pio=pio@entry=0x0, spa=0x55f21512da40, bp=bp@entry=0x7f8378742920, done=0x7f8384e729c0 <arc_getbuf_func>, private=private@entry=0x7f83787427e8, priority=priority@entry=ZIO_PRIORITY_SYNC_READ, zio_flags=25166224, arc_flags=0x7f83787427e4, zb=0x7f83787427f0) at ../../module/zfs/arc.c:6301\r\n#4  0x00007f8384f8bd91 in zil_read_log_block (decrypt=decrypt@entry=B_FALSE, bp=bp@entry=0x7f8378742920, nbp=nbp@entry=0x7f83787429a0, dst=dst@entry=0x7f82d4320000, end=0x7f8378742918, zilog=<optimized out>, zilog=<optimized out>) at ../../module/zfs/zil.c:217\r\n#5  0x00007f8384f8c66c in zil_parse (zilog=zilog@entry=0x7f82d4007810, parse_blk_func=parse_blk_func@entry=0x7f8384f8a700 <zil_free_log_block>, parse_lr_func=parse_lr_func@entry=0x7f8384f8c230 <zil_free_log_record>, arg=arg@entry=0x7f82d403adf0, txg=0, decrypt=decrypt@entry=B_FALSE) at ../../module/zfs/zil.c:374\r\n#6  0x00007f8384f8ccc4 in zil_destroy_sync (zilog=zilog@entry=0x7f82d4007810, tx=tx@entry=0x7f82d403adf0) at ../../module/zfs/zil.c:695\r\n#7  0x00007f8384f8cea3 in zil_destroy (zilog=zilog@entry=0x7f82d4007810, keep_first=keep_first@entry=B_FALSE) at ../../module/zfs/zil.c:684\r\n#8  0x00007f8384f8f99e in zil_suspend (osname=osname@entry=0x7f82d40008f0 \"ztest/temp_10\", cookiep=cookiep@entry=0x0) at ../../module/zfs/zil.c:2133\r\n#9  0x00007f8384f8fd5a in zil_vdev_offline (osname=osname@entry=0x7f82d40008f0 \"ztest/temp_10\", arg=arg@entry=0x0) at ../../module/zfs/zil.c:2350\r\n#10 0x00007f8384e90740 in dmu_objset_find_impl (spa=spa@entry=0x55f21512da40, name=name@entry=0x7f82d40008f0 \"ztest/temp_10\", func=func@entry=0x7f8384f8fd40 <zil_vdev_offline>, arg=arg@entry=0x0, flags=flags@entry=2) at ../../module/zfs/dmu_objset.c:2617\r\n#11 0x00007f8384e90806 in dmu_objset_find_impl (spa=0x55f21512da40, name=name@entry=0x55f21512da40 \"ztest\", func=func@entry=0x7f8384f8fd40 <zil_vdev_offline>, arg=arg@entry=0x0, flags=flags@entry=2) at ../../module/zfs/dmu_objset.c:2560\r\n#12 0x00007f8384e94e90 in dmu_objset_find (name=0x55f21512da40 \"ztest\", func=0x7f8384f8fd40 <zil_vdev_offline>, arg=arg@entry=0x0, flags=flags@entry=2) at ../../module/zfs/dmu_objset.c:2633\r\n#13 0x00007f8384ef4c22 in spa_offline_log (spa=spa@entry=0x55f21512da40) at ../../module/zfs/spa.c:1941\r\n#14 0x00007f8384f10cfe in vdev_offline_locked (spa=spa@entry=0x55f21512da40, guid=guid@entry=6459914980364466541, flags=flags@entry=0) at ../../module/zfs/vdev.c:2700\r\n#15 0x00007f8384f10e8f in vdev_offline (spa=0x55f21512da40, guid=6459914980364466541, flags=0) at ../../module/zfs/vdev.c:2753\r\n#16 0x000055f212e2f427 in ztest_fault_inject (zd=<optimized out>, id=<optimized out>) at ztest.c:5521\r\n#17 0x000055f212e2877c in ztest_execute (test=<optimized out>, zi=0x55f213040340 <ztest_info+512>, id=2) at ztest.c:6237\r\n#18 0x000055f212e2ee03 in ztest_thread (arg=0x2) at ztest.c:6284\r\n#19 0x00007f83848f66da in start_thread (arg=0x7f8378746700) at pthread_create.c:456\r\n#20 0x00007f8384630d7f in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:105\r\n```\r\n\r\nI'm attaching a file containing the stacks of all threads from the corefile, [gdb.txt](https://github.com/zfsonlinux/zfs/files/1507843/gdb.txt), as well as the corefile itself (compressed with `zip`), [core.17724.zip](https://github.com/zfsonlinux/zfs/files/1507837/core.17724.zip).", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6901/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/2fe61a7ecc507d031451c21b3077fae549b58ec3", "message": "OpenZFS 8909 - 8585 can cause a use-after-free kernel panic\n\nAuthored by: Prakash Surya <prakash.surya@delphix.com>\nReviewed by: John Kennedy <jwk404@gmail.com>\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\nReviewed by: George Wilson <george.wilson@delphix.com>\nReviewed by: Brad Lewis <brad.lewis@delphix.com>\nReviewed by: Igor Kozhukhov <igor@dilos.org>\nReviewed by: Brian Behlendorf <behlendorf1@llnl.gov>\nApproved by: Robert Mustacchi <rm@joyent.com>\nPorted-by: Prakash Surya <prakash.surya@delphix.com>\n\nPROBLEM\n=======\n\nThere's a race condition that exists if `zil_free_lwb` races with either\n`zil_commit_waiter_timeout` and/or `zil_lwb_flush_vdevs_done`.\n\nHere's an example panic due to this bug:\n\n    > ::status\n    debugging crash dump vmcore.0 (64-bit) from ip-10-110-205-40\n    operating system: 5.11 dlpx-5.2.2.0_2017-12-04-17-28-32b6ba51fb (i86pc)\n    image uuid: 4af0edfb-e58e-6ed8-cafc-d3e9167c7513\n    panic message:\n    BAD TRAP: type=e (#pf Page fault) rp=ffffff0010555970 addr=60 occurred in module \"zfs\" due to a NULL pointer dereference\n    dump content: kernel pages only\n\n    > $c\n    zio_shrink+0x12()\n    zil_lwb_write_issue+0x30d(ffffff03dcd15cc0, ffffff03e0730e20)\n    zil_commit_waiter_timeout+0xa2(ffffff03dcd15cc0, ffffff03d97ffcf8)\n    zil_commit_waiter+0xf3(ffffff03dcd15cc0, ffffff03d97ffcf8)\n    zil_commit+0x80(ffffff03dcd15cc0, 9a9)\n    zfs_write+0xc34(ffffff03dc38b140, ffffff0010555e60, 40, ffffff03e00fb758, 0)\n    fop_write+0x5b(ffffff03dc38b140, ffffff0010555e60, 40, ffffff03e00fb758, 0)\n    write+0x250(42, fffffd7ff4832000, 2000)\n    sys_syscall+0x177()\n\nIf there's an outstanding lwb that's in `zil_commit_waiter_timeout`\nwaiting to timeout, waiting on it's waiter's CV, we must be sure not to\ncall `zil_free_lwb`. If we end up calling `zil_free_lwb`, then that LWB\nmay be freed and can result in a use-after-free situation where the\nstale lwb pointer stored in the `zil_commit_waiter_t` structure of the\nthread waiting on the waiter's CV is used.\n\nA similar situation can occur if an lwb is issued to disk, and thus in\nthe `LWB_STATE_ISSUED` state, and `zil_free_lwb` is called while the\ndisk is servicing that lwb. In this situation, the lwb will be freed by\n`zil_free_lwb`, which will result in a use-after-free situation when the\nlwb's zio completes, and `zil_lwb_flush_vdevs_done` is called.\n\nThis race condition is prevented in `zil_close` by calling `zil_commit`\nbefore `zil_free_lwb` is called, which will ensure all outstanding (i.e.\nall lwb's in the `LWB_STATE_OPEN` and/or `LWB_STATE_ISSUED` states)\nreach the `LWB_STATE_DONE` state before the lwb's are freed\n(`zil_commit` will not return untill all the lwb's are\n`LWB_STATE_DONE`).\n\nFurther, this race condition is prevented in `zil_sync` by only calling\n`zil_free_lwb` for lwb's that do not have their `lwb_buf` pointer set.\nAll lwb's not in the `LWB_STATE_DONE` state will have a non-null value\nfor this pointer; the pointer is only cleared in\n`zil_lwb_flush_vdevs_done`, at which point the lwb's state will be\nchanged to `LWB_STATE_DONE`.\n\nThis race *is* present in `zil_suspend`, leading to this bug.\n\nAt first glance, it would appear as though this would not be true\nbecause `zil_suspend` will call `zil_commit`, just like `zil_close`, but\nthe problem is that `zil_suspend` will set the zilog's `zl_suspend`\nfield prior to calling `zil_commit`. Further, in `zil_commit`, if\n`zl_suspend` is set, `zil_commit` will take a special branch of logic\nand use `txg_wait_synced` instead of performing the normal `zil_commit`\nlogic.\n\nThis call to `txg_wait_synced` might be good enough for the data to\nreach disk safely before it returns, but it does not ensure that all\noutstanding lwb's reach the `LWB_STATE_DONE` state before it returns.\nThis is because, if there's an lwb \"stuck\" in\n`zil_commit_waiter_timeout`, waiting for it's lwb to timeout, it will\nmaintain a non-null value for it's `lwb_buf` field and thus `zil_sync`\nwill not free that lwb. Thus, even though the lwb's data is already on\ndisk, the lwb will be left lingering, waiting on the CV, and will\neventually timeout and be issued to disk even though the write is\nunnecessary.\n\nSo, after `zil_commit` is called from `zil_suspend`, we incorrectly\nassume that there are not outstanding lwb's, and proceed to free all\nlwb's found on the zilog's lwb list. As a result, we free the lwb that\nwill later be used `zil_commit_waiter_timeout`.\n\nSOLUTION\n========\n\nThe solution to this, is to ensure all outstanding lwb's complete before\ncalling `zil_free_lwb` via `zil_destroy` in `zil_suspend`. This patch\naccomplishes this goal by forcing the normal `zil_commit` logic when\ncalled from `zil_sync`.\n\nNow, `zil_suspend` will call `zil_commit_impl` which will always use the\nnormal logic of waiting/issuing lwb's to disk before it returns. As a\nresult, any lwb's outstanding when `zil_commit_impl` is called will be\nguaranteed to reach the `LWB_STATE_DONE` state by the time it returns.\n\nFurther, no new lwb's will be created via `zil_commit` since the zilog's\n`zl_suspend` flag will be set. This will force all new callers of\n`zil_commit` to use `txg_wait_synced` instead of creating and issuing\nnew lwb's.\n\nThus, all lwb's left on the zilog's lwb list when `zil_destroy` is\ncalled will be in the `LWB_STATE_DONE` state, and we'll avoid this race\ncondition.\n\nOpenZFS-issue: https://www.illumos.org/issues/8909\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/ece62b6f8d\nCloses #6940"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/1b2b0acab54ad4320e9fab9f46612fdb2a71cf87", "message": "OpenZFS 8603 - rename zilog's \"zl_writer_lock\" to \"zl_issuer_lock\"\n\nThis is a purely cosmetic change. The zilog's \"zl_writer_lock\" field is\nbeing renamed to \"zl_issuer_lock\" to try and make the code easier to\nunderstand; no other changes are made.\n\nAuthored by: Prakash Surya <prakash.surya@delphix.com>\nReviewed by: C Fraire <cfraire@me.com>\nApproved by: Dan McDonald <danmcd@joyent.com>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nPorted-by: Giuseppe Di Natale <dinatale2@llnl.gov>\n\nOpenZFS-issue: https://www.illumos.org/issues/8603\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/2daf06546b\nCloses #6927"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/1ce23dcaff6c3d777cb0d9a4a2cf02b43f777d78", "message": "OpenZFS 8585 - improve batching done in zil_commit()\n\nAuthored by: Prakash Surya <prakash.surya@delphix.com>\r\nReviewed by: Brad Lewis <brad.lewis@delphix.com>\r\nReviewed by: Matt Ahrens <mahrens@delphix.com>\r\nReviewed by: George Wilson <george.wilson@delphix.com>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nApproved by: Dan McDonald <danmcd@joyent.com>\r\nPorted-by: Prakash Surya <prakash.surya@delphix.com>\r\n\r\nProblem\r\n=======\r\n\r\nThe current implementation of zil_commit() can introduce significant\r\nlatency, beyond what is inherent due to the latency of the underlying\r\nstorage. The additional latency comes from two main problems:\r\n\r\n 1. When there's outstanding ZIL blocks being written (i.e. there's\r\n    already a \"writer thread\" in progress), then any new calls to\r\n    zil_commit() will block waiting for the currently oustanding ZIL\r\n    blocks to complete. The blocks written for each \"writer thread\" is\r\n    coined a \"batch\", and there can only ever be a single \"batch\" being\r\n    written at a time. When a batch is being written, any new ZIL\r\n    transactions will have to wait for the next batch to be written,\r\n    which won't occur until the current batch finishes.\r\n\r\n    As a result, the underlying storage may not be used as efficiently\r\n    as possible. While \"new\" threads enter zil_commit() and are blocked\r\n    waiting for the next batch, it's possible that the underlying\r\n    storage isn't fully utilized by the current batch of ZIL blocks. In\r\n    that case, it'd be better to allow these new threads to generate\r\n    (and issue) a new ZIL block, such that it could be serviced by the\r\n    underlying storage concurrently with the other ZIL blocks that are\r\n    being serviced.\r\n\r\n 2. Any call to zil_commit() must wait for all ZIL blocks in its \"batch\"\r\n    to complete, prior to zil_commit() returning. The size of any given\r\n    batch is proportional to the number of ZIL transaction in the queue\r\n    at the time that the batch starts processing the queue; which\r\n    doesn't occur until the previous batch completes. Thus, if there's a\r\n    lot of transactions in the queue, the batch could be composed of\r\n    many ZIL blocks, and each call to zil_commit() will have to wait for\r\n    all of these writes to complete (even if the thread calling\r\n    zil_commit() only cared about one of the transactions in the batch).\r\n\r\nTo further complicate the situation, these two issues result in the\r\nfollowing side effect:\r\n\r\n 3. If a given batch takes longer to complete than normal, this results\r\n    in larger batch sizes, which then take longer to complete and\r\n    further drive up the latency of zil_commit(). This can occur for a\r\n    number of reasons, including (but not limited to): transient changes\r\n    in the workload, and storage latency irregularites.\r\n\r\nSolution\r\n========\r\n\r\nThe solution attempted by this change has the following goals:\r\n\r\n 1. no on-disk changes; maintain current on-disk format.\r\n 2. modify the \"batch size\" to be equal to the \"ZIL block size\".\r\n 3. allow new batches to be generated and issued to disk, while there's\r\n    already batches being serviced by the disk.\r\n 4. allow zil_commit() to wait for as few ZIL blocks as possible.\r\n 5. use as few ZIL blocks as possible, for the same amount of ZIL\r\n    transactions, without introducing significant latency to any\r\n    individual ZIL transaction. i.e. use fewer, but larger, ZIL blocks.\r\n\r\nIn theory, with these goals met, the new allgorithm will allow the\r\nfollowing improvements:\r\n\r\n 1. new ZIL blocks can be generated and issued, while there's already\r\n    oustanding ZIL blocks being serviced by the storage.\r\n 2. the latency of zil_commit() should be proportional to the underlying\r\n    storage latency, rather than the incoming synchronous workload.\r\n\r\nPorting Notes\r\n=============\r\n\r\nDue to the changes made in commit 119a394ab0, the lifetime of an itx\r\nstructure differs than in OpenZFS. Specifically, the itx structure is\r\nkept around until the data associated with the itx is considered to be\r\nsafe on disk; this is so that the itx's callback can be called after the\r\ndata is committed to stable storage. Since OpenZFS doesn't have this itx\r\ncallback mechanism, it's able to destroy the itx structure immediately\r\nafter the itx is committed to an lwb (before the lwb is written to\r\ndisk).\r\n\r\nTo support this difference, and to ensure the itx's callbacks can still\r\nbe called after the itx's data is on disk, a few changes had to be made:\r\n\r\n  * A list of itxs was added to the lwb structure. This list contains\r\n    all of the itxs that have been committed to the lwb, such that the\r\n    callbacks for these itxs can be called from zil_lwb_flush_vdevs_done(),\r\n    after the data for the itxs is committed to disk.\r\n\r\n  * A list of itxs was added on the stack of the zil_process_commit_list()\r\n    function; the \"nolwb_itxs\" list. In some circumstances, an itx may\r\n    not be committed to an lwb (e.g. if allocating the \"next\" ZIL block\r\n    on disk fails), so this list is used to keep track of which itxs\r\n    fall into this state, such that their callbacks can be called after\r\n    the ZIL's writer pipeline is \"stalled\".\r\n\r\n  * The logic to actually call the itx's callback was moved into the\r\n    zil_itx_destroy() function. Since all consumers of zil_itx_destroy()\r\n    were effectively performing the same logic (i.e. if callback is\r\n    non-null, call the callback), it seemed like useful code cleanup to\r\n    consolidate this logic into a single function.\r\n\r\nAdditionally, the existing Linux tracepoint infrastructure dealing with\r\nthe ZIL's probes and structures had to be updated to reflect these code\r\nchanges. Specifically:\r\n\r\n  * The \"zil__cw1\" and \"zil__cw2\" probes were removed, so they had to be\r\n    removed from \"trace_zil.h\" as well.\r\n\r\n  * Some of the zilog structure's fields were removed, which affected\r\n    the tracepoint definitions of the structure.\r\n\r\n  * New tracepoints had to be added for the following 3 new probes:\r\n      * zil__process__commit__itx\r\n      * zil__process__normal__itx\r\n      * zil__commit__io__error\r\n\r\nOpenZFS-issue: https://www.illumos.org/issues/8585\r\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/5d95a3a\r\nCloses #6566"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/0c484ab5677ffaa3e0e2371456deb4f8eb370388", "message": "Run ztest for longer on \"Coverage\" builders\n\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Prakash Surya <prakash.surya@delphix.com>\r\nCloses #6675"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/acf044420b134b022da5c866b19df69934ad3778", "message": "Add support for \"--enable-code-coverage\" option\n\nThis change adds support for a new option that can be passed to the\r\nconfigure script: \"--enable-code-coverage\". Further, the \"--enable-gcov\"\r\noption has been removed, as this new option provides the same\r\nfunctionality (plus more).\r\n\r\nWhen using this new option the following make targets are available:\r\n\r\n * check-code-coverage\r\n * code-coverage-capture\r\n * code-coverage-clean\r\n\r\nNote: these make targets can only be run from the root of the project.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Prakash Surya <prakash.surya@delphix.com>\r\nCloses #6670"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/6384cf4132a39dd0ada7991d551554b718041da9", "message": "Make \"-fno-inline\" compile option more accessible\n\nWhen functions are inlined, it can make the system much more difficult\r\nto instrument using tools such as ftrace, BPF, crash, etc. Thus, to aid\r\ndevelopment and increase the system's observability, when the\r\n\"--enable-debuginfo\" flag is specified, the \"-fno-inline\" compilation\r\noption will be used for both userspace and kernel modules.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Prakash Surya <prakash.surya@delphix.com>\r\nCloses #6605"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/8900345", "body": "I don't know, but I'd be interested in the answer if you can get in contact with a linux kernel developer. Either way, it's a damn shame.. There's some neat debugging, performance analysis, and just general sysadmin tools that we could build around tracepoints; but nope, they're limited to GPL modules :(\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/8900345/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "cptMikky": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6890", "title": "task z_zvol blocked for more than 120 seconds.", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Arch Linux\r\nDistribution Version    |  latest\r\nLinux Kernel                 |  linux-ck 4.13.14.1-ck\r\nArchitecture                 | 64\r\nZFS Version                  |  0.7.3-1 (AUR, dkms)\r\nSPL Version                  |  0.7.3-1 (AUR, dkms)\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nZFS hangs on zfs send | receive combo on local machine from LUKS-encrypted nvme partition (tank) to LUKS-encrypted SATA HDD connected via USB3.0 dock (dozer). The HDD is *possibly* faulty, however badblocks performed before the send|receive combo did not yield any problems.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nzpool tank has about 300 snapshots of 10 (out of 18) datasets made by zfs-auto-snapshot.\r\n\r\n```\r\nzfs send -RLc tank@send | zfs receive -Fusvd dozer\r\n```\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n\r\n-->\r\ndmesg:\r\n```\r\n[  204.780233] usb 2-4: new SuperSpeed USB device number 2 using xhci_hcd\r\n[  204.802181] usb-storage 2-4:1.0: USB Mass Storage device detected\r\n[  204.802222] usb-storage 2-4:1.0: Quirks match for vid 174c pid 55aa: 400000\r\n[  204.802236] scsi host3: usb-storage 2-4:1.0\r\n[  205.830501] scsi 3:0:0:0: Direct-Access     ASMedia  ASM1153          0    PQ: 0 ANSI: 6\r\n[  205.881441] sd 3:0:0:0: [sdb] 625134827 512-byte logical blocks: (320 GB/298 GiB)\r\n[  205.881675] sd 3:0:0:0: [sdb] Write Protect is off\r\n[  205.881677] sd 3:0:0:0: [sdb] Mode Sense: 43 00 00 00\r\n[  205.881876] sd 3:0:0:0: [sdb] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA\r\n[  205.930655]  sdb: sdb1\r\n[  205.931489] sd 3:0:0:0: [sdb] Attached SCSI disk\r\n[  858.070057] INFO: task z_zvol:17045 blocked for more than 120 seconds.\r\n[  858.070061]       Tainted: P     U     O    4.13.14-1-ck-skylake #1\r\n[  858.070062] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[  858.070063] z_zvol          D    0 17045      2 0x00000000\r\n[  858.070065] Call Trace:\r\n[  858.070070]  __schedule+0x6eb/0xcd0\r\n[  858.070072]  schedule+0x3d/0xd0\r\n[  858.070076]  taskq_wait_outstanding+0x87/0xd0 [spl]\r\n[  858.070078]  ? wait_woken+0x80/0x80\r\n[  858.070094]  zvol_task_cb+0x1fa/0x5a0 [zfs]\r\n[  858.070097]  taskq_thread+0x254/0x480 [spl]\r\n[  858.070100]  ? wake_up_q+0x80/0x80\r\n[  858.070102]  kthread+0x124/0x140\r\n[  858.070104]  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\n[  858.070106]  ? kthread_create_on_node+0x70/0x70\r\n[  858.070108]  ret_from_fork+0x25/0x30\r\n[  980.960061] INFO: task z_zvol:17045 blocked for more than 120 seconds.\r\n[  980.960066]       Tainted: P     U     O    4.13.14-1-ck-skylake #1\r\n[  980.960067] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[  980.960068] z_zvol          D    0 17045      2 0x00000000\r\n[  980.960070] Call Trace:\r\n[  980.960076]  __schedule+0x6eb/0xcd0\r\n[  980.960079]  schedule+0x3d/0xd0\r\n[  980.960083]  taskq_wait_outstanding+0x87/0xd0 [spl]\r\n[  980.960086]  ? wait_woken+0x80/0x80\r\n[  980.960102]  zvol_task_cb+0x1fa/0x5a0 [zfs]\r\n[  980.960105]  taskq_thread+0x254/0x480 [spl]\r\n[  980.960108]  ? wake_up_q+0x80/0x80\r\n[  980.960111]  kthread+0x124/0x140\r\n[  980.960114]  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\n[  980.960115]  ? kthread_create_on_node+0x70/0x70\r\n[  980.960118]  ret_from_fork+0x25/0x30\r\n[ 1103.830057] INFO: task z_zvol:17045 blocked for more than 120 seconds.\r\n[ 1103.830061]       Tainted: P     U     O    4.13.14-1-ck-skylake #1\r\n[ 1103.830062] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1103.830063] z_zvol          D    0 17045      2 0x00000000\r\n[ 1103.830066] Call Trace:\r\n[ 1103.830071]  __schedule+0x6eb/0xcd0\r\n[ 1103.830073]  schedule+0x3d/0xd0\r\n[ 1103.830077]  taskq_wait_outstanding+0x87/0xd0 [spl]\r\n[ 1103.830080]  ? wait_woken+0x80/0x80\r\n[ 1103.830095]  zvol_task_cb+0x1fa/0x5a0 [zfs]\r\n[ 1103.830099]  taskq_thread+0x254/0x480 [spl]\r\n[ 1103.830102]  ? wake_up_q+0x80/0x80\r\n[ 1103.830104]  kthread+0x124/0x140\r\n[ 1103.830106]  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\n[ 1103.830108]  ? kthread_create_on_node+0x70/0x70\r\n[ 1103.830110]  ret_from_fork+0x25/0x30\r\n[ 1226.710103] INFO: task z_zvol:17045 blocked for more than 120 seconds.\r\n[ 1226.710106]       Tainted: P     U     O    4.13.14-1-ck-skylake #1\r\n[ 1226.710107] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1226.710108] z_zvol          D    0 17045      2 0x00000000\r\n[ 1226.710110] Call Trace:\r\n[ 1226.710115]  __schedule+0x6eb/0xcd0\r\n[ 1226.710117]  schedule+0x3d/0xd0\r\n[ 1226.710121]  taskq_wait_outstanding+0x87/0xd0 [spl]\r\n[ 1226.710123]  ? wait_woken+0x80/0x80\r\n[ 1226.710138]  zvol_task_cb+0x1fa/0x5a0 [zfs]\r\n[ 1226.710141]  taskq_thread+0x254/0x480 [spl]\r\n[ 1226.710143]  ? wake_up_q+0x80/0x80\r\n[ 1226.710145]  kthread+0x124/0x140\r\n[ 1226.710147]  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\n[ 1226.710149]  ? kthread_create_on_node+0x70/0x70\r\n[ 1226.710151]  ret_from_fork+0x25/0x30\r\n[ 1349.590140] INFO: task z_zvol:17045 blocked for more than 120 seconds.\r\n[ 1349.590144]       Tainted: P     U     O    4.13.14-1-ck-skylake #1\r\n[ 1349.590145] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1349.590146] z_zvol          D    0 17045      2 0x00000000\r\n[ 1349.590148] Call Trace:\r\n[ 1349.590153]  __schedule+0x6eb/0xcd0\r\n[ 1349.590155]  schedule+0x3d/0xd0\r\n[ 1349.590159]  taskq_wait_outstanding+0x87/0xd0 [spl]\r\n[ 1349.590162]  ? wait_woken+0x80/0x80\r\n[ 1349.590178]  zvol_task_cb+0x1fa/0x5a0 [zfs]\r\n[ 1349.590180]  taskq_thread+0x254/0x480 [spl]\r\n[ 1349.590183]  ? wake_up_q+0x80/0x80\r\n[ 1349.590185]  kthread+0x124/0x140\r\n[ 1349.590187]  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\n[ 1349.590189]  ? kthread_create_on_node+0x70/0x70\r\n[ 1349.590191]  ret_from_fork+0x25/0x30\r\n[ 1436.501551] zfs[27076]: segfault at 10 ip 00007f0bd40474b1 sp 00007fff83d42008 error 4 in libc-2.26.so[7f0bd3ef0000+1ae000]\r\n[ 1472.470050] INFO: task z_zvol:17045 blocked for more than 120 seconds.\r\n[ 1472.470053]       Tainted: P     U     O    4.13.14-1-ck-skylake #1\r\n[ 1472.470054] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1472.470056] z_zvol          D    0 17045      2 0x00000000\r\n[ 1472.470058] Call Trace:\r\n[ 1472.470063]  __schedule+0x6eb/0xcd0\r\n[ 1472.470065]  schedule+0x3d/0xd0\r\n[ 1472.470069]  taskq_wait_outstanding+0x87/0xd0 [spl]\r\n[ 1472.470071]  ? wait_woken+0x80/0x80\r\n[ 1472.470087]  zvol_task_cb+0x1fa/0x5a0 [zfs]\r\n[ 1472.470090]  taskq_thread+0x254/0x480 [spl]\r\n[ 1472.470092]  ? wake_up_q+0x80/0x80\r\n[ 1472.470095]  kthread+0x124/0x140\r\n[ 1472.470097]  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\n[ 1472.470098]  ? kthread_create_on_node+0x70/0x70\r\n[ 1472.470100]  ret_from_fork+0x25/0x30\r\n[ 1595.350031] INFO: task z_zvol:389 blocked for more than 120 seconds.\r\n[ 1595.350034]       Tainted: P     U     O    4.13.14-1-ck-skylake #1\r\n[ 1595.350035] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1595.350036] z_zvol          D    0   389      2 0x00000000\r\n[ 1595.350039] Call Trace:\r\n[ 1595.350044]  __schedule+0x6eb/0xcd0\r\n[ 1595.350046]  schedule+0x3d/0xd0\r\n[ 1595.350050]  taskq_wait_outstanding+0x87/0xd0 [spl]\r\n[ 1595.350052]  ? wait_woken+0x80/0x80\r\n[ 1595.350068]  zvol_task_cb+0x1fa/0x5a0 [zfs]\r\n[ 1595.350071]  taskq_thread+0x254/0x480 [spl]\r\n[ 1595.350073]  ? wake_up_q+0x80/0x80\r\n[ 1595.350076]  kthread+0x124/0x140\r\n[ 1595.350078]  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\n[ 1595.350079]  ? kthread_create_on_node+0x70/0x70\r\n[ 1595.350081]  ? kthread_create_on_node+0x70/0x70\r\n[ 1595.350082]  ret_from_fork+0x25/0x30\r\n[ 1595.350106] INFO: task z_zvol:17045 blocked for more than 120 seconds.\r\n[ 1595.350107]       Tainted: P     U     O    4.13.14-1-ck-skylake #1\r\n[ 1595.350108] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1595.350109] z_zvol          D    0 17045      2 0x00000000\r\n[ 1595.350110] Call Trace:\r\n[ 1595.350112]  __schedule+0x6eb/0xcd0\r\n[ 1595.350113]  schedule+0x3d/0xd0\r\n[ 1595.350116]  taskq_wait_outstanding+0x87/0xd0 [spl]\r\n[ 1595.350117]  ? wait_woken+0x80/0x80\r\n[ 1595.350130]  zvol_task_cb+0x1fa/0x5a0 [zfs]\r\n[ 1595.350133]  taskq_thread+0x254/0x480 [spl]\r\n[ 1595.350135]  ? wake_up_q+0x80/0x80\r\n[ 1595.350137]  kthread+0x124/0x140\r\n[ 1595.350139]  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\n[ 1595.350140]  ? kthread_create_on_node+0x70/0x70\r\n[ 1595.350142]  ret_from_fork+0x25/0x30\r\n[ 1718.230091] INFO: task z_zvol:389 blocked for more than 120 seconds.\r\n[ 1718.230094]       Tainted: P     U     O    4.13.14-1-ck-skylake #1\r\n[ 1718.230095] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1718.230097] z_zvol          D    0   389      2 0x00000000\r\n[ 1718.230099] Call Trace:\r\n[ 1718.230103]  __schedule+0x6eb/0xcd0\r\n[ 1718.230105]  schedule+0x3d/0xd0\r\n[ 1718.230109]  taskq_wait_outstanding+0x87/0xd0 [spl]\r\n[ 1718.230112]  ? wait_woken+0x80/0x80\r\n[ 1718.230127]  zvol_task_cb+0x1fa/0x5a0 [zfs]\r\n[ 1718.230130]  taskq_thread+0x254/0x480 [spl]\r\n[ 1718.230133]  ? wake_up_q+0x80/0x80\r\n[ 1718.230135]  kthread+0x124/0x140\r\n[ 1718.230137]  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\n[ 1718.230139]  ? kthread_create_on_node+0x70/0x70\r\n[ 1718.230140]  ? kthread_create_on_node+0x70/0x70\r\n[ 1718.230142]  ret_from_fork+0x25/0x30\r\n[ 1718.230164] INFO: task z_zvol:17045 blocked for more than 120 seconds.\r\n[ 1718.230166]       Tainted: P     U     O    4.13.14-1-ck-skylake #1\r\n[ 1718.230167] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1718.230167] z_zvol          D    0 17045      2 0x00000000\r\n[ 1718.230169] Call Trace:\r\n[ 1718.230170]  __schedule+0x6eb/0xcd0\r\n[ 1718.230172]  schedule+0x3d/0xd0\r\n[ 1718.230175]  taskq_wait_outstanding+0x87/0xd0 [spl]\r\n[ 1718.230176]  ? wait_woken+0x80/0x80\r\n[ 1718.230190]  zvol_task_cb+0x1fa/0x5a0 [zfs]\r\n[ 1718.230192]  taskq_thread+0x254/0x480 [spl]\r\n[ 1718.230195]  ? wake_up_q+0x80/0x80\r\n[ 1718.230196]  kthread+0x124/0x140\r\n[ 1718.230198]  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\n[ 1718.230200]  ? kthread_create_on_node+0x70/0x70\r\n[ 1718.230201]  ret_from_fork+0x25/0x30\r\n[ 2342.854530] zfs[9384]: segfault at 10 ip 00007f43a3ebb4b1 sp 00007ffd2f425268 error 4 in libc-2.26.so[7f43a3d64000+1ae000]\r\n[ 2343.138402] zfs[9544]: segfault at 10 ip 00007f505f1434b1 sp 00007ffdcb1d1e48 error 4 in libc-2.26.so[7f505efec000+1ae000]\r\n[ 3239.855885] zfs[14132]: segfault at 10 ip 00007f75c5b7b4b1 sp 00007ffec4e010d8 error 4 in libc-2.26.so[7f75c5a24000+1ae000]\r\n```\r\n\r\nzdb (anonymised):\r\n```\r\nroot@g ~ # zdb \r\ndozer:\r\n    version: 5000\r\n    name: 'dozer'\r\n    state: 0\r\n    txg: 4\r\n    pool_guid: <number>\r\n    errata: 0\r\n    hostid: <number>\r\n    hostname: 'g'\r\n    com.delphix:has_per_vdev_zaps\r\n    vdev_children: 1\r\n    vdev_tree:\r\n        type: 'root'\r\n        id: 0\r\n        guid: <number>\r\n        create_txg: 4\r\n        children[0]:\r\n            type: 'disk'\r\n            id: 0\r\n            guid: <number>\r\n            path: '/dev/mapper/g'\r\n            devid: 'dm-uuid-CRYPT-LUKS1-<uuid>-g'\r\n            whole_disk: 0\r\n            metaslab_array: 256\r\n            metaslab_shift: 31\r\n            ashift: 9\r\n            asize: 320062095360\r\n            is_log: 0\r\n            create_txg: 4\r\n            com.delphix:vdev_zap_leaf: 129\r\n            com.delphix:vdev_zap_top: 130\r\n    features_for_read:\r\n        com.delphix:hole_birth\r\n        com.delphix:embedded_data\r\ntank:\r\n    version: 5000\r\n    name: 'tank'\r\n    state: 0\r\n    txg: 893048\r\n    pool_guid: <number>\r\n    errata: 0\r\n    hostid: <number>\r\n    hostname: 'glock'\r\n    com.delphix:has_per_vdev_zaps\r\n    vdev_children: 1\r\n    vdev_tree:\r\n        type: 'root'\r\n        id: 0\r\n        guid: <number>\r\n        children[0]:\r\n            type: 'disk'\r\n            id: 0\r\n            guid: <number>\r\n            path: '/dev/mapper/h'\r\n            devid: 'dm-uuid-CRYPT-LUKS1-<uuid>-h'\r\n            whole_disk: 0\r\n            metaslab_array: 256\r\n            metaslab_shift: 30\r\n            ashift: 12\r\n            asize: 254978818048\r\n            is_log: 0\r\n            DTL: 120\r\n            create_txg: 4\r\n            com.delphix:vdev_zap_leaf: 58\r\n            com.delphix:vdev_zap_top: 59\r\n    features_for_read:\r\n        com.delphix:hole_birth\r\n        com.delphix:embedded_data\r\n```\r\n\r\nHappy to provide any additional info, just tell me what you want to know. ", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6890/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tanabarr": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6876", "title": "ZED should issue history event when changing zpool \"cachefile\" property", "body": "When setting the `cachefile` property on a zpool, no history event regarding the property update is emitted by ZED.\r\n\r\nIssuing the following commands\r\n\r\n```\r\n[root@vm6 ~]# zpool set cachefile=none testPool3\r\n[root@vm6 ~]# zpool set cachefile= testPool3\r\n[root@vm6 ~]# zpool set multihost=on testPool3\r\n[root@vm6 ~]# zpool set multihost=off testPool3\r\n```\r\n\r\nResults in the following events being omitted\r\n\r\n```\r\nNov 16 06:40:59 vm6 zed[10194]: eid=83 class=config_sync pool_guid=0xF18E758C02D2207E\r\nNov 16 06:41:07 vm6 zed[10226]: eid=84 class=config_sync pool_guid=0xF18E758C02D2207E\r\nNov 16 06:41:30 vm6 zed[10264]: eid=85 class=history_event pool_guid=0xF18E758C02D2207E\r\nNov 16 06:41:50 vm6 zed[10331]: eid=86 class=history_event pool_guid=0xF18E758C02D2207E\r\n```\r\n(history events for both the `multihost` set commands, config_sync events for both the `cachefile` set commands)\r\n\r\nBelow is the contents of the config_sync event, which doesn't contain any information regarding the property setting\r\n\r\n```\r\nIFS=\r\nPATH=/usr/bin:/bin:/usr/sbin:/sbin\r\nPWD=/\r\nSHLVL=1\r\nZDB=/sbin/zdb\r\nZED=/sbin/zed\r\nZED_PID=4028\r\nZED_ZEDLET_DIR=/etc/zfs/zed.d\r\nZEVENT_CLASS=sysevent.fs.zfs.config_sync\r\nZEVENT_EID=83\r\nZEVENT_POOL=testPool3\r\nZEVENT_POOL_CONTEXT=0\r\nZEVENT_POOL_GUID=0xF18E758C02D2207E\r\nZEVENT_POOL_STATE=0\r\nZEVENT_POOL_STATE_STR=ACTIVE\r\nZEVENT_SUBCLASS=config_sync\r\nZEVENT_TIME=1510843259 58356210\r\nZEVENT_TIME_NSECS=58356210\r\nZEVENT_TIME_SECS=1510843259\r\nZEVENT_TIME_STRING=2017-11-16 06:40:59-0800\r\nZEVENT_VERSION=0\r\nZFS=/sbin/zfs\r\nZFS_ALIAS=zfs-0.7.3-1\r\nZFS_RELEASE=1\r\nZFS_VERSION=0.7.3\r\nZINJECT=/sbin/zinject\r\nZPOOL=/sbin/zpool\r\n_=/usr/bin/printenv\r\n```\r\n\r\nIs there a reason behind this or is it unintended? \r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6876/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6743", "title": "zpool import unique exit code when importable with force", "body": "When MMP / `multihost=on` , and a pool has not been cleanly exported (e.g. power loss while pool is imported). Another host can import using `-f` and `zpool import` error message reflects that. Would it be possible for the exit code from `zpool import` to be unique in this situation to aid in programmatic detection rather than parsing the stderr?\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6743/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "deepy": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6875", "title": "rollback hung", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian\r\nDistribution Version    | stretch\r\nLinux Kernel                 | 4.9.0-4\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.6.5.9-5\r\nSPL Version                  | 0.6.5.9-1\r\n\r\n### Describe the problem you're observing\r\nrollback hangs\r\n\r\n### Describe how to reproduce the problem\r\nForgot to shut down application using data on dataset before starting rollback, pkilled that application during rollback, rollback never finished.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\ndmesg\r\n```\r\n[24771.358656] INFO: task zfs:1755 blocked for more than 120 seconds.\r\n[24771.358660] Tainted: P O 4.9.0-4-amd64 #1 Debian 4.9.51-1\r\n[24771.358662] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[24771.358665] zfs D 0 1755 1754 0x00000004\r\n[24771.358669] ffff8bde50375800 0000000000000000 ffff8bde4f5b0000 ffff8bea65c58240\r\n[24771.358673] ffff8bea1e8ff000 ffffa750685dfa00 ffffffffba2038e3 ffff8bde4f5b0000\r\n[24771.358678] 0000000000000000 ffff8bea65c58240 000000000000055d ffff8bde4f5b0000\r\n[24771.358682] Call Trace:\r\n[24771.358687] [<ffffffffba2038e3>] ? __schedule+0x233/0x6d0\r\n[24771.358691] [<ffffffffba204550>] ? bit_wait+0x50/0x50\r\n[24771.358694] [<ffffffffba203db2>] ? schedule+0x32/0x80\r\n[24771.358698] [<ffffffffba20711e>] ? schedule_timeout+0x1de/0x350\r\n[24771.358702] [<ffffffffb9cec47b>] ? ktime_get+0x3b/0xb0\r\n[24771.358705] [<ffffffffba204550>] ? bit_wait+0x50/0x50\r\n[24771.358709] [<ffffffffba20364d>] ? io_schedule_timeout+0x9d/0x100\r\n[24771.358712] [<ffffffffb9cb8b77>] ? prepare_to_wait_exclusive+0x57/0x80\r\n[24771.358716] [<ffffffffba204567>] ? bit_wait_io+0x17/0x60\r\n[24771.358719] [<ffffffffba2041b4>] ? __wait_on_bit_lock+0x54/0xb0\r\n[24771.358724] [<ffffffffb9d7bb62>] ? __lock_page+0x82/0xa0\r\n[24771.358727] [<ffffffffb9cb8f20>] ? wake_atomic_t_function+0x60/0x60\r\n[24771.358733] [<ffffffffb9d8fb5a>] ? truncate_inode_pages_range+0x3fa/0x7e0\r\n[24771.358739] [<ffffffffb9d8fff3>] ? truncate_pagecache+0x43/0x60\r\n[24771.358811] [<ffffffffc0c4186c>] ? zpl_evict_inode+0x2c/0x50 [zfs]\r\n[24771.358817] [<ffffffffb9e1f806>] ? evict+0xb6/0x180\r\n[24771.358821] [<ffffffffb9e1b257>] ? __dentry_kill+0xa7/0x150\r\n[24771.358824] [<ffffffffb9e1b7dd>] ? shrink_dentry_list+0xfd/0x2e0\r\n[24771.358828] [<ffffffffb9e1bb52>] ? shrink_dcache_sb+0xc2/0x140\r\n[24771.358902] [<ffffffffc0c2413a>] ? zfs_sb_teardown+0x8a/0x390 [zfs]\r\n[24771.358977] [<ffffffffc0c1a388>] ? zfs_ioc_rollback+0x38/0xa0 [zfs]\r\n[24771.359050] [<ffffffffc0c1a6d6>] ? zfsdev_ioctl+0x246/0x4f0 [zfs]\r\n[24771.359054] [<ffffffffb9dba279>] ? vma_merge+0x229/0x330\r\n[24771.359057] [<ffffffffb9e16f1f>] ? do_vfs_ioctl+0x9f/0x600\r\n[24771.359060] [<ffffffffb9e174f4>] ? SyS_ioctl+0x74/0x80\r\n[24771.359066] [<ffffffffba2085bb>] ? system_call_fast_compare_end+0xc/0x9b\r\n[24892.190121] INFO: task kworker/u16:0:18636 blocked for more than 120 seconds.\r\n[24892.190129] Tainted: P O 4.9.0-4-amd64 #1 Debian 4.9.51-1\r\n[24892.190131] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[24892.190134] kworker/u16:0 D 0 18636 2 0x00000000\r\n[24892.190147] Workqueue: writeback wb_workfn (flush-zfs-4)\r\n[24892.190152] ffff8bea1906bc00 0000000000000000 ffff8be84e0bb0c0 ffff8bea65d18240\r\n[24892.190157] ffff8bea1e40f0c0 ffffa7506b03b8d0 ffffffffba2038e3 ffff8bde7bb96700\r\n[24892.190162] 00ffa7506b03b908 ffff8bea65d18240 00000000ffffffff ffff8be84e0bb0c0\r\n[24892.190167] Call Trace:\r\n[24892.190176] [<ffffffffba2038e3>] ? __schedule+0x233/0x6d0\r\n[24892.190180] [<ffffffffba204550>] ? bit_wait+0x50/0x50\r\n[24892.190183] [<ffffffffba203db2>] ? schedule+0x32/0x80\r\n[24892.190188] [<ffffffffba20711e>] ? schedule_timeout+0x1de/0x350\r\n[24892.190276] [<ffffffffc0c2d739>] ? zfs_putpage+0x2d9/0x510 [zfs]\r\n[24892.190281] [<ffffffffb9efc6a6>] ? submit_bio+0x76/0x140\r\n[24892.190286] [<ffffffffb9cec47b>] ? ktime_get+0x3b/0xb0\r\n[24892.190290] [<ffffffffba204550>] ? bit_wait+0x50/0x50\r\n[24892.190294] [<ffffffffba20364d>] ? io_schedule_timeout+0x9d/0x100\r\n[24892.190298] [<ffffffffb9cb8b77>] ? prepare_to_wait_exclusive+0x57/0x80\r\n[24892.190301] [<ffffffffba204567>] ? bit_wait_io+0x17/0x60\r\n[24892.190305] [<ffffffffba2041b4>] ? __wait_on_bit_lock+0x54/0xb0\r\n[24892.190310] [<ffffffffb9d7c5b8>] ? find_get_pages_tag+0x158/0x2e0\r\n[24892.190314] [<ffffffffb9d7bb62>] ? __lock_page+0x82/0xa0\r\n[24892.190318] [<ffffffffb9cb8f20>] ? wake_atomic_t_function+0x60/0x60\r\n[24892.190323] [<ffffffffb9d8aef5>] ? write_cache_pages+0x2d5/0x470\r\n[24892.190399] [<ffffffffc0c3fb30>] ? zpl_write_common_iovec+0xf0/0xf0 [zfs]\r\n[24892.190405] [<ffffffffb9f29736>] ? cpumask_next_and+0x26/0x40\r\n[24892.190410] [<ffffffffb9cb20f6>] ? update_sd_lb_stats+0xe6/0x4b0\r\n[24892.190414] [<ffffffffb9ca8662>] ? update_cfs_rq_load_avg+0x212/0x490\r\n[24892.190486] [<ffffffffc0c3fc5a>] ? zpl_writepages+0x8a/0x150 [zfs]\r\n[24892.190491] [<ffffffffb9e3192d>] ? __writeback_single_inode+0x3d/0x310\r\n[24892.190495] [<ffffffffb9e320c1>] ? writeback_sb_inodes+0x221/0x4f0\r\n[24892.190500] [<ffffffffb9e32417>] ? __writeback_inodes_wb+0x87/0xb0\r\n[24892.190503] [<ffffffffb9e32788>] ? wb_writeback+0x278/0x310\r\n[24892.190507] [<ffffffffb9e330de>] ? wb_workfn+0x2ae/0x3d0\r\n[24892.190511] [<ffffffffb9c90444>] ? process_one_work+0x184/0x410\r\n[24892.190515] [<ffffffffb9c9071d>] ? worker_thread+0x4d/0x480\r\n[24892.190518] [<ffffffffb9c906d0>] ? process_one_work+0x410/0x410\r\n[24892.190522] [<ffffffffb9c96697>] ? kthread+0xd7/0xf0\r\n[24892.190526] [<ffffffffb9c965c0>] ? kthread_park+0x60/0x60\r\n[24892.190532] [<ffffffffba208835>] ? ret_from_fork+0x25/0x30\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6875/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "scotws": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6873", "title": "Add a clean rewrite of arc_summary.py in Python 3", "body": "**Background**. arc_summary.py is a Python script to report basic information on the ZFS cache system. The original version was written 2008 by Ben Rockwood in Perl for Solaris machines (see http://cuddletech.com/?p=454 for his original blog posting). It has since been ported to Python and Linux (see https://github.com/zfsonlinux/zfs/blob/master/cmd/arc_summary/arc_summary.py)\r\n\r\n**The Problem.** The current version of arc_summary.py must work with various distributions, some of which use ancient versions of Python (e.g. CentOS 6, which seems to be using Python 2.6.6, now seven years old; see https://danieleriksson.net/2017/02/08/how-to-install-latest-python-on-centos/ for details). This means it is stuck with (for example) the `getopt` library instead of `argparse`, and primitive `subprocess` code. At the same time, it must work with Python 3. Also, despite various rewrites by many people, it still betrays its Perl roots with downright un-Pythonic code, making it harder to understand, maintain and enhance. This will only get worse as Python 2 approaches EOL in now only slightly more than two years (see https://pythonclock.org/).\r\n\r\n**Suggestion.** To fix this, we could add a version of arc_summary.py rewritten, mostly from scratch, in pure Python 3, possibly with the name **arc_summary3.py**. It would be included in the same folder as the existing script, which would be kept and maintained for backward use. The hope is that more Pythonic code will make it easier for more people to add to and improve upon the new version.\r\n\r\n**Complications.** Any rewrite of this scale would almost certainly introduce bugs. The ZoL test suites would have to be retooled to accept a Python 3 program even though it could only run on a subgroup of systems. It can be assumed that fewer people will show an interest in working on the older version of arc_summary.py if there is a newer version around, possibly orphaning the original, still important version.\r\n\r\n**Who would write this?** I have spent the last couple of weeks with the current arc_summary.py version (see PRs) and also have written a (very experimental) partial port in Go (golang) at https://github.com/scotws/arc_summary_go. I could put together an initial version of arc_summary3.py as a code base; however I have no idea how to include this in the build and tests systems of ZoL. Also, I'm sure any Python code I write could be improved upon.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6873/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/8d187769736688044c3c58a6a56069a54210a656", "message": "Fix data on evict_skips in arc_summary.py\n\nDisplay correct data from kstat arcstats for evict_skips,\r\nwhich is currently repeating the data from mutex_misses.\r\nFixes #6882\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Scot W. Stevenson <scot.stevenson@gmail.com>\r\nCloses #6882 \r\nCloses #6883"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/e301113c17673a290098850830cf2e6d1a1fcbe3", "message": "Minor code cleanups in arc_python.py\n\nRemove unused library re and associated variable kstat_pobj. Add note\r\nto documentation at start of program about required support for old\r\nversions of Python. Change variable \"format\" (which is a built-in\r\nfunction) to \"fmt\".\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Scot W. Stevenson <scot.stevenson@gmail.com>\r\nCloses #6869"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/5277f208f290ea4e2204800a66c3ba20a03fe503", "message": "Fix arc_summary.py -d crash with Python3\n\nPrevents arc_summary.py crashing when called with parameter -d or\r\nlong form --description with Python3.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Scot W. Stevenson <scot.stevenson@gmail.com>\r\nCloses #6849 \r\nCloses #6850"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/681957fe2efc6a15b9675387075514957cbcd821", "message": "Sort output of tunables in arc_summary.py\n\nSort list of tunables printed by _tunable_summary()\r\nalphabetically\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Scot W. Stevenson <scot.stevenson@gmail.com>\r\nCloses #6828"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/23ea00a1fe9cafa081f910a8326b8f41cfb9f8d4", "message": "Add documentation strings to arc_summary.py\n\nInclude docstrings (PEP8, PEP257) for module and all functions.\r\nSeparately, remove outdated section in comment at start of\r\nmodule. Separately, remove unused global constant \"usetunable\".\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Scot W. Stevenson <scot.stevenson@gmail.com>\r\nCloses #6818"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/cd1813d36e78ce21fa0b0e48b050e0493f9f6c93", "message": "Rewrite fHits() in arc_summary.py with SI units\n\nComplete rewrite of fHits(). Move units from non-standard English\r\nabbreviations to SI units, thereby avoiding confusion because of\r\n\"long scale\" and \"short scale\" numbers. Remove unused parameter\r\n\"Decimal\". Add function string. Aim to confirm to PEP8.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Scot W. Stevenson <scot.stevenson@gmail.com>\r\nCloses #6815"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/df1f129bc4150fd6ea3f23a01154a71ffa48bf12", "message": "Minor code cleanup in arc_summary.py\n\nSimplify and inline single-use function div1(); inline twice-used\r\nfunction div2(); add function comment to zfs_header(); replace\r\nvariable \"unused\" in get_Kstat() with \"_\" following convention.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Scot W. Stevenson <scot.stevenson@gmail.com>\r\nCloses #6802"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/47c8e7fd97d8406f3fe51882a4a2787103012a82", "message": "Rewrite of function fBytes() in arc_summary.py\n\nReplace if-elif-else construction with shorter loop;\nremove unused parameter \"Decimal\"; centralize format\nstring; add function documentation string; conform to\nPEP8.\n\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nSigned-off-by: Scot W. Stevenson <scot.stevenson@gmail.com>\nCloses #6784"}], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6892", "title": "Add Python 3 rewrite of arc_summary.py (#6873)", "body": "### Description\r\n\r\nAdd new Python 3 script `arc_summary3.py` as a complete rewrite of `arc_summary.py` to display basic information on the ARC status and various other parameters. This is provided in addition - not as a replacement - to the existing `arc_summary.py` tool. See #6873 for a discussion of the reasoning behind adding a new version of this tool while keeping a legacy version as well.\r\n\r\nNew options:\r\n\r\n        -g/--graph    - Display crude graphic representation of ARC status and quit\r\n        -r/--raw      - Print all available information as minimally formatted list and quit\r\n        -s/--section  - Print a single section. This supersedes -p/--page, which is kept for\r\n                        backwards use but marked as DEPRECIATED\r\n\r\nAdds new sections with information on the ZIL and SPL. \r\n\r\nWe now notify the user if sections L2ARC and VDEV are skipped instead of failing silently; note VDEV caching is currently disabled and slated for possible removal (see source code). Adds information on the ZFS and SPL versions to the header.\r\n\r\nThe **-s/--section** option is intended to replace the page number system, which required the user to remember which page number was of interest. The -p/--page options are still supported, but marked as DEPRECIATED. Current legal sections are `arc archits dmu l2arc spl tunables vdev zil`. It should be easier now to add and modify sections.\r\n\r\nThe **-r/--raw** option is intended to work with other tools such as `grep`. It respects the -a/-d options (alternate output format / descriptions included) where possible. \r\n\r\nThe output of the **-g/--graph** option is intended to give a quick, rough overview as a visual orientation. An example (Ubuntu 16.04 LTS x86_64 with 24 GB RAM, 8 GB ARC max, ZFS stock version 0.6.5.9-2 with `/home` as ZFS mirror pool immediately after starting _Civilization VI_ on Steam on otherwise quiet machine): \r\n```\r\n        ARC: 3.0 GiB (37.5 %)  MFU: 610.5 MiB  MRU: 2.3 GiB\r\n    +----------------------------------------------------------+\r\n    |FFFFRRRRRRRRRRRRRRRRR                                     |\r\n    +----------------------------------------------------------+\r\n```\r\n`F` is for MFU, `R` for MRU, and `O` is used for \"other\" if necessary (not present in this example). \r\n\r\n`arc_summary3.py` was developed for Python 3.5. This follows the version of Python currently installed in Ubuntu 16.04 LTS. Few systems will have Python 3.6 installed yet.\r\n\r\n### Known issues\r\n\r\nThe new script is based on the same internal logic as the original, so any error or issue present there will probably show up here as well. For instance, the number of anonymous hits can be negative the way it is calculated in both scripts; they both simply hide any negative value.\r\n\r\nThis script will probably make a bunch of test suites unhappy where Python 3 is not included. There is no experience with this script under extreme conditions (for example ARC throttling).\r\n\r\n### How Has This Been Tested?\r\n\r\nThere is a unittest script `test_arc_summary3.py` at https://gist.github.com/scotws/aaf5d9c9317081e249b664a371ec4907\r\nMost testing was done in-tree, comparing the output to that of the current `arc_summary.py` version.\r\n\r\nThe L2ARC section has **not seen any real-world use** because I do not have access to a L2ARC device on my machine.\r\n\r\n### Other \r\n\r\nSwitching to Python 3 results in a noticeably smaller file size despite the addition of several new features. Output of `wc` for both scripts:\r\n```\r\n    837    2586   28021 arc_summary3.py\r\n   1020    2593   35538 arc_summary.py\r\n```\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [x] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [ ] I have updated the documentation accordingly.\r\n- [ ] I have read the **CONTRIBUTING** document.\r\n- [x] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [x] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "colttt": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6871", "title": "stats needs to be documented", "body": "Hello,\r\n\r\nzfs/ZoL ahs a lot of stats (mostly in /proc/spl/kstat/zfs/), but the meaning of this or what are good/bad values are completely missing. It would be nice if you add this in the documentation. Thats also be needed if you want to monitor you zfs\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6871/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chjohnst": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6851", "title": "NFS: v4 server returned a bad sequence-id error on an unconfirmed sequence", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS\r\nDistribution Version    | 7.3\r\nLinux Kernel                 | 3.10.0-514.26.2.el7.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.9\r\nSPL Version                  | 0.6.5.9\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nRepeated errors from my NFS clients on ZFS exported filesystems.  I have several non-ZFS exported filesystems that do not exhibit this error.  This found by looking in tcpdump and capturing for the error in particular.  (nfs.status == 10026)\r\n\r\n[1008951.143700] NFS: v4 server returned a bad sequence-id error on an unconfirmed sequence ffff8817c8fbee20!\r\n[1008976.459382] NFS: v4 server returned a bad sequence-id error on an unconfirmed sequence ffff88069e299020!\r\n[1008976.520790] NFS: v4 server returned a bad sequence-id error on an unconfirmed sequence ffff88069e29aa20!\r\n[1008976.632506] NFS: v4 server returned a bad sequence-id error on an unconfirmed sequence ffff88069e29ba20!\r\n[1008976.843640] NFS: v4 server returned a bad sequence-id error on an unconfirmed sequence ffff880080706220!\r\n[1008977.254953] NFS: v4 server returned a bad sequence-id error on an unconfirmed sequence ffff880080704620!\r\n[1008980.346921] NFS: v4 server returned a bad sequence-id error on an unconfirmed sequence ffff8800b25ed020!\r\n[1008980.407991] NFS: v4 server returned a bad sequence-id error on an unconfirmed sequence ffff8800b25eda20!\r\n[1008980.519267] NFS: v4 server returned a bad sequence-id error on an unconfirmed sequence ffff88057788e820!\r\n[1008980.731117] NFS: v4 server returned a bad sequence-id error on an unconfirmed sequence ffff88057788d420!\r\n\r\n### Describe how to reproduce the problem\r\nMult-user system likely doing a lot of reads/writes and shared access to files\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6851/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6197", "title": "NFS Stale File Handle", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS\r\nDistribution Version    | 7.3\r\nLinux Kernel                 | 3.10.0-514.16.1.el7.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.9-1\r\nSPL Version                  | 0.6.5.9-1\r\n\r\n### Describe the problem you're observing\r\nNoticing a Stale File Handle error when you have one machine write to NFS and another client reading from a file that is atomically replaced with a mv operation.  I normally use ext4/xfs or btrfs and do not see this issue (although I see the counters go up on the NFS server side for stale file handles in /proc/net/rpc/nfsd)\r\n\r\n### Describe how to reproduce the problem\r\nA simple python script writing on one node, and a client reading the file (watch -n.1 md5sum file 2>out) after a few seconds to minutes eventually I will hit the race condition where the file is being moved to replace it.  I know that perhaps the expected behavior is to see a \"Stale File Handle\" but with other filesystems such as ext4, xfs and btrfs I do not see this behavior. \r\n\r\nimport shutil\r\nimport time\r\n\r\nwhile True:\r\n  time.sleep(.5)\r\n  shutil.copyfile('file', 'file.tmp')\r\n  f = open('file', 'a+')\r\n  f.write('XXXXXXXXXXXXXX\\n')\r\n  f.flush()\r\n  shutil.move('file.tmp', 'file')\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6197/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mabod": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6843", "title": "zfs crashes when changing scheduler to bfq-sq when zfs_prefetch_disable=0", "body": "### System information\r\nDistribution Name       |  Manjaro\r\nDistribution Version    |  rolling\r\nLinux Kernel                 |  4.13.11 (not happening with 4.9.60)\r\nArchitecture                 |  amd64\r\nZFS Version                  | 0.7.2-1\r\nSPL Version                  | 0.7.2-1\r\n\r\n### Describe the problem you're observing\r\n\r\nWhen kernel is 4.13.11 and zfs_prefetch_disable=0 the zfs module crashes with core dump in the journal when switching the scheduler to bfq-sq.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nI am doing performance tests for different schedulers. I am using the following script to switch the schedulers and exceute fio for the benchmark:\r\n\r\n```\r\nFILE=/mnt/zstore/BENCHMARK/fiotest.$$\r\nOUT=/mnt/zstore/BENCHMARK/zfs-compare-scheduler-prefetch-disable-1-arc-limit-4.out\r\nSCHEDULER=\"noop deadline cfq bfq-sq\"\r\n\r\nfor s in $SCHEDULER;\r\ndo\r\n    echo \"scheduler: $s\" \r\n    echo \"#############\" >> $OUT\r\n    echo \"scheduler: $s\" >> $OUT\r\n    for d in /sys/block/sda/queue/scheduler /sys/block/sdb/queue/scheduler /sys/block/sdc/queue/scheduler; \r\n    do \r\n        echo \"$s\" > $d; \r\n    done \r\n    fio --rw=rw --name=$FILE --size=20G -bs=1M  --refill_buffers --ioengine=libaio --gtod_reduce=1 >> $OUT\r\n    echo \"#############\" >> $OUT\r\ndone\r\n```\r\nWhen the script reaches scheduler bfq-sq the fio process hangs. It sits for about a minute before the following messages show up in the journal:\r\n```\r\nNov 07 20:27:16 rakete kernel: INFO: task z_wr_iss:1445 blocked for more than 120 seconds.\r\nNov 07 20:27:16 rakete kernel:       Tainted: P           O    4.13.11-1-MANJARO #1\r\nNov 07 20:27:16 rakete kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nNov 07 20:27:16 rakete kernel: z_wr_iss        D    0  1445      2 0x00000000\r\nNov 07 20:27:16 rakete kernel: Call Trace:\r\nNov 07 20:27:16 rakete kernel:  __schedule+0x239/0x890\r\nNov 07 20:27:16 rakete kernel:  schedule+0x3d/0x90\r\nNov 07 20:27:16 rakete kernel:  io_schedule+0x16/0x40\r\nNov 07 20:27:16 rakete kernel:  wbt_wait+0x178/0x350\r\nNov 07 20:27:16 rakete kernel:  ? wait_woken+0x80/0x80\r\nNov 07 20:27:16 rakete kernel:  blk_queue_bio+0x101/0x410\r\nNov 07 20:27:16 rakete kernel:  generic_make_request+0x125/0x320\r\nNov 07 20:27:16 rakete kernel:  submit_bio+0x73/0x150\r\nNov 07 20:27:16 rakete kernel:  ? submit_bio+0x73/0x150\r\nNov 07 20:27:16 rakete kernel:  vdev_disk_io_start+0x4b2/0x6f0 [zfs]\r\nNov 07 20:27:16 rakete kernel:  zio_vdev_io_start+0xa2/0x340 [zfs]\r\nNov 07 20:27:16 rakete kernel:  ? zio_vdev_io_start+0xa2/0x340 [zfs]\r\nNov 07 20:27:16 rakete kernel:  zio_nowait+0xab/0x160 [zfs]\r\nNov 07 20:27:16 rakete kernel:  vdev_raidz_io_start+0x1de/0x300 [zfs]\r\nNov 07 20:27:16 rakete kernel:  zio_vdev_io_start+0xa2/0x340 [zfs]\r\nNov 07 20:27:16 rakete kernel:  zio_nowait+0xab/0x160 [zfs]\r\nNov 07 20:27:16 rakete kernel:  ? vdev_config_sync+0x180/0x180 [zfs]\r\nNov 07 20:27:16 rakete kernel:  vdev_mirror_io_start+0x94/0x180 [zfs]\r\nNov 07 20:27:16 rakete kernel:  zio_vdev_io_start+0x1aa/0x340 [zfs]\r\nNov 07 20:27:16 rakete kernel:  ? taskq_member+0x18/0x30 [spl]\r\nNov 07 20:27:16 rakete kernel:  zio_execute+0x8a/0xe0 [zfs]\r\nNov 07 20:27:16 rakete kernel:  taskq_thread+0x254/0x480 [spl]\r\nNov 07 20:27:16 rakete kernel:  ? wake_up_q+0x80/0x80\r\nNov 07 20:27:16 rakete kernel:  kthread+0x125/0x140\r\nNov 07 20:27:16 rakete kernel:  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\nNov 07 20:27:16 rakete kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 07 20:27:16 rakete kernel:  ret_from_fork+0x25/0x30\r\nNov 07 20:27:16 rakete kernel: INFO: task txg_sync:1966 blocked for more than 120 seconds.\r\nNov 07 20:27:16 rakete kernel:       Tainted: P           O    4.13.11-1-MANJARO #1\r\nNov 07 20:27:16 rakete kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nNov 07 20:27:16 rakete kernel: txg_sync        D    0  1966      2 0x00000000\r\nNov 07 20:27:16 rakete kernel: Call Trace:\r\nNov 07 20:27:16 rakete kernel:  __schedule+0x239/0x890\r\nNov 07 20:27:16 rakete kernel:  schedule+0x3d/0x90\r\nNov 07 20:27:16 rakete kernel:  io_schedule+0x16/0x40\r\nNov 07 20:27:16 rakete kernel:  cv_wait_common+0xb0/0x130 [spl]\r\nNov 07 20:27:16 rakete kernel:  ? wait_woken+0x80/0x80\r\nNov 07 20:27:16 rakete kernel:  __cv_wait_io+0x18/0x20 [spl]\r\nNov 07 20:27:16 rakete kernel:  zio_wait+0xf2/0x1b0 [zfs]\r\nNov 07 20:27:16 rakete kernel:  dsl_pool_sync+0xb8/0x430 [zfs]\r\nNov 07 20:27:16 rakete kernel:  spa_sync+0x43f/0xd80 [zfs]\r\nNov 07 20:27:16 rakete kernel:  txg_sync_thread+0x2d2/0x4a0 [zfs]\r\nNov 07 20:27:16 rakete kernel:  ? finish_task_switch+0x75/0x200\r\nNov 07 20:27:16 rakete kernel:  ? txg_delay+0x1b0/0x1b0 [zfs]\r\nNov 07 20:27:16 rakete kernel:  thread_generic_wrapper+0x72/0x80 [spl]\r\nNov 07 20:27:16 rakete kernel:  kthread+0x125/0x140\r\nNov 07 20:27:16 rakete kernel:  ? __thread_exit+0x20/0x20 [spl]\r\nNov 07 20:27:16 rakete kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 07 20:27:16 rakete kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 07 20:27:16 rakete kernel:  ret_from_fork+0x25/0x30\r\nNov 07 20:27:16 rakete kernel: INFO: task fio:24704 blocked for more than 120 seconds.\r\nNov 07 20:27:16 rakete kernel:       Tainted: P           O    4.13.11-1-MANJARO #1\r\nNov 07 20:27:16 rakete kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nNov 07 20:27:16 rakete kernel: fio             D    0 24704  24684 0x00000000\r\nNov 07 20:27:16 rakete kernel: Call Trace:\r\nNov 07 20:27:16 rakete kernel:  __schedule+0x239/0x890\r\nNov 07 20:27:16 rakete kernel:  schedule+0x3d/0x90\r\nNov 07 20:27:16 rakete kernel:  cv_wait_common+0x11c/0x130 [spl]\r\nNov 07 20:27:16 rakete kernel:  ? wait_woken+0x80/0x80\r\nNov 07 20:27:16 rakete kernel:  __cv_wait+0x15/0x20 [spl]\r\nNov 07 20:27:16 rakete kernel:  txg_wait_open+0xb0/0x100 [zfs]\r\nNov 07 20:27:16 rakete kernel:  dmu_tx_wait+0x37c/0x390 [zfs]\r\nNov 07 20:27:16 rakete kernel:  dmu_tx_assign+0x8f/0x480 [zfs]\r\nNov 07 20:27:16 rakete kernel:  zfs_write+0x403/0xd30 [zfs]\r\nNov 07 20:27:16 rakete kernel:  ? zio_destroy+0xc3/0xd0 [zfs]\r\nNov 07 20:27:16 rakete kernel:  ? zio_wait+0x139/0x1b0 [zfs]\r\nNov 07 20:27:16 rakete kernel:  ? spl_kmem_free+0x33/0x40 [spl]\r\nNov 07 20:27:16 rakete kernel:  ? spl_kmem_free+0x33/0x40 [spl]\r\nNov 07 20:27:16 rakete kernel:  ? zfs_range_unlock+0x1ec/0x2c0 [zfs]\r\nNov 07 20:27:16 rakete kernel:  zpl_write_common_iovec+0x8c/0xe0 [zfs]\r\nNov 07 20:27:16 rakete kernel:  zpl_iter_write+0xae/0xe0 [zfs]\r\nNov 07 20:27:16 rakete kernel:  aio_write+0x113/0x170\r\nNov 07 20:27:16 rakete kernel:  ? __check_object_size+0xaf/0x1b0\r\nNov 07 20:27:16 rakete kernel:  ? _copy_to_user+0x2a/0x40\r\nNov 07 20:27:16 rakete kernel:  ? aio_read_events+0x245/0x390\r\nNov 07 20:27:16 rakete kernel:  do_io_submit+0x387/0x730\r\nNov 07 20:27:16 rakete kernel:  ? do_io_submit+0x387/0x730\r\nNov 07 20:27:16 rakete kernel:  SyS_io_submit+0x10/0x20\r\nNov 07 20:27:16 rakete kernel:  ? SyS_io_submit+0x10/0x20\r\nNov 07 20:27:16 rakete kernel:  entry_SYSCALL_64_fastpath+0x1a/0xa5\r\nNov 07 20:27:16 rakete kernel: RIP: 0033:0x7fd2010f46c7\r\nNov 07 20:27:16 rakete kernel: RSP: 002b:00007ffc6e157498 EFLAGS: 00000246 ORIG_RAX: 00000000000000d1\r\nNov 07 20:27:16 rakete kernel: RAX: ffffffffffffffda RBX: 00007fd1e4855000 RCX: 00007fd2010f46c7\r\nNov 07 20:27:16 rakete kernel: RDX: 000055560c8417a0 RSI: 0000000000000001 RDI: 00007fd214bcd000\r\nNov 07 20:27:16 rakete kernel: RBP: 000055560c8415c0 R08: 0000000000000001 R09: 000055560c841660\r\nNov 07 20:27:16 rakete kernel: R10: 000000005e2e0000 R11: 0000000000000246 R12: 00007fd1fb939510\r\nNov 07 20:27:16 rakete kernel: R13: 0000000000000001 R14: 0000000000100000 R15: 0000000000000000\r\nNov 07 20:29:19 rakete kernel: INFO: task z_wr_iss:1445 blocked for more than 120 seconds.\r\nNov 07 20:29:19 rakete kernel:       Tainted: P           O    4.13.11-1-MANJARO #1\r\nNov 07 20:29:19 rakete kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nNov 07 20:29:19 rakete kernel: z_wr_iss        D    0  1445      2 0x00000000\r\nNov 07 20:29:19 rakete kernel: Call Trace:\r\nNov 07 20:29:19 rakete kernel:  __schedule+0x239/0x890\r\nNov 07 20:29:19 rakete kernel:  schedule+0x3d/0x90\r\nNov 07 20:29:19 rakete kernel:  io_schedule+0x16/0x40\r\nNov 07 20:29:19 rakete kernel:  wbt_wait+0x178/0x350\r\nNov 07 20:29:19 rakete kernel:  ? wait_woken+0x80/0x80\r\nNov 07 20:29:19 rakete kernel:  blk_queue_bio+0x101/0x410\r\nNov 07 20:29:19 rakete kernel:  generic_make_request+0x125/0x320\r\nNov 07 20:29:19 rakete kernel:  submit_bio+0x73/0x150\r\nNov 07 20:29:19 rakete kernel:  ? submit_bio+0x73/0x150\r\nNov 07 20:29:19 rakete kernel:  vdev_disk_io_start+0x4b2/0x6f0 [zfs]\r\nNov 07 20:29:19 rakete kernel:  zio_vdev_io_start+0xa2/0x340 [zfs]\r\nNov 07 20:29:19 rakete kernel:  ? zio_vdev_io_start+0xa2/0x340 [zfs]\r\nNov 07 20:29:19 rakete kernel:  zio_nowait+0xab/0x160 [zfs]\r\nNov 07 20:29:19 rakete kernel:  vdev_raidz_io_start+0x1de/0x300 [zfs]\r\nNov 07 20:29:19 rakete kernel:  zio_vdev_io_start+0xa2/0x340 [zfs]\r\nNov 07 20:29:19 rakete kernel:  zio_nowait+0xab/0x160 [zfs]\r\nNov 07 20:29:19 rakete kernel:  ? vdev_config_sync+0x180/0x180 [zfs]\r\nNov 07 20:29:19 rakete kernel:  vdev_mirror_io_start+0x94/0x180 [zfs]\r\nNov 07 20:29:19 rakete kernel:  zio_vdev_io_start+0x1aa/0x340 [zfs]\r\nNov 07 20:29:19 rakete kernel:  ? taskq_member+0x18/0x30 [spl]\r\nNov 07 20:29:19 rakete kernel:  zio_execute+0x8a/0xe0 [zfs]\r\nNov 07 20:29:19 rakete kernel:  taskq_thread+0x254/0x480 [spl]\r\nNov 07 20:29:19 rakete kernel:  ? wake_up_q+0x80/0x80\r\nNov 07 20:29:19 rakete kernel:  kthread+0x125/0x140\r\nNov 07 20:29:19 rakete kernel:  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\nNov 07 20:29:19 rakete kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 07 20:29:19 rakete kernel:  ret_from_fork+0x25/0x30\r\nNov 07 20:29:19 rakete kernel: INFO: task txg_sync:1966 blocked for more than 120 seconds.\r\nNov 07 20:29:19 rakete kernel:       Tainted: P           O    4.13.11-1-MANJARO #1\r\nNov 07 20:29:19 rakete kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nNov 07 20:29:19 rakete kernel: txg_sync        D    0  1966      2 0x00000000\r\nNov 07 20:29:19 rakete kernel: Call Trace:\r\nNov 07 20:29:19 rakete kernel:  __schedule+0x239/0x890\r\nNov 07 20:29:19 rakete kernel:  schedule+0x3d/0x90\r\nNov 07 20:29:19 rakete kernel:  io_schedule+0x16/0x40\r\nNov 07 20:29:19 rakete kernel:  cv_wait_common+0xb0/0x130 [spl]\r\nNov 07 20:29:19 rakete kernel:  ? wait_woken+0x80/0x80\r\nNov 07 20:29:19 rakete kernel:  __cv_wait_io+0x18/0x20 [spl]\r\nNov 07 20:29:19 rakete kernel:  zio_wait+0xf2/0x1b0 [zfs]\r\nNov 07 20:29:19 rakete kernel:  dsl_pool_sync+0xb8/0x430 [zfs]\r\nNov 07 20:29:19 rakete kernel:  spa_sync+0x43f/0xd80 [zfs]\r\nNov 07 20:29:19 rakete kernel:  txg_sync_thread+0x2d2/0x4a0 [zfs]\r\nNov 07 20:29:19 rakete kernel:  ? finish_task_switch+0x75/0x200\r\nNov 07 20:29:19 rakete kernel:  ? txg_delay+0x1b0/0x1b0 [zfs]\r\nNov 07 20:29:19 rakete kernel:  thread_generic_wrapper+0x72/0x80 [spl]\r\nNov 07 20:29:19 rakete kernel:  kthread+0x125/0x140\r\nNov 07 20:29:19 rakete kernel:  ? __thread_exit+0x20/0x20 [spl]\r\nNov 07 20:29:19 rakete kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 07 20:29:19 rakete kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 07 20:29:19 rakete kernel:  ret_from_fork+0x25/0x30\r\nNov 07 20:29:19 rakete kernel: INFO: task fio:24704 blocked for more than 120 seconds.\r\nNov 07 20:29:19 rakete kernel:       Tainted: P           O    4.13.11-1-MANJARO #1\r\nNov 07 20:29:19 rakete kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nNov 07 20:29:19 rakete kernel: fio             D    0 24704  24684 0x00000000\r\nNov 07 20:29:19 rakete kernel: Call Trace:\r\nNov 07 20:29:19 rakete kernel:  __schedule+0x239/0x890\r\nNov 07 20:29:19 rakete kernel:  schedule+0x3d/0x90\r\nNov 07 20:29:19 rakete kernel:  cv_wait_common+0x11c/0x130 [spl]\r\nNov 07 20:29:19 rakete kernel:  ? wait_woken+0x80/0x80\r\nNov 07 20:29:19 rakete kernel:  __cv_wait+0x15/0x20 [spl]\r\nNov 07 20:29:19 rakete kernel:  txg_wait_open+0xb0/0x100 [zfs]\r\nNov 07 20:29:19 rakete kernel:  dmu_tx_wait+0x37c/0x390 [zfs]\r\nNov 07 20:29:19 rakete kernel:  dmu_tx_assign+0x8f/0x480 [zfs]\r\nNov 07 20:29:19 rakete kernel:  zfs_write+0x403/0xd30 [zfs]\r\nNov 07 20:29:19 rakete kernel:  ? zio_destroy+0xc3/0xd0 [zfs]\r\nNov 07 20:29:19 rakete kernel:  ? zio_wait+0x139/0x1b0 [zfs]\r\nNov 07 20:29:19 rakete kernel:  ? spl_kmem_free+0x33/0x40 [spl]\r\nNov 07 20:29:19 rakete kernel:  ? spl_kmem_free+0x33/0x40 [spl]\r\nNov 07 20:29:19 rakete kernel:  ? zfs_range_unlock+0x1ec/0x2c0 [zfs]\r\nNov 07 20:29:19 rakete kernel:  zpl_write_common_iovec+0x8c/0xe0 [zfs]\r\nNov 07 20:29:19 rakete kernel:  zpl_iter_write+0xae/0xe0 [zfs]\r\nNov 07 20:29:19 rakete kernel:  aio_write+0x113/0x170\r\nNov 07 20:29:19 rakete kernel:  ? __check_object_size+0xaf/0x1b0\r\nNov 07 20:29:19 rakete kernel:  ? _copy_to_user+0x2a/0x40\r\nNov 07 20:29:19 rakete kernel:  ? aio_read_events+0x245/0x390\r\nNov 07 20:29:19 rakete kernel:  do_io_submit+0x387/0x730\r\nNov 07 20:29:19 rakete kernel:  ? do_io_submit+0x387/0x730\r\nNov 07 20:29:19 rakete kernel:  SyS_io_submit+0x10/0x20\r\nNov 07 20:29:19 rakete kernel:  ? SyS_io_submit+0x10/0x20\r\nNov 07 20:29:19 rakete kernel:  entry_SYSCALL_64_fastpath+0x1a/0xa5\r\nNov 07 20:29:19 rakete kernel: RIP: 0033:0x7fd2010f46c7\r\nNov 07 20:29:19 rakete kernel: RSP: 002b:00007ffc6e157498 EFLAGS: 00000246 ORIG_RAX: 00000000000000d1\r\nNov 07 20:29:19 rakete kernel: RAX: ffffffffffffffda RBX: 00007fd1e4855000 RCX: 00007fd2010f46c7\r\nNov 07 20:29:19 rakete kernel: RDX: 000055560c8417a0 RSI: 0000000000000001 RDI: 00007fd214bcd000\r\nNov 07 20:29:19 rakete kernel: RBP: 000055560c8415c0 R08: 0000000000000001 R09: 000055560c841660\r\nNov 07 20:29:19 rakete kernel: R10: 000000005e2e0000 R11: 0000000000000246 R12: 00007fd1fb939510\r\nNov 07 20:29:19 rakete kernel: R13: 0000000000000001 R14: 0000000000100000 R15: 0000000000000000\r\nNov 07 20:29:58 rakete gnome-terminal-[3506]: Allocating size to GtkScrollbar 0x563a70df4500 without calling gtk_widget_get_preferred_width/h\r\nNov 07 20:29:58 rakete gnome-terminal-[3506]: Allocating size to GtkScrollbar 0x563a70df4500 without calling gtk_widget_get_preferred_width/h\r\nNov 07 20:29:58 rakete gnome-terminal-[3506]: Allocating size to GtkScrollbar 0x563a70df4500 without calling gtk_widget_get_preferred_width/h\r\nNov 07 20:29:58 rakete gnome-terminal-[3506]: Allocating size to GtkScrollbar 0x563a70df4500 without calling gtk_widget_get_preferred_width/h\r\nNov 07 20:29:58 rakete gnome-terminal-[3506]: Allocating size to GtkScrollbar 0x563a70df4500 without calling gtk_widget_get_preferred_width/h\r\nNov 07 20:29:58 rakete gnome-terminal-[3506]: Allocating size to GtkScrollbar 0x563a70df4500 without calling gtk_widget_get_preferred_width/h\r\nNov 07 20:30:01 rakete crond[25044]: pam_unix(crond:session): session opened for user matthias by (uid=0)\r\nNov 07 20:30:01 rakete CROND[25045]: (matthias) CMD (getmail --rcfile kontent-rc --rcfile google-rc --rcfile nexgo-rc --rcfile telekom-rc --r\r\nNov 07 20:30:05 rakete CROND[25044]: pam_unix(crond:session): session closed for user matthias\r\nNov 07 20:31:22 rakete kernel: INFO: task z_wr_iss:1445 blocked for more than 120 seconds.\r\nNov 07 20:31:22 rakete kernel:       Tainted: P           O    4.13.11-1-MANJARO #1\r\nNov 07 20:31:22 rakete kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nNov 07 20:31:22 rakete kernel: z_wr_iss        D    0  1445      2 0x00000000\r\nNov 07 20:31:22 rakete kernel: Call Trace:\r\nNov 07 20:31:22 rakete kernel:  __schedule+0x239/0x890\r\nNov 07 20:31:22 rakete kernel:  schedule+0x3d/0x90\r\nNov 07 20:31:22 rakete kernel:  io_schedule+0x16/0x40\r\nNov 07 20:31:22 rakete kernel:  wbt_wait+0x178/0x350\r\nNov 07 20:31:22 rakete kernel:  ? wait_woken+0x80/0x80\r\nNov 07 20:31:22 rakete kernel:  blk_queue_bio+0x101/0x410\r\nNov 07 20:31:22 rakete kernel:  generic_make_request+0x125/0x320\r\nNov 07 20:31:22 rakete kernel:  submit_bio+0x73/0x150\r\nNov 07 20:31:22 rakete kernel:  ? submit_bio+0x73/0x150\r\nNov 07 20:31:22 rakete kernel:  vdev_disk_io_start+0x4b2/0x6f0 [zfs]\r\nNov 07 20:31:22 rakete kernel:  zio_vdev_io_start+0xa2/0x340 [zfs]\r\nNov 07 20:31:22 rakete kernel:  ? zio_vdev_io_start+0xa2/0x340 [zfs]\r\nNov 07 20:31:22 rakete kernel:  zio_nowait+0xab/0x160 [zfs]\r\nNov 07 20:31:22 rakete kernel:  vdev_raidz_io_start+0x1de/0x300 [zfs]\r\nNov 07 20:31:22 rakete kernel:  zio_vdev_io_start+0xa2/0x340 [zfs]\r\nNov 07 20:31:22 rakete kernel:  zio_nowait+0xab/0x160 [zfs]\r\nNov 07 20:31:22 rakete kernel:  ? vdev_config_sync+0x180/0x180 [zfs]\r\nNov 07 20:31:22 rakete kernel:  vdev_mirror_io_start+0x94/0x180 [zfs]\r\nNov 07 20:31:22 rakete kernel:  zio_vdev_io_start+0x1aa/0x340 [zfs]\r\nNov 07 20:31:22 rakete kernel:  ? taskq_member+0x18/0x30 [spl]\r\nNov 07 20:31:22 rakete kernel:  zio_execute+0x8a/0xe0 [zfs]\r\nNov 07 20:31:22 rakete kernel:  taskq_thread+0x254/0x480 [spl]\r\nNov 07 20:31:22 rakete kernel:  ? wake_up_q+0x80/0x80\r\nNov 07 20:31:22 rakete kernel:  kthread+0x125/0x140\r\nNov 07 20:31:22 rakete kernel:  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\nNov 07 20:31:22 rakete kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 07 20:31:22 rakete kernel:  ret_from_fork+0x25/0x30\r\nNov 07 20:31:22 rakete kernel: INFO: task txg_sync:1966 blocked for more than 120 seconds.\r\nNov 07 20:31:22 rakete kernel:       Tainted: P           O    4.13.11-1-MANJARO #1\r\nNov 07 20:31:22 rakete kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nNov 07 20:31:22 rakete kernel: txg_sync        D    0  1966      2 0x00000000\r\nNov 07 20:31:22 rakete kernel: Call Trace:\r\nNov 07 20:31:22 rakete kernel:  __schedule+0x239/0x890\r\nNov 07 20:31:22 rakete kernel:  schedule+0x3d/0x90\r\nNov 07 20:31:22 rakete kernel:  io_schedule+0x16/0x40\r\nNov 07 20:31:22 rakete kernel:  cv_wait_common+0xb0/0x130 [spl]\r\nNov 07 20:31:22 rakete kernel:  ? wait_woken+0x80/0x80\r\nNov 07 20:31:22 rakete kernel:  __cv_wait_io+0x18/0x20 [spl]\r\nNov 07 20:31:22 rakete kernel:  zio_wait+0xf2/0x1b0 [zfs]\r\nNov 07 20:31:22 rakete kernel:  dsl_pool_sync+0xb8/0x430 [zfs]\r\nNov 07 20:31:22 rakete kernel:  spa_sync+0x43f/0xd80 [zfs]\r\nNov 07 20:31:22 rakete kernel:  txg_sync_thread+0x2d2/0x4a0 [zfs]\r\nNov 07 20:31:22 rakete kernel:  ? finish_task_switch+0x75/0x200\r\nNov 07 20:31:22 rakete kernel:  ? txg_delay+0x1b0/0x1b0 [zfs]\r\nNov 07 20:31:22 rakete kernel:  thread_generic_wrapper+0x72/0x80 [spl]\r\nNov 07 20:31:22 rakete kernel:  kthread+0x125/0x140\r\nNov 07 20:31:22 rakete kernel:  ? __thread_exit+0x20/0x20 [spl]\r\nNov 07 20:31:22 rakete kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 07 20:31:22 rakete kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 07 20:31:22 rakete kernel:  ret_from_fork+0x25/0x30\r\nNov 07 20:31:22 rakete kernel: INFO: task fio:24704 blocked for more than 120 seconds.\r\nNov 07 20:31:22 rakete kernel:       Tainted: P           O    4.13.11-1-MANJARO #1\r\nNov 07 20:31:22 rakete kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nNov 07 20:31:22 rakete kernel: fio             D    0 24704  24684 0x00000000\r\nNov 07 20:31:22 rakete kernel: Call Trace:\r\nNov 07 20:31:22 rakete kernel:  __schedule+0x239/0x890\r\nNov 07 20:31:22 rakete kernel:  schedule+0x3d/0x90\r\nNov 07 20:31:22 rakete kernel:  cv_wait_common+0x11c/0x130 [spl]\r\nNov 07 20:31:22 rakete kernel:  ? wait_woken+0x80/0x80\r\nNov 07 20:31:22 rakete kernel:  __cv_wait+0x15/0x20 [spl]\r\nNov 07 20:31:22 rakete kernel:  txg_wait_open+0xb0/0x100 [zfs]\r\nNov 07 20:31:22 rakete kernel:  dmu_tx_wait+0x37c/0x390 [zfs]\r\nNov 07 20:31:22 rakete kernel:  dmu_tx_assign+0x8f/0x480 [zfs]\r\nNov 07 20:31:22 rakete kernel:  zfs_write+0x403/0xd30 [zfs]\r\nNov 07 20:31:22 rakete kernel:  ? zio_destroy+0xc3/0xd0 [zfs]\r\nNov 07 20:31:22 rakete kernel:  ? zio_wait+0x139/0x1b0 [zfs]\r\nNov 07 20:31:22 rakete kernel:  ? spl_kmem_free+0x33/0x40 [spl]\r\nNov 07 20:31:22 rakete kernel:  ? spl_kmem_free+0x33/0x40 [spl]\r\nNov 07 20:31:22 rakete kernel:  ? zfs_range_unlock+0x1ec/0x2c0 [zfs]\r\nNov 07 20:31:22 rakete kernel:  zpl_write_common_iovec+0x8c/0xe0 [zfs]\r\nNov 07 20:31:22 rakete kernel:  zpl_iter_write+0xae/0xe0 [zfs]\r\nNov 07 20:31:22 rakete kernel:  aio_write+0x113/0x170\r\nNov 07 20:31:22 rakete kernel:  ? __check_object_size+0xaf/0x1b0\r\nNov 07 20:31:22 rakete kernel:  ? _copy_to_user+0x2a/0x40\r\nNov 07 20:31:22 rakete kernel:  ? aio_read_events+0x245/0x390\r\nNov 07 20:31:22 rakete kernel:  do_io_submit+0x387/0x730\r\nNov 07 20:31:22 rakete kernel:  ? do_io_submit+0x387/0x730\r\nNov 07 20:31:22 rakete kernel:  SyS_io_submit+0x10/0x20\r\nNov 07 20:31:22 rakete kernel:  ? SyS_io_submit+0x10/0x20\r\nNov 07 20:31:22 rakete kernel:  entry_SYSCALL_64_fastpath+0x1a/0xa5\r\nNov 07 20:31:22 rakete kernel: RIP: 0033:0x7fd2010f46c7\r\nNov 07 20:31:22 rakete kernel: RSP: 002b:00007ffc6e157498 EFLAGS: 00000246 ORIG_RAX: 00000000000000d1\r\nNov 07 20:31:22 rakete kernel: RAX: ffffffffffffffda RBX: 00007fd1e4855000 RCX: 00007fd2010f46c7\r\nNov 07 20:31:22 rakete kernel: RDX: 000055560c8417a0 RSI: 0000000000000001 RDI: 00007fd214bcd000\r\nNov 07 20:31:22 rakete kernel: RBP: 000055560c8415c0 R08: 0000000000000001 R09: 000055560c841660\r\nNov 07 20:31:22 rakete kernel: R10: 000000005e2e0000 R11: 0000000000000246 R12: 00007fd1fb939510\r\nNov 07 20:31:22 rakete kernel: R13: 0000000000000001 R14: 0000000000100000 R15: 0000000000000000\r\nNov 07 20:33:25 rakete kernel: INFO: task z_wr_iss:1445 blocked for more than 120 seconds.\r\nNov 07 20:33:25 rakete kernel:       Tainted: P           O    4.13.11-1-MANJARO #1\r\nNov 07 20:33:25 rakete kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nNov 07 20:33:25 rakete kernel: z_wr_iss        D    0  1445      2 0x00000000\r\nNov 07 20:33:25 rakete kernel: Call Trace:\r\nNov 07 20:33:25 rakete kernel:  __schedule+0x239/0x890\r\nNov 07 20:33:25 rakete kernel:  schedule+0x3d/0x90\r\nNov 07 20:33:25 rakete kernel:  io_schedule+0x16/0x40\r\nNov 07 20:33:25 rakete kernel:  wbt_wait+0x178/0x350\r\nNov 07 20:33:25 rakete kernel:  ? wait_woken+0x80/0x80\r\nNov 07 20:33:25 rakete kernel:  blk_queue_bio+0x101/0x410\r\nNov 07 20:33:25 rakete kernel:  generic_make_request+0x125/0x320\r\nNov 07 20:33:25 rakete kernel:  submit_bio+0x73/0x150\r\nNov 07 20:33:25 rakete kernel:  ? submit_bio+0x73/0x150\r\nNov 07 20:33:25 rakete kernel:  vdev_disk_io_start+0x4b2/0x6f0 [zfs]\r\nNov 07 20:33:25 rakete kernel:  zio_vdev_io_start+0xa2/0x340 [zfs]\r\nNov 07 20:33:25 rakete kernel:  ? zio_vdev_io_start+0xa2/0x340 [zfs]\r\nNov 07 20:33:25 rakete kernel:  zio_nowait+0xab/0x160 [zfs]\r\nNov 07 20:33:25 rakete kernel:  vdev_raidz_io_start+0x1de/0x300 [zfs]\r\nNov 07 20:33:25 rakete kernel:  zio_vdev_io_start+0xa2/0x340 [zfs]\r\nNov 07 20:33:25 rakete kernel:  zio_nowait+0xab/0x160 [zfs]\r\nNov 07 20:33:25 rakete kernel:  ? vdev_config_sync+0x180/0x180 [zfs]\r\nNov 07 20:33:25 rakete kernel:  vdev_mirror_io_start+0x94/0x180 [zfs]\r\nNov 07 20:33:25 rakete kernel:  zio_vdev_io_start+0x1aa/0x340 [zfs]\r\nNov 07 20:33:25 rakete kernel:  ? taskq_member+0x18/0x30 [spl]\r\nNov 07 20:33:25 rakete kernel:  zio_execute+0x8a/0xe0 [zfs]\r\nNov 07 20:33:25 rakete kernel:  taskq_thread+0x254/0x480 [spl]\r\nNov 07 20:33:25 rakete kernel:  ? wake_up_q+0x80/0x80\r\nNov 07 20:33:25 rakete kernel:  kthread+0x125/0x140\r\nNov 07 20:33:25 rakete kernel:  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\nNov 07 20:33:25 rakete kernel:  ? kthread_create_on_node+0x70/0x70\r\nNov 07 20:33:25 rakete kernel:  ret_from_fork+0x25/0x30\r\n```\r\n\r\nThis only happens with kernel 4.13.11 and zfs_prefetch_disable=0. This is reproducible when the script switches to scheduler bfq-sq. No issue with the other schedulers.\r\n\r\nWith kernel 4.9.60 it does not happen regardless of zfs_prefetch_disable. By the way, for kernel 4.9.60 I have to use bfq instead of bfq-sq.\r\n\r\n\r\n\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6843/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6833", "title": "zfs not able to switch to noop on usb drives", "body": "### System information\r\nDistribution Name       |  Manjaro\r\nDistribution Version    |  rolling\r\nLinux Kernel                 |  4.9.60 and 4.13.11\r\nArchitecture                 |  x86_64\r\nZFS Version                  |  0.7.2-1\r\nSPL Version                  |  0.7.2-1\r\n\r\n\r\n### Describe the problem you're observing\r\nImporting pools from external USB JBOD will only switch few of the drives to noop scheduler. Most of the drives stay on the Manjaro default scheduler \"bfq\".\r\n\r\n### Describe how to reproduce the problem\r\n\r\nI have 3 pools:\r\nzstore: internal raidz 3x SATA\r\nzf1: \texternal USB JBOD raidz with 4 drives. case: FANTEC QB-35US3-6G\r\nzf2-mirror: external USB JBOD mirror with 7 drives. case FANTEC QB-X8US3-6G\r\n\r\nOnly the internal pool zstore is reliably switched to noop. All drives in zf1 stay on bfq which is default in Manjaro and for zf2-mirror always the same 4 drives are changed to noop and the 3 remaining drives stay on bfq.\r\n\r\nSwitching scheduler via\r\n\r\n`echo \"noop\" > /sys/block/sde/queue/scheduler`\r\n\r\nworks just fine.\r\n\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\n```\r\n/sys/block/sda/queue/scheduler : [noop] deadline cfq bfq -> zstore\r\n/sys/block/sdb/queue/scheduler : [noop] deadline cfq bfq -> zstore\r\n/sys/block/sdc/queue/scheduler : [noop] deadline cfq bfq -> zstore\r\n/sys/block/sdd/queue/scheduler : noop deadline cfq [bfq] -> internal SSD on ext4. bfq is ok\r\n/sys/block/sde/queue/scheduler : noop deadline cfq [bfq] -> zf1. should be noop\r\n/sys/block/sdf/queue/scheduler : noop deadline cfq [bfq] -> zf1. should be noop\r\n/sys/block/sdg/queue/scheduler : noop deadline cfq [bfq] -> zf1. should be noop\r\n/sys/block/sdh/queue/scheduler : noop deadline cfq [bfq] -> zf1. should be noop\r\n/sys/block/sdi/queue/scheduler : [noop] deadline cfq bfq -> zf2-mirror \r\n/sys/block/sdj/queue/scheduler : [noop] deadline cfq bfq -> zf2-mirror\r\n/sys/block/sdk/queue/scheduler : [noop] deadline cfq bfq -> zf2-mirror\r\n/sys/block/sdl/queue/scheduler : [noop] deadline cfq bfq -> zf2-mirror\r\n/sys/block/sdm/queue/scheduler : noop deadline cfq [bfq] -> zf2-mirror. should be noop \r\n/sys/block/sdn/queue/scheduler : noop deadline cfq [bfq] -> zf2-mirror. should be noop\r\n/sys/block/sdo/queue/scheduler : noop deadline cfq [bfq] -> zf2-mirror. should be noop\r\n\r\n\r\nzpool status:\r\n\r\n  pool: zf1\r\n state: ONLINE\r\n  scan: scrub repaired 0B in 41h2m with 0 errors on Fri Nov  3 08:55:28 2017\r\nconfig:\r\n\r\n\tNAME                       STATE     READ WRITE CKSUM\r\n\tzf1                        ONLINE       0     0     0\r\n\t  raidz1-0                 ONLINE       0     0     0\r\n\t    f1-slot-1-oben-part1   ONLINE       0     0     0\r\n\t    f1-slot-2-part1        ONLINE       0     0     0\r\n\t    f1-slot-3-part1        ONLINE       0     0     0\r\n\t    f1-slot-4-unten-part1  ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n\r\n  pool: zf2-mirror\r\n state: ONLINE\r\n  scan: scrub repaired 0B in 23h56m with 0 errors on Mon Oct 30 10:20:36 2017\r\nconfig:\r\n\r\n\tNAME                            STATE     READ WRITE CKSUM\r\n\tzf2-mirror                      ONLINE       0     0     0\r\n\t  mirror-0                      ONLINE       0     0     0\r\n\t    f2-slot-1-WD20EARX          ONLINE       0     0     0\r\n\t    f2-slot-2-WD2002FAEX        ONLINE       0     0     0\r\n\t  mirror-1                      ONLINE       0     0     0\r\n\t    f2-slot-5-SP2504C           ONLINE       0     0     0\r\n\t    f2-slot-4-Sea320            ONLINE       0     0     0\r\n\t  mirror-2                      ONLINE       0     0     0\r\n\t    f2-slot-6-SSHD500-part1     ONLINE       0     0     0\r\n\t    f2-slot-7-WD5001AALS-part1  ONLINE       0     0     0\r\n\tspares\r\n\t  sdo1                          AVAIL   \r\n\r\nerrors: No known data errors\r\n\r\n  pool: zstore\r\n state: ONLINE\r\n  scan: scrub repaired 0B in 6h18m with 0 errors on Wed Nov  1 15:34:24 2017\r\nconfig:\r\n\r\n\tNAME                     STATE     READ WRITE CKSUM\r\n\tzstore                   ONLINE       0     0     0\r\n\t  raidz1-0               ONLINE       0     0     0\r\n\t    sda-WD-WCC4E5HF3P4S  ONLINE       0     0     0\r\n\t    sdb-WD-WCC4E1SSP28F  ONLINE       0     0     0\r\n\t    sdc-WD-WCC4E1SSP6NC  ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n\r\nwith the following drive mappings:\r\n\r\nlrwxrwxrwx 1 root root 10  4. Nov 07:46 f1-slot-1-oben-part1 -> ../../sde1\r\nlrwxrwxrwx 1 root root 10  4. Nov 07:46 f1-slot-2-part1 -> ../../sdf1\r\nlrwxrwxrwx 1 root root 10  4. Nov 07:46 f1-slot-3-part1 -> ../../sdg1\r\nlrwxrwxrwx 1 root root 10  4. Nov 07:46 f1-slot-4-unten-part1 -> ../../sdh1\r\nlrwxrwxrwx 1 root root 10  4. Nov 07:47 f2-slot-1-WD20EARX-part1 -> ../../sdi1\r\nlrwxrwxrwx 1 root root 10  4. Nov 07:47 f2-slot-2-WD2002FAEX-part1 -> ../../sdj1\r\nlrwxrwxrwx 1 root root 10  4. Nov 07:47 f2-slot-4-Sea320-part1 -> ../../sdk1\r\nlrwxrwxrwx 1 root root 10  4. Nov 07:47 f2-slot-5-SP2504C-part1 -> ../../sdl1\r\nlrwxrwxrwx 1 root root 10  4. Nov 07:47 f2-slot-6-SSHD500-part1 -> ../../sdm1\r\nlrwxrwxrwx 1 root root 10  4. Nov 07:47 f2-slot-7-WD5001AALS-part1 -> ../../sdn1\r\nlrwxrwxrwx 1 root root 10  4. Nov 07:47 f2-slot-8-WD7500AACS-part1 -> ../../sdo1\r\nlrwxrwxrwx 1 root root 10  4. Nov 07:45 sda-WD-WCC4E5HF3P4S-part1 -> ../../sda1\r\nlrwxrwxrwx 1 root root 10  4. Nov 07:45 sdb-WD-WCC4E1SSP28F-part1 -> ../../sdb1\r\nlrwxrwxrwx 1 root root 10  4. Nov 07:45 sdc-WD-WCC4E1SSP6NC-part1 -> ../../sdc1\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6833/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6656", "title": "crash when removing device from JBOD", "body": "### System information\r\n```\r\n18# inxi \r\nCPU~Quad core Intel Core i7-7700K (-HT-MCP-) speed/max~4200/4500 MHz Kernel~4.13.2-1-MANJARO x86_64 Up~4 min Mem~2563.1/16010.8MB HDD~19323.9GB(3.3% used) Procs~485 Client~Shell inxi~2.3.38  \r\n```\r\n \r\nmodinfo zfs | grep -iw version:\r\nversion:        0.7.0-1\r\n\r\nmodinfo spl | grep -iw version:\r\nversion:        0.7.0-1\r\n\r\n### Describe the problem you're observing\r\n\r\nzfs crashing. zfs hangs. Only reboot helps. \r\n\r\n### Describe how to reproduce the problem\r\n\r\nI have a JBOD in a fantec usb case. 3 drives making up one raidz. I use this for testing purposes. \r\n\r\n```\r\n21# zpool status zf2-raidz\r\n  pool: zf2-raidz\r\n state: ONLINE\r\n  scan: scrub repaired 0B in 0h8m with 0 errors on Tue Sep 19 07:58:49 2017\r\nconfig:\r\n\r\n\tNAME                         STATE     READ WRITE CKSUM\r\n\tzf2-raidz                    ONLINE       0     0     0\r\n\t  raidz1-0                   ONLINE       0     0     0\r\n\t    fantec2-Seagate-500SSHD  ONLINE       0     0     0\r\n\t    fantec2-WD750            ONLINE       0     0     0\r\n\t    fantec2-WD500            ONLINE       0     0     0\r\n```\r\nI wanted to test the drive fail behaviour of zfs. I started a dd file copy and in between I unplugged one drive form the raidz. the dd came to a halt and the following crash is reported in the journal:\r\n\r\n```\r\nSep 19 07:40:07 rakete kernel: INFO: task txg_sync:29104 blocked for more than 120 seconds.\r\nSep 19 07:40:07 rakete kernel:       Tainted: P           O    4.13.2-1-MANJARO #1\r\nSep 19 07:40:07 rakete kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message\r\nSep 19 07:40:07 rakete kernel: txg_sync        D    0 29104      2 0x00000000\r\nSep 19 07:40:07 rakete kernel: Call Trace:\r\nSep 19 07:40:07 rakete kernel:  __schedule+0x239/0x890\r\nSep 19 07:40:07 rakete kernel:  schedule+0x3d/0x90\r\nSep 19 07:40:07 rakete kernel:  io_schedule+0x16/0x40\r\nSep 19 07:40:07 rakete kernel:  cv_wait_common+0xb0/0x130 [spl]\r\nSep 19 07:40:07 rakete kernel:  ? wait_woken+0x80/0x80\r\nSep 19 07:40:07 rakete kernel:  __cv_wait_io+0x18/0x20 [spl]\r\nSep 19 07:40:07 rakete kernel:  zio_wait+0xf2/0x1b0 [zfs]\r\nSep 19 07:40:07 rakete kernel:  dsl_pool_sync+0xb8/0x440 [zfs]\r\nSep 19 07:40:07 rakete kernel:  spa_sync+0x43f/0xd80 [zfs]\r\nSep 19 07:40:07 rakete kernel:  txg_sync_thread+0x2d2/0x4a0 [zfs]\r\nSep 19 07:40:07 rakete kernel:  ? finish_task_switch+0x75/0x200\r\nSep 19 07:40:07 rakete kernel:  ? txg_delay+0x170/0x170 [zfs]\r\nSep 19 07:40:07 rakete kernel:  thread_generic_wrapper+0x72/0x80 [spl]\r\nSep 19 07:40:07 rakete kernel:  kthread+0x125/0x140\r\nSep 19 07:40:07 rakete kernel:  ? __thread_exit+0x20/0x20 [spl]\r\nSep 19 07:40:07 rakete kernel:  ? kthread_create_on_node+0x70/0x70\r\nSep 19 07:40:07 rakete kernel:  ret_from_fork+0x25/0x30\r\n```\r\nzfs is telling me that a device is missing and I need to plug it in again and do a \"zpool clear\" to recover. I tried that but it did not help. zfs did not recover. In fact the issue is, that the fantec hardware is resetting the complete usb bus if one device is plugged in or unplugged. So the hardware is not hotplug ready.\r\n\r\nWhen I plugin the drive again, the usb bus is resetted, my systemd automount service kicks in and tries to mount the pool \"zf2-raidz\" again. \r\n\r\nThe automount servcie works fine. It looks like this:\r\n\r\n```\r\n23# cat /etc/systemd/system/zf2-raidz-automount.service\r\n[Unit]\r\nAfter=dev-disk-by\\x2dvdev-fantec2\\x2dWD750\\x2dpart1.device\r\n\r\n[Service]\r\nExecStart=/usr/sbin/zpool import zf2-raidz\r\nExecStartPost=/usr/bin/logger \"started ZFS pool zf2-raidz\"\r\n\r\n[Install]\r\nWantedBy=dev-disk-by\\x2dvdev-fantec2\\x2dWD750\\x2dpart1.device\r\n```\r\n\r\nThe crash report is this:\r\n\r\n```\r\nSep 19 07:42:10 rakete kernel: INFO: task txg_sync:29104 blocked for more than 120 seconds.\r\nSep 19 07:42:10 rakete kernel:       Tainted: P           O    4.13.2-1-MANJARO #1\r\nSep 19 07:42:10 rakete kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message\r\nSep 19 07:42:10 rakete kernel: txg_sync        D    0 29104      2 0x00000000\r\nSep 19 07:42:10 rakete kernel: Call Trace:\r\nSep 19 07:42:10 rakete kernel:  __schedule+0x239/0x890\r\nSep 19 07:42:10 rakete kernel:  schedule+0x3d/0x90\r\nSep 19 07:42:10 rakete kernel:  io_schedule+0x16/0x40\r\nSep 19 07:42:10 rakete kernel:  cv_wait_common+0xb0/0x130 [spl]\r\nSep 19 07:42:10 rakete kernel:  ? wait_woken+0x80/0x80\r\nSep 19 07:42:10 rakete kernel:  __cv_wait_io+0x18/0x20 [spl]\r\nSep 19 07:42:10 rakete kernel:  zio_wait+0xf2/0x1b0 [zfs]\r\nSep 19 07:42:10 rakete kernel:  dsl_pool_sync+0xb8/0x440 [zfs]\r\nSep 19 07:42:10 rakete kernel:  spa_sync+0x43f/0xd80 [zfs]\r\nSep 19 07:42:10 rakete kernel:  txg_sync_thread+0x2d2/0x4a0 [zfs]\r\nSep 19 07:42:10 rakete kernel:  ? finish_task_switch+0x75/0x200\r\nSep 19 07:42:10 rakete kernel:  ? txg_delay+0x170/0x170 [zfs]\r\nSep 19 07:42:10 rakete kernel:  thread_generic_wrapper+0x72/0x80 [spl]\r\nSep 19 07:42:10 rakete kernel:  kthread+0x125/0x140\r\nSep 19 07:42:10 rakete kernel:  ? __thread_exit+0x20/0x20 [spl]\r\nSep 19 07:42:10 rakete kernel:  ? kthread_create_on_node+0x70/0x70\r\nSep 19 07:42:10 rakete kernel:  ret_from_fork+0x25/0x30\r\nSep 19 07:42:10 rakete kernel: INFO: task dd:29417 blocked for more than 120 seconds.\r\nSep 19 07:42:10 rakete kernel:       Tainted: P           O    4.13.2-1-MANJARO #1\r\nSep 19 07:42:10 rakete kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message\r\nSep 19 07:42:10 rakete kernel: dd              D    0 29417  27419 0x00000004\r\nSep 19 07:42:10 rakete kernel: Call Trace:\r\nSep 19 07:42:10 rakete kernel:  __schedule+0x239/0x890\r\nSep 19 07:42:10 rakete kernel:  schedule+0x3d/0x90\r\nSep 19 07:42:10 rakete kernel:  cv_wait_common+0x11c/0x130 [spl]\r\nSep 19 07:42:10 rakete kernel:  ? wait_woken+0x80/0x80\r\nSep 19 07:42:10 rakete kernel:  __cv_wait+0x15/0x20 [spl]\r\nSep 19 07:42:10 rakete kernel:  txg_wait_open+0xb0/0x100 [zfs]\r\nSep 19 07:42:10 rakete kernel:  dmu_tx_wait+0x384/0x390 [zfs]\r\nSep 19 07:42:10 rakete kernel:  dmu_tx_assign+0x8f/0x490 [zfs]\r\nSep 19 07:42:10 rakete kernel:  zfs_write+0x403/0xd30 [zfs]\r\nSep 19 07:42:10 rakete kernel:  ? blk_finish_plug+0x2c/0x40\r\nSep 19 07:42:10 rakete kernel:  ? __do_page_cache_readahead+0x1f4/0x2a0\r\nSep 19 07:42:10 rakete kernel:  ? dequeue_entity+0xed/0x4d0\r\nSep 19 07:42:10 rakete kernel:  ? radix_tree_lookup_slot+0x22/0x50\r\nSep 19 07:42:10 rakete kernel:  ? copyout+0x29/0x30\r\nSep 19 07:42:10 rakete kernel:  zpl_write_common_iovec+0x8c/0xe0 [zfs]\r\nSep 19 07:42:10 rakete kernel:  zpl_iter_write+0xae/0xe0 [zfs]\r\nSep 19 07:42:10 rakete kernel:  new_sync_write+0xcf/0x120\r\nSep 19 07:42:10 rakete kernel:  __vfs_write+0x26/0x40\r\nSep 19 07:42:10 rakete kernel:  vfs_write+0xb1/0x1a0\r\nSep 19 07:42:10 rakete kernel:  SyS_write+0x55/0xc0\r\nSep 19 07:42:10 rakete kernel:  entry_SYSCALL_64_fastpath+0x1a/0xa5\r\nSep 19 07:42:10 rakete kernel: RIP: 0033:0x7f9e60ef8844\r\nSep 19 07:42:10 rakete kernel: RSP: 002b:00007ffd0d5024d8 EFLAGS: 00000246 ORIG_RAX: 0000000000000001\r\nSep 19 07:42:10 rakete kernel: RAX: ffffffffffffffda RBX: 00005618ffa7b160 RCX: 00007f9e60ef8844\r\nSep 19 07:42:10 rakete kernel: RDX: 0000000000100000 RSI: 00007f9e61295000 RDI: 0000000000000001\r\nSep 19 07:42:10 rakete kernel: RBP: 0000000000100000 R08: 00007f9e61398540 R09: 00007ffd0d50249f\r\nSep 19 07:42:10 rakete kernel: R10: 00007f9e60f85940 R11: 0000000000000246 R12: 0000000000100000\r\nSep 19 07:42:10 rakete kernel: R13: 0000000000000000 R14: 0000000000000000 R15: 00007f9e61295000\r\n```\r\n\r\nOnly a reboot helps. the watchdog is letting me wait 3 min before it enters successfully final \"shutdown target\". Even my internal SATA raidz which holds all my data is unmounted properly. So that is good. Seems like zfs is not completely gone. But unfortunately I have a side effect with my /home (ext4) which can not be unmounted. \r\n\r\nIn summary:\r\n1) I think zfs should not crash like this with back trace etc.\r\n2) The side effect with my /home is weird. Any idea why that happens?\r\n\r\nKind Regards\r\nMatthias\r\n\r\n\r\n\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6656/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gmelikov": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6839", "title": "Test case zpool_import_missing_003_pos", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | all\r\nDistribution Version    | all\r\nLinux Kernel                 | all\r\nArchitecture                 | all\r\nZFS Version                  | master\r\nSPL Version                  | master\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nRarely observed failure of zpool_import_missing_003_pos during automated testing (due to timeout).\r\n\r\n### Describe how to reproduce the problem\r\nReproducible by the buildbot.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nhttps://s3.amazonaws.com/archive.travis-ci.org/jobs/297654895/log.txt?X-Amz-Expires=30&X-Amz-Date=20171107T115211Z&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJRYRXRSVGNKPKO5A/20171107/us-east-1/s3/aws4_request&X-Amz-SignedHeaders=host&X-Amz-Signature=26b6633b1fce4c548fba29ed50aff957a1ccee85bb902bd5fd7c66823fb43aaa\r\n```\r\nTest: /usr/share/zfs/zfs-tests/tests/functional/cli_root/zpool_import/zpool_import_missing_003_pos (run as root) [10:00] [KILLED]\r\n\r\nTest: /usr/share/zfs/zfs-tests/tests/functional/cli_root/zpool_import/zpool_import_missing_003_pos (run as root) [10:00] [KILLED]\r\n19:35:19.61 ASSERTION: Verify that import could handle device overlapped.\r\n19:35:21.41 SUCCESS: tar cf /var/tmp/dev_import-test/archive_import-test disk0 disk1 disk2 disk3 disk4\r\n19:35:41.75 SUCCESS: zpool create -f testpool1 /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:35:41.76 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:35:41.80 SUCCESS: zfs create testpool1/testfs\r\n19:35:41.85 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:35:41.86 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:35:41.90 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:35:42.00 SUCCESS: zpool export testpool1\r\n19:35:42.11 SUCCESS: zpool create -f testpool2 /var/tmp/dev_import-test/disk2 /var/tmp/dev_import-test/disk3 /var/tmp/dev_import-test/disk4\r\n19:35:42.12 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:35:42.16 SUCCESS: zfs create testpool2/testfs\r\n19:35:42.21 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:35:42.21 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:35:42.26 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:35:42.34 SUCCESS: zpool export testpool2\r\n19:35:42.46 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1 exited 1\r\n19:35:42.68 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:35:42.69 SUCCESS: poolexists testpool2\r\n19:35:42.70 SUCCESS: ismounted testpool2/testfs\r\n19:35:42.79 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:35:42.79 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:35:42.88 SUCCESS: zpool destroy -f testpool2\r\n19:35:45.19 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:36:18.76 SUCCESS: zpool create -f testpool1 /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:36:18.78 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:36:18.83 SUCCESS: zfs create testpool1/testfs\r\n19:36:18.89 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:36:18.89 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:36:18.94 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:36:19.03 SUCCESS: zpool export testpool1\r\n19:36:19.15 SUCCESS: zpool create -f testpool2 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2 /var/tmp/dev_import-test/disk3\r\n19:36:19.16 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:36:19.21 SUCCESS: zfs create testpool2/testfs\r\n19:36:19.27 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:36:19.28 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:36:19.33 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:36:19.41 SUCCESS: zpool export testpool2\r\n19:36:19.50 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1 exited 1\r\n19:36:19.75 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:36:19.76 SUCCESS: poolexists testpool2\r\n19:36:19.77 SUCCESS: ismounted testpool2/testfs\r\n19:36:19.86 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:36:19.86 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:36:19.96 SUCCESS: zpool destroy -f testpool2\r\n19:36:21.72 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:36:38.65 SUCCESS: zpool create -f testpool1 /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:36:38.66 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:36:38.72 SUCCESS: zfs create testpool1/testfs\r\n19:36:38.76 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:36:38.77 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:36:38.80 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:36:38.89 SUCCESS: zpool export testpool1\r\n19:36:39.02 SUCCESS: zpool create -f testpool2 /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:36:39.03 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:36:39.08 SUCCESS: zfs create testpool2/testfs\r\n19:36:39.13 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:36:39.14 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:36:39.17 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:36:39.26 SUCCESS: zpool export testpool2\r\n19:36:39.27 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1 exited 1\r\n19:36:39.52 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:36:39.53 SUCCESS: poolexists testpool2\r\n19:36:39.55 SUCCESS: ismounted testpool2/testfs\r\n19:36:39.64 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:36:39.64 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:36:39.73 SUCCESS: zpool destroy -f testpool2\r\n19:36:41.43 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:36:43.28 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:37:06.09 SUCCESS: zpool create -f testpool1 /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:37:06.10 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:37:07.75 SUCCESS: zfs create testpool1/testfs\r\n19:37:07.79 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:37:07.79 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:37:07.83 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:37:07.89 SUCCESS: zpool export testpool1\r\n19:37:08.03 SUCCESS: zpool create -f testpool2 mirror /var/tmp/dev_import-test/disk2 /var/tmp/dev_import-test/disk3 /var/tmp/dev_import-test/disk4\r\n19:37:08.04 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:37:08.09 SUCCESS: zfs create testpool2/testfs\r\n19:37:08.14 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:37:08.14 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:37:08.18 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:37:08.26 SUCCESS: zpool export testpool2\r\n19:37:08.35 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1 exited 1\r\n19:37:08.57 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:37:08.58 SUCCESS: poolexists testpool2\r\n19:37:08.59 SUCCESS: ismounted testpool2/testfs\r\n19:37:08.66 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:37:08.66 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:37:08.74 SUCCESS: zpool destroy -f testpool2\r\n19:37:10.30 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:37:34.49 SUCCESS: zpool create -f testpool1 /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:37:34.50 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:37:35.24 SUCCESS: zfs create testpool1/testfs\r\n19:37:35.29 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:37:35.29 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:37:35.32 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:37:35.39 SUCCESS: zpool export testpool1\r\n19:37:35.52 SUCCESS: zpool create -f testpool2 mirror /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2 /var/tmp/dev_import-test/disk3\r\n19:37:35.53 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:37:35.57 SUCCESS: zfs create testpool2/testfs\r\n19:37:35.64 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:37:35.64 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:37:35.68 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:37:35.76 SUCCESS: zpool export testpool2\r\n19:37:35.83 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1 exited 1\r\n19:37:36.04 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:37:36.05 SUCCESS: poolexists testpool2\r\n19:37:36.06 SUCCESS: ismounted testpool2/testfs\r\n19:37:36.15 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:37:36.15 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:37:36.24 SUCCESS: zpool destroy -f testpool2\r\n19:37:37.95 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:37:54.14 SUCCESS: zpool create -f testpool1 /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:37:54.15 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:37:54.20 SUCCESS: zfs create testpool1/testfs\r\n19:37:54.25 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:37:54.25 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:37:54.29 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:37:54.37 SUCCESS: zpool export testpool1\r\n19:37:54.49 SUCCESS: zpool create -f testpool2 mirror /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:37:54.50 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:37:54.55 SUCCESS: zfs create testpool2/testfs\r\n19:37:54.59 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:37:54.60 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:37:54.64 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:37:54.73 SUCCESS: zpool export testpool2\r\n19:37:54.75 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1 exited 1\r\n19:37:55.02 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:37:55.02 SUCCESS: poolexists testpool2\r\n19:37:55.03 SUCCESS: ismounted testpool2/testfs\r\n19:37:55.12 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:37:55.12 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:37:55.21 SUCCESS: zpool destroy -f testpool2\r\n19:37:56.76 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:37:58.45 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:38:18.37 SUCCESS: zpool create -f testpool1 /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:38:18.39 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:38:18.44 SUCCESS: zfs create testpool1/testfs\r\n19:38:18.49 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:38:18.49 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:38:18.53 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:38:18.62 SUCCESS: zpool export testpool1\r\n19:38:23.97 SUCCESS: zpool create -f testpool2 raidz /var/tmp/dev_import-test/disk2 /var/tmp/dev_import-test/disk3 /var/tmp/dev_import-test/disk4\r\n19:38:23.98 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:38:24.02 SUCCESS: zfs create testpool2/testfs\r\n19:38:24.06 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:38:24.07 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:38:24.11 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:38:24.19 SUCCESS: zpool export testpool2\r\n19:38:24.31 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1 exited 1\r\n19:38:24.56 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:38:24.56 SUCCESS: poolexists testpool2\r\n19:38:24.58 SUCCESS: ismounted testpool2/testfs\r\n19:38:24.67 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:38:24.67 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:38:24.76 SUCCESS: zpool destroy -f testpool2\r\n19:38:26.42 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:38:43.14 SUCCESS: zpool create -f testpool1 /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:38:43.15 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:38:43.19 SUCCESS: zfs create testpool1/testfs\r\n19:38:43.23 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:38:43.24 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:38:43.27 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:38:43.36 SUCCESS: zpool export testpool1\r\n19:38:47.43 SUCCESS: zpool create -f testpool2 raidz /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2 /var/tmp/dev_import-test/disk3\r\n19:38:47.44 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:38:47.47 SUCCESS: zfs create testpool2/testfs\r\n19:38:51.46 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:38:51.46 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:38:51.50 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:38:51.58 SUCCESS: zpool export testpool2\r\n19:38:51.64 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1 exited 1\r\n19:38:51.87 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:38:51.88 SUCCESS: poolexists testpool2\r\n19:38:51.89 SUCCESS: ismounted testpool2/testfs\r\n19:38:51.96 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:38:51.96 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:38:52.04 SUCCESS: zpool destroy -f testpool2\r\n19:38:53.53 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:39:10.06 SUCCESS: zpool create -f testpool1 /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:39:10.07 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:39:10.12 SUCCESS: zfs create testpool1/testfs\r\n19:39:10.17 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:39:10.17 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:39:10.21 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:39:10.29 SUCCESS: zpool export testpool1\r\n19:39:10.41 SUCCESS: zpool create -f testpool2 raidz /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:39:10.43 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:39:10.47 SUCCESS: zfs create testpool2/testfs\r\n19:39:10.52 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:39:10.52 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:39:10.56 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:39:10.64 SUCCESS: zpool export testpool2\r\n19:39:10.65 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1 exited 1\r\n19:39:10.86 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:39:10.87 SUCCESS: poolexists testpool2\r\n19:39:10.88 SUCCESS: ismounted testpool2/testfs\r\n19:39:10.96 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:39:10.96 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:39:11.07 SUCCESS: zpool destroy -f testpool2\r\n19:39:12.60 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:39:29.24 SUCCESS: zpool create -f testpool1 mirror /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:39:29.25 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:39:29.30 SUCCESS: zfs create testpool1/testfs\r\n19:39:29.36 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:39:29.36 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:39:29.41 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:39:29.48 SUCCESS: zpool export testpool1\r\n19:39:37.85 SUCCESS: zpool create -f testpool2 /var/tmp/dev_import-test/disk2 /var/tmp/dev_import-test/disk3 /var/tmp/dev_import-test/disk4\r\n19:39:37.86 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:39:37.90 SUCCESS: zfs create testpool2/testfs\r\n19:39:37.95 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:39:37.95 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:39:37.99 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:39:38.07 SUCCESS: zpool export testpool2\r\n19:39:38.29 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1\r\n19:39:38.51 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:39:38.52 SUCCESS: poolexists testpool1\r\n19:39:38.53 SUCCESS: ismounted testpool1/testfs\r\n19:39:38.55 SUCCESS: poolexists testpool2\r\n19:39:38.57 SUCCESS: ismounted testpool2/testfs\r\n19:39:38.65 SUCCESS: zfs destroy -r testpool1/testfs\r\n19:39:38.65 SUCCESS: rm -rf /var/tmp/testdir1\r\n19:39:38.74 SUCCESS: zpool destroy -f testpool1\r\n19:39:38.80 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:39:38.81 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:39:38.90 SUCCESS: zpool destroy -f testpool2\r\n19:39:40.47 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:39:56.87 SUCCESS: zpool create -f testpool1 mirror /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:39:56.89 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:39:56.93 SUCCESS: zfs create testpool1/testfs\r\n19:39:56.99 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:39:57.00 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:39:57.05 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:39:57.14 SUCCESS: zpool export testpool1\r\n19:40:05.50 SUCCESS: zpool create -f testpool2 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2 /var/tmp/dev_import-test/disk3\r\n19:40:05.51 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:40:05.56 SUCCESS: zfs create testpool2/testfs\r\n19:40:05.61 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:40:05.61 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:40:05.65 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:40:05.73 SUCCESS: zpool export testpool2\r\n19:40:05.91 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1\r\n19:40:06.13 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:40:06.13 SUCCESS: poolexists testpool1\r\n19:40:06.15 SUCCESS: ismounted testpool1/testfs\r\n19:40:06.16 SUCCESS: poolexists testpool2\r\n19:40:06.17 SUCCESS: ismounted testpool2/testfs\r\n19:40:06.25 SUCCESS: zfs destroy -r testpool1/testfs\r\n19:40:06.25 SUCCESS: rm -rf /var/tmp/testdir1\r\n19:40:06.34 SUCCESS: zpool destroy -f testpool1\r\n19:40:06.41 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:40:06.41 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:40:06.50 SUCCESS: zpool destroy -f testpool2\r\n19:40:08.06 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:40:24.09 SUCCESS: zpool create -f testpool1 mirror /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:40:24.10 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:40:24.14 SUCCESS: zfs create testpool1/testfs\r\n19:40:24.19 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:40:24.19 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:40:24.24 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:40:24.31 SUCCESS: zpool export testpool1\r\n19:40:24.41 SUCCESS: zpool create -f testpool2 /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:40:24.42 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:40:24.47 SUCCESS: zfs create testpool2/testfs\r\n19:40:24.51 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:40:24.52 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:40:24.55 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:40:24.63 SUCCESS: zpool export testpool2\r\n19:40:24.64 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1 exited 1\r\n19:40:24.86 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:40:24.87 SUCCESS: poolexists testpool2\r\n19:40:24.88 SUCCESS: ismounted testpool2/testfs\r\n19:40:24.95 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:40:24.96 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:40:25.04 SUCCESS: zpool destroy -f testpool2\r\n19:40:26.70 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:40:28.33 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:40:49.45 SUCCESS: zpool create -f testpool1 mirror /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:40:49.48 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:40:51.71 SUCCESS: zfs create testpool1/testfs\r\n19:40:51.76 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:40:51.76 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:40:51.80 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:40:51.87 SUCCESS: zpool export testpool1\r\n19:40:52.02 SUCCESS: zpool create -f testpool2 mirror /var/tmp/dev_import-test/disk2 /var/tmp/dev_import-test/disk3 /var/tmp/dev_import-test/disk4\r\n19:40:52.02 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:40:52.07 SUCCESS: zfs create testpool2/testfs\r\n19:40:52.13 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:40:52.13 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:40:52.17 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:40:52.25 SUCCESS: zpool export testpool2\r\n19:40:52.44 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1\r\n19:40:52.64 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:40:52.65 SUCCESS: poolexists testpool1\r\n19:40:52.67 SUCCESS: ismounted testpool1/testfs\r\n19:40:52.69 SUCCESS: poolexists testpool2\r\n19:40:52.69 SUCCESS: ismounted testpool2/testfs\r\n19:40:52.76 SUCCESS: zfs destroy -r testpool1/testfs\r\n19:40:52.77 SUCCESS: rm -rf /var/tmp/testdir1\r\n19:40:52.84 SUCCESS: zpool destroy -f testpool1\r\n19:40:52.92 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:40:52.93 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:40:53.01 SUCCESS: zpool destroy -f testpool2\r\n19:40:54.58 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:41:18.11 SUCCESS: zpool create -f testpool1 mirror /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:41:18.11 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:41:19.38 SUCCESS: zfs create testpool1/testfs\r\n19:41:19.42 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:41:19.42 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:41:19.45 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:41:19.53 SUCCESS: zpool export testpool1\r\n19:41:19.67 SUCCESS: zpool create -f testpool2 mirror /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2 /var/tmp/dev_import-test/disk3\r\n19:41:19.69 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:41:19.72 SUCCESS: zfs create testpool2/testfs\r\n19:41:19.77 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:41:19.77 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:41:19.81 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:41:19.89 SUCCESS: zpool export testpool2\r\n19:41:20.06 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1\r\n19:41:20.28 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:41:20.29 SUCCESS: poolexists testpool1\r\n19:41:20.30 SUCCESS: ismounted testpool1/testfs\r\n19:41:20.31 SUCCESS: poolexists testpool2\r\n19:41:20.32 SUCCESS: ismounted testpool2/testfs\r\n19:41:20.38 SUCCESS: zfs destroy -r testpool1/testfs\r\n19:41:20.39 SUCCESS: rm -rf /var/tmp/testdir1\r\n19:41:20.46 SUCCESS: zpool destroy -f testpool1\r\n19:41:20.51 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:41:20.52 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:41:20.59 SUCCESS: zpool destroy -f testpool2\r\n19:41:22.11 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:41:38.25 SUCCESS: zpool create -f testpool1 mirror /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:41:38.26 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:41:38.31 SUCCESS: zfs create testpool1/testfs\r\n19:41:38.36 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:41:38.37 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:41:38.41 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:41:38.48 SUCCESS: zpool export testpool1\r\n19:41:38.59 SUCCESS: zpool create -f testpool2 mirror /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:41:38.60 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:41:38.65 SUCCESS: zfs create testpool2/testfs\r\n19:41:38.69 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:41:38.70 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:41:38.74 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:41:38.80 SUCCESS: zpool export testpool2\r\n19:41:38.81 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1 exited 1\r\n19:41:39.00 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:41:39.01 SUCCESS: poolexists testpool2\r\n19:41:39.02 SUCCESS: ismounted testpool2/testfs\r\n19:41:39.08 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:41:39.09 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:41:39.16 SUCCESS: zpool destroy -f testpool2\r\n19:41:40.76 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:41:42.37 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:42:04.57 SUCCESS: zpool create -f testpool1 mirror /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:42:04.58 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:42:05.79 SUCCESS: zfs create testpool1/testfs\r\n19:42:05.84 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:42:05.85 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:42:05.89 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:42:05.96 SUCCESS: zpool export testpool1\r\n19:42:06.11 SUCCESS: zpool create -f testpool2 raidz /var/tmp/dev_import-test/disk2 /var/tmp/dev_import-test/disk3 /var/tmp/dev_import-test/disk4\r\n19:42:06.12 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:42:06.17 SUCCESS: zfs create testpool2/testfs\r\n19:42:06.23 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:42:06.23 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:42:06.27 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:42:06.35 SUCCESS: zpool export testpool2\r\n19:42:06.59 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1\r\n19:42:06.84 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:42:06.85 SUCCESS: poolexists testpool1\r\n19:42:06.86 SUCCESS: ismounted testpool1/testfs\r\n19:42:06.88 SUCCESS: poolexists testpool2\r\n19:42:06.89 SUCCESS: ismounted testpool2/testfs\r\n19:42:06.97 SUCCESS: zfs destroy -r testpool1/testfs\r\n19:42:06.97 SUCCESS: rm -rf /var/tmp/testdir1\r\n19:42:07.06 SUCCESS: zpool destroy -f testpool1\r\n19:42:07.14 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:42:07.15 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:42:07.25 SUCCESS: zpool destroy -f testpool2\r\n19:42:08.93 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:42:24.78 SUCCESS: zpool create -f testpool1 mirror /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:42:24.79 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:42:24.83 SUCCESS: zfs create testpool1/testfs\r\n19:42:24.87 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:42:24.88 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:42:24.92 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:42:24.99 SUCCESS: zpool export testpool1\r\n19:42:33.75 SUCCESS: zpool create -f testpool2 raidz /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2 /var/tmp/dev_import-test/disk3\r\n19:42:33.76 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:42:33.80 SUCCESS: zfs create testpool2/testfs\r\n19:42:33.85 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:42:33.86 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:42:33.90 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:42:33.98 SUCCESS: zpool export testpool2\r\n19:42:34.15 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1\r\n19:42:34.38 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:42:34.39 SUCCESS: poolexists testpool1\r\n19:42:34.40 SUCCESS: ismounted testpool1/testfs\r\n19:42:34.41 SUCCESS: poolexists testpool2\r\n19:42:34.43 SUCCESS: ismounted testpool2/testfs\r\n19:42:34.49 SUCCESS: zfs destroy -r testpool1/testfs\r\n19:42:34.50 SUCCESS: rm -rf /var/tmp/testdir1\r\n19:42:34.56 SUCCESS: zpool destroy -f testpool1\r\n19:42:34.64 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:42:34.64 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:42:34.74 SUCCESS: zpool destroy -f testpool2\r\n19:42:36.28 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:42:52.99 SUCCESS: zpool create -f testpool1 mirror /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:42:53.00 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:42:53.05 SUCCESS: zfs create testpool1/testfs\r\n19:42:53.10 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:42:53.10 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:42:53.15 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:42:53.23 SUCCESS: zpool export testpool1\r\n19:42:53.35 SUCCESS: zpool create -f testpool2 raidz /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:42:53.37 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:42:53.41 SUCCESS: zfs create testpool2/testfs\r\n19:42:53.46 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:42:53.46 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:42:53.50 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:42:53.58 SUCCESS: zpool export testpool2\r\n19:42:53.59 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1 exited 1\r\n19:42:53.82 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:42:53.82 SUCCESS: poolexists testpool2\r\n19:42:53.83 SUCCESS: ismounted testpool2/testfs\r\n19:42:53.91 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:42:53.92 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:42:54.00 SUCCESS: zpool destroy -f testpool2\r\n19:42:55.57 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:43:11.84 SUCCESS: zpool create -f testpool1 raidz /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:43:11.85 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:43:11.91 SUCCESS: zfs create testpool1/testfs\r\n19:43:11.95 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:43:11.96 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:43:12.00 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:43:12.08 SUCCESS: zpool export testpool1\r\n19:43:20.78 SUCCESS: zpool create -f testpool2 /var/tmp/dev_import-test/disk2 /var/tmp/dev_import-test/disk3 /var/tmp/dev_import-test/disk4\r\n19:43:20.79 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:43:20.83 SUCCESS: zfs create testpool2/testfs\r\n19:43:20.87 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:43:20.88 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:43:20.92 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:43:21.01 SUCCESS: zpool export testpool2\r\n19:43:21.21 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1\r\n19:43:21.44 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:43:21.45 SUCCESS: poolexists testpool1\r\n19:43:21.46 SUCCESS: ismounted testpool1/testfs\r\n19:43:21.48 SUCCESS: poolexists testpool2\r\n19:43:21.49 SUCCESS: ismounted testpool2/testfs\r\n19:43:21.55 SUCCESS: zfs destroy -r testpool1/testfs\r\n19:43:21.56 SUCCESS: rm -rf /var/tmp/testdir1\r\n19:43:21.64 SUCCESS: zpool destroy -f testpool1\r\n19:43:21.70 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:43:21.70 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:43:21.80 SUCCESS: zpool destroy -f testpool2\r\n19:43:23.36 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:43:40.14 SUCCESS: zpool create -f testpool1 raidz /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:43:40.15 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:43:40.23 SUCCESS: zfs create testpool1/testfs\r\n19:43:40.29 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:43:40.30 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:43:40.34 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:43:40.44 SUCCESS: zpool export testpool1\r\n19:43:48.54 SUCCESS: zpool create -f testpool2 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2 /var/tmp/dev_import-test/disk3\r\n19:43:48.54 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:43:48.60 SUCCESS: zfs create testpool2/testfs\r\n19:43:48.64 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:43:48.65 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:43:48.68 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:43:48.75 SUCCESS: zpool export testpool2\r\n19:43:48.78 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1 exited 1\r\n19:43:49.01 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:43:49.02 SUCCESS: poolexists testpool2\r\n19:43:49.03 SUCCESS: ismounted testpool2/testfs\r\n19:43:49.11 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:43:49.11 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:43:49.20 SUCCESS: zpool destroy -f testpool2\r\n19:43:50.82 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:44:07.01 SUCCESS: zpool create -f testpool1 raidz /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:44:07.02 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:44:07.07 SUCCESS: zfs create testpool1/testfs\r\n19:44:07.12 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:44:07.13 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:44:07.17 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:44:07.26 SUCCESS: zpool export testpool1\r\n19:44:07.39 SUCCESS: zpool create -f testpool2 /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:44:07.41 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:44:07.45 SUCCESS: zfs create testpool2/testfs\r\n19:44:07.50 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:44:07.50 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:44:07.54 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:44:07.63 SUCCESS: zpool export testpool2\r\n19:44:07.64 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1 exited 1\r\n19:44:07.88 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:44:07.88 SUCCESS: poolexists testpool2\r\n19:44:07.90 SUCCESS: ismounted testpool2/testfs\r\n19:44:07.98 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:44:07.99 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:44:08.08 SUCCESS: zpool destroy -f testpool2\r\n19:44:09.81 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:44:11.78 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:44:38.51 SUCCESS: zpool create -f testpool1 raidz /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:44:38.52 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:44:38.98 SUCCESS: zfs create testpool1/testfs\r\n19:44:39.03 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:44:39.04 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:44:39.09 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:44:39.18 SUCCESS: zpool export testpool1\r\n19:44:39.31 SUCCESS: zpool create -f testpool2 mirror /var/tmp/dev_import-test/disk2 /var/tmp/dev_import-test/disk3 /var/tmp/dev_import-test/disk4\r\n19:44:39.32 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:44:39.37 SUCCESS: zfs create testpool2/testfs\r\n19:44:39.42 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:44:39.42 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:44:39.47 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:44:39.55 SUCCESS: zpool export testpool2\r\n19:44:39.80 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1\r\n19:44:40.05 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:44:40.06 SUCCESS: poolexists testpool1\r\n19:44:40.07 SUCCESS: ismounted testpool1/testfs\r\n19:44:40.08 SUCCESS: poolexists testpool2\r\n19:44:40.10 SUCCESS: ismounted testpool2/testfs\r\n19:44:40.17 SUCCESS: zfs destroy -r testpool1/testfs\r\n19:44:40.17 SUCCESS: rm -rf /var/tmp/testdir1\r\n19:44:40.25 SUCCESS: zpool destroy -f testpool1\r\n19:44:40.31 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:44:40.32 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:44:40.41 SUCCESS: zpool destroy -f testpool2\r\n19:44:42.11 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n19:44:57.93 SUCCESS: zpool create -f testpool1 raidz /var/tmp/dev_import-test/disk0 /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2\r\n19:44:57.94 SUCCESS: mkdir -p /var/tmp/testdir1\r\n19:44:57.97 SUCCESS: zfs create testpool1/testfs\r\n19:44:58.03 SUCCESS: zfs set mountpoint=/var/tmp/testdir1 testpool1/testfs\r\n19:44:58.03 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir1/testfile0\r\n19:44:58.07 SUCCESS: zfs umount /var/tmp/testdir1\r\n19:44:58.14 SUCCESS: zpool export testpool1\r\n19:45:06.90 SUCCESS: zpool create -f testpool2 mirror /var/tmp/dev_import-test/disk1 /var/tmp/dev_import-test/disk2 /var/tmp/dev_import-test/disk3\r\n19:45:06.91 SUCCESS: mkdir -p /var/tmp/testdir2\r\n19:45:06.94 SUCCESS: zfs create testpool2/testfs\r\n19:45:06.99 SUCCESS: zfs set mountpoint=/var/tmp/testdir2 testpool2/testfs\r\n19:45:07.00 SUCCESS: cp /usr/share/zfs/zfs-tests/include/libtest.shlib /var/tmp/testdir2/testfile0\r\n19:45:07.03 SUCCESS: zfs umount /var/tmp/testdir2\r\n19:45:07.10 SUCCESS: zpool export testpool2\r\n19:45:07.14 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool1 exited 1\r\n19:45:07.35 SUCCESS: zpool import -d /var/tmp/dev_import-test testpool2\r\n19:45:07.36 SUCCESS: poolexists testpool2\r\n19:45:07.37 SUCCESS: ismounted testpool2/testfs\r\n19:45:07.45 SUCCESS: zfs destroy -r testpool2/testfs\r\n19:45:07.45 SUCCESS: rm -rf /var/tmp/testdir2\r\n19:45:07.53 SUCCESS: zpool destroy -f testpool2\r\n19:45:09.13 SUCCESS: tar xf /var/tmp/dev_import-test/archive_import-test\r\n```\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6839/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/13042a6ccdf3450a758511c4b64e800a371bb891", "message": "Add .travis.yml\n\nTravis builders have maximum work time ~49 minutes,\r\nso we have to use 5 builders and spread the ZTS over\r\nthem using test group tags.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: George Melikov <mail@gmelikov.ru>\r\nCloses #6829"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/b58b73ce74b032184a089f7084c638f71453bdfa", "message": "Disable zpool_import_missing_003_pos\n\nRarely observed failure of zpool_import_missing_003_pos during\r\nautomated testing due to timeout.  Disable the test case until\r\nit can be improved.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: George Melikov <mail@gmelikov.ru>\r\nIssue #6839 \r\nCloses #6840"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/7c9abcf88781947d02863b81ae98c58b18ed71d0", "message": "OpenZFS 8435 - zpool.1m and zfs.1m: minor cleanup\n\n3796 listsnapshots not documented in zpool man page\r\n\r\nAuthored by: George Melikov <mail@gmelikov.ru>\r\nReviewed by: Matt Ahrens <mahrens@delphix.com>\r\nReviewed by: Yuri Pankov <yuripv@gmx.com>\r\nApproved by: Dan McDonald <danmcd@joyent.com>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nPorted-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nPorted-by: George Melikov mail@gmelikov.ru\r\n\r\nOpenZFS-issue: https://www.illumos.org/issues/8435\r\nOpenZFS-commit: openzfs/openzfs@a058d1c\r\n\r\nPorting notes: OpenZFS review applied,\r\nsome ZoL changes were reverted.\r\nSee https://github.com/openzfs/openzfs/pull/415"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/076e9b946ef7a706ce2926c29912c467cdeddab6", "message": "Remove copyright duplicate in zpool man page\n\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: George Melikov <mail@gmelikov.ru>\r\nCloses #6553"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/4265a9293e49008bf0393fe678c8e8d53429aa1f", "message": "Fix coverity defects: CID 165757\n\nCID 165757: Control flow issues (MISSING_BREAK)\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: George Melikov <mail@gmelikov.ru>\r\nCloses #6348"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/9c3dcab5e229723ba41c67926223e942794a9ab2", "message": "ZTS: replace su commands by run_user function\n\nNeeded for PATH variable to be passed into su.  The\r\nposix* tests were fixed, but they need further investigation\r\nbefore they can be enabled.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: George Melikov <mail@gmelikov.ru>\r\nCloses #6303"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "DeHackEd": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6825", "title": "Feature: rollback and preserve old data", "body": "From an IRC discussion I'm submitting this feature request. Maybe I'll try implementing it myself.\r\n\r\n<pre>\r\nzfs rollback -k [-p] [-o property=value...] pool/fs@snap pool/preservedfs\r\n\r\nRollback similarly to the regular 'zfs rollback' command, but keep the old dataset around\r\nas pool/preservedfs as a clone of the original dataset.\r\n\r\n -p   Create any missing filesystems required to create pool/preservedfs\r\n -o   Set properties on the cloned filesystem\r\n</pre>\r\n\r\nThe scenario presented was analyzing the bad data while still getting the good data back into place, maybe after an attack of sorts", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6825/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6393", "title": "VERIFY3(0 == zap_update(scn->scn_dp->dp_meta_objset, 1, \"scan\", ...)) failed (0 == 6)", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS\r\nDistribution Version    | 6.x\r\nLinux Kernel                 | 4.4.23\r\nArchitecture                 | x86_64\r\nZFS Version                  | 9baaa7deae45c8556dfd79b2011234da5cb37b3a + ABD (old)\r\nSPL Version                  | v0.7.0-rc1_6_g341dfdb\r\n\r\n### Describe the problem you're observing\r\nI had an external USB multi-drive enclosure fail when one of the disks inside started making bad noises. The system hadn't noticed until a scrub began when it all went downhill fast. Rather than the pool suspending itself, instead I had an assertion failure.\r\n\r\n### Describe how to reproduce the problem\r\nNearest I can tell it happened when all the drives failed simultaneously just as the `zpool scrub` command was executed. The pool drives appeared to be safe from the OS point of view prior to executing the scrub command.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n```\r\nVERIFY3(0 == zap_update(scn->scn_dp->dp_meta_objset, 1, \"scan\", sizeof (uint64_t), (sizeof (dsl_scan_phys_t) / sizeof (uint64_t)), &scn->scn_phys, tx)) failed (0 == 6)\r\nPANIC at dsl_scan.c:437:dsl_scan_sync_state()\r\nShowing stack for process 8203\r\nCPU: 1 PID: 8203 Comm: txg_sync Tainted: P           O    4.4.23-bahamut #4\r\nHardware name: HP HP EliteBook 850 G3/8079, BIOS N75 Ver. 01.10 07/31/2016\r\n 0000000000000296 0000000000000000 ffffffff81190ef2 0000000000000400\r\n 000000000000036a ffffffffa02ae8f0 ffffffffa02be2fb 00000000000001b5\r\n ffffffffa01ce467 2833594649524556 70617a203d3d2030 286574616470755f\r\nCall Trace:\r\n [<ffffffff81190ef2>] ? dump_stack+0x4a/0x68\r\n [<ffffffffa01ce467>] ? spl_panic+0xae/0xe7 [spl]\r\n [<ffffffffa0270b09>] ? zap_update+0x43/0x176 [zfs]\r\n [<ffffffffa023d20c>] ? dsl_scan_setup_sync+0x1ab/0x1e5 [zfs]\r\n [<ffffffffa02406ea>] ? dsl_sync_task_sync+0xaf/0xe6 [zfs]\r\n [<ffffffffa023a863>] ? dsl_pool_sync+0x3bf/0x3e6 [zfs]\r\n [<ffffffffa025250d>] ? spa_sync+0x33e/0x624 [zfs]\r\n [<ffffffffa025e50c>] ? txg_sync_thread+0x31a/0x3ea [zfs]\r\n [<ffffffffa025e1f2>] ? txg_thread_exit+0x3e/0x3e [zfs]\r\n [<ffffffffa01cbbe1>] ? __thread_create+0x11e/0x11e [spl]\r\n [<ffffffffa01cbc4b>] ? thread_generic_wrapper+0x6a/0x75 [spl]\r\n [<ffffffffa01cbbe1>] ? __thread_create+0x11e/0x11e [spl]\r\n [<ffffffff81058f4a>] ? kthread+0xc3/0xcb\r\n [<ffffffff81058e87>] ? kthread_freezable_should_stop+0x61/0x61\r\n [<ffffffff8139b59f>] ? ret_from_fork+0x3f/0x70\r\n [<ffffffff81058e87>] ? kthread_freezable_should_stop+0x61/0x61\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6393/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6275", "title": "Large_blocks feature activation not consistent with man page", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  CentOS\r\nDistribution Version    |  6 and 7 tested\r\nLinux Kernel                 | 4.9.x and 3.10.0-xx.el7\r\nArchitecture                 | x86_64\r\nZFS Version                  | Recent Git head + Metadata allocation classes\r\nSPL Version                  | GIT head\r\n\r\n### Describe the problem you're observing\r\nIf you create a pool large blocks enabled on the root dataset, the `feature@large_blocks` feature flag does not transition to `active`.\r\n\r\n### Describe how to reproduce the problem\r\n<pre>\r\n# cd /tmp\r\n# dd if=/dev/zero bs=1M of=testfile count=256\r\n# zpool create -O recordsize=1M testpool /tmp/testfile \r\n# zpool get all testpool\r\nNAME      PROPERTY                       VALUE                          SOURCE\r\ntestpool  size                           240M                           -\r\ntestpool  capacity                       0%                             -\r\ntestpool  altroot                        -                              default\r\ntestpool  health                         ONLINE                         -\r\ntestpool  guid                           8958191487329258713            -\r\ntestpool  version                        -                              default\r\ntestpool  bootfs                         -                              default\r\ntestpool  delegation                     on                             default\r\ntestpool  autoreplace                    off                            default\r\ntestpool  cachefile                      -                              default\r\ntestpool  failmode                       wait                           default\r\ntestpool  listsnapshots                  off                            default\r\ntestpool  autoexpand                     off                            default\r\ntestpool  dedupditto                     0                              default\r\ntestpool  dedupratio                     1.00x                          -\r\ntestpool  free                           240M                           -\r\ntestpool  allocated                      102K                           -\r\ntestpool  readonly                       off                            -\r\ntestpool  ashift                         0                              default\r\ntestpool  comment                        -                              default\r\ntestpool  expandsize                     -                              -\r\ntestpool  freeing                        0                              -\r\ntestpool  fragmentation                  1%                             -\r\ntestpool  leaked                         0                              -\r\ntestpool  feature@async_destroy          enabled                        local\r\ntestpool  feature@empty_bpobj            enabled                        local\r\ntestpool  feature@lz4_compress           active                         local\r\ntestpool  feature@multi_vdev_crash_dump  enabled                        local\r\ntestpool  feature@spacemap_histogram     active                         local\r\ntestpool  feature@enabled_txg            active                         local\r\ntestpool  feature@hole_birth             active                         local\r\ntestpool  feature@extensible_dataset     active                         local\r\ntestpool  feature@embedded_data          active                         local\r\ntestpool  feature@bookmarks              enabled                        local\r\ntestpool  feature@filesystem_limits      enabled                        local\r\n<b>testpool  feature@large_blocks           enabled                        local</b>\r\ntestpool  feature@large_dnode            enabled                        local\r\ntestpool  feature@sha512                 enabled                        local\r\ntestpool  feature@skein                  enabled                        local\r\ntestpool  feature@edonr                  enabled                        local\r\ntestpool  feature@userobj_accounting     active                         local\r\ntestpool  feature@allocation_classes     enabled                        local\r\n\r\n# zfs get all testpool\r\n# zfs get all testpool\r\nNAME      PROPERTY              VALUE                  SOURCE\r\ntestpool  type                  filesystem             -\r\ntestpool  creation              Tue Jun 27 11:00 2017  -\r\ntestpool  used                  4.08M                  -\r\ntestpool  available             116M                   -\r\ntestpool  referenced            4.02M                  -\r\ntestpool  compressratio         1.00x                  -\r\ntestpool  mounted               yes                    -\r\ntestpool  quota                 none                   default\r\ntestpool  reservation           none                   default\r\n<b>testpool  recordsize            1M                     local</b>\r\ntestpool  mountpoint            /testpool              default\r\ntestpool  sharenfs              off                    default\r\ntestpool  checksum              on                     default\r\ntestpool  compression           off                    default\r\ntestpool  atime                 on                     default\r\ntestpool  devices               on                     default\r\ntestpool  exec                  on                     default\r\ntestpool  setuid                on                     default\r\ntestpool  readonly              off                    default\r\ntestpool  zoned                 off                    default\r\ntestpool  snapdir               hidden                 default\r\ntestpool  aclinherit            restricted             default\r\ntestpool  canmount              on                     default\r\ntestpool  xattr                 on                     default\r\ntestpool  copies                1                      default\r\ntestpool  version               5                      -\r\ntestpool  utf8only              off                    -\r\ntestpool  normalization         none                   -\r\ntestpool  casesensitivity       sensitive              -\r\ntestpool  vscan                 off                    default\r\ntestpool  nbmand                off                    default\r\ntestpool  sharesmb              off                    default\r\ntestpool  refquota              none                   default\r\ntestpool  refreservation        none                   default\r\ntestpool  primarycache          all                    default\r\ntestpool  secondarycache        all                    default\r\ntestpool  usedbysnapshots       0                      -\r\ntestpool  usedbydataset         4.02M                  -\r\ntestpool  usedbychildren        57K                    -\r\ntestpool  usedbyrefreservation  0                      -\r\ntestpool  logbias               latency                default\r\ntestpool  dedup                 off                    default\r\ntestpool  mlslabel              none                   default\r\ntestpool  sync                  standard               default\r\ntestpool  dnodesize             legacy                 default\r\ntestpool  refcompressratio      1.00x                  -\r\ntestpool  written               4.02M                  -\r\ntestpool  logicalused           4.03M                  -\r\ntestpool  logicalreferenced     4.01M                  -\r\ntestpool  filesystem_limit      none                   default\r\ntestpool  snapshot_limit        none                   default\r\ntestpool  filesystem_count      none                   default\r\ntestpool  snapshot_count        none                   default\r\ntestpool  snapdev               hidden                 default\r\ntestpool  acltype               off                    default\r\ntestpool  context               none                   default\r\ntestpool  fscontext             none                   default\r\ntestpool  defcontext            none                   default\r\ntestpool  rootcontext           none                   default\r\ntestpool  relatime              off                    default\r\ntestpool  redundant_metadata    all                    default\r\ntestpool  overlay               off                    default\r\n</pre>\r\n\r\nDouble-checked by zdb\r\n<pre>\r\n# /testpool\r\n# dd if=/dev/urandom bs=1M of=randomfile count=4\r\n# ls -li randomfile\r\n7 -rw-r--r-- 1 root root 4194304 Jun 27 11:02 /testpool/randomfile\r\n# zdb -ddddddd testpool/ 7\r\nDataset testpool [ZPL], ID 51, cr_txg 1, 4.02M, 7 objects, rootbp DVA[0]=<0:415a00:200> DVA[1]=<0:1014a00:200> [L0 DMU objset] fletcher4 lz4 LE contiguous unique double size=800L/200P birth=22L/22P fill=7 cksum=b7beb1623:3e0e34ef556:b09b977ec9c3:15fef4772301ec\r\n\r\n    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  type\r\n         7    2   128K     1M  4.00M     512     4M  100.00  ZFS plain file (K=inherit) (Z=inherit)\r\n                                               168   bonus  System attributes\r\n\tdnode flags: USED_BYTES USERUSED_ACCOUNTED USEROBJUSED_ACCOUNTED \r\n\tdnode maxblkid: 3\r\n\tpath\t/randomfile\r\n\tuid     0\r\n\tgid     0\r\n\tatime\tTue Jun 27 11:02:35 2017\r\n\tmtime\tTue Jun 27 11:02:35 2017\r\n\tctime\tTue Jun 27 11:02:35 2017\r\n\tcrtime\tTue Jun 27 11:02:35 2017\r\n\tgen\t19\r\n\tmode\t100644\r\n\tsize\t4194304\r\n\tparent\t4\r\n\tlinks\t1\r\n\tpflags\t40800000004\r\nIndirect blocks:\r\n               0 L1  0:410a00:400 0:100fa00:400 20000L/400P F=4 B=19/19\r\n               0  L0 0:10a00:100000 100000L/100000P F=1 B=19/19\r\n          100000  L0 0:210a00:100000 100000L/100000P F=1 B=19/19\r\n          200000  L0 0:110a00:100000 100000L/100000P F=1 B=19/19\r\n          300000  L0 0:310a00:100000 100000L/100000P F=1 B=19/19\r\n\r\n\t\tsegment [0000000000000000, 0000000000400000) size    4M\r\n</pre>\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nNothing here", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6275/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6123", "title": "VERIFY3(hdr->b_type == type) failed (0 == 1)", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS\r\nDistribution Version    | 6\r\nLinux Kernel                 | 4.9.22 (custom build)\r\nArchitecture                 | amd64\r\nZFS Version                  | v0.7.0-rc3_207_g06226b5\r\nSPL Version                  | v0.7.0-rc3_9_g4c95b46\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n<pre>May 11 20:22:56 atmaweapon kernel: VERIFY3(hdr-&gt;b_type == type) failed (0 == 1)\r\nMay 11 20:22:56 atmaweapon kernel: PANIC at arc.c:1542:arc_buf_type()\r\nMay 11 20:22:56 atmaweapon kernel: Showing stack for process 3355\r\nMay 11 20:22:56 atmaweapon kernel: Hardware name: ASUS All Series/Z97-A, BIOS 1304 07/11/2014\r\nMay 11 20:22:56 atmaweapon kernel: ffffa627c785fba8 ffffffffaf2b5249 0000000000000001 0000000108254cc1\r\nMay 11 20:22:56 atmaweapon kernel: ffffa627c785fd58 ffffffffc0d839ed ffffa627c785fbf8 ffffffffc0f0b0c0\r\nMay 11 20:22:56 atmaweapon kernel: [&lt;ffffffffaf2b5249&gt;] dump_stack+0x51/0x78\r\nMay 11 20:22:56 atmaweapon kernel: [&lt;ffffffffc0d839ed&gt;] spl_panic+0xbd/0x100 [spl]\r\nMay 11 20:22:56 atmaweapon kernel: [&lt;ffffffffaf07aecb&gt;] ? pick_next_entity+0x7b/0x120\r\nMay 11 20:22:56 atmaweapon kernel: [&lt;ffffffffc0d7e361&gt;] ? spl_kmem_alloc+0x91/0x1a0 [spl]\r\nMay 11 20:22:56 atmaweapon kernel: [&lt;ffffffffc0e0a8e1&gt;] remove_reference+0x41/0x70 [zfs]\r\nMay 11 20:22:56 atmaweapon kernel: [&lt;ffffffffc0d80880&gt;] ? __thread_create+0x150/0x150 [spl]\r\nMay 11 20:22:56 atmaweapon kernel: [&lt;ffffffffc0e6a20a&gt;] ? spa_get_random+0x1a/0x30 [zfs]\r\nMay 11 20:22:56 atmaweapon kernel: [&lt;ffffffffc0e14266&gt;] dbuf_evict_one+0x96/0xc0 [zfs]\r\nMay 11 20:22:56 atmaweapon kernel: [&lt;ffffffffc0e17815&gt;] dbuf_evict_thread+0xb5/0x140 [zfs]\r\nMay 11 20:22:56 atmaweapon kernel: [&lt;ffffffffaf06cc38&gt;] kthread+0xc8/0xf0\r\nMay 11 20:22:56 atmaweapon kernel: [&lt;ffffffffaf06cb70&gt;] ? __kthread_init_worker+0x30/0x30\r\n</pre>\r\nThread is the dbuf_evict kernel thread.\r\n\r\n### Describe how to reproduce the problem\r\nUnknown exact cause, but I can't find a similar dump in the issue tracker. I was playing a Steam game at the time when I noticed the panic alert in my second monitor. From there on apps slowly started locking up but the game (which doesn't run primarily on a ZFS partition) kept running.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nNo warning signs before the above failure, so I'll post these.\r\n\r\n```\r\n# zpool status\r\n  pool: atmaweapon\r\n state: ONLINE\r\nstatus: Some supported features are not enabled on the pool. The pool can\r\n\tstill be used, but some features are unavailable.\r\naction: Enable all features using 'zpool upgrade'. Once this is done,\r\n\tthe pool may no longer be accessible by software that does not support\r\n\tthe features. See zpool-features(5) for details.\r\n  scan: scrub canceled on Sat Apr 29 18:09:57 2017\r\nconfig:\r\n\r\n\tNAME             STATE     READ WRITE CKSUM\r\n\tatmaweapon       ONLINE       0     0     0\r\n\t  mirror-0       ONLINE       0     0     0\r\n\t    sdd          ONLINE       0     0     0\r\n\t    sdc          ONLINE       0     0     0\r\n\tlogs\r\n\t  zemus-l2arc_2  ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n# zpool get all atmaweapon\r\nNAME        PROPERTY                       VALUE                          SOURCE\r\natmaweapon  size                           2.73T                          -\r\natmaweapon  capacity                       84%                            -\r\natmaweapon  altroot                        -                              default\r\natmaweapon  health                         ONLINE                         -\r\natmaweapon  guid                           13063543294611499435           -\r\natmaweapon  version                        -                              default\r\natmaweapon  bootfs                         -                              default\r\natmaweapon  delegation                     on                             default\r\natmaweapon  autoreplace                    off                            default\r\natmaweapon  cachefile                      -                              default\r\natmaweapon  failmode                       wait                           default\r\natmaweapon  listsnapshots                  off                            default\r\natmaweapon  autoexpand                     off                            default\r\natmaweapon  dedupditto                     0                              default\r\natmaweapon  dedupratio                     1.00x                          -\r\natmaweapon  free                           423G                           -\r\natmaweapon  allocated                      2.31T                          -\r\natmaweapon  readonly                       off                            -\r\natmaweapon  ashift                         12                             local\r\natmaweapon  comment                        -                              default\r\natmaweapon  expandsize                     -                              -\r\natmaweapon  freeing                        0                              -\r\natmaweapon  fragmentation                  -                              -\r\natmaweapon  leaked                         0                              -\r\natmaweapon  feature@async_destroy          enabled                        local\r\natmaweapon  feature@empty_bpobj            active                         local\r\natmaweapon  feature@lz4_compress           active                         local\r\natmaweapon  feature@multi_vdev_crash_dump  disabled                       local\r\natmaweapon  feature@spacemap_histogram     disabled                       local\r\natmaweapon  feature@enabled_txg            disabled                       local\r\natmaweapon  feature@hole_birth             disabled                       local\r\natmaweapon  feature@extensible_dataset     disabled                       local\r\natmaweapon  feature@embedded_data          disabled                       local\r\natmaweapon  feature@bookmarks              disabled                       local\r\natmaweapon  feature@filesystem_limits      disabled                       local\r\natmaweapon  feature@large_blocks           disabled                       local\r\natmaweapon  feature@large_dnode            disabled                       local\r\natmaweapon  feature@sha512                 disabled                       local\r\natmaweapon  feature@skein                  disabled                       local\r\natmaweapon  feature@edonr                  disabled                       local\r\natmaweapon  feature@userobj_accounting     disabled                       local\r\n```\r\nNote: an slog is used that is named l2arc.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6123/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d658b2caa95726c13d99123874910cdedc7ce866", "message": "Remove l2arc_nocompress from zfs-module-parameters(5)\n\nParameter was removed in d3c2ae1c0806\r\n(OpenZFS 6950 - ARC should cache compressed data)\r\n\r\nReviewed by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: DHE <git@dehacked.net>\r\nCloses #7043"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/460f239e6999195dbcf9b8443c029f07765b21e9", "message": "Fix -fsanitize=address memory leak\n\nkmem_alloc(0, ...) in userspace returns a leakable pointer.\n\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nSigned-off-by: DHE <git@dehacked.net>\nIssue #6941"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/1c68856bcaad053bb21acdde4ca952b764edd664", "message": "zpool(8): Fix \"zpool import -t\"\n\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: DHE <git@dehacked.net>\r\nCloses #6894"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/da5d4697a84f0baf7d8fb9dbdf2e1312a370c075", "message": "Fix ARC pointer overrun\n\nOnly access the `b_crypt_hdr` field of an ARC header if the content\r\nis encrypted.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Tom Caputi <tcaputi@datto.com>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: DHE <git@dehacked.net>\r\nCloses #6877"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/7e98073379353a05498ac5a2f1a5df2a2257d6b0", "message": "Fix printk() calls missing log level\n\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: DHE <git@dehacked.net>\r\nCloses #6672"}], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6673", "title": "Fail importing if cached config has wrong number of vdev children", "body": "During import compare the labels' configs with the import config\r\nand fail if the labels indicate more vdevs than the cached config\r\n\r\nSigned-off-by: DHE <git@dehacked.net>\r\nFixes #6671\r\n\r\n### Description\r\nWhen the zpool.cache says `vdev_children=X` but the pool actually has `vdev_children=Y` where `X<Y`, blkptr errors will occur during the import process. We detect this specific case and refuse imports from this cache file.\r\n\r\n### Motivation and Context\r\nSee #6671\r\n\r\n### How Has This Been Tested?\r\n`ztest` only\r\n\r\n### Types of changes\r\n- [X] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n- [X] My code follows the ZFS on Linux code style requirements.\r\n- [ ] I have updated the documentation accordingly.\r\n- [ ] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [X] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/12508683", "body": "I don't think libzpool should be forcing its applications to exit if it runs out of memory.\n\nOn the other hand the current codepath doesn't return errors to the application so the \"best\" result might be to stop printing the information.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/12508683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "sjau": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6824", "title": "Multiple encryption keys/key methods", "body": "I'd like to propose a feature request to support multiple key slots/method slots like LUKS does it.\r\n\r\nThis can be very useful so that for example 2 different people have different passwords and they can independantly open up the encrypted dataset without needing to know the other's (highly secure and complex) passphrase.\r\n\r\nAlso, if you have multiple zpools it could be useful to provide path to the keyfile to automatically unlock it if needed...", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6824/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6821", "title": "spl_panic when receiving encrypted dataset", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Nixos\r\nDistribution Version    |  Nixos Unstable Small\r\nLinux Kernel                 |  4.9.58\r\nArchitecture                 |   x86_64\r\nZFS Version                  |   0.7.0-1\r\nSPL Version                  |   0.7.0-1\r\n\r\n### Describe the problem you're observing\r\n\r\nI'm trying to backup encrypted datasets from my notebook to my homeserver. Both run same Nixos version. However it doesn't work.\r\n\r\nWhen I use `-wR` options for full dataset sending, then on the receiving end spl_panic appears and it never finishes.\r\n\r\nIf I omit the `-R` option, then full dataset sending works. However when I then try send an incremental set, dame things happens again - spl_panic\r\n\r\n### Describe how to reproduce the problem\r\n\r\nOn notebook I have:\r\ntank/encZFS/Nixos -> encZFS was create this way:\r\n`  zfs create -o encryption=aes-256-gcm -o keyformat=passphrase -o mountpoint=none -o atime=off ${zfsPool}/encZFS`\r\n\r\nOn the server I have:\r\nserviTank/BU/subi -> None of those is encrypted\r\n\r\nI then took a snapshot and tried to send like this:\r\n\r\n`zfs send -wR tank/encZFS/Nixos@encZFSSend_2017-11-04_12-31 | ssh root@10.0.0.3 'zfs receive serviTank/BU/subi/Nixos'`\r\n\r\nIt seems all was transferred correctly:\r\n\r\n```\r\nzfs list\r\nNAME                       USED  AVAIL  REFER  MOUNTPOINT\r\nserviTank                  137G  3.38T    96K  /serviTank\r\nserviTank/BU              95.8G  3.38T    96K  none\r\nserviTank/BU/subi         95.8G  3.38T    96K  none\r\nserviTank/BU/subi/Nixos   95.8G  3.38T  95.8G  legacy\r\nserviTank/encZFS          40.8G  3.38T  1.39M  none\r\nserviTank/encZFS/BU       2.78M  3.38T  1.39M  none\r\nserviTank/encZFS/BU/subi  1.39M  3.38T  1.39M  none\r\nserviTank/encZFS/Nixos    40.8G  3.38T  5.64G  legacy\r\n```\r\n\r\nHowever the zfs send/receive command never \"finishes\" and on the server side dmesg shows spl_panic\r\n\r\n```\r\n[ 1556.014734] VERIFY3(0 == dmu_object_dirty_raw(os, object, tx)) failed (0 == 17)\r\n[ 1556.014757] PANIC at dmu.c:937:dmu_free_long_object_impl()\r\n[ 1556.014770] Showing stack for process 18808\r\n[ 1556.014772] CPU: 5 PID: 18808 Comm: receive_writer Tainted: P           O    4.9.58 #1-NixOS\r\n[ 1556.014772] Hardware name: To Be Filled By O.E.M. To Be Filled By O.E.M./Z97 Pro4, BIOS P2.50 05/27/2016\r\n[ 1556.014773]  ffffaf4e11137b38 ffffffff942f7a12 ffffffffc02c4e03 00000000000003a9\r\n[ 1556.014775]  ffffaf4e11137b48 ffffffffc00759f2 ffffaf4e11137cd0 ffffffffc0075ac5\r\n[ 1556.014776]  0000000000000000 ffffaf4e00000030 ffffaf4e11137ce0 ffffaf4e11137c80\r\n[ 1556.014777] Call Trace:\r\n[ 1556.014781]  [<ffffffff942f7a12>] dump_stack+0x63/0x81\r\n[ 1556.014785]  [<ffffffffc00759f2>] spl_dumpstack+0x42/0x50 [spl]\r\n[ 1556.014787]  [<ffffffffc0075ac5>] spl_panic+0xc5/0x100 [spl]\r\n[ 1556.014806]  [<ffffffffc016ec26>] ? dbuf_rele+0x36/0x40 [zfs]\r\n[ 1556.014816]  [<ffffffffc0190107>] ? dnode_hold_impl+0xb57/0xc40 [zfs]\r\n[ 1556.014825]  [<ffffffffc0190443>] ? dnode_setdirty+0x83/0x100 [zfs]\r\n[ 1556.014826]  [<ffffffff945671e2>] ? mutex_lock+0x12/0x30\r\n[ 1556.014839]  [<ffffffffc01bf84b>] ? multilist_sublist_unlock+0x2b/0x40 [zfs]\r\n[ 1556.014848]  [<ffffffffc019020b>] ? dnode_hold+0x1b/0x20 [zfs]\r\n[ 1556.014857]  [<ffffffffc017aa7a>] dmu_free_long_object_impl.part.11+0xba/0xf0 [zfs]\r\n[ 1556.014865]  [<ffffffffc017ab24>] dmu_free_long_object_raw+0x34/0x40 [zfs]\r\n[ 1556.014873]  [<ffffffffc0187858>] receive_freeobjects.isra.11+0x58/0x110 [zfs]\r\n[ 1556.014881]  [<ffffffffc0187cb5>] receive_writer_thread+0x3a5/0xd50 [zfs]\r\n[ 1556.014883]  [<ffffffff941ce021>] ? __slab_free+0xa1/0x2e0\r\n[ 1556.014884]  [<ffffffff940a5200>] ? set_next_entity+0x70/0x890\r\n[ 1556.014886]  [<ffffffffc006ff53>] ? spl_kmem_free+0x33/0x40 [spl]\r\n[ 1556.014887]  [<ffffffffc00725d0>] ? __thread_exit+0x20/0x20 [spl]\r\n[ 1556.014895]  [<ffffffffc0187910>] ? receive_freeobjects.isra.11+0x110/0x110 [zfs]\r\n[ 1556.014896]  [<ffffffffc00725d0>] ? __thread_exit+0x20/0x20 [spl]\r\n[ 1556.014898]  [<ffffffffc0072642>] thread_generic_wrapper+0x72/0x80 [spl]\r\n[ 1556.014899]  [<ffffffff9408e457>] kthread+0xd7/0xf0\r\n[ 1556.014899]  [<ffffffff9408e380>] ? kthread_park+0x60/0x60\r\n[ 1556.014901]  [<ffffffff9456a155>] ret_from_fork+0x25/0x30\r\n[ 1721.304223] INFO: task txg_quiesce:468 blocked for more than 120 seconds.\r\n[ 1721.304317]       Tainted: P           O    4.9.58 #1-NixOS\r\n[ 1721.304376] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1721.304456] txg_quiesce     D    0   468      2 0x00000000\r\n[ 1721.304463]  ffff92d789a44400 0000000000000000 ffff92d7afbd7ec0 ffff92d783539a80\r\n[ 1721.304469]  ffff92d78c355cc0 ffffaf4e10e1fd30 ffffffff94565082 ffffaf4e10e1fd00\r\n[ 1721.304474]  0000000000000246 0000000180200010 ffffaf4e10e1fd50 ffff92d783539a80\r\n[ 1721.304479] Call Trace:\r\n[ 1721.304493]  [<ffffffff94565082>] ? __schedule+0x192/0x660\r\n[ 1721.304500]  [<ffffffff94565586>] schedule+0x36/0x80\r\n[ 1721.304511]  [<ffffffffc0077cb8>] cv_wait_common+0x128/0x140 [spl]\r\n[ 1721.304518]  [<ffffffff940ad390>] ? wake_atomic_t_function+0x60/0x60\r\n[ 1721.304525]  [<ffffffffc0077ce5>] __cv_wait+0x15/0x20 [spl]\r\n[ 1721.304591]  [<ffffffffc01de633>] txg_quiesce_thread+0x2e3/0x3f0 [zfs]\r\n[ 1721.304640]  [<ffffffffc01de350>] ? txg_wait_open+0x100/0x100 [zfs]\r\n[ 1721.304647]  [<ffffffffc00725d0>] ? __thread_exit+0x20/0x20 [spl]\r\n[ 1721.304654]  [<ffffffffc0072642>] thread_generic_wrapper+0x72/0x80 [spl]\r\n[ 1721.304658]  [<ffffffff9408e457>] kthread+0xd7/0xf0\r\n[ 1721.304662]  [<ffffffff9408e380>] ? kthread_park+0x60/0x60\r\n[ 1721.304664]  [<ffffffff9408e380>] ? kthread_park+0x60/0x60\r\n[ 1721.304669]  [<ffffffff9456a155>] ret_from_fork+0x25/0x30\r\n[ 1721.304686] INFO: task zfs:15048 blocked for more than 120 seconds.\r\n[ 1721.304753]       Tainted: P           O    4.9.58 #1-NixOS\r\n[ 1721.304810] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1721.304890] zfs             D    0 15048  15040 0x00000000\r\n[ 1721.304894]  ffff92d7617f0400 0000000000000000 ffff92d7afb57ec0 ffff92d78bf7cf80\r\n[ 1721.304900]  ffff92d78c354240 ffffaf4e032438c8 ffffffff94565082 ffffffffc016ec26\r\n[ 1721.304904]  ffff92d68c3e9730 0000000000000001 ffffaf4e032438d0 ffff92d78bf7cf80\r\n[ 1721.304909] Call Trace:\r\n[ 1721.304916]  [<ffffffff94565082>] ? __schedule+0x192/0x660\r\n[ 1721.304953]  [<ffffffffc016ec26>] ? dbuf_rele+0x36/0x40 [zfs]\r\n[ 1721.304959]  [<ffffffff94565586>] schedule+0x36/0x80\r\n[ 1721.304967]  [<ffffffffc0077cb8>] cv_wait_common+0x128/0x140 [spl]\r\n[ 1721.304972]  [<ffffffff940ad390>] ? wake_atomic_t_function+0x60/0x60\r\n[ 1721.304979]  [<ffffffffc0077ce5>] __cv_wait+0x15/0x20 [spl]\r\n[ 1721.305015]  [<ffffffffc0173f92>] bqueue_enqueue+0x62/0xe0 [zfs]\r\n[ 1721.305059]  [<ffffffffc01898c1>] dmu_recv_stream+0x691/0x11c0 [zfs]\r\n[ 1721.305066]  [<ffffffffc009062a>] ? nv_mem_zalloc.isra.12+0x2a/0x40 [znvpair]\r\n[ 1721.305116]  [<ffffffffc02108fa>] ? zfs_set_prop_nvlist+0x2fa/0x510 [zfs]\r\n[ 1721.305190]  [<ffffffffc0211057>] zfs_ioc_recv_impl+0x407/0x1170 [zfs]\r\n[ 1721.305241]  [<ffffffffc02123f9>] zfs_ioc_recv_new+0x369/0x400 [zfs]\r\n[ 1721.305254]  [<ffffffffc00702cc>] ? spl_kmem_alloc_impl+0x9c/0x180 [spl]\r\n[ 1721.305263]  [<ffffffffc00724a9>] ? spl_vmem_alloc+0x19/0x20 [spl]\r\n[ 1721.305270]  [<ffffffffc00958af>] ? nv_alloc_sleep_spl+0x1f/0x30 [znvpair]\r\n[ 1721.305276]  [<ffffffffc009062a>] ? nv_mem_zalloc.isra.12+0x2a/0x40 [znvpair]\r\n[ 1721.305283]  [<ffffffffc00906ff>] ? nvlist_xalloc.part.13+0x5f/0xc0 [znvpair]\r\n[ 1721.305330]  [<ffffffffc020f0eb>] zfsdev_ioctl+0x20b/0x660 [zfs]\r\n[ 1721.305340]  [<ffffffff941ff604>] do_vfs_ioctl+0x94/0x5c0\r\n[ 1721.305347]  [<ffffffff9405dece>] ? __do_page_fault+0x25e/0x4c0\r\n[ 1721.305352]  [<ffffffff941ffba9>] SyS_ioctl+0x79/0x90\r\n[ 1721.305359]  [<ffffffff94569ef7>] entry_SYSCALL_64_fastpath+0x1a/0xa9\r\n[ 1721.305366] INFO: task receive_writer:18808 blocked for more than 120 seconds.\r\n[ 1721.305442]       Tainted: P           O    4.9.58 #1-NixOS\r\n[ 1721.305500] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1721.305637] receive_writer  D    0 18808      2 0x00000000\r\n[ 1721.305644]  ffff92d789a44400 0000000000000000 ffff92d7afb57ec0 ffff92d78bf7ea00\r\n[ 1721.305654]  ffff92d78bf78000 ffffaf4e11137b30 ffffffff94565082 0000000000000000\r\n[ 1721.305662]  ffffffffc02af5d0 00ffffffc02c51d0 0000000000000001 ffff92d78bf7ea00\r\n[ 1721.305671] Call Trace:\r\n[ 1721.305681]  [<ffffffff94565082>] ? __schedule+0x192/0x660\r\n[ 1721.305691]  [<ffffffff94565586>] schedule+0x36/0x80\r\n[ 1721.305703]  [<ffffffffc0075aeb>] spl_panic+0xeb/0x100 [spl]\r\n[ 1721.305765]  [<ffffffffc016ec26>] ? dbuf_rele+0x36/0x40 [zfs]\r\n[ 1721.305821]  [<ffffffffc0190107>] ? dnode_hold_impl+0xb57/0xc40 [zfs]\r\n[ 1721.305873]  [<ffffffffc0190443>] ? dnode_setdirty+0x83/0x100 [zfs]\r\n[ 1721.305879]  [<ffffffff945671e2>] ? mutex_lock+0x12/0x30\r\n[ 1721.305943]  [<ffffffffc01bf84b>] ? multilist_sublist_unlock+0x2b/0x40 [zfs]\r\n[ 1721.305997]  [<ffffffffc019020b>] ? dnode_hold+0x1b/0x20 [zfs]\r\n[ 1721.306051]  [<ffffffffc017aa7a>] dmu_free_long_object_impl.part.11+0xba/0xf0 [zfs]\r\n[ 1721.306102]  [<ffffffffc017ab24>] dmu_free_long_object_raw+0x34/0x40 [zfs]\r\n[ 1721.306147]  [<ffffffffc0187858>] receive_freeobjects.isra.11+0x58/0x110 [zfs]\r\n[ 1721.306207]  [<ffffffffc0187cb5>] receive_writer_thread+0x3a5/0xd50 [zfs]\r\n[ 1721.306214]  [<ffffffff941ce021>] ? __slab_free+0xa1/0x2e0\r\n[ 1721.306221]  [<ffffffff940a5200>] ? set_next_entity+0x70/0x890\r\n[ 1721.306231]  [<ffffffffc006ff53>] ? spl_kmem_free+0x33/0x40 [spl]\r\n[ 1721.306244]  [<ffffffffc00725d0>] ? __thread_exit+0x20/0x20 [spl]\r\n[ 1721.306288]  [<ffffffffc0187910>] ? receive_freeobjects.isra.11+0x110/0x110 [zfs]\r\n[ 1721.306296]  [<ffffffffc00725d0>] ? __thread_exit+0x20/0x20 [spl]\r\n[ 1721.306303]  [<ffffffffc0072642>] thread_generic_wrapper+0x72/0x80 [spl]\r\n[ 1721.306307]  [<ffffffff9408e457>] kthread+0xd7/0xf0\r\n[ 1721.306316]  [<ffffffff9408e380>] ? kthread_park+0x60/0x60\r\n[ 1721.306323]  [<ffffffff9456a155>] ret_from_fork+0x25/0x30\r\n[ 1844.182739] INFO: task txg_quiesce:468 blocked for more than 120 seconds.\r\n[ 1844.182831]       Tainted: P           O    4.9.58 #1-NixOS\r\n[ 1844.182889] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1844.182967] txg_quiesce     D    0   468      2 0x00000000\r\n[ 1844.182974]  ffff92d789a44400 0000000000000000 ffff92d7afbd7ec0 ffff92d783539a80\r\n[ 1844.182980]  ffff92d78c355cc0 ffffaf4e10e1fd30 ffffffff94565082 ffffaf4e10e1fd00\r\n[ 1844.182985]  0000000000000246 0000000180200010 ffffaf4e10e1fd50 ffff92d783539a80\r\n[ 1844.182990] Call Trace:\r\n[ 1844.183003]  [<ffffffff94565082>] ? __schedule+0x192/0x660\r\n[ 1844.183009]  [<ffffffff94565586>] schedule+0x36/0x80\r\n[ 1844.183019]  [<ffffffffc0077cb8>] cv_wait_common+0x128/0x140 [spl]\r\n[ 1844.183026]  [<ffffffff940ad390>] ? wake_atomic_t_function+0x60/0x60\r\n[ 1844.183033]  [<ffffffffc0077ce5>] __cv_wait+0x15/0x20 [spl]\r\n[ 1844.183094]  [<ffffffffc01de633>] txg_quiesce_thread+0x2e3/0x3f0 [zfs]\r\n[ 1844.183144]  [<ffffffffc01de350>] ? txg_wait_open+0x100/0x100 [zfs]\r\n[ 1844.183151]  [<ffffffffc00725d0>] ? __thread_exit+0x20/0x20 [spl]\r\n[ 1844.183157]  [<ffffffffc0072642>] thread_generic_wrapper+0x72/0x80 [spl]\r\n[ 1844.183162]  [<ffffffff9408e457>] kthread+0xd7/0xf0\r\n[ 1844.183165]  [<ffffffff9408e380>] ? kthread_park+0x60/0x60\r\n[ 1844.183168]  [<ffffffff9408e380>] ? kthread_park+0x60/0x60\r\n[ 1844.183172]  [<ffffffff9456a155>] ret_from_fork+0x25/0x30\r\n[ 1844.183190] INFO: task zfs:15048 blocked for more than 120 seconds.\r\n[ 1844.183255]       Tainted: P           O    4.9.58 #1-NixOS\r\n[ 1844.183312] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1844.183392] zfs             D    0 15048  15040 0x00000000\r\n[ 1844.183397]  ffff92d7617f0400 0000000000000000 ffff92d7afb57ec0 ffff92d78bf7cf80\r\n[ 1844.183402]  ffff92d78c354240 ffffaf4e032438c8 ffffffff94565082 ffffffffc016ec26\r\n[ 1844.183407]  ffff92d68c3e9730 0000000000000001 ffffaf4e032438d0 ffff92d78bf7cf80\r\n[ 1844.183411] Call Trace:\r\n[ 1844.183418]  [<ffffffff94565082>] ? __schedule+0x192/0x660\r\n[ 1844.183455]  [<ffffffffc016ec26>] ? dbuf_rele+0x36/0x40 [zfs]\r\n[ 1844.183461]  [<ffffffff94565586>] schedule+0x36/0x80\r\n[ 1844.183468]  [<ffffffffc0077cb8>] cv_wait_common+0x128/0x140 [spl]\r\n[ 1844.183473]  [<ffffffff940ad390>] ? wake_atomic_t_function+0x60/0x60\r\n[ 1844.183480]  [<ffffffffc0077ce5>] __cv_wait+0x15/0x20 [spl]\r\n[ 1844.183516]  [<ffffffffc0173f92>] bqueue_enqueue+0x62/0xe0 [zfs]\r\n[ 1844.183560]  [<ffffffffc01898c1>] dmu_recv_stream+0x691/0x11c0 [zfs]\r\n[ 1844.183567]  [<ffffffffc009062a>] ? nv_mem_zalloc.isra.12+0x2a/0x40 [znvpair]\r\n[ 1844.183617]  [<ffffffffc02108fa>] ? zfs_set_prop_nvlist+0x2fa/0x510 [zfs]\r\n[ 1844.183663]  [<ffffffffc0211057>] zfs_ioc_recv_impl+0x407/0x1170 [zfs]\r\n[ 1844.183731]  [<ffffffffc02123f9>] zfs_ioc_recv_new+0x369/0x400 [zfs]\r\n[ 1844.183744]  [<ffffffffc00702cc>] ? spl_kmem_alloc_impl+0x9c/0x180 [spl]\r\n[ 1844.183751]  [<ffffffffc00724a9>] ? spl_vmem_alloc+0x19/0x20 [spl]\r\n[ 1844.183758]  [<ffffffffc00958af>] ? nv_alloc_sleep_spl+0x1f/0x30 [znvpair]\r\n[ 1844.183764]  [<ffffffffc009062a>] ? nv_mem_zalloc.isra.12+0x2a/0x40 [znvpair]\r\n[ 1844.183770]  [<ffffffffc00906ff>] ? nvlist_xalloc.part.13+0x5f/0xc0 [znvpair]\r\n[ 1844.183819]  [<ffffffffc020f0eb>] zfsdev_ioctl+0x20b/0x660 [zfs]\r\n[ 1844.183828]  [<ffffffff941ff604>] do_vfs_ioctl+0x94/0x5c0\r\n[ 1844.183835]  [<ffffffff9405dece>] ? __do_page_fault+0x25e/0x4c0\r\n[ 1844.183841]  [<ffffffff941ffba9>] SyS_ioctl+0x79/0x90\r\n[ 1844.183846]  [<ffffffff94569ef7>] entry_SYSCALL_64_fastpath+0x1a/0xa9\r\n[ 1844.183852] INFO: task receive_writer:18808 blocked for more than 120 seconds.\r\n[ 1844.183929]       Tainted: P           O    4.9.58 #1-NixOS\r\n[ 1844.183987] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1844.184067] receive_writer  D    0 18808      2 0x00000000\r\n[ 1844.184071]  ffff92d789a44400 0000000000000000 ffff92d7afb57ec0 ffff92d78bf7ea00\r\n[ 1844.184076]  ffff92d78bf78000 ffffaf4e11137b30 ffffffff94565082 0000000000000000\r\n[ 1844.184082]  ffffffffc02af5d0 00ffffffc02c51d0 0000000000000001 ffff92d78bf7ea00\r\n[ 1844.184088] Call Trace:\r\n[ 1844.184095]  [<ffffffff94565082>] ? __schedule+0x192/0x660\r\n[ 1844.184104]  [<ffffffff94565586>] schedule+0x36/0x80\r\n[ 1844.184112]  [<ffffffffc0075aeb>] spl_panic+0xeb/0x100 [spl]\r\n[ 1844.184149]  [<ffffffffc016ec26>] ? dbuf_rele+0x36/0x40 [zfs]\r\n[ 1844.184196]  [<ffffffffc0190107>] ? dnode_hold_impl+0xb57/0xc40 [zfs]\r\n[ 1844.184256]  [<ffffffffc0190443>] ? dnode_setdirty+0x83/0x100 [zfs]\r\n[ 1844.184262]  [<ffffffff945671e2>] ? mutex_lock+0x12/0x30\r\n[ 1844.184334]  [<ffffffffc01bf84b>] ? multilist_sublist_unlock+0x2b/0x40 [zfs]\r\n[ 1844.184391]  [<ffffffffc019020b>] ? dnode_hold+0x1b/0x20 [zfs]\r\n[ 1844.184446]  [<ffffffffc017aa7a>] dmu_free_long_object_impl.part.11+0xba/0xf0 [zfs]\r\n[ 1844.184498]  [<ffffffffc017ab24>] dmu_free_long_object_raw+0x34/0x40 [zfs]\r\n[ 1844.184557]  [<ffffffffc0187858>] receive_freeobjects.isra.11+0x58/0x110 [zfs]\r\n[ 1844.184607]  [<ffffffffc0187cb5>] receive_writer_thread+0x3a5/0xd50 [zfs]\r\n[ 1844.184616]  [<ffffffff941ce021>] ? __slab_free+0xa1/0x2e0\r\n[ 1844.184622]  [<ffffffff940a5200>] ? set_next_entity+0x70/0x890\r\n[ 1844.184630]  [<ffffffffc006ff53>] ? spl_kmem_free+0x33/0x40 [spl]\r\n[ 1844.184642]  [<ffffffffc00725d0>] ? __thread_exit+0x20/0x20 [spl]\r\n[ 1844.184705]  [<ffffffffc0187910>] ? receive_freeobjects.isra.11+0x110/0x110 [zfs]\r\n[ 1844.184714]  [<ffffffffc00725d0>] ? __thread_exit+0x20/0x20 [spl]\r\n[ 1844.184719]  [<ffffffffc0072642>] thread_generic_wrapper+0x72/0x80 [spl]\r\n[ 1844.184724]  [<ffffffff9408e457>] kthread+0xd7/0xf0\r\n[ 1844.184730]  [<ffffffff9408e380>] ? kthread_park+0x60/0x60\r\n[ 1844.184736]  [<ffffffff9456a155>] ret_from_fork+0x25/0x30\r\n[ 1967.061258] INFO: task txg_quiesce:468 blocked for more than 120 seconds.\r\n[ 1967.061345]       Tainted: P           O    4.9.58 #1-NixOS\r\n[ 1967.061403] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1967.061483] txg_quiesce     D    0   468      2 0x00000000\r\n[ 1967.061490]  ffff92d789a44400 0000000000000000 ffff92d7afbd7ec0 ffff92d783539a80\r\n[ 1967.061496]  ffff92d78c355cc0 ffffaf4e10e1fd30 ffffffff94565082 ffffaf4e10e1fd00\r\n[ 1967.061501]  0000000000000246 0000000180200010 ffffaf4e10e1fd50 ffff92d783539a80\r\n[ 1967.061507] Call Trace:\r\n[ 1967.061521]  [<ffffffff94565082>] ? __schedule+0x192/0x660\r\n[ 1967.061528]  [<ffffffff94565586>] schedule+0x36/0x80\r\n[ 1967.061539]  [<ffffffffc0077cb8>] cv_wait_common+0x128/0x140 [spl]\r\n[ 1967.061546]  [<ffffffff940ad390>] ? wake_atomic_t_function+0x60/0x60\r\n[ 1967.061554]  [<ffffffffc0077ce5>] __cv_wait+0x15/0x20 [spl]\r\n[ 1967.061638]  [<ffffffffc01de633>] txg_quiesce_thread+0x2e3/0x3f0 [zfs]\r\n[ 1967.061690]  [<ffffffffc01de350>] ? txg_wait_open+0x100/0x100 [zfs]\r\n[ 1967.061697]  [<ffffffffc00725d0>] ? __thread_exit+0x20/0x20 [spl]\r\n[ 1967.061704]  [<ffffffffc0072642>] thread_generic_wrapper+0x72/0x80 [spl]\r\n[ 1967.061708]  [<ffffffff9408e457>] kthread+0xd7/0xf0\r\n[ 1967.061712]  [<ffffffff9408e380>] ? kthread_park+0x60/0x60\r\n[ 1967.061714]  [<ffffffff9408e380>] ? kthread_park+0x60/0x60\r\n[ 1967.061719]  [<ffffffff9456a155>] ret_from_fork+0x25/0x30\r\n[ 1967.061738] INFO: task zfs:15048 blocked for more than 120 seconds.\r\n[ 1967.061804]       Tainted: P           O    4.9.58 #1-NixOS\r\n[ 1967.061862] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1967.061942] zfs             D    0 15048  15040 0x00000000\r\n[ 1967.061947]  ffff92d7617f0400 0000000000000000 ffff92d7afb57ec0 ffff92d78bf7cf80\r\n[ 1967.061952]  ffff92d78c354240 ffffaf4e032438c8 ffffffff94565082 ffffffffc016ec26\r\n[ 1967.061957]  ffff92d68c3e9730 0000000000000001 ffffaf4e032438d0 ffff92d78bf7cf80\r\n[ 1967.061961] Call Trace:\r\n[ 1967.061968]  [<ffffffff94565082>] ? __schedule+0x192/0x660\r\n[ 1967.062006]  [<ffffffffc016ec26>] ? dbuf_rele+0x36/0x40 [zfs]\r\n[ 1967.062012]  [<ffffffff94565586>] schedule+0x36/0x80\r\n[ 1967.062020]  [<ffffffffc0077cb8>] cv_wait_common+0x128/0x140 [spl]\r\n[ 1967.062024]  [<ffffffff940ad390>] ? wake_atomic_t_function+0x60/0x60\r\n[ 1967.062032]  [<ffffffffc0077ce5>] __cv_wait+0x15/0x20 [spl]\r\n[ 1967.062068]  [<ffffffffc0173f92>] bqueue_enqueue+0x62/0xe0 [zfs]\r\n[ 1967.062112]  [<ffffffffc01898c1>] dmu_recv_stream+0x691/0x11c0 [zfs]\r\n[ 1967.062119]  [<ffffffffc009062a>] ? nv_mem_zalloc.isra.12+0x2a/0x40 [znvpair]\r\n[ 1967.062170]  [<ffffffffc02108fa>] ? zfs_set_prop_nvlist+0x2fa/0x510 [zfs]\r\n[ 1967.062243]  [<ffffffffc0211057>] zfs_ioc_recv_impl+0x407/0x1170 [zfs]\r\n[ 1967.062293]  [<ffffffffc02123f9>] zfs_ioc_recv_new+0x369/0x400 [zfs]\r\n[ 1967.062306]  [<ffffffffc00702cc>] ? spl_kmem_alloc_impl+0x9c/0x180 [spl]\r\n[ 1967.062315]  [<ffffffffc00724a9>] ? spl_vmem_alloc+0x19/0x20 [spl]\r\n[ 1967.062322]  [<ffffffffc00958af>] ? nv_alloc_sleep_spl+0x1f/0x30 [znvpair]\r\n[ 1967.062328]  [<ffffffffc009062a>] ? nv_mem_zalloc.isra.12+0x2a/0x40 [znvpair]\r\n[ 1967.062335]  [<ffffffffc00906ff>] ? nvlist_xalloc.part.13+0x5f/0xc0 [znvpair]\r\n[ 1967.062381]  [<ffffffffc020f0eb>] zfsdev_ioctl+0x20b/0x660 [zfs]\r\n[ 1967.062391]  [<ffffffff941ff604>] do_vfs_ioctl+0x94/0x5c0\r\n[ 1967.062398]  [<ffffffff9405dece>] ? __do_page_fault+0x25e/0x4c0\r\n[ 1967.062403]  [<ffffffff941ffba9>] SyS_ioctl+0x79/0x90\r\n[ 1967.062410]  [<ffffffff94569ef7>] entry_SYSCALL_64_fastpath+0x1a/0xa9\r\n[ 1967.062418] INFO: task receive_writer:18808 blocked for more than 120 seconds.\r\n[ 1967.062494]       Tainted: P           O    4.9.58 #1-NixOS\r\n[ 1967.062552] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1967.062632] receive_writer  D    0 18808      2 0x00000000\r\n[ 1967.062637]  ffff92d789a44400 0000000000000000 ffff92d7afb57ec0 ffff92d78bf7ea00\r\n[ 1967.062645]  ffff92d78bf78000 ffffaf4e11137b30 ffffffff94565082 0000000000000000\r\n[ 1967.062649]  ffffffffc02af5d0 00ffffffc02c51d0 0000000000000001 ffff92d78bf7ea00\r\n[ 1967.062654] Call Trace:\r\n[ 1967.062661]  [<ffffffff94565082>] ? __schedule+0x192/0x660\r\n[ 1967.062667]  [<ffffffff94565586>] schedule+0x36/0x80\r\n[ 1967.062677]  [<ffffffffc0075aeb>] spl_panic+0xeb/0x100 [spl]\r\n[ 1967.062714]  [<ffffffffc016ec26>] ? dbuf_rele+0x36/0x40 [zfs]\r\n[ 1967.062760]  [<ffffffffc0190107>] ? dnode_hold_impl+0xb57/0xc40 [zfs]\r\n[ 1967.062804]  [<ffffffffc0190443>] ? dnode_setdirty+0x83/0x100 [zfs]\r\n[ 1967.062813]  [<ffffffff945671e2>] ? mutex_lock+0x12/0x30\r\n[ 1967.062870]  [<ffffffffc01bf84b>] ? multilist_sublist_unlock+0x2b/0x40 [zfs]\r\n[ 1967.062913]  [<ffffffffc019020b>] ? dnode_hold+0x1b/0x20 [zfs]\r\n[ 1967.062955]  [<ffffffffc017aa7a>] dmu_free_long_object_impl.part.11+0xba/0xf0 [zfs]\r\n[ 1967.062995]  [<ffffffffc017ab24>] dmu_free_long_object_raw+0x34/0x40 [zfs]\r\n[ 1967.063038]  [<ffffffffc0187858>] receive_freeobjects.isra.11+0x58/0x110 [zfs]\r\n[ 1967.063077]  [<ffffffffc0187cb5>] receive_writer_thread+0x3a5/0xd50 [zfs]\r\n[ 1967.063085]  [<ffffffff941ce021>] ? __slab_free+0xa1/0x2e0\r\n[ 1967.063092]  [<ffffffff940a5200>] ? set_next_entity+0x70/0x890\r\n[ 1967.063099]  [<ffffffffc006ff53>] ? spl_kmem_free+0x33/0x40 [spl]\r\n[ 1967.063105]  [<ffffffffc00725d0>] ? __thread_exit+0x20/0x20 [spl]\r\n[ 1967.063145]  [<ffffffffc0187910>] ? receive_freeobjects.isra.11+0x110/0x110 [zfs]\r\n[ 1967.063153]  [<ffffffffc00725d0>] ? __thread_exit+0x20/0x20 [spl]\r\n[ 1967.063168]  [<ffffffffc0072642>] thread_generic_wrapper+0x72/0x80 [spl]\r\n[ 1967.063174]  [<ffffffff9408e457>] kthread+0xd7/0xf0\r\n[ 1967.063180]  [<ffffffff9408e380>] ? kthread_park+0x60/0x60\r\n[ 1967.063187]  [<ffffffff9456a155>] ret_from_fork+0x25/0x30\r\n[ 2089.939773] INFO: task txg_quiesce:468 blocked for more than 120 seconds.\r\n[ 2089.939861]       Tainted: P           O    4.9.58 #1-NixOS\r\n[ 2089.939919] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 2089.940000] txg_quiesce     D    0   468      2 0x00000000\r\n[ 2089.940007]  ffff92d789a44400 0000000000000000 ffff92d7afbd7ec0 ffff92d783539a80\r\n[ 2089.940013]  ffff92d78c355cc0 ffffaf4e10e1fd30 ffffffff94565082 ffffaf4e10e1fd00\r\n[ 2089.940018]  0000000000000246 0000000180200010 ffffaf4e10e1fd50 ffff92d783539a80\r\n[ 2089.940023] Call Trace:\r\n[ 2089.940037]  [<ffffffff94565082>] ? __schedule+0x192/0x660\r\n[ 2089.940043]  [<ffffffff94565586>] schedule+0x36/0x80\r\n[ 2089.940054]  [<ffffffffc0077cb8>] cv_wait_common+0x128/0x140 [spl]\r\n[ 2089.940061]  [<ffffffff940ad390>] ? wake_atomic_t_function+0x60/0x60\r\n[ 2089.940068]  [<ffffffffc0077ce5>] __cv_wait+0x15/0x20 [spl]\r\n[ 2089.940132]  [<ffffffffc01de633>] txg_quiesce_thread+0x2e3/0x3f0 [zfs]\r\n[ 2089.940180]  [<ffffffffc01de350>] ? txg_wait_open+0x100/0x100 [zfs]\r\n[ 2089.940188]  [<ffffffffc00725d0>] ? __thread_exit+0x20/0x20 [spl]\r\n[ 2089.940194]  [<ffffffffc0072642>] thread_generic_wrapper+0x72/0x80 [spl]\r\n[ 2089.940199]  [<ffffffff9408e457>] kthread+0xd7/0xf0\r\n[ 2089.940202]  [<ffffffff9408e380>] ? kthread_park+0x60/0x60\r\n[ 2089.940205]  [<ffffffff9408e380>] ? kthread_park+0x60/0x60\r\n```\r\n\r\nAs said, if I don't use the `-R` option on first dataset sending to server it works fine, but when I then try to send an incremental snapshot the same thing happens.\r\n\r\nI also tried to send the snapshots to an encrypted child dataset on the server with the same results.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6821/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6793", "title": "zfs receive -F cannot be used to destroy an encrypted filesystem", "body": "## System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Nixos\r\nDistribution Version    | Unstable Small\r\nLinux Kernel                 | 4.9.58\r\nArchitecture                 | x64\r\nZFS Version                  | 0.7.0-1\r\nSPL Version                  | 0.7.0-1\r\n\r\n### Describe the problem you're observing\r\n\r\nUsing an encrypted dataset with several child sets on my notebook and homeserver. I wanted to setup automatic snapshot backup services to my homeset using znapzend.\r\n\r\nHowever it complains about:\r\n\r\n`cannot receive new filesystem stream: zfs receive -F cannot be used to destroy an encrypted filesystem`\r\n\r\n### Describe how to reproduce the problem\r\n\r\nI setup the rules for the first dataset for testing:\r\n\r\n```\r\n#!/usr/bin/env bash\r\n\r\nznapzendzetup create                                                           \\\r\n    --recursive                                                                \\\r\n    --tsformat='%Y-%m-%d_%H-%M-%S'                                             \\\r\n    SRC '1h=>15min,1d=>1h,7d=>1d' tank/encZFS/VMs                              \\\r\n    DST:notebook '1h=>15min,1d=>1h,30d=>1d' root@10.200.0.3:serviTank/encZFS/BU/subi/VMs\r\n```\r\nand then I run `znapzend --runonce=tank/encZFS/VMs -d --autoCreation` \r\n\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\nThat's the log output I got:\r\n\r\n```\r\nroot@subi:~/.nixos# znapzend --runonce=tank/encZFS/VMs -d --autoCreation\r\n[Fri Oct 27 15:33:56 2017] [info] znapzend (PID=16948) starting up ...\r\n[Fri Oct 27 15:33:56 2017] [info] refreshing backup plans...\r\n[Fri Oct 27 15:33:57 2017] [info] found a valid backup plan for tank/encZFS/VMs...\r\n[Fri Oct 27 15:33:57 2017] [info] znapzend (PID=16948) initialized -- resuming normal operations.\r\n[Fri Oct 27 15:33:57 2017] [debug] snapshot worker for tank/encZFS/VMs spawned (17097)\r\n[Fri Oct 27 15:33:57 2017] [info] creating recursive snapshot on tank/encZFS/VMs\r\n# zfs snapshot -r tank/encZFS/VMs@2017-10-27_15-33-57\r\n[Fri Oct 27 15:34:03 2017] [debug] snapshot worker for tank/encZFS/VMs done (17097)\r\n[Fri Oct 27 15:34:03 2017] [debug] send/receive worker for tank/encZFS/VMs spawned (18065)\r\n[Fri Oct 27 15:34:03 2017] [info] starting work on backupSet tank/encZFS/VMs\r\n# zfs list -H -r -o name -t filesystem,volume tank/encZFS/VMs\r\n[Fri Oct 27 15:34:03 2017] [debug] sending snapshots from tank/encZFS/VMs to root@10.200.0.3:serviTank/encZFS/BU/subi/VMs\r\n# zfs list -H -o name -t snapshot -s creation -d 1 tank/encZFS/VMs\r\n# ssh -o batchMode=yes -o ConnectTimeout=30 root@10.200.0.3 zfs list -H -o name -t snapshot -s creation -d 1 serviTank/encZFS/BU/subi/VMs\r\n# zfs send tank/encZFS/VMs@2017-10-27_15-33-57|ssh -o batchMode=yes -o ConnectTimeout=30 'root@10.200.0.3' 'zfs recv -F serviTank/encZFS/BU/subi/VMs'\r\ncannot receive new filesystem stream: zfs receive -F cannot be used to destroy an encrypted filesystem\r\nwarning: cannot send 'tank/encZFS/VMs@2017-10-27_15-33-57': signal received\r\n[Fri Oct 27 15:34:03 2017] [warn] ERROR: cannot send snapshots to serviTank/encZFS/BU/subi/VMs on root@10.200.0.3\r\n# ssh -o batchMode=yes -o ConnectTimeout=30 root@10.200.0.3 zfs list -H -o name -t snapshot -s creation -d 1 serviTank/encZFS/BU/subi/VMs\r\n[Fri Oct 27 15:34:03 2017] [debug] cleaning up snapshots on root@10.200.0.3:serviTank/encZFS/BU/subi/VMs\r\n[Fri Oct 27 15:34:03 2017] [warn] ERROR: suspending cleanup source dataset because at least one send task failed\r\n[Fri Oct 27 15:34:03 2017] [info] done with backupset tank/encZFS/VMs in 0 seconds\r\n[Fri Oct 27 15:34:03 2017] [debug] send/receive worker for tank/encZFS/VMs done (18065)\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6793/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "FireDrunk": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6823", "title": "Extreme load when deleting large file on full ZFS Filesystem", "body": "### System information\r\nDistribution Name       | Fedora\r\nDistribution Version    | 25 (Minimal)\r\nLinux Kernel                 | 4.12.13-200.fc25.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.2-1.fc25 (DKMS)\r\nSPL Version                  | 0.7.2-1.fc25 (DKMS)\r\n\r\nSystem Specifications:\r\nXeon E5-2620v3 (6-core)\r\n32GB ECC Registered RAM\r\nSupermicro X8SRH Motherboard\r\nSSD to boot Fedora from, unrelated\r\n6 * HGST 4K7000 7200RPM Harddisks in RAIDZ2\r\nDisks are attached to local SATA ports (not the onboard SAS ports)\r\nScrub has been done fairly recent (~40 days ago)\r\n\r\n### Describe the problem you're observing\r\nExtreme load, unresponsive system (regarding IO).\r\n\r\nI wrote a (sub)filesystem to 0b free doing a dd-backup of a large USB attached harddisk. The pool had >1.2T free, and the disk was ~500G, so I thought it shouldn't be a problem, because I also piped it through gzip.\r\n\r\nUnfortunately the filesystem became full anyway. To free up some space I tried deleting the target file, which took quite a long time (multiple minutes, for ~275G).\r\n\r\nDuring that time, the system load increased to > 60 and was barely responsive regarding IO. (the pool is not the root pool, my OS is on a separate SSD). The main pool consists of 6 * 4TB HGST 7200RPM harddrives configured in RAIDZ2.\r\n\r\nI noticed the sluggishness because I was installing an RPM in the meantime, which took quite some more time (10's of seconds instead of a few seconds for a 51MB rpm). When I started top, I noticed the load. After a few minutes the extreme load subsided, but the delete job has been running for more than 20 minutes now.\r\n\r\nThe zpool list output:\r\n```\r\narchive  21.8T  20.9T   847G         -    45%    96%  2.30x  ONLINE  -\r\n```\r\nThe zfs list output:\r\n```\r\nNAME                             USED  AVAIL  REFER  MOUNTPOINT\r\narchive                         14.5T   207G   352K  /archive\r\narchive/Backups                 1.44T   207G  1.44T  /archive/Backups\r\narchive/CrashPlanBackups         320K   207G   192K  /archive/CrashPlanBackups\r\narchive/Docker                  11.7G   207G  6.34G  /archive/Docker\r\narchive/Docker/Storage           216K   207G   216K  /archive/Docker/Storage\r\narchive/Documents               1.21G   207G  1.17G  /archive/Documents\r\narchive/Games                   20.0G   207G  20.0G  /archive/Games\r\narchive/Incoming                1.18T   207G  1.18T  /archive/Incoming\r\narchive/Movies                  5.38T   207G  5.38T  /archive/Movies\r\narchive/Music                   51.7G   207G  51.7G  /archive/Music\r\narchive/Programs                 303G   207G   259G  /archive/Programs\r\narchive/Scans                   17.2M   207G  17.0M  /archive/Scans\r\narchive/Series                  5.98T   207G  5.98T  /archive/Series\r\narchive/VM                      94.6G   207G  54.7G  /archive/VM\r\narchive/git                      340M   207G   339M  /archive/git\r\n```\r\n\r\n### Describe how to reproduce the problem\r\nI've no idea whether this is reproducible on other systems, but I can reproduce it by filling up my filesystem and deleting the file :-)\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nTop output snippet during the load > 60\r\n```\r\n  943 root       1 -19       0      0      0 S   5.0  0.0  38:33.67 z_wr_iss\r\n    2 root      20   0       0      0      0 S   4.6  0.0  48:24.43 kthreadd\r\n14944 root      20   0       0      0      0 D   3.6  0.0   0:00.63 kworker/u24:2\r\n  776 root       0 -20       0      0      0 S   3.0  0.0  26:47.64 spl_dynamic_tas\r\n  946 root       0 -20       0      0      0 S   2.0  0.0  19:34.87 z_wr_int_0\r\n  947 root       0 -20       0      0      0 S   2.0  0.0  19:35.06 z_wr_int_1\r\n  948 root       0 -20       0      0      0 S   2.0  0.0  19:35.84 z_wr_int_2\r\n  949 root       0 -20       0      0      0 S   2.0  0.0  19:34.68 z_wr_int_3\r\n  951 root       0 -20       0      0      0 S   2.0  0.0  19:35.51 z_wr_int_5\r\n  952 root       0 -20       0      0      0 S   2.0  0.0  19:35.26 z_wr_int_6\r\n  937 root       1 -19       0      0      0 S   1.7  0.0  38:34.74 z_wr_iss\r\n  941 root       1 -19       0      0      0 S   1.7  0.0  38:35.65 z_wr_iss\r\n  950 root       0 -20       0      0      0 S   1.7  0.0  19:35.21 z_wr_int_4\r\n  953 root       0 -20       0      0      0 S   1.7  0.0  19:34.90 z_wr_int_7\r\n  936 root       1 -19       0      0      0 S   1.3  0.0  38:35.36 z_wr_iss\r\n  938 root       1 -19       0      0      0 S   1.3  0.0  38:37.74 z_wr_iss\r\n  939 root       1 -19       0      0      0 S   1.3  0.0  38:37.94 z_wr_iss\r\n  940 root       1 -19       0      0      0 S   1.3  0.0  38:38.68 z_wr_iss\r\n  942 root       1 -19       0      0      0 S   1.3  0.0  38:35.38 z_wr_iss\r\n  944 root       1 -19       0      0      0 S   1.3  0.0  38:38.28 z_wr_iss\r\n```\r\nI double checked the free space, because this is the third time I've written this pool to 0b free (stupid me, I know). The weird thing is, that the zpool had > 1.2T free. But the filesystem only could write < 300g ( I understand that the zpool space is without parity calculations, but the difference is much more).\r\n\r\nSome caveats: The pool is 'rather' old, and has undergone some migrations.\r\nAnd, it's 49% fragmented, since is rather heavily used :-)\r\nCompression (LZ4) is enabled for all filesystems, dedup only for some specific filesystems. It IS enabled, for the target filesystem that was being written to.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6823/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gordan-bobic": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6820", "title": "SPL Build Failure on aarch64", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  CentOS\r\nDistribution Version    |  7\r\nLinux Kernel                 |  4.4.71\r\nArchitecture                 |  aarch64\r\nZFS Version                  |  0.7.3\r\nSPL Version                  |  0.7.3\r\n\r\n### Describe the problem you're observing\r\nSPL ./configure fails to guess the architecture. It has to be invoked with:\r\n./configure --build=aarch64-unknown-linux-gnu\r\n\r\nThat succeeds,. and the make step succeeds, but make rpms then fails with:\r\nchecking build system type... Invalid configuration `aarch64-redhat-linux-gnu': machine `aarch64-redhat' not recognized\r\n\r\nWhat fixes it is changing /usr/lib/rpm/macros entry from\r\n%_host aarch64-redhat-linux-gnu\r\nto\r\n%_host aarch64-unknown-linux-gnu\r\n\r\nThis works correctly on x86-64, and x86_64-redhat-linux-gnu seems to get recognized.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6820/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "youzhongyang": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6812", "title": "VERIFY3(0 == zap_remove_int(zfsvfs->z_os, zfsvfs->z_unlinkedobj, zp->z_id, tx)) failed (0 == 2)", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Ubuntu\r\nDistribution Version    |  17.04\r\nLinux Kernel                 |  4.13.0-16-generic\r\nArchitecture                 |  x86_64\r\nZFS Version                  |    0.7.2-1\r\nSPL Version                  |    0.7.2-1\r\n\r\n### Describe the problem you're observing\r\nspl_panic() causing zfs process to hang forever. 'reboot' also hung so it had to be power reset. This same issue also occurred on Centos 7.4.1708.\r\n\r\n### Describe how to reproduce the problem\r\nI don't know how to simply reproduce the issue. \r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n```\r\nroot     11549  2036  0 08:55 ?        00:00:00  \\_ /usr/sbin/CRON -f\r\nroot     11551 11549  0 08:55 ?        00:00:00      \\_ /bin/sh -c env PATH=/usr/sbin:/usr/bin:/bin:/sbin /opt/admin/bin/clean_replication >> /var/log/clean_replication.log 2>&1\r\nroot     11561 11551  0 08:55 ?        00:00:00          \\_ /usr/bin/perl -w /opt/admin/bin/clean_replication\r\nroot     11596 11561  0 08:55 ?        00:00:00              \\_ sh -c (systemctl stop recursive_hsreplicate ; sleep 10 ; zfs destroy -r -R clusters/replica/Bmain_PP@Bmain_PP.742966.pass ; systemctl start recursive_hsreplicate) 2>&1\r\nroot     11597 11596  0 08:55 ?        00:00:00                  \\_ sh -c (systemctl stop recursive_hsreplicate ; sleep 10 ; zfs destroy -r -R clusters/replica/Bmain_PP@Bmain_PP.742966.pass ; systemctl start recursive_hsreplicate) 2>&1\r\nroot     11624 11597  0 08:55 ?        00:00:00                      \\_ zfs destroy -r -R clusters/replica/Bmain_PP Bmain_PP.742966.pass\r\nroot     12845 11624  0 08:55 ?        00:00:01                          \\_ /bin/umount -t zfs /replica/Bmain.744194.Bmain_PP.742966.pass\r\n\r\nroot      6947     1  0 Oct18 ?        00:01:35 python /opt/graphite_mon_redis/filer_throttles/runcfsyscallmonitors.py -c /opt/graphite_mon_redis/filer_throttles/syscallmonitorlog.conf\r\nsnmp      6959     1  0 Oct18 ?        00:07:19 /usr/sbin/snmpd -Lsd -Lf /dev/null -u snmp -g snmp -I -smux mteTrigger mteTriggerConf -p /run/snmpd.pid\r\nroot      7753     1  1 Oct18 ?        06:07:23 python /opt/graphite_mon_redis/filer_throttles/runclusterthrottles.py -c /opt/graphite_mon_redis/filer_throttles/throttlelog.conf\r\nroot      7754     1  0 Oct18 ?        00:01:01 python /opt/graphite_mon_redis/filer_throttles/runclusterthrottleapi.py -c /opt/graphite_mon_redis/filer_throttles/apilog.conf\r\nroot      4783     1  0 Oct18 ?        00:01:00 /usr/bin/python /opt/batfsrest/bin/batfsrest.py\r\nroot     21327     1  0 Oct25 ?        00:00:04 /usr/bin/python -u /opt/batfsrest/bin/hsreplicate.py\r\nroot     21336 21327  0 Oct25 ?        00:02:09  \\_ /usr/bin/python -u /opt/batfsrest/bin/repslave.py Bmain_PP\r\nroot     14322 21336  0 08:58 ?        00:00:00      \\_ zfs recv -Fv clusters/replica/Bmain_PP\r\nroot     14323 21336  0 08:58 ?        00:00:00      \\_ /opt/batfsrest/bin/lz4 -qq -d /vmgr/zlogs/Bmain_PP/Bmain_PP.zlog000078761_ts1509627495.lz4 -\r\nroot     16217     1  0 09:55 ?        00:00:00 /usr/bin/python -u /opt/batfsrest/bin/recursive_hsreplicate.py\r\nroot     16233 16217  0 09:55 ?        00:00:00  \\_ /usr/bin/python -u /opt/batfsrest/bin/recursive_init_slave.py Bmain\r\nroot     16260 16233  0 09:55 ?        00:00:00      \\_ /bin/csh /opt/batfsrest/scripts/recursive_hs_install/create_slave.csh Bmain.747831.Bmain_PP.746084.pass Bmain\r\nroot     16261 16260  0 09:55 ?        00:00:00          \\_ zfs create clusters/vmgr/zlogs/Bmain.747831.Bmain_PP.746084.pass\r\n\r\nroot@batfs9940:~# cat /proc/11624/stack\r\n[<ffffffff93289386>] do_wait+0x1c6/0x240\r\n[<ffffffff9328a7b9>] kernel_wait4+0x89/0x130\r\n[<ffffffff9328a8f5>] SYSC_wait4+0x95/0xa0\r\n[<ffffffff9328a9ae>] SyS_wait4+0xe/0x10\r\n[<ffffffff93b10bfb>] entry_SYSCALL_64_fastpath+0x1e/0xa9\r\n[<ffffffffffffffff>] 0xffffffffffffffff\r\n\r\nroot@batfs9940:~# cat /proc/12845/stack\r\n[<ffffffffc0c649ba>] spl_panic+0xfa/0x110 [spl]\r\n[<ffffffffc15dbd04>] zfs_rmnode+0x304/0x340 [zfs]\r\n[<ffffffffc1601700>] zfs_zinactive+0xd0/0xe0 [zfs]\r\n[<ffffffffc15f9089>] zfs_inactive+0x79/0x210 [zfs]\r\n[<ffffffffc1613473>] zpl_evict_inode+0x43/0x60 [zfs]\r\n[<ffffffff93472817>] evict+0xc7/0x1a0\r\n[<ffffffff93472929>] dispose_list+0x39/0x50\r\n[<ffffffff93472a99>] evict_inodes+0x159/0x1a0\r\n[<ffffffff93455274>] generic_shutdown_super+0x44/0x120\r\n[<ffffffff934555d2>] kill_anon_super+0x12/0x20\r\n[<ffffffffc161358a>] zpl_kill_sb+0x1a/0x20 [zfs]\r\n[<ffffffff93455893>] deactivate_locked_super+0x43/0x70\r\n[<ffffffff93455dae>] deactivate_super+0x4e/0x60\r\n[<ffffffff93477a6f>] cleanup_mnt+0x3f/0x80\r\n[<ffffffff93477af2>] __cleanup_mnt+0x12/0x20\r\n[<ffffffff932a4e70>] task_work_run+0x80/0xa0\r\n[<ffffffff932031c4>] exit_to_usermode_loop+0xc4/0xd0\r\n[<ffffffff93203a19>] syscall_return_slowpath+0x59/0x60\r\n[<ffffffff93b10c84>] entry_SYSCALL_64_fastpath+0xa7/0xa9\r\n[<ffffffffffffffff>] 0xffffffffffffffff\r\n\r\nroot@batfs9940:~# cat /proc/14322/stack\r\n[<ffffffffc0c6696e>] cv_wait_common+0x11e/0x140 [spl]\r\n[<ffffffffc0c669a5>] __cv_wait+0x15/0x20 [spl]\r\n[<ffffffffc15b4d9d>] txg_wait_synced+0xdd/0x130 [zfs]\r\n[<ffffffffc158cea4>] dsl_sync_task+0x184/0x270 [zfs]\r\n[<ffffffffc1567c1c>] dmu_recv_begin+0x16c/0x1c0 [zfs]\r\n[<ffffffffc15e73c1>] zfs_ioc_recv_impl+0xe1/0x10e0 [zfs]\r\n[<ffffffffc15e85b8>] zfs_ioc_recv+0x1f8/0x330 [zfs]\r\n[<ffffffffc15e5b44>] zfsdev_ioctl+0x5d4/0x660 [zfs]\r\n[<ffffffff93468d93>] do_vfs_ioctl+0xa3/0x610\r\n[<ffffffff93469379>] SyS_ioctl+0x79/0x90\r\n[<ffffffff93b10bfb>] entry_SYSCALL_64_fastpath+0x1e/0xa9\r\n[<ffffffffffffffff>] 0xffffffffffffffff\r\n\r\nroot@batfs9940:~# cat /proc/16261/stack\r\n[<ffffffffc0c6696e>] cv_wait_common+0x11e/0x140 [spl]\r\n[<ffffffffc0c669a5>] __cv_wait+0x15/0x20 [spl]\r\n[<ffffffffc15b4d9d>] txg_wait_synced+0xdd/0x130 [zfs]\r\n[<ffffffffc158cea4>] dsl_sync_task+0x184/0x270 [zfs]\r\n[<ffffffffc155ccd3>] dmu_objset_create+0x63/0x80 [zfs]\r\n[<ffffffffc15e918d>] zfs_ioc_create+0x17d/0x3d0 [zfs]\r\n[<ffffffffc15e577b>] zfsdev_ioctl+0x20b/0x660 [zfs]\r\n[<ffffffff93468d93>] do_vfs_ioctl+0xa3/0x610\r\n[<ffffffff93469379>] SyS_ioctl+0x79/0x90\r\n[<ffffffff93b10bfb>] entry_SYSCALL_64_fastpath+0x1e/0xa9\r\n[<ffffffffffffffff>] 0xffffffffffffffff\r\n\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384210] VERIFY3(0 == zap_remove_int(zfsvfs->z_os, zfsvfs->z_unlinkedobj, zp->z_id, tx)) failed (0 == 2)\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384251] PANIC at zfs_dir.c:724:zfs_rmnode()\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384268] Showing stack for process 12845\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384271] CPU: 11 PID: 12845 Comm: umount Tainted: P           OE   4.13.0-16-generic #19\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384271] Hardware name: Supermicro X8DTU-6+/X8DTU-6+, BIOS 2.1b       11/15/2011\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384272] Call Trace:\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384280]  dump_stack+0x63/0x8b\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384293]  spl_dumpstack+0x42/0x50 [spl]\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384296]  spl_panic+0xc8/0x110 [spl]\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384461]  ? dmu_buf_rele+0x36/0x40 [zfs]\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384498]  ? zap_unlockdir+0x3f/0x50 [zfs]\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384535]  ? zap_remove_norm+0x76/0xa0 [zfs]\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384572]  ? zap_remove+0x13/0x20 [zfs]\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384608]  zfs_rmnode+0x304/0x340 [zfs]\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384645]  ? zfs_znode_hold_exit+0xf9/0x130 [zfs]\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384681]  zfs_zinactive+0xd0/0xe0 [zfs]\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384718]  zfs_inactive+0x79/0x210 [zfs]\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384720]  ? truncate_pagecache+0x5a/0x70\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384757]  zpl_evict_inode+0x43/0x60 [zfs]\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384759]  evict+0xc7/0x1a0\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384761]  dispose_list+0x39/0x50\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384762]  evict_inodes+0x159/0x1a0\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384764]  generic_shutdown_super+0x44/0x120\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384765]  kill_anon_super+0x12/0x20\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384801]  zpl_kill_sb+0x1a/0x20 [zfs]\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384803]  deactivate_locked_super+0x43/0x70\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384804]  deactivate_super+0x4e/0x60\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384806]  cleanup_mnt+0x3f/0x80\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384808]  __cleanup_mnt+0x12/0x20\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384812]  task_work_run+0x80/0xa0\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384815]  exit_to_usermode_loop+0xc4/0xd0\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384816]  syscall_return_slowpath+0x59/0x60\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384818]  entry_SYSCALL_64_fastpath+0xa7/0xa9\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384819] RIP: 0033:0x7f55ac4adde7\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384820] RSP: 002b:00007ffc12d0e428 EFLAGS: 00000246 ORIG_RAX: 00000000000000a6\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384821] RAX: 0000000000000000 RBX: 00005596a7bf0030 RCX: 00007f55ac4adde7\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384822] RDX: 0000000000000001 RSI: 0000000000000000 RDI: 00005596a7bf3b80\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384823] RBP: 00005596a7bf3b80 R08: 00005596a7bf3b10 R09: 0000000000000013\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384823] R10: 00000000000006b4 R11: 0000000000000246 R12: 00007f55ac9b5844\r\nNov  2 08:55:18 batfs9940 kernel: [1275912.384824] R13: 0000000000000000 R14: 00005596a7bf0210 R15: 00007ffc12d0e6c0\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6812/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "morphinz": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6810", "title": "ZFS send recv problems", "body": "System Information:\r\n- ZFS tunning = http://paste.ubuntu.com/25839332/\r\n- zfs-linux 0.7.2.4.13.4.1-1\r\n- zfs-utils-common 0.7.2-1\r\n- Kernel= Arch linux 4.13.4-1\r\n\r\nProblem 1:  \r\nI have to send 80TB pool to another in same system.  \r\nWhen I start send pool to pool zfs is too slow like 200-500mb/second.   \r\nIf I send dataset snapshots same time to be parallel (I have 10 dataset) I see 2gb/second.   \r\n\r\nProblem 2:\r\nWhen i start \"Send -Rcv |  Recv\" I see some \"Possible memory allocation deadlock: size=1653832 lflags=0x1404200\"  in my dmesg...\r\n- BTW I have 500GB Memory...\r\n\r\n```\r\n[Thu Nov  2 11:44:43 2017] Possible memory allocation deadlock: size=1653832 lflags=0x1404200\r\n[Thu Nov  2 11:44:43 2017] CPU: 8 PID: 37022 Comm: zpool Tainted: P           O    4.13.4-1-ARCH #1\r\n[Thu Nov  2 11:44:43 2017] Hardware name: Supermicro X10DRH LN4/X10DRH-CLN4, BIOS 2.0 01/30/2016\r\n[Thu Nov  2 11:44:43 2017] Call Trace:\r\n[Thu Nov  2 11:44:43 2017]  dump_stack+0x63/0x8b\r\n[Thu Nov  2 11:44:43 2017]  spl_kmem_alloc_impl+0x11f/0x180 [spl]\r\n[Thu Nov  2 11:44:43 2017]  spl_vmem_alloc+0x19/0x20 [spl]\r\n[Thu Nov  2 11:44:43 2017]  nv_alloc_sleep_spl+0x1f/0x30 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  nv_mem_zalloc.isra.0+0x13/0x40 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  nvlist_xpack+0xb4/0x110 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  ? nvlist_common.part.88+0xf1/0x210 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  nvlist_pack+0x34/0x40 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  fnvlist_pack+0x3e/0xa0 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  put_nvlist+0x4c/0x100 [zfs]\r\n[Thu Nov  2 11:44:43 2017]  zfs_ioc_pool_stats+0x4d/0x90 [zfs]\r\n[Thu Nov  2 11:44:43 2017]  zfsdev_ioctl+0x1d6/0x600 [zfs]\r\n[Thu Nov  2 11:44:43 2017]  do_vfs_ioctl+0xa5/0x600\r\n[Thu Nov  2 11:44:43 2017]  ? handle_mm_fault+0xb7/0x200\r\n[Thu Nov  2 11:44:43 2017]  ? __do_page_fault+0x275/0x510\r\n[Thu Nov  2 11:44:43 2017]  SyS_ioctl+0x79/0x90\r\n[Thu Nov  2 11:44:43 2017]  entry_SYSCALL_64_fastpath+0x1a/0xa5\r\n[Thu Nov  2 11:44:43 2017] RIP: 0033:0x7f52b89100c7\r\n[Thu Nov  2 11:44:43 2017] RSP: 002b:00007ffe09d69138 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\r\n[Thu Nov  2 11:44:43 2017] RAX: ffffffffffffffda RBX: 00007f52b8bd4aa0 RCX: 00007f52b89100c7\r\n[Thu Nov  2 11:44:43 2017] RDX: 00007ffe09d69160 RSI: 0000000000005a05 RDI: 0000000000000003\r\n[Thu Nov  2 11:44:43 2017] RBP: 0000000000193c48 R08: 0000561460e16d40 R09: 0000000000000003\r\n[Thu Nov  2 11:44:43 2017] R10: 0000561460e16d40 R11: 0000000000000246 R12: 00000000000112d0\r\n[Thu Nov  2 11:44:43 2017] R13: 0000561460e16d30 R14: 00007ffe09d6c754 R15: 0000561460833000\r\n[Thu Nov  2 11:44:43 2017] Possible memory allocation deadlock: size=1653832 lflags=0x1404200\r\n[Thu Nov  2 11:44:43 2017] CPU: 8 PID: 37022 Comm: zpool Tainted: P           O    4.13.4-1-ARCH #1\r\n[Thu Nov  2 11:44:43 2017] Hardware name: Supermicro X10DRH LN4/X10DRH-CLN4, BIOS 2.0 01/30/2016\r\n[Thu Nov  2 11:44:43 2017] Call Trace:\r\n[Thu Nov  2 11:44:43 2017]  dump_stack+0x63/0x8b\r\n[Thu Nov  2 11:44:43 2017]  spl_kmem_alloc_impl+0x11f/0x180 [spl]\r\n[Thu Nov  2 11:44:43 2017]  spl_vmem_alloc+0x19/0x20 [spl]\r\n[Thu Nov  2 11:44:43 2017]  nv_alloc_sleep_spl+0x1f/0x30 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  nv_mem_zalloc.isra.0+0x13/0x40 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  nvlist_xpack+0xb4/0x110 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  ? nvlist_common.part.88+0xf1/0x210 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  nvlist_pack+0x34/0x40 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  fnvlist_pack+0x3e/0xa0 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  put_nvlist+0x4c/0x100 [zfs]\r\n[Thu Nov  2 11:44:43 2017]  zfs_ioc_pool_stats+0x4d/0x90 [zfs]\r\n[Thu Nov  2 11:44:43 2017]  zfsdev_ioctl+0x1d6/0x600 [zfs]\r\n[Thu Nov  2 11:44:43 2017]  do_vfs_ioctl+0xa5/0x600\r\n[Thu Nov  2 11:44:43 2017]  ? handle_mm_fault+0xb7/0x200\r\n[Thu Nov  2 11:44:43 2017]  ? __do_page_fault+0x275/0x510\r\n[Thu Nov  2 11:44:43 2017]  SyS_ioctl+0x79/0x90\r\n[Thu Nov  2 11:44:43 2017]  entry_SYSCALL_64_fastpath+0x1a/0xa5\r\n[Thu Nov  2 11:44:43 2017] RIP: 0033:0x7f52b89100c7\r\n[Thu Nov  2 11:44:43 2017] RSP: 002b:00007ffe09d69138 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\r\n[Thu Nov  2 11:44:43 2017] RAX: ffffffffffffffda RBX: 00007f52b8bd4aa0 RCX: 00007f52b89100c7\r\n[Thu Nov  2 11:44:43 2017] RDX: 00007ffe09d69160 RSI: 0000000000005a05 RDI: 0000000000000003\r\n[Thu Nov  2 11:44:43 2017] RBP: 0000000000193c48 R08: 0000561460e16d40 R09: 0000000000000003\r\n[Thu Nov  2 11:44:43 2017] R10: 0000561460e16d40 R11: 0000000000000246 R12: 00000000000112d0\r\n[Thu Nov  2 11:44:43 2017] R13: 0000561460e16d30 R14: 00007ffe09d6c754 R15: 0000561460833000\r\n[Thu Nov  2 11:44:43 2017] Possible memory allocation deadlock: size=1653832 lflags=0x1404200\r\n[Thu Nov  2 11:44:43 2017] CPU: 8 PID: 37022 Comm: zpool Tainted: P           O    4.13.4-1-ARCH #1\r\n[Thu Nov  2 11:44:43 2017] Hardware name: Supermicro X10DRH LN4/X10DRH-CLN4, BIOS 2.0 01/30/2016\r\n[Thu Nov  2 11:44:43 2017] Call Trace:\r\n[Thu Nov  2 11:44:43 2017]  dump_stack+0x63/0x8b\r\n[Thu Nov  2 11:44:43 2017]  spl_kmem_alloc_impl+0x11f/0x180 [spl]\r\n[Thu Nov  2 11:44:43 2017]  spl_vmem_alloc+0x19/0x20 [spl]\r\n[Thu Nov  2 11:44:43 2017]  nv_alloc_sleep_spl+0x1f/0x30 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  nv_mem_zalloc.isra.0+0x13/0x40 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  nvlist_xpack+0xb4/0x110 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  ? nvlist_common.part.88+0xf1/0x210 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  nvlist_pack+0x34/0x40 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  fnvlist_pack+0x3e/0xa0 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  put_nvlist+0x4c/0x100 [zfs]\r\n[Thu Nov  2 11:44:43 2017]  zfs_ioc_pool_stats+0x4d/0x90 [zfs]\r\n[Thu Nov  2 11:44:43 2017]  zfsdev_ioctl+0x1d6/0x600 [zfs]\r\n[Thu Nov  2 11:44:43 2017]  do_vfs_ioctl+0xa5/0x600\r\n[Thu Nov  2 11:44:43 2017]  ? handle_mm_fault+0xb7/0x200\r\n[Thu Nov  2 11:44:43 2017]  ? __do_page_fault+0x275/0x510\r\n[Thu Nov  2 11:44:43 2017]  SyS_ioctl+0x79/0x90\r\n[Thu Nov  2 11:44:43 2017]  entry_SYSCALL_64_fastpath+0x1a/0xa5\r\n[Thu Nov  2 11:44:43 2017] RIP: 0033:0x7f52b89100c7\r\n[Thu Nov  2 11:44:43 2017] RSP: 002b:00007ffe09d69138 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\r\n[Thu Nov  2 11:44:43 2017] RAX: ffffffffffffffda RBX: 00007f52b8bd4aa0 RCX: 00007f52b89100c7\r\n[Thu Nov  2 11:44:43 2017] RDX: 00007ffe09d69160 RSI: 0000000000005a05 RDI: 0000000000000003\r\n[Thu Nov  2 11:44:43 2017] RBP: 0000000000193c48 R08: 0000561460e16d40 R09: 0000000000000003\r\n[Thu Nov  2 11:44:43 2017] R10: 0000561460e16d40 R11: 0000000000000246 R12: 00000000000112d0\r\n[Thu Nov  2 11:44:43 2017] R13: 0000561460e16d30 R14: 00007ffe09d6c754 R15: 0000561460833000\r\n[Thu Nov  2 11:44:43 2017] Possible memory allocation deadlock: size=1653832 lflags=0x1404200\r\n[Thu Nov  2 11:44:43 2017] CPU: 8 PID: 37022 Comm: zpool Tainted: P           O    4.13.4-1-ARCH #1\r\n[Thu Nov  2 11:44:43 2017] Hardware name: Supermicro X10DRH LN4/X10DRH-CLN4, BIOS 2.0 01/30/2016\r\n[Thu Nov  2 11:44:43 2017] Call Trace:\r\n[Thu Nov  2 11:44:43 2017]  dump_stack+0x63/0x8b\r\n[Thu Nov  2 11:44:43 2017]  spl_kmem_alloc_impl+0x11f/0x180 [spl]\r\n[Thu Nov  2 11:44:43 2017]  spl_vmem_alloc+0x19/0x20 [spl]\r\n[Thu Nov  2 11:44:43 2017]  nv_alloc_sleep_spl+0x1f/0x30 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  nv_mem_zalloc.isra.0+0x13/0x40 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  nvlist_xpack+0xb4/0x110 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  ? nvlist_common.part.88+0xf1/0x210 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  nvlist_pack+0x34/0x40 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  fnvlist_pack+0x3e/0xa0 [znvpair]\r\n[Thu Nov  2 11:44:43 2017]  put_nvlist+0x4c/0x100 [zfs]\r\n[Thu Nov  2 11:44:43 2017]  zfs_ioc_pool_stats+0x4d/0x90 [zfs]\r\n[Thu Nov  2 11:44:43 2017]  zfsdev_ioctl+0x1d6/0x600 [zfs]\r\n[Thu Nov  2 11:44:43 2017]  do_vfs_ioctl+0xa5/0x600\r\n[Thu Nov  2 11:44:43 2017]  ? handle_mm_fault+0xb7/0x200\r\n[Thu Nov  2 11:44:43 2017]  ? __do_page_fault+0x275/0x510\r\n[Thu Nov  2 11:44:43 2017]  SyS_ioctl+0x79/0x90\r\n[Thu Nov  2 11:44:43 2017]  entry_SYSCALL_64_fastpath+0x1a/0xa5\r\n[Thu Nov  2 11:44:43 2017] RIP: 0033:0x7f52b89100c7\r\n[Thu Nov  2 11:44:43 2017] RSP: 002b:00007ffe09d69138 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\r\n[Thu Nov  2 11:44:43 2017] RAX: ffffffffffffffda RBX: 00007f52b8bd4aa0 RCX: 00007f52b89100c7\r\n[Thu Nov  2 11:44:43 2017] RDX: 00007ffe09d69160 RSI: 0000000000005a05 RDI: 0000000000000003\r\n[Thu Nov  2 11:44:43 2017] RBP: 0000000000193c48 R08: 0000561460e16d40 R09: 0000000000000003\r\n[Thu Nov  2 11:44:43 2017] R10: 0000561460e16d40 R11: 0000000000000246 R12: 00000000000112d0\r\n[Thu Nov  2 11:44:43 2017] R13: 0000561460e16d30 R14: 00007ffe09d6c754 R15: 0000561460833000\r\n[Thu Nov  2 11:45:43 2017] spl_kmem_alloc_impl: 5352135 callbacks suppressed\r\n[Thu Nov  2 11:45:43 2017] Possible memory allocation deadlock: size=1653832 lflags=0x1404200\r\n[Thu Nov  2 11:45:43 2017] CPU: 1 PID: 37022 Comm: zpool Tainted: P           O    4.13.4-1-ARCH #1\r\n[Thu Nov  2 11:45:43 2017] Hardware name: Supermicro X10DRH LN4/X10DRH-CLN4, BIOS 2.0 01/30/2016\r\n[Thu Nov  2 11:45:43 2017] Call Trace:\r\n[Thu Nov  2 11:45:43 2017]  dump_stack+0x63/0x8b\r\n[Thu Nov  2 11:45:43 2017]  spl_kmem_alloc_impl+0x11f/0x180 [spl]\r\n[Thu Nov  2 11:45:43 2017]  spl_vmem_alloc+0x19/0x20 [spl]\r\n[Thu Nov  2 11:45:43 2017]  nv_alloc_sleep_spl+0x1f/0x30 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  nv_mem_zalloc.isra.0+0x13/0x40 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  nvlist_xpack+0xb4/0x110 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  ? nvlist_common.part.88+0xf1/0x210 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  nvlist_pack+0x34/0x40 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  fnvlist_pack+0x3e/0xa0 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  put_nvlist+0x4c/0x100 [zfs]\r\n[Thu Nov  2 11:45:43 2017]  zfs_ioc_pool_stats+0x4d/0x90 [zfs]\r\n[Thu Nov  2 11:45:43 2017]  zfsdev_ioctl+0x1d6/0x600 [zfs]\r\n[Thu Nov  2 11:45:43 2017]  do_vfs_ioctl+0xa5/0x600\r\n[Thu Nov  2 11:45:43 2017]  ? handle_mm_fault+0xb7/0x200\r\n[Thu Nov  2 11:45:43 2017]  ? __do_page_fault+0x275/0x510\r\n[Thu Nov  2 11:45:43 2017]  SyS_ioctl+0x79/0x90\r\n[Thu Nov  2 11:45:43 2017]  entry_SYSCALL_64_fastpath+0x1a/0xa5\r\n[Thu Nov  2 11:45:43 2017] RIP: 0033:0x7f52b89100c7\r\n[Thu Nov  2 11:45:43 2017] RSP: 002b:00007ffe09d69138 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\r\n[Thu Nov  2 11:45:43 2017] RAX: ffffffffffffffda RBX: 00007f52b8bd4aa0 RCX: 00007f52b89100c7\r\n[Thu Nov  2 11:45:43 2017] RDX: 00007ffe09d69160 RSI: 0000000000005a05 RDI: 0000000000000003\r\n[Thu Nov  2 11:45:43 2017] RBP: 0000000000193c48 R08: 0000561460e16d40 R09: 0000000000000003\r\n[Thu Nov  2 11:45:43 2017] R10: 0000561460e16d40 R11: 0000000000000246 R12: 00000000000112d0\r\n[Thu Nov  2 11:45:43 2017] R13: 0000561460e16d30 R14: 00007ffe09d6c754 R15: 0000561460833000\r\n[Thu Nov  2 11:45:43 2017] Possible memory allocation deadlock: size=1653832 lflags=0x1404200\r\n[Thu Nov  2 11:45:43 2017] CPU: 1 PID: 37022 Comm: zpool Tainted: P           O    4.13.4-1-ARCH #1\r\n[Thu Nov  2 11:45:43 2017] Hardware name: Supermicro X10DRH LN4/X10DRH-CLN4, BIOS 2.0 01/30/2016\r\n[Thu Nov  2 11:45:43 2017] Call Trace:\r\n[Thu Nov  2 11:45:43 2017]  dump_stack+0x63/0x8b\r\n[Thu Nov  2 11:45:43 2017]  spl_kmem_alloc_impl+0x11f/0x180 [spl]\r\n[Thu Nov  2 11:45:43 2017]  spl_vmem_alloc+0x19/0x20 [spl]\r\n[Thu Nov  2 11:45:43 2017]  nv_alloc_sleep_spl+0x1f/0x30 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  nv_mem_zalloc.isra.0+0x13/0x40 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  nvlist_xpack+0xb4/0x110 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  ? nvlist_common.part.88+0xf1/0x210 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  nvlist_pack+0x34/0x40 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  fnvlist_pack+0x3e/0xa0 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  put_nvlist+0x4c/0x100 [zfs]\r\n[Thu Nov  2 11:45:43 2017]  zfs_ioc_pool_stats+0x4d/0x90 [zfs]\r\n[Thu Nov  2 11:45:43 2017]  zfsdev_ioctl+0x1d6/0x600 [zfs]\r\n[Thu Nov  2 11:45:43 2017]  do_vfs_ioctl+0xa5/0x600\r\n[Thu Nov  2 11:45:43 2017]  ? handle_mm_fault+0xb7/0x200\r\n[Thu Nov  2 11:45:43 2017]  ? __do_page_fault+0x275/0x510\r\n[Thu Nov  2 11:45:43 2017]  SyS_ioctl+0x79/0x90\r\n[Thu Nov  2 11:45:43 2017]  entry_SYSCALL_64_fastpath+0x1a/0xa5\r\n[Thu Nov  2 11:45:43 2017] RIP: 0033:0x7f52b89100c7\r\n[Thu Nov  2 11:45:43 2017] RSP: 002b:00007ffe09d69138 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\r\n[Thu Nov  2 11:45:43 2017] RAX: ffffffffffffffda RBX: 00007f52b8bd4aa0 RCX: 00007f52b89100c7\r\n[Thu Nov  2 11:45:43 2017] RDX: 00007ffe09d69160 RSI: 0000000000005a05 RDI: 0000000000000003\r\n[Thu Nov  2 11:45:43 2017] RBP: 0000000000193c48 R08: 0000561460e16d40 R09: 0000000000000003\r\n[Thu Nov  2 11:45:43 2017] R10: 0000561460e16d40 R11: 0000000000000246 R12: 00000000000112d0\r\n[Thu Nov  2 11:45:43 2017] R13: 0000561460e16d30 R14: 00007ffe09d6c754 R15: 0000561460833000\r\n[Thu Nov  2 11:45:43 2017] Possible memory allocation deadlock: size=1653832 lflags=0x1404200\r\n[Thu Nov  2 11:45:43 2017] CPU: 1 PID: 37022 Comm: zpool Tainted: P           O    4.13.4-1-ARCH #1\r\n[Thu Nov  2 11:45:43 2017] Hardware name: Supermicro X10DRH LN4/X10DRH-CLN4, BIOS 2.0 01/30/2016\r\n[Thu Nov  2 11:45:43 2017] Call Trace:\r\n[Thu Nov  2 11:45:43 2017]  dump_stack+0x63/0x8b\r\n[Thu Nov  2 11:45:43 2017]  spl_kmem_alloc_impl+0x11f/0x180 [spl]\r\n[Thu Nov  2 11:45:43 2017]  spl_vmem_alloc+0x19/0x20 [spl]\r\n[Thu Nov  2 11:45:43 2017]  nv_alloc_sleep_spl+0x1f/0x30 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  nv_mem_zalloc.isra.0+0x13/0x40 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  nvlist_xpack+0xb4/0x110 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  ? nvlist_common.part.88+0xf1/0x210 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  nvlist_pack+0x34/0x40 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  fnvlist_pack+0x3e/0xa0 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  put_nvlist+0x4c/0x100 [zfs]\r\n[Thu Nov  2 11:45:43 2017]  zfs_ioc_pool_stats+0x4d/0x90 [zfs]\r\n[Thu Nov  2 11:45:43 2017]  zfsdev_ioctl+0x1d6/0x600 [zfs]\r\n[Thu Nov  2 11:45:43 2017]  do_vfs_ioctl+0xa5/0x600\r\n[Thu Nov  2 11:45:43 2017]  ? handle_mm_fault+0xb7/0x200\r\n[Thu Nov  2 11:45:43 2017]  ? __do_page_fault+0x275/0x510\r\n[Thu Nov  2 11:45:43 2017]  SyS_ioctl+0x79/0x90\r\n[Thu Nov  2 11:45:43 2017]  entry_SYSCALL_64_fastpath+0x1a/0xa5\r\n[Thu Nov  2 11:45:43 2017] RIP: 0033:0x7f52b89100c7\r\n[Thu Nov  2 11:45:43 2017] RSP: 002b:00007ffe09d69138 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\r\n[Thu Nov  2 11:45:43 2017] RAX: ffffffffffffffda RBX: 00007f52b8bd4aa0 RCX: 00007f52b89100c7\r\n[Thu Nov  2 11:45:43 2017] RDX: 00007ffe09d69160 RSI: 0000000000005a05 RDI: 0000000000000003\r\n[Thu Nov  2 11:45:43 2017] RBP: 0000000000193c48 R08: 0000561460e16d40 R09: 0000000000000003\r\n[Thu Nov  2 11:45:43 2017] R10: 0000561460e16d40 R11: 0000000000000246 R12: 00000000000112d0\r\n[Thu Nov  2 11:45:43 2017] R13: 0000561460e16d30 R14: 00007ffe09d6c754 R15: 0000561460833000\r\n[Thu Nov  2 11:45:43 2017] Possible memory allocation deadlock: size=1653832 lflags=0x1404200\r\n[Thu Nov  2 11:45:43 2017] CPU: 1 PID: 37022 Comm: zpool Tainted: P           O    4.13.4-1-ARCH #1\r\n[Thu Nov  2 11:45:43 2017] Hardware name: Supermicro X10DRH LN4/X10DRH-CLN4, BIOS 2.0 01/30/2016\r\n[Thu Nov  2 11:45:43 2017] Call Trace:\r\n[Thu Nov  2 11:45:43 2017]  dump_stack+0x63/0x8b\r\n[Thu Nov  2 11:45:43 2017]  spl_kmem_alloc_impl+0x11f/0x180 [spl]\r\n[Thu Nov  2 11:45:43 2017]  spl_vmem_alloc+0x19/0x20 [spl]\r\n[Thu Nov  2 11:45:43 2017]  nv_alloc_sleep_spl+0x1f/0x30 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  nv_mem_zalloc.isra.0+0x13/0x40 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  nvlist_xpack+0xb4/0x110 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  ? nvlist_common.part.88+0xf1/0x210 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  nvlist_pack+0x34/0x40 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  fnvlist_pack+0x3e/0xa0 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  put_nvlist+0x4c/0x100 [zfs]\r\n[Thu Nov  2 11:45:43 2017]  zfs_ioc_pool_stats+0x4d/0x90 [zfs]\r\n[Thu Nov  2 11:45:43 2017]  zfsdev_ioctl+0x1d6/0x600 [zfs]\r\n[Thu Nov  2 11:45:43 2017]  do_vfs_ioctl+0xa5/0x600\r\n[Thu Nov  2 11:45:43 2017]  ? handle_mm_fault+0xb7/0x200\r\n[Thu Nov  2 11:45:43 2017]  ? __do_page_fault+0x275/0x510\r\n[Thu Nov  2 11:45:43 2017]  SyS_ioctl+0x79/0x90\r\n[Thu Nov  2 11:45:43 2017]  entry_SYSCALL_64_fastpath+0x1a/0xa5\r\n[Thu Nov  2 11:45:43 2017] RIP: 0033:0x7f52b89100c7\r\n[Thu Nov  2 11:45:43 2017] RSP: 002b:00007ffe09d69138 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\r\n[Thu Nov  2 11:45:43 2017] RAX: ffffffffffffffda RBX: 00007f52b8bd4aa0 RCX: 00007f52b89100c7\r\n[Thu Nov  2 11:45:43 2017] RDX: 00007ffe09d69160 RSI: 0000000000005a05 RDI: 0000000000000003\r\n[Thu Nov  2 11:45:43 2017] RBP: 0000000000193c48 R08: 0000561460e16d40 R09: 0000000000000003\r\n[Thu Nov  2 11:45:43 2017] R10: 0000561460e16d40 R11: 0000000000000246 R12: 00000000000112d0\r\n[Thu Nov  2 11:45:43 2017] R13: 0000561460e16d30 R14: 00007ffe09d6c754 R15: 0000561460833000\r\n[Thu Nov  2 11:45:43 2017] Possible memory allocation deadlock: size=1653832 lflags=0x1404200\r\n[Thu Nov  2 11:45:43 2017] CPU: 1 PID: 37022 Comm: zpool Tainted: P           O    4.13.4-1-ARCH #1\r\n[Thu Nov  2 11:45:43 2017] Hardware name: Supermicro X10DRH LN4/X10DRH-CLN4, BIOS 2.0 01/30/2016\r\n[Thu Nov  2 11:45:43 2017] Call Trace:\r\n[Thu Nov  2 11:45:43 2017]  dump_stack+0x63/0x8b\r\n[Thu Nov  2 11:45:43 2017]  spl_kmem_alloc_impl+0x11f/0x180 [spl]\r\n[Thu Nov  2 11:45:43 2017]  spl_vmem_alloc+0x19/0x20 [spl]\r\n[Thu Nov  2 11:45:43 2017]  nv_alloc_sleep_spl+0x1f/0x30 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  nv_mem_zalloc.isra.0+0x13/0x40 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  nvlist_xpack+0xb4/0x110 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  ? nvlist_common.part.88+0xf1/0x210 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  nvlist_pack+0x34/0x40 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  fnvlist_pack+0x3e/0xa0 [znvpair]\r\n[Thu Nov  2 11:45:43 2017]  put_nvlist+0x4c/0x100 [zfs]\r\n[Thu Nov  2 11:45:43 2017]  zfs_ioc_pool_stats+0x4d/0x90 [zfs]\r\n[Thu Nov  2 11:45:43 2017]  zfsdev_ioctl+0x1d6/0x600 [zfs]\r\n[Thu Nov  2 11:45:43 2017]  do_vfs_ioctl+0xa5/0x600\r\n[Thu Nov  2 11:45:43 2017]  ? handle_mm_fault+0xb7/0x200\r\n[Thu Nov  2 11:45:43 2017]  ? __do_page_fault+0x275/0x510\r\n[Thu Nov  2 11:45:43 2017]  SyS_ioctl+0x79/0x90\r\n[Thu Nov  2 11:45:43 2017]  entry_SYSCALL_64_fastpath+0x1a/0xa5\r\n[Thu Nov  2 11:45:43 2017] RIP: 0033:0x7f52b89100c7\r\n[Thu Nov  2 11:45:43 2017] RSP: 002b:00007ffe09d69138 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\r\n[Thu Nov  2 11:45:43 2017] RAX: ffffffffffffffda RBX: 00007f52b8bd4aa0 RCX: 00007f52b89100c7\r\n[Thu Nov  2 11:45:43 2017] RDX: 00007ffe09d69160 RSI: 0000000000005a05 RDI: 0000000000000003\r\n[Thu Nov  2 11:45:43 2017] RBP: 0000000000193c48 R08: 0000561460e16d40 R09: 0000000000000003\r\n[Thu Nov  2 11:45:43 2017] R10: 0000561460e16d40 R11: 0000000000000246 R12: 00000000000112d0\r\n[Thu Nov  2 11:45:43 2017] R13: 0000561460e16d30 R14: 00007ffe09d6c754 R15: 0000561460833000\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dotmanila": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6806", "title": "Encrypted ZFS restore Problem", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu 16.04\r\nDistribution Version    | AWS r4.xlarge instance with 2 EBS volumes 20GB each\r\nLinux Kernel                 | Linux ip-10-1-2-117 4.4.0-1039-aws \r\nArchitecture                 | x86_64\r\nZFS Version                  | v0.7.0-157_gc9427c4, ZFS pool version 5000, ZFS filesystem version 5\r\nSPL Version                  | version:        0.7.0-20_g35a44fc\r\n\r\n### Describe the problem you're observing\r\nWhen restoring an encrypted volume/pool I can either get a crash (AWS instance crashes and restarts) \r\n\r\n```\r\nNov  1 16:27:14 ip-10-1-2-117 kernel: [ 9752.942899]  xvdb: xvdb1 xvdb9\r\nNov  1 16:27:14 ip-10-1-2-117 kernel: [ 9752.948188]  xvdb: xvdb1 xvdb9\r\nNov  1 16:32:39 ip-10-1-2-117 kernel: [    0.000000] Initializing cgroup subsys cpuset```\r\n\r\nOr an error about key echange:\r\n\r\n```\r\nubuntu@ip-10-1-2-87:/backups$ sudo cp zfskey /dev/shm/\r\nubuntu@ip-10-1-2-87:/backups$ sudo zfs load-key mysql-data\r\ncannot open 'mysql-data': dataset does not exist\r\nubuntu@ip-10-1-2-87:/backups$ sudo zfs load-key mysql/data\r\nubuntu@ip-10-1-2-87:/backups$ sudo zfs load-key mysql/logs\r\nubuntu@ip-10-1-2-87:/backups$ sudo zfs mount -a\r\nfilesystem 'mysql/data' can not be mounted: Invalid exchange\r\ncannot mount 'mysql/data': Invalid argument\r\nubuntu@ip-10-1-2-87:/backups$```\r\n\r\n### Describe how to reproduce the problem\r\nThis gist will walk through bootstrapping an instance, building ZFS running the tests up to the problem:\r\n\r\nhttps://gist.github.com/dotmanila/82c1dc620149827fc81fafe6af30130d\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6806/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "AaronFriel": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6805", "title": "ZFS pool corrupt, core dump on running zfs list: \"internal error: Invalid exchange\", can no longer zpool import.", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 17.10\r\nLinux Kernel                 | 4.13.0-16\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.11-1ubuntu3\r\nSPL Version                  | 0.6.5.11-1ubuntu1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nFound my continuous integration service using Docker in Docker was not working, this was the cause:\r\n\r\n```\r\n\u279c docker logs e038e8d38a06\r\n... (miscellaneous)\r\ntime=\"2017-11-01T12:19:54.982955029Z\" level=info msg=\"Setting the storage driver from the $DOCKER_DRIVER environment variable (zfs)\"\r\nError starting daemon: error initializing graphdriver: Cannot find root filesystem main/docker: signal: aborted (core dumped): \"/usr/sbin/zfs zfs list -rHp -t filesystem -o name,origin,used,available,mountpoint,compression,type,volsize,quota,written,logicalused,usedbydataset main/docker\" => internal error: No error information\r\n```\r\n\r\nSo I tried running the command myself:\r\n\r\n```\r\n\u279c sudo zfs list -rHp -t filesystem -o name,origin,used,available,mountpoint,compression,type,volsize,quota,written,logicalused,usedbydataset main/docker\r\ninternal error: Invalid exchange\r\n[1]    89325 abort      sudo zfs list -rHp -t filesystem -o  main/docker\r\n```\r\n\r\nUpon rebooting, the ZFS pool was not imported, and attempts to import were met with:\r\n\r\n```\r\n\u279c sudo zpool import -aN\r\ncannot import 'main': one or more devices is currently unavailable\r\n```\r\n\r\nHowever, all of the devices are available: there is a zil (mounted at /dev/vg-main/log) and a data volume (mounted at /dev/vg-main/capacity). Both are available, as seen here:\r\n\r\n```\r\n\u279c sudo zpool import -F -n -m\r\n   pool: main\r\n     id: 8637001018325559436\r\n  state: ONLINE\r\n action: The pool can be imported using its name or numeric identifier.\r\n config:\r\n\r\n        main                 ONLINE\r\n          vg--main-capacity  ONLINE\r\n        logs\r\n          log                ONLINE\r\n```\r\n\r\nAnd I cannot seem to force the pool to import, because of this invalid exchange error:\r\n\r\n```\r\n\u279c sudo zpool import -a -f -m -F\r\ninternal error: Invalid exchange\r\n[1]    30818 abort      sudo zpool import -a -f -m -F\r\n```\r\n\r\n### Describe how to reproduce the problem\r\n\r\nNo idea. Needed to reboot system to see if that would clear it up. It did not.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\nNo warnings or errors other than the zpool import one above, and the sudden inability to run `zfs list` commands.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6805/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "testbird": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6804", "title": "Doc: possibility to prepare root fs, and then to continue with default installer method?", "body": "It's rather a question.\r\n\r\nI had somehow hoped the \"root on ZFS\" howtos like\r\nhttps://github.com/zfsonlinux/zfs/wiki/Debian-Stretch-Root-on-ZFS\r\nwould simply show how to open a terminal after booting some regular distribution install media, then download packages and setup the target filesystem and then just explain how to go on using the normal installer to put the system onto the ZFS.\r\n\r\nShouldn't that be a prefererable ZFS introduction method? The wiki only explaining how to add ZFS to the regular, well documented install procedures?\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6804/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Blub": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6800", "title": "setgid bit in chmod dropped inconsistently", "body": "### System information\r\nType                         | Version/Name\r\n  ---                        |     --- \r\nDistribution Name            | Debian\r\nDistribution Version         | Stretch\r\nLinux Kernel                 | 4.13.4-1-pve\r\nArchitecture                 | x86_64\r\nZFS Version                  | tested git-master & 0.7.2\r\nSPL Version                  | tested git-master & 0.7.2\r\n\r\n### Describe the problem you're observing\r\n\r\nThe setgid bit of a `chmod()` call as root (with all usual capabilities\r\n(CAP_FSETID, CAP_FOWNER, ...) from within a user namespace drops the setgid bit\r\nif the user (root) is the owner of the file while the file belongs to a group\r\none is currently not part of.\r\n```\r\n# id\r\nuid=0(root) gid=0(root) groups=0(root)\r\n# touch asdf\r\n# chmod 2755 asdf\r\n# ls -l asdf\r\n-rwxr-sr-x 1 root root 0 Oct 30 15:33 asdf     <- ok - worked\r\n# chgrp 88 asdf\r\n# ls -l asdf\r\n-rwxr-xr-x 1 root 88 0 Oct 30 15:33 asdf       <- ok - lost setgid as as expected\r\n# chmod 2755 asdf\r\n# ls -l asdf\r\n-rwxr-xr-x 1 root 88 0 Oct 30 15:33 asdf       <- wrong - setgid ignored\r\n# chown 88 asdf\r\n# chmod 2755 asdf\r\n# ls -l asdf\r\n-rwxr-sr-x 1 88 88 0 Oct 30 15:33 asdf         <- ok - setgid honored (inconsistent!)\r\n```\r\n\r\nNote that as root one can simply use `setgroups()` to work around this issue\r\nusually.\r\n\r\nThis seems to have happened together with the `zfs allow` change set. At least\r\nwhen adapting `secpolicy_vnode_setids_setgids()` in module/zfs/policy.c to use\r\nan `ns_capable(current_user_ns(), cap)` check rather than the regular\r\n`capable()` check from `priv_policy()` the setgid bit is not dropped anymore.\r\nI used the following change to test (it also adds a check for whether the gid\r\nhas a mapping in the current namespace):\r\nhttps://github.com/Blub/zfs/commit/dd97fcf695f130e9c5473cf885ccec4a80a78513\r\n\r\nI'm not sure whether the other functions using `priv_policy()` also require more\r\nuser namespace specific adjustments, and the comment above the function doesn't\r\nhelp me figure that out ;-)\r\nI was, however, unable to find additional easily triggerable code-paths which\r\nmisbehave in user namespaces, but I'm quite new to the code, so there is that...\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6800/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6865", "title": "user namespace bugfixes and features", "body": "This series can be seen as 4 separate \"chunks\":\r\n\r\nChunk 1: setgid mode bugfix & regression test:\r\n* Patch 1 fixes the main issue.\r\n* Patch 2 adds a helper for running user namespace tests. Currently uses a fixed\r\n  user id range. (I saw no reason for anything more complex than that.)\r\n* Patch 3 adds a regression test for the issue fixed in patch 1.\r\n\r\nChunk 2: mounting from user namespaces (RFC):\r\n* Patch 4 is an RFC useful for when a user can have a mount namespace (usually\r\n  in combination with user namespaces. Eg. giving `zfs allow`ing create+mount\r\n  permissions to a container.\r\n* Patch 5 is necessary when including the third chunk but is otherwise there\r\n  since it made writing the test case of patch 6 more convenient.\r\n* Patch 6 tests create+mount permissions with user namespaces.\r\n\r\nChunk 3: mapping user ids when using zfs allow from within user namespaces.\r\n* Patch 7 causes `ZFS_IOC_GET_FSACL` and `ZFS_IOC_SET_FSACL` to perform user id\r\n  mapping (as well as checking!) on the sent/received data. Otherwise root in a\r\n  user namespace would not be able to run `zfs allow` with the user IDs as seen\r\n  from within its namespace, but would have to perform the mapping to real IDs.\r\n  This is also what easily enables users to create allow entries for user IDs\r\n  which do not exist in the host namespace's `/etc/passwd` and therefore would\r\n  show up empty and indistinguishable to the host (making patch 5 a\r\n  requirement).\r\n\r\nChunk 4: change the 'unallow' check:\r\n* Patch 8 allows users who have CAP_SYS_ADMIN in the current namespace (iow.\r\n  root in containers) to remove permissions of others if they're also allowed\r\n  to add the permission.\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements. (at least according to `make checkstyle`)\r\n- [ ] I have updated the documentation accordingly. (not yet)\r\n- [x] I have read the **CONTRIBUTING** document.\r\n- [x] I have added tests to cover my changes.\r\n- [x] All new and existing tests passed.\r\n- [x] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gregfr": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6799", "title": "local fs .. does not have fromsnap (... in stream); must have been deleted locally; ignoring", "body": "Greetings\r\nI'm using zfs snapshots to periodically backup my data. It's been working for years, however now I get this strange message : \r\n\r\n`local fs DESTINATION does not have fromsnap (... in stream); must have been deleted locally; ignoring`\r\n\r\nIf I understand correctly, it says that a snapshot that I did and which is listed in `zfs list` is no longer on disk. I used `zfs scrub` but that gave me no error.\r\nIf I take a look inside the `.zfs` magical folder, I can see the snapshot and read its files.\r\n\r\nIf I delete the fromsnap on DESTINATION, then send/receive it again, it's working, I can backup.\r\n\r\n\r\nWhy is this message displayed? why is it preventing my backup? and if the destination is corrupted, how can I tell?\r\n\r\nThanks in advance\r\n\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | Ubuntu 16.04.3 LTS\r\nLinux Kernel                 | 4.4.0-97\r\nArchitecture                 | x86_64\r\nZFS Version                  |  0.6.5.6-0ubuntu16\r\nSPL Version                  | 0.6.5.6-0ubuntu4\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\n`zfs receive` gives a strange error message and doesn't write the data.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nProbably a corrupted filesystem?\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6799/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6244", "title": "\"zpool import\" hangs", "body": "Greetings\r\nI have an old proxmox server with 2 zpools. Yesterday, a VM hanged forcing me to restart. Now, one of the pools can be imported, but the other hangs. After some research on Google, I tried \"import -Fn\" and also importing readonly, no luck in any case.\r\nFrom the moment I run the \"zpool import\" command, all zfs command hang (like \"zfs list\") until I restart; the rest of the system is still responsive.\r\nWhat can I do to remount the pool?\r\nThanks in advance\r\ngreg\r\n\r\n### System information\r\n\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  proxmox\r\nDistribution Version    |  3.4\r\nLinux Kernel                 |  2.6.32-48-pve\r\nArchitecture                 |  x86_64\r\nZFS Version                  |  0.6.5.7-8_gf1b07c5 -> 0.6.5.10\r\nSPL Version                  |  0.6.5.7-3_g8455153 -> 0.6.5.10\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6244/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "UnConundrum": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6798", "title": "zfs on boot partition how-to doesn't use zfs", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 16.10\r\nLinux Kernel                 | 4.8.0-59-generic\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.8-0ubuntu4.2\r\nSPL Version                  | 0.6.5.8-2\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nI'm following https://github.com/zfsonlinux/zfs/wiki/Ubuntu-16.10-Root-on-ZFS for an unencrypted UDFI boot.  After a successful install, I added a drive to mirror the install.  \r\nAt section 2.2 we create the boot partition.  \r\nAt section 4.8b we install set the partition to fat32 and install grub.  \r\nAt section 6.8 we copy the boot partition from the first drive to the 2nd and tell efi about it.  \r\n\r\nI can't see where a pool is created for the boot partition, or where the boot partition of disk1 is mirrored to the boot partition of disk2.  df lists the partition, but not as part of a pool:\r\n`/dev/sdc3            511M  256K  511M   1% /boot/efi`\r\nNothing in zfs list indicates a boot pool or any dataset including /boot/efi\r\n\r\n I realize that establishing the duplicate boot partition on drive2 at seciton 6.8 makes it bootable, but I can't see where any changes to grub will be mirrored to it.  Am I missing something?\r\n\r\n### Describe how to reproduce the problem\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6798/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "JoakimZiegler": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6795", "title": "Exporting a ZFS dataset over NFS over RDMA generates RDMA errors", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS\r\nDistribution Version    | 7.4.1708\r\nLinux Kernel                 | 3.10.0-693.5.2.el7.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.3-1\r\nSPL Version                  | 0.7.3-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nI've just started testing ZFS on Linux for our file servers. We currently run Areca hardware RAID cards and share the volumes over NFS over RDMA, on Mellanox Infiniband adapters and switches. I've basically set up a ZFS pool consisting of two raidz2 vdevs with 12 drives each on an existing fileserver (Avago 12Gbps SAS adapters, multipath, Raid Machine enclosure with LSI expanders, 10TB Seagate 12Gbps SAS Enterprise Capacity drives), and copied a fair amount of data (about 15 TB) from a production RAID on the same server to the ZFS pool, to have some real data to test with.\r\n\r\nI'm exporting the ZFS dataset with the same options in /etc/exports as I use for the legacy RAID on the same server, and mounting it on the client also with the same options. /etc/exports on the server has:\r\n\r\n```\r\n/pool/data_14-1 *(rw,all_squash,anonuid=500,anongid=500,async,crossmnt)\r\n```\r\n\r\nAnd fstab on the client has:\r\n\r\n```\r\nshinjuku-ib:/pool/data_14-1  /pool/data_14-1  nfs4  rw,intr,hard,rsize=262144,wsize=262144,nolock,rdma,port=20049 0 0\r\n```\r\n\r\nBrowsing files on the dataset from the client generally works fine, but when I start reading files, I get RDMA errors in dmesg on both the client and the server, and while it seems that data eventually gets through, it's extremely slow.\r\n\r\nThe interesting thing is that this does not happen on the non-ZFS shares that are exported and mounted in the same way between the same server and the same client.\r\n\r\nI'm using the CentOS inbox Mellanox/OFED drivers, the server has a single-port Connect-IB FDR card (MCB193A) with the mlx5 driver, and one client is on CentOS 7.3 and has a dual-port ConnectX-VPI QDR card (MCX354A if I'm not totally mistaken) with the mlx4 driver, another client is on CentOS 7.4 and has the same Connect-IB card and driver as the server. They're connected through an FDR10 switch, the server running at FDR10, one client at QDR, the other client at FDR10.\r\n\r\nMounting the ZFS dataset from the same server to the same clients using NFS 4 over IPoIB (that is, normal IP over the same Infiniband adapters) works fine and produces no errors, even when all the export and mount parameters are otherwise the same. As mentioned, non-ZFS filesystems (the legacy RAIDs have ext4 filesystems) export using NFS over RDMA with no errors, even when the options are identical.\r\n\r\nI should also note that for now, I'm exporting with /etc/exports, to make sure all parameters are identical, but I've also tried adjusting exports and fstab parameters without it making any difference (including wsize/rsize), and I've also tried setting the exportnfs attribute on the dataset to make ZFS export it automatically. None of this makes any difference.\r\n\r\nThis may ultimately be an RDMA/OFED problem, but it seems to be a very specific interaction between ZFS and RDMA, so it might also be a ZFS bug...\r\n\r\nI'm a bit mystified in general, but would love pointers to getting more info out of this and figuring out what's going on.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nMost likely having a ZFS dataset and exporting it over NFS over RDMA, but I don't for the moment have multiple file servers with ZFS set up, although I might well test this with another server to see if it happens again.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\nServer errors from dmesg:\r\n\r\n```\r\n[Thu Oct 26 19:35:31 2017] mlx5_0:dump_cqe:262:(pid 0): dump error cqe\r\n[Thu Oct 26 19:35:31 2017] 00000000 00000000 00000000 00000000\r\n[Thu Oct 26 19:35:31 2017] 00000000 00000000 00000000 00000000\r\n[Thu Oct 26 19:35:31 2017] 00000000 00000000 00000000 00000000\r\n[Thu Oct 26 19:35:31 2017] 00000000 00008a12 0a000037 00038ad2\r\n[Thu Oct 26 19:35:31 2017] svcrdma: send: invalid request error (9/0x8a)\r\n```\r\n\r\nCentOS 7.3 client errors from dmesg:\r\n\r\n```\r\n[Thu Oct 26 19:35:19 2017] rpcrdma: connection to 10.20.0.1:20049 on mlx4_0, memreg 'frwr', 128 credits, 16 responders\r\n[Thu Oct 26 19:35:19 2017] RPC:       rpcrdma_recvcq_process_wc: rep ffff880fb3abc300: local length error\r\n```\r\n\r\nCentOS 7.4 client errors from dmesg:\r\n\r\n```\r\n[Fri Oct 27 13:08:38 2017] rpcrdma: connection to 10.20.0.1:20049 on mlx5_0, memreg 'frwr', 128 credits, 16 responders\r\n[Fri Oct 27 13:08:38 2017] mlx5_0:dump_cqe:262:(pid 0): dump error cqe\r\n[Fri Oct 27 13:08:38 2017] 00000000 00000000 00000000 00000000\r\n[Fri Oct 27 13:08:38 2017] 00000000 00000000 00000000 00000000\r\n[Fri Oct 27 13:08:38 2017] 00000000 00000000 00000000 00000000\r\n[Fri Oct 27 13:08:38 2017] 00000000 0000d701 00002518 0002f5e1\r\n[Fri Oct 27 13:08:38 2017] rpcrdma: Recv: local length error (1/0xd7)\r\n[Fri Oct 27 13:08:38 2017] rpcrdma: connection to 10.20.0.1:20049 closed (-103)\r\n```\r\n\r\nThe CentOS 7.4 client seems to be a bit more verbose, but mostly it just seems to be because the mlx5 driver reports errors a bit differently than the mlx4 driver, that is, it's because of the different adapter models. The mlx5 driver also reports this error far more frequently, several times per second, as opposed to a few times per minute from the mlx4 driver on the other client.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6795/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mbaldini1": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6791", "title": "ZFS module crash with   IP: dbuf_find+0x97/0x1a0 [zfs]", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |   Proxmox VE (based on Debian)\r\nDistribution Version    |    Proxmox 5.1 (based on Debian 9.2)\r\nLinux Kernel                 |   4.13.4-1-pve #1 SMP PVE 4.13.4-25\r\nArchitecture                 |   x64\r\nZFS Version                  |   0.7.2-1\r\nSPL Version                  |  0.7.2-1\r\n\r\n### Describe the problem you're observing\r\n\r\nI have this server with Proxmox that after update to ZFS 0.7.2  every night have a crash in ZFS module, after the crash the server is still up but disks are unusable. I have / in zfs rpool, virtual machines with kvm have block devices as zfs vols. After the crash,  virtual machines using only memory are working (I have a pfSense VM configured with /var and /tmp in RAM and that still works) but every disk access from proxmox and from virtual machines is impossible, i.e. trying a touch ~/prova will freeze bash forever. The only way to have the disks working again is to hard-reset the server.\r\n\r\nMy ZFS rpool has a very easy setup, two SATA hdd in RAID-1, I have a NVME Samsung 960 EVO that I used as ZIL and L2ARC but I disabled that after this problem arose, but did not change anything.\r\n\r\n```\r\n$ zpool status\r\n  pool: rpool\r\n state: ONLINE\r\n  scan: scrub repaired 0B in 3h39m with 0 errors on Sun Oct  8 04:03:54 2017\r\nconfig:\r\n\r\n        NAME        STATE     READ WRITE CKSUM\r\n        rpool       ONLINE       0     0     0\r\n          mirror-0  ONLINE       0     0     0\r\n            sdc2    ONLINE       0     0     0\r\n            sdg2    ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n\r\n\r\n$ zdb\r\nrpool:\r\n    version: 5000\r\n    name: 'rpool'\r\n    state: 0\r\n    txg: 6375513\r\n    pool_guid: 197416030411497508\r\n    errata: 0\r\n    hostid: 2831157250\r\n    hostname: 'pve-hs-main'\r\n    com.delphix:has_per_vdev_zaps\r\n    vdev_children: 1\r\n    vdev_tree:\r\n        type: 'root'\r\n        id: 0\r\n        guid: 197416030411497508\r\n        children[0]:\r\n            type: 'mirror'\r\n            id: 0\r\n            guid: 720290835225638711\r\n            metaslab_array: 35\r\n            metaslab_shift: 33\r\n            ashift: 12\r\n            asize: 1000190509056\r\n            is_log: 0\r\n            create_txg: 4\r\n            com.delphix:vdev_zap_top: 59\r\n            children[0]:\r\n                type: 'disk'\r\n                id: 0\r\n                guid: 3208937268298614423\r\n                path: '/dev/sdc2'\r\n                whole_disk: 0\r\n                DTL: 1343\r\n                create_txg: 4\r\n                com.delphix:vdev_zap_leaf: 60\r\n            children[1]:\r\n                type: 'disk'\r\n                id: 1\r\n                guid: 12783127680851901291\r\n                path: '/dev/sdg2'\r\n                whole_disk: 0\r\n                DTL: 1342\r\n                create_txg: 4\r\n                com.delphix:vdev_zap_leaf: 61\r\n    features_for_read:\r\n        com.delphix:hole_birth\r\n        com.delphix:embedded_data\r\n\r\n\r\n$ arc_summary\r\n------------------------------------------------------------------------\r\nZFS Subsystem Report                            Fri Oct 27 08:50:20 2017\r\nARC Summary: (HEALTHY)\r\n        Memory Throttle Count:                  0\r\n\r\nARC Misc:\r\n        Deleted:                                33.21m\r\n        Mutex Misses:                           11.01k\r\n        Evict Skips:                            11.01k\r\n\r\nARC Size:                               100.07% 6.00    GiB\r\n        Target Size: (Adaptive)         100.00% 6.00    GiB\r\n        Min Size (Hard Limit):          16.67%  1.00    GiB\r\n        Max Size (High Water):          6:1     6.00    GiB\r\n\r\nARC Size Breakdown:\r\n        Recently Used Cache Size:       2.48%   152.36  MiB\r\n        Frequently Used Cache Size:     97.52%  5.86    GiB\r\n\r\nARC Hash Breakdown:\r\n        Elements Max:                           1.46m\r\n        Elements Current:               95.43%  1.39m\r\n        Collisions:                             3.19m\r\n        Chain Max:                              5\r\n        Chains:                                 87.14k\r\n\r\nARC Total accesses:                                     54.47m\r\n        Cache Hit Ratio:                35.82%  19.51m\r\n        Cache Miss Ratio:               64.18%  34.96m\r\n        Actual Hit Ratio:               29.88%  16.27m\r\n\r\n        Data Demand Efficiency:         49.40%  31.93m\r\n        Data Prefetch Efficiency:       13.90%  21.39m\r\n\r\n        CACHE HITS BY CACHE LIST:\r\n          Anonymously Used:             16.10%  3.14m\r\n          Most Recently Used:           81.50%  15.90m\r\n          Most Frequently Used:         1.91%   372.93k\r\n          Most Recently Used Ghost:     0.42%   81.39k\r\n          Most Frequently Used Ghost:   0.06%   12.19k\r\n\r\n        CACHE HITS BY DATA TYPE:\r\n          Demand Data:                  80.84%  15.77m\r\n          Prefetch Data:                15.23%  2.97m\r\n          Demand Metadata:              2.26%   441.31k\r\n          Prefetch Metadata:            1.66%   324.62k\r\n\r\n        CACHE MISSES BY DATA TYPE:\r\n          Demand Data:                  46.21%  16.15m\r\n          Prefetch Data:                52.68%  18.42m\r\n          Demand Metadata:              0.42%   146.63k\r\n          Prefetch Metadata:            0.70%   244.03k\r\n\r\n\r\nDMU Prefetch Efficiency:                                        6.31m\r\n        Hit Ratio:                      32.55%  2.06m\r\n        Miss Ratio:                     67.45%  4.26m\r\n\r\n\r\n\r\nZFS Tunable:\r\n        zvol_volmode                                      1\r\n        l2arc_headroom                                    2\r\n        dbuf_cache_max_shift                              5\r\n        zfs_free_leak_on_eio                              0\r\n        zfs_free_max_blocks                               100000\r\n        zfs_read_chunk_size                               1048576\r\n        metaslab_preload_enabled                          1\r\n        zfs_dedup_prefetch                                0\r\n        zfs_txg_history                                   0\r\n        zfs_scrub_delay                                   4\r\n        zfs_vdev_async_read_max_active                    3\r\n        zfs_read_history                                  0\r\n        zfs_arc_sys_free                                  0\r\n        l2arc_write_max                                   8388608\r\n        zil_slog_bulk                                     786432\r\n        zfs_dbuf_state_index                              0\r\n        zfs_sync_taskq_batch_pct                          75\r\n        metaslab_debug_unload                             0\r\n        zvol_inhibit_dev                                  0\r\n        zfs_abd_scatter_enabled                           1\r\n        zfs_arc_pc_percent                                0\r\n        zfetch_max_streams                                8\r\n        zfs_recover                                       0\r\n        metaslab_fragmentation_factor_enabled             1\r\n        zfs_deadman_checktime_ms                          5000\r\n        zfs_sync_pass_rewrite                             2\r\n        zfs_object_mutex_size                             64\r\n        zfs_arc_min_prefetch_lifespan                     0\r\n        zfs_arc_meta_prune                                10000\r\n        zfs_read_history_hits                             0\r\n        zfetch_max_distance                               8388608\r\n        l2arc_norw                                        0\r\n        zfs_dirty_data_max_percent                        10\r\n        zfs_per_txg_dirty_frees_percent                   30\r\n        zfs_arc_meta_min                                  0\r\n        metaslabs_per_vdev                                200\r\n        zfs_arc_meta_adjust_restarts                      4096\r\n        spa_load_verify_maxinflight                       10000\r\n        spa_load_verify_metadata                          1\r\n        zfs_multihost_history                             0\r\n        zfs_send_corrupt_data                             0\r\n        zfs_delay_min_dirty_percent                       60\r\n        zfs_vdev_sync_read_max_active                     10\r\n        zfs_dbgmsg_enable                                 0\r\n        zfs_metaslab_segment_weight_enabled               1\r\n        zio_requeue_io_start_cut_in_line                  1\r\n        l2arc_headroom_boost                              200\r\n        zfs_zevent_cols                                   80\r\n        zfs_dmu_offset_next_sync                          0\r\n        spa_config_path                                   /etc/zfs/zpool.cache\r\n        zfs_vdev_cache_size                               0\r\n        dbuf_cache_hiwater_pct                            10\r\n        zfs_multihost_interval                            1000\r\n        zfs_multihost_fail_intervals                      5\r\n        zio_dva_throttle_enabled                          1\r\n        zfs_vdev_sync_write_min_active                    10\r\n        zfs_vdev_scrub_max_active                         2\r\n        ignore_hole_birth                                 1\r\n        zvol_major                                        230\r\n        zil_replay_disable                                0\r\n        zfs_dirty_data_max_max_percent                    25\r\n        zfs_expire_snapshot                               300\r\n        zfs_sync_pass_deferred_free                       2\r\n        spa_asize_inflation                               24\r\n        dmu_object_alloc_chunk_shift                      7\r\n        zfs_vdev_mirror_rotating_seek_offset              1048576\r\n        l2arc_feed_secs                                   1\r\n        zfs_autoimport_disable                            1\r\n        zfs_arc_p_aggressive_disable                      1\r\n        zfs_zevent_len_max                                64\r\n        zfs_arc_meta_limit_percent                        75\r\n        l2arc_noprefetch                                  1\r\n        zfs_vdev_raidz_impl                               [fastest] original scalar sse2 ssse3 avx2\r\n        zfs_arc_meta_limit                                0\r\n        zfs_flags                                         0\r\n        zfs_dirty_data_max_max                            4294967296\r\n        zfs_arc_average_blocksize                         8192\r\n        zfs_vdev_cache_bshift                             16\r\n        zfs_vdev_async_read_min_active                    1\r\n        zfs_arc_dnode_reduce_percent                      10\r\n        zfs_free_bpobj_enabled                            1\r\n        zfs_arc_grow_retry                                0\r\n        zfs_vdev_mirror_rotating_inc                      0\r\n        l2arc_feed_again                                  1\r\n        zfs_vdev_mirror_non_rotating_inc                  0\r\n        zfs_arc_lotsfree_percent                          10\r\n        zfs_zevent_console                                0\r\n        zvol_prefetch_bytes                               131072\r\n        zfs_free_min_time_ms                              1000\r\n        zfs_arc_dnode_limit_percent                       10\r\n        zio_taskq_batch_pct                               75\r\n        dbuf_cache_max_bytes                              104857600\r\n        spa_load_verify_data                              1\r\n        zfs_delete_blocks                                 20480\r\n        zfs_vdev_mirror_non_rotating_seek_inc             1\r\n        zfs_multihost_import_intervals                    10\r\n        zfs_dirty_data_max                                4209932697\r\n        zfs_vdev_async_write_max_active                   10\r\n        zfs_dbgmsg_maxsize                                4194304\r\n        zfs_nocacheflush                                  0\r\n        zfetch_array_rd_sz                                1048576\r\n        zfs_arc_meta_strategy                             1\r\n        zfs_dirty_data_sync                               67108864\r\n        zvol_max_discard_blocks                           16384\r\n        zvol_threads                                      32\r\n        zfs_vdev_async_write_active_max_dirty_percent     60\r\n        zfs_arc_p_dampener_disable                        1\r\n        zfs_txg_timeout                                   5\r\n        metaslab_aliquot                                  524288\r\n        zfs_mdcomp_disable                                0\r\n        zfs_vdev_sync_read_min_active                     10\r\n        zfs_arc_dnode_limit                               0\r\n        dbuf_cache_lowater_pct                            10\r\n        zfs_abd_scatter_max_order                         10\r\n        metaslab_debug_load                               0\r\n        zfs_vdev_aggregation_limit                        131072\r\n        metaslab_lba_weighting_enabled                    1\r\n        zfs_vdev_scheduler                                noop\r\n        zfs_vdev_scrub_min_active                         1\r\n        zfs_no_scrub_io                                   0\r\n        zfs_vdev_cache_max                                16384\r\n        zfs_scan_idle                                     50\r\n        zfs_arc_shrink_shift                              0\r\n        spa_slop_shift                                    5\r\n        zfs_vdev_mirror_rotating_seek_inc                 5\r\n        zfs_deadman_synctime_ms                           1000000\r\n        send_holes_without_birth_time                     1\r\n        metaslab_bias_enabled                             1\r\n        zvol_request_sync                                 0\r\n        zfs_admin_snapshot                                1\r\n        zfs_no_scrub_prefetch                             0\r\n        zfs_metaslab_fragmentation_threshold              70\r\n        zfs_max_recordsize                                1048576\r\n        zfs_arc_min                                       1073741824\r\n        zfs_nopwrite_enabled                              1\r\n        zfs_arc_p_min_shift                               0\r\n        zfs_multilist_num_sublists                        0\r\n        zfs_vdev_queue_depth_pct                          1000\r\n        zfs_mg_fragmentation_threshold                    85\r\n        l2arc_write_boost                                 8388608\r\n        zfs_prefetch_disable                              0\r\n        l2arc_feed_min_ms                                 200\r\n        zio_delay_max                                     30000\r\n        zfs_vdev_write_gap_limit                          4096\r\n        zfs_pd_bytes_max                                  52428800\r\n        zfs_scan_min_time_ms                              1000\r\n        zfs_resilver_min_time_ms                          3000\r\n        zfs_delay_scale                                   500000\r\n        zfs_vdev_async_write_active_min_dirty_percent     30\r\n        zfs_vdev_sync_write_max_active                    10\r\n        zfs_mg_noalloc_threshold                          0\r\n        zfs_deadman_enabled                               1\r\n        zfs_resilver_delay                                2\r\n        zfs_metaslab_switch_threshold                     2\r\n        zfs_arc_max                                       6442450944\r\n        zfs_top_maxinflight                               32\r\n        zfetch_min_sec_reap                               2\r\n        zfs_immediate_write_sz                            32768\r\n        zfs_vdev_async_write_min_active                   2\r\n        zfs_sync_pass_dont_compress                       5\r\n        zfs_vdev_read_gap_limit                           32768\r\n        zfs_compressed_arc_enabled                        1\r\n        zfs_vdev_max_active                               1000\r\n\r\n```\r\n\r\n\r\n\r\n### Describe how to reproduce the problem\r\n\r\nHard to say, usually hte problem happens during the night, the only scheduled process running during the night is vzdump (Proxmox own VM backup) but I tried to launch vzdump during day and it went ok. It seems a random crash after some hours of use, in fact after I hard reset the server there are no problems in using it until the night, when it crashes in a random time from 23.00 (11.00 pm) and 4.00 am\r\n\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\nBefore hard-reset the server, I can view dmesg\r\n\r\n```\r\nkern  :alert : [Oct26 23:50] BUG: unable to handle kernel paging request at 0000000000c74063\r\nkern  :alert : [  +0.000051] IP: dbuf_find+0x97/0x1a0 [zfs]\r\nkern  :warn  : [  +0.000012] PGD 0\r\nkern  :warn  : [  +0.000000] P4D 0\r\n\r\nkern  :warn  : [  +0.000020] Oops: 0000 [#1] SMP\r\nkern  :warn  : [  +0.000009] Modules linked in: tcp_diag inet_diag ip_set xfs ip6table_filter ip6_tables libcrc32c iptable_filter bonding softdog nfnetlink_log nfnetlink ppdev intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd glue_helper cryptd intel_cstate intel_rapl_perf snd_pcm snd_timer snd soundcore pcspkr i915 hci_uart btbcm serdev btqca btintel bluetooth drm_kms_helper ecdh_generic wmi drm i2c_algo_bit fb_sys_fops syscopyarea sysfillrect sysimgblt parport_pc mei_me parport intel_lpss_acpi mei intel_lpss acpi_als shpchp video tpm_infineon kfifo_buf industrialio acpi_pad mac_hid vhost_net vhost tap ib_iser rdma_cm iw_cm ib_cm sunrpc ib_core iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi\r\nkern  :warn  : [  +0.000180]  ip_tables x_tables autofs4 zfs(PO) zunicode(PO) zavl(PO) icp(PO) zcommon(PO) znvpair(PO) spl(O) btrfs xor raid6_pq uas usb_storage i2c_i801 r8169 8139too 8139cp mii ahci libahci pinctrl_sunrisepoint pinctrl_intel i2c_hid hid\r\nkern  :warn  : [  +0.000057] CPU: 0 PID: 261 Comm: zvol Tainted: P           O    4.13.4-1-pve #1\r\nkern  :warn  : [  +0.000018] Hardware name: Gigabyte Technology Co., Ltd. H170-HD3/H170-HD3-CF, BIOS F20 11/07/2016\r\nkern  :warn  : [  +0.000022] task: ffff9c91476a8000 task.stack: ffffbce286d94000\r\nkern  :warn  : [  +0.000031] RIP: 0010:dbuf_find+0x97/0x1a0 [zfs]\r\nkern  :warn  : [  +0.000013] RSP: 0018:ffffbce286d97b00 EFLAGS: 00010206\r\nkern  :warn  : [  +0.000014] RAX: ffff9c91476a8000 RBX: 0000000000c74063 RCX: 0000000000268986\r\nkern  :warn  : [  +0.000017] RDX: ffff9c91476a8000 RSI: ffffbce294002000 RDI: ffffffffc05a40d0\r\nkern  :warn  : [  +0.000017] RBP: ffffbce286d97b58 R08: 0000000000000001 R09: ffff9c907aae6800\r\nkern  :warn  : [  +0.000017] R10: ffffecf22164d940 R11: 0000000000000000 R12: ffffffffc05a40d0\r\nkern  :warn  : [  +0.000017] R13: 0000000000054010 R14: 0000000000054000 R15: ffff9c91476a8000\r\nkern  :warn  : [  +0.000018] FS:  0000000000000000(0000) GS:ffff9c917ec00000(0000) knlGS:0000000000000000\r\nkern  :warn  : [  +0.000019] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nkern  :warn  : [  +0.000015] CR2: 0000000000c74063 CR3: 000000093a9d2000 CR4: 00000000003426f0\r\nkern  :warn  : [  +0.000017] Call Trace:\r\nkern  :warn  : [  +0.000023]  dbuf_prefetch+0xc4/0x510 [zfs]\r\nkern  :warn  : [  +0.000029]  dmu_zfetch+0x29e/0x4a0 [zfs]\r\nkern  :warn  : [  +0.000027]  dmu_buf_hold_array_by_dnode+0x40b/0x470 [zfs]\r\nkern  :warn  : [  +0.000029]  dmu_read_uio_dnode+0x49/0xf0 [zfs]\r\nkern  :warn  : [  +0.000035]  zvol_read+0xe3/0x280 [zfs]\r\nkern  :warn  : [  +0.000014]  taskq_thread+0x25e/0x460 [spl]\r\nkern  :warn  : [  +0.000013]  ? wake_up_q+0x80/0x80\r\nkern  :warn  : [  +0.000010]  kthread+0x109/0x140\r\nkern  :warn  : [  +0.000616]  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\nkern  :warn  : [  +0.000637]  ? kthread_create_on_node+0x70/0x70\r\nkern  :warn  : [  +0.000631]  ret_from_fork+0x25/0x30\r\nkern  :warn  : [  +0.000611] Code: 00 00 49 89 85 e8 00 55 c0 4c 8b 45 c8 4a 8b 1c fe 49 89 c7 48 85 db 75 12 e9 a6 00 00 00 48 8b 5b 38 48 85 db 0f 84 99 00 00 00 <4c> 3b 03 75 ee 4c 3b 4b 20 75 e8 0f b6 45 c7 3a 43 50 75 df 48\r\nkern  :alert : [  +0.001327] RIP: dbuf_find+0x97/0x1a0 [zfs] RSP: ffffbce286d97b00\r\nkern  :warn  : [  +0.000648] CR2: 0000000000c74063\r\nkern  :warn  : [  +0.000649] ---[ end trace c2004997d5d6b6f4 ]---\r\n\r\n\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6791/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "BloodBlight": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6783", "title": "Large Deletes & Memory Consumption", "body": "\r\n\r\nFirst time posting to GitHub, be gentle.  :)\r\n\r\n### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 16.04 LTS / 16.10 on USB\r\nLinux Kernel                 | Will post soon, current in LTS / 4.8.0-22 on USB\r\nArchitecture                 | Intel Xeon\r\nZFS Version                  | Will post soon, current in LTS / zfsutils-linux 0.6.5.8 on USB\r\nSPL Version                  | Will post soon, current in LTS / ?\r\nRAM:\t\t|\t96GBs of ECC RAM\r\n\r\n### Other Config Information\r\n- 2 Mirrored VDEVs for a total of 10TBs usable.\r\n- L2ARC, metadata only\r\n- Dedup and compression are enabled. (Ya I know\u2026  But I have 96GBs, that should be plenty).\r\n- About 11TBs of data and about 2TBs free (after dedup and compression).\r\n- Just a handful of apps and users.  No major load on the system (it just hosts files).\r\n- Exact details to follow once I can get back in.\r\n- Will post these as soon as I can:\r\n- modinfo zfs | grep -iw version\r\n- modinfo spl | grep -iw version \r\n\r\n### Trigger\r\nDelete a large file 1TB+\r\n\r\n### Issue\r\nSystem will slowly consume all memory over the course of several hours (about 12) and hard lock.  This happens both after the delete and while importing zpool on reboot.\r\n\r\nI have had this happen before, I added a 32GB swap file (on SSD) and that seemed to help.  It eventually cleared up after several attempts to reboot (took about two weeks, 12 hours a pop).  I made the assumption that the delete was working, but something was causing the memory to not be released.  So eventually...\r\n\r\nThis time I booted off of a live boot USB, added zfs-utils and I was surprised that not only did it attempt to mount the zpool right away (while in apt), but after about an hour it succeeded!\r\n\r\nI thought \u201cCool, it cleared!\u201d and rebooted.  No go, 12 hours later, out of memory and locked (still at the boot screen with an out of memory error).\r\n\r\nAlright, booted back into the USB stick, again, hung for about an hour, then booted!  \u201cAlright, that\u2019s odd.\u201d\r\n\r\nAt this point I noticed that the mount point for the tank was already taken and I could not access the volume.  So, I exported the zpool, took a bit and completed.  Moved the folder and re-mount.  Watched the memory slowly climb and lock after 12 hours.\r\n\r\nI move the USB boot to another system and removed ZFS.  I now have the box booted again, re-blocked the mount point and have just re-installed ZFS.  I am waiting for the mount to complete.  I am hopping it will complete in an hour or so.\r\n\r\nFYI, I will be on vacation for several days and unable to access the server after tomorrow.\r\n\r\nWhat else should I grab as I am limited in what I can get right now.?  Is this a known issue?  Should I go to a newer build?\r\n\r\nI have looked at several other open and closed issues including:\r\nhttps://github.com/zfsonlinux/zfs/issues/3725\r\nhttps://github.com/zfsonlinux/zfs/pull/5706\r\nhttps://github.com/zfsonlinux/zfs/pull/5449\r\nhttps://github.com/zfsonlinux/zfs/issues/3976\r\nhttps://github.com/zfsonlinux/zfs/issues/5923", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6783/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mahmoudhanafi": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6782", "title": "zfs build should check for kernel varients.", "body": "\r\nDistribution Name       centos7 or redhat7\r\n\r\nWhen building zfs it looks for kernel-devel. But if you have a kernel varient like centosplus kernel-plus-devel it fails.\r\n\r\ninstall kernel-plus and kernel-plus-devel and try to build zfs\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6782/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "marcinkuk": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6781", "title": "ZFS v0.7.2 call trace - zfs_range_lock issue - zpool hungs", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Proxmox (Debian)\r\nDistribution Version    | Proxmox 5.0 - Debian 9.1\r\nLinux Kernel                 | 4.13.4-1-pve\r\nArchitecture                 | x86_64\r\nZFS Version                  | v0.7.2-1\r\nSPL Version                  | v0.7.2-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nZpool functionality hungs - upload > 100\r\n\r\n### Describe how to reproduce the problem\r\nDon't know how to reproduce. Just use server.\r\nI have 4 similar configured servers. On 2 of them I have this issue average 1 time per hour.\r\nOn 2 others servers never happened.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n```\r\n[ 2538.605247] general protection fault: 0000 [#1] SMP\r\n[ 2538.605259] Modules linked in: xt_tcpudp xt_multiport bluetooth ecdh_generic veth ip_set ip6table_filter ip6_tables iptable_filter softdog nfnetlink_log nfnetlink adt7475 hwmon_vid intel_powerclamp snd_hda_codec_analog snd_hda_codec_generic coretemp kvm_intel kvm dell_wmi dell_smbios irqbypass dcdbas gpio_ich ppdev sparse_keymap wmi_bmof dell_smm_hwmon nouveau intel_cstate mxm_wmi video snd_hda_intel ttm pcspkr serio_raw snd_hda_codec drm_kms_helper snd_hda_core lpc_ich snd_hwdep drm snd_pcm snd_timer i2c_algo_bit fb_sys_fops syscopyarea snd sysfillrect sysimgblt soundcore i7core_edac parport_pc parport wmi shpchp mac_hid vhost_net vhost tap sunrpc ib_iser rdma_cm iw_cm ib_cm ib_core iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi ip_tables x_tables autofs4 zfs(PO) zunicode(PO) zavl(PO) icp(PO)\r\n[ 2538.605316]  zcommon(PO) znvpair(PO) spl(O) btrfs xor raid6_pq firewire_ohci psmouse ahci libahci i2c_i801 firewire_core mptsas crc_itu_t tg3 mptscsih ptp mptbase pps_core scsi_transport_sas\r\n[ 2538.605333] CPU: 0 PID: 1679 Comm: z_wr_iss Tainted: P           O    4.13.4-1-pve #1\r\n[ 2538.605337] Hardware name: Dell Inc. Precision WorkStation T7500  /06FW8P, BIOS A16 05/28/2013\r\n[ 2538.605342] task: ffff8e074c330000 task.stack: ffffa78a13504000\r\n[ 2538.605414] RIP: 0010:zfs_range_compare+0x5/0x30 [zfs]\r\n[ 2538.605418] RSP: 0018:ffffa78a13507a80 EFLAGS: 00010286\r\n[ 2538.605422] RAX: 0000000000000000 RBX: bb8b4cfffffc4ce1 RCX: 0000000011c30000\r\n[ 2538.605426] RDX: 0000000000000000 RSI: bb8b4cfffffc4ce1 RDI: ffff8e0562ba4600\r\n[ 2538.605430] RBP: ffffa78a13507ab8 R08: ffff8e075241f500 R09: ffff8e0752007180\r\n[ 2538.605434] R10: ffff8e0562ba4600 R11: 0000000000b80000 R12: ffff8e074613a560\r\n[ 2538.605438] R13: ffff8e0562ba4600 R14: bb8b4cfffffc4ce9 R15: bb8b4cfffffc4ce9\r\n[ 2538.605443] FS:  0000000000000000(0000) GS:ffff8e0752400000(0000) knlGS:0000000000000000\r\n[ 2538.605447] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[ 2538.605451] CR2: 00007f7b9737b148 CR3: 000000021b89a000 CR4: 00000000000006f0\r\n[ 2538.605456] Call Trace:\r\n[ 2538.605461]  ? avl_add+0x59/0x90 [zavl]\r\n[ 2538.605494]  zfs_range_lock+0x42b/0x5c0 [zfs]\r\n[ 2538.605536]  zvol_request+0x1e5/0x300 [zfs]\r\n[ 2538.605542]  generic_make_request+0x125/0x300\r\n[ 2538.605546]  submit_bio+0x73/0x150\r\n[ 2538.605549]  ? submit_bio+0x73/0x150\r\n[ 2538.605582]  vdev_disk_io_start+0x616/0x700 [zfs]\r\n[ 2538.605615]  zio_vdev_io_start+0xbf/0x340 [zfs]\r\n[ 2538.605648]  ? zio_vdev_io_start+0xbf/0x340 [zfs]\r\n[ 2538.605681]  ? vdev_config_sync+0x190/0x190 [zfs]\r\n[ 2538.605714]  zio_nowait+0xb6/0x150 [zfs]\r\n[ 2538.605747]  vdev_mirror_io_start+0xa1/0x180 [zfs]\r\n[ 2538.605781]  zio_vdev_io_start+0x223/0x340 [zfs]\r\n[ 2538.605931]  ? taskq_member+0x18/0x30 [spl]\r\n[ 2538.606185]  zio_execute+0x8a/0xe0 [zfs]\r\n[ 2538.606675]  taskq_thread+0x25e/0x460 [spl]\r\n[ 2538.607178]  ? wake_up_q+0x80/0x80\r\n[ 2538.607680]  kthread+0x109/0x140\r\n[ 2538.608163]  ? taskq_thread_should_stop+0x70/0x70 [spl]\r\n[ 2538.608651]  ? kthread_create_on_node+0x70/0x70\r\n[ 2538.609155]  ret_from_fork+0x25/0x30\r\n[ 2538.609653] Code: 48 c7 c7 88 91 53 c0 41 bf 5f 00 00 00 e8 34 cf fe ff e9 59 fd ff ff e8 ba 91 20 cd 66 2e 0f 1f 84 00 00 00 00 00 66 66 66 66 90 <48> 8b 56 20 48 8b 4f 20 31 c0 55 48 39 d1 48 89 e5 0f 92 c2 0f \r\n[ 2538.610745] RIP: zfs_range_compare+0x5/0x30 [zfs] RSP: ffffa78a13507a80\r\n[ 2538.611281] ---[ end trace 36702b944b69d225 ]---\r\n```\r\n\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6781/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kingneutron": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6778", "title": "Cannot compile 0.7.3 on Antix 16-64 bit, kernel 4.8 (0.7.0 compiles OK)", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Antix 16 64-bit \r\nDistribution Version    | Debian Stable/Jessie\r\nLinux Kernel                 | 4.8 (from distro repo)\r\nArchitecture                 | AMD64\r\nZFS Version                  |  modinfo zfs  0.7.0-139_gf8cd871\r\nSPL Version                  |  modinfo spl   0.7.3-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nCannot compile ZFS from git 0.7.3 on Antix 16-64 bit, kernel 4.8 (0.7.0 compiles and runs OK)\r\n\r\n--Script attached, logfile with errors attached\r\n\r\n  CC       zap_micro.lo\r\n  CC       zfeature.lo\r\nMakefile:539: recipe for target 'all-recursive' failed\r\nMakefile:712: recipe for target 'all-recursive' failed\r\nMakefile:580: recipe for target 'all' failed\r\n\r\n\r\n### Describe how to reproduce the problem\r\nRun attached script twice - 1st run compiles 0.7.0 fine, then will get a compile error for 0.7.3 (please note I am not proficient at git, apologies in advance)\r\n[zfs-compile-error.tar.gz](https://github.com/zfsonlinux/zfs/files/1408721/zfs-compile-error.tar.gz)\r\n\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6778/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Infinisil": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6773", "title": "Receiving with -n flag reports that it would fail, even though it wouldn't", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | NixOS\r\nDistribution Version    | 18.03.git.7cf6e91b1d (Impala)\r\nLinux Kernel                 | 4.9.56\r\nArchitecture                 | x86-64\r\nZFS Version                  | 0.7.2-1\r\nSPL Version                  | 0.7.2-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nUsing zfs receive -n with a -R stream being sent makes it output an error about the receiving filesystem missing, running it without -n works though.\r\n\r\nThis was also confirmed by @rincebrain with version 0.6.5.11.\r\n\r\n### Describe how to reproduce the problem\r\n```\r\n$ zfs create foo/bar\r\n$ zfs snap foo/bar@1\r\n$ zfs snap foo/bar@2\r\n$ zfs send -R foo/bar@2 | zfs recv -n baz\r\ncannot receive incremental stream: destination 'baz' does not exist\r\n$ zfs send -R foo/bar@2 | zfs recv baz\r\n$ echo $?\r\n0\r\n```\r\n\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6773/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Kramerican": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6763", "title": "Feature request: zfs disk i/o monitoring tool on Ubuntu/Debian", "body": "This is a feature request for a simple disk i/o monitoring tool for zfs on Ubuntu/Debian\r\n\r\n**Problem:**\r\n\r\nCurrently there is no easy way to identify which datapools are using the most disk i/o. This can be problematic in e.g. LXD containerized environments (which is my use case) where you may have 3rd parties running software in containers which might be misbehaving.\r\n\r\nAs a sysadmin it is currently a tremendous pain to track down which LXD container is (ab)using disk i/o.\r\n\r\n**Wanted:**\r\n\r\nA simple tool which gives a clear picture of what datapools are doing at the moment. Think a simpler version of iotop would suffice. Something like:\r\n\r\n```\r\n~# zfstop -d 5\r\n\r\nNAME READ WRITE\r\nlxd/containers/test 234.4 12.4\r\nlxd/containers/foo 4.4 2.4\r\n... etc.\r\n```\r\n\r\n\r\nHere I'm guessing a tool can be constructed which shows read/write in mb/s and polls for data every x seconds, therefore my fanciful -d 5 \"iotop-like\" example.\r\n\r\n**Bounty:**\r\n\r\nI am willing to sponsor the development of this tool, if it is at all possible to build, for contribution back to the community. If you are interested in building something like this, please post your intended methodology. I would like at least a single consenting voice in the community to agree with your chosen solution before committing. Feel free to contact me privately with regards to compensation.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6763/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mklemm2": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6759", "title": "ZFS hangs after kernel oops", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Arch Linux\r\nDistribution Version    |  rolling\r\nLinux Kernel                 |  4.13.4-1-ARCH\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.0-104_g57f4ef2e8\r\nSPL Version                  | 0.7.0-13_ge8474f9\r\n\r\n### Describe the problem you're observing\r\nWhile copying 100G data to the system via scp, the transfer suddenly stalled. I logged into the system and found ZFS to be the root cause. Every process that tries to access the same zpool just hangs indefinitely. Accessing other zpools works just fine.\r\n\r\n### Describe how to reproduce the problem\r\nNo idea.\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\n```\r\n[218075.896295] BUG: unable to handle kernel NULL pointer dereference at           (null)\r\n[218075.896778] IP: arc_release+0x1d/0x740 [zfs]\r\n[218075.897014] PGD 0 \r\n[218075.897015] P4D 0 \r\n\r\n[218075.897337] Oops: 0000 [#1] PREEMPT SMP\r\n[218075.897556] Modules linked in: tun devlink veth xt_CHECKSUM xt_tcpudp iptable_mangle ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack libcrc32c crc32c_generic iptable_filter fuse bridge stp llc amdkfd amd_iommu_v2 radeon snd_hda_codec_realtek snd_hda_codec_generic intel_rapl sb_edac x86_pkg_temp_thermal intel_powerclamp snd_hda_intel mei_wdt ttm coretemp drm_kms_helper kvm_intel snd_hda_codec iTCO_wdt hp_wmi iTCO_vendor_support snd_hda_core sparse_keymap wmi_bmof rfkill kvm snd_hwdep raid1 isci mpt3sas nvme drm snd_pcm irqbypass nvme_core firewire_ohci snd_timer tpm_infineon raid_class evdev libsas intel_cstate mei_me input_leds tpm_tis syscopyarea ehci_pci sysfillrect tpm_tis_core md_mod sysimgblt snd led_class firewire_core fb_sys_fops\r\n[218075.899057] e1000e: enp1s0 NIC Link is Up 1000 Mbps Full Duplex, Flow Control: Rx/Tx\r\n[218075.902008]  ioatdma intel_rapl_perf psmouse mac_hid pcspkr i2c_algo_bit crc_itu_t soundcore ehci_hcd mei shpchp i2c_i801 lpc_ich dca tpm wmi button sch_fq_codel ip_tables x_tables algif_skcipher af_alg dm_crypt dm_mod dax serio_raw atkbd libps2 crct10dif_pclmul crc32_pclmul crc32c_intel ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd glue_helper cryptd xhci_pci xhci_hcd usbcore usb_common ata_generic pata_acpi i8042 serio e1000e(O) ptp pps_core zfs(PO) zunicode(PO) zavl(PO) icp(PO) sd_mod zcommon(PO) znvpair(PO) spl(O) mptsas scsi_transport_sas mptscsih mptbase ahci libahci libata scsi_mod lz4 lz4_compress\r\n[218075.905112] CPU: 4 PID: 28095 Comm: rs:main Q:Reg Tainted: P           O    4.13.4-1-ARCH #1\r\n[218075.905577] Hardware name: Hewlett-Packard HP Z820 Workstation/158B, BIOS J63 v03.69 03/25/2014\r\n[218075.906067] task: ffff8953517eda00 task.stack: ffffb6bfa5960000\r\n[218075.906419] RIP: 0010:arc_release+0x1d/0x740 [zfs]\r\n[218075.906681] RSP: 0018:ffffb6bfa5963948 EFLAGS: 00010296\r\n[218075.906978] RAX: 0000000000000010 RBX: ffff895258afee80 RCX: 0000000000000000\r\n[218075.907381] RDX: 0000000000000000 RSI: ffff894a01a810a0 RDI: 0000000000000000\r\n[218075.907783] RBP: ffffb6bfa59639b8 R08: ffff894a39407000 R09: ffff8947f4afd400\r\n[218075.908185] R10: ffff895258afee80 R11: 0000000000000001 R12: ffff8948b97fafc0\r\n[218075.908587] R13: ffff8947f4afd400 R14: 0000000000000000 R15: ffff894a01a81190\r\n[218075.908990] FS:  00007fe794945700(0000) GS:ffff894a39b00000(0000) knlGS:0000000000000000\r\n[218075.909444] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[218075.909769] CR2: 0000000000000000 CR3: 00000012e5edd000 CR4: 00000000000406e0\r\n[218075.910172] Call Trace:\r\n[218075.910319]  ? spl_kmem_zalloc+0xc2/0x170 [spl]\r\n[218075.910575]  ? spl_kmem_zalloc+0xc2/0x170 [spl]\r\n[218075.910843]  dbuf_dirty+0x6a6/0x840 [zfs]\r\n[218075.911076]  dmu_buf_will_dirty_impl+0x116/0x130 [zfs]\r\n[218075.911369]  dbuf_new_size+0x67/0x180 [zfs]\r\n[218075.911615]  dnode_set_blksz+0x2f1/0x310 [zfs]\r\n[218075.911872]  dmu_object_set_blocksize+0x50/0x90 [zfs]\r\n[218075.912167]  zfs_grow_blocksize+0x70/0xa0 [zfs]\r\n[218075.912432]  zfs_write+0x8a9/0xd30 [zfs]\r\n[218075.912650]  ? __switch_to+0x1fc/0x4d0\r\n[218075.912862]  ? schedule+0x3d/0x90\r\n[218075.913051]  ? futex_wait_queue_me+0xca/0x120\r\n[218075.913315]  zpl_write_common_iovec+0x8c/0xe0 [zfs]\r\n[218075.913600]  zpl_iter_write+0xae/0xe0 [zfs]\r\n[218075.913835]  __vfs_write+0xf4/0x150\r\n[218075.914031]  vfs_write+0xb1/0x1a0\r\n[218075.914222]  SyS_write+0x55/0xc0\r\n[218075.914409]  do_syscall_64+0x54/0xc0\r\n[218075.914612]  entry_SYSCALL64_slow_path+0x25/0x25\r\n[218075.914873] RIP: 0033:0x7fe79596e7cd\r\n[218075.915075] RSP: 002b:00007fe794943c20 EFLAGS: 00000293 ORIG_RAX: 0000000000000001\r\n[218075.915501] RAX: ffffffffffffffda RBX: 0000000000000050 RCX: 00007fe79596e7cd\r\n[218075.915903] RDX: 0000000000000050 RSI: 0000563711d50360 RDI: 0000000000000001\r\n[218075.916305] RBP: 0000563711d50360 R08: 00005637103e6fb0 R09: 203a653030303165\r\n[218075.916707] R10: 4e20307331706e65 R11: 0000000000000293 R12: 0000000000000000\r\n[218075.917110] R13: 0000563711d50140 R14: 00007fe7949440a8 R15: 0000000000000001\r\n[218075.937882] Code: 1f 44 00 00 66 2e 0f 1f 84 00 00 00 00 00 66 66 66 66 90 55 48 8d 47 10 48 89 e5 41 57 41 56 41 55 41 54 49 89 fe 53 48 83 ec 48 <4c> 8b 3f 48 89 c7 48 89 45 d0 e8 84 58 2a c4 65 48 8b 04 25 00 \r\n[218075.981121] RIP: arc_release+0x1d/0x740 [zfs] RSP: ffffb6bfa5963948\r\n[218076.002638] CR2: 0000000000000000\r\n[218076.023830] ---[ end trace 6ed37d4a8c9ac315 ]---\r\n[218076.031710] BUG: unable to handle kernel NULL pointer dereference at           (null)\r\n[218076.031737] IP: arc_release+0x1d/0x740 [zfs]\r\n[218076.031738] PGD 0 \r\n[218076.031739] P4D 0 \r\n\r\n[218076.031741] Oops: 0000 [#2] PREEMPT SMP\r\n[218076.031743] Modules linked in: tun devlink veth xt_CHECKSUM xt_tcpudp iptable_mangle ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack libcrc32c crc32c_generic iptable_filter fuse bridge stp llc amdkfd amd_iommu_v2 radeon snd_hda_codec_realtek snd_hda_codec_generic intel_rapl sb_edac x86_pkg_temp_thermal intel_powerclamp snd_hda_intel mei_wdt ttm coretemp drm_kms_helper kvm_intel snd_hda_codec iTCO_wdt hp_wmi iTCO_vendor_support snd_hda_core sparse_keymap wmi_bmof rfkill kvm snd_hwdep raid1 isci mpt3sas nvme drm snd_pcm irqbypass nvme_core firewire_ohci snd_timer tpm_infineon raid_class evdev libsas intel_cstate mei_me input_leds tpm_tis syscopyarea ehci_pci sysfillrect tpm_tis_core md_mod sysimgblt snd led_class firewire_core fb_sys_fops\r\n[218076.031777]  ioatdma intel_rapl_perf psmouse mac_hid pcspkr i2c_algo_bit crc_itu_t soundcore ehci_hcd mei shpchp i2c_i801 lpc_ich dca tpm wmi button sch_fq_codel ip_tables x_tables algif_skcipher af_alg dm_crypt dm_mod dax serio_raw atkbd libps2 crct10dif_pclmul crc32_pclmul crc32c_intel ghash_clmulni_intel pcbc aesni_intel aes_x86_64 crypto_simd glue_helper cryptd xhci_pci xhci_hcd usbcore usb_common ata_generic pata_acpi i8042 serio e1000e(O) ptp pps_core zfs(PO) zunicode(PO) zavl(PO) icp(PO) sd_mod zcommon(PO) znvpair(PO) spl(O) mptsas scsi_transport_sas mptscsih mptbase ahci libahci libata scsi_mod lz4 lz4_compress\r\n[218076.031808] CPU: 17 PID: 23689 Comm: rs:main Q:Reg Tainted: P      D    O    4.13.4-1-ARCH #1\r\n[218076.031809] Hardware name: Hewlett-Packard HP Z820 Workstation/158B, BIOS J63 v03.69 03/25/2014\r\n[218076.031810] task: ffff8953a890bc00 task.stack: ffffb6bfa6024000\r\n[218076.031824] RIP: 0010:arc_release+0x1d/0x740 [zfs]\r\n[218076.031824] RSP: 0018:ffffb6bfa60279a8 EFLAGS: 00010292\r\n[218076.031825] RAX: 0000000000000010 RBX: ffff895389622080 RCX: 0000000000000000\r\n[218076.031826] RDX: 0000000000000000 RSI: ffff8947f85d2ab0 RDI: 0000000000000000\r\n[218076.031827] RBP: ffffb6bfa6027a18 R08: ffff894a39407000 R09: ffff8948c7f1ac00\r\n[218076.031827] R10: ffff895389622080 R11: 0000000000000001 R12: ffff8948c8575ec0\r\n[218076.031828] R13: ffff8948c7f1ac00 R14: 0000000000000000 R15: ffff8947f85d2ba0\r\n[218076.031829] FS:  00007f1960ac9700(0000) GS:ffff894a39cc0000(0000) knlGS:0000000000000000\r\n[218076.031830] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[218076.031831] CR2: 0000000000000000 CR3: 00000012c33e5000 CR4: 00000000000406e0\r\n[218076.031832] Call Trace:\r\n[218076.031837]  ? spl_kmem_zalloc+0xc2/0x170 [spl]\r\n[218076.031840]  ? spl_kmem_zalloc+0xc2/0x170 [spl]\r\n[218076.031854]  dbuf_dirty+0x6a6/0x840 [zfs]\r\n[218076.031867]  dmu_buf_will_dirty_impl+0x116/0x130 [zfs]\r\n[218076.031879]  dmu_buf_will_dirty+0x16/0x20 [zfs]\r\n[218076.031894]  dmu_write_uio_dnode+0x93/0x140 [zfs]\r\n[218076.031907]  dmu_write_uio_dbuf+0x51/0x70 [zfs]\r\n[218076.031925]  zfs_write+0x8ef/0xd30 [zfs]\r\n[218076.031930]  ? z3fold_zpool_unmap+0x85/0xa0\r\n[218076.031934]  ? __radix_tree_delete+0x81/0xa0\r\n[218076.031952]  zpl_write_common_iovec+0x8c/0xe0 [zfs]\r\n[218076.031970]  zpl_iter_write+0xae/0xe0 [zfs]\r\n[218076.031972]  __vfs_write+0xf4/0x150\r\n[218076.031974]  vfs_write+0xb1/0x1a0\r\n[218076.031975]  SyS_write+0x55/0xc0\r\n[218076.031978]  do_syscall_64+0x54/0xc0\r\n[218076.031981]  entry_SYSCALL64_slow_path+0x25/0x25\r\n[218076.031982] RIP: 0033:0x7f1961af27cd\r\n[218076.031983] RSP: 002b:00007f1960ac7ba0 EFLAGS: 00000293 ORIG_RAX: 0000000000000001\r\n[218076.031984] RAX: ffffffffffffffda RBX: 0000000000000400 RCX: 00007f1961af27cd\r\n[218076.031985] RDX: 0000000000000400 RSI: 00005558fb4b8350 RDI: 0000000000000001\r\n[218076.031985] RBP: 00005558fb4b8350 R08: 6963686520616572 R09: 75706e6920656d5f\r\n[218076.031986] R10: 74207364656c5f74 R11: 0000000000000293 R12: 0000000000000000\r\n[218076.031986] R13: 00005558fb4b8130 R14: 00007f1960ac8028 R15: 0000000000000004\r\n[218076.031988] Code: 1f 44 00 00 66 2e 0f 1f 84 00 00 00 00 00 66 66 66 66 90 55 48 8d 47 10 48 89 e5 41 57 41 56 41 55 41 54 49 89 fe 53 48 83 ec 48 <4c> 8b 3f 48 89 c7 48 89 45 d0 e8 84 58 2a c4 65 48 8b 04 25 00 \r\n[218076.032017] RIP: arc_release+0x1d/0x740 [zfs] RSP: ffffb6bfa60279a8\r\n[218076.032018] CR2: 0000000000000000\r\n[218076.032019] ---[ end trace 6ed37d4a8c9ac316 ]---\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6759/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "YFLOPS": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6757", "title": "Change ashift for pool", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Centos \r\nDistribution Version    |  7.3\r\nLinux Kernel                 | 2.6.32\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.1\r\nSPL Version                  | 0.7.1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\nNot exactly a ZFS Problem.\r\n\r\nI don't see any feature requests to change the ashift value of a pool.  Generally I understand why this is done... however we have hundreds of disks in ashift=9, and now our storage provider is no longer replacing with 512n disks, and 512e/4kn disks absolutely kill the ZFS system when rebuilding... if it doesn't kill the disk trying.\r\n\r\nIf it was a small storage setup it might not be that big of a deal, but it's 588 disks backing Lustre, so not something we can easily move around.\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6757/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6474", "title": "Panic - PANIC at zfs_vfsops.c:583:zfs_space_delta_cb() ", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  CentOS\r\nDistribution Version    |  7.3.1611\r\nLinux Kernel                 |  3.10.0-514.26.1.el7.x86_64\r\nArchitecture                 |  x86_64\r\nZFS Version                  | 0.7.0-1\r\nSPL Version                  | 0.7.0-1\r\n\r\n### Describe the problem you're observing\r\n\r\nLustre MDS - running a loop of mdtest will kpanic the MDS\r\n\r\n### Describe how to reproduce the problem\r\n\r\nInstall mdtest, run it in a loop on a client node.\r\n-or-\r\nRun mds-survey on MDT\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\n[ 6004.896506] VERIFY3(sa.sa_magic == 0x2F505A) failed (24 == 3100762)\r\n[ 6004.903567] PANIC at zfs_vfsops.c:583:zfs_space_delta_cb()\r\n[ 6004.909720] Showing stack for process 130238\r\n[ 6004.909724] CPU: 5 PID: 130238 Comm: mdt01_006 Tainted: P           OE  ------------   3.10.0-514.26.1.el7.x86_64 #1\r\n[ 6004.909725] Hardware name: Dell Inc. PowerEdge C6320/082F9M, BIOS 2.4.2 01/09/2017\r\n[ 6004.909726]  ffffffffa18ad43b 0000000028956b86 ffff8826264fb420 ffffffff81687233\r\n[ 6004.909729]  ffff8826264fb430 ffffffffa027c234 ffff8826264fb5b8 ffffffffa027c309\r\n[ 6004.909730]  000000030000000e ffff882c00000030 ffff8826264fb5c8 ffff8826264fb568\r\n[ 6004.909732] Call Trace:\r\n[ 6004.909749]  [<ffffffff81687233>] dump_stack+0x19/0x1b\r\n[ 6004.909766]  [<ffffffffa027c234>] spl_dumpstack+0x44/0x50 [spl]\r\n[ 6004.909771]  [<ffffffffa027c309>] spl_panic+0xc9/0x110 [spl]\r\n[ 6004.909776]  [<ffffffff811de800>] ? kmem_cache_open+0x4c0/0x4d0\r\n[ 6004.909780]  [<ffffffffa0278319>] ? spl_kmem_cache_alloc+0x99/0x150 [spl]\r\n[ 6004.909786]  [<ffffffff8168aa82>] ? mutex_lock+0x12/0x2f\r\n[ 6004.909814]  [<ffffffffa174d191>] ? dbuf_find+0x141/0x150 [zfs]\r\n[ 6004.909819]  [<ffffffffa0278319>] ? spl_kmem_cache_alloc+0x99/0x150 [spl]\r\n[ 6004.909821]  [<ffffffff8168aa82>] ? mutex_lock+0x12/0x2f\r\n[ 6004.909828]  [<ffffffff810eacce>] ? getrawmonotonic64+0x2e/0xc0\r\n[ 6004.909849]  [<ffffffffa176c40d>] ? dmu_zfetch+0x40d/0x4a0 [zfs]\r\n[ 6004.909854]  [<ffffffffa027851d>] ? spl_kmem_cache_free+0x14d/0x1d0 [spl]\r\n[ 6004.909882]  [<ffffffffa17f1afc>] zfs_space_delta_cb+0x9c/0x200 [zfs]\r\n[ 6004.909897]  [<ffffffffa175f6ae>] dmu_objset_userquota_get_ids+0x13e/0x3e0 [zfs]\r\n[ 6004.909913]  [<ffffffffa176df2e>] dnode_setdirty+0x3e/0x120 [zfs]\r\n[ 6004.909929]  [<ffffffffa176e2f6>] dnode_allocate+0x186/0x220 [zfs]\r\n[ 6004.909943]  [<ffffffffa175bb1e>] dmu_object_alloc_dnsize+0x29e/0x360 [zfs]\r\n[ 6004.909964]  [<ffffffffa17d2b92>] zap_create_flags_dnsize+0x42/0xc0 [zfs]\r\n[ 6004.909973]  [<ffffffffa087f9d8>] __osd_zap_create+0x88/0x100 [osd_zfs]\r\n[ 6004.909978]  [<ffffffffa087fc27>] osd_mkdir+0x97/0x140 [osd_zfs]\r\n[ 6004.909981]  [<ffffffffa087ed12>] osd_create+0x2b2/0x9d0 [osd_zfs]\r\n[ 6004.909986]  [<ffffffffa02771a0>] ? spl_kmem_zalloc+0xc0/0x170 [spl]\r\n[ 6004.910005]  [<ffffffffa112d3b5>] lod_sub_create+0x1f5/0x480 [lod]\r\n[ 6004.910012]  [<ffffffffa1122419>] lod_create+0x69/0x2c0 [lod]\r\n[ 6004.910025]  [<ffffffffa118e9d5>] mdd_create_object_internal+0xb5/0x280 [mdd]\r\n[ 6004.910030]  [<ffffffffa117a355>] mdd_create_object+0x75/0xb80 [mdd]\r\n[ 6004.910034]  [<ffffffffa11805a8>] ? mdd_declare_create+0x578/0xe20 [mdd]\r\n[ 6004.910038]  [<ffffffffa11843ce>] mdd_create+0xd2e/0x1330 [mdd]\r\n[ 6004.910056]  [<ffffffffa1073d56>] mdt_create+0x846/0xbb0 [mdt]\r\n[ 6004.910087]  [<ffffffffa0c880e4>] ? lprocfs_stats_lock+0x24/0xd0 [obdclass]\r\n[ 6004.910098]  [<ffffffffa0c87d5d>] ? lprocfs_stats_unlock+0x3d/0x50 [obdclass]\r\n[ 6004.910106]  [<ffffffffa107422b>] mdt_reint_create+0x16b/0x350 [mdt]\r\n[ 6004.910114]  [<ffffffffa1075730>] mdt_reint_rec+0x80/0x210 [mdt]\r\n[ 6004.910120]  [<ffffffffa10572fb>] mdt_reint_internal+0x5fb/0x9c0 [mdt]\r\n[ 6004.910127]  [<ffffffffa1062e37>] mdt_reint+0x67/0x140 [mdt]\r\n[ 6004.910185]  [<ffffffffa0ee3915>] tgt_request_handle+0x915/0x1360 [ptlrpc]\r\n[ 6004.910208]  [<ffffffffa0e8d1b3>] ptlrpc_server_handle_request+0x233/0xa90 [ptlrpc]\r\n[ 6004.910230]  [<ffffffffa0e8a9a8>] ? ptlrpc_wait_event+0x98/0x340 [ptlrpc]\r\n[ 6004.910235]  [<ffffffff810c54f2>] ? default_wake_function+0x12/0x20\r\n[ 6004.910239]  [<ffffffff810ba628>] ? __wake_up_common+0x58/0x90\r\n[ 6004.910260]  [<ffffffffa0e91190>] ptlrpc_main+0xaa0/0x1dd0 [ptlrpc]\r\n[ 6004.910280]  [<ffffffffa0e906f0>] ? ptlrpc_register_service+0xe30/0xe30 [ptlrpc]\r\n[ 6004.910284]  [<ffffffff810b0a4f>] kthread+0xcf/0xe0\r\n[ 6004.910286]  [<ffffffff810b0980>] ? kthread_create_on_node+0x140/0x140\r\n[ 6004.910289]  [<ffffffff81697858>] ret_from_fork+0x58/0x90\r\n[ 6004.910292]  [<ffffffff810b0980>] ? kthread_create_on_node+0x140/0x140\r\n```\r\n-->\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6474/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stuartthebruce": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6741", "title": "umount/mount modifies readonly filesystem", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Scientific Linux\r\nDistribution Version    | 7.4\r\nLinux Kernel                 | 3.10.0-693.2.2.el7.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.2-1\r\nSPL Version                  | 0.7.2-1\r\n\r\n### Describe the problem you're observing\r\nunmounting and mounting a reports a readonly filesystem has been modified\r\n\r\n### Describe how to reproduce the problem\r\n```\r\n[root@dcc3 ~]# zfs get readonly,written dcc/data\r\nNAME      PROPERTY  VALUE    SOURCE\r\ndcc/data  readonly  on       local\r\ndcc/data  written   0        -\r\n[root@dcc3 ~]# zfs umount dcc/data && zfs mount dcc/data\r\n[root@dcc3 ~]# zfs get readonly,written dcc/data\r\nNAME      PROPERTY  VALUE    SOURCE\r\ndcc/data  readonly  on       local\r\ndcc/data  written   141M     -\r\n```\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nThis results in zfs receiving failing to accept an incremental snapshot, \"cannot receive incremental stream: destination XXX has been modified\"", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6741/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6489", "title": "0.7.1 Input/output error from /bin/ls", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | SL\r\nDistribution Version    | 7.3\r\nLinux Kernel                 | 3.10.0-514.26.1.el7.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.1-1\r\nSPL Version                  | 0.7.1-1\r\n\r\n### Describe the problem you're observing\r\n/bin/ls generations \"Input/output error\" while running zfs receive for an incremental snapshot update. This happens significantly more frequently if a background scrub is running.\r\n\r\n### Describe how to reproduce the problem\r\nwatch -n.1 ls -l /some/ZFS/path\r\nwhile receiving a ZFS incremental snapshot (discovered using zrep script)\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nNone seen\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6489/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Harvie": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6740", "title": "ZoL not really willing to evict caches from RAM when needed", "body": "I have overall impression that when ZFS allocates some memory (eg. for L1ARC) it will NOT free it up fast enough if needed by processes. When other filesystems on Linux cache something it gets evicted from cache as soon as some process needs memory.\r\n\r\nHowever with L1ARC (and possibly other ZFS caches) it will lead to OOM (or even full system crash and reboot) once there is not enough memory/swap for processes. Even when i had 30 GB of data cached (which in my opinion can be droped from cache almost immediately). This makes my system very unstable and effectively means you can really subtract zfs_arc_max from your total system memory, it's no use anymore. which is not nice in my opinion.\r\n\r\nAlso in htop the cache memory used by ZFS is \"green\" like for \"process\" memory rather than \"yellow\" for \"cache\". Which indicates that something about ZFS caching is kinda arrogant (in terms of memory allocation).", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6740/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6739", "title": "zdb -S tank leads to system crash", "body": "eI've run **zdb -S tank** but then i changed my mind and stoped it using \"ctrl+c\". During next hours something managed to eat almost 30GB of RAM cache and resulted in complete system crash due to insufficient ram and i have strong suspicion that it was caused by me executing this command. (L1ARC was limited to 20GB max). Pool Usage 48.86% (877.46 GiB of 1.75 TiB).\r\n\r\n1.) I guess this is a bug\r\n2.) Is there way to stop the deduplication simulation without crash? (dedup is set to off)\r\n3.) If this is something i should expect, it should probably be mentioned in **man zdb**\r\n4.) Moved to #6740\r\n\r\n![image](https://user-images.githubusercontent.com/276504/31322664-67cd2960-ac9c-11e7-83fd-c857f373fcbb.png)\r\n\r\nThat middle bar in graph with cache jumping up and down is probably somehow related to ZFS. It was OK after reboot.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ronnyegner": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6736", "title": "task z_zvol blocked for more than 120 seconds during ZFS send/ZFS recv", "body": "### System information \r\n<!--  add version after \"|\" character -->\r\nDistribution Name       |  Ubuntu\r\n  ---                                  |     --- \r\nDistribution Version    |  17.04\r\nLinux Kernel                 |  4.10.0-24-generic\r\nArchitecture                 | x86_64\r\nZFS Version                  | v0.7.0-50_g1ea8942fa\r\nSPL Version                  | v0.7.0-12_g9df9692\r\n\r\nI have now noticed several times that while some zfs send/zfs recv operations are running i get the following errors related to ZVOLs. Interesting here is that i don\u00b4t ave any ZVOL configured:\r\n\r\n```\r\nOct  8 10:06:53 homenas kernel: [83374.561093] INFO: task z_zvol:5609 blocked for more than 120 seconds.\r\nOct  8 10:06:53 homenas kernel: [83374.562910]       Tainted: P        W  OE   4.10.0-24-generic #28-Ubuntu\r\nOct  8 10:06:53 homenas kernel: [83374.564463] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nOct  8 10:06:53 homenas kernel: [83374.566143] z_zvol          D    0  5609      2 0x00000000\r\nOct  8 10:06:53 homenas kernel: [83374.566148] Call Trace:\r\nOct  8 10:06:53 homenas kernel: [83374.566161]  __schedule+0x233/0x6f0\r\nOct  8 10:06:53 homenas kernel: [83374.566165]  schedule+0x36/0x80\r\nOct  8 10:06:53 homenas kernel: [83374.566194]  taskq_wait_outstanding+0x8c/0xd0 [spl]\r\nOct  8 10:06:53 homenas kernel: [83374.566199]  ? wake_atomic_t_function+0x60/0x60\r\nOct  8 10:06:53 homenas kernel: [83374.566268]  zvol_task_cb+0x1f9/0x5b0 [zfs]\r\nOct  8 10:06:53 homenas kernel: [83374.566275]  ? __schedule+0x23b/0x6f0\r\nOct  8 10:06:53 homenas kernel: [83374.566284]  taskq_thread+0x2b5/0x4e0 [spl]\r\nOct  8 10:06:53 homenas kernel: [83374.566287]  ? wake_up_q+0x80/0x80\r\nOct  8 10:06:53 homenas kernel: [83374.566297]  kthread+0x109/0x140\r\nOct  8 10:06:53 homenas kernel: [83374.566302]  ? task_done+0xb0/0xb0 [spl]\r\nOct  8 10:06:53 homenas kernel: [83374.566305]  ? kthread_create_on_node+0x60/0x60\r\nOct  8 10:06:53 homenas kernel: [83374.566308]  ret_from_fork+0x2c/0x40\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6736/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "UralZima": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6732", "title": "ZFS CRASH on RECV, then on LIST after creating new pool and trying to send raw dataset", "body": "Hello. I am using zfs since 0.6, now using latest master, last recompile yesterday. \r\nI waited and wanted to test sending and receiving with resume, sending raw streams, so I started to do that and got some strange issues.\r\n\r\nAfter importing a new pool bkpool, I am starting experiencing problems. \r\nAfter import:\r\n\r\n```\r\nlocalhost user # zpool status\r\n  pool: bkpool\r\n state: ONLINE\r\n  scan: none requested\r\nconfig:\r\n\r\n        NAME        STATE     READ WRITE CKSUM\r\n        bkpool      ONLINE       0     0     0\r\n          bk1       ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n\r\n  pool: mypool\r\n state: ONLINE\r\n  scan: scrub repaired 0B in 1h18m with 0 errors on Thu Mar 30 12:32:17 2017\r\nconfig:\r\n\r\n        NAME        STATE     READ WRITE CKSUM\r\n        mypool      ONLINE       0     0     0\r\n          root      ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n\r\n  pool: tpool\r\n state: ONLINE\r\nstatus: Some supported features are not enabled on the pool. The pool can\r\n        still be used, but some features are unavailable.\r\naction: Enable all features using 'zpool upgrade'. Once this is done,\r\n        the pool may no longer be accessible by software that does not support\r\n        the features. See zpool-features(5) for details.\r\n  scan: scrub repaired 0B in 1h28m with 0 errors on Thu Mar 30 12:42:16 2017\r\nconfig:\r\n\r\n        NAME        STATE     READ WRITE CKSUM\r\n        tpool       ONLINE       0     0     0\r\n          ot        ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n\r\nlocalhost user # zfs list\r\n[1]    31283 abort      zfs list\r\n```\r\n\r\nAfter importing bkpool, zfs list is not working anymore. I created some datasets already and tried to send them via:\r\n```\r\nlocalhost user # zfs send -w -c -v -R mypool/n/R@first | zfs recv -u -d -F -s -v bkpool/work/data\r\n...\r\ntotal estimated size is 16.4G\r\nTIME        SENT   SNAPSHOT\r\n[1]    733 broken pipe  zfs send -w -c -v -R mypool/n/R@first |\r\n       734 abort        zfs recv -u -d -F -s -v bkpool/work/data\r\n\r\n```\r\n\r\nAgain crash.\r\nThe only thing in dmesg is:\r\n[ 3846.362443] grsec: denied resource overstep by requesting 4096 for RLIMIT_CORE against limit 0 for /sbin/zfs[zfs:734] uid/euid:0/0 gid/egid:0/0, parent /bin/zsh[zsh:23888] uid/euid:0/0 gid/egid:0/0\r\n\r\nwhich means, that program crashed with core dump. Every crash on grsec system will have such log entry.\r\n\r\nI am using gentoo-hardened and kernel 4.9.24 with grsecurity. I am using ZFS in such setup about 2 years, and all was fine. This is first time something strange happens.\r\nI use zfs-9999, zfs-kmod-9999 and spl-9999, recompile them all together on update.\r\n\r\nPlease help\r\n\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6732/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "BerserkerTroll": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6730", "title": "HOWTOS: mount with --make-rslave", "body": "@rlaager \r\n\r\nI suggest to add the `--make-rslave` to bind-mounts, i.e. write\r\n```\r\n# mount --rbind --make-rslave /dev  /mnt/dev\r\n# mount --rbind --make-rslave /proc /mnt/proc\r\n# mount --rbind --make-rslave /sys  /mnt/sys\r\n```\r\n\r\nThis makes possible unmounting of `/mnt/{dev,proc,sys}` using `umount -R`, in case one fucked something up and wishes to destroy and recreate the pool.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "arturpzol": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6729", "title": "Renaming a zvol which is opened does not move/rename the symlink in /dev/Pool/ and we lose access to zvol device", "body": "### System information\r\n<!--  add version after \"|\" character -->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian Jessie\r\nDistribution Version    | 8\r\nLinux Kernel                 | 4.4.45, 3.10\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.2, 0.6.5.6\r\nSPL Version                  | 0.7.2, 0.6.5.6\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nWhen we rename the zvol which is opened, udev does not change zvol symlink in /dev/Pool/.\r\nAccess to zvol device is also not possible after it.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nZFS 0.6.5.6 and 0.7.2:\r\n\r\n```\r\nzpool create Pool-0 mirror scsi-SQEMU_QEMU_HARDDISK_24121-1 scsi-SQEMU_QEMU_HARDDISK_24121-2 -m /Pools/Pool-0 -f\r\nzfs create -s -b 131072  -V 1073741824 Pool-0/1\r\n\r\nls /dev/zvol/Pool-0/  \r\n1\r\n\r\nzfs rename Pool-0/1 Pool-0/2\r\nls /dev/zvol/Pool-0/\r\n2\r\n\r\nzfs get type -t volume \r\nNAME      PROPERTY  VALUE   SOURCE\r\nPool-0/2  type      volume  -\r\n\r\n-----\r\n\r\nzfs rename Pool-0/2 Pool-0/3\r\nls /dev/zvol/Pool-0/\r\n3\r\n\r\nzfs get type -t volume \r\nNAME      PROPERTY  VALUE   SOURCE\r\nPool-0/3  type      volume  -\r\n\r\n-----\r\n\r\ndd if=/dev/zero of=/dev/zvol/Pool-0/3 &\r\n[1] 11040\r\n\r\nzfs rename Pool-0/3 Pool-0/4\r\nls /dev/zvol/Pool-0/\r\n3\r\n\r\nzfs get type -t volume \r\nNAME      PROPERTY  VALUE   SOURCE\r\nPool-0/4  type      volume  -\r\n\r\n```\r\n\r\nthen access to the zvol is not possible:\r\n\r\n```\r\nls /dev/zvol/Pool-0/\r\n3\r\n\r\nzfs get type -t volume \r\nNAME      PROPERTY  VALUE   SOURCE\r\nPool-0/4  type      volume  -\r\n\r\n\r\ndd if=/dev/zero of=/dev/zvol/Pool-0/3\r\ndd: failed to open '/dev/zvol/Pool-0/3': No such file or directory\r\n\r\nls /dev/zd0 \r\n/dev/zd0\r\n\r\ndd if=/dev/zero of=/dev/zd0\r\ndd: failed to open '/dev/zd0': No such file or directory\r\n\r\n```\r\n\r\n\r\n`udevadm trigger` and `udevadm settle` executed after it does not fix the symlink. Only one solve is export and import Pool again.\r\n\r\nWhen zvol is not used/opened during rename we can see udev events from `udevadm monitor`:\r\n\r\n```\r\nKERNEL[943.715587] change   /devices/virtual/block/zd0 (block)\r\nKERNEL[943.715599] change   /devices/virtual/block/zd0 (block)\r\nUDEV  [943.719099] change   /devices/virtual/block/zd0 (block)\r\nUDEV  [943.720067] change   /devices/virtual/block/zd0 (block)\r\nUDEV  [943.723341] change   /devices/virtual/block/zd0 (block)\r\nUDEV  [943.726020] change   /devices/virtual/block/zd0 (block)\r\n```\r\n\r\n\r\nWhen zvol is used/opened during rename above udev events are not generated.\r\n\r\nIt works correctly in **0.6.5.5** and issue was introduced in **0.6.5.6** where some zvol changes have been made.\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6729/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6554", "title": "Unable to handle kernel NULL pointer dereference  __kmalloc_node", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian Jessie\r\nDistribution Version    | 8\r\nLinux Kernel                 | 4.4.45\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.1-1\r\nSPL Version                  | 0.7.1-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nHanged system. \r\n### Describe how to reproduce the problem\r\nExecute `zpool get all ` during high I/O.\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n\r\n-->\r\n```\r\nAug 23 18:30:29 [kern.alert] [ 4633.329012] BUG: unable to handle kernel NULL pointer dereference at 0000000000000085\r\nAug 23 18:30:29 [kern.alert] [ 4633.329020] IP: [<ffffffff8112fb4b>] __kmalloc_node+0xdb/0x190\r\nAug 23 18:30:29 [kern.warning] [ 4633.329031] PGD 161c4b3067 PUD 14465b8067 PMD 0 \r\nAug 23 18:30:29 [kern.warning] [ 4633.329034] Oops: 0000 [#1] SMP \r\nAug 23 18:30:29 [kern.warning] [ 4633.329036] Modules linked in: sha256_generic drbg mptctl mptbase iscsi_scst(O) scst_vdisk(O) scst(O) target_core_iblock target_core_pscsi iscsi_target_mod target_core_mod bonding ib_iser iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi fuse ipmi_devintf zfs(PO) zunicode(PO) zavl(PO) icp(PO) x86_pkg_temp_thermal crc32c_intel aesni_intel lrw gf128mul glue_helper ablk_helper cryptd zcommon(PO) znvpair(PO) spl(O) igb(O) ixgbe(O) vxlan udp_tunnel ptp pps_core ipmi_si ipmi_msghandler processor button nls_iso8859_1 nls_cp437 sg mpt3sas(O) megaraid_sas(O) isci raid_class libsas scsi_transport_sas vfat fat aufs\r\nAug 23 18:30:29 [kern.warning] [ 4633.329070] CPU: 6 PID: 86535 Comm: zpool Tainted: P           O    4.4.45 #13\r\nAug 23 18:30:29 [kern.warning] [ 4633.329074] Hardware name: Intel Corporation S2600GZ/S2600GZ, BIOS SE5C600.86B.02.03.0003.041920141333 04/19/2014\r\nAug 23 18:30:29 [kern.warning] [ 4633.329076] task: ffff881938d4be80 ti: ffff880819034000 task.ti: ffff880819034000\r\nAug 23 18:30:29 [kern.warning] [ 4633.329077] RIP: 0010:[<ffffffff8112fb4b>]  [<ffffffff8112fb4b>] __kmalloc_node+0xdb/0x190\r\nAug 23 18:30:29 [kern.warning] [ 4633.329081] RSP: 0018:ffff880819037a28  EFLAGS: 00010246\r\nAug 23 18:30:29 [kern.warning] [ 4633.329082] RAX: 0000000000000000 RBX: 0000000002404200 RCX: 0000000001776e86\r\nAug 23 18:30:29 [kern.warning] [ 4633.329083] RDX: 0000000001776e85 RSI: 0000000000000000 RDI: 0000000000000007\r\nAug 23 18:30:29 [kern.warning] [ 4633.329084] RBP: 0000000000000085 R08: 0000000000018680 R09: 0000000000000000\r\nAug 23 18:30:29 [kern.warning] [ 4633.329086] R10: 0000000000000000 R11: 0000000000000000 R12: 0000000002404200\r\nAug 23 18:30:29 [kern.warning] [ 4633.329087] R13: 00000000ffffffff R14: ffffffffa095e0b5 R15: ffff880fff007980\r\nAug 23 18:30:29 [kern.warning] [ 4633.329089] FS:  00007f709de53780(0000) GS:ffff881ffee40000(0000) knlGS:0000000000000000\r\nAug 23 18:30:29 [kern.warning] [ 4633.329090] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nAug 23 18:30:29 [kern.warning] [ 4633.329091] CR2: 0000000000000085 CR3: 0000001f85cfe000 CR4: 00000000000406e0\r\nAug 23 18:30:29 [kern.warning] [ 4633.329092] Stack:\r\nAug 23 18:30:29 [kern.warning] [ 4633.329094]  0000000002404200 0000000000000001 0000000000000000 0000000000002000\r\nAug 23 18:30:29 [kern.warning] [ 4633.329096]  00000000ffffffff 0000000000000040 ffffffffa095e0b5 00000000a095e0b5\r\nAug 23 18:30:29 [kern.warning] [ 4633.329098]  ffff881d5ba26c01 0000000000000040 ffff880819037b20 0000000000000008\r\nAug 23 18:30:29 [kern.warning] [ 4633.329100] Call Trace:\r\nAug 23 18:30:29 [kern.warning] [ 4633.329111]  [<ffffffffa095e0b5>] ? spl_kmem_alloc_impl+0xa5/0x180 [spl]\r\nAug 23 18:30:29 [kern.warning] [ 4633.329122]  [<ffffffffa097f66b>] ? nv_mem_zalloc.isra.14+0xb/0x30 [znvpair]\r\nAug 23 18:30:29 [kern.warning] [ 4633.329126]  [<ffffffffa0980d44>] ? nvlist_add_common.part.53+0x114/0x400 [znvpair]\r\nAug 23 18:30:29 [kern.warning] [ 4633.329132]  [<ffffffffa09813fe>] ? nvlist_add_uint64+0x2e/0x40 [znvpair]\r\nAug 23 18:30:29 [kern.warning] [ 4633.329137]  [<ffffffffa0983482>] ? fnvlist_add_uint64+0x12/0x60 [znvpair]\r\nAug 23 18:30:29 [kern.warning] [ 4633.329205]  [<ffffffffa1275d2d>] ? vdev_config_generate_stats+0xed/0x370 [zfs]\r\nAug 23 18:30:29 [kern.warning] [ 4633.329244]  [<ffffffffa127648a>] ? vdev_config_generate+0x4da/0x650 [zfs]\r\nAug 23 18:30:29 [kern.warning] [ 4633.329247]  [<ffffffff8112fa92>] ? __kmalloc_node+0x22/0x190\r\nAug 23 18:30:29 [kern.warning] [ 4633.329251]  [<ffffffffa095dd3a>] ? spl_kmem_alloc+0x9a/0x170 [spl]\r\nAug 23 18:30:29 [kern.warning] [ 4633.329289]  [<ffffffffa1276449>] ? vdev_config_generate+0x499/0x650 [zfs]\r\nAug 23 18:30:29 [kern.warning] [ 4633.329294]  [<ffffffffa0980f1b>] ? nvlist_add_common.part.53+0x2eb/0x400 [znvpair]\r\nAug 23 18:30:29 [kern.warning] [ 4633.329331]  [<ffffffffa1263077>] ? spa_config_generate+0x1e7/0x470 [zfs]\r\nAug 23 18:30:29 [kern.warning] [ 4633.329366]  [<ffffffffa125e50f>] ? spa_open_common+0xaf/0x490 [zfs]\r\nAug 23 18:30:29 [kern.warning] [ 4633.329371]  [<ffffffffa097f4fd>] ? nvlist_free+0x4d/0xa0 [znvpair]\r\nAug 23 18:30:29 [kern.warning] [ 4633.329406]  [<ffffffffa125e948>] ? spa_get_stats+0x38/0x4d0 [zfs]\r\nAug 23 18:30:29 [kern.warning] [ 4633.329410]  [<ffffffff810f5afe>] ? alloc_kmem_pages_node+0x3e/0xc0\r\nAug 23 18:30:29 [kern.warning] [ 4633.329412]  [<ffffffff8112c245>] ? kmalloc_large_node+0x25/0x50\r\nAug 23 18:30:29 [kern.warning] [ 4633.329415]  [<ffffffff8112fb88>] ? __kmalloc_node+0x118/0x190\r\nAug 23 18:30:29 [kern.warning] [ 4633.329455]  [<ffffffffa12954df>] ? zfs_ioc_pool_stats+0x1f/0x60 [zfs]\r\nAug 23 18:30:29 [kern.warning] [ 4633.329496]  [<ffffffffa129929e>] ? zfsdev_ioctl+0x59e/0x640 [zfs]\r\nAug 23 18:30:29 [kern.warning] [ 4633.329500]  [<ffffffff81150ddf>] ? do_vfs_ioctl+0x2bf/0x490\r\nAug 23 18:30:29 [kern.warning] [ 4633.329502]  [<ffffffff81151021>] ? SyS_ioctl+0x71/0x80\r\nAug 23 18:30:29 [kern.warning] [ 4633.329506]  [<ffffffff8170feae>] ? entry_SYSCALL_64_fastpath+0x12/0x71\r\nAug 23 18:30:29 [kern.warning] [ 4633.329507] Code: 6c d6 20 00 8b 05 76 f2 c3 00 85 c0 0f 8f bc 00 00 00 48 89 e8 5b 5d 41 5c 41 5d 41 5e 41 5f c3 49 63 47 20 48 8d 4a 01 4d 8b 07 <48> 8b 5c 05 00 48 89 e8 65 49 0f c7 08 0f 94 c0 84 c0 0f 84 5e \r\nAug 23 18:30:29 [kern.alert] [ 4633.329528] RIP  [<ffffffff8112fb4b>] __kmalloc_node+0xdb/0x190\r\nAug 23 18:30:29 [kern.warning] [ 4633.329531]  RSP <ffff880819037a28>\r\nAug 23 18:30:29 [kern.warning] [ 4633.329532] CR2: 0000000000000085\r\nAug 23 18:30:29 [kern.warning] [ 4633.329536] ---[ end trace 44f831883cb4326b ]---\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6554/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ab-oe": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6728", "title": "Poor zvol discard performance.", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  all\r\nDistribution Version    | \r\nLinux Kernel                 |  4.4\r\nArchitecture                 |  x86_64\r\nZFS Version                  |  0.7.2\r\nSPL Version                  |  0.7.2\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nSince the commit 539d33c791da2f970cfa5a1bddf0533b23146265 zvol discard operation time greatly fell down no matter how the ```zfs_per_txg_dirty_frees_percent``` parameter is set.\r\n\r\n### Describe how to reproduce the problem\r\nThe easiest way is to run mkfs.ext4 which do the discard by default on the zvol.\r\n\r\nOn the 0.7.2 with reverted 539d33c791da2f970cfa5a1bddf0533b23146265 the whole mkfs.ext4 on 1TiB thin provisioned zvol takes 6 seconds.\r\n\r\n```\r\ntime mkfs.ext4 /dev/zd16\r\nmke2fs 1.42.12 (29-Aug-2014)\r\n/dev/zd16 contains a ext4 file system\r\n\tcreated on Fri Oct  6 10:00:28 2017\r\nProceed anyway? (y,n) y\r\nDiscarding device blocks: done                            \r\nCreating filesystem with 268435456 4k blocks and 67108864 inodes\r\nFilesystem UUID: 37e643bc-f80f-4b91-867b-c6b12ef11261\r\nSuperblock backups stored on blocks: \r\n\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, \r\n\t4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, \r\n\t102400000, 214990848\r\n\r\nAllocating group tables: done                            \r\nWriting inode tables: done                            \r\nCreating journal (32768 blocks): done\r\nWriting superblocks and filesystem accounting information: done     \r\n\r\n\r\nreal\t0m6.305s\r\nuser\t0m0.030s\r\nsys\t0m0.140s\r\n```\r\n\r\nOn official release this operation takes 2 minutes and 19 seconds:\r\n\r\n```\r\ntime mkfs.ext4 /dev/zd1\r\nmke2fs 1.42.12 (29-Aug-2014)\r\n/dev/zd16 contains a ext4 file system\r\n\tcreated on Fri Oct  6 10:00:45 2017\r\nProceed anyway? (y,n) y\r\nDiscarding device blocks: done                            \r\nCreating filesystem with 268435456 4k blocks and 67108864 inodes\r\nFilesystem UUID: aa37f90c-34e7-49d4-97b1-57ff9f71ee64\r\nSuperblock backups stored on blocks: \r\n\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, \r\n\t4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, \r\n\t102400000, 214990848\r\n\r\nAllocating group tables: done                            \r\nWriting inode tables: done                            \r\nCreating journal (32768 blocks): done\r\nWriting superblocks and filesystem accounting information: done     \r\n\r\n\r\nreal\t2m19.554s\r\nuser\t0m0.040s\r\nsys\t0m0.130s\r\n```\r\n\r\nFor the 10TiB volume it took almost 20 minutes.\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6728/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/c0daec32f839f687a7b631ea8c292dfb2637478a", "message": "Long hold the dataset during upgrade\n\nIf the receive or rollback is performed while filesystem is upgrading\r\nthe objset may be evicted in `dsl_dataset_clone_swap_sync_impl`. This\r\nwill lead to NULL pointer dereference when upgrade tries to access\r\nevicted objset.\r\n\r\nThis commit adds long hold of dataset during whole upgrade process.\r\nThe receive and rollback will return an EBUSY error until the\r\nupgrade is not finished.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Arkadiusz Buba\u0142a <arkadiusz.bubala@open-e.com>\r\nCloses #5295 \r\nCloses #6837"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d3f2cd7e3b70679f127dd471ea6d37ece27463f2", "message": "Added no_scrub_restart flag to zpool reopen\n\nAdded -n flag to zpool reopen that allows a running scrub\r\noperation to continue if there is a device with Dirty Time Log.\r\n\r\nBy default if a component device has a DTL and zpool reopen\r\nis executed all running scan operations will be restarted.\r\n\r\nAdded functional tests for `zpool reopen`\r\n\r\nTests covers following scenarios:\r\n* `zpool reopen` without arguments,\r\n* `zpool reopen` with pool name as argument,\r\n* `zpool reopen` while scrubbing,\r\n* `zpool reopen -n` while scrubbing,\r\n* `zpool reopen -n` while resilvering,\r\n* `zpool reopen` with bad arguments.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Tom Caputi <tcaputi@datto.com>\r\nSigned-off-by: Arkadiusz Buba\u0142a <arkadiusz.bubala@open-e.com>\r\nCloses #6076 \r\nCloses #6746"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d9549cba9640cd3b09d76b8cbd54387728b7be24", "message": "Fix false config_cache_write events\n\nOn pool import when the old cache file is removed\r\nthe ereport.fs.zfs.config_cache_write event is generated.\r\nBecause zpool export always removes cache file it happens\r\nevery export - import sequence.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: loli10K <ezomori.nozomu@gmail.com>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Arkadiusz Buba\u0142a <arkadiusz.bubala@open-e.com>\r\nCloses #6617"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/94b25662c51696ec081494e69efb5896566dede2", "message": "Reschedule processes on -ERESTARTSYS\n\nOn the single core machine the system may hang when the\r\nspa_namespare_lock acquisition fails in the zvol_first_open\r\nfunction. It returns -ERESTARTSYS error what causes the\r\nendless loop in __blkdev_get function.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Arkadiusz Buba\u0142a <arkadiusz.bubala@open-e.com>\r\nCloses #6283 \r\nCloses #6312"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jgrund": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6727", "title": "Pools not immediately importable in STONITH situation", "body": "ZFS 0.7.1\r\n\r\nIn a STONITH situation, it appears to take a little time for pools to be importable on the node being failed over to. You can tell the pool is importable when the import error message changes from:\r\n\r\n```\r\nExport the pool on the other system, then run 'zpool import'.\r\n```\r\n\r\nto\r\n\r\n```The pool can be imported, use 'zpool import -f' to import the pool.```\r\n\r\nJust want to confirm this is expected behavior.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6727/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cehteh": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6725", "title": "ZFS 'just' hangs", "body": "\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Debian\r\nDistribution Version    |  9.1\r\nLinux Kernel                 |  4.13.4\r\nArchitecture                 |  x86_64\r\nZFS Version                  |  0.7.0-97_g5f88d2c8a (that is zfs with encryption from tcaputi, with cytrinox's work on debian packaging  github.com/cytrinox/zfs ) \r\nSPL Version                  |  0.7.0-15_g275146c ditto see above\r\n\r\n### Describe the problem you're observing\r\n\r\nZFS 'just' hang. As in no operations where possible, no (kernel-) threads done any work, no disk I/O.\r\nIssuing the command line tools (zpool / zfs / zdb) stuck as well. The Kernel threads eventually got a 'hung task timeout' warning in the kernel. logs and dmesg show nothing (except the hung task info). \r\n\r\n\r\n### Describe how to reproduce the problem\r\n\r\nI had a rather complex situation here, likely not (easily) reproducible. Eventually I rebooted the system. I am now trying to reproduce the problem.\r\n\r\nA list and annotations about what was going on:\r\n- everything below on encrypted datasets/zvols\r\n- resilvering to a striped md devices (2x4TB disks as one 8TB)\r\n- git annex was running checksumming a media library\r\n- the zfs-auto-snapshot service was running but the zvol's are excluded\r\n- created a zvol\r\n- qemu-img convert from one old .qcow2 to a zvol\r\n  The first image conversion skyrocketed the system load, first one was at load 30-40 but eventually completed over night.\r\n  The next image brought the ZFS down, load gone up to 70+ and no progress/effects as\r\n  described above.\r\n - I've noticed that there was a lot pointless write load to the l2arc, i tried to offline them around the time the filesystem got stuck, load was already above 70, possibly the filesystem hang already.\r\n\r\nTrying to reproduce with no success so far:\r\n- the high load condition with 'qemu-img' is reproducible\r\n- online/offline the cache devices a few times works.\r\n\r\n\r\npool configuration:\r\n\r\n  pool: data\r\n  state: DEGRADED\r\n  status: One or more devices is currently being resilvered.  The pool will\r\n\tcontinue to function, possibly in a degraded state.\r\n  action: Wait for the resilver to complete.\r\n  scan: resilver in progress since Wed Oct  4 14:58:57 2017\r\n\t6,84T scanned out of 16,3T at 13,9M/s, 198h44m to go\r\n\t1,37T resilvered, 41,89% done\r\n  config:\r\n\r\n\tNAME                                               STATE     READ WRITE CKSUM\r\n\tdata                                               DEGRADED     0     0     0\r\n\t  raidz2-0                                         DEGRADED     0     0     0\r\n\t    replacing-0                                    DEGRADED     0     0     0\r\n\t      /root/spare1                                 OFFLINE      0     0     0\r\n\t      md-uuid-97addc40:1ac606c6:bbc357af:9c81950d  ONLINE       0     0     0  (resilvering)\r\n\t    /root/spare2                                   OFFLINE      0     0     0\r\n\t    sdf                                            ONLINE       0     0     0\r\n\t    sdg                                            ONLINE       0     0     0\r\n\t    sdh                                            ONLINE       0     0     0\r\n\tlogs\r\n\t  mirror-1                                         ONLINE       0     0     0\r\n\t    nvme0n1p5                                      ONLINE       0     0     0\r\n\t    nvme1n1p5                                      ONLINE       0     0     0\r\n\tcache\r\n\t  nvme0n1p6                                        OFFLINE      0     0     0\r\n\t  nvme1n1p6                                        OFFLINE      0     0     0\r\n\r\n\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nyeah, sorry, no nothing logged\r\n\r\nnote: Looks to me like some rare race/deadlock problem under high load.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6725/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sdm900": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6711", "title": "PANIC: blkptr at ffff881e68398440 has invalid CHECKSUM 0", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Centos\r\nDistribution Version    | 7.3\r\nLinux Kernel                 | 3.10.0-514.6.1.el7.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.1-1\r\nSPL Version                  | 0.7.1-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nOn the MDS of a production lustre file system, attempting to do\r\n\r\n    zfs send -RLc\r\n\r\nwe got a PANIC\r\n\r\n```\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.499919] PANIC: blkptr at ffff881e68398440 has invalid CHECKSUM 0\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.507850] Showing stack for process 220\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.513426] CPU: 7 PID: 220 Comm: spl_system_task Tainted: P           OE  ------------   3.10.0-514.6.1.el7.x86_64 #1\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.525672] Hardware name: GIGABYTE D120-C21/MC20-S10, BIOS F02 05/19/2016\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.534105]  0000000000000003 00000000d7f4d3c5 ffff881fe81d33a0 ffffffff816862ac\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.543157]  ffff881fe81d33b0 ffffffffa02be254 ffff881fe81d34d8 ffffffffa02be3dc\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.552223]  ffff881ed3f9c000 61207274706b6c62 3838666666662074 3438393338366531\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.561355] Call Trace:\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.565376]  [<ffffffff816862ac>] dump_stack+0x19/0x1b\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.572080]  [<ffffffffa02be254>] spl_dumpstack+0x44/0x50 [spl]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.579512]  [<ffffffffa02be3dc>] vcmn_err+0x6c/0x110 [spl]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.586696]  [<ffffffffa04c0030>] ? raidz_syn_qr_abd+0x300/0x420 [zfs]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.594727]  [<ffffffffa044a5cb>] ? zio_vdev_io_done+0x4b/0x200 [zfs]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.602632]  [<ffffffffa044b03c>] ? zio_execute+0x9c/0x100 [zfs]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.610115]  [<ffffffff811de0c5>] ? kmem_cache_alloc+0x35/0x1e0\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.617463]  [<ffffffffa02ba319>] ? spl_kmem_cache_alloc+0x99/0x150 [spl]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.625677]  [<ffffffffa02ba319>] ? spl_kmem_cache_alloc+0x99/0x150 [spl]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.633845]  [<ffffffffa03f13d9>] zfs_panic_recover+0x69/0x90 [zfs]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.641480]  [<ffffffffa0383b90>] ? arc_read+0xac0/0xac0 [zfs]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.648691]  [<ffffffffa044d361>] zfs_blkptr_verify+0x311/0x370 [zfs]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.656539]  [<ffffffffa044d3f4>] zio_read+0x34/0xe0 [zfs]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.663270]  [<ffffffff811dd4c1>] ? __kmalloc_node+0x1d1/0x2b0\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.670336]  [<ffffffffa02b91a0>] ? spl_kmem_zalloc+0xc0/0x170 [spl]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.677980]  [<ffffffffa02b91a0>] ? spl_kmem_zalloc+0xc0/0x170 [spl]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.685581]  [<ffffffffa0383b90>] ? arc_read+0xac0/0xac0 [zfs]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.692673]  [<ffffffffa03836ff>] arc_read+0x62f/0xac0 [zfs]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.699518]  [<ffffffffa03a8544>] ? traverse_prefetcher+0x54/0x1e0 [zfs]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.707365]  [<ffffffffa03a8682>] traverse_prefetcher+0x192/0x1e0 [zfs]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.715189]  [<ffffffffa03a8bd9>] traverse_visitbp+0x509/0x870 [zfs]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.722705]  [<ffffffffa03a95f1>] traverse_dnode+0xb1/0x1f0 [zfs]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.729989]  [<ffffffffa03a8dab>] traverse_visitbp+0x6db/0x870 [zfs]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.737511]  [<ffffffffa03a89e5>] traverse_visitbp+0x315/0x870 [zfs]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.745027]  [<ffffffffa03a89e5>] traverse_visitbp+0x315/0x870 [zfs]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.752521]  [<ffffffffa03a89e5>] traverse_visitbp+0x315/0x870 [zfs]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.759871]  [<ffffffffa03a89e5>] traverse_visitbp+0x315/0x870 [zfs]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.767288]  [<ffffffffa03a89e5>] traverse_visitbp+0x315/0x870 [zfs]\r\n2017-10-02T21:33:52-05:00 hmet0004 kernel: [269800.774605]  [<ffffffffa03a89e5>] traverse_visitbp+0x315/0x870 [zfs]\r\n2017-10-02T21:33:53-05:00 hmet0004 kernel: [269800.781877]  [<ffffffffa03a95f1>] traverse_dnode+0xb1/0x1f0 [zfs]\r\n2017-10-02T21:33:53-05:00 hmet0004 kernel: [269800.788852]  [<ffffffffa03a8e85>] traverse_visitbp+0x7b5/0x870 [zfs]\r\n2017-10-02T21:33:53-05:00 hmet0004 kernel: [269800.796105]  [<ffffffffa03a94a8>] traverse_prefetch_thread+0x108/0x1a0 [zfs]\r\n2017-10-02T21:33:53-05:00 hmet0004 kernel: [269800.804104]  [<ffffffffa03a84f0>] ? prefetch_needed.isra.5+0x40/0x40 [zfs]\r\n2017-10-02T21:33:53-05:00 hmet0004 kernel: [269800.811842]  [<ffffffffa02bbec6>] taskq_thread+0x246/0x470 [spl]\r\n2017-10-02T21:33:53-05:00 hmet0004 kernel: [269800.818629]  [<ffffffff810c4fd0>] ? wake_up_state+0x20/0x20\r\n2017-10-02T21:33:53-05:00 hmet0004 kernel: [269800.825027]  [<ffffffffa02bbc80>] ? taskq_thread_spawn+0x60/0x60 [spl]\r\n2017-10-02T21:33:53-05:00 hmet0004 kernel: [269800.832295]  [<ffffffff810b064f>] kthread+0xcf/0xe0\r\n2017-10-02T21:33:53-05:00 hmet0004 kernel: [269800.837930]  [<ffffffff810b0580>] ? kthread_create_on_node+0x140/0x140\r\n2017-10-02T21:33:53-05:00 hmet0004 kernel: [269800.845173]  [<ffffffff81696818>] ret_from_fork+0x58/0x90\r\n2017-10-02T21:33:53-05:00 hmet0004 kernel: [269800.851274]  [<ffffffff810b0580>] ? kthread_create_on_node+0x140/0x140\r\n```\r\n\r\n\r\n### Describe how to reproduce the problem\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6711/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6537", "title": "Hung zfs send", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  CentOS\r\nDistribution Version    | 7.3.1611\r\nLinux Kernel                 | 3.10.0-514.6.1.el7.x86_64\r\nArchitecture                 | intel x64\r\nZFS Version                  | 0.7.1-1\r\nSPL Version                  | 0.7.1-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nzfs send slowed and eventually hung, unable to be killed\r\n### Describe how to reproduce the problem\r\nzfs send -vLRc lustre@s1 | zfs receive -sF lustre-ssd\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\nThe send process has hung, unable to be killed or ctrl-c\r\n\r\n```\r\n20:06:08   57.0G   lustre/h4-MDT0000@s1\r\n20:06:09   57.0G   lustre/h4-MDT0000@s1\r\n20:06:10   57.0G   lustre/h4-MDT0000@s1\r\n20:06:11   57.0G   lustre/h4-MDT0000@s1\r\n20:06:12   57.0G   lustre/h4-MDT0000@s1\r\n20:06:13   57.0G   lustre/h4-MDT0000@s1\r\n^C\r\n\r\n^C\r\n^C^C^C\r\n```\r\n\r\nand\r\n\r\n```\r\n# ps -ef | grep zfs\r\nroot     19900 16450  0 Aug20 pts/8    00:02:58 zfs send -vLRc lustre s1\r\n# kill -9 19900\r\n# ps -ef | grep zfs\r\nroot     19900 16450  0 Aug20 pts/8    00:02:58 zfs send -vLRc lustre s1\r\n```\r\n\r\nand predictably...\r\n\r\n```\r\n# strace -p 19900\r\nProcess 19900 attached\r\n\r\n^C\r\n```\r\n\r\nand the stack trace of the process\r\n\r\n```\r\n# cat /proc/19900/stack \r\n[<ffffffffa02c0485>] cv_wait_common+0x125/0x150 [spl]\r\n[<ffffffffa02c04c5>] __cv_wait+0x15/0x20 [spl]\r\n[<ffffffffa03831f3>] arc_read+0x123/0xac0 [zfs]\r\n[<ffffffffa03a3fb4>] do_dump+0x434/0x9e0 [zfs]\r\n[<ffffffffa03a4b94>] dmu_send_impl+0x634/0xa30 [zfs]\r\n[<ffffffffa03a6833>] dmu_send_obj+0x1b3/0x250 [zfs]\r\n[<ffffffffa0422b79>] zfs_ioc_send+0xf9/0x310 [zfs]\r\n[<ffffffffa0428626>] zfsdev_ioctl+0x606/0x650 [zfs]\r\n[<ffffffff81212025>] do_vfs_ioctl+0x2d5/0x4b0\r\n[<ffffffff812122a1>] SyS_ioctl+0xa1/0xc0\r\n[<ffffffff816968c9>] system_call_fastpath+0x16/0x1b\r\n[<ffffffffffffffff>] 0xffffffffffffffff\r\n```\r\n\r\nAttached is a system stack trace dump.\r\n\r\nThis is running on a live production lustre meta-data server, so their will no doubt be a LOT of threads not involved in this.\r\n\r\n[zfs_stack_trace.txt](https://github.com/zfsonlinux/zfs/files/1240647/zfs_stack_trace.txt)\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6537/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "VERTlG0": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6710", "title": "root pool import now takes much longer to import at boot.", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Gentoo\r\nDistribution Version    | \r\nLinux Kernel                 | 4.13.4-gentoo\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.0-98_g01ff0d75\r\nSPL Version                   | 0.7.0-13_ge8474f9\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nAt boot, importing my root pool now takes considerably longer. Once \"Importing ZFS pool rpool\" is shown on the screen 36 seconds go by before it seems to starts to import the pool. This used to take 2-3 seconds. \r\n### Describe how to reproduce the problem\r\nI rebooted after the first boot, and the same thing occurs. Ive also used the system for the day, and rebooted and still the same. Ive rebooted three times in a row to see if it would go back to being a fast import but still the same on all three reboots. \r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\nheres a video of me booting the system, importing, and then repeating teh boot process a second time. https://www.youtube.com/watch?v=-YhIAJNj6hQ", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6710/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bjquinn": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6709", "title": "zpool clear hang when resuming suspended pool", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS\r\nDistribution Version    | 6.3\r\nLinux Kernel                 | 4.0.1 (4.0.1-1.el6.elrepo.x86_64)\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.7.1-1 (zfs-dkms-0.7.1-1.el6.noarch)\r\nSPL Version                  | 0.7.1-1 (spl-0.7.1-1.el6.x86_64)\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nSimilar to #3256.  Flaky USB enclosure (I assume) causes USB drive to temporarily disconnect and reconnect (in this specific case, disconnects as sdd, reconnects as sde).  Pool is built with by-id names.  Making sure drive is reconnected and issuing a zpool clear just hangs, even if the drive is disconnected long enough to bring it back up as sdd again.\r\n\r\nRe-opening a new bug as that was what was suggested in #3256 if this issue recurred with 0.7.1.  Would really prefer zpool export -F for suspended pools, but zpool clear would be sufficient if it would work.\r\n\r\nEDIT: I should add that the current state is still an improvement over 0.6.5.x, as a system in this state can run zpool/zfs commands on other pools, as well as a zpool list or zpool status affecting the suspended pool, which was not always the case when this happened previously.  Also, for the first time, I experienced a clean, normal reboot that didn't hang, whereas previously a hard reboot was always required due to hanging on shutdown while trying to unmount the suspended pool.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nI have a backup script that runs fairly heavly zfs send and rsync to an external SATA drive in a USB enclosure.  Once every month or two, the drive will drop out during the backup process and get into the state as described above.  When this happens, you can see something like the following logged at the console --\r\n\r\n```\r\nblk_update_request: I/O error, dev sdd, sector 25528\r\nsd 12:0:0:0: [sde] No Caching mode page found\r\nsd 12:0:0:0: [sde] Assuming drive cache: write through\r\n```\r\n\r\n... which is an indication that the drive dropped, but came back.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n```\r\nSep 29 17:32:28 server zed[8335]: agent post event: mapping 'EC_dev_remove' to 'resource.fs.zfs.removed'\r\nSep 29 17:32:28 server zed[8335]: agent post event: mapping 'EC_dev_remove' to 'resource.fs.zfs.removed'\r\nSep 29 17:32:28 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.removed for vdev 0\r\nSep 29 17:32:28 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.removed'\r\nSep 29 17:32:28 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.removed for vdev 11610466796109647632\r\nSep 29 17:32:28 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.removed'\r\nSep 29 17:32:28 server zed[8335]: Diagnosis Engine: opening case for vdev 0 due to 'ereport.fs.zfs.io_failure'\r\nSep 29 17:32:28 server zed[8335]: Diagnosis Engine: error event 'ereport.fs.zfs.io_failure'\r\nSep 29 17:32:28 server zed[8335]: Diagnosis Engine: solving fault 'fault.fs.zfs.io_failure_wait'\r\nSep 29 17:32:28 server zed[8335]: #011class: fault.fs.zfs.io_failure_wait\r\nSep 29 17:32:28 server zed[8335]: #011scheme: zfs\r\nSep 29 17:32:28 server zed[8335]: Retire Agent: zfs_retire_recv: 'list.suspect'\r\nSep 29 17:32:48 server zed[8335]: zfs_slm_event: EC_dev_add.disk\r\nSep 29 17:32:48 server zed[8335]: zfs_deliver_add: adding usb-Generic_External_57442D575834314432354C31-0:0-part1 (pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0) (is_slice 1)\r\nSep 29 17:32:48 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by devid)\r\nSep 29 17:32:48 server zed[8335]: zfs_iter_pool: evaluating vdevs on datapool (by devid)\r\nSep 29 17:32:48 server zed[8335]: zfs_iter_pool: evaluating vdevs on datapoolssd (by devid)\r\nSep 29 17:32:48 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by phys_path)\r\nSep 29 17:32:48 server zed[8335]:   zfs_iter_vdev: matched phys_path on pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0\r\nSep 29 17:32:48 server zed[8335]: zfs_process_add: /dev/disk/by-path/pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0-part1 is already healthy, skip it.\r\nSep 29 17:32:48 server zed[8335]: zfs_slm_event: EC_dev_add.disk\r\nSep 29 17:32:48 server zed[8335]: zfs_deliver_add: adding usb-Generic_External_57442D575834314432354C31-0:0-part9 (pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0) (is_slice 1)\r\nSep 29 17:32:48 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by devid)\r\nSep 29 17:32:48 server zed[8335]: zfs_iter_pool: evaluating vdevs on datapool (by devid)\r\nSep 29 17:32:48 server zed[8335]: zfs_iter_pool: evaluating vdevs on datapoolssd (by devid)\r\nSep 29 17:32:48 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by phys_path)\r\nSep 29 17:32:48 server zed[8335]:   zfs_iter_vdev: matched phys_path on pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0\r\nSep 29 17:32:48 server zed[8335]: zfs_process_add: /dev/disk/by-path/pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0-part1 is already healthy, skip it.\r\nOct  2 18:32:26 server zed[8335]: Diagnosis Engine: opening case for vdev 11610466796109647632 due to 'ereport.fs.zfs.vdev.too_small'\r\nOct  2 18:32:26 server zed[8335]: Diagnosis Engine: error event 'ereport.fs.zfs.vdev.too_small'\r\nOct  2 18:32:26 server zed[8335]: Diagnosis Engine: solving fault 'fault.fs.zfs.device'\r\nOct  2 18:32:26 server zed[8335]: #011class: fault.fs.zfs.device\r\nOct  2 18:32:26 server zed[8335]: #011scheme: zfs\r\nOct  2 18:32:26 server zed[8335]: Retire Agent: zfs_retire_recv: 'list.suspect'\r\nOct  2 18:32:26 server zed[8335]: Diagnosis Engine: resource event 'resource.fs.zfs.statechange'\r\nOct  2 18:32:26 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.statechange'\r\nOct  2 18:32:26 server zed[8335]: Diagnosis Engine: resource event 'resource.fs.zfs.statechange'\r\nOct  2 18:32:26 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.statechange'\r\nOct  2 18:32:26 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.statechange for vdev 11610466796109647632\r\nOct  2 18:32:26 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.statechange'\r\nOct  2 18:35:04 server zed[8335]: agent post event: mapping 'EC_dev_remove' to 'resource.fs.zfs.removed'\r\nOct  2 18:35:04 server zed[8335]: agent post event: mapping 'EC_dev_remove' to 'resource.fs.zfs.removed'\r\nOct  2 18:35:04 server zed[8335]: Diagnosis Engine: resource event 'resource.fs.zfs.removed'\r\nOct  2 18:35:04 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.removed'\r\nOct  2 18:35:04 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.removed for vdev 1955849306711511642\r\nOct  2 18:35:04 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.removed'\r\nOct  2 18:35:25 server zed[8335]: zfs_slm_event: EC_dev_add.disk\r\nOct  2 18:35:25 server zed[8335]: zfs_deliver_add: adding usb-Generic_External_57442D575834314432354C31-0:0-part1 (pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0) (is_slice 1)\r\nOct  2 18:35:25 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by devid)\r\nOct  2 18:35:25 server zed[8335]: zfs_iter_pool: evaluating vdevs on datapool (by devid)\r\nOct  2 18:35:25 server zed[8335]: zfs_iter_pool: evaluating vdevs on datapoolssd (by devid)\r\nOct  2 18:35:25 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by phys_path)\r\nOct  2 18:35:25 server zed[8335]:   zfs_iter_vdev: matched phys_path on pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0\r\nOct  2 18:35:25 server zed[8335]: zfs_process_add: pool 'backup' vdev '/dev/disk/by-path/pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0-part1', phys 'pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0' wholedisk 1, dm 0 (11610466796109647632)\r\nOct  2 18:35:25 server zed[8335]: zfs_slm_event: EC_dev_add.disk\r\nOct  2 18:35:25 server zed[8335]: zfs_deliver_add: adding usb-Generic_External_57442D575834314432354C31-0:0-part9 (pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0) (is_slice 1)\r\nOct  2 18:35:25 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by devid)\r\nOct  2 18:35:25 server zed[8335]: zfs_iter_pool: evaluating vdevs on datapool (by devid)\r\nOct  2 18:35:25 server zed[8335]: zfs_iter_pool: evaluating vdevs on datapoolssd (by devid)\r\nOct  2 18:35:25 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by phys_path)\r\nOct  2 18:35:25 server zed[8335]:   zfs_iter_vdev: matched phys_path on pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0\r\nOct  2 18:35:25 server zed[8335]: zfs_process_add: pool 'backup' vdev '/dev/disk/by-path/pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0-part1', phys 'pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0' wholedisk 1, dm 0 (11610466796109647632)\r\nOct  2 18:35:45 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.statechange for vdev 11610466796109647632\r\nOct  2 18:35:45 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.statechange'\r\nOct  2 18:35:45 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.statechange for vdev 11610466796109647632\r\nOct  2 18:35:45 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.statechange'\r\nOct  2 18:35:45 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.statechange for vdev 11610466796109647632\r\nOct  2 18:35:45 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.statechange'\r\nOct  2 18:35:45 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.statechange for vdev 11610466796109647632\r\nOct  2 18:35:45 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.statechange'\r\nOct  2 18:42:38 server zed[8335]: agent post event: mapping 'EC_dev_remove' to 'resource.fs.zfs.removed'\r\nOct  2 18:42:38 server zed[8335]: agent post event: mapping 'EC_dev_remove' to 'resource.fs.zfs.removed'\r\nOct  2 18:42:38 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.removed for vdev 1955849306711511642\r\nOct  2 18:42:38 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.removed'\r\nOct  2 18:42:38 server zed[8335]: Diagnosis Engine: resource event 'resource.fs.zfs.removed'\r\nOct  2 18:42:38 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.removed'\r\nOct  2 18:42:58 server zed[8335]: zfs_slm_event: EC_dev_add.disk\r\nOct  2 18:42:58 server zed[8335]: zfs_deliver_add: adding usb-Generic_External_323733384B344B3046353644-0:0-part1 (pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0) (is_slice 1)\r\nOct  2 18:42:58 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by devid)\r\nOct  2 18:42:58 server zed[8335]:   zfs_iter_vdev: matched devid on usb-Generic_External_323733384B344B3046353644-0:0-part1\r\nOct  2 18:42:58 server zed[8335]: zfs_process_add: pool 'backup' vdev '/dev/disk/by-path/pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0-part1', phys 'pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0' wholedisk 1, dm 0 (11610466796109647632)\r\nOct  2 18:42:58 server zed[8335]: zfs_slm_event: EC_dev_add.disk\r\nOct  2 18:42:58 server zed[8335]: zfs_deliver_add: adding usb-Generic_External_323733384B344B3046353644-0:0-part9 (pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0) (is_slice 1)\r\nOct  2 18:42:58 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by devid)\r\nOct  2 18:42:58 server zed[8335]: zfs_iter_pool: evaluating vdevs on datapool (by devid)\r\nOct  2 18:42:58 server zed[8335]: zfs_iter_pool: evaluating vdevs on datapoolssd (by devid)\r\nOct  2 18:42:58 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by phys_path)\r\nOct  2 18:42:58 server zed[8335]:   zfs_iter_vdev: matched phys_path on pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0\r\nOct  2 18:42:58 server zed[8335]: zfs_process_add: pool 'backup' vdev '/dev/disk/by-path/pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0-part1', phys 'pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0' wholedisk 1, dm 0 (11610466796109647632)\r\nOct  2 18:43:11 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.statechange for vdev 11610466796109647632\r\nOct  2 18:43:11 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.statechange'\r\nOct  2 18:43:11 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.statechange for vdev 11610466796109647632\r\nOct  2 18:43:11 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.statechange'\r\nOct  2 18:43:11 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.statechange for vdev 11610466796109647632\r\nOct  2 18:43:11 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.statechange'\r\nOct  2 18:43:11 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.statechange for vdev 11610466796109647632\r\nOct  2 18:43:11 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.statechange'\r\nOct  2 18:43:52 server zed[8335]: agent post event: mapping 'EC_dev_remove' to 'resource.fs.zfs.removed'\r\nOct  2 18:43:52 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.removed for vdev 11610466796109647632\r\nOct  2 18:43:52 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.removed'\r\nOct  2 18:43:52 server zed[8335]: agent post event: mapping 'EC_dev_remove' to 'resource.fs.zfs.removed'\r\nOct  2 18:43:52 server zed[8335]: Diagnosis Engine: resource event 'resource.fs.zfs.removed'\r\nOct  2 18:43:52 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.removed'\r\nOct  2 18:44:29 server zed[8335]: zfs_slm_event: EC_dev_add.disk\r\nOct  2 18:44:29 server zed[8335]: zfs_deliver_add: adding usb-ASMT_2105_AC0000000001-0:0-part1 (pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0) (is_slice 1)\r\nOct  2 18:44:29 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by devid)\r\nOct  2 18:44:29 server zed[8335]: zfs_iter_pool: evaluating vdevs on datapool (by devid)\r\nOct  2 18:44:29 server zed[8335]: zfs_iter_pool: evaluating vdevs on datapoolssd (by devid)\r\nOct  2 18:44:29 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by phys_path)\r\nOct  2 18:44:29 server zed[8335]:   zfs_iter_vdev: matched phys_path on pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0\r\nOct  2 18:44:29 server zed[8335]: zfs_process_add: pool 'backup' vdev '/dev/disk/by-path/pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0-part1', phys 'pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0' wholedisk 1, dm 0 (11610466796109647632)\r\nOct  2 18:44:59 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.statechange for vdev 11610466796109647632\r\nOct  2 18:44:59 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.statechange'\r\nOct  2 18:44:59 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.statechange for vdev 11610466796109647632\r\nOct  2 18:44:59 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.statechange'\r\nOct  2 18:44:59 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.statechange for vdev 11610466796109647632\r\nOct  2 18:44:59 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.statechange'\r\nOct  2 18:44:59 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.statechange for vdev 11610466796109647632\r\nOct  2 18:44:59 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.statechange'\r\nOct  2 18:51:23 server zed[8335]: agent post event: mapping 'EC_dev_remove' to 'resource.fs.zfs.removed'\r\nOct  2 18:51:23 server zed[8335]: agent post event: mapping 'EC_dev_remove' to 'resource.fs.zfs.removed'\r\nOct  2 18:51:23 server zed[8335]: Diagnosis Engine: resource event 'resource.fs.zfs.removed'\r\nOct  2 18:51:23 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.removed'\r\nOct  2 18:51:23 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.removed for vdev 6792593964022512890\r\nOct  2 18:51:23 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.removed'\r\nOct  2 18:51:46 server zed[8335]: zfs_slm_event: EC_dev_add.disk\r\nOct  2 18:51:46 server zed[8335]: zfs_deliver_add: adding usb-Generic_External_323733384B344B3046353644-0:0-part1 (pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0) (is_slice 1)\r\nOct  2 18:51:46 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by devid)\r\nOct  2 18:51:46 server zed[8335]:   zfs_iter_vdev: matched devid on usb-Generic_External_323733384B344B3046353644-0:0-part1\r\nOct  2 18:51:46 server zed[8335]: zfs_process_add: pool 'backup' vdev '/dev/disk/by-path/pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0-part1', phys 'pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0' wholedisk 1, dm 0 (11610466796109647632)\r\nOct  2 18:51:46 server zed[8335]: zfs_slm_event: EC_dev_add.disk\r\nOct  2 18:51:46 server zed[8335]: zfs_deliver_add: adding usb-Generic_External_323733384B344B3046353644-0:0-part9 (pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0) (is_slice 1)\r\nOct  2 18:51:46 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by devid)\r\nOct  2 18:51:46 server zed[8335]: zfs_iter_pool: evaluating vdevs on datapool (by devid)\r\nOct  2 18:51:46 server zed[8335]: zfs_iter_pool: evaluating vdevs on datapoolssd (by devid)\r\nOct  2 18:51:46 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by phys_path)\r\nOct  2 18:51:46 server zed[8335]:   zfs_iter_vdev: matched phys_path on pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0\r\nOct  2 18:51:46 server zed[8335]: zfs_process_add: pool 'backup' vdev '/dev/disk/by-path/pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0-part1', phys 'pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0' wholedisk 1, dm 0 (11610466796109647632)\r\nOct  2 18:56:03 server zed[8335]: agent post event: mapping 'EC_dev_remove' to 'resource.fs.zfs.removed'\r\nOct  2 18:56:03 server zed[8335]: agent post event: mapping 'EC_dev_remove' to 'resource.fs.zfs.removed'\r\nOct  2 18:56:03 server zed[8335]: Diagnosis Engine: resource event 'resource.fs.zfs.removed'\r\nOct  2 18:56:03 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.removed'\r\nOct  2 18:56:03 server zed[8335]: Diagnosis Engine: discarding 'resource.fs.zfs.removed for vdev 11610466796109647632\r\nOct  2 18:56:03 server zed[8335]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.removed'\r\nOct  2 18:56:25 server zed[8335]: zfs_slm_event: EC_dev_add.disk\r\nOct  2 18:56:25 server zed[8335]: zfs_deliver_add: adding usb-Generic_External_57442D575834314432354C31-0:0-part1 (pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0) (is_slice 1)\r\nOct  2 18:56:25 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by devid)\r\nOct  2 18:56:25 server zed[8335]: zfs_iter_pool: evaluating vdevs on datapool (by devid)\r\nOct  2 18:56:25 server zed[8335]: zfs_iter_pool: evaluating vdevs on datapoolssd (by devid)\r\nOct  2 18:56:25 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by phys_path)\r\nOct  2 18:56:25 server zed[8335]:   zfs_iter_vdev: matched phys_path on pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0\r\nOct  2 18:56:25 server zed[8335]: zfs_process_add: pool 'backup' vdev '/dev/disk/by-path/pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0-part1', phys 'pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0' wholedisk 1, dm 0 (11610466796109647632)\r\nOct  2 18:56:25 server zed[8335]: zfs_slm_event: EC_dev_add.disk\r\nOct  2 18:56:25 server zed[8335]: zfs_deliver_add: adding usb-Generic_External_57442D575834314432354C31-0:0-part9 (pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0) (is_slice 1)\r\nOct  2 18:56:25 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by devid)\r\nOct  2 18:56:25 server zed[8335]: zfs_iter_pool: evaluating vdevs on datapool (by devid)\r\nOct  2 18:56:25 server zed[8335]: zfs_iter_pool: evaluating vdevs on datapoolssd (by devid)\r\nOct  2 18:56:25 server zed[8335]: zfs_iter_pool: evaluating vdevs on backup (by phys_path)\r\nOct  2 18:56:25 server zed[8335]:   zfs_iter_vdev: matched phys_path on pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0\r\nOct  2 18:56:25 server zed[8335]: zfs_process_add: pool 'backup' vdev '/dev/disk/by-path/pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0-part1', phys 'pci-0000:00:14.0-usb-0:8:1.0-scsi-0:0:0:0' wholedisk 1, dm 0 (11610466796109647632)\r\nOct  2 18:59:14 server zed[7824]: zfs_unavail_pool: examining 'datapool' (state 7)\r\nOct  2 18:59:14 server zed[7824]: zfs_unavail_pool: examining 'datapoolssd' (state 7)\r\nOct  2 18:59:14 server zed[7824]: Diagnosis Engine: discarding 'resource.fs.zfs.statechange for vdev 11610466796109647632\r\nOct  2 18:59:14 server zed[7824]: Retire Agent: zfs_retire_recv: 'resource.fs.zfs.statechange'\r\nOct  2 18:59:14 server zed[7824]: Diagnosis Engine: ignoring 'ereport.fs.zfs.zpool' during import\r\nOct  2 19:16:58 server kernel: [<ffffffffa04e07c2>] zvol_create_minors_impl+0xe2/0x220 [zfs]\r\nOct  2 19:16:58 server kernel: [<ffffffffa04e1bac>] zvol_task_cb+0x10c/0x150 [zfs]\r\nOct  2 19:24:58 server kernel: [<ffffffffa04e07c2>] zvol_create_minors_impl+0xe2/0x220 [zfs]\r\nOct  2 19:24:58 server kernel: [<ffffffffa04e1bac>] zvol_task_cb+0x10c/0x150 [zfs]\r\n```\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6709/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "phik": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6705", "title": "Corrupted directory; scrub reports no errors.", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS\r\nDistribution Version    | 7.3.1611\r\nLinux Kernel                 | 3.10.0-514.6.1.el7.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.1-1\r\nSPL Version                  | 0.7.1-1\r\n\r\n### Describe the problem you're observing\r\n\r\nThis is a Lustre MDS with the following history:\r\n\r\n* Installed originally as Lustre 2.9.52 and ZFS 0.7-rc3\r\n* **21 Jul** - Discovered that our performance woes and vanishing MDS space were due to dnode size.  Upgraded to ZFS 0.7-rc5 and changed dnodesize=auto\r\n* **26 Jul** - Encountered the corruption described in #6414\r\n* **16 Aug** - Upgraded ZFS to 0.7.1\r\n* **11 Sep** - Upgraded Lustre to 2.10.50 + LU-9305\r\n* **28 Jul** - A directory was created.\r\n\r\nThis directory existed for a long time -- hundreds of cluster jobs created, and later used, files inside this tree -- so we know that the directory was functional.  i.e. there's no chance that it was simply never written to disk.\r\n\r\nSometime between July 31 and now, that directory has become unusable.  Any attempt to stat() it from a Lustre client gives -ENOENT, and this appears in the MDS syslog:\r\n\r\n```\r\n2017-09-19T01:49:55-05:00 hmet0004 kernel: [598548.046650] LustreError: 13315:0:(osd_object.c:409:osd_object_init()) h4-MDT0000: lookup [0x2000111b9:0x1cf6:0x0]/0x1238cc1 failed: rc = -2\r\n```\r\n\r\nCreating and mounting a ZFS snapshot shows that the problem isn't with Lustre, but rather the underlying filesystem:\r\n\r\n```\r\n# pwd\r\n/mnt/mount_test/ROOT/000scratch/corrupt/010rays_Rays\r\n# ls -la\r\nls: cannot access 030rays: No such file or directory\r\ntotal 50\r\ndrwxrwsr-x 3 phils   root       2 Sep 18 05:06 .\r\ndrwxrwsr-x 6 phils   root       2 Sep 18 05:07 ..\r\nd????????? ? ?       ?          ?            ? 030rays\r\n```\r\n\r\nThere are no errors listed in zpool status, and a scrub found zero errors.\r\n\r\nSo how can this happen?  And how can we fix it?\r\n\r\nIt does seem to me like #6439 could result in an old dnode being clobbered by a new one.  But my understanding was that the parent of the clobbered dnode held the checksum for its child -- and that when the clobbered dnode was later read, and it found that the checksums differed, ZFS would print something in the syslog (or panic, as seems to be the default behaviour)?  But perhaps I've misunderstood how the hierarchical checksums work.\r\n\r\nFor me, there are two issues:\r\n\r\n1. Obviously, understanding the nature of this corruption so that we can all be sure it doesn't happen again.  Fortunately this directory is relatively unimportant -- but if it happened to a top-level directory, it would be a total catastrophe.\r\n\r\n2. There were millions of files inside this directory, representing gigabytes of metadata.  Without a zfsck, how do we clean up the MDS objects that are no longer attached to the filesystem tree?  Given that it's a Lustre MDS, we don't even have the option of rsyncing all of the (remaining, reachable) data to a new empty filesystem.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6705/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6634", "title": "PANIC: blkptr at ffff881e4c9c8440 has invalid BLK_BIRTH 0", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS\r\nDistribution Version    | 7.3.1611\r\nLinux Kernel                 | 3.10.0-514.6.1.el7.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.1-1\r\nSPL Version                  | 0.7.1-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n```\r\nPANIC: blkptr at ffff881d7f8a4440 has invalid BLK_BIRTH 0\r\nShowing stack for process 29978\r\nCPU: 5 PID: 29978 Comm: dp_sync_taskq Tainted: P           OE  ------------   3.10.0-514.6.1.el7.x86_64 #1\r\nHardware name: GIGABYTE D120-C21/MC20-S10, BIOS F02 05/19/2016\r\n 0000000000000003 00000000e540ae0c ffff881eed773938 ffffffff816862ac\r\n ffff881eed773948 ffffffffa02be254 ffff881eed773a70 ffffffffa02be3dc\r\n ffff881f5de46d50 61207274706b6c62 3838666666662074 3434613866376431\r\nCall Trace:\r\n [<ffffffff816862ac>] dump_stack+0x19/0x1b\r\n [<ffffffffa02be254>] spl_dumpstack+0x44/0x50 [spl]\r\n [<ffffffffa02be3dc>] vcmn_err+0x6c/0x110 [spl]\r\n [<ffffffffa038f888>] ? dbuf_dirty+0x3f8/0x810 [zfs]\r\n [<ffffffffa03ad83c>] ? dnode_setdirty+0xdc/0x120 [zfs]\r\n [<ffffffffa038f888>] ? dbuf_dirty+0x3f8/0x810 [zfs]\r\n [<ffffffffa02ba319>] ? spl_kmem_cache_alloc+0x99/0x150 [spl]\r\n [<ffffffffa03f13d9>] zfs_panic_recover+0x69/0x90 [zfs]\r\n [<ffffffffa03bafa6>] dsl_deadlist_insert+0x366/0x3d0 [zfs]\r\n [<ffffffff8168a7a8>] ? __mutex_lock_slowpath+0x1a8/0x1c0\r\n [<ffffffff811dc341>] ? __slab_free+0x81/0x2f0\r\n [<ffffffffa03efde9>] ? dva_get_dsize_sync+0x39/0x50 [zfs]\r\n [<ffffffff81689b22>] ? mutex_lock+0x12/0x2f\r\n [<ffffffffa03b0b2b>] dsl_dataset_block_kill+0x24b/0x4c0 [zfs]\r\n [<ffffffffa03af16f>] free_blocks+0x14f/0x270 [zfs]\r\n [<ffffffffa03af832>] dnode_sync_free_range+0x242/0x2a0 [zfs]\r\n [<ffffffffa038de8e>] ? dbuf_destroy+0x24e/0x370 [zfs]\r\n [<ffffffffa03af5f0>] ? free_children+0x360/0x360 [zfs]\r\n [<ffffffffa03d7f26>] range_tree_vacate+0x66/0x140 [zfs]\r\n [<ffffffffa03afecc>] dnode_sync+0x2ac/0x8a0 [zfs]\r\n [<ffffffffa039c0a2>] sync_dnodes_task+0x82/0xc0 [zfs]\r\n [<ffffffffa02bbec6>] taskq_thread+0x246/0x470 [spl]\r\n [<ffffffff810c4fd0>] ? wake_up_state+0x20/0x20\r\n [<ffffffffa02bbc80>] ? taskq_thread_spawn+0x60/0x60 [spl]\r\n [<ffffffff810b064f>] kthread+0xcf/0xe0\r\n [<ffffffff810b0580>] ? kthread_create_on_node+0x140/0x140\r\n [<ffffffff81696818>] ret_from_fork+0x58/0x90\r\n [<ffffffff810b0580>] ? kthread_create_on_node+0x140/0x140\r\n```\r\n\r\nAfter reboot, while replaying Lustre requests, it triggered the same panic with a nearly-identical stack trace which I include only for the sake of completeness:\r\n\r\n```\r\nPANIC: blkptr at ffff881e4c9c8440 has invalid BLK_BIRTH 0\r\nShowing stack for process 29997\r\nCPU: 3 PID: 29997 Comm: dp_sync_taskq Tainted: P           OE  ------------   3.10.0-514.6.1.el7.x86_64 #1\r\nHardware name: GIGABYTE D120-C21/MC20-S10, BIOS F02 05/19/2016\r\n 0000000000000003 00000000f2cd8365 ffff881ed1a2b938 ffffffff816862ac\r\n ffff881ed1a2b948 ffffffffa02be254 ffff881ed1a2ba70 ffffffffa02be3dc\r\n ffff881f12076010 61207274706b6c62 3838666666662074 3438633963346531\r\nCall Trace:\r\n [<ffffffff816862ac>] dump_stack+0x19/0x1b\r\n [<ffffffffa02be254>] spl_dumpstack+0x44/0x50 [spl]\r\n [<ffffffffa02be3dc>] vcmn_err+0x6c/0x110 [spl]\r\n [<ffffffffa038f888>] ? dbuf_dirty+0x3f8/0x810 [zfs]\r\n [<ffffffffa03ad83c>] ? dnode_setdirty+0xdc/0x120 [zfs]\r\n [<ffffffffa038f888>] ? dbuf_dirty+0x3f8/0x810 [zfs]\r\n [<ffffffffa03f13d9>] zfs_panic_recover+0x69/0x90 [zfs]\r\n [<ffffffffa03bafa6>] dsl_deadlist_insert+0x366/0x3d0 [zfs]\r\n [<ffffffff810c4e18>] ? try_to_wake_up+0x1c8/0x320\r\n [<ffffffff810c4fe2>] ? default_wake_function+0x12/0x20\r\n [<ffffffff810ba238>] ? __wake_up_common+0x58/0x90\r\n [<ffffffffa03efde9>] ? dva_get_dsize_sync+0x39/0x50 [zfs]\r\n [<ffffffff81689b22>] ? mutex_lock+0x12/0x2f\r\n [<ffffffffa03b0b2b>] dsl_dataset_block_kill+0x24b/0x4c0 [zfs]\r\n [<ffffffffa03ad759>] ? dnode_rele+0x39/0x40 [zfs]\r\n [<ffffffffa03af16f>] free_blocks+0x14f/0x270 [zfs]\r\n [<ffffffffa03af832>] dnode_sync_free_range+0x242/0x2a0 [zfs]\r\n [<ffffffffa03af5f0>] ? free_children+0x360/0x360 [zfs]\r\n [<ffffffffa03d7f26>] range_tree_vacate+0x66/0x140 [zfs]\r\n [<ffffffffa03afecc>] dnode_sync+0x2ac/0x8a0 [zfs]\r\n [<ffffffff81029569>] ? __switch_to+0xd9/0x4c0\r\n [<ffffffffa039c0a2>] sync_dnodes_task+0x82/0xc0 [zfs]\r\n [<ffffffffa02bbec6>] taskq_thread+0x246/0x470 [spl]\r\n [<ffffffff810c4fd0>] ? wake_up_state+0x20/0x20\r\n [<ffffffffa02bbc80>] ? taskq_thread_spawn+0x60/0x60 [spl]\r\n [<ffffffff810b064f>] kthread+0xcf/0xe0\r\n [<ffffffff810b0580>] ? kthread_create_on_node+0x140/0x140\r\n [<ffffffff81696818>] ret_from_fork+0x58/0x90\r\n [<ffffffff810b0580>] ? kthread_create_on_node+0x140/0x140\r\n```\r\n\r\nMy limited interpretation of those tea leaves leads me to believe that it might be related to dnode deletion?  Indeed we're doing almost round-the-clock deletions in an effort to remove tens of millions of old files.  These are mostly, but not exclusively, old dnodes (i.e. created before dnodesize=auto).\r\n\r\nThe fact that we hit the identical crash during Lustre recovery makes me wonder if it's triggered by the deletion of a specific file (which would've then been replayed during recovery, causing the same crash).  We decided to kill the deletion task, and preemptively evict all clients, which has allowed us to get the system back up.\r\n\r\nWe'll avoid deleting the two directories that were in progress at the time of the crash.  Perhaps one of those two would allow us to reproduce the crash on demand, if required.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6634/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6497", "title": "``zpool import -T`` attempts to read the (entire?) filesystem at 2 MB/s", "body": "In issue #6414, a bug caused the creation of an invalid block but with a valid checksum -- leading to a kernel panic on import.\r\n\r\n``zpool import -FX`` didn't work, hitting the same panic (which is the topic of #6496)\r\n\r\nWe also attempted ``zpool import -T txg``, and while this _didn't_ cause the panic, it also didn't really work.  It started reading the (10 TB x 4) filesystem at about 2 MB/s, an effort that we gave up on after about 4 hours.\r\n\r\nManually zeroing the latest transactions (using the terrifying zfs_revert script at https://gist.github.com/jshoward/5685757) does work, however, and imports instantly.\r\n\r\nSo the fundamental idea of mounting this particular damaged filesystem from an earlier transaction is sound -- just need to figure out why ``import -T`` wants to do a 23-day read of the filesystem first.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6497/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6496", "title": "zpool import -F can't recover from a specific type of corrupted block", "body": "In issue #6414, a bug caused the creation of an invalid block but with a valid checksum -- leading to a kernel panic on import.\r\n\r\nWe attempted to use ``zpool import -FX`` and ``zpool import -FXn``, but both failed with the same panic.\r\n\r\nI can appreciate how challenging it may be to get arbitrarily-far through the import process, detect a problem, and then somehow roll all the way back to the beginning and try again with a different txg!  But Brian suggested that this should work, and I'm dutifully filing this issue.\r\n\r\nThe good news is that it sounds very easy to create a small test filesystem with this defect, by using 0.7.0rc5 and the test that was added in #6414", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6496/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gregwalters": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6698", "title": "CentOS 2.6.32-696.10.3 unknown symbols in module", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS\r\nDistribution Version    | 6.9\r\nLinux Kernel                 | 2.6.32-696.10.3.el6\r\nArchitecture                 | x86_64\r\n\r\nZFS Version                  | 0.7.2-1.el6\r\nSPL Version                  | 0.7.2-1.el6\r\n\r\n### Describe the problem you're observing\r\nUnable to import zfs module after updating.\r\n\r\n### Describe how to reproduce the problem\r\n`modprobe zfs`\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nFrom dmesg:\r\n```\r\nzfs: Unknown symbol dataset_namecheck                                                                                                                      [0/1685]\r\nzfs: Unknown symbol fletcher_2_incremental_byteswap\r\nzfs: Unknown symbol fletcher_2_incremental_native\r\nzfs: Unknown symbol fletcher_2_native\r\nzfs: Unknown symbol fletcher_4_abd_ops\r\nzfs: Unknown symbol fletcher_4_incremental_byteswap\r\nzfs: Unknown symbol fletcher_4_incremental_native\r\nzfs: Unknown symbol fletcher_4_native_varsize\r\nzfs: Unknown symbol fletcher_init\r\nzfs: Unknown symbol pool_namecheck\r\nzfs: Unknown symbol zfs_component_namecheck\r\nzfs: Unknown symbol zfs_name_to_prop\r\nzfs: Unknown symbol zfs_prop_default_numeric\r\nzfs: Unknown symbol zfs_prop_default_string\r\nzfs: Unknown symbol zfs_prop_get_type\r\nzfs: Unknown symbol zfs_prop_index_to_string\r\nzfs: Unknown symbol zfs_prop_inheritable\r\nzfs: Unknown symbol zfs_prop_readonly\r\nzfs: Unknown symbol zfs_prop_setonce\r\nzfs: Unknown symbol zfs_prop_to_name\r\nzfs: Unknown symbol zfs_ratelimit\r\nzfs: Unknown symbol zfs_ratelimit_fini\r\nzfs: Unknown symbol zfs_ratelimit_init\r\nzfs: Unknown symbol zpool_name_to_prop\r\nzfs: Unknown symbol zpool_prop_default_numeric\r\nzfs: Unknown symbol zpool_prop_get_type\r\nzfs: Unknown symbol zpool_prop_index_to_string\r\nzfs: Unknown symbol zpool_prop_to_name\r\n```\r\n\r\nYum output when re-installing:\r\n```\r\nRunning Transaction\r\n  Installing : spl-0.7.2-1.el6.x86_64                                        1/4\r\n  Installing : kmod-spl-0.7.2-1.el6.x86_64                                   2/4\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zpios.ko needs unknown symbol dmu_tx_hold_write\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zpios.ko needs unknown symbol dmu_read\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zpios.ko needs unknown symbol dmu_tx_assign\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zpios.ko needs unknown symbol dmu_tx_create\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zpios.ko needs unknown symbol dmu_object_alloc\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zpios.ko needs unknown symbol dmu_object_free\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zpios.ko needs unknown symbol dmu_objset_own\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zpios.ko needs unknown symbol dsl_destroy_head\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zpios.ko needs unknown symbol dmu_write\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zpios.ko needs unknown symbol dmu_objset_disown\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zpios.ko needs unknown symbol dmu_tx_commit\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zpios.ko needs unknown symbol dmu_tx_wait\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zpios.ko needs unknown symbol dmu_tx_abort\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zpios.ko needs unknown symbol dmu_object_set_blocksize\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zpios.ko needs unknown symbol dmu_objset_create\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zpios.ko needs unknown symbol dmu_tx_hold_free\r\n  Installing : kmod-zfs-0.7.2-1.el6.x86_64                                   3/4\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zfs/zfs/zfs.ko needs unknown symbol zfs_ratelimit\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zfs/zfs/zfs.ko needs unknown symbol fletcher_4_native_varsize\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zfs/zfs/zfs.ko needs unknown symbol zfs_ratelimit_fini\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zfs/zfs/zfs.ko needs unknown symbol fletcher_init\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zfs/zfs/zfs.ko needs unknown symbol fletcher_2_incremental_native\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zfs/zfs/zfs.ko needs unknown symbol fletcher_4_abd_ops\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zfs/zfs/zfs.ko needs unknown symbol zfs_ratelimit_init\r\nWARNING: /lib/modules/2.6.32-696.6.3.el6.x86_64/weak-updates/zfs/zfs/zfs.ko needs unknown symbol fletcher_2_incremental_byteswap\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6698/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gilcn": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6697", "title": "System crash/freeze", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS\r\nDistribution Version    | 7.4.1708\r\nLinux Kernel                 | 3.10.0-693.2.2.el7.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.1 / 0.7.2\r\nSPL Version                  | 0.7.1 / 0.7.2\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nLast sunday, I found a host (a mailstore hosting 49k mailboxes) completely frozen for no particular reason. The system, which was working perfectly before, was upgraded from 0.6.5.11 to 0.7.1 a couple of days before the crash.\r\n\r\nThe freeze happened during the execution of a pretty IO intensive script which runs every sunday. The server screen was black, system was unresponsive and, after rebooting, I didn't find any relevant infos in the system logs. \u2639\ufe0f\r\n\r\nYesterday, while experimenting with zfs send/receive on two test servers (different hardware, same OS & ZFS/SPL versions), I managed to freeze the receiving host in a reproducible manner. This freeze is happening with both 0.7.1 & 0.7.2 versions (kmod packages).\r\n\r\nI don't know if this is the same bug in both cases but the result is the same : complete freeze, no message whether on the screen or in the logs!\r\n\r\n\r\n### Describe how to reproduce the problem\r\nreceiverhost# nc -l -p 12345 | zfs receive -v tank/dataset\r\nsenderhost# zfs send -D -L -c -v tank/dataset@snap01 | nc receiverhost 12345\r\n\r\nAfter a few seconds, the receiving host freezes. Omitting the -D flag doesn't trigger this freeze.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6697/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bparker06": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6690", "title": "free space listed by \"zfs list\" increases over time", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 16.04.3 LTS\r\nLinux Kernel                 | 4.11.0-14-generic\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.0-86_g5df5d06\r\nSPL Version                  | 0.7.0-12_g9df9692\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nThe amount of free space listed by `zfs list` seems to keep increasing dramatically over time.\r\n\r\nI have a 3x4TB raid5 array that appears as 7.19T in `zpool list`, which seems correct, but currently it says 1.38P free in `zfs list`. And two days ago it said 562T. The only thing that has changed is I've been steadily copying data into the pool. Nothing regarding the disk array or hardware has changed at all.\r\n\r\n### Describe how to reproduce the problem\r\nSimply run `zfs list` as time progresses.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n\r\nThe filesystem settings that I've changed from the defaults are encryption, compression and deduplication:\r\n\r\n```\r\n$ zfs get encryption,compress,dedup\r\nNAME             PROPERTY     VALUE          SOURCE\r\nstorage          encryption   off            default\r\nstorage          compression  lz4            local\r\nstorage          dedup        off            default\r\nstorage/storage  encryption   aes-256-ccm    -\r\nstorage/storage  compression  lz4            local\r\nstorage/storage  dedup        on             local\r\n```\r\n\r\nCurrent `zpool list`:\r\n\r\n```\r\n$ zpool list\r\nNAME      SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT\r\nstorage  7.19T   230G  6.96T         -     1%     3%  1.13x  ONLINE  -\r\n```\r\n\r\n`zfs list` from two days ago:\r\n\r\n```\r\n$ zfs list\r\nNAME              USED  AVAIL  REFER  MOUNTPOINT\r\nstorage           171G   562T    24K  none\r\n```\r\n\r\n`zfs list` from today:\r\n\r\n```\r\n$ zfs list\r\nNAME              USED  AVAIL  REFER  MOUNTPOINT\r\nstorage           241G  1.38P    24K  none\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6690/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tobia": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6687", "title": "Symlink permission issue", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian\r\nDistribution Version    | 8.9\r\nLinux Kernel                 | 4.4.76-93 packaged by Proxmox PVE\r\nArchitecture                 | x86_64\r\nZFS Version                  | v0.6.5.9-1\r\nSPL Version                  | v0.6.5.9-1\r\n\r\n### Describe the problem you're observing\r\n\r\nI have two symlinks to the same target. The target is readable by anybody. One of the symlinks (\"bad\") is owned by 1001 and only works for user 1001. *Not ever root can use it, it results in Permission denied.* The other symlink (\"good\") can be used by anybody. \r\n\r\n    # ls -lni\r\n    10609277 lrwxrwxrwx 1 1001 100 12 Nov  7  2016 bad -> dir\r\n    10609301 drwxrwxr-x 9 1001 100 11 Jun  3  2016 dir\r\n    17429605 lrwxrwxrwx 1    0   0 12 Sep 27 12:54 good -> dir\r\n    # cmp <(readlink good) <(readlink bad)\r\n\r\n    # id\r\n    uid=0(root) gid=0(root) groups=0(root)\r\n    # ls good\r\n    (directory entries...)\r\n    # ls bad\r\n    ls: cannot access bad: Permission denied\r\n\r\n    $ id\r\n    uid=1001(tobia) ...\r\n    $ ls good\r\n    (directory entries...)\r\n    $ ls bad\r\n    (directory entries...)\r\n\r\n### Describe how to reproduce the problem\r\n\r\nI don't know. I ran a few `chown -R`, `chmod -R`, and `setfacl -R` on that directory in the past few days, and I ended up with this weird link. I have not yet rebooted.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\nEvery time I try to access the bad link, I get the following line in the system log:\r\n\r\n`Sep 27 13:31:39 krishna kernel: [1222740.665800] audit: type=1302 audit(1506511899.188:5701): item=0 name=\"/rpool/data/subvol-106-disk-1/opt/bad\" inode=10609277 dev=00:be mode=0120777 ouid=1001 ogid=100 rdev=00:00 nametype=NORMAL`\r\n\r\nHere is the `zdb -dddd` output for the two links:\r\n\r\n```\r\n# zdb -dddd rpool/data/subvol-106-disk-1 10609277\r\nDataset rpool/data/subvol-106-disk-1 [ZPL], ID 394, cr_txg 2471211, 75.3G, 5029913 objects, rootbp DVA[0]=<0:81dd947000:1000> DVA[1]=<0:1ec7d92000:1000> [L0 DMU objset] fletcher4 lz4 LE contiguous unique double size=800L/200P birth=5895167L/5895167P fill=5029913 cksum=1344f425d5:64bf7d6ee0d:121e33346391e:255a7cfe497934\r\n\r\n    Object  lvl   iblk   dblk  dsize  lsize   %full  type\r\n  10609277    1    16K    512      0    512    0.00  ZFS plain file\r\n                                        188   bonus  System attributes\r\n\tdnode flags: USERUSED_ACCOUNTED \r\n\tdnode maxblkid: 0\r\n\tpath\t/opt/bad\r\n\tuid     1001\r\n\tgid     100\r\n\tatime\tFri Jun 30 18:33:43 2017\r\n\tmtime\tMon Nov  7 13:06:32 2016\r\n\tctime\tWed Sep 27 12:54:22 2017\r\n\tcrtime\tFri Jun 30 18:33:43 2017\r\n\tgen\t2472900\r\n\tmode\t120777\r\n\tsize\t12\r\n\tparent\t19383\r\n\tlinks\t1\r\n\tpflags\t40800000104\r\n\r\n# zdb -dddd rpool/data/subvol-106-disk-1 17429605\r\nDataset rpool/data/subvol-106-disk-1 [ZPL], ID 394, cr_txg 2471211, 75.3G, 5029913 objects, rootbp DVA[0]=<0:81d3ed5000:1000> DVA[1]=<0:1ec80a8000:1000> [L0 DMU objset] fletcher4 lz4 LE contiguous unique double size=800L/200P birth=5895170L/5895170P fill=5029913 cksum=150faf3a2b:6b85e4e7a73:12e1a4166a94f:2612f2363b6b5b\r\n\r\n    Object  lvl   iblk   dblk  dsize  lsize   %full  type\r\n  17429605    1    16K    512      0    512    0.00  ZFS plain file\r\n                                        188   bonus  System attributes\r\n\tdnode flags: USERUSED_ACCOUNTED \r\n\tdnode maxblkid: 0\r\n\tpath\t/opt/good\r\n\tuid     0\r\n\tgid     0\r\n\tatime\tWed Sep 27 12:54:30 2017\r\n\tmtime\tWed Sep 27 12:54:30 2017\r\n\tctime\tWed Sep 27 12:54:30 2017\r\n\tcrtime\tWed Sep 27 12:54:30 2017\r\n\tgen\t5894890\r\n\tmode\t120777\r\n\tsize\t12\r\n\tparent\t19383\r\n\tlinks\t1\r\n\tpflags\t40800000104\r\n```\r\n\r\n*Wild guess:* could there be some orphan code from Solaris that still reads and writes a symlink's own permissions?", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6687/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dcb314": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6683", "title": "zfs-0.7.2/module/icp/io/skein_mod.c:339]: (style) Array index 'vec_idx' is used before limits check.", "body": "Source code is\r\n\r\n    for (vec_idx = 0; offset >= uio->uio_iov[vec_idx].iov_len &&\r\n        vec_idx < uio->uio_iovcnt;\r\n        offset -= uio->uio_iov[vec_idx++].iov_len)\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6682", "title": "zfs-0.7.2/module/icp/io/sha2_mod.c: 2 * bad array index ?", "body": "zfs-0.7.2/module/icp/io/sha2_mod.c:1265]: (style) Array index 'vec_idx' is used before limits check.\r\nzfs-0.7.2/module/icp/io/sha2_mod.c:366]: (style) Array index 'vec_idx' is used before limits check.\r\n\r\nSource code for the first one is\r\n\r\n            offset >= mac->cd_uio->uio_iov[vec_idx].iov_len &&\r\n            vec_idx < mac->cd_uio->uio_iovcnt;\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6682/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6681", "title": "zfs-0.7.2/module/icp/io/sha1_mod.c: 2 * bad array index ?", "body": "\r\nzfs-0.7.2/module/icp/io/sha1_mod.c:1109]: (style) Array index 'vec_idx' is used before limits check.\r\nzfs-0.7.2/module/icp/io/sha1_mod.c:342]: (style) Array index 'vec_idx' is used before limits check.\r\n\r\nSource code for the first one is\r\n\r\n           offset >= mac->cd_uio->uio_iov[vec_idx].iov_len &&\r\n            vec_idx < mac->cd_uio->uio_iovcnt;\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mlvensel": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6676", "title": "Feature: multi-mount, multi-write, multi-host zfs ", "body": "I would like to see the ability to mount a zfs volume on both an active/standby configuration and a multi-active configuration.\r\n\r\nSome of the ways i thought about this would be to set a flag in the header and assign physical block areas (defined in shared space) with an intent log for each host it is mounted on. This intent log is processed by the master node who is elected and commits the changes to the unified intent log and commits changes. This should all be done on disk as to not be limited by network connectivity between hosts. The hosts should also have a heartbeat area where they update a timestamp field every second so the cluster is aware of both the membership and to keep \"disk time\" in sync with the master writer. ", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6676/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "frizop": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6671", "title": "PANIC: blkptr at ffff880858559848 DVA 0 has invalid VDEV 1", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  CentOS\r\nDistribution Version    |  7\r\nLinux Kernel                 | 3.10.0-514.26.2.el7.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.1-1\r\nSPL Version                  | spl-0.7.1-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nAfter adding in a mirror to my root pool, my system failed to boot\r\n\r\n### Describe how to reproduce the problem\r\n\r\nStarting with the following design:\r\n\r\n```config:\r\n\r\n        NAME                                        STATE     READ WRITE CKSUM\r\n        rpool                                       ONLINE       0     0     0\r\n          mirror-0                                  ONLINE       0     0     0\r\n            ata-ST8000VN0022-2EL112_ZA17YWA4-part2  ONLINE       0     0     0\r\n            ata-ST8000VN0022-2EL112_ZA17LLRT-part2  ONLINE       0     0     0\r\n```\r\n\r\nThen add a new mirror'd pool, mine looks like the following:\r\n\r\n```\r\nconfig:\r\n\r\n        NAME                                        STATE     READ WRITE CKSUM\r\n        rpool                                       ONLINE       0     0     0\r\n          mirror-0                                  ONLINE       0     0     0\r\n            ata-ST8000VN0022-2EL112_ZA17YWA4-part2  ONLINE       0     0     0\r\n            ata-ST8000VN0022-2EL112_ZA17LLRT-part2  ONLINE       0     0     0\r\n          mirror-1                                  ONLINE       0     0     0\r\n            ata-ST8000VN0022-2EL112_ZA17SKCL-part2  ONLINE       0     0     0\r\n            ata-ST8000VN0022-2EL112_ZA17Z73E-part2  ONLINE       0     0     0\r\n```\r\n\r\nFrom there, I believe the only thing done was to just reboot. \r\n\r\nI'll explain the history of my design; I started with a single disk, expanded it to a mirror, then added in another mirror. I had rebooted and everything seemed fine after the first mirror expansion, that was a few weeks ago. \r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n![image](https://user-images.githubusercontent.com/3605906/30768489-93ec05d8-9fcd-11e7-9b66-5a288d319ffa.png)\r\n![image](https://user-images.githubusercontent.com/3605906/30768491-9aa02bd4-9fcd-11e7-8cd9-11901d5c382e.png)\r\n\r\nBooting the http://list.zfsonlinux.org/zfs-iso/ resulted a clean import, no issues\r\n![image](https://user-images.githubusercontent.com/3605906/30768498-b79e1dc2-9fcd-11e7-9052-a0bc427d4cb2.png)\r\n\r\nThe issue only seemed to be present after grub loaded the kernel and it failed to fully initialize. \r\n\r\nPMT on #freenode advised me to take a copy of the initramfs I was using, and then the new one (that eventually worked). So I can provide those as well.\r\n\r\nAs far as what was mentioned, I expect the problem was that the zpool cache was never updated and adding in another mirror-1 to rpool caused it to not know it needed those disks to completely build my `rpool`. \r\n\r\nAs far as what I did to solve my issue, I booted into the system rescue iso (from above) and followed these instructions:\r\n\r\n```\r\nzpool import rpool\r\nln -s /dev/disk/by-id/* . -i\r\nfor dir in proc sys dev;do mount --bind /$dir /rpool/ROOT/$dir;done\r\nchroot /rpool/ROOT /bin/bash\r\ngrub2-mkconfig -o /boot/grub2/grub.cfg\r\nmv /etc/zfs/zpool.cache /\r\ndracut -f -v /boot/initramfs-3.10.0-514.26.2.el7.x86_64.img 3.10.0-514.26.2.el7.x86_64\r\ngrub-install --boot-directory=/boot /dev/sda\r\n```\r\n\r\nPreviously I had tried to just do a \"dracut\" without either the mkconfig/install and it resulted in some timeouts when trying to boot (not zfs related) so I followed the complete instructions and that seemed to do the trick.\r\n\r\nI also kept a copy of the /etc/zfs/zpool.cache \r\n\r\nWhat should the expected behavior be when using a root pool, moving from a mirror to a two mirror setup be?\r\n\r\nEdit: Pulling the zpool.cache out of my initramfs, we found an issue -\r\n```\r\nrpool:\r\n    version: 5000\r\n    name: 'rpool'\r\n    state: 0\r\n    txg: 5780\r\n    pool_guid: 4010814619963324040\r\n    errata: 0\r\n    hostname: 'removed.local'\r\n    com.delphix:has_per_vdev_zaps\r\n    vdev_children: 1\r\n    vdev_tree:\r\n        type: 'root'\r\n        id: 0\r\n        guid: 4010814619963324040\r\n        children[0]:\r\n            type: 'disk'\r\n            id: 0\r\n            guid: 1552324377184475286\r\n            path: '/dev/disk/by-id/ata-ST8000VN0022-2EL112_ZA17YWA4-part2'\r\n            whole_disk: 0\r\n            metaslab_array: 256\r\n            metaslab_shift: 27\r\n            ashift: 12\r\n            asize: 8001556316160\r\n            is_log: 0\r\n            create_txg: 4\r\n            com.delphix:vdev_zap_leaf: 129\r\n            com.delphix:vdev_zap_top: 130\r\n    features_for_read:\r\n```\r\n\r\nSo this basically points to a single disk, not the other 3 disks I have available in that pool.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6671/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Celmor": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6650", "title": "internal error: out of memory (on 'zfs set')", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Manjaro\r\nDistribution Version    | Gellivara 17.0.2\r\nLinux Kernel                 | 4.12.11-1-MANJARO\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.0-1\r\nSPL Version                  | 0.7.0-1\r\n\r\n### Describe the problem you're observing\r\nAny `zfs set` command on one pool fails and outputs 'internal error: out of memory'\r\n`free -m` reports over 4G free memory\r\n`zfs set` commands on another pool seemed to succeed although I only attempted such after exporting the pool I originally attempted these commands on\r\n### Describe how to reproduce the problem\r\nissue only appeared once so far\r\n### solution\r\nexport and import pool\r\n### shell log\r\n(dmesg doesn't contain anything unusual, didn't get an `strace` before I tried exporting the pool)\r\n```\r\n$ sudo \\zfs list -r Data/Backup | grep -i server\r\nData/Backup/VMWinServer2016DesktopDE  61.2G  33.1G  61.2G  none\r\n$ sudo \\zfs set mountpoint=/srv/nfs/share Data/Backup/VMWinServer2016DesktopDE\r\ninternal error: out of memory\r\n\r\n$ sudo \\zfs set mountpoint=/srv/nfs/share Data/Backup/VMWinServer2016DesktopDE\r\ninternal error: out of memory\r\n$ free -m\r\n              total        used        free      shared  buff/cache   available\r\nMem:          15979        9673        4710         299        1595        5748\r\nSwap:             0           0           0\r\n$ sudo su\r\n# zfs set mountpoint=/srv/nfs/share Data/Backup/VMWinServer2016DesktopDE\r\ninternal error: out of memory\r\n# zfs list  Data/Backup/VMWinServer2016DesktopDE\r\nNAME                                   USED  AVAIL  REFER  MOUNTPOINT\r\nData/Backup/VMWinServer2016DesktopDE  61.2G  33.1G  61.2G  none\r\n# zfs get mountpoint Data/Backup/VMWinServer2016DesktopDE\r\nNAME                                  PROPERTY    VALUE       SOURCE\r\nData/Backup/VMWinServer2016DesktopDE  mountpoint  none        inherited from Data/Backup\r\n# zfs set canmount=on Data/Backup/VMWinServer2016DesktopDE\r\ninternal error: out of memory\r\n# zfs set canmount=on Data\r\ninternal error: out of memory\r\n# free -m\r\n              total        used        free      shared  buff/cache   available\r\nMem:          15979        9676        4699         306        1603        5739\r\nSwap:             0           0           0\r\n# exit\r\n$ df /tmp\r\nFilesystem      Size  Used Avail Use% Mounted on\r\ntmpfs           7.9G  5.3M  7.8G   1% /tmp\r\n$ sudo sh -c 'sync && echo 3 > /proc/sys/vm/drop_caches' && free -m              total        used        free      shared  buff/cache   available\r\nMem:          15979        8636        6832         318         509        6770\r\nSwap:             0           0           0\r\n$ sudo \\zfs set mountpoint=/srv/nfs/share Data/Backup/VMWinServer2016DesktopDE\r\ninternal error: out of memory\r\n$ sudo \\zpool export Data\r\n$ sudo \\zfs list -t filesystem -r VM\r\nNAME   USED  AVAIL  REFER  MOUNTPOINT\r\nVM     333G   128G    96K  none\r\n$ sudo \\zfs set canmount=on VM\r\n$ sudo \\zfs set canmount=off VM\r\n$ sudo \\zpool import Data\r\ncannot share 'Data/share': smb add share failed\r\n$ sudo \\zfs set mountpoint=/srv/nfs/share Data/Backup/VMWinServer2016DesktopDE\r\n```\r\n### Other info:\r\n(the 'degraded] comes from me offlining one of the disks of that mirrored pool)\r\n```\r\n$ sudo \\zpool get all Data\r\nNAME  PROPERTY                       VALUE                          SOURCE\r\nData  size                           3.62T                          -\r\nData  capacity                       95%                            -\r\nData  altroot                        -                              default\r\nData  health                         DEGRADED                       -\r\nData  guid                           2751875717627853497            -\r\nData  version                        -                              default\r\nData  bootfs                         -                              default\r\nData  delegation                     on                             default\r\nData  autoreplace                    off                            default\r\nData  cachefile                      -                              default\r\nData  failmode                       wait                           default\r\nData  listsnapshots                  off                            default\r\nData  autoexpand                     off                            default\r\nData  dedupditto                     0                              default\r\nData  dedupratio                     1.00x                          -\r\nData  free                           149G                           -\r\nData  allocated                      3.48T                          -\r\nData  readonly                       off                            -\r\nData  ashift                         12                             local\r\nData  comment                        -                              default\r\nData  expandsize                     -                              -\r\nData  freeing                        0                              -\r\nData  fragmentation                  30%                            -\r\nData  leaked                         0                              -\r\nData  multihost                      off                            default\r\nData  feature@async_destroy          enabled                        local\r\nData  feature@empty_bpobj            active                         local\r\nData  feature@lz4_compress           active                         local\r\nData  feature@multi_vdev_crash_dump  disabled                       local\r\nData  feature@spacemap_histogram     active                         local\r\nData  feature@enabled_txg            active                         local\r\nData  feature@hole_birth             active                         local\r\nData  feature@extensible_dataset     enabled                        local\r\nData  feature@embedded_data          active                         local\r\nData  feature@bookmarks              enabled                        local\r\nData  feature@filesystem_limits      enabled                        local\r\nData  feature@large_blocks           enabled                        local\r\nData  feature@large_dnode            disabled                       local\r\nData  feature@sha512                 disabled                       local\r\nData  feature@skein                  disabled                       local\r\nData  feature@edonr                  disabled                       local\r\nData  feature@userobj_accounting     disabled                       local\r\n$ sudo \\zfs get all Data\r\nNAME  PROPERTY              VALUE                  SOURCE\r\nData  type                  filesystem             -\r\nData  creation              Fri May 20 21:35 2016  -\r\nData  used                  3.48T                  -\r\nData  available             33.1G                  -\r\nData  referenced            1.25T                  -\r\nData  compressratio         1.04x                  -\r\nData  mounted               yes                    -\r\nData  quota                 none                   default\r\nData  reservation           none                   default\r\nData  recordsize            128K                   default\r\nData  mountpoint            /Data                  local\r\nData  sharenfs              on                     local\r\nData  checksum              on                     default\r\nData  compression           off                    default\r\nData  atime                 on                     default\r\nData  devices               on                     default\r\nData  exec                  on                     default\r\nData  setuid                on                     default\r\nData  readonly              off                    default\r\nData  zoned                 off                    default\r\nData  snapdir               hidden                 default\r\nData  aclinherit            restricted             default\r\nData  createtxg             1                      -\r\nData  canmount              on                     default\r\nData  xattr                 on                     default\r\nData  copies                1                      default\r\nData  version               5                      -\r\nData  utf8only              off                    -\r\nData  normalization         none                   -\r\nData  casesensitivity       sensitive              -\r\nData  vscan                 off                    default\r\nData  nbmand                off                    local\r\nData  sharesmb              off                    local\r\nData  refquota              none                   default\r\nData  refreservation        none                   default\r\nData  guid                  13675242256420761066   -\r\nData  primarycache          all                    default\r\nData  secondarycache        all                    default\r\nData  usedbysnapshots       0B                     -\r\nData  usedbydataset         1.25T                  -\r\nData  usedbychildren        2.23T                  -\r\nData  usedbyrefreservation  0B                     -\r\nData  logbias               latency                default\r\nData  dedup                 off                    default\r\nData  mlslabel              none                   default\r\nData  sync                  standard               default\r\nData  dnodesize             legacy                 default\r\nData  refcompressratio      1.00x                  -\r\nData  written               1.25T                  -\r\nData  logicalused           3.61T                  -\r\nData  logicalreferenced     1.25T                  -\r\nData  volmode               default                default\r\nData  filesystem_limit      none                   default\r\nData  snapshot_limit        none                   default\r\nData  filesystem_count      none                   default\r\nData  snapshot_count        none                   default\r\nData  snapdev               hidden                 default\r\nData  acltype               off                    default\r\nData  context               none                   default\r\nData  fscontext             none                   default\r\nData  defcontext            none                   default\r\nData  rootcontext           none                   default\r\nData  relatime              off                    default\r\nData  redundant_metadata    all                    default\r\nData  overlay               off                    default\r\n$ uname -r\r\nLinux Celmor-PC 4.12.11-1-MANJARO #1 SMP PREEMPT Thu Sep 7 10:57:16 UTC 2017 x86_64 GNU/Linux\r\n$ pacman -Qs zfs | grep local\r\nlocal/linux412-zfs 0.7.0-8 (linux412-extramodules)\r\nlocal/spl-utils 0.7.0-1 (manjarozfs)\r\nlocal/zfs-utils 0.7.0-1 (manjarozfs)\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6650/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "octothorpe42": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6643", "title": "Cannot split whole-disk mirror", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS and Ubuntu\r\nDistribution Version    | CentOS 7, Ubuntu 16.04 LTS\r\nLinux Kernel                 | CentOS: 3.10.0-514.26.2.el7.x86_64, Ubuntu: 4.4.0-93-generic\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.9-1 (CentOS), 0.6.5.6-0ubuntu16\r\nSPL Version                  | 0.6.5.9-1 (CentOS), 0.6.5.6-0ubuntu4\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nAttempts to split a mirror made from whole disks consistently fail.\r\n\r\n### Describe how to reproduce the problem\r\nExample is from CentOS, but Ubuntu behaves identically.\r\nThe problem appears to have been around since at least 2013; see this thread:\r\nhttp://list.zfsonlinux.org/pipermail/zfs-discuss/2013-October/011424.html\r\n```\r\n[root@castle ~]# zpool create -f test1 mirror /dev/disk/by-id/ata-WDC_WD5002AALX-00J37A0_WD-WMAYUJ692875 /dev/disk/by-id/ata-WDC_WD5002AALX-00J37A0_WD-WMAYUL204701 \r\n[root@castle ~]# zpool split test1 test2 /dev/disk/by-id/ata-WDC_WD5002AALX-00J37A0_WD-WMAYUL204701\r\nthe kernel failed to rescan the partition table: 16\r\ncannot label 'sdk': try using parted(8) and then provide a specific slice: -1\r\n```\r\nIt is possible to split a mirror created from partitions, but a spurious \"Device or resource busy\" message is shown.\r\n```\r\n[root@castle ~]# zpool destroy test1\r\n[root@castle ~]# zpool create -f test1 mirror /dev/disk/by-id/ata-WDC_WD5002AALX-00J37A0_WD-WMAYUJ692875-part1 /dev/disk/by-id/ata-WDC_WD5002AALX-00J37A0_WD-WMAYUL204701-part1\r\n[root@castle ~]# zpool split test1 test2 /dev/disk/by-id/ata-WDC_WD5002AALX-00J37A0_WD-WMAYUL204701-part1\r\ncannot open '/dev/disk/by-id/ata-WDC_WD5002AALX-00J37A0_WD-WMAYUL204701-part1': Device or resource busy\r\n[root@castle ~]# zpool status test1\r\n  pool: test1\r\n state: ONLINE\r\n  scan: none requested\r\nconfig:\r\n\r\n\tNAME                                                STATE     READ WRITE CKSUM\r\n\ttest1                                               ONLINE       0     0     0\r\n\t  ata-WDC_WD5002AALX-00J37A0_WD-WMAYUJ692875-part1  ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n[root@castle ~]# zpool import test2\r\n[root@castle ~]# zpool status test2\r\n  pool: test2\r\n state: ONLINE\r\n  scan: none requested\r\nconfig:\r\n\r\n\tNAME                                                STATE     READ WRITE CKSUM\r\n\ttest2                                               ONLINE       0     0     0\r\n\t  ata-WDC_WD5002AALX-00J37A0_WD-WMAYUL204701-part1  ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n[root@castle ~]# \r\n```\r\n### Include any warning/errors/backtraces from the system logs\r\nNo warnings in logs.\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6643/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "acastaner": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6638", "title": "Unable to install from EL7_3 - 404 not found", "body": "I'm running CentOS 7.3, fresh install and fully patch. I installed the repo from this link http://download.zfsonlinux.org/epel/zfs-release.el7_3.noarch.rpm\r\n\r\nNow when I do a `yum install kernel-devel zfs` I get this: \r\n\r\n```\r\nDownloading packages:\r\nDelta RPMs disabled because /usr/bin/applydeltarpm not installed.\r\nlibnvpair1-0.7.1-1.el7_4.x86_6 FAILED\r\nhttp://download.zfsonlinux.org/epel/7.3/x86_64/libnvpair1-0.7.1-1.el7_4.x86_64.rpm: [Errno 14] HTTP Error 404 - Not Found                                                    ]  0.0 B/s |    0 B  --:--:-- ETA\r\nTrying other mirror.\r\nTo address this issue please refer to the below knowledge base article\r\n\r\nhttps://access.redhat.com/articles/1320623\r\n\r\nIf above article doesn't help to resolve this issue please create a bug on https://bugs.centos.org/\r\n\r\nlibuutil1-0.7.1-1.el7_4.x86_64 FAILED\r\nhttp://download.zfsonlinux.org/epel/7.3/x86_64/libuutil1-0.7.1-1.el7_4.x86_64.rpm: [Errno 14] HTTP Error 404 - Not Found                                                     ]  0.0 B/s |    0 B  --:--:-- ETA\r\nTrying other mirror.\r\nlibzfs2-0.7.1-1.el7_4.x86_64.r FAILED\r\nhttp://download.zfsonlinux.org/epel/7.3/x86_64/libzfs2-0.7.1-1.el7_4.x86_64.rpm: [Errno 14] HTTP Error 404 - Not Found                                                       ]  0.0 B/s |    0 B  --:--:-- ETA\r\nTrying other mirror.\r\nlibzpool2-0.7.1-1.el7_4.x86_64 FAILED\r\nhttp://download.zfsonlinux.org/epel/7.3/x86_64/libzpool2-0.7.1-1.el7_4.x86_64.rpm: [Errno 14] HTTP Error 404 - Not Found                                                     ]  0.0 B/s |    0 B  --:--:-- ETA\r\nTrying other mirror.\r\nspl-0.7.1-1.el7_4.x86_64.rpm   FAILED\r\nhttp://download.zfsonlinux.org/epel/7.3/x86_64/spl-0.7.1-1.el7_4.x86_64.rpm: [Errno 14] HTTP Error 404 - Not Found                                                           ]  0.0 B/s |    0 B  --:--:-- ETA\r\nTrying other mirror.\r\nspl-dkms-0.7.1-1.el7_4.noarch. FAILED\r\nhttp://download.zfsonlinux.org/epel/7.3/x86_64/spl-dkms-0.7.1-1.el7_4.noarch.rpm: [Errno 14] HTTP Error 404 - Not Found                                                      ]  0.0 B/s |    0 B  --:--:-- ETA\r\nTrying other mirror.\r\nzfs-0.7.1-1.el7_4.x86_64.rpm   FAILED\r\nhttp://download.zfsonlinux.org/epel/7.3/x86_64/zfs-0.7.1-1.el7_4.x86_64.rpm: [Errno 14] HTTP Error 404 - Not Found                                                           ]  0.0 B/s |    0 B  --:--:-- ETA\r\nTrying other mirror.\r\nzfs-dkms-0.7.1-1.el7_4.noarch. FAILED\r\nhttp://download.zfsonlinux.org/epel/7.3/x86_64/zfs-dkms-0.7.1-1.el7_4.noarch.rpm: [Errno 14] HTTP Error 404 - Not Found                                                      ]  0.0 B/s |    0 B  --:--:-- ETA\r\nTrying other mirror.\r\n\r\n\r\nError downloading packages:\r\n  zfs-0.7.1-1.el7_4.x86_64: [Errno 256] No more mirrors to try.\r\n  libuutil1-0.7.1-1.el7_4.x86_64: [Errno 256] No more mirrors to try.\r\n  libnvpair1-0.7.1-1.el7_4.x86_64: [Errno 256] No more mirrors to try.\r\n  libzpool2-0.7.1-1.el7_4.x86_64: [Errno 256] No more mirrors to try.\r\n  libzfs2-0.7.1-1.el7_4.x86_64: [Errno 256] No more mirrors to try.\r\n  spl-0.7.1-1.el7_4.x86_64: [Errno 256] No more mirrors to try.\r\n  spl-dkms-0.7.1-1.el7_4.noarch: [Errno 256] No more mirrors to try.\r\n  zfs-dkms-0.7.1-1.el7_4.noarch: [Errno 256] No more mirrors to try.\r\n```\r\n\r\nUsing EL7_4 I don't get 404's anymore, but I reckon this repo is for RHEL 7.4 not CentOS 7.3?", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6638/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "partoneoftwo": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6630", "title": "ARC_PRUNE Storm - Server becomes unusable - 1 SSD drive, 60 zfs filesystems mounted", "body": "\r\nType | Version/Name\r\n-- | --\r\nDistribution Name | Debian\r\nDistribution Version | 9.1\r\nLinux Kernel | 4.10.17-1-pve (as shipped with Proxmox v5.0)\r\nArchitecture | x64\r\nZFS Version | 0.6.5.9-1 (as shipped with Proxmox v5.0)\r\nSPL Version | 0.6.5.9-1 (as shipped with Proxmox v5.0)\r\n\r\n### Describe the problem you're observing\r\nWe're experiencing an extremely high CPU load (400-450 continous, for hours) and soft lockups multiple times a day involving arc_prune and spl_system_task:\r\n\r\nAs a result of this bug, the machine becomes practically inoperable.\r\nSSH logins are still possible. The system is not able to stat most things in /proc/\r\nSo top works, htop does not.\r\nPs ax works, ps aux does not.\r\n\r\n\r\n### Describe how to reproduce the problem\r\nThe box has 92G ECC RAM, Xeon E5620 x 2 (16 cores) and, possibly crucially, 60 mounted zfs filesystems (base filesystems + snapshots).\r\nThe ZFS filesystem is installed on an SSD drive. The machine has over 50% free memory and CPU utilization is 2%-10%.\r\n\r\nNumber of LXC containers using ZFS, running at the same time: 59\r\nNumber of KVM machines using ZFS, booted up: 1\r\n\r\nLXC containers using ZFS filesystems are not causing the issue.\r\nThe ARC_PRUNE storm, as I've learnt this phomenon is called occurs when booting up the KVM virtual machines and performing OS installs, or copying 10GB+ files onto the VM filesystem.\r\n\r\nThe server then becomes bogged down by ARC_PRUNE processes that never seem to end. Load averages goes from 90, 85, 15 to 350, 345, 320 to 450, 450, 450.\r\n\r\n#### Output of lshw -class disk\r\n```\r\n  *-disk\r\n       description: ATA Disk\r\n       product: Samsung SSD 850\r\n       physical id: 0.0.0\r\n       bus info: scsi@3:0.0.0\r\n       logical name: /dev/sdf\r\n       version: 2B6Q\r\n       serial: S2BBNWAG112461J\r\n       size: 953GiB (1024GB)\r\n       capabilities: gpt-1.00 partitioned partitioned:gpt\r\n       configuration: ansiversion=5 guid=ffd6d002-c0cb-eb41-bf7d-09896421be99 logicalsectorsize=512 sectorsize=512\r\n```\r\n\r\n#### Output of iostat\r\n\r\n```\r\nroot@thebank:~# iostat\r\nLinux 4.10.17-1-pve (thebank)   09/11/2017      _x86_64_        (16 CPU)\r\n\r\navg-cpu:  %user   %nice %system %iowait  %steal   %idle\r\n           2.56    0.02    4.15    4.58    0.00   88.69\r\n\r\nDevice:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn\r\nsdf             132.53      3700.79      1116.19 1021636972  308135390\r\n```\r\n\r\n\r\n#### Output of iostat -x\r\n```\r\nroot@thebank:~# iostat -x\r\nLinux 4.10.17-1-pve (thebank)   09/11/2017      _x86_64_        (16 CPU)\r\n\r\navg-cpu:  %user   %nice %system %iowait  %steal   %idle\r\n           2.56    0.02    4.15    4.58    0.00   88.69\r\n\r\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\r\nsdf               3.31     0.79   96.67   35.89  3701.82  1116.50    72.70     0.62    4.65    1.98   11.85   2.40  31.75\r\n```\r\n#### Output of ps ax\r\n\r\nroot@thebank:~# ps ax | grep arc_prune\r\n```\r\n  462 ?        S      0:00 [arc_prune]\r\n  463 ?        S      0:00 [arc_prune]\r\n  464 ?        S      0:00 [arc_prune]\r\n  465 ?        S      0:00 [arc_prune]\r\n  466 ?        S      0:00 [arc_prune]\r\n  467 ?        S      0:00 [arc_prune]\r\n  468 ?        S      0:00 [arc_prune]\r\n  469 ?        S      0:00 [arc_prune]\r\n  470 ?        S      0:00 [arc_prune]\r\n  471 ?        S      0:00 [arc_prune]\r\n  472 ?        S      0:00 [arc_prune]\r\n  473 ?        S      0:00 [arc_prune]\r\n  474 ?        S      0:00 [arc_prune]\r\n  475 ?        S      0:00 [arc_prune]\r\n  476 ?        S      0:00 [arc_prune]\r\n  477 ?        S      0:00 [arc_prune]\r\n  478 ?        S      0:00 [arc_prune]\r\n  479 ?        S      0:00 [arc_prune]\r\n  480 ?        S      0:00 [arc_prune]\r\n  481 ?        S      0:00 [arc_prune]\r\n  482 ?        S      0:00 [arc_prune]\r\n  483 ?        S      0:00 [arc_prune]\r\n  484 ?        S      0:00 [arc_prune]\r\n  485 ?        S      0:00 [arc_prune]\r\n  486 ?        S      0:00 [arc_prune]\r\n  487 ?        S      0:00 [arc_prune]\r\n  488 ?        S      0:00 [arc_prune]\r\n  489 ?        S      0:00 [arc_prune]\r\n  490 ?        S      0:00 [arc_prune]\r\n  491 ?        S      0:00 [arc_prune]\r\n  492 ?        S      0:00 [arc_prune]\r\n  493 ?        S      0:00 [arc_prune]\r\n\r\n  427 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  428 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  429 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  430 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  431 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  432 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  433 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  434 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  435 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  436 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  437 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  438 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  439 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  440 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  441 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  442 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  443 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  444 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  445 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  446 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  447 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  448 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_system_task\r\n  449 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 spl_dynamic_tas\r\n  462 root      20   0       0      0      0 S   0.0  0.0   0:00.00 arc_prune\r\n  463 root      20   0       0      0      0 S   0.0  0.0   0:00.00 arc_prune\r\n  464 root      20   0       0      0      0 S   0.0  0.0   0:00.00 arc_prune\r\n  465 root      20   0       0      0      0 S   0.0  0.0   0:00.00 arc_prune\r\n  466 root      20   0       0      0      0 S   0.0  0.0   0:00.00 arc_prune\r\n  467 root      20   0       0      0      0 S   0.0  0.0   0:00.00 arc_prune\r\n  468 root      20   0       0      0      0 S   0.0  0.0   0:00.00 arc_prune\r\n  469 root      20   0       0      0      0 S   0.0  0.0   0:00.00 arc_prune\r\n  470 root      20   0       0      0      0 S   0.0  0.0   0:00.00 arc_prune\r\n  471 root      20   0       0      0      0 S   0.0  0.0   0:00.00 arc_prune\r\n  472 root      20   0       0      0      0 S   0.0  0.0   0:00.00 arc_prune\r\n```\r\n\r\n#### output of arcstat\r\n```\r\nroot@thebank:~# arcstat\r\n    time  read  miss  miss%  dmis  dm%  pmis  pm%  mmis  mm%  arcsz     c\r\n20:48:28    17     8     47     8   47     0    0     0    0   4.6G  361M\r\n\r\n#### output of arc_summary\r\nroot@thebank:~# arc_summary\r\n\r\n------------------------------------------------------------------------\r\nZFS Subsystem Report                            Mon Sep 11 20:48:51 2017\r\nARC Summary: (HEALTHY)\r\n        Memory Throttle Count:                  0\r\n\r\nARC Misc:\r\n        Deleted:                                37.56m\r\n        Mutex Misses:                           1.06m\r\n        Evict Skips:                            1.06m\r\n\r\nARC Size:                               12.25%  4.58    GiB\r\n        Target Size: (Adaptive)         0.97%   369.22  MiB\r\n        Min Size (Hard Limit):          0.08%   32.00   MiB\r\n        Max Size (High Water):          1195:1  37.36   GiB\r\n\r\nARC Size Breakdown:\r\n        Recently Used Cache Size:       7.39%   346.14  MiB\r\n        Frequently Used Cache Size:     92.61%  4.24    GiB\r\n\r\nARC Hash Breakdown:\r\n        Elements Max:                           2.65m\r\n        Elements Current:               6.14%   162.56k\r\n        Collisions:                             2.47m\r\n        Chain Max:                              4\r\n        Chains:                                 680\r\n\r\nARC Total accesses:                                     6.39b\r\n        Cache Hit Ratio:                98.15%  6.27b\r\n        Cache Miss Ratio:               1.85%   118.43m\r\n        Actual Hit Ratio:               97.22%  6.21b\r\n\r\n        Data Demand Efficiency:         99.63%  6.16b\r\n        Data Prefetch Efficiency:       76.86%  64.59m\r\n\r\n        CACHE HITS BY CACHE LIST:\r\n          Anonymously Used:             0.74%   46.74m\r\n          Most Recently Used:           1.61%   101.23m\r\n          Most Frequently Used:         97.44%  6.11b\r\n          Most Recently Used Ghost:     0.05%   3.04m\r\n          Most Frequently Used Ghost:   0.16%   9.74m\r\n\r\n        CACHE HITS BY DATA TYPE:\r\n          Demand Data:                  97.86%  6.14b\r\n          Prefetch Data:                0.79%   49.64m\r\n          Demand Metadata:              1.17%   73.61m\r\n          Prefetch Metadata:            0.17%   10.78m\r\n\r\n        CACHE MISSES BY DATA TYPE:\r\n          Demand Data:                  19.03%  22.54m\r\n          Prefetch Data:                12.62%  14.95m\r\n          Demand Metadata:              64.63%  76.54m\r\n          Prefetch Metadata:            3.72%   4.40m\r\n\r\n\r\nFile-Level Prefetch: (HEALTHY)\r\nDMU Efficiency:                                 638.82m\r\n        Hit Ratio:                      84.37%  539.00m\r\n        Miss Ratio:                     15.63%  99.82m\r\n\r\n        Colinear:                               99.82m\r\n          Hit Ratio:                    0.07%   71.09k\r\n          Miss Ratio:                   99.93%  99.75m\r\n\r\n        Stride:                                 452.04m\r\n          Hit Ratio:                    98.49%  445.20m\r\n          Miss Ratio:                   1.51%   6.84m\r\n\r\nDMU Misc:\r\n        Reclaim:                                99.75m\r\n          Successes:                    3.07%   3.06m\r\n          Failures:                     96.93%  96.69m\r\n\r\n        Streams:                                93.83m\r\n          +Resets:                      0.03%   27.03k\r\n          -Resets:                      99.97%  93.80m\r\n          Bogus:                                0\r\n\r\n\r\nZFS Tunable:\r\n        l2arc_headroom                                    2\r\n        zfs_free_leak_on_eio                              0\r\n        zfs_free_max_blocks                               100000\r\n        zfs_read_chunk_size                               1048576\r\n        metaslab_preload_enabled                          1\r\n        zfs_dedup_prefetch                                0\r\n        zfs_txg_history                                   0\r\n        zfs_scrub_delay                                   4\r\n        zfs_vdev_async_read_max_active                    3\r\n        zfs_read_history                                  0\r\n        zfs_arc_sys_free                                  0\r\n        l2arc_write_max                                   8388608\r\n        zfs_dbuf_state_index                              0\r\n        metaslab_debug_unload                             0\r\n        zvol_inhibit_dev                                  0\r\n        zfetch_max_streams                                8\r\n        zfs_recover                                       0\r\n        metaslab_fragmentation_factor_enabled             1\r\n        zfs_sync_pass_rewrite                             2\r\n        zfs_object_mutex_size                             64\r\n        zfs_arc_min_prefetch_lifespan                     0\r\n        zfs_arc_meta_prune                                10000\r\n        zfs_read_history_hits                             0\r\n        l2arc_norw                                        0\r\n        zfs_dirty_data_max_percent                        10\r\n        zfs_arc_meta_min                                  0\r\n        metaslabs_per_vdev                                200\r\n        zfs_arc_meta_adjust_restarts                      4096\r\n        zil_slog_limit                                    1048576\r\n        spa_load_verify_maxinflight                       10000\r\n        spa_load_verify_metadata                          1\r\n        zfs_send_corrupt_data                             0\r\n        zfs_delay_min_dirty_percent                       60\r\n        zfs_vdev_sync_read_max_active                     10\r\n        zfs_dbgmsg_enable                                 0\r\n        zio_requeue_io_start_cut_in_line                  1\r\n        l2arc_headroom_boost                              200\r\n        zfs_zevent_cols                                   80\r\n        spa_config_path                                   /etc/zfs/zpool.cache\r\n        zfs_vdev_cache_size                               0\r\n        zfs_vdev_sync_write_min_active                    10\r\n        zfs_vdev_scrub_max_active                         2\r\n        zfs_disable_dup_eviction                          0\r\n        ignore_hole_birth                                 1\r\n        zvol_major                                        230\r\n        zil_replay_disable                                0\r\n        zfs_dirty_data_max_max_percent                    25\r\n        zfs_expire_snapshot                               300\r\n        zfs_sync_pass_deferred_free                       2\r\n        spa_asize_inflation                               24\r\n        zfs_vdev_mirror_switch_us                         10000\r\n        l2arc_feed_secs                                   1\r\n        zfs_autoimport_disable                            1\r\n        zfs_arc_p_aggressive_disable                      1\r\n        zfs_zevent_len_max                                512\r\n        l2arc_noprefetch                                  1\r\n        zfs_arc_meta_limit                                0\r\n        zfs_flags                                         0\r\n        zfs_dirty_data_max_max                            20057063424\r\n        zfs_arc_average_blocksize                         8192\r\n        zfs_vdev_cache_bshift                             16\r\n        zfs_vdev_async_read_min_active                    1\r\n        zfs_arc_num_sublists_per_state                    16\r\n        zfs_arc_grow_retry                                0\r\n        l2arc_feed_again                                  1\r\n        zfs_arc_lotsfree_percent                          10\r\n        zfs_zevent_console                                0\r\n        zvol_prefetch_bytes                               131072\r\n        zfs_free_min_time_ms                              1000\r\n        zio_taskq_batch_pct                               75\r\n        zfetch_block_cap                                  256\r\n        spa_load_verify_data                              1\r\n        zfs_dirty_data_max                                8022825369\r\n        zfs_vdev_async_write_max_active                   10\r\n        zfs_dbgmsg_maxsize                                4194304\r\n        zfs_nocacheflush                                  0\r\n        zfetch_array_rd_sz                                1048576\r\n        zfs_arc_meta_strategy                             1\r\n        zfs_dirty_data_sync                               67108864\r\n        zvol_max_discard_blocks                           16384\r\n        zfs_vdev_async_write_active_max_dirty_percent     60\r\n        zfs_arc_p_dampener_disable                        1\r\n        zfs_txg_timeout                                   5\r\n        metaslab_aliquot                                  524288\r\n        zfs_mdcomp_disable                                0\r\n        zfs_vdev_sync_read_min_active                     10\r\n        metaslab_debug_load                               0\r\n        zfs_vdev_aggregation_limit                        131072\r\n        l2arc_nocompress                                  0\r\n        metaslab_lba_weighting_enabled                    1\r\n        zfs_vdev_scheduler                                noop\r\n        zfs_vdev_scrub_min_active                         1\r\n        zfs_no_scrub_io                                   0\r\n        zfs_vdev_cache_max                                16384\r\n        zfs_scan_idle                                     50\r\n        zfs_arc_shrink_shift                              0\r\n        spa_slop_shift                                    5\r\n        zfs_deadman_synctime_ms                           1000000\r\n        metaslab_bias_enabled                             1\r\n        zfs_admin_snapshot                                0\r\n        zfs_no_scrub_prefetch                             0\r\n        zfs_metaslab_fragmentation_threshold              70\r\n        zfs_max_recordsize                                1048576\r\n        zfs_arc_min                                       0\r\n        zfs_nopwrite_enabled                              1\r\n        zfs_arc_p_min_shift                               0\r\n        zfs_mg_fragmentation_threshold                    85\r\n        l2arc_write_boost                                 8388608\r\n        zfs_prefetch_disable                              0\r\n        l2arc_feed_min_ms                                 200\r\n        zio_delay_max                                     30000\r\n        zfs_vdev_write_gap_limit                          4096\r\n        zfs_pd_bytes_max                                  52428800\r\n        zfs_scan_min_time_ms                              1000\r\n        zfs_resilver_min_time_ms                          3000\r\n        zfs_delay_scale                                   500000\r\n        zfs_vdev_async_write_active_min_dirty_percent     30\r\n        zfs_vdev_sync_write_max_active                    10\r\n        zfs_mg_noalloc_threshold                          0\r\n        zfs_deadman_enabled                               1\r\n        zfs_resilver_delay                                2\r\n        zfs_arc_max                                       0\r\n        zfs_top_maxinflight                               32\r\n        zfetch_min_sec_reap                               2\r\n        zfs_immediate_write_sz                            32768\r\n        zfs_vdev_async_write_min_active                   1\r\n        zfs_sync_pass_dont_compress                       5\r\n        zfs_vdev_read_gap_limit                           32768\r\n        zfs_vdev_max_active                               1000\r\n```\r\n\r\n### Suggestions I have found elsewhere with solutions\r\n\r\n\r\n### Why I am adding a ticket on this\r\nI am affected of this error in September 2017, and I am hoping to learn if I can solve the issue with the current installed versions of ZFS and SPF.\r\n\r\nThe rich discussion on a similar/identical ticket, provided a solution where setting a DNODE_MAX seems to have solved the issue. https://github.com/zfsonlinux/zfs/issues/6223 \r\nIt seems that the following solves it:\r\n```\r\nThere's another trap for young and old there: per arc_tuning_update(), the only time zfs_arc_dnode_limit_percent is used is if you actually change zfs_arc_max. (And whilst we're here, if you change zfs_arc_meta_min then arc_dnode_limit is automatically set back to 10% of arc_meta_limit, regardless of zfs_arc_dnode_limit or zfs_arc_dnode_limit_percent.)\r\nWith my zfs_arc_dnode_limit_percent set to 75 the dnode_size/limit looks much healthier:\r\n# grep dnode /proc/spl/kstat/zfs/arcstats\r\ndnode_size                      4    38120264000\r\narc_dnode_limit                 4    67645734912\r\nAnd I've yet to see another arc_prune storm. Yippee!\r\n\r\nFrom <https://github.com/zfsonlinux/zfs/issues/6223> \r\n```\r\nI don't have ZFS v0.7 or newer installed, so I can't tune the DNODE_SIZE setting.\r\nAre there any other things I can do?", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6630/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nsc-zino": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6629", "title": "Kernel Panic, likely snapshot related", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS\r\nDistribution Version    | 6.9\r\nLinux Kernel                 | 2.6.32-696.10.1.el6.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.1-1\r\nSPL Version                  | 0.7.1-1\r\n\r\n### Describe the problem you're observing\r\nCopying a file from a snapshot caused an immediate Kernel Panic. I've seen similar crashes several times before over the last year, but those where suspected to be caused by user snapshot access over NFS in zfs 0.6. Probably in combination zfs-auto-snapshot deleting or creating snapshots.\r\n\r\nThis time it happened when I had CWD in snapshots/ on the server and executed \"cp -a zfs-auto-snap_daily-2017-09-07-0132/user/dirname /poolname/home/user/dirname.youroldfiles\".\r\n\r\nThe moment Enter was pressed the console returned:\r\n```\r\nMessage from syslogd@d6 at Sep 11 11:49:53 ...\r\n kernel:Kernel panic - not syncing: Fatal exception\r\n```\r\ntogether with a backtrace listed below.\r\n\r\nThe time is interesting, because it's not a time when zfs-auto-snapshot is expected to create or delete snapshots.\r\n\r\n### Describe how to reproduce the problem\r\nI do not have a reliable reproducer at this time.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n```\r\nBUG: Dentry ffff88037726ce40{i=4,n=/} still in use (1) [unmount of zfs zfs]\r\n------------[ cut here ]------------\r\nkernel BUG at fs/dcache.c:672!\r\ninvalid opcode: 0000 [#1] SMP\r\nlast sysfs file: /sys/devices/system/cpu/online\r\nCPU 19\r\nModules linked in: nfsd exportfs autofs4 nfs lockd fscache auth_rpcgss nfs_acl sunrpc cpufreq_ondemand freq_table pcc_cpufreq ipt_REJECT nf_conntrack_ipv4 nf_defrag_ipv4 iptable_filter ip_tables ip6t_REJECT nf_conntrack_ipv6 nf_defrag_ipv6 xt_state nf_conntrack ip6table_filter ip6_tables ipv6 zfs(P)(U) zcommon(P)(U) znvpair(P)(U) icp(P)(U) spl(U) zlib_deflate zavl(P)(U) zunicode(P)(U) mlx4_en ipmi_devintf power_meter acpi_ipmi ipmi_si ipmi_msghandler microcode iTCO_wdt iTCO_vendor_support hpilo hpwdt igb dca i2c_algo_bit i2c_core ptp pps_core sg mlx4_core serio_raw lpc_ich mfd_core shpchp ext4 jbd2 mbcache sd_mod crc_t10dif hpsa(U) ahci dm_mirror dm_region_hash dm_log dm_mod [last unloaded: scsi_wait_scan]\r\n\r\nPid: 12045, comm: umount Tainted: P           -- ------------    2.6.32-696.10.1.el6.x86_64 #1 HP ProLiant SL4540 Gen8 /\r\nRIP: 0010:[<ffffffff811b50a8>]  [<ffffffff811b50a8>] shrink_dcache_for_umount_subtree+0x2a8/0x2b0\r\nRSP: 0018:ffff8809d95b7dc8  EFLAGS: 00010292\r\nRAX: 0000000000000052 RBX: ffff88037726ce40 RCX: 0000000000002675\r\nRDX: 0000000000000000 RSI: 0000000000000046 RDI: 0000000000000246\r\nRBP: ffff8809d95b7e08 R08: 000000000002e03a R09: 00000000fffffffe\r\nR10: 0000000000000000 R11: 0000000000000004 R12: 0000000000000000\r\nR13: ffffffff81a863c0 R14: ffff8803db766c00 R15: ffff88037726cea0\r\nFS:  00007f52ce1f4740(0000) GS:ffff8806954e0000(0000) knlGS:0000000000000000\r\nCS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b\r\nCR2: 00007f52cd8718d5 CR3: 0000000bd0b6c000 CR4: 00000000001407e0\r\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\nDR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400\r\nProcess umount (pid: 12045, threadinfo ffff8809d95b4000, task ffff880c5c248ab0)\r\nStack:\r\n ffff8803db766e70 0000000000000246 ffff8809d95b7e48 ffff8803db766c00\r\n<d> ffffffffa04c2440 ffffffff81c1aec0 ffff8803db766c00 ffff8805d4e949c0\r\n<d> ffff8809d95b7e28 ffffffff811b50e6 ffff8809d95b7e88 ffff8803db766c00\r\nCall Trace:\r\n [<ffffffff811b50e6>] shrink_dcache_for_umount+0x36/0x60\r\n [<ffffffff8119c22f>] generic_shutdown_super+0x1f/0xe0\r\n [<ffffffff8119c356>] kill_anon_super+0x16/0x60\r\n [<ffffffffa047ee5e>] zpl_kill_sb+0x1e/0x30 [zfs]\r\n [<ffffffff8119caf7>] deactivate_super+0x57/0x80\r\n [<ffffffff811bca4f>] mntput_no_expire+0xbf/0x110\r\n [<ffffffff811bd59b>] sys_umount+0x7b/0x3a0\r\n [<ffffffff8100b0d2>] system_call_fastpath+0x16/0x1b\r\nCode: 50 30 4c 8b 0a 31 d2 48 85 f6 74 04 48 8b 56 40 48 05 70 02 00 00 48 89 de 48 c7 c7 b0 74 7d 81 48 89 04 24 31 c0 e8 b8 50 39 00 <0f> 0b eb fe 0f 0b eb fe 55 48 89 e5 53 48 83 ec 08 0f 1f 44 00\r\nRIP  [<ffffffff811b50a8>] shrink_dcache_for_umount_subtree+0x2a8/0x2b0\r\n RSP <ffff8809d95b7dc8>\r\n---[ end trace d2fae48425fb4201 ]---\r\nKernel panic - not syncing: Fatal exception\r\nPid: 12045, comm: umount Tainted: P      D    -- ------------    2.6.32-696.10.1.el6.x86_64 #1\r\nCall Trace:\r\n [<ffffffff8154a08e>] ? panic+0xa7/0x179\r\n [<ffffffff8154ef24>] ? oops_end+0xe4/0x100\r\n [<ffffffff8101101b>] ? die+0x5b/0x90\r\n [<ffffffff8154e744>] ? do_trap+0xc4/0x160\r\n [<ffffffff8100cd85>] ? do_invalid_op+0x95/0xb0\r\n [<ffffffff811b50a8>] ? shrink_dcache_for_umount_subtree+0x2a8/0x2b0\r\n [<ffffffff81074d44>] ? enqueue_task_fair+0x64/0x100\r\n [<ffffffff8100c01b>] ? invalid_op+0x1b/0x20\r\n [<ffffffff811b50a8>] ? shrink_dcache_for_umount_subtree+0x2a8/0x2b0\r\n [<ffffffff811b50a8>] ? shrink_dcache_for_umount_subtree+0x2a8/0x2b0\r\n [<ffffffff811b50e6>] ? shrink_dcache_for_umount+0x36/0x60\r\n [<ffffffff8119c22f>] ? generic_shutdown_super+0x1f/0xe0\r\n [<ffffffff8119c356>] ? kill_anon_super+0x16/0x60\r\n [<ffffffffa047ee5e>] ? zpl_kill_sb+0x1e/0x30 [zfs]\r\n [<ffffffff8119caf7>] ? deactivate_super+0x57/0x80\r\n [<ffffffff811bca4f>] ? mntput_no_expire+0xbf/0x110\r\n [<ffffffff811bd59b>] ? sys_umount+0x7b/0x3a0\r\n [<ffffffff8100b0d2>] ? system_call_fastpath+0x16/0x1b\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "PeterFeicht": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6626", "title": "Slow writes to pool created by FreeNAS", "body": "I'm trying to write to a pool created by FreeNAS using ZFS on Linux ([details here](https://forums.freenas.org/index.php?threads/recover-thecus-raid5-on-freenas.50277/#post-399804)), which works fine.\r\n\r\nHowever, after copying some stuff over, about the same amount as I have RAM in that machine, writing to the ZFS volume gets really slow, like 30MB/s to a raidz2 with 8 drives. Write speeds are not that great to begin with, just 150 to 200MB/s. I don't know whether this is a configuration/usage issue or a proper bug, so any input would be appreciated.\r\n\r\nI noticed the `arc_reclaim` process hogging some CPU, which made me think this might be related to issue #6531 somehow. I don't have anything valuable on the pool yet, so I can do whatever is necessary for troubleshooting.\r\n\r\n## Hardware:\r\n\r\nWhat        | Model\r\n----------- | -----\r\nMotherboard | Intel S5520UR\r\nCPU         | Dual Xeon L5640\r\nRAM         | 96GB DDR3 ECC\r\nHBA         | LSI SAS9207-4i4e (cross-flashed from HP H222), firmware version P20\r\nDrives      | 8x WD80EFZX\r\n\r\n## Linux: Arch Linux\r\n\r\nZFS installed from [archzfs](https://wiki.archlinux.org/index.php/Unofficial_user_repositories#archzfs) repo.\r\n\r\nWhat            | Version\r\n--------------- | -------\r\nLinux Kernel    | 4.12.10-1\r\nZFS             | 0.7.1-1\r\nSPL             | 0.7.1-1\r\n\r\n`zpool status` output:\r\n\r\n```\r\n  pool: tank\r\n state: ONLINE\r\nstatus: Some supported features are not enabled on the pool. The pool can\r\n        still be used, but some features are unavailable.\r\naction: Enable all features using 'zpool upgrade'. Once this is done,\r\n        the pool may no longer be accessible by software that does not support\r\n        the features. See zpool-features(5) for details.\r\n  scan: scrub repaired 0B in 0h6m with 0 errors on Mon Aug 28 20:02:07 2017\r\nconfig:\r\n\r\n        NAME        STATE     READ WRITE CKSUM\r\n        tank        ONLINE       0     0     0\r\n          raidz2-0  ONLINE       0     0     0\r\n            sdb     ONLINE       0     0     0\r\n            sdc     ONLINE       0     0     0\r\n            sda     ONLINE       0     0     0\r\n            sdf     ONLINE       0     0     0\r\n            sde     ONLINE       0     0     0\r\n            sdd     ONLINE       0     0     0\r\n            sdg     ONLINE       0     0     0\r\n            sdh     ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n```\r\n\r\n`zdb` output:\r\n\r\n```\r\ntank:\r\n    version: 5000\r\n    name: 'tank'\r\n    state: 0\r\n    txg: 8054\r\n    pool_guid: 6775885842032388289\r\n    errata: 0\r\n    hostname: 'antares'\r\n    com.delphix:has_per_vdev_zaps\r\n    vdev_children: 1\r\n    vdev_tree:\r\n        type: 'root'\r\n        id: 0\r\n        guid: 6775885842032388289\r\n        children[0]:\r\n            type: 'raidz'\r\n            id: 0\r\n            guid: 6686971514524699808\r\n            nparity: 2\r\n            metaslab_array: 45\r\n            metaslab_shift: 39\r\n            ashift: 12\r\n            asize: 63995287437312\r\n            is_log: 0\r\n            create_txg: 4\r\n            com.delphix:vdev_zap_top: 36\r\n            children[0]:\r\n                type: 'disk'\r\n                id: 0\r\n                guid: 18032278711795630256\r\n                path: '/dev/sdb2'\r\n                phys_path: 'id1,enc@n500019b9ff4c5e3e/type@0/slot@1/elmdesc@SLOT_001'\r\n                whole_disk: 1\r\n                DTL: 64\r\n                create_txg: 4\r\n                com.delphix:vdev_zap_leaf: 37\r\n            children[1]:\r\n                type: 'disk'\r\n                id: 1\r\n                guid: 13530303699855493619\r\n                path: '/dev/sdc2'\r\n                phys_path: 'id1,enc@n500019b9ff4c5e3e/type@0/slot@3/elmdesc@SLOT_003'\r\n                whole_disk: 1\r\n                DTL: 63\r\n                create_txg: 4\r\n                com.delphix:vdev_zap_leaf: 38\r\n            children[2]:\r\n                type: 'disk'\r\n                id: 2\r\n                guid: 15037164158216564558\r\n                path: '/dev/sda2'\r\n                phys_path: 'id1,enc@n500019b9ff4c5e3e/type@0/slot@2/elmdesc@SLOT_002'\r\n                whole_disk: 1\r\n                DTL: 62\r\n                create_txg: 4\r\n                com.delphix:vdev_zap_leaf: 39\r\n            children[3]:\r\n                type: 'disk'\r\n                id: 3\r\n                guid: 5640001431923885195\r\n                path: '/dev/sdf2'\r\n                phys_path: 'id1,enc@n500019b9ff4c5e3e/type@0/slot@6/elmdesc@SLOT_006'\r\n                whole_disk: 1\r\n                DTL: 61\r\n                create_txg: 4\r\n                com.delphix:vdev_zap_leaf: 40\r\n            children[4]:\r\n                type: 'disk'\r\n                id: 4\r\n                guid: 18025663285868450324\r\n                path: '/dev/sde2'\r\n                phys_path: 'id1,enc@n500019b9ff4c5e3e/type@0/slot@5/elmdesc@SLOT_005'\r\n                whole_disk: 1\r\n                DTL: 60\r\n                create_txg: 4\r\n                com.delphix:vdev_zap_leaf: 41\r\n            children[5]:\r\n                type: 'disk'\r\n                id: 5\r\n                guid: 1150609568386899083\r\n                path: '/dev/sdd2'\r\n                phys_path: 'id1,enc@n500019b9ff4c5e3e/type@0/slot@4/elmdesc@SLOT_004'\r\n                whole_disk: 1\r\n                DTL: 59\r\n                create_txg: 4\r\n                com.delphix:vdev_zap_leaf: 42\r\n            children[6]:\r\n                type: 'disk'\r\n                id: 6\r\n                guid: 3578974144461137932\r\n                path: '/dev/sdg2'\r\n                phys_path: 'id1,enc@n500019b9ff4c5e3e/type@0/slot@7/elmdesc@SLOT_007'\r\n                whole_disk: 1\r\n                DTL: 58\r\n                create_txg: 4\r\n                com.delphix:vdev_zap_leaf: 43\r\n            children[7]:\r\n                type: 'disk'\r\n                id: 7\r\n                guid: 14532033329246330435\r\n                path: '/dev/sdh2'\r\n                phys_path: 'id1,enc@n500019b9ff4c5e3e/type@0/slot@8/elmdesc@SLOT_008'\r\n                whole_disk: 1\r\n                DTL: 57\r\n                create_txg: 4\r\n                com.delphix:vdev_zap_leaf: 44\r\n    features_for_read:\r\n        com.delphix:hole_birth\r\n        com.delphix:embedded_data\r\n```\r\n\r\n## BSD: FreeNAS 11.0-U2\r\n\r\nWhat            | Version\r\n--------------- | -------\r\nFreeBSD Kernel  | 11.0-STABLE\r\nZFS             | ??\r\n\r\n`zpool status` output:\r\n\r\n```\r\n  pool: tank\r\n state: ONLINE\r\n  scan: scrub repaired 0 in 0h6m with 0 errors on Mon Aug 28 20:02:07 2017\r\nconfig:\r\n\r\n        NAME                                            STATE     READ WRITE CKSUM\r\n        tank                                            ONLINE       0     0     0\r\n          raidz2-0                                      ONLINE       0     0     0\r\n            gptid/c072ac4d-8c10-11e7-bda6-001517bd58c0  ONLINE       0     0     0\r\n            gptid/c11dfad2-8c10-11e7-bda6-001517bd58c0  ONLINE       0     0     0\r\n            gptid/c1d5c249-8c10-11e7-bda6-001517bd58c0  ONLINE       0     0     0\r\n            gptid/c290fa3e-8c10-11e7-bda6-001517bd58c0  ONLINE       0     0     0\r\n            gptid/c33d7a88-8c10-11e7-bda6-001517bd58c0  ONLINE       0     0     0\r\n            gptid/c3dad10f-8c10-11e7-bda6-001517bd58c0  ONLINE       0     0     0\r\n            gptid/c4832b2f-8c10-11e7-bda6-001517bd58c0  ONLINE       0     0     0\r\n            gptid/c5226b94-8c10-11e7-bda6-001517bd58c0  ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n```\r\n\r\n`zdb` refuses to acknowledge that `tank` exists:\r\n\r\n```\r\nzdb: can't open 'tank': No such file or directory\r\n\r\nZFS_DBGMSG(zdb):\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6626/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stewartadam": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6624", "title": "Filesystem created via zfs send --raw between encrypted roots fails to mount", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Fedora\r\nDistribution Version    | 26\r\nLinux Kernel                 | 4.12.9-300.fc26.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.1-1 (with encryption patches)\r\nSPL Version                  | 0.7.1-1\r\n\r\nI've installed SPL from the upstream f26 repo and compiled 0.7.1 from source, patching in:\r\n* Add libtpool (thread pools) (46364cb)\r\n* Send / Recv Fixes following b52563 (9b84076)\r\n* Native Encryption for ZFS on Linux (b525630)\r\n\r\nTransferring a filesystem between two encryption roots with `zfs send --raw` succeeds, but cannot be mounted.\r\n```\r\nrm -f /root/src.img /root/replica.img\r\n\r\ntruncate -s 100M /root/src.img\r\ntruncate -s 100M /root/replica.img\r\n\r\nzpool create src /root/src.img\r\nzpool create replica /root/replica.img\r\nzfs create -o encryption=on -o keyformat=passphrase -o keylocation=prompt src/encrypted\r\nzfs create -o encryption=on -o keyformat=passphrase -o keylocation=prompt replica/encrypted\r\n\r\nzfs create src/encrypted/myfiles\r\ndd if=/dev/urandom of=/src/encrypted/myfiles/random.bin bs=1M count=30\r\nzfs snapshot -r src@migration\r\n\r\nzfs send -cpv --raw src/encrypted/myfiles@migration | zfs recv -s replica/encrypted/myfiles\r\n```\r\nResults in:\r\n```\r\nfull send of src/encrypted/myfiles@migration estimated size is 30.7M\r\ntotal estimated size is 30.7M\r\nTIME        SENT   SNAPSHOT\r\nfilesystem 'replica/encrypted/myfiles' can not be mounted: Permission denied\r\ncannot mount 'replica/encrypted/myfiles': Invalid argument\r\n```\r\n\r\nAttempting to import all keys fails to request keys for the transferred filesystem: \r\n```\r\nzpool import -d /root replica\r\nzfs load-key -a\r\nEnter passphrase for 'replica/encrypted': \r\n1 / 1 key(s) successfully loaded\r\n\r\nfilesystem 'replica/encrypted/myfiles' can not be mounted: Permission denied\r\ncannot mount 'replica/encrypted/myfiles': Invalid argument\r\n```\r\n\r\nIn this scenario I would expect the raw transfer to result in a new encryption root and then `zfs load-key` would to prompt for both `replica/encrypted` and `replica/encrypted/myfiles`, but that's clearly not the case:\r\n```\r\nzfs get -r encryptionroot replica/encrypted\r\nNAME                                 PROPERTY        VALUE              SOURCE\r\nreplica/encrypted                    encryptionroot  replica/encrypted  -\r\nreplica/encrypted/myfiles            encryptionroot  replica/encrypted  -\r\nreplica/encrypted/myfiles@migration  encryptionroot  replica/encrypted  -\r\n```\r\n\r\nWe should clarify (a) if nested encryption roots are supported and (b) validate raw transfer between encrypted filesystems.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6624/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6621", "title": "zfs recv hangs, locking up all pool I/O", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Fedora\r\nDistribution Version    | 26\r\nLinux Kernel                 | 4.12.9-300.fc26.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.1-1 (with encryption patches)\r\nSPL Version                  | 0.7.1-1\r\n\r\nI've installed SPL from the upstream f26 repo and compiled 0.7.1 from source, patching in:\r\n* Add libtpool (thread pools) (46364cb)\r\n* Send / Recv Fixes following b52563 (9b84076)\r\n* Native Encryption for ZFS on Linux (b525630)\r\n\r\n`zfs send -cpv --raw src@snap | zfs recv -s dst` between two local (encrypted) pools hung after 159GB of ~1.6TB. Ctrl+C to interrupt the process resulted in `zfs send` existing but `zfs recv` lingered.\r\n\r\nI have adjusted `zfs_arc_min` to 1GB to ensure that transfers were not stalled due to memory. The `zfs recv` process could not be killed with SIGKILL and after attempting to export the pool, all zpool/zfs commands hung.\r\n\r\nOutput of `cat /proc/$PID/stack`\r\n```\r\n[<ffffffffc07b4bc3>] cv_wait_common+0x113/0x130 [spl]\r\n[<ffffffffc07b4bf5>] __cv_wait+0x15/0x20 [spl]\r\n[<ffffffffc0c8e46d>] txg_wait_synced+0xdd/0x120 [zfs]\r\n[<ffffffffc0c36aa9>] dmu_recv_cleanup_ds+0x49/0xc0 [zfs]\r\n[<ffffffffc0c3b7c7>] dmu_recv_stream+0x5b7/0x1120 [zfs]\r\n[<ffffffffc0cc112d>] zfs_ioc_recv_impl+0x48d/0x10f0 [zfs]\r\n[<ffffffffc0cc234e>] zfs_ioc_recv_new+0x31e/0x3e0 [zfs]\r\n[<ffffffffc0cbe122>] zfsdev_ioctl+0x442/0x600 [zfs]\r\n[<ffffffffae279a95>] do_vfs_ioctl+0xa5/0x600\r\n[<ffffffffae27a069>] SyS_ioctl+0x79/0x90\r\n[<ffffffffae003ba7>] do_syscall_64+0x67/0x140\r\n[<ffffffffae877767>] entry_SYSCALL64_slow_path+0x25/0x25\r\n[<ffffffffffffffff>] 0xffffffffffffffff\r\n```\r\n\r\nIssues:\r\n1. Deadlock described above\r\n2. recv deadlock kills results in any zfs/zpool execution hanging", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6621/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6574", "title": "Receiving stream with bad parameters results in misleading error \"kernel modules must be upgraded\"", "body": "Creating separate issue from 'Scenario 4' described in #6547 per request.\r\n\r\n### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Fedora\r\nDistribution Version    | 25\r\nLinux Kernel                 | 4.12.8-200.fc25\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.1-1 (with encryption patches)\r\nSPL Version                  | 0.7.1-1\r\n\r\nI've installed SPL from the upstream f25 repo and compiled 0.7.1 from source, patching in:\r\n* Add libtpool (thread pools) (46364cb2f35545a7fc915df9593b719a94c43a83)\r\n* Send / Recv Fixes following b52563 (9b8407638da71ea9f4afb21375f991869f19811f)\r\n* Native Encryption for ZFS on Linux (b52563034230b35f0562b6f40ad1a00f02bd9a05)\r\n\r\n### Describe the problem you're observing\r\nWhen resuming a `zfs send` transfer but specifying an incorrect destination, ZFS produces an incorrect error message `cannot receive incremental stream: kernel modules must be upgraded to receive this stream.`.\r\n\r\nThe problem is unrelated entirely to kernel modules and rather that the wrong dataset has been specified for the `zfs recv` operation.\r\n\r\n### Describe how to reproduce the problem\r\n```\r\ntruncate -s 100M /root/src.img\r\ntruncate -s 100M /root/replica.img\r\nzpool create src /root/src.img\r\nzpool create replica /root/replica.img\r\nzfs create -o encryption=on -o keyformat=passphrase -o keylocation=prompt replica/encrypted\r\nzfs create src/myfiles\r\ndd if=/dev/urandom of=/src/myfiles/random.bin bs=1M count=30\r\nzfs snapshot src/myfiles@migration\r\nzfs create src/myfiles/child\r\ndd if=/dev/urandom of=/src/myfiles/child/random.bin bs=1M count=3\r\nzfs snapshot src/myfiles/child@migration\r\nzfs send src/myfiles@migration | zfs recv -s replica/encrypted/myfiles\r\nzfs send src/myfiles/child@migration | pv -L 500K | zfs recv -s replica/encrypted/myfiles/child\r\n# press ^c after a second or two of transfer starting\r\n\r\nTOKEN=$(zfs get -H receive_resume_token \"replica/encrypted/myfiles/child\" | cut -f3 | grep -ve '^-$')\r\n\r\nzfs send -t \"$TOKEN\" | zfs recv -s replica/encrypted/myfiles\r\n# cannot receive incremental stream: kernel modules must be upgraded to receive this stream.\r\n\r\nzfs send -t \"$TOKEN\" | zfs recv -s replica/encrypted/myfiles@migration\r\n# cannot receive incremental stream: kernel modules must be upgraded to receive this stream.\r\n\r\nzfs send -t \"$TOKEN\" | zfs recv -s replica/encrypted/myfiles/child@migration\r\n# works\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6574/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stefarossi": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6622", "title": "PANIC at dmu.c:1016:dmu_write()", "body": "I have a problem with a pool, consisting of a single 8TB drive.\r\nI think the problem originated from a software issue rather than a hardware one, as I can't find anything weird in the SMART log.\r\nThe drive was attached to a Mac running [O3X](https://openzfsonosx.org). One morning when I woke up I found that the Mac had restarted due to a kernel panic. My pool had been exported, and my other pool (two 3TB drives in mirroring mode) was in a degraded state with one of the mirrors marked as faulty. I was able to import the 8TB pool again with `zpool import -fFX`, but then the system started panicking. I was only able to solve this by detaching the drive.\r\n\r\nI now moved all my drives to a Debian box. I fixed the 3TB pool by re-adding the mirror, but I can't fix the 8TB one. I can only import it with `-o readonly=on`, but one of the three dataset remains inaccessible:\r\n```\r\nunable to fetch ZFS version for filesystem 'Pool3/DatasetName'\r\ncannot mount 'Pool3/DatasetName': Resource temporarily unavailable\r\n```\r\n`zpool status -v` shows this:\r\n ```\r\n  pool: Pool3\r\n state: ONLINE\r\nstatus: One or more devices has experienced an error resulting in data\r\n\tcorruption.  Applications may be affected.\r\naction: Restore the file in question if possible.  Otherwise restore the\r\n\tentire pool from backup.\r\n   see: http://zfsonlinux.org/msg/ZFS-8000-8A\r\n  scan: none requested\r\nconfig:\r\n\r\n\tNAME        STATE     READ WRITE CKSUM\r\n\tPool3       ONLINE       0     0     1\r\n\t  sdb       ONLINE       0     0     4\r\n\r\nerrors: Permanent errors have been detected in the following files:\r\n\r\n        <metadata>:<0x2c>\r\n        <metadata>:<0x62>\r\n        Pool3/DatasetName:<0x0>\r\n```\r\n\r\nIf I try importing the pool in RW mode, I get this:\r\n```\r\nMessage from syslogd@ hostname at Sep  9 20:53:23 ...\r\n\r\n kernel:[75891.300835] VERIFY3(0 == dmu_buf_hold_array(os, object, offset, size, 0, ((char *)__func__), &numbufs, &dbp)) failed (0 == 5)\r\n\r\nMessage from syslogd@hostname at Sep  9 20:53:23 ...\r\n\r\n kernel:[75891.300926] PANIC at dmu.c:1016:dmu_write()\r\n```\r\n\r\nand, in /var/log/messages:\r\n```\r\nSep  9 20:53:23 puck kernel: [75891.300964] CPU: 1 PID: 26095 Comm: txg_sync Tainted: P           O    4.9.0-3-amd64 #1 Debian 4.9.30-2+deb9u3\r\nSep  9 20:53:23 puck kernel: [75891.300967] Hardware name: HP ProLiant MicroServer, BIOS O41     10/01/2013\r\nSep  9 20:53:23 puck kernel: [75891.300971]  0000000000000000 ffffffffaef28574 ffffffffc11eecc4 ffffa633c162fad0\r\nSep  9 20:53:23 puck kernel: [75891.300979]  ffffffffc06e23a5 0000000000000286 0000000000000030 ffffa633c162fae0\r\nSep  9 20:53:23 puck kernel: [75891.300986]  ffffa633c162fa80 2833594649524556 756d64203d3d2030 6c6f685f6675625f\r\nSep  9 20:53:23 puck kernel: [75891.300992] Call Trace:\r\nSep  9 20:53:23 puck kernel: [75891.301006]  [<ffffffffaef28574>] ? dump_stack+0x5c/0x78\r\nSep  9 20:53:23 puck kernel: [75891.301032]  [<ffffffffc06e23a5>] ? spl_panic+0xc5/0x100 [spl]\r\nSep  9 20:53:23 puck kernel: [75891.301174]  [<ffffffffc10b99e0>] ? dmu_buf_hold_array_by_dnode+0x3b0/0x470 [zfs]\r\nSep  9 20:53:23 puck kernel: [75891.301291]  [<ffffffffc10cd81e>] ? dnode_rele_and_unlock+0x4e/0x80 [zfs]\r\nSep  9 20:53:23 puck kernel: [75891.301402]  [<ffffffffc10b9f90>] ? dmu_buf_hold_array.constprop.12+0x80/0xb0 [zfs]\r\nSep  9 20:53:23 puck kernel: [75891.301513]  [<ffffffffc10ba17f>] ? dmu_write.part.7+0xef/0x100 [zfs]\r\nSep  9 20:53:23 puck kernel: [75891.301644]  [<ffffffffc1110bd4>] ? space_map_write+0x314/0x560 [zfs]\r\nSep  9 20:53:23 puck kernel: [75891.301660]  [<ffffffffc06dd08a>] ? spl_kmem_zalloc+0x8a/0x160 [spl]\r\nSep  9 20:53:23 puck kernel: [75891.301666]  [<ffffffffaecec192>] ? getrawmonotonic64+0x32/0xc0\r\nSep  9 20:53:23 puck kernel: [75891.301791]  [<ffffffffc10f0ecd>] ? metaslab_sync+0x14d/0x7a0 [zfs]\r\nSep  9 20:53:23 puck kernel: [75891.301901]  [<ffffffffc10b7347>] ? ddt_sync+0x337/0x620 [zfs]\r\nSep  9 20:53:23 puck kernel: [75891.302023]  [<ffffffffc10ea29a>] ? dsl_scan_sync+0x4a/0xc40 [zfs]\r\nSep  9 20:53:23 puck kernel: [75891.302129]  [<ffffffffc10ac0ff>] ? bplist_iterate+0xaf/0x100 [zfs]\r\nSep  9 20:53:23 puck kernel: [75891.302260]  [<ffffffffc11176bd>] ? vdev_sync+0x5d/0x120 [zfs]\r\nSep  9 20:53:23 puck kernel: [75891.302389]  [<ffffffffc1100738>] ? spa_sync+0x468/0xd80 [zfs]\r\nSep  9 20:53:23 puck kernel: [75891.302520]  [<ffffffffc1112ad1>] ? txg_sync_thread+0x2e1/0x4b0 [zfs]\r\nSep  9 20:53:23 puck kernel: [75891.302652]  [<ffffffffc11127f0>] ? txg_delay+0x1a0/0x1a0 [zfs]\r\nSep  9 20:53:23 puck kernel: [75891.302667]  [<ffffffffc06df230>] ? __thread_exit+0x20/0x20 [spl]\r\nSep  9 20:53:23 puck kernel: [75891.302682]  [<ffffffffc06df29d>] ? thread_generic_wrapper+0x6d/0x80 [spl]\r\nSep  9 20:53:23 puck kernel: [75891.302689]  [<ffffffffaec965d7>] ? kthread+0xd7/0xf0\r\nSep  9 20:53:23 puck kernel: [75891.302694]  [<ffffffffaec96500>] ? kthread_park+0x60/0x60\r\nSep  9 20:53:23 puck kernel: [75891.302699]  [<ffffffffaec96500>] ? kthread_park+0x60/0x60\r\nSep  9 20:53:23 puck kernel: [75891.302705]  [<ffffffffaf2064f5>] ? ret_from_fork+0x25/0x30\r\n```\r\n\r\nI tried the FreeBSD live cd, but it too crashes and reboots.\r\nIs there any way to recover that dataset?\r\n\r\nSystem information:\r\n```\r\nType                         | Version/Name\r\n---                          | --- \r\nDistribution Name            | Debian\r\nDistribution Version         | 9\r\nLinux Kernel                 | 4.9.0-3\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.7.1-1\r\nSPL Version                  | 0.7.1-1\r\n```\r\n\r\nThanks a lot.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6622/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cousins": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6613", "title": "resilvering continually restarts on read error and cannot offline bad drive", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |   CentOS \r\nDistribution Version    |   6.9\r\nLinux Kernel                 |  2.6.32-696.6.3\r\nArchitecture                 |  x86_64\r\nZFS Version                  |  0.6.5.11-1\r\nSPL Version                  |  0.6.5.11-1\r\n\r\nI have a pool made up of 60 drives with six 10-disk RAIDZ2s concatenated together. It is on version 0.6.5.11-1 (CentOS 6.9). The last update I did was in July and it is using kmod with the kernel =  2.6.32-696.6.3.el6.x86_64\r\n\r\nA drive started getting READ and CKSUM errors during a recent scrub. I usually \"zpool offline\" the drive, pull it, put a new one in and \"zpool replace\" it with the help of a script I wrote that deals with drive blinking, multipath, updating vdev_id.conf, udev and unblinking the drive. In this case however, I was away for a while and I wanted to replace it with one that was set as a Hot-Spare. I should have taken the drive offline before doing the replace I think but instead I ran \"zpool replace\" with the drive that was set up as a Hot-Spare.\r\n\r\nIt looks like this now:\r\n\r\n```\r\n\t  raidz2-3     ONLINE       0     0     0\r\n\t    mpathbd    ONLINE       0     0     0\r\n\t    mpathbf    ONLINE       0     0     0\r\n\t    mpathbg    ONLINE       0     0     0\r\n\t    spare-3    ONLINE       0     0    46\r\n\t      mpathbx  ONLINE      79     0     6  (resilvering)\r\n\t      1-44     ONLINE       0     0     0  (resilvering)\r\n\t    0-31       ONLINE       0     0     0\r\n\t    mpathl     ONLINE       0     0     0\r\n\t    mpathm     ONLINE       0     0     0\r\n\t    mpathn     ONLINE       0     0     0\r\n\t    mpathas    ONLINE       0     0     0\r\n\t    1-31       ONLINE       0     0     0\r\n```\r\n\r\nThe mix of mpath?? device names and 1-44 names has to do with this being an earlier system when I was just starting to use /etc/zfs/vdev_id.conf. I should re-import it with the consistent vdev names but my scripts work either way so it hasn't been a high priority. \r\n\r\nWhat is happening is that it will resilver for a day or more and then it will get a READ error on that bad drive (mpathbx) and then the resilvering starts over from the beginning.\r\n\r\nSo now, what should have taken a long but manageable time, will now possibly never finish. Currently it has gone on for two weeks now. \r\n\r\nI have tried to offline and detach mpathbx but get:\r\n\r\n```\r\n[root@nfs1 etc]#  zpool offline pool2 mpathbx\r\ncannot offline mpathbx: no valid replicas\r\n\r\n[root@nfs1 etc]#  zpool detach pool2 mpathbx\r\ncannot detach mpathbx: no valid replicas\r\n```\r\n\r\nI thought zed was causing the resilvering to restart so I tried stopping zed but that has not helped. It continues to restart the resilvering process every time it gets a read error on mpathbx.\r\n\r\nIt seems that there is either a bug here or something simple that I have overlooked. I sent this to the zfs-discuss list but got nothing much. The last suggestion was to just pull mpathbx and the spare  and see what happens. As this is a large system (I do have an off site replica of the data) it would take a long time to deal with a mistake and so I'd like to hear what others think. \r\n\r\nAny ideas?\r\n\r\nThanks,\r\n\r\nSteve", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6613/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sdelafond": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6609", "title": "Importing pool fails and hangs forever (INFO: task l2arc_feed:796 blocked for more than...)", "body": "\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Debian\r\nDistribution Version    |  unstable\r\nLinux Kernel                 |  4.12.0-1-amd64 #1 SMP Debian 4.12.6-1 (2017-08-12)\r\nArchitecture                 | amd64\r\nZFS Version                  |  0.6.5.11-1\r\nSPL Version                  | 0.6.5.11-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\nMy system crashed brutally this afternoon, didn't get a stack trace at all. Upon reboot, \"zfs import storage\" hangs forever, with the load gradually reaching 3. In dmesg I see a bunch of the following:\r\n```\r\n[ 1571.829354] INFO: task l2arc_feed:796 blocked for more than 120 seconds.\r\n[ 1571.836183]       Tainted: P          IO    4.12.0-1-amd64 #1 Debian 4.12.6-1\r\n[ 1571.843434] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1571.851395] l2arc_feed      D    0   796      2 0x00000000\r\n[ 1571.851399] Call Trace:\r\n[ 1571.851406]  ? __schedule+0x3c5/0x850\r\n[ 1571.851409]  ? try_to_del_timer_sync+0x4d/0x80\r\n[ 1571.851414]  ? schedule+0x32/0x80\r\n[ 1571.851416]  ? schedule_preempt_disabled+0xa/0x10\r\n[ 1571.851418]  ? __mutex_lock.isra.4+0x2c3/0x4f0\r\n[ 1571.851467]  ? l2arc_feed_thread+0x1bf/0xd80 [zfs]\r\n[ 1571.851501]  ? l2arc_feed_thread+0x1bf/0xd80 [zfs]\r\n[ 1571.851505]  ? dequeue_entity+0xe4/0x460\r\n[ 1571.851508]  ? pick_next_task_fair+0x133/0x540\r\n[ 1571.851512]  ? __switch_to+0x234/0x430\r\n[ 1571.851521]  ? thread_generic_wrapper+0x62/0x80 [spl]\r\n[ 1571.851556]  ? l2arc_evict+0x370/0x370 [zfs]\r\n[ 1571.851563]  ? thread_generic_wrapper+0x6d/0x80 [spl]\r\n[ 1571.851568]  ? kthread+0xfc/0x130\r\n[ 1571.851574]  ? __thread_exit+0x20/0x20 [spl]\r\n[ 1571.851577]  ? kthread_create_on_node+0x70/0x70\r\n[ 1571.851580]  ? ret_from_fork+0x25/0x30\r\n[ 1571.851601] INFO: task zpool:12432 blocked for more than 120 seconds.\r\n[ 1571.858210]       Tainted: P          IO    4.12.0-1-amd64 #1 Debian 4.12.6-1\r\n[ 1571.865511] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1571.873470] zpool           D    0 12432   4470 0x00000000\r\n[ 1571.873472] Call Trace:\r\n[ 1571.873476]  ? __schedule+0x3c5/0x850\r\n[ 1571.873533]  ? zil_claim+0x250/0x250 [zfs]\r\n[ 1571.873538]  ? schedule+0x32/0x80\r\n[ 1571.873549]  ? taskq_wait+0x7e/0xd0 [spl]\r\n[ 1571.873556]  ? remove_wait_queue+0x60/0x60\r\n[ 1571.873601]  ? dmu_objset_find_dp+0x113/0x1b0 [zfs]\r\n[ 1571.873656]  ? spa_load+0x18b1/0x1c80 [zfs]\r\n[ 1571.873662]  ? zpool_get_rewind_policy+0x74/0x1a0 [zcommon]\r\n[ 1571.873716]  ? spa_load_best+0x5a/0x280 [zfs]\r\n[ 1571.873770]  ? spa_import+0x1cb/0x710 [zfs]\r\n[ 1571.873825]  ? get_nvlist+0xc6/0xf0 [zfs]\r\n[ 1571.873880]  ? zfs_ioc_pool_import+0xf5/0x120 [zfs]\r\n[ 1571.873936]  ? zfsdev_ioctl+0x462/0x4f0 [zfs]\r\n[ 1571.873939]  ? do_vfs_ioctl+0x9f/0x600\r\n[ 1571.873943]  ? SyS_ioctl+0x74/0x80\r\n[ 1571.873946]  ? system_call_fast_compare_end+0xc/0x97\r\n[ 1571.873989] INFO: task dmu_objset_find:13497 blocked for more than 120 seconds.\r\n[ 1571.881430]       Tainted: P          IO    4.12.0-1-amd64 #1 Debian 4.12.6-1\r\n[ 1571.888634] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[ 1571.896594] dmu_objset_find D    0 13497      2 0x00000000\r\n[ 1571.896596] Call Trace:\r\n[ 1571.896600]  ? __schedule+0x3c5/0x850\r\n[ 1571.896602]  ? schedule+0x32/0x80\r\n[ 1571.896611]  ? cv_wait_common+0x115/0x130 [spl]\r\n[ 1571.896617]  ? remove_wait_queue+0x60/0x60\r\n[ 1571.896662]  ? dmu_buf_hold_array_by_dnode+0x1bc/0x4a0 [zfs]\r\n[ 1571.896705]  ? dmu_read+0x9f/0x1a0 [zfs]\r\n[ 1571.896762]  ? space_map_load+0x1c5/0x520 [zfs]\r\n[ 1571.896814]  ? metaslab_load+0x2e/0xc0 [zfs]\r\n[ 1571.896865]  ? metaslab_activate+0x82/0xb0 [zfs]\r\n[ 1571.896916]  ? metaslab_claim+0x192/0x480 [zfs]\r\n[ 1571.896971]  ? zio_dva_claim+0x1a/0x30 [zfs]\r\n[ 1571.897024]  ? zio_wait+0x72/0x140 [zfs]\r\n[ 1571.897078]  ? zil_parse+0x1ec/0x910 [zfs]\r\n[ 1571.897132]  ? zil_replaying+0x60/0x60 [zfs]\r\n[ 1571.897194]  ? zil_claim_log_block+0xa0/0xa0 [zfs]\r\n[ 1571.897254]  ? zil_check_log_chain+0xe0/0x190 [zfs]\r\n[ 1571.897299]  ? dmu_objset_find_dp_impl+0x18a/0x3c0 [zfs]\r\n[ 1571.897343]  ? dmu_objset_find_dp_cb+0x25/0x40 [zfs]\r\n[ 1571.897354]  ? taskq_thread+0x27e/0x4b0 [spl]\r\n[ 1571.897361]  ? wake_up_q+0x70/0x70\r\n[ 1571.897367]  ? kthread+0xfc/0x130\r\n[ 1571.897374]  ? taskq_thread_spawn+0x50/0x50 [spl]\r\n[ 1571.897376]  ? kthread_create_on_node+0x70/0x70\r\n[ 1571.897379]  ? ret_from_fork+0x25/0x30\r\n```\r\n\r\nIf I reboot and don't try to import, \"zfs import\" reports that my pool is as follows:\r\n\r\n```\r\n   pool: storage                                                              \r\n     id: 12616089183921587906                                                  \r\n  state: ONLINE                                                                      \r\n action: The pool can be imported using its name or numeric identifier.        \r\n config:                                                                       \r\n                                                                                \r\n        storage     ONLINE                                                     \r\n          mirror-0  ONLINE                                                        \r\n            sdb     ONLINE                                                           \r\n            sdd     ONLINE                                                    \r\n        cache                                                                 \r\n          sdc2                                                                 \r\n          sda2                                                                \r\n        logs                                                                         \r\n          mirror-1  ONLINE                                                     \r\n            sda3    ONLINE                                                    \r\n            sdc1    ONLINE     \r\n```                                                \r\n\r\nI wasn't sure if cache partitions from a not-yet-imported volume were supposed to show as ONLINE as well, so I tried simply removing /dev/sd[ac]2 but that didn't change the symptoms upon importing.\r\n\r\nzbl -l does see LABELs on each of the partition making up my pool, in case it's worth mentioning.\r\n\r\nMaybe more importantly, I can import  the pool fine if I pass -o readonly=on. All the filesystems  then get mounted, and it appears I can read my data just fine, without any spurious logs in dmesg.\r\n\r\nWhen the pool is imported read-only, zpool status storage shows:\r\n\r\n```\r\n  pool: storage\r\n state: ONLINE                                \r\n  scan: scrub repaired 0 in 9h23m with 0 errors on Sun Aug 13 09:47:43 2017\r\nconfig:                                       \r\n             \r\n        NAME        STATE     READ WRITE CKSUM\r\n        storage     ONLINE       0     0     0\r\n          mirror-0  ONLINE       0     0     0\r\n            sdb     ONLINE       0     0     0\r\n            sdd     ONLINE       0     0     0                                 \r\n        logs                                            \r\n          mirror-1  ONLINE       0     0     0                                 \r\n            sda3    ONLINE       0     0     0                                 \r\n            sdc1    ONLINE       0     0     0\r\n        cache\r\n          sdc2      ONLINE       0     0     0\r\n          sda2      ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n```\r\n\r\nsdb and sdc are 2 identical 3To SATA drives, while sda and sdc are SSD, with sda1 being /. SMART doesn't see any problems with them.\r\n\r\nShort of changing all four hard-drives, what are my options here ? If one HD is truly going bad, and causing the issue, how can I identify it ? More generally, what would cause ZFS to act like this ?\r\n\r\nThanks a lot for your time ! ", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6609/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jwittlincohen": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6606", "title": "Various Packaging issues on Debian (Stretch/Buster) and likely Ubuntu", "body": "Type                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |   Debian\r\nDistribution Version    |    Stretch (9.1) and Buster (Testing)\r\nLinux Kernel                |   Stretch (4.9.0-3-amd64 #1 SMP Debian 4.9.30-2+deb9u3) and Buster (4.12.0-1-amd64 #1 SMP Debian 4.12.6-1)\r\nArchitecture                 |    AMD64\r\nZFS Version                 |    0.7.0-1 \r\nSPL Version                 |    0.7.0-1\r\n\r\nNote: I'm not using ZFS 0.7.1 because of #6577.\r\n\r\n### Describe the problem you're observing\r\n\r\nI've experienced multiple packaging issues building custom kmod packages on Debian which are not documented on either the custom build howto or in the Debian-specific section.  \r\n\r\n1) systemd unit files are not packaged by default, and ./configure --enable-systemd is ignored  #6591.  The default directory, `/usr/lib/systemd/` is not used by Debian or Ubuntu.  Rather, both distributions use `/lib/systemd/`  While this should be trivial to change manually with a ./configure option, it's not clear whether the option is respected, given that the systemd unit files aren't packaged at all.\r\n \r\n2) The systemd unit files won't work on Debian (they shouldn't work on Ubuntu either) because the paths for the binaries and cache files are different.\r\n\r\nFor example, the systemd files reference `/usr/local/etc/zfs/zpool.cache`, `/usr/local/sbin/zpool`, `/usr/local/sbin/zfs`, and `/usr/local/zed`.    The corresponding locations on Debian are `/etc/zfs/zpool.cache`, `/sbin/zpool`, `/sbin/zfs`, and `/usr/sbin/zed`.\r\n\r\n3) The ZoL packages do not include `/usr/share/initramfs-tools/conf.d/zfs` which is required to provide ZFS support in the initramfs.\r\n\r\n### Workaround\r\n\r\nUse the following [guide](https://github.com/zfsonlinux/zfs/wiki/Custom-Packages) to compile and install the SPL and ZFS packages.  However, prior to running `./configure` for SPL and ZFS, run `autoreconf --force --install` to regenerate the configure script using Debian's version of libtool.  This resolves Issue #6577.\r\n\r\n1) Copy systemd unit files and config files from Debian's [zfsutils-linux](https://packages.debian.org/buster/zfsutils-linux) package.  Specifically, I copied the unit files to `/lib/systemd/system`, the default configuration to `/lib/systemd/system-preset` and the module config to `/lib/modules-load.d/`.  The paths are the same as indicated [here](https://packages.debian.org/sid/amd64/zfsutils-linux/filelist).\r\n\r\n2) Unmask all the systemd services with `systemctl unmask`\r\n\r\n3) To boot off a ZFS root pool, create `/usr/share/initramfs-tools/conf.d/zfs ` with the following content: \r\n\r\n```\r\nfor x in $(cat /proc/cmdline)\r\ndo\r\n    case $x in\r\n        root=ZFS=*)\r\n            BOOT=zfs\r\n            ;;\r\n    esac\r\ndone\r\n```\r\n\r\nThen update the initramfs by running `update-initramfs -u -k all`\r\n\r\nIt would be nice to include these workarounds either on the Custom Build Howto page or on a separate page related to Debian/Ubuntu specifically, at least until these issues are resolved.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6606/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6577", "title": "zfs: error while loading shared libraries: libnvpair.so.1 (0.7.1 Regression vs 0.7.0)", "body": "Distribution Name       |    Debian\r\nDistribution Version    |     Stretch (9.1)\r\nLinux Kernel                |     4.9.0-3-amd64 #1 SMP Debian 4.9.30-2+deb9u3 (2017-08-06) x86_64 \r\nArchitecture                 |     AMD64\r\nZFS Version                  |   0.7.1-1\r\nSPL Version                  |   0.7.1-1\r\n\r\n### Describe the problem you're observing\r\n\r\nI built custom kmod packages of SPL and ZFS 0.7.1 using the instructions available [here](https://github.com/zfsonlinux/zfs/wiki/Custom-Packages).  The ./configure and compilation steps did not result in any errors, and I was able to successfully install all resulting packages.  However, when running the `zfs` or `zpool` commands, I receive the following error:\r\n\r\n`zfs: error while loading shared libraries: libnvpair.so.1: cannot open shared object file: No such file or directory`\r\n\r\nThis library is packaged by `libnvpair1` and is located at `/lib64`.  When I compiled SPL and ZFS 0.7.0 following the exact same procedure, I did not receive any errors running the `zfs` or `zpool` commands.  Thus, this appears to be a regression in 0.7.1\r\n\r\n### Describe how to reproduce the problem\r\n\r\nCompile ZFS and SPL 0.7.1 using the instructions [here](https://github.com/zfsonlinux/zfs/wiki/Custom-Packages).  Then attempt to run the `zfs` or `zpool` commands.\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6577/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nunompad": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6600", "title": "undefined reference to `SHA256TransformBlocks'", "body": "Type                                | zfs-0.7.0 and zfs-0.7.1\r\n  ---                                  |     --- \r\nDistribution Name       |  debian\r\nDistribution Version    | 8.9\r\nLinux Kernel                 | Linux 3.10.107\r\nArchitecture                 |  arm\r\nZFS Version                  |  0.7 and 0.7.1\r\nSPL Version                  |  0.7 and 0.7.1\r\n\r\nThe version 0.6.5.11 compiles without error.\r\n\r\nThe classes SHA256 and SHA512 are only declared in x86.\r\n\r\nCompile error:\r\n```\r\n  CCLD     sha2_test\r\n../../../../../lib/libicp/.libs/libicp.a(sha2.o): In function `SHA2Update':\r\n/home/nmesqui/git/BPI-M2U-bsp/zfs/lib/libicp/../../module/icp/algs/sha2/sha2.c:864: undefined reference to `SHA512TransformBlocks'\r\n/home/nmesqui/git/BPI-M2U-bsp/zfs/lib/libicp/../../module/icp/algs/sha2/sha2.c:834: undefined reference to `SHA256TransformBlocks'\r\n/home/nmesqui/git/BPI-M2U-bsp/zfs/lib/libicp/../../module/icp/algs/sha2/sha2.c:857: undefined reference to `SHA256TransformBlocks'\r\n/home/nmesqui/git/BPI-M2U-bsp/zfs/lib/libicp/../../module/icp/algs/sha2/sha2.c:836: undefined reference to `SHA512TransformBlocks'\r\ncollect2: error: ld returned 1 exit status\r\nMakefile:643: recipe for target 'sha2_test' failed\r\nmake[6]: *** [sha2_test] Error 1\r\n```\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6600/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dd1dd1": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6589", "title": "arcstat.py cache hit and miss reversed", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  CentOS\r\nDistribution Version    |  7.3\r\nLinux Kernel                 |  3.10.0-514.26.2.el7.x86_64\r\nArchitecture                 | \r\nZFS Version                  |  0.7.0-1\r\nSPL Version                  |  0.7.0-1\r\n\r\n### Describe the problem you're observing\r\n\r\narcstat.py reports 100% cache miss instead of 0% cache miss when running from cached data. \"hits\" and \"misses\" counters in /proc/spl/kstat/zfs/arcstats appear to be swapped (\"misses\" counts cache hits, \"hits\" counts cache misses).\r\n\r\n### Describe how to reproduce the problem\r\n\r\ngo to a zfs pool and run \"du -ks *\" 10 times. Before this, ensure there is nobody else is using the pool (no scrub running, etc, simplest is to see if \"iostat -x 1\" reports all disks idle). run \"iostat -x 1\" and \"vmstat 1\" while doing this to observe presence of disk i/o. run \"arcstat.py 1\" \r\n\r\nThe first time the cache is warmed, you should see some disk i/o, on subsequent runs, the cache is warm and you will see no disk i/o (or very little disk i/o). (I presume your test machine is not a raspberry pi with 0.5GB RAM and you have not otherwise reduced the size of the arc cache to 1 byte or less).\r\n\r\nSo on these subsequent runs of \"du\" you should see no disk i/o and \"arcstat.py\" should report 0% cache misses (all read requests come from cache, there is no disk i/o, yes?).\r\n\r\nBut instead, arcstat.py reports \"100\" for cache misses in miss%, dm%, mm%.\r\n\r\nThis is confirmed by looking at \"hits\" and \"misses\" in /proc/spl/kstat/zfs/arcstats, on can easily see\r\nthat \"hits\" is rarely incremented while \"misses\" is incremented all the time.\r\n\r\nK.O.\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6589/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zviratko": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6584", "title": "Crazy write amplification", "body": "\r\n### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Ubuntu\r\nDistribution Version    |  17.04\r\nLinux Kernel                 |  4.10.0-33-generic\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.0\r\nSPL Version                  | 0.7.0\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nPool created:\r\nzpool create -o ashift=12 -O recordsize=4k -O compression=off -O checksum=off ev3 /dev/ev3mema\r\n\r\nThen I run fio:\r\n\r\n`fio --filename=/ev3/fiotest --ioengine=libaio --direct=0 --rw=randwrite --bs=4k --iodepth=128 --size=15G --group_reporting --name=test`\r\n\r\n\r\nPerformance decresed over time, now (after a few minutes):\r\n\r\n```\r\nJobs: 1 (f=1): [w(1)] [48.6% done] [0KB/3576KB/0KB /s] [0/894/0 iops] [eta 11m:18s]\r\n\r\nzpool iostat -v ev3 1\r\n              capacity     operations     bandwidth\r\npool        alloc   free   read  write   read  write\r\n----------  -----  -----  -----  -----  -----  -----\r\nev3         15.2G   739M  2.62K  69.2K  15.9M   420M\r\n  ev3mema   15.2G   739M  2.62K  69.2K  15.9M   420M\r\n----------  -----  -----  -----  -----  -----  -----\r\n```\r\n\r\nIf I restart fio, it does between 100-2000 IOPS (though it seems to now mostly stay on the 100-200 mark), while zfs writes >1GB/s to the device\r\n\r\nThis makes no sense to me.\r\n\r\nWith 128k iosize, IOPS stay around ~1300 and zfs writes ~300MB/s to the device.\r\nWith 1B iosize, IOPS drop to 100 and zfs writes 1.2GB to the drive. W00T.\r\nWith 4K iosize and iodepth=1, fio starts at ~10K IOPS but drops to 700 while zfs writes 1.6GB/s.\r\n\r\nThis is without removing the /ev3/fiotest file, it is left dirty.\r\n\r\nAlso z_wr_iss threads are eating ~40% of all CPUs\r\n\r\nIf I create a ZVOL and run the same fio benchmark on that, it behaves as it should - 150K IOPS and 600MB/s, on subsequent runs (without recreating the ZVOL) it isn't nearly as stable and performs about 30% worse, but since fio is doing buffered writes I see it writing to the device long after fio itself has finished and I'm guessing there's some amplification as well (200%?).\r\n\r\nSwitching fio to direct=1 and bs=4k shows write amplification of 1000%:\r\n```\r\n  write: io=913540KB, bw=64970KB/s, iops=16242, runt= 14061msec\r\n\r\n              capacity     operations     bandwidth\r\npool        alloc   free   read  write   read  write\r\n----------  -----  -----  -----  -----  -----  -----\r\nev3         14.1G  1.81G    868  67.3K   434K   677M\r\n  ev3mema   14.1G  1.81G    868  67.3K   434K   677M\r\n----------  -----  -----  -----  -----  -----  -----\r\n```\r\n\r\nOn subsequent runs it rises even more (15MB/s vs 650MB/s).\r\n\r\nWith larger matched volblocksize=32k and iosize=32k it looks stable at 200% write amplification (even with logbias=throughput, so even that doesn't make sense to me)\r\n\r\n\r\nFYI this is fio on the block device itself:\r\n```\r\nfio --filename=/dev/ev3mema --ioengine=libaio --direct=1 --rw=randwrite --bs=4k --iodepth=128 --size=14G --group_reporting --name=file1\r\n\r\n  write: io=14336MB, bw=1840.4MB/s, iops=471118, runt=  7790msec\r\n```\r\n\r\nThis is fio on ext4 using this block device:\r\n```\r\nfio --filename=/ev3ext4/fiotest --ioengine=libaio --direct=1 --rw=randwrite --bs=4k --iodepth=128 --size=14G --group_reporting --name=file1\r\n\r\n  write: io=14336MB, bw=558071KB/s, iops=139517, runt= 26305msec\r\n```\r\n\r\n(block device shows ~600MB/s)\r\n\r\nAm I missing something or is it seriously broken?\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6584/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6504", "title": "ZFS receive dataset busy - race with /lib/udev/zvol_id", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Ubuntu\r\nDistribution Version    |  14.04, 16.04\r\nLinux Kernel                 | 4.4 / 4.10\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.6.5.6, 0.6.5.9, 0.7.0\r\nSPL Version                  | 0.6.5.6, 0.6.5.9, 0.7.0\r\n\r\n### Describe the problem you're observing\r\n\r\nZFS receive fails occassionaly when receiving many ZVOLs with snapshots/clones.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nEither a recursive or a sequential receive of many datasets (=ZVOLs) sometimes fails with \"dataset is busy\".\r\nThe workaround is to either chmod -x /lib/udev/zvol_id, or to set zvol_inhibit_dev=1\r\nNeither solution is nice in case one needs to use this copy (backup).\r\n\r\nI trigger this almost any time I start a new full backup, sometimes even with incremental backup that contains new datasets. Iit happens on Ubuntu 14 with zfs-dkms as well as on two different machines with 16.04 (4.4 or 4.10 HWE kernel, even with ZFS 0.7.0). The source storage is fast (full flash) connected via 10G to the backup server. Backup server uses spindles.\r\n\r\nWhat seems to happen is that once a ZVOL is received, zvol_id triggers and wants to open this ZVOL and create a symlink, however receive wants to do more work on that same dataset because it's also receiving a whole bunch of snapshots and clones of this dataset - this fails if zvol_id holds a lock.\r\n\r\n### Proposed solutions\r\n\r\n1) make zfs receive wait a little while for the lock before giving up\r\n2) make zvol_id not lock the dataset\r\n\r\n**3) allow user to configure device creation on a dataset level**\r\nI have thousands of /dev/zd* devices on my backup storage and I don't even need them most of the time. What I need is to create a dev node for a particular dataset and then make it go away when I've used it. This setting could be for one singular dataset or a parent dataset and recursive.\r\nI believe something like:\r\nzfs set zvol_inhibit_dev=1 backup/zfs1.prodcluster/tank\r\nWould be very useful", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6504/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "k-u-bux": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6572", "title": "zfs send -R fails cannot send more than 65535 snapshots ", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  gentoo\r\nDistribution Version    |  rolling release\r\nLinux Kernel                 |  4.4.6\r\nArchitecture                 |  amd64\r\nZFS Version                  |  0.7.1-r0-gentoo\r\nSPL Version                  | 0.7.1-r0-gentoo\r\n\r\n### Describe the problem you're observing\r\nTrying to replicate a zfs dataset with send -R fails for datasets with many snapshots.\r\n\r\nI created a list snapshots.home of all snapshots and used binary search to find out how many snapshots I can send. Turns out, the break point is 2^16:\r\n\r\n```\r\nzfs send -vR $( awk 'FNR==65536' snapshots.home ) >/dev/null 2>send.log\r\n```\r\nfails with an internal error. Here is the tail of send.log:\r\n\r\n```\r\n...\r\nsend from @2016-11-27_21-10-01 to backup/home@2016-11-27_21-15-01 estimated size is 1.44M\r\nsend from @2016-11-27_21-15-01 to backup/home@2016-11-27_21-20-01 estimated size is 9.77M\r\ntotal estimated size is 846G\r\ncannot hold: operation not applicable to datasets of this type\r\ninternal error: Invalid argument\r\n```\r\nThe command\r\n```\r\nzfs send -vR $( awk 'FNR==65535' snapshots.home ) >/dev/null 2>send.log\r\n```\r\ncompletes just fine.\r\n\r\n\r\nI conjecture that this is caused by using an unsigned short in the avl data structure:\r\n\r\n```\r\nstruct avl_node {\r\n        struct avl_node *avl_child[2];  /* left/right children */\r\n        struct avl_node *avl_parent;    /* this node's parent */\r\n        unsigned short avl_child_index; /* my index in parent's avl_child[] */\r\n        short avl_balance;              /* balance value: -1, 0, +1 */\r\n};\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6572/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "daxtens": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6560", "title": "member access within null pointer of type 'struct mount'", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 16.04\r\nLinux Kernel                 | 4.10.17+ (Ubuntu Zesty kernel + KASAN + UBSAN)\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.7.1 from source\r\nSPL Version                  | 0.7.1 from source\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nWhen running trinity tests against ZFS, I see the following kernel BUG.\r\n\r\n### Describe how to reproduce the problem\r\n\r\n```\r\nzpool create -f foo /dev/vdb\r\ncp -r ~/zfs/scripts /foo\r\n/trinity/trinity -V /foo/scripts/ -a 64 -c mount -c umount --dangerous\r\n```\r\n\r\nWait 15-60 mins or so, and observer the following on dmesg:\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\n```\r\n[ 2326.871163] ================================================================================\r\n[ 2326.872084] UBSAN: Undefined behaviour in /home/dja/dev/linux/linux/fs/pnode.c:229:18\r\n[ 2326.875421] member access within null pointer of type 'struct mount'\r\n[ 2326.875421] CPU: 0 PID: 8746 Comm: trinity-c3 Tainted: P           OE   4.10.17+ #42\r\n[ 2326.880320] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.10.1-1ubuntu1 04/01/2014\r\n[ 2326.880320] Call Trace:\r\n[ 2326.880320]  dump_stack+0xb2/0x104\r\n[ 2326.880320]  ? _atomic_dec_and_lock+0xcc/0xcc\r\n[ 2326.880320]  ? printk+0x5/0xc8\r\n[ 2326.880320]  ubsan_epilogue+0xd/0x81\r\n[ 2326.880320]  __ubsan_handle_type_mismatch+0x156/0x40d\r\n[ 2326.880320]  ? ubsan_epilogue+0x81/0x81\r\n[ 2326.880320]  ? is_subdir+0x5/0xd0\r\n[ 2326.880320]  propagate_one+0x915/0xa40\r\n[ 2326.880320]  ? next_group+0x4c0/0x4c0\r\n[ 2326.880320]  ? propagate_one+0x5/0xa40\r\n[ 2326.880320]  ? get_mountpoint+0x14c/0x430\r\n[ 2326.880320]  propagate_mnt+0x31f/0x520\r\n[ 2326.880320]  attach_recursive_mnt+0x4a7/0x9d0\r\n[ 2326.880320]  ? count_mounts+0x260/0x260\r\n[ 2326.891280]  ? 0xffffffffc04f8077\r\n[ 2326.891280]  ? attach_recursive_mnt+0x5/0x9d0\r\n[ 2326.891280]  graft_tree+0x1f6/0x360\r\n[ 2326.893345]  do_mount+0x1827/0x31f0\r\n[ 2326.893345]  ? 0xffffffffc04f8077\r\n[ 2326.894544]  ? copy_mount_string+0x20/0x20\r\n[ 2326.894544]  ? check_stack+0xe60/0xe60\r\n[ 2326.894544]  ? kasan_unpoison_shadow+0x35/0x50\r\n[ 2326.894544]  ? 0xffffffffc04f8077\r\n[ 2326.894544]  ? kmem_cache_alloc_trace+0x11c/0x360\r\n[ 2326.894544]  ? copy_mount_options+0x5a/0x3b0\r\n[ 2326.894544]  ? do_mount+0x5/0x31f0\r\n[ 2326.894544]  ? copy_mount_options+0x5a/0x3b0\r\n[ 2326.894544]  SyS_mount+0x83/0xd0\r\n[ 2326.894544]  ? copy_mnt_ns+0x1290/0x1290\r\n[ 2326.894544]  do_syscall_64+0x199/0x410\r\n[ 2326.894544]  ? do_syscall_64+0x5/0x410\r\n[ 2326.894544]  entry_SYSCALL64_slow_path+0x25/0x25\r\n[ 2326.894544] RIP: 0033:0x7f2b81990499\r\n[ 2326.894544] RSP: 002b:00007ffc63be4818 EFLAGS: 00000246 ORIG_RAX: 00000000000000a5\r\n[ 2326.894544] RAX: ffffffffffffffda RBX: 00000000000000a5 RCX: 00007f2b81990499\r\n[ 2326.894544] RDX: 0000000000000004 RSI: 0000000002c68890 RDI: 0000000002c688d0\r\n[ 2326.894544] RBP: 00007f2b8205d000 R08: 0000000000000004 R09: 00007fffffffffff\r\n[ 2326.894544] R10: 0000000038609004 R11: 0000000000000246 R12: 0000000000000002\r\n[ 2326.894544] R13: 00007f2b8205d058 R14: 00007f2b8207a698 R15: 00007f2b8205d000\r\n[ 2326.894544] ================================================================================\r\n[ 2326.915107] kasan: CONFIG_KASAN_INLINE enabled\r\n[ 2326.915913] kasan: GPF could be caused by NULL-ptr deref or user memory access\r\n[ 2326.917339] general protection fault: 0000 [#1] SMP KASAN\r\n[ 2326.918324] Modules linked in: kcm strparser nfc af_alg caif_socket caif pn_pep phonet fcrypt af_rxrpc hidp cmtp kernelcapi bnep rfcomm bluetooth can_bcm can_raw can pptp gre l2tp_ppp l2tp_netlink l2tp_core ip6_udp_tunnel udp_tunnel pppoe pppox irda crypto_user dn_rtmsg nfnetlink xfrm_user xfrm_algo llc2 dccp_ipv6 atm appletalk ipx p8023 psnap p8022 llc sctp dccp_ipv4 dccp zfs(POE) icp(POE) zcommon(POE) zunicode(POE) znvpair(POE) zavl(POE) splat(OE) spl(OE) snd_hda_codec_generic snd_hda_intel snd_hda_codec snd_hda_core snd_hwdep snd_pcm input_leds snd_timer serio_raw snd joydev soundcore qemu_fw_cfg i2c_piix4 mac_hid ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi autofs4 btrfs raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx\r\n[ 2326.928957]  xor raid6_pq libcrc32c raid1 raid0 multipath linear crct10dif_pclmul crc32_pclmul ghash_clmulni_intel pcbc hid_generic usbhid qxl ttm aesni_intel hid drm_kms_helper aes_x86_64 syscopyarea sysfillrect crypto_simd glue_helper sysimgblt cryptd fb_sys_fops psmouse virtio_net drm virtio_blk floppy pata_acpi\r\n[ 2326.928957] CPU: 0 PID: 8746 Comm: trinity-c3 Tainted: P           OE   4.10.17+ #42\r\n[ 2326.928957] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.10.1-1ubuntu1 04/01/2014\r\n[ 2326.928957] task: ffff88010e986000 task.stack: ffff880030860000\r\n[ 2326.928957] RIP: 0010:propagate_one+0x278/0xa40\r\n[ 2326.928957] RSP: 0018:ffff880030867aa0 EFLAGS: 00010202\r\n[ 2326.928957] RAX: ffff88010bd4e700 RBX: 0000000000000000 RCX: 0000000000000006\r\n[ 2326.928957] RDX: dffffc0000000000 RSI: 0000000000000002 RDI: 0000000000000010\r\n[ 2326.928957] RBP: ffff880030867b48 R08: ffff88010e9d88c0 R09: fffffbfff79f1c87\r\n[ 2326.928957] R10: 0000000000000698 R11: fffffbfff79f1c88 R12: ffff88010443c1c0\r\n[ 2326.928957] R13: ffff88010bd4fa40 R14: 1ffff1000610cf58 R15: ffff88010bd4e700\r\n[ 2326.928957] FS:  00007f2b8207a700(0000) GS:ffff88011a800000(0000) knlGS:0000000000000000\r\n[ 2326.928957] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[ 2326.928957] CR2: 00007fff4e115ff8 CR3: 00000000b66ac000 CR4: 00000000003406f0\r\n[ 2326.928957] Call Trace:\r\n[ 2326.928957]  ? next_group+0x4c0/0x4c0\r\n[ 2326.928957]  ? propagate_one+0x5/0xa40\r\n[ 2326.928957]  ? get_mountpoint+0x14c/0x430\r\n[ 2326.928957]  propagate_mnt+0x31f/0x520\r\n[ 2326.928957]  attach_recursive_mnt+0x4a7/0x9d0\r\n[ 2326.928957]  ? count_mounts+0x260/0x260\r\n[ 2326.928957]  ? 0xffffffffc04f8077\r\n[ 2326.928957]  ? attach_recursive_mnt+0x5/0x9d0\r\n[ 2326.928957]  graft_tree+0x1f6/0x360\r\n[ 2326.928957]  do_mount+0x1827/0x31f0\r\n[ 2326.928957]  ? 0xffffffffc04f8077\r\n[ 2326.928957]  ? copy_mount_string+0x20/0x20\r\n[ 2326.928957]  ? check_stack+0xe60/0xe60\r\n[ 2326.928957]  ? kasan_unpoison_shadow+0x35/0x50\r\n[ 2326.928957]  ? 0xffffffffc04f8077\r\n[ 2326.928957]  ? kmem_cache_alloc_trace+0x11c/0x360\r\n[ 2326.928957]  ? copy_mount_options+0x5a/0x3b0\r\n[ 2326.928957]  ? do_mount+0x5/0x31f0\r\n[ 2326.928957]  ? copy_mount_options+0x5a/0x3b0\r\n[ 2326.928957]  SyS_mount+0x83/0xd0\r\n[ 2326.928957]  ? copy_mnt_ns+0x1290/0x1290\r\n[ 2326.928957]  do_syscall_64+0x199/0x410\r\n[ 2326.928957]  ? do_syscall_64+0x5/0x410\r\n[ 2326.928957]  entry_SYSCALL64_slow_path+0x25/0x25\r\n[ 2326.928957] RIP: 0033:0x7f2b81990499\r\n[ 2326.928957] RSP: 002b:00007ffc63be4818 EFLAGS: 00000246 ORIG_RAX: 00000000000000a5\r\n[ 2326.928957] RAX: ffffffffffffffda RBX: 00000000000000a5 RCX: 00007f2b81990499\r\n[ 2326.928957] RDX: 0000000000000004 RSI: 0000000002c68890 RDI: 0000000002c688d0\r\n[ 2326.928957] RBP: 00007f2b8205d000 R08: 0000000000000004 R09: 00007fffffffffff\r\n[ 2326.928957] R10: 0000000038609004 R11: 0000000000000246 R12: 0000000000000002\r\n[ 2326.928957] R13: 00007f2b8205d058 R14: 00007f2b8207a698 R15: 00007f2b8205d000\r\n[ 2326.928957] Code: 3c 16 00 0f 85 c3 05 00 00 48 8b 9b d8 00 00 00 48 89 1d 1c c5 e5 04 48 85 db 0f 84 85 06 00 00 48 8d 7b 10 48 89 fe 48 c1 ee 03 <80> 3c 16 00 0f 85 3c 06 00 00 49 39 d8 4c 8b 6b 10 0f 84 bf 00 \r\n[ 2326.928957] RIP: propagate_one+0x278/0xa40 RSP: ffff880030867aa0\r\n[ 2326.980113] ---[ end trace 8caa9c8c326f25fa ]---\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6560/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Lady-Galadriel": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6557", "title": "Native encryption - Odd behavior for copies=3: inherited and at dataset creation", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Gentoo\r\nDistribution Version    | Rolling\r\nLinux Kernel                 | 4.9.34\r\nArchitecture                 | AMD64\r\nZFS Version                  | 0.7.0-33_g08de8c16\r\nSPL Version                  | 0.7.0-12_g9df9692\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nThis is for ZFS native encrypted datasets only. You can inherit `copies=3` or create a dataset with `copies=3` without warning or error. It still only makes 2 copies, since 3 copies is not allowed for native ZFS encryption.\r\n\r\nPlease consider disallowing `-o copies=3` during encrypted dataset creation. And perhaps a warning on inheriting `copies=3`, that it will be turned into `copies=2` on the encrypted dataset.\r\n\r\nNote that changing `copies=3` after ZFS encrypted dataset creation does give a useful error message;\r\n`cannot set property for 'crpytpool/test': encrypted datasets cannot have 3 copies`\r\n\r\n### Describe how to reproduce the problem\r\n`zfs set copies=3 POOL`\r\n`zfs create -o encryption=on ... POOL/DATASET`\r\n`zfs get copies POOL POOL/DATASET`\r\n`NAME      PROPERTY  VALUE   SOURCE`\r\n`POOL      copies    3       local`\r\n`POOL/DATASET  copies    3       local`\r\n\r\n`zfs create -o copies=3 -o encryption=on ... POOL/DATASET`\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\nThere aren't any errors per say, just odd behaviour.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6556", "title": "Native encryption - request to allow raw keys on devices, (like USB flash)", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Gentoo\r\nDistribution Version    | Rolling\r\nLinux Kernel                 | 4.9.34\r\nArchitecture                 | AMD64\r\nZFS Version                  | 0.7.0-33_g08de8c16\r\nSPL Version                  | 0.7.0-12_g9df9692\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nIt would be nice if we could use a USB flash drive as the raw keysource. Meaning if the file was a device file, read only the first 32 bytes for the raw key.\r\n\r\nThis would be helpful when using a physical USB flash drive as a raw key. You only need it inserted during boot, pool import or dataset mount to un-lock the ZFS encryption. Afterwards, SysAdmin or Operator can remove the USB flash drive and lock it up. Any theft of the equipment, (disks or entire server), would then not include the raw key.\r\n\r\nOf course the USB flash drive could have a real file system on it, with either a raw key file or a hex key file. Except that this then requires extra security to make sure normal users can't read that file system, and can't read the {raw/hex} key file. It would be more secure to use a partition and raw data from it.\r\n\r\nFurther, the same USB flash drive can have dozens of keys on it, for different datasets, pools or servers. All with their own partition.\r\n\r\n### Describe how to reproduce the problem\r\n`dd if=/dev/urandom bs=32 count=1 of=/dev/sdb1`\r\n\r\n`zfs create -o encryption=on -o keyformat=raw -o keylocation=file:///dev/disk/by-id/usb-USB_2.0_Flash_Disk_12345678-part1 rpool/encrypt_test`\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\ncannot create 'rpool/encrypt_test': Raw key too long (expected 32).", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6556/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "GregorKopka": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6534", "title": "nop-write not in effect", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Gentoo\r\nDistribution Version    | rolling, stale as of Jun 2016\r\nLinux Kernel                 | 4.1.12-gentoo SMP mod_unload modversions\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.6.5-305_g46ab359\r\nSPL Version                  | 0.6.5-62_g16fc1ec\r\n\r\n### Describe the problem you're observing\r\nhttp://open-zfs.org/wiki/Features#nop-write states that\r\n\r\n> **nop-write**\r\n>\r\n> ZFS supports end-to-end checksumming of every data block. When a cryptographically secure checksum is being used OpenZFS will compare the checksums of incoming writes to checksum of the existing on-disk data and avoid issuing any write i/o for data that has not changed. This can help performance and snapshot space usage in situations were the same files are regularly overwritten with almost-identical data (e.g. regular full-backups of large random-access files). \r\n\r\nAccording to the link above and https://github.com/zfsonlinux/zfs/pull/1489 nop-write was merged Nov 2013, so this _should_ be in my installed version of ZFS (even when it's older version from Jun 2016).\r\n\r\nRewriting a file with identical data still inflates _written_ on the dataset and fully ends up in send streams.\r\n### Describe how to reproduce the problem\r\n\r\n```\r\nTESTFILE=\"path_to_test_file\"  #  avi backed by zfs filesystem on same pool used for testing\r\nDATASET=\"pool/rewritetest\"\r\nzfs create $DATASET -o checksum=sha256 -o atime=off -o xattr=sa\r\ncp \"$TESTFILE\" /$DATASET/file\r\nzfs snapshot $DATASET@write\r\nzfs send $DATASET@write | wc -c\r\n```\r\n> 197774600  # ok as being the initial send\r\n\r\n```\r\ncp \"$TESTFILE\" /$DATASET/file\r\nzfs snapshot $DATASET@rewrite_cp\r\nzfs send -i write $DATASET@rewrite_cp | wc -c\r\n```\r\n> 197738872  # possibly ok as cp might unlink the existing file before copying\r\n\r\n```\r\ndd if=\"$TESTFILE\" of=/$DATASET/file\r\n```\r\n> 385197+1 records in\r\n> 385197+1 records out\r\n> 197220966 bytes (197 MB) copied, 4,76443 s, 41,4 MB/s\r\n\r\n```\r\nzfs snapshot $DATASET@rewrite_dd\r\nzfs send -i rewrite_cp $DATASET@rewrite_dd | wc -c\r\n```\r\n> 197738872  # not ok as dd dosn't unlink but write directly\r\n\r\n```\r\nzfs get written $DATASET -r -t snapshot\r\n```\r\n> NAME                                  PROPERTY  VALUE    SOURCE\r\n> pool/rewritetest@write       written   188M     -\r\n> pool/rewritetest@rewrite_cp  written   188M     -\r\n> pool/rewritetest@rewrite_dd  written   188M     -\r\n\r\nClearly rewrites are not turned into nop but the data is committed to disk and works its way into the snapshot streams.\r\n\r\nI expected checksum=sha256 to fulfill the ```When a cryptographically secure checksum is being used``` part, so the expected result was that at least (unsure if cp unlinks the original before the write on rewrite but quite confident that dd dosn't) $DATASET@rewrite_dd having a written and send stream size of only a few KB.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nNo warning/errors/backtraces.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6534/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dsw0350": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6533", "title": "PANIC at zil.c:565:zil_create()", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | \r\nDistribution Version    | \r\nLinux Kernel                 | linux-4.4.66\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.8-1\r\nSPL Version                  | 0.6.5.8-1\r\n\r\n### Describe the problem you're observing\r\nWhen I execute the command \"zpool import\", the system is panic. I got the following message.\r\n```\r\n[13928.096574] VERIFY(zh->zh_claim_txg == 0) failed\r\n[13928.097080] PANIC at zil.c:565:zil_create()\r\n[13928.097531] Showing stack for process 14186\r\n[13928.097534] CPU: 1 PID: 14186 Comm: dd Tainted: P           O    4.4.66-v64 #1\r\n[13928.097534] Hardware name: Supermicro X9SCL/X9SCM/X9SCL/X9SCM, BIOS 2.10 01/09/2014\r\n[13928.097536]  0000000000000000 0000000000000235 ffffffff812ad192 0000000000000000\r\n[13928.097538]  ffffffff8165b340 0000000000000235 ffff88011d2bfa98 ffffffffa048abd4\r\n[13928.097539]  ffffffffa0336107 ffff88012f465260 ffffffffa04e7528 7a28594649524556\r\n[13928.097541] Call Trace:\r\n[13928.097547]  [<ffffffff812ad192>] ? dump_stack+0x4a/0x68\r\n[13928.097552]  [<ffffffffa0336107>] ? spl_panic+0xb7/0xf0 [spl]\r\n[13928.097555]  [<ffffffff810cd08c>] ? find_get_pages_tag+0xdc/0x180\r\n[13928.097557]  [<ffffffff8110c300>] ? slab_destroy+0x20/0x70\r\n[13928.097558]  [<ffffffff810cd08c>] ? find_get_pages_tag+0xdc/0x180\r\n[13928.097580]  [<ffffffffa03e7dd0>] ? rrn_find+0x10/0x40 [zfs]\r\n[13928.097592]  [<ffffffffa0407542>] ? txg_wait_synced+0x122/0x1c0 [zfs]\r\n[13928.097602]  [<ffffffffa04597fb>] ? zil_create+0x36b/0x430 [zfs]\r\n[13928.097612]  [<ffffffffa0457cb3>] ? zil_async_to_sync+0xb3/0x230 [zfs]\r\n[13928.097621]  [<ffffffffa045bb0b>] ? zil_commit+0x97b/0xb80 [zfs]\r\n[13928.097623]  [<ffffffff810ce039>] ? filemap_fdatawait_range+0x9/0x50\r\n[13928.097625]  [<ffffffff811393e8>] ? inode_io_list_del_locked+0x48/0x60\r\n[13928.097634]  [<ffffffffa045c124>] ? zil_close+0x14/0x320 [zfs]\r\n[13928.097644]  [<ffffffffa0473f95>] ? zvol_last_close+0x15/0xb0 [zfs]\r\n[13928.097654]  [<ffffffffa04740cf>] ? zvol_release+0x9f/0x100 [zfs]\r\n[13928.097655]  [<ffffffff811464ac>] ? __blkdev_put+0x1bc/0x1f0\r\n[13928.097657]  [<ffffffff8114662f>] ? blkdev_close+0x1f/0x30\r\n[13928.097658]  [<ffffffff81115055>] ? __fput+0xa5/0x1f0\r\n[13928.097661]  [<ffffffff81063bd1>] ? task_work_run+0x51/0x80\r\n[13928.097663]  [<ffffffff810011b3>] ? exit_to_usermode_loop+0x93/0xa0\r\n[13928.097664]  [<ffffffff811147c0>] ? vfs_write+0x140/0x180\r\n[13928.097665]  [<ffffffff810014be>] ? syscall_return_slowpath+0x5e/0x70\r\n[13928.097668]  [<ffffffff815e8ba7>] ? int_ret_from_sys_call+0x25/0x8f\r\n```\r\n### Describe how to reproduce the problem\r\nI find zpool import and zpool export commands are not stable when I write zvol.  So I merge a patch(OpenZFS 3821 - Race in rollback, zil close, and zil flush,commit:55922e73b4294fc6c3014be27b61201b7962088c).\r\n\r\nAfter that ,I write a script to test the zpool import and zpool export command.\r\n\r\n```\r\n#!/bin/bash\r\nindex=1\r\nwhile :\r\ndo\r\n    zpool import -d /dev/disk/by-id/ POOL00\r\n    while [ ! -b /dev/zvol/POOL00/volume00 ]\r\n    do\r\n        sleep 1\r\n    done\r\n    dd if=/dev/zero of=/dev/zvol/POOL00/volume00 bs=1M count=100\r\n    zpool export POOL00\r\n\techo $index\r\n    sleep 5\r\n    ((index=index+1))\r\ndone\r\n```\r\nSystem is panic when the value of index equal to 1527.\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6533/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "brendangregg": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6531", "title": "ARC consuming 38% CPU for no reason", "body": "This is a production system that has ZFS installed, but is not using ZFS. No pools, datasets, or ARC buffers.\r\n\r\nIt has suffered a performance loss as ZFS was consuming 38% CPU system-wide. This is a 4 CPU system. Here is the bottom of a CPU flame graph (open in a new tab to zoom):\r\n\r\n![screen shot 2017-08-18 at 3 07 50 pm](https://user-images.githubusercontent.com/1101211/29479462-3b07cc8e-8427-11e7-853b-67a0af07e216.png)\r\n\r\nZooming into the arc_reclaim thread:\r\n\r\n![screen shot 2017-08-18 at 3 08 08 pm](https://user-images.githubusercontent.com/1101211/29479491-5d99b1d6-8427-11e7-8901-76b218454f1f.png)\r\n\r\nThis multilist work is new to me, but... do we really need to be selecting eviction lists using an entropy-based random function? Could this just be round robin?\r\n\r\nThere's also the shrink_zone CPU consumer on the right, which I'd guess is related to the ARC holding onto locks while in arc_adjust().\r\n\r\nThis system was almost running at memory capacity (about 99%), so I would think it is frequently entering arc_reclaim() and shrink_zone(). The workaround has been to reduce the Java heap size by a tiny bit.\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | Xenial\r\nLinux Kernel                 | 4.4.0-87-generic\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.6-0ubuntu16\r\nSPL Version                  | 0.6.5.6-0ubuntu4", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6531/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nedbass": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6527", "title": " (!copied) is equivalent to (hdr->b_l1hdr.b_freeze_cksum == NULL)", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | TOSS (RHEL 7.3 derivative)\r\nDistribution Version    | toss-release-3.1-4.1rc2\r\nLinux Kernel                 | 3.10.0-514.26.2.1chaos.ch6_1.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | v0.7.0-31_gc8f9061\r\nSPL Version                  | v0.7.0-12_g9df9692\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nSystem panics when generating a send stream.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nI hit the problem running the following script from #6524 under a debug build. The command that panics is `$ZFS send $DS@a > senda`\r\n\r\n```\r\n#!/bin/bash\r\n\r\nZFS=./cmd/zfs/zfs\r\nZPOOL=./cmd/zpool/zpool\r\nPOOL=tank\r\nDS=$POOL/fish\r\nROOTMTPT=/mnt/$POOL\r\nMTPT=/mnt/$DS\r\nNEWDS=$POOL/new\r\nRFILE=./rfile\r\n\r\ndd if=/dev/urandom of=$RFILE bs=262144 count=1\r\n\r\necho 1 > /sys/module/zfs/parameters/zfs_dbgmsg_enable\r\necho 512 > /sys/module/zfs/parameters/zfs_flags\r\n\r\ntruncate -s 10g /tmp/a\r\n$ZPOOL create -f -O mountpoint=$ROOTMTPT $POOL /tmp/a\r\n\r\nmkfiles () {\r\n        for ((i=0;i<5000;i++)) ; do\r\n                dd if=$RFILE of=$MTPT/file-$i bs=262144 > /dev/null\r\n        done\r\n}\r\n\r\n$ZFS create -o dnsize=auto $DS\r\nmkfiles\r\numount $MTPT\r\n$ZFS snap $DS@a\r\n\r\n$ZFS set dnsize=legacy $DS\r\n$ZFS mount $DS\r\nrm $MTPT/*\r\nmkfiles\r\numount $MTPT\r\n$ZFS snap $DS@b\r\n\r\n$ZFS send $DS@a > senda\r\n$ZFS send -i $DS@a $DS@b > sendb\r\n$ZFS recv $NEWDS < senda\r\n$ZFS recv $NEWDS < sendb\r\n```\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n\r\n```\r\n\r\n[  367.132385] SPL: Loaded module v0.7.0-12_g9df9692\r\n[  367.178933] SPLAT: Loaded module v0.7.0-12_g9df9692\r\n[  369.264850] ZFS: Loaded module v0.7.0-31_gc8f9061 (DEBUG mode), ZFS pool version 5000, ZFS filesystem version 5\r\n[  404.202115] (!copied) is equivalent to (hdr->b_l1hdr.b_freeze_cksum == NULL)\r\n[  404.209827] PANIC at arc.c:1811:arc_buf_try_copy_decompressed_data()\r\n[  404.218359] Showing stack for process 173266\r\n[  404.223695] CPU: 13 PID: 173266 Comm: lt-zfs Tainted: P           OE  ------------   3.10.0-514.26.2.1chaos.ch6_1.x86_64 #1\r\n[  404.236714] Hardware name: Intel Corporation S2600WTTR/S2600WTTR, BIOS SE5C610.86B.01.01.0016.033120161139 03/31/2016\r\n[  404.236717]  ffffffffa0fc58bc 00000000161d01d4 ffff881f2f76b998 ffffffff8169d4bc\r\n[  404.236719]  ffff881f2f76b9a8 ffffffffa02c32d4 ffff881f2f76bb30 ffffffffa02c33a9\r\n[  404.236720]  ffff881fedb03b80 ffff880f00000020 ffff881f2f76bb40 ffff881f2f76bae0\r\n[  404.236721] Call Trace:\r\n[  404.236738]  [<ffffffff8169d4bc>] dump_stack+0x19/0x1b\r\n[  404.236748]  [<ffffffffa02c32d4>] spl_dumpstack+0x44/0x50 [spl]\r\n[  404.236751]  [<ffffffffa02c33a9>] spl_panic+0xc9/0x110 [spl]\r\n[  404.236756]  [<ffffffff816b0171>] ? ftrace_call+0x5/0x2f\r\n[  404.236757]  [<ffffffff816b0171>] ? ftrace_call+0x5/0x2f\r\n[  404.236760]  [<ffffffffa02bf359>] ? spl_kmem_cache_alloc+0x99/0x150 [spl]\r\n[  404.236808]  [<ffffffffa0e09d75>] arc_buf_fill+0x575/0x1750 [zfs]\r\n[  404.236810]  [<ffffffff816b0171>] ? ftrace_call+0x5/0x2f\r\n[  404.236814]  [<ffffffffa02bf359>] ? spl_kmem_cache_alloc+0x99/0x150 [spl]\r\n[  404.236817]  [<ffffffffa02bf359>] ? spl_kmem_cache_alloc+0x99/0x150 [spl]\r\n[  404.236828]  [<ffffffffa0e0b283>] arc_buf_alloc_impl+0x313/0x6e0 [zfs]\r\n[  404.236838]  [<ffffffffa0e0d15b>] arc_read+0xefb/0x1990 [zfs]\r\n[  404.236840]  [<ffffffff816b0171>] ? ftrace_call+0x5/0x2f\r\n[  404.236851]  [<ffffffffa0e149b0>] ? arc_buf_destroy+0x330/0x330 [zfs]\r\n[  404.236867]  [<ffffffffa0e39acf>] dmu_objset_open_impl+0x1bf/0xb30 [zfs]\r\n[  404.236881]  [<ffffffffa0e3a594>] dmu_objset_from_ds+0x154/0x290 [zfs]\r\n[  404.236909]  [<ffffffffa0eedc3c>] zfs_ioc_snapshot_list_next+0x17c/0x200 [zfs]\r\n[  404.236930]  [<ffffffffa0ef7425>] zfsdev_ioctl+0x655/0x790 [zfs]\r\n[  404.236961]  [<ffffffffa0ef6dd0>] ? pool_status_check+0x140/0x140 [zfs]\r\n[  404.236966]  [<ffffffff8121bba5>] do_vfs_ioctl+0x305/0x510\r\n[  404.236969]  [<ffffffff8121be51>] SyS_ioctl+0xa1/0xc0\r\n[  404.236970]  [<ffffffff816ae449>] system_call_fastpath+0x16/0x1b\r\n```\r\n```\r\n[root@jet22:~]# cat /proc/`pidof lt-zfs`/stack\r\n[<ffffffffa02c33d5>] spl_panic+0xf5/0x110 [spl]\r\n[<ffffffffa0e09d75>] arc_buf_fill+0x575/0x1750 [zfs]\r\n[<ffffffffa0e0b283>] arc_buf_alloc_impl+0x313/0x6e0 [zfs]\r\n[<ffffffffa0e0d15b>] arc_read+0xefb/0x1990 [zfs]\r\n[<ffffffffa0e39acf>] dmu_objset_open_impl+0x1bf/0xb30 [zfs]\r\n[<ffffffffa0e3a594>] dmu_objset_from_ds+0x154/0x290 [zfs]\r\n[<ffffffffa0eedc3c>] zfs_ioc_snapshot_list_next+0x17c/0x200 [zfs]\r\n[<ffffffffa0ef7425>] zfsdev_ioctl+0x655/0x790 [zfs]\r\n[<ffffffff8121bba5>] do_vfs_ioctl+0x305/0x510\r\n[<ffffffff8121be51>] SyS_ioctl+0xa1/0xc0\r\n[<ffffffff816ae449>] system_call_fastpath+0x16/0x1b\r\n[<ffffffffffffffff>] 0xffffffffffffffff\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6527/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/39f56627ae988d09b4e3803c01c22b2026b2310e", "message": "receive_freeobjects() skips freeing some objects\n\nWhen receiving a FREEOBJECTS record, receive_freeobjects()\r\nincorrectly skips a freed object in some cases. Specifically, this\r\nhappens when the first object in the range to be freed doesn't exist,\r\nbut the second object does. This leaves an object allocated on disk\r\non the receiving side which is unallocated on the sending side, which\r\nmay cause receiving subsequent incremental streams to fail.\r\n\r\nThe bug was caused by an incorrect increment of the object index\r\nvariable when current object being freed doesn't exist.  The\r\nincrement is incorrect because incrementing the object index is\r\nhandled by a call to dmu_object_next() in the increment portion of\r\nthe for loop statement.\r\n\r\nAdd test case that exposes this bug.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Ned Bass <bass6@llnl.gov>\r\nCloses #6694 \r\nCloses #6695"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/65dcb0f67a4d72ee4e1e534703db5caacf1ec85f", "message": "Handle new dnode size in incremental backup stream\n\nWhen receiving an incremental backup stream, call\r\ndmu_object_reclaim_dnsize() if an object's dnode size differs between\r\nthe incremental source and target. Otherwise it may appear that a\r\ndnode which has shrunk is still occupying slots which are in fact\r\nfree. This will cause a failure to receive new objects that should\r\noccupy the now-free slots.\r\n\r\nAdd a test case to verify that an incremental stream containing\r\nobjects with changed dnode sizes can be received without error. This\r\ntest case fails without this change.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Ned Bass <bass6@llnl.gov>\r\nCloses #6366 \r\nCloses #6576"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/6a8ee4f71dd957096922a50e318fd5350d2e9061", "message": "Add debug log entries for failed receive records\n\nLog contents of a receive record if an error occurs while writing\r\nit out to the pool. This may help determine the cause when backup\r\nstreams are rejected as invalid.\r\n\r\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Ned Bass <bass6@llnl.gov>\r\nCloses #6465"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/ecb2b7dc7f8f7273c215fb30c984bde08e75b852", "message": "Use SET_ERROR for constant non-zero return codes\n\nUpdate many return and assignment statements to follow the convention\r\nof using the SET_ERROR macro when returning a hard-coded non-zero\r\nvalue from a function. This aids debugging by recording the error\r\ncodes in the debug log.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\r\nSigned-off-by: Ned Bass <bass6@llnl.gov>\r\nCloses #6441"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/8740cf4a2f5f7ff7fb6c214e0baf06356b2870b8", "message": "Add line info and SET_ERROR() to ZFS debug log\n\nRedefine the SET_ERROR macro in terms of __dprintf() so the error\r\nreturn codes get logged as both tracepoint events (if tracepoints are\r\nenabled) and as ZFS debug log entries.  This also allows us to use\r\nthe same definition of SET_ERROR() in kernel and user space.\r\n\r\nDefine a new debug flag ZFS_DEBUG_SET_ERROR=512 that may be bitwise\r\nor'd into zfs_flags. Setting this flag enables both dprintf() and\r\nSET_ERROR() messages in the debug log. That is, setting\r\nZFS_DEBUG_SET_ERROR and ZFS_DEBUG_DPRINTF|ZFS_DEBUG_SET_ERROR are\r\nequivalent (this was done for sake of simplicity). Leaving\r\nZFS_DEBUG_SET_ERROR unset suppresses the SET_ERROR() messages which\r\nhelps avoid cluttering up the logs.\r\n\r\nTo enable SET_ERROR() logging, run:\r\n\r\n  echo 1 >   /sys/module/zfs/parameters/zfs_dbgmsg_enable\r\n  echo 512 > /sys/module/zfs/parameters/zfs_flags\r\n\r\nRemove the zfs_set_error_class tracepoints event class since\r\nSET_ERROR() now uses __dprintf(). This sacrifices a bit of\r\ngranularity when selecting individual tracepoint events to enable but\r\nit makes the code simpler.\r\n\r\nInclude file, function, and line number information in debug log\r\nentries.  The information is now added to the message buffer in\r\n__dprintf() and as a result the zfs_dprintf_class tracepoints event\r\nclass was changed from a 4 parameter interface to a single parameter.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Ned Bass <bass6@llnl.gov>\r\nCloses #6400"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/73aac4aa410121ddcc08bd7fd15d987caf101584", "message": "Some additional send stream validity checking\n\nCheck in the DMU whether an object record in a send stream being\r\nreceived contains an unsupported dnode slot count, and return an\r\nerror if it does. Failure to catch an unsupported dnode slot count\r\nwould result in a panic when the SPA attempts to increment the\r\nreference count for the large_dnode feature and the pool has the\r\nfeature disabled. This is not normally an issue for a well-formed\r\nsend stream which would have the DMU_BACKUP_FEATURE_LARGE_DNODE flag\r\nset if it contains large dnodes, so it will be rejected as\r\nunsupported if the required feature is disabled. This change adds a\r\nmissing object record field validation.\r\n\r\nAdd missing stream feature flag checks in\r\ndmu_recv_resume_begin_check().\r\n\r\nConsolidate repetitive comment blocks in dmu_recv_begin_check().\r\n\r\nUpdate zstreamdump to print the dnode slot count (dn_slots) for an\r\nobject record when running in verbose mode.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\r\nSigned-off-by: Ned Bass <bass6@llnl.gov>\r\nCloses #6396"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/7a8ed6b8b7bb5dde7b5713c75f99aee9bfbc12f2", "message": "Minor fixes in zpool iostat -c documentation (#6370)\n\n- Use nested [] notation to denote optional script list elements\r\n- Fix space before comma after smarctl(8)\r\n- Fix typo and formatting error in reference to -v option\r\n- Fix spelling errors\r\n\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Tony Hutter <hutter2@llnl.gov>\r\nSigned-off-by: Ned Bass <bass6@llnl.gov>\r\nCloses #6370"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/333338", "body": "Mixing tabs and spaces here\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/333338/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/333382", "body": "Done.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/333382/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/334541", "body": "Darn, missed this misaligned fi!  Oh well, I'll commit a new fix\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/334541/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/409893", "body": "Yay for killing abominable code!  Not sure if dbuf_hold_impl() is in the same call path, but \nfc5bb51f08a6c91ff9ad3559d0266eeeab0b1f61 employs the same hack to reduce its stack.\nYou may want to check if it can now be safely reverted as well.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/409893/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/413647", "body": "Opened Issue #263 to track this.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/413647/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2504162", "body": "I suppose there is a small race here.  i.e. having NULL `d_op` but `DCACHE_OP_REVALIDATE` set could lead to a NULL deref. I thought about this in context of #1241 but it's probably a long-shot.  Plus I'm not sure if he's running a recent enough build to have this patch.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2504162/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "thewacokid": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6518", "title": "ZFS 0.7.1-1 overreads with larger than 1 MB record size (up to 16x read amplification)", "body": "Type                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | TOSS (RHEL 6.7 derivative)\r\nDistribution Version    | 2.4-9 \r\nLinux Kernel                 | 2.6.32-573.26.1.1chaos.ch5.4.x86_64 #1 SMP Wed May 4 15:27:57 PDT 2016 x86_64 x86_64 x86_64 GNU/Linux\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.1-1\r\nSPL Version                  | 0.7.1-1\r\n\r\n### Describe the problem you're observing\r\nIt appears that setting a large recordsize causes read amplification based on [new block size / 1 MB].  This is a large pool with ~60 TB of space free out of 145 TB total (17+3 array of 8 TB drives).  Fresh ZFS filesystem, however the pool is 17% fragmented.  I'm going to recreate on a totally fresh pool to ensure it's not fragmentation related, though this still feels like a bug even if it is.  Write speeds are not affected, which is why I would expect fragmentation doesn't come into play here.\r\n\r\n\r\n### Describe how to reproduce the problem\r\nzfs set recordsize=1M\r\ndd if=/dev/zero of=testfile1 bs=1M count=10000\r\nzfs set recordsize=4M\r\ndd if=/dev/zero of=testfile4 bs=1M count=10000\r\nzfs set recordsize=16M\r\ndd if=/dev/zero of=testfile16 bs=1M count=10000\r\n\r\n\r\nConfirm record size:\r\n```\r\n[root@stb-dsu-sn002 test_large_blocks]# stat testfile1 | grep Blocks\r\n  Size: 10485760000\tBlocks: 19428575   IO Block: 1048576 regular file\r\n[root@stb-dsu-sn002 test_large_blocks]# stat testfile4 | grep Blocks\r\n  Size: 10485760000\tBlocks: 19299995   IO Block: 4194304 regular file\r\n[root@stb-dsu-sn002 test_large_blocks]# stat testfile16 | grep Blocks\r\n  Size: 10485760000\tBlocks: 19251861   IO Block: 16777216 regular file\r\n```\r\n\r\n\r\nRead them back with 1 MB blocks:\r\n```\r\n[root@stb-dsu-sn002 test_large_blocks]# dd if=testfile1 of=/dev/null bs=1M\r\n10000+0 records in\r\n10000+0 records out\r\n10485760000 bytes (10 GB) copied, 16.9706 s, 618 MB/s\r\n[root@stb-dsu-sn002 test_large_blocks]# dd if=testfile4 of=/dev/null bs=1M\r\n10000+0 records in\r\n10000+0 records out\r\n10485760000 bytes (10 GB) copied, 58.9244 s, 178 MB/s\r\n[root@stb-dsu-sn002 test_large_blocks]# dd if=testfile16 of=/dev/null bs=1M\r\n10000+0 records in\r\n10000+0 records out\r\n10485760000 bytes (10 GB) copied, 286.38 s, 36.6 MB/s\r\n```\r\n\r\n\r\nCheck to see if larger read blocks make a difference:\r\ndd if=testfile4 of=/dev/null bs=4M \r\ndd if=testfile16 of=/dev/null bs=16M\r\n(very similar results here, read block size doesn't improve things)\r\n\r\nWatching the disk statistics shows that they are indeed serving data at the same rate, they're just being hammered for far more data than is being passed back to the application.  Perhaps a bug in the sequential read logic that keeps such data out of the ARC?\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nNone noted.\r\n\r\nzfs.conf:\r\n```\r\noptions zfs zfs_dirty_data_sync=4294967296\r\noptions zfs zfs_arc_min=4294967296\r\noptions zfs zfs_txg_timeout=120\r\noptions zfs zfs_top_maxinflight=128\r\noptions zfs zfs_vdev_aggregation_limit=16777216\r\noptions zfs zfs_max_recordsize=16777216\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6518/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "AXDOOMER": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6508", "title": "Kernel Oops - list_del corruption caused a null pointer dereference; RIP is at dbuf_write_physdone+0x26/0x40", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian GNU/Linux\r\nDistribution Version    | 8.9 (jessie)\r\nLinux Kernel                 | Debian 4.8.15-2~bpo8+2\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.6.5.9-2~bpo8+1\r\nSPL Version                  | 0.6.5.9-1~bpo8+1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nMy system crashed.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nI don't think this can be reproduced manually. \r\n\r\nFrom the call trace, I can tell that the error occurred when `zio_remove_child` was called, then `list_del` must have detected a data corruption. \r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n```\r\nAug 14 02:29:52 jupiter kernel: [8987905.287783] ------------[ cut here ]------------\r\nAug 14 02:29:52 jupiter kernel: [8987905.287789] WARNING: CPU: 3 PID: 1801 at /build/linux-aPrr8L/linux-4.8.15/lib/list_debug.c:62 list_del+0x9/0x20\r\nAug 14 02:29:52 jupiter kernel: [8987905.287790] list_del corruption. next->prev should be ffff8f520c8277d0, but was ffff8f520c8278c0\r\nAug 14 02:29:52 jupiter kernel: [8987905.287791] Modules linked in: nf_conntrack_netlink nls_utf8 fuse btrfs xor raid6_pq ufs qnx4 hfsplus hfs minix ntfs vfat msdos fat jfs xfs libcrc32c crc32c_generic nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc nfnetlink_queue nfnetlink_log nfnetlink bluetooth rfkill hid_generic usbhid hid xt_conntrack xfrm_user xfrm_algo xt_addrtype iptable_filter veth xt_nat xt_tcpudp ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack ip_tables x_tables bridge stp llc intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate zfs(POE) zunicode(POE) zcommon(POE) znvpair(POE) intel_uncore iTCO_wdt iTCO_vendor_support spl(OE) evdev zavl(POE) intel_rapl_perf serio_raw cdc_ether ast pcspkr usbnet ttm drm_kms_helper r8152 mii drm i2c_i801 i2c_smbus lpc_ich mfd_core ipmi_si ie31200_edac mei_me ipmi_msghandler mei shpchp battery tpm_infineon edac_core video tpm_tis tpm_tis_core tpm button overlay autofs4 ext4 crc16 jbd2 fscrypto mbcache dm_mod raid1 md_mod sg sd_mod crc32c_intel ahci libahci aesni_intel mpt3sas libata aes_x86_64 xhci_pci glue_helper lrw xhci_hcd gf128mul ablk_helper ehci_pci cryptd raid_class ehci_hcd scsi_transport_sas igb scsi_mod psmouse usbcore i2c_algo_bit dca ptp pps_core usb_common thermal fan fjes\r\nAug 14 02:29:52 jupiter kernel: [8987905.287846] CPU: 3 PID: 1801 Comm: z_wr_int_3 Tainted: P           OE   4.8.0-0.bpo.2-amd64 #1 Debian 4.8.15-2~bpo8+2\r\nAug 14 02:29:52 jupiter kernel: [8987905.287847] Hardware name: ASUSTeK COMPUTER INC. P9D-M Series/P9D-M Series, BIOS 1204 07/14/2015\r\nAug 14 02:29:52 jupiter kernel: [8987905.287848]  0000000000000286 00000000efd0fbbb ffffffff8c922b05 ffff8f523e34bca8\r\nAug 14 02:29:52 jupiter kernel: [8987905.287850]  0000000000000000 ffffffff8c676a94 ffff8f520c8277d0 ffff8f523e34bd00\r\nAug 14 02:29:52 jupiter kernel: [8987905.287851]  ffff8f520c8277b0 ffff8f4c62251528 ffff8f4c622510d8 ffff8f4c62250db0\r\nAug 14 02:29:52 jupiter kernel: [8987905.287853] Call Trace:\r\nAug 14 02:29:52 jupiter kernel: [8987905.287856]  [<ffffffff8c922b05>] ? dump_stack+0x5c/0x77\r\nAug 14 02:29:52 jupiter kernel: [8987905.287859]  [<ffffffff8c676a94>] ? __warn+0xc4/0xe0\r\nAug 14 02:29:52 jupiter kernel: [8987905.287860]  [<ffffffff8c676b0f>] ? warn_slowpath_fmt+0x5f/0x80\r\nAug 14 02:29:52 jupiter kernel: [8987905.287862]  [<ffffffff8c9409f9>] ? list_del+0x9/0x20\r\nAug 14 02:29:52 jupiter kernel: [8987905.287908]  [<ffffffffc0e79ae3>] ? zio_remove_child+0x43/0xc0 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.287937]  [<ffffffffc0e7d788>] ? zio_done+0x4a8/0xcc0 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.287939]  [<ffffffff8cbea83e>] ? mutex_lock+0xe/0x30\r\nAug 14 02:29:52 jupiter kernel: [8987905.287966]  [<ffffffffc0e7db1b>] ? zio_done+0x83b/0xcc0 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.287967]  [<ffffffff8cbea83e>] ? mutex_lock+0xe/0x30\r\nAug 14 02:29:52 jupiter kernel: [8987905.287994]  [<ffffffffc0e78f8e>] ? zio_execute+0x8e/0xf0 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.287999]  [<ffffffffc086f1cd>] ? taskq_thread+0x24d/0x440 [spl]\r\nAug 14 02:29:52 jupiter kernel: [8987905.288001]  [<ffffffff8c6a0800>] ? wake_up_q+0x60/0x60\r\nAug 14 02:29:52 jupiter kernel: [8987905.288004]  [<ffffffffc086ef80>] ? taskq_thread_spawn+0x50/0x50 [spl]\r\nAug 14 02:29:52 jupiter kernel: [8987905.288005]  [<ffffffff8c695d5f>] ? kthread+0xdf/0x100\r\nAug 14 02:29:52 jupiter kernel: [8987905.288007]  [<ffffffff8c62478b>] ? __switch_to+0x2bb/0x710\r\nAug 14 02:29:52 jupiter kernel: [8987905.288009]  [<ffffffff8cbecf2f>] ? ret_from_fork+0x1f/0x40\r\nAug 14 02:29:52 jupiter kernel: [8987905.288010]  [<ffffffff8c695c80>] ? kthread_park+0x50/0x50\r\nAug 14 02:29:52 jupiter kernel: [8987905.288011] ---[ end trace 85818f66365046e5 ]---\r\nAug 14 02:29:52 jupiter kernel: [8987905.298934] ------------[ cut here ]------------\r\nAug 14 02:29:52 jupiter kernel: [8987905.298940] WARNING: CPU: 2 PID: 1774 at /build/linux-aPrr8L/linux-4.8.15/lib/list_debug.c:59 list_del+0x9/0x20\r\nAug 14 02:29:52 jupiter kernel: [8987905.298941] list_del corruption. prev->next should be ffff8f520c8273e0, but was ffff8f520c8275c0\r\nAug 14 02:29:52 jupiter kernel: [8987905.298941] Modules linked in: nf_conntrack_netlink nls_utf8 fuse btrfs xor raid6_pq ufs qnx4 hfsplus hfs minix ntfs vfat msdos fat jfs xfs libcrc32c crc32c_generic nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc nfnetlink_queue nfnetlink_log nfnetlink bluetooth rfkill hid_generic usbhid hid xt_conntrack xfrm_user xfrm_algo xt_addrtype iptable_filter veth xt_nat xt_tcpudp ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack ip_tables x_tables bridge stp llc intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate zfs(POE) zunicode(POE) zcommon(POE) znvpair(POE) intel_uncore iTCO_wdt iTCO_vendor_support spl(OE) evdev zavl(POE) intel_rapl_perf serio_raw cdc_ether ast pcspkr usbnet ttm drm_kms_helper r8152 mii drm i2c_i801 i2c_smbus lpc_ich mfd_core ipmi_si ie31200_edac mei_me ipmi_msghandler mei shpchp battery tpm_infineon edac_core video tpm_tis tpm_tis_core tpm button overlay autofs4 ext4 crc16 jbd2 fscrypto mbcache dm_mod raid1 md_mod sg sd_mod crc32c_intel ahci libahci aesni_intel mpt3sas libata aes_x86_64 xhci_pci glue_helper lrw xhci_hcd gf128mul ablk_helper ehci_pci cryptd raid_class ehci_hcd scsi_transport_sas igb scsi_mod psmouse usbcore i2c_algo_bit dca ptp pps_core usb_common thermal fan fjes\r\nAug 14 02:29:52 jupiter kernel: [8987905.298996] CPU: 2 PID: 1774 Comm: z_wr_int_1 Tainted: P        W  OE   4.8.0-0.bpo.2-amd64 #1 Debian 4.8.15-2~bpo8+2\r\nAug 14 02:29:52 jupiter kernel: [8987905.298996] Hardware name: ASUSTeK COMPUTER INC. P9D-M Series/P9D-M Series, BIOS 1204 07/14/2015\r\nAug 14 02:29:52 jupiter kernel: [8987905.298997]  0000000000000286 00000000e6acae5a ffffffff8c922b05 ffff8f523f767ca8\r\nAug 14 02:29:52 jupiter kernel: [8987905.298999]  0000000000000000 ffffffff8c676a94 ffff8f520c8273e0 ffff8f523f767d00\r\nAug 14 02:29:52 jupiter kernel: [8987905.299000]  ffff8f520c8273c0 ffff8f4c62250838 ffff8f4c622510d8 ffff8f4c62250db0\r\nAug 14 02:29:52 jupiter kernel: [8987905.299001] Call Trace:\r\nAug 14 02:29:52 jupiter kernel: [8987905.299005]  [<ffffffff8c922b05>] ? dump_stack+0x5c/0x77\r\nAug 14 02:29:52 jupiter kernel: [8987905.299008]  [<ffffffff8c676a94>] ? __warn+0xc4/0xe0\r\nAug 14 02:29:52 jupiter kernel: [8987905.299009]  [<ffffffff8c676b0f>] ? warn_slowpath_fmt+0x5f/0x80\r\nAug 14 02:29:52 jupiter kernel: [8987905.299010]  [<ffffffff8c9409f9>] ? list_del+0x9/0x20\r\nAug 14 02:29:52 jupiter kernel: [8987905.299050]  [<ffffffffc0e79ae3>] ? zio_remove_child+0x43/0xc0 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.299076]  [<ffffffffc0e7d788>] ? zio_done+0x4a8/0xcc0 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.299078]  [<ffffffff8cbea83e>] ? mutex_lock+0xe/0x30\r\nAug 14 02:29:52 jupiter kernel: [8987905.299099]  [<ffffffffc0e7db1b>] ? zio_done+0x83b/0xcc0 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.299100]  [<ffffffff8cbea83e>] ? mutex_lock+0xe/0x30\r\nAug 14 02:29:52 jupiter kernel: [8987905.299121]  [<ffffffffc0e78f8e>] ? zio_execute+0x8e/0xf0 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.299127]  [<ffffffffc086f1cd>] ? taskq_thread+0x24d/0x440 [spl]\r\nAug 14 02:29:52 jupiter kernel: [8987905.299129]  [<ffffffff8c6a0800>] ? wake_up_q+0x60/0x60\r\nAug 14 02:29:52 jupiter kernel: [8987905.299132]  [<ffffffffc086ef80>] ? taskq_thread_spawn+0x50/0x50 [spl]\r\nAug 14 02:29:52 jupiter kernel: [8987905.299134]  [<ffffffff8c695d5f>] ? kthread+0xdf/0x100\r\nAug 14 02:29:52 jupiter kernel: [8987905.299136]  [<ffffffff8c62478b>] ? __switch_to+0x2bb/0x710\r\nAug 14 02:29:52 jupiter kernel: [8987905.299137]  [<ffffffff8cbecf2f>] ? ret_from_fork+0x1f/0x40\r\nAug 14 02:29:52 jupiter kernel: [8987905.299138]  [<ffffffff8c695c80>] ? kthread_park+0x50/0x50\r\nAug 14 02:29:52 jupiter kernel: [8987905.299152] ---[ end trace 85818f66365046e6 ]---\r\nAug 14 02:29:52 jupiter kernel: [8987905.503334] BUG: unable to handle kernel NULL pointer dereference at 0000000000000038\r\nAug 14 02:29:52 jupiter kernel: [8987905.503849] IP: [<ffffffffc0ddc556>] dbuf_write_physdone+0x26/0x40 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.504381] PGD 0 \r\nAug 14 02:29:52 jupiter kernel: [8987905.504837] Oops: 0000 [#1] SMP\r\nAug 14 02:29:52 jupiter kernel: [8987905.505263] Modules linked in: nf_conntrack_netlink nls_utf8 fuse btrfs xor raid6_pq ufs qnx4 hfsplus hfs minix ntfs vfat msdos fat jfs xfs libcrc32c crc32c_generic nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc nfnetlink_queue nfnetlink_log nfnetlink bluetooth rfkill hid_generic usbhid hid xt_conntrack xfrm_user xfrm_algo xt_addrtype iptable_filter veth xt_nat xt_tcpudp ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack ip_tables x_tables bridge stp llc intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate zfs(POE) zunicode(POE) zcommon(POE) znvpair(POE) intel_uncore iTCO_wdt iTCO_vendor_support spl(OE) evdev zavl(POE) intel_rapl_perf serio_raw cdc_ether ast pcspkr usbnet ttm drm_kms_helper r8152 mii drm i2c_i801 i2c_smbus lpc_ich mfd_core ipmi_si ie31200_edac mei_me ipmi_msghandler mei shpchp battery tpm_infineon edac_core video tpm_tis tpm_tis_core tpm button overlay autofs4 ext4 crc16 jbd2 fscrypto mbcache dm_mod raid1 md_mod sg sd_mod crc32c_intel ahci libahci aesni_intel mpt3sas libata aes_x86_64 xhci_pci glue_helper lrw xhci_hcd gf128mul ablk_helper ehci_pci cryptd raid_class ehci_hcd scsi_transport_sas igb scsi_mod psmouse usbcore i2c_algo_bit dca ptp pps_core usb_common thermal fan fjes\r\nAug 14 02:29:52 jupiter kernel: [8987905.511384] CPU: 1 PID: 1817 Comm: z_wr_int_5 Tainted: P        W  OE   4.8.0-0.bpo.2-amd64 #1 Debian 4.8.15-2~bpo8+2\r\nAug 14 02:29:52 jupiter kernel: [8987905.511944] Hardware name: ASUSTeK COMPUTER INC. P9D-M Series/P9D-M Series, BIOS 1204 07/14/2015\r\nAug 14 02:29:52 jupiter kernel: [8987905.512502] task: ffff8f523d8c6040 task.stack: ffff8f5249150000\r\nAug 14 02:29:52 jupiter kernel: [8987905.513056] RIP: 0010:[<ffffffffc0ddc556>]  [<ffffffffc0ddc556>] dbuf_write_physdone+0x26/0x40 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.513641] RSP: 0018:ffff8f5249153d78  EFLAGS: 00010246\r\nAug 14 02:29:52 jupiter kernel: [8987905.514201] RAX: 0000000000000000 RBX: ffff8f4c223675d0 RCX: 0000000000742422\r\nAug 14 02:29:52 jupiter kernel: [8987905.514799] RDX: 0000000000000000 RSI: ffff8f51629439a0 RDI: ffff8f523f43b000\r\nAug 14 02:29:52 jupiter kernel: [8987905.515399] RBP: ffff8f50e5d7a000 R08: ffff8f5234754df0 R09: 0000000000000000\r\nAug 14 02:29:52 jupiter kernel: [8987905.516085] R10: 0000000000000080 R11: 000000000000000a R12: 0000000000000100\r\nAug 14 02:29:52 jupiter kernel: [8987905.516688] R13: 00000000ffffffff R14: ffff8f4d11f01180 R15: ffff8f51463055d0\r\nAug 14 02:29:52 jupiter kernel: [8987905.517278] FS:  0000000000000000(0000) GS:ffff8f526fc40000(0000) knlGS:0000000000000000\r\nAug 14 02:29:52 jupiter kernel: [8987905.517949] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nAug 14 02:29:52 jupiter kernel: [8987905.518593] CR2: 0000000000000038 CR3: 00000006a9206000 CR4: 00000000001406e0\r\nAug 14 02:29:52 jupiter kernel: [8987905.519199] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\nAug 14 02:29:52 jupiter kernel: [8987905.519833] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\r\nAug 14 02:29:52 jupiter kernel: [8987905.520391] Stack:\r\nAug 14 02:29:52 jupiter kernel: [8987905.520977]  ffff8f51463055d0 ffff8f5245f54000 ffffffffc0e790f6 0000000000080000\r\nAug 14 02:29:52 jupiter kernel: [8987905.521552]  ffff8f51463058f8 ffff8f4d232de000 ffffffffc0e7db1b ffffffff8cbea83e\r\nAug 14 02:29:52 jupiter kernel: [8987905.522119]  ffff8f4d11f01180 ffff8f4d11f014a8 ffff8f4d11f014d0 ffff8f523d8c6040\r\nAug 14 02:29:52 jupiter kernel: [8987905.522682] Call Trace:\r\nAug 14 02:29:52 jupiter kernel: [8987905.523269]  [<ffffffffc0e790f6>] ? zio_vdev_io_assess+0x96/0x1c0 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.523861]  [<ffffffffc0e7db1b>] ? zio_done+0x83b/0xcc0 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.524439]  [<ffffffff8cbea83e>] ? mutex_lock+0xe/0x30\r\nAug 14 02:29:52 jupiter kernel: [8987905.525031]  [<ffffffffc0e78f8e>] ? zio_execute+0x8e/0xf0 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.525606]  [<ffffffffc086f1cd>] ? taskq_thread+0x24d/0x440 [spl]\r\nAug 14 02:29:52 jupiter kernel: [8987905.526176]  [<ffffffff8c6a0800>] ? wake_up_q+0x60/0x60\r\nAug 14 02:29:52 jupiter kernel: [8987905.526738]  [<ffffffffc086ef80>] ? taskq_thread_spawn+0x50/0x50 [spl]\r\nAug 14 02:29:52 jupiter kernel: [8987905.527338]  [<ffffffff8c695d5f>] ? kthread+0xdf/0x100\r\nAug 14 02:29:52 jupiter kernel: [8987905.527887]  [<ffffffff8c62478b>] ? __switch_to+0x2bb/0x710\r\nAug 14 02:29:52 jupiter kernel: [8987905.528429]  [<ffffffff8cbecf2f>] ? ret_from_fork+0x1f/0x40\r\nAug 14 02:29:52 jupiter kernel: [8987905.529010]  [<ffffffff8c695c80>] ? kthread_park+0x50/0x50\r\nAug 14 02:29:52 jupiter kernel: [8987905.529589] Code: 0f 1f 44 00 00 0f 1f 44 00 00 55 53 48 89 fb 48 8b 7a 20 48 89 d5 e8 9a e7 00 00 48 89 c7 48 8b 85 e8 00 00 00 31 d2 48 8b 4b 50 <8b> 40 38 48 f7 b3 f0 02 00 00 5b 5d 48 89 ca 48 63 f0 e9 13 f4 \r\nAug 14 02:29:52 jupiter kernel: [8987905.530818] RIP  [<ffffffffc0ddc556>] dbuf_write_physdone+0x26/0x40 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.531399]  RSP <ffff8f5249153d78>\r\nAug 14 02:29:52 jupiter kernel: [8987905.531952] CR2: 0000000000000038\r\nAug 14 02:29:52 jupiter kernel: [8987905.534435] ---[ end trace 85818f66365046e7 ]---\r\nAug 14 02:29:52 jupiter kernel: [8987905.534437] BUG: unable to handle kernel NULL pointer dereference at 0000000000000038\r\nAug 14 02:29:52 jupiter kernel: [8987905.534452] IP: [<ffffffffc0ddc556>] dbuf_write_physdone+0x26/0x40 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.534453] PGD 0 \r\nAug 14 02:29:52 jupiter kernel: [8987905.534453] Oops: 0000 [#2] SMP\r\nAug 14 02:29:52 jupiter kernel: [8987905.534469] Modules linked in: nf_conntrack_netlink nls_utf8 fuse btrfs xor raid6_pq ufs qnx4 hfsplus hfs minix ntfs vfat msdos fat jfs xfs libcrc32c crc32c_generic nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc nfnetlink_queue nfnetlink_log nfnetlink bluetooth rfkill hid_generic usbhid hid xt_conntrack xfrm_user xfrm_algo xt_addrtype iptable_filter veth xt_nat xt_tcpudp ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack ip_tables x_tables bridge stp llc intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate zfs(POE) zunicode(POE) zcommon(POE) znvpair(POE) intel_uncore iTCO_wdt iTCO_vendor_support spl(OE) evdev zavl(POE) intel_rapl_perf serio_raw cdc_ether ast pcspkr usbnet ttm drm_kms_helper r8152 mii drm i2c_i801 i2c_smbus lpc_ich mfd_core ipmi_si ie31200_edac mei_me ipmi_msghandler mei shpchp battery tpm_infineon edac_core video tpm_tis tpm_tis_core tpm button overlay autofs4 ext4 crc16 jbd2 fscrypto mbcache dm_mod raid1 md_mod sg sd_mod crc32c_intel ahci libahci aesni_intel mpt3sas libata aes_x86_64 xhci_pci glue_helper lrw xhci_hcd gf128mul ablk_helper ehci_pci cryptd raid_class ehci_hcd scsi_transport_sas igb scsi_mod psmouse usbcore i2c_algo_bit dca ptp pps_core usb_common thermal fan fjes\r\nAug 14 02:29:52 jupiter kernel: [8987905.534481] CPU: 4 PID: 1843 Comm: z_wr_int_7 Tainted: P      D W  OE   4.8.0-0.bpo.2-amd64 #1 Debian 4.8.15-2~bpo8+2\r\nAug 14 02:29:52 jupiter kernel: [8987905.534482] Hardware name: ASUSTeK COMPUTER INC. P9D-M Series/P9D-M Series, BIOS 1204 07/14/2015\r\nAug 14 02:29:52 jupiter kernel: [8987905.534482] task: ffff8f5249167080 task.stack: ffff8f523e360000\r\nAug 14 02:29:52 jupiter kernel: [8987905.534493] RIP: 0010:[<ffffffffc0ddc556>]  [<ffffffffc0ddc556>] dbuf_write_physdone+0x26/0x40 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.534494] RSP: 0018:ffff8f523e363d78  EFLAGS: 00010246\r\nAug 14 02:29:52 jupiter kernel: [8987905.534494] RAX: 0000000000000000 RBX: ffff8f4c223675d0 RCX: 0000000000742422\r\nAug 14 02:29:52 jupiter kernel: [8987905.534495] RDX: 0000000000000000 RSI: ffff8f51629439a0 RDI: ffff8f523f43b000\r\nAug 14 02:29:52 jupiter kernel: [8987905.534495] RBP: ffff8f50e5d7a000 R08: ffff8f5171293520 R09: ffff8f516e78eb40\r\nAug 14 02:29:52 jupiter kernel: [8987905.534495] R10: 009a02d24a38eaf7 R11: ffffffffffffffd7 R12: 0000000000000100\r\nAug 14 02:29:52 jupiter kernel: [8987905.534496] R13: 00000000ffffffff R14: ffff8f5231d46db0 R15: ffff8f5146304490\r\nAug 14 02:29:52 jupiter kernel: [8987905.534496] FS:  0000000000000000(0000) GS:ffff8f526fd00000(0000) knlGS:0000000000000000\r\nAug 14 02:29:52 jupiter kernel: [8987905.534497] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nAug 14 02:29:52 jupiter kernel: [8987905.534497] CR2: 0000000000000038 CR3: 00000006a9206000 CR4: 00000000001406e0\r\nAug 14 02:29:52 jupiter kernel: [8987905.534497] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\nAug 14 02:29:52 jupiter kernel: [8987905.534498] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\r\nAug 14 02:29:52 jupiter kernel: [8987905.534498] Stack:\r\nAug 14 02:29:52 jupiter kernel: [8987905.534499]  ffff8f5146304490 ffff8f5246bc8000 ffffffffc0e790f6 0000000000080000\r\nAug 14 02:29:52 jupiter kernel: [8987905.534500]  ffff8f51463047b8 ffff8f518e1b3a60 ffffffffc0e7db1b ffffffff8cbea83e\r\nAug 14 02:29:52 jupiter kernel: [8987905.534501]  ffff8f5231d46db0 ffff8f5231d470d8 ffff8f5231d47100 ffff8f5249167080\r\nAug 14 02:29:52 jupiter kernel: [8987905.534501] Call Trace:\r\nAug 14 02:29:52 jupiter kernel: [8987905.534522]  [<ffffffffc0e790f6>] ? zio_vdev_io_assess+0x96/0x1c0 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.534541]  [<ffffffffc0e7db1b>] ? zio_done+0x83b/0xcc0 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.534542]  [<ffffffff8cbea83e>] ? mutex_lock+0xe/0x30\r\nAug 14 02:29:52 jupiter kernel: [8987905.534562]  [<ffffffffc0e78f8e>] ? zio_execute+0x8e/0xf0 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.534565]  [<ffffffffc086f1cd>] ? taskq_thread+0x24d/0x440 [spl]\r\nAug 14 02:29:52 jupiter kernel: [8987905.534566]  [<ffffffff8c6a0800>] ? wake_up_q+0x60/0x60\r\nAug 14 02:29:52 jupiter kernel: [8987905.534568]  [<ffffffffc086ef80>] ? taskq_thread_spawn+0x50/0x50 [spl]\r\nAug 14 02:29:52 jupiter kernel: [8987905.534569]  [<ffffffff8c695d5f>] ? kthread+0xdf/0x100\r\nAug 14 02:29:52 jupiter kernel: [8987905.534571]  [<ffffffff8c62478b>] ? __switch_to+0x2bb/0x710\r\nAug 14 02:29:52 jupiter kernel: [8987905.534572]  [<ffffffff8cbecf2f>] ? ret_from_fork+0x1f/0x40\r\nAug 14 02:29:52 jupiter kernel: [8987905.534573]  [<ffffffff8c695c80>] ? kthread_park+0x50/0x50\r\nAug 14 02:29:52 jupiter kernel: [8987905.534581] Code: 0f 1f 44 00 00 0f 1f 44 00 00 55 53 48 89 fb 48 8b 7a 20 48 89 d5 e8 9a e7 00 00 48 89 c7 48 8b 85 e8 00 00 00 31 d2 48 8b 4b 50 <8b> 40 38 48 f7 b3 f0 02 00 00 5b 5d 48 89 ca 48 63 f0 e9 13 f4 \r\nAug 14 02:29:52 jupiter kernel: [8987905.534592] RIP  [<ffffffffc0ddc556>] dbuf_write_physdone+0x26/0x40 [zfs]\r\nAug 14 02:29:52 jupiter kernel: [8987905.534592]  RSP <ffff8f523e363d78>\r\nAug 14 02:29:52 jupiter kernel: [8987905.534593] CR2: 0000000000000038\r\nAug 14 02:29:52 jupiter kernel: [8987905.534594] ---[ end trace 85818f66365046e8 ]---\r\nAug 14 02:29:52 jupiter kernel: [8987905.613643] BUG: unable to handle kernel paging request at 0000000000019260\r\nAug 14 02:29:52 jupiter kernel: [8987905.613647] IP: [<ffffffff8c6c0ff4>] native_queued_spin_lock_slowpath+0x104/0x190\r\nAug 14 02:29:52 jupiter kernel: [8987905.613647] PGD 0 \r\nAug 14 02:29:52 jupiter kernel: [8987905.613648] Oops: 0002 [#3] SMP\r\nAug 14 02:29:52 jupiter kernel: [8987905.613678] Modules linked in: nf_conntrack_netlink nls_utf8 fuse btrfs xor raid6_pq ufs qnx4 hfsplus hfs minix ntfs vfat msdos fat jfs xfs libcrc32c crc32c_generic nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc nfnetlink_queue nfnetlink_log nfnetlink bluetooth rfkill hid_generic usbhid hid xt_conntrack xfrm_user xfrm_algo xt_addrtype iptable_filter veth xt_nat xt_tcpudp ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack ip_tables x_tables bridge stp llc intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate zfs(POE) zunicode(POE) zcommon(POE) znvpair(POE) intel_uncore iTCO_wdt iTCO_vendor_support spl(OE) evdev zavl(POE) intel_rapl_perf serio_raw cdc_ether ast pcspkr usbnet ttm drm_kms_helper r8152 mii drm i2c_i801 i2c_smbus lpc_ich mfd_core ipmi_si ie31200_edac mei_me ipmi_msghandler mei shpchp battery tpm_infineon edac_core video tpm_tis tpm_tis_core tpm button overlay autofs4 ext4 crc16 jbd2 fscrypto mbcache dm_mod raid1 md_mod sg sd_mod crc32c_intel ahci libahci aesni_intel mpt3sas libata aes_x86_64 xhci_pci glue_helper lrw xhci_hcd gf128mul ablk_helper ehci_pci cryptd raid_class ehci_hcd scsi_transport_sas igb scsi_mod psmouse usbcore i2c_algo_bit dca ptp pps_core usb_common thermal fan fjes\r\nAug 14 02:29:52 jupiter kernel: [8987905.613696] CPU: 4 PID: 1843 Comm: z_wr_int_7 Tainted: P      D W  OE   4.8.0-0.bpo.2-amd64 #1 Debian 4.8.15-2~bpo8+2\r\nAug 14 02:29:52 jupiter kernel: [8987905.613697] Hardware name: ASUSTeK COMPUTER INC. P9D-M Series/P9D-M Series, BIOS 1204 07/14/2015\r\nAug 14 02:29:52 jupiter kernel: [8987905.613698] task: ffff8f5249167080 task.stack: ffff8f523e360000\r\nAug 14 02:29:52 jupiter kernel: [8987905.613702] RIP: 0010:[<ffffffff8c6c0ff4>]  [<ffffffff8c6c0ff4>] native_queued_spin_lock_slowpath+0x104/0x190\r\nAug 14 02:29:52 jupiter kernel: [8987905.613703] RSP: 0018:ffff8f523e363e90  EFLAGS: 00010006\r\nAug 14 02:29:52 jupiter kernel: [8987905.613703] RAX: 0000000000001771 RBX: 0000000000000282 RCX: ffff8f526fd19240\r\nAug 14 02:29:52 jupiter kernel: [8987905.613705] RDX: 0000000000019260 RSI: 000000005dcaf62b RDI: ffff8f523e363f18\r\nAug 14 02:29:52 jupiter kernel: [8987905.613705] RBP: ffff8f523e363f10 R08: 0000000000140000 R09: 0000000000000000\r\nAug 14 02:29:52 jupiter kernel: [8987905.613706] R10: 0000000000000008 R11: 0000000000000000 R12: ffff8f523e363cc8\r\nAug 14 02:29:52 jupiter kernel: [8987905.613706] R13: 0000000000000000 R14: 0000000000000046 R15: 0000000000000001\r\nAug 14 02:29:52 jupiter kernel: [8987905.613707] FS:  0000000000000000(0000) GS:ffff8f526fd00000(0000) knlGS:0000000000000000\r\nAug 14 02:29:52 jupiter kernel: [8987905.613708] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nAug 14 02:29:52 jupiter kernel: [8987905.613709] CR2: 0000000000019260 CR3: 00000006a9206000 CR4: 00000000001406e0\r\nAug 14 02:29:52 jupiter kernel: [8987905.613709] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\nAug 14 02:29:52 jupiter kernel: [8987905.613710] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\r\nAug 14 02:29:52 jupiter kernel: [8987905.613710] Stack:\r\nAug 14 02:29:52 jupiter kernel: [8987905.613712]  ffffffff8cbecc42 ffff8f523e363f18 ffffffff8c6b9428 ffff8f52491677a8\r\nAug 14 02:29:52 jupiter kernel: [8987905.613714]  ffff8f5249167080 ffff8f523e363cc8 ffffffff8c673ee0 ffff8f5249167080\r\nAug 14 02:29:52 jupiter kernel: [8987905.613715]  0000000000000009 ffffffff8c67a573 ffffffff8c62478b 000000004b41b480\r\nAug 14 02:29:52 jupiter kernel: [8987905.613715] Call Trace:\r\nAug 14 02:29:52 jupiter kernel: [8987905.613717]  [<ffffffff8cbecc42>] ? _raw_spin_lock_irqsave+0x32/0x40\r\nAug 14 02:29:52 jupiter kernel: [8987905.613719]  [<ffffffff8c6b9428>] ? complete+0x18/0x40\r\nAug 14 02:29:52 jupiter kernel: [8987905.613723]  [<ffffffff8c673ee0>] ? mm_release+0xb0/0x130\r\nAug 14 02:29:52 jupiter kernel: [8987905.613725]  [<ffffffff8c67a573>] ? do_exit+0x153/0xb70\r\nAug 14 02:29:52 jupiter kernel: [8987905.613727]  [<ffffffff8c62478b>] ? __switch_to+0x2bb/0x710\r\nAug 14 02:29:52 jupiter kernel: [8987905.613729]  [<ffffffff8cbee4d7>] ? rewind_stack_do_exit+0x17/0x20\r\nAug 14 02:29:52 jupiter kernel: [8987905.613731]  [<ffffffff8c695c80>] ? kthread_park+0x50/0x50\r\nAug 14 02:29:52 jupiter kernel: [8987905.613766] Code: c1 e0 10 45 31 c9 85 c0 74 44 48 89 c2 c1 e8 12 48 c1 ea 0c 83 e8 01 83 e2 30 48 98 48 81 c2 40 92 01 00 48 03 14 c5 40 e6 11 8d <48> 89 0a 8b 41 08 85 c0 75 09 f3 90 8b 41 08 85 c0 74 f7 4c 8b \r\nAug 14 02:29:52 jupiter kernel: [8987905.613771] RIP  [<ffffffff8c6c0ff4>] native_queued_spin_lock_slowpath+0x104/0x190\r\nAug 14 02:29:52 jupiter kernel: [8987905.613771]  RSP <ffff8f523e363e90>\r\nAug 14 02:29:52 jupiter kernel: [8987905.613771] CR2: 0000000000019260\r\nAug 14 02:29:52 jupiter kernel: [8987905.613772] ---[ end trace 85818f66365046e9 ]---\r\nAug 14 02:29:52 jupiter kernel: [8987905.706282] Fixing recursive fault but reboot is needed!\r\nAug 14 02:29:53 jupiter kernel: [8987905.864630] general protection fault: 0000 [#4] SMP\r\nAug 14 02:29:53 jupiter kernel: [8987905.865243] Modules linked in: nf_conntrack_netlink nls_utf8 fuse btrfs xor raid6_pq ufs qnx4 hfsplus hfs minix ntfs vfat msdos fat jfs xfs libcrc32c crc32c_generic nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc nfnetlink_queue nfnetlink_log nfnetlink bluetooth rfkill hid_generic usbhid hid xt_conntrack xfrm_user xfrm_algo xt_addrtype iptable_filter veth xt_nat xt_tcpudp ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack ip_tables x_tables bridge stp llc intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate zfs(POE) zunicode(POE) zcommon(POE) znvpair(POE) intel_uncore iTCO_wdt iTCO_vendor_support spl(OE) evdev zavl(POE) intel_rapl_perf serio_raw cdc_ether ast pcspkr usbnet ttm drm_kms_helper r8152 mii drm i2c_i801 i2c_smbus lpc_ich mfd_core ipmi_si ie31200_edac mei_me ipmi_msghandler mei shpchp battery tpm_infineon edac_core video tpm_tis tpm_tis_core tpm button overlay autofs4 ext4 crc16 jbd2 fscrypto mbcache dm_mod raid1 md_mod sg sd_mod crc32c_intel ahci libahci aesni_intel mpt3sas libata aes_x86_64 xhci_pci glue_helper lrw xhci_hcd gf128mul ablk_helper ehci_pci cryptd raid_class ehci_hcd scsi_transport_sas igb scsi_mod psmouse usbcore i2c_algo_bit dca ptp pps_core usb_common thermal fan fjes\r\nAug 14 02:29:53 jupiter kernel: [8987905.871695] CPU: 1 PID: 1817 Comm: z_wr_int_5 Tainted: P      D W  OE   4.8.0-0.bpo.2-amd64 #1 Debian 4.8.15-2~bpo8+2\r\nAug 14 02:29:53 jupiter kernel: [8987905.872263] Hardware name: ASUSTeK COMPUTER INC. P9D-M Series/P9D-M Series, BIOS 1204 07/14/2015\r\nAug 14 02:29:53 jupiter kernel: [8987905.872826] task: ffff8f523d8c6040 task.stack: ffff8f5249150000\r\nAug 14 02:29:53 jupiter kernel: [8987905.873381] RIP: 0010:[<ffffffff8c6c0ff4>]  [<ffffffff8c6c0ff4>] native_queued_spin_lock_slowpath+0x104/0x190\r\nAug 14 02:29:53 jupiter kernel: [8987905.873951] RSP: 0018:ffff8f5249153e90  EFLAGS: 00010002\r\nAug 14 02:29:53 jupiter kernel: [8987905.874506] RAX: 0000000000003c64 RBX: 0000000000000282 RCX: ffff8f526fc59240\r\nAug 14 02:29:53 jupiter kernel: [8987905.875060] RDX: 48c7388100019270 RSI: 00000000f1971de8 RDI: ffff8f5249153f18\r\nAug 14 02:29:53 jupiter kernel: [8987905.875634] RBP: ffff8f5249153f10 R08: 0000000000080000 R09: 0000000000000000\r\nAug 14 02:29:53 jupiter kernel: [8987905.876189] R10: 0000000000000008 R11: 0000000000000000 R12: ffff8f5249153cc8\r\nAug 14 02:29:53 jupiter kernel: [8987905.876743] R13: 0000000000000000 R14: 0000000000000046 R15: 0000000000000001\r\nAug 14 02:29:53 jupiter kernel: [8987905.877296] FS:  0000000000000000(0000) GS:ffff8f526fc40000(0000) knlGS:0000000000000000\r\nAug 14 02:29:53 jupiter kernel: [8987905.877855] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nAug 14 02:29:53 jupiter kernel: [8987905.878411] CR2: 0000000000000038 CR3: 00000006a9206000 CR4: 00000000001406e0\r\nAug 14 02:29:53 jupiter kernel: [8987905.878970] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\nAug 14 02:29:53 jupiter kernel: [8987905.879562] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\r\nAug 14 02:29:53 jupiter kernel: [8987905.880114] Stack:\r\nAug 14 02:29:53 jupiter kernel: [8987905.880666]  ffffffff8cbecc42 ffff8f5249153f18 ffffffff8c6b9428 ffff8f523d8c6768\r\nAug 14 02:29:53 jupiter kernel: [8987905.881232]  ffff8f523d8c6040 ffff8f5249153cc8 ffffffff8c673ee0 ffff8f523d8c6040\r\nAug 14 02:29:53 jupiter kernel: [8987905.881804]  0000000000000009 ffffffff8c67a573 ffffffff8c62478b 0000000049352200\r\nAug 14 02:29:53 jupiter kernel: [8987905.882369] Call Trace:\r\nAug 14 02:29:53 jupiter kernel: [8987905.882926]  [<ffffffff8cbecc42>] ? _raw_spin_lock_irqsave+0x32/0x40\r\nAug 14 02:29:53 jupiter kernel: [8987905.883514]  [<ffffffff8c6b9428>] ? complete+0x18/0x40\r\nAug 14 02:29:53 jupiter kernel: [8987905.884072]  [<ffffffff8c673ee0>] ? mm_release+0xb0/0x130\r\nAug 14 02:29:53 jupiter kernel: [8987905.884627]  [<ffffffff8c67a573>] ? do_exit+0x153/0xb70\r\nAug 14 02:29:53 jupiter kernel: [8987905.885184]  [<ffffffff8c62478b>] ? __switch_to+0x2bb/0x710\r\nAug 14 02:29:53 jupiter kernel: [8987905.885742]  [<ffffffff8cbee4d7>] ? rewind_stack_do_exit+0x17/0x20\r\nAug 14 02:29:53 jupiter kernel: [8987905.886303]  [<ffffffff8c695c80>] ? kthread_park+0x50/0x50\r\nAug 14 02:29:53 jupiter kernel: [8987905.886862] Code: c1 e0 10 45 31 c9 85 c0 74 44 48 89 c2 c1 e8 12 48 c1 ea 0c 83 e8 01 83 e2 30 48 98 48 81 c2 40 92 01 00 48 03 14 c5 40 e6 11 8d <48> 89 0a 8b 41 08 85 c0 75 09 f3 90 8b 41 08 85 c0 74 f7 4c 8b \r\nAug 14 02:29:53 jupiter kernel: [8987905.888105] RIP  [<ffffffff8c6c0ff4>] native_queued_spin_lock_slowpath+0x104/0x190\r\nAug 14 02:29:53 jupiter kernel: [8987905.888676]  RSP <ffff8f5249153e90>\r\nAug 14 02:29:53 jupiter kernel: [8987905.889237] ---[ end trace 85818f66365046ea ]---\r\nAug 14 02:29:53 jupiter kernel: [8987905.972159] Fixing recursive fault but reboot is needed!\r\nAug 14 02:29:53 jupiter kernel: [8987905.992926] BUG: unable to handle kernel NULL pointer dereference at 0000000000000038\r\nAug 14 02:29:53 jupiter kernel: [8987905.993588] IP: [<ffffffffc0ddc556>] dbuf_write_physdone+0x26/0x40 [zfs]\r\nAug 14 02:29:53 jupiter kernel: [8987905.994271] PGD 0 \r\nAug 14 02:29:53 jupiter kernel: [8987905.994865] Oops: 0000 [#5] SMP\r\nAug 14 02:29:53 jupiter kernel: [8987905.995497] Modules linked in: nf_conntrack_netlink nls_utf8 fuse btrfs xor raid6_pq ufs qnx4 hfsplus hfs minix ntfs vfat msdos fat jfs xfs libcrc32c crc32c_generic nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc nfnetlink_queue nfnetlink_log nfnetlink bluetooth rfkill hid_generic usbhid hid xt_conntrack xfrm_user xfrm_algo xt_addrtype iptable_filter veth xt_nat xt_tcpudp ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack ip_tables x_tables bridge stp llc intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate zfs(POE) zunicode(POE) zcommon(POE) znvpair(POE) intel_uncore iTCO_wdt iTCO_vendor_support spl(OE) evdev zavl(POE) intel_rapl_perf serio_raw cdc_ether ast pcspkr usbnet ttm drm_kms_helper r8152 mii drm i2c_i801 i2c_smbus lpc_ich mfd_core ipmi_si ie31200_edac mei_me ipmi_msghandler mei shpchp battery tpm_infineon edac_core video tpm_tis tpm_tis_core tpm button overlay autofs4 ext4 crc16 jbd2 fscrypto mbcache dm_mod raid1 md_mod sg sd_mod crc32c_intel ahci libahci aesni_intel mpt3sas libata aes_x86_64 xhci_pci glue_helper lrw xhci_hcd gf128mul ablk_helper ehci_pci cryptd raid_class ehci_hcd scsi_transport_sas igb scsi_mod psmouse usbcore i2c_algo_bit dca ptp pps_core usb_common thermal fan fjes\r\nAug 14 02:29:53 jupiter kernel: [8987906.002473] CPU: 3 PID: 1849 Comm: z_wr_int_7 Tainted: P      D W  OE   4.8.0-0.bpo.2-amd64 #1 Debian 4.8.15-2~bpo8+2\r\nAug 14 02:29:53 jupiter kernel: [8987906.003057] Hardware name: ASUSTeK COMPUTER INC. P9D-M Series/P9D-M Series, BIOS 1204 07/14/2015\r\nAug 14 02:29:53 jupiter kernel: [8987906.003693] task: ffff8f523e35a080 task.stack: ffff8f523e378000\r\nAug 14 02:29:53 jupiter kernel: [8987906.004259] RIP: 0010:[<ffffffffc0ddc556>]  [<ffffffffc0ddc556>] dbuf_write_physdone+0x26/0x40 [zfs]\r\nAug 14 02:29:53 jupiter kernel: [8987906.004876] RSP: 0018:ffff8f523e37bd78  EFLAGS: 00010246\r\nAug 14 02:29:53 jupiter kernel: [8987906.005443] RAX: 0000000000000000 RBX: ffff8f4c223675d0 RCX: 0000000000742422\r\nAug 14 02:29:53 jupiter kernel: [8987906.006016] RDX: 0000000000000000 RSI: ffff8f51629439a0 RDI: ffff8f523f43b000\r\nAug 14 02:29:53 jupiter kernel: [8987906.006588] RBP: ffff8f50e5d7a000 R08: ffff8f519b69f520 R09: ffff8f4c204c6940\r\nAug 14 02:29:53 jupiter kernel: [8987906.007191] R10: 009a02d24a38eaf7 R11: 0000000000000000 R12: 0000000000000100\r\nAug 14 02:29:53 jupiter kernel: [8987906.007790] R13: 00000000ffffffff R14: ffff8f4bd4265690 R15: ffff8f51463048e0\r\nAug 14 02:29:53 jupiter kernel: [8987906.008388] FS:  0000000000000000(0000) GS:ffff8f526fcc0000(0000) knlGS:0000000000000000\r\nAug 14 02:29:53 jupiter kernel: [8987906.008985] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nAug 14 02:29:53 jupiter kernel: [8987906.009581] CR2: 0000000000000038 CR3: 00000006a9206000 CR4: 00000000001406e0\r\nAug 14 02:29:53 jupiter kernel: [8987906.010162] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\nAug 14 02:29:53 jupiter kernel: [8987906.010765] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\r\nAug 14 02:29:53 jupiter kernel: [8987906.011421] Stack:\r\nAug 14 02:29:53 jupiter kernel: [8987906.012016]  ffff8f51463048e0 ffff8f5246a63000 ffffffffc0e790f6 0000000000080000\r\nAug 14 02:29:53 jupiter kernel: [8987906.012636]  ffff8f5146304c08 ffff8f518e1b31c0 ffffffffc0e7db1b ffffffff8cbea83e\r\nAug 14 02:29:53 jupiter kernel: [8987906.013239]  ffff8f4bd4265690 ffff8f4bd42659b8 ffff8f4bd42659e0 ffff8f523e35a080\r\nAug 14 02:29:53 jupiter kernel: [8987906.013835] Call Trace:\r\nAug 14 02:29:53 jupiter kernel: [8987906.014436]  [<ffffffffc0e790f6>] ? zio_vdev_io_assess+0x96/0x1c0 [zfs]\r\nAug 14 02:29:53 jupiter kernel: [8987906.015042]  [<ffffffffc0e7db1b>] ? zio_done+0x83b/0xcc0 [zfs]\r\nAug 14 02:29:53 jupiter kernel: [8987906.015654]  [<ffffffff8cbea83e>] ? mutex_lock+0xe/0x30\r\nAug 14 02:29:53 jupiter kernel: [8987906.016257]  [<ffffffffc0e78f8e>] ? zio_execute+0x8e/0xf0 [zfs]\r\nAug 14 02:29:53 jupiter kernel: [8987906.016857]  [<ffffffffc086f1cd>] ? taskq_thread+0x24d/0x440 [spl]\r\nAug 14 02:29:53 jupiter kernel: [8987906.017456]  [<ffffffff8c6a0800>] ? wake_up_q+0x60/0x60\r\nAug 14 02:29:53 jupiter kernel: [8987906.018054]  [<ffffffffc086ef80>] ? taskq_thread_spawn+0x50/0x50 [spl]\r\nAug 14 02:29:53 jupiter kernel: [8987906.018649]  [<ffffffff8c695d5f>] ? kthread+0xdf/0x100\r\nAug 14 02:29:53 jupiter kernel: [8987906.019246]  [<ffffffff8c62478b>] ? __switch_to+0x2bb/0x710\r\nAug 14 02:29:53 jupiter kernel: [8987906.019799]  [<ffffffff8cbecf2f>] ? ret_from_fork+0x1f/0x40\r\nAug 14 02:29:53 jupiter kernel: [8987906.020347]  [<ffffffff8c695c80>] ? kthread_park+0x50/0x50\r\nAug 14 02:29:53 jupiter kernel: [8987906.020892] Code: 0f 1f 44 00 00 0f 1f 44 00 00 55 53 48 89 fb 48 8b 7a 20 48 89 d5 e8 9a e7 00 00 48 89 c7 48 8b 85 e8 00 00 00 31 d2 48 8b 4b 50 <8b> 40 38 48 f7 b3 f0 02 00 00 5b 5d 48 89 ca 48 63 f0 e9 13 f4 \r\nAug 14 02:29:53 jupiter kernel: [8987906.022075] RIP  [<ffffffffc0ddc556>] dbuf_write_physdone+0x26/0x40 [zfs]\r\nAug 14 02:29:53 jupiter kernel: [8987906.022654]  RSP <ffff8f523e37bd78>\r\nAug 14 02:29:53 jupiter kernel: [8987906.023244] CR2: 0000000000000038\r\nAug 14 02:29:53 jupiter kernel: [8987906.023808] ---[ end trace 85818f66365046eb ]---\r\nAug 14 02:29:53 jupiter kernel: [8987906.105294] BUG: unable to handle kernel paging request at 0000000000019260\r\nAug 14 02:29:53 jupiter kernel: [8987906.106360] IP: [<ffffffff8c6c0ff4>] native_queued_spin_lock_slowpath+0x104/0x190\r\nAug 14 02:29:53 jupiter kernel: [8987906.107011] PGD 0 \r\nAug 14 02:29:53 jupiter kernel: [8987906.107574] Oops: 0002 [#6] SMP\r\nAug 14 02:29:53 jupiter kernel: [8987906.108094] Modules linked in: nf_conntrack_netlink nls_utf8 fuse btrfs xor raid6_pq ufs qnx4 hfsplus hfs minix ntfs vfat msdos fat jfs xfs libcrc32c crc32c_generic nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc nfnetlink_queue nfnetlink_log nfnetlink bluetooth rfkill hid_generic usbhid hid xt_conntrack xfrm_user xfrm_algo xt_addrtype iptable_filter veth xt_nat xt_tcpudp ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack ip_tables x_tables bridge stp llc intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate zfs(POE) zunicode(POE) zcommon(POE) znvpair(POE) intel_uncore iTCO_wdt iTCO_vendor_support spl(OE) evdev zavl(POE) intel_rapl_perf serio_raw cdc_ether ast pcspkr usbnet ttm drm_kms_helper r8152 mii drm i2c_i801 i2c_smbus lpc_ich mfd_core ipmi_si ie31200_edac mei_me ipmi_msghandler mei shpchp battery tpm_infineon edac_core video tpm_tis tpm_tis_core tpm button overlay autofs4 ext4 crc16 jbd2 fscrypto mbcache dm_mod raid1 md_mod sg sd_mod crc32c_intel ahci libahci aesni_intel mpt3sas libata aes_x86_64 xhci_pci glue_helper lrw xhci_hcd gf128mul ablk_helper ehci_pci cryptd raid_class ehci_hcd scsi_transport_sas igb scsi_mod psmouse usbcore i2c_algo_bit dca ptp pps_core usb_common thermal fan fjes\r\nAug 14 02:29:53 jupiter kernel: [8987906.114116] CPU: 3 PID: 1849 Comm: z_wr_int_7 Tainted: P      D W  OE   4.8.0-0.bpo.2-amd64 #1 Debian 4.8.15-2~bpo8+2\r\nAug 14 02:29:53 jupiter kernel: [8987906.114662] Hardware name: ASUSTeK COMPUTER INC. P9D-M Series/P9D-M Series, BIOS 1204 07/14/2015\r\nAug 14 02:29:53 jupiter kernel: [8987906.115252] task: ffff8f523e35a080 task.stack: ffff8f523e378000\r\nAug 14 02:29:53 jupiter kernel: [8987906.115827] RIP: 0010:[<ffffffff8c6c0ff4>]  [<ffffffff8c6c0ff4>] native_queued_spin_lock_slowpath+0x104/0x190\r\nAug 14 02:29:53 jupiter kernel: [8987906.116397] RSP: 0018:ffff8f523e37be90  EFLAGS: 00010006\r\nAug 14 02:29:53 jupiter kernel: [8987906.116962] RAX: 00000000000015f8 RBX: 0000000000000282 RCX: ffff8f526fcd9240\r\nAug 14 02:29:53 jupiter kernel: [8987906.117529] RDX: 0000000000019260 RSI: 0000000057e6c8d3 RDI: ffff8f523e37bf18\r\nAug 14 02:29:53 myserver kernel: [8987906.118091] RBP: ffff8f523e37bf10 R08: 0000000000100000 R09: 0000000000000000\r\nAug 14 02:29:53 jupiter kernel: [8987906.118652] R10: 0000000000000008 R11: 0000000000000000 R12: ffff8f523e37bcc8\r\nAug 14 02:29:53 jupiter kernel: [8987906.119255] R13: 0000000000000000 R14: 0000000000000046 R15: 0000000000000001\r\nAug 14 02:29:53 jupiter kernel: [8987906.119821] FS:  0000000000000000(0000) GS:ffff8f526fcc0000(0000) knlGS:0000000000000000\r\nAug 14 02:29:53 jupiter kernel: [8987906.120386] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nAug 14 02:29:53 jupiter kernel: [8987906.120946] CR2: 0000000000019260 CR3: 00000006a9206000 CR4: 00000000001406e0\r\nAug 14 02:29:53 jupiter kernel: [8987906.121513] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\nAug 14 02:29:53 jupiter kernel: [8987906.122077] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\r\nAug 14 02:29:53 jupiter kernel: [8987906.122633] Stack:\r\nAug 14 02:29:53 jupiter kernel: [8987906.123214]  ffffffff8cbecc42 ffff8f523e37bf18 ffffffff8c6b9428 ffff8f523e35a7a8\r\nAug 14 02:29:53 jupiter kernel: [8987906.123780]  ffff8f523e35a080 ffff8f523e37bcc8 ffffffff8c673ee0 ffff8f523e35a080\r\nAug 14 02:29:53 jupiter kernel: [8987906.124344]  0000000000000009 ffffffff8c67a573 ffffffff8c62478b 0000000047439c80\r\nAug 14 02:29:53 jupiter kernel: [8987906.124911] Call Trace:\r\nAug 14 02:29:53 jupiter kernel: [8987906.125472]  [<ffffffff8cbecc42>] ? _raw_spin_lock_irqsave+0x32/0x40\r\nAug 14 02:29:53 jupiter kernel: [8987906.126041]  [<ffffffff8c6b9428>] ? complete+0x18/0x40\r\nAug 14 02:29:53 jupiter kernel: [8987906.126609]  [<ffffffff8c673ee0>] ? mm_release+0xb0/0x130\r\nAug 14 02:29:53 jupiter kernel: [8987906.127211]  [<ffffffff8c67a573>] ? do_exit+0x153/0xb70\r\nAug 14 02:29:53 jupiter kernel: [8987906.127776]  [<ffffffff8c62478b>] ? __switch_to+0x2bb/0x710\r\nAug 14 02:29:53 jupiter kernel: [8987906.128341]  [<ffffffff8cbee4d7>] ? rewind_stack_do_exit+0x17/0x20\r\nAug 14 02:29:53 jupiter kernel: [8987906.128908]  [<ffffffff8c695c80>] ? kthread_park+0x50/0x50\r\nAug 14 02:29:53 jupiter kernel: [8987906.129463] Code: c1 e0 10 45 31 c9 85 c0 74 44 48 89 c2 c1 e8 12 48 c1 ea 0c 83 e8 01 83 e2 30 48 98 48 81 c2 40 92 01 00 48 03 14 c5 40 e6 11 8d <48> 89 0a 8b 41 08 85 c0 75 09 f3 90 8b 41 08 85 c0 74 f7 4c 8b \r\nAug 14 02:29:53 jupiter kernel: [8987906.130646] RIP  [<ffffffff8c6c0ff4>] native_queued_spin_lock_slowpath+0x104/0x190\r\nAug 14 02:29:53 jupiter kernel: [8987906.131251]  RSP <ffff8f523e37be90>\r\nAug 14 02:29:53 jupiter kernel: [8987906.131818] CR2: 0000000000019260\r\nAug 14 02:29:53 jupiter kernel: [8987906.132382] ---[ end trace 85818f66365046ec ]---\r\nAug 14 02:29:53 jupiter kernel: [8987906.134812] Fixing recursive fault but reboot is needed!\r\n```\r\nI've put another copy here if you prefer:  https://paste.debian.net/981268/\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6508/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dbavatar": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6506", "title": "Metadata not counted as evictable/not being reclaimed. Leaked?", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | x\r\nDistribution Version    | x\r\nLinux Kernel                 | 4.4 through 4.9.24+\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.1\r\nSPL Version                  | 0.7.1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nMetadata is not being counted as evictable, and is not evicting. In fact, I can't get it to reduce in size short of deleting the files or unmounting, i.e. pressure from arc data has no effect. As a side note, 7.x seems to also be using twice as much memory per file as 6.5.x, although that's not part of this bug.\r\n\r\nI've bisected this back to commit d3c2ae1c0806b183a315e3d43cc8018cfdca79b5. Unfortunately it's 4500 lines.\r\n\r\n### Describe how to reproduce the problem\r\n1. Load zfs cleanly and import non-root zpool such that arcstats are minimal.\r\n2. Create enough files to measure effect with high confidence, in my case 3 dirs with 32k files each, 0 bytes, uses ~64MB to ~128MB  To refill metadata after reload: \"ls -ltr [path]* > /dev/null\"\r\n3. Observe that arcstats shows a large amount of size/metadata_size and mru_size/mfu_size. However mru_evictable* and mfu_evictable* are tiny.\r\n4. Observe that echo 3 > /proc/sys/vm/drop_caches does not shrink metadata_size.\r\n\r\nMy bisect script:\r\n\r\n```sh\r\nsh autogen.sh && ./configure --with-config=kernel  --prefix=/ && make -j 10 && make install\r\nzpool export -a\r\nkillall zed; rmmod zfs zunicode zcommon zavl znvpair icp spl;\r\ndepmod -a\r\nmodprobe zfs\r\nzpool import -a\r\nls -ltr /tank/d* > /dev/null\r\necho 3 > /proc/sys/vm/drop_caches\r\nsleep 1\r\necho 3 > /proc/sys/vm/drop_caches\r\nsleep 1\r\necho 3 > /proc/sys/vm/drop_caches\r\ncat /proc/spl/kstat/zfs/arcstats |grep ^metadata_size | awk '{ if ($3 > 50000000) { print \"Bad: \" $3; exit 1} print \"Good: \" $3 }'\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6506/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6047", "title": "Quota not observed when file is allocated via mmap", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | \r\nDistribution Version    | \r\nLinux Kernel                 | 4.4.57+\r\nArchitecture                 | x86_64\r\nZFS Version                  | 7.0-rc3\r\nSPL Version                  | 7.0-rc3\r\n\r\n\r\n### Describe the problem you're observing\r\nZFS allows quota to be exceeded, and loses accounting of those blocks when writing to a sparse file via mmap\r\n\r\n### Describe how to reproduce the problem\r\nzfs set quota=2M tank/b\r\nzfs set compression=off tank/b\r\n```\r\n#define SZ (8*1024*1024)\r\nfd = open(\"/tank/b/testfile\", O_RDWR|O_CREAT|O_TRUNC);\r\nftruncate(fd, SZ);\r\nmap = mmap(NULL, SZ, PROT_WRITE, MAP_SHARED, fd, 0);\r\nmemset(map, 0x6c, SZ);\r\n```\r\n```\r\nstat /tank/b/testfile \r\n  File: '/tank/b/testfile'\r\n  Size: 8388608   \tBlocks: 4113       IO Block: 131072 regular file\r\nDevice: 22h/34d\tInode: 20          Links: 1\r\nAccess: (0000/----------)  Uid: (    0/    root)   Gid: (    0/    root)\r\nAccess: 2017-04-20 21:03:23.633604550 +0000\r\nModify: 2017-04-20 21:05:20.272604965 +0000\r\nChange: 2017-04-20 21:05:20.272604965 +0000\r\n Birth: -\r\n\r\n hexdump testfile \r\n0000000 6c6c 6c6c 6c6c 6c6c 6c6c 6c6c 6c6c 6c6c\r\n*\r\n0800000\r\n\r\n```\r\n\r\nThe whole file was written and is accessible, despite block counts appearing to obey the quota! \r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6047/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/2209e40981e887c773914ec0f3b73cedf45ddb7d", "message": "Linux 4.8+ compatibility fix for vm stats\n\nvm_node_stat must be used instead of vm_zone_stat. Unfortunately the\r\nold code still compiles potentially leading to silent failure of\r\narc_evictable_memory()\r\n\r\nAKAMAI: CR 3816601: Regression in zfs dropcache test\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Chunwei Chen <tuxoko@gmail.com>\r\nSigned-off-by: Debabrata Banerjee <dbanerje@akamai.com>\r\nCloses #6528"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "slburson": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6503", "title": "Feature suggestion: cachefile versioning", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  openSUSE\r\nDistribution Version    |  Leap 42.1\r\nLinux Kernel                 |  4.1.39-56\r\nArchitecture                 |  x86_64\r\nZFS Version                  |  0.7.0-1\r\nSPL Version                  |  0.7.0-1\r\n\r\n### Describe the problem you're observing\r\n\r\nAfter upgrading from 0.6.5.8 to 0.7.0, an attempt to import my pool produced:\r\n\r\nVERIFY3(size <= rt->rt_space) failed (25617 <= 25)\r\nPANIC at range_tree.c:240:range_tree_remove()\r\n\r\nAfter some fiddling, it dawned on me to try rebuilding the cachefile, and indeed that fixed the problem.  But it would have saved me some time and anxiety if the cachefile had simply been rejected for being old and incompatible.  Seems like a version number in the cachefile would suffice.\r\n\r\nI should add that I had also just upgraded the kernel from 4.1.36-44, so I don't know if the incompatibility was rooted in the kernel or ZFS.  Maybe both versions should be checked.\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6503/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Yevgeny1": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6502", "title": "zfs 0.7.1 building problem on OEL 7.4", "body": "distr :   OEL 7.4\r\narch :    x86-64\r\nkernel : UEK4 R4\r\nLinux oel7_1 4.1.12-94.5.7.el7uek.x86_64 #2 SMP Thu Jul 20 18:44:17 PDT 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\nZFS Version   :               0.7.1\r\nmodinfo zfs | grep -iw version :                0.7.1-1\r\nmodinfo spl | grep -iw version :                0.7.1-1\r\n\r\n## problem\r\n\r\nrebuild after upgrade (OEL 7.3 --> 7.4),  DKMS  build incorrect modules, for non-active kernels\r\n(installed both kernel trees,  RHEL kernel 3.10.xxx  and UEK kernel 4.1.xxx, active is 4.1.xxx,  but build script build module for fresh-installed and non-active kernel 3.10.xxx, not for currently used  4.1.xxx kernel).\r\n\r\nsolution : reboot, remove all zfs* spl* rpm's, install & rebuild.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6502/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "evujumenuk": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6501", "title": "'edonr' and 'skein' zpool features are not supported for root pools even though UEFI is a thing", "body": "Exhibit A:\r\n\r\nhttps://github.com/zfsonlinux/zfs/blob/376994828fd3753aba75d492859727ca76f6a293/module/zfs/zfs_ioctl.c#L3991-L4004\r\n\r\nExhibit B:\r\n\r\nhttps://github.com/zfsonlinux/zfs/blob/376994828fd3753aba75d492859727ca76f6a293/lib/libzfs/libzfs_dataset.c#L1593-L1598\r\n\r\n(376994828fd3753aba75d492859727ca76f6a293 is the current `master`.)\r\n\r\nAs for the rationale, @skiselkov stated the following in https://www.listbox.com/member/archive/post_content.html?post_id=20131007080502:B04DC60C-2F48-11E3-9198-9C9438B0F59B&address=000:\r\n\r\n> No, this is to cover the ERANGE return from zfs_ioctl.c:3784 where we\r\ncheck to make sure nobody enables any salted checksums on a root pool,\r\nsince GRUB doesn't have support for that feature. Bootloader support on\r\nFreeBSD might differ, though.\r\n\r\nOn UEFI systems, GRUB is not even used. Prohibiting the setting and use of specific flags is, in this configuration, rather unnecessary.\r\n\r\nhttps://github.com/zfsonlinux/zfs/issues/6202 notes the same: disallowing zpool features is not warranted when GRUB is not used, which is a configuration that has worked for several years now.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6501/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hyegeek": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6500", "title": "PANIC at sa.c:1283:sa_build_index()", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Gentoo\r\nDistribution Version    | NA (mostly stable and up to date)\r\nLinux Kernel                 | 4.9.41-gentoo\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.0 (started on 0.6.5.9)\r\nSPL Version                  | 0.7.0\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nGet a kernel panic when I try to unmerge an older kernel version. Probably related to touching a file with issues.\r\n### Describe how to reproduce the problem\r\nemerge -C =sys-kernel/gentoo-sources-4.9.13\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n\r\n```\r\n-->\r\n```\r\nAug 10 18:39:25 gar kernel: VERIFY(BSWAP_32(sa_hdr_phys->sa_magic) == SA_MAGIC) failed\r\nAug 10 18:39:25 gar kernel: PANIC at sa.c:1283:sa_build_index()\r\nAug 10 18:39:25 gar kernel: Showing stack for process 18520\r\nAug 10 18:39:25 gar kernel: CPU: 0 PID: 18520 Comm: emerge Tainted: P           O    4.9.41-gentoo.zfs #1\r\nAug 10 18:39:25 gar kernel: Hardware name: ASUS All Series/B85M-E, BIOS 2304 05/25/2015\r\nAug 10 18:39:25 gar kernel:  ffffc9002617b548 ffffffff81319d3b ffffffffa054c8b9 0000000000000503\r\nAug 10 18:39:25 gar kernel:  ffffc9002617b558 ffffffffa002618d ffffc9002617b6d8 ffffffffa002623d\r\nAug 10 18:39:25 gar kernel:  0000000002404200 0000000000000028 ffffc9002617b6e8 ffffc9002617b688\r\nAug 10 18:39:25 gar kernel: Call Trace:\r\nAug 10 18:39:25 gar kernel:  [<ffffffff81319d3b>] dump_stack+0x4d/0x72\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa002618d>] spl_dumpstack+0x3d/0x40 [spl]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa002623d>] spl_panic+0xad/0xf0 [spl]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa005a146>] ? avl_find+0x56/0xa0 [zavl]\r\nAug 10 18:39:25 gar kernel:  [<ffffffff8117e980>] ? address_space_init_once+0x30/0x60\r\nAug 10 18:39:25 gar kernel:  [<ffffffff8117ea24>] ? inode_init_once+0x74/0x90\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa044d042>] ? sa_cache_init+0x72/0x90 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa0022859>] ? spl_kmem_cache_alloc+0xf9/0x860 [spl]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa0409476>] ? dbuf_rele_and_unlock+0x256/0x500 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa044ef6c>] sa_byteswap+0x23c/0xf90 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa0450837>] sa_handle_get_from_db+0x127/0x190 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa04aa839>] zfs_znode_hold_compare+0x959/0xf40 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa04ab397>] zfs_inode_update+0x187/0x6b0 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa0424ac5>] ? dmu_zfetch+0x3b5/0x1230 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa0409476>] ? dbuf_rele_and_unlock+0x256/0x500 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffff810b0aab>] ? getrawmonotonic64+0x2b/0xc0\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa0027a7c>] ? __cv_init+0x3c/0x60 [spl]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa0424ed1>] ? dmu_zfetch+0x7c1/0x1230 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa0409476>] ? dbuf_rele_and_unlock+0x256/0x500 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa0027a7c>] ? __cv_init+0x3c/0x60 [spl]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa04086c9>] ? dbuf_fill_done+0xe9/0xf0 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa0022859>] ? spl_kmem_cache_alloc+0xf9/0x860 [spl]\r\nAug 10 18:39:25 gar kernel:  [<ffffffff8114bc49>] ? __kmalloc_node+0x29/0x1a0\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa0020dfa>] ? spl_kmem_alloc+0xaa/0x180 [spl]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa040a3b1>] ? dbuf_read+0x591/0x920 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa0414b94>] ? dmu_object_info_from_dnode+0x64/0x80 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa04adf49>] zfs_zget+0x169/0x1f0 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa048a294>] zfs_dirent_lock+0x524/0x610 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa048a3f5>] zfs_dirlook+0x75/0x270 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa04a13a4>] zfs_lookup+0x314/0x380 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa04be9ac>] zpl_fallocate_common+0x63c/0x850 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffff811717ae>] ? lookup_fast+0x1be/0x2e0\r\nAug 10 18:39:25 gar kernel:  [<ffffffff8117d8a1>] ? __d_lookup+0x81/0x140\r\nAug 10 18:39:25 gar kernel:  [<ffffffff8116ed51>] lookup_slow+0x91/0x140\r\nAug 10 18:39:25 gar kernel:  [<ffffffff81171d48>] walk_component+0x1b8/0x270\r\nAug 10 18:39:25 gar kernel:  [<ffffffff81170106>] ? path_init+0x1f6/0x370\r\nAug 10 18:39:25 gar kernel:  [<ffffffff8118463f>] ? mntput+0x1f/0x30\r\nAug 10 18:39:25 gar kernel:  [<ffffffff81172418>] path_lookupat+0x58/0x100\r\nAug 10 18:39:25 gar kernel:  [<ffffffff811742a9>] filename_lookup+0x99/0x150\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa044cab8>] ? rrw_exit+0x48/0x130 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffffa044cde1>] ? rrm_exit+0x41/0x70 [zfs]\r\nAug 10 18:39:25 gar kernel:  [<ffffffff81173f2d>] ? getname_flags+0x6d/0x1f0\r\nAug 10 18:39:25 gar kernel:  [<ffffffff81174401>] user_path_at_empty+0x31/0x40\r\nAug 10 18:39:25 gar kernel:  [<ffffffff81169b7e>] vfs_fstatat+0x4e/0xa0\r\nAug 10 18:39:25 gar kernel:  [<ffffffff8116a042>] SyS_newlstat+0x22/0x40\r\nAug 10 18:39:25 gar kernel:  [<ffffffff8153c2e4>] entry_SYSCALL_64_fastpath+0x17/0x98\r\n\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6500/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gusioo": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6490", "title": "System freeze (rcu lock) ", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian\r\nDistribution Version    | Jessie\r\nLinux Kernel                 | 3.10.103\r\nArchitecture                 | x64\r\nZFS Version                  | 0.6.5.4-1\r\nSPL Version                  | 0.6.5.4-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nMy system freezed during tests connected with zfs. I supsect that it may be connected with large number of snapshots present in the system. When call trace occured I had approx. 6200 of them.\r\n\r\n### Describe how to reproduce the problem\r\n- create snapshot\r\n- send it via send/receive\r\n- delete an old snapshot (snapshot rotation)\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n```\r\nAug  8 21:11:22\r\n/usr/sbin/zfs list -H -o name -t snapshot -s creation -d 1 Pool-0/b12\r\n\r\nAug  8 21:11:24 [local1.err] (pid: 26187, thread: MainThread) \"Remote call to node: '547bd734' via link: '10.11.12.81' was successful.\"\r\nAug  8 21:11:30 [kern.err] [35733.887391] INFO: rcu_sched self-detected stall on CPU { 0}  (t=6000 jiffies g=544478 c=544477 q=9929824)\r\nAug  8 21:11:30 [kern.warning] [35733.887404] NMI backtrace for cpu 0\r\nAug  8 21:11:30 [kern.warning] [35733.887406] CPU: 0 PID: 21621 Comm: dbu_evict Tainted: P           O 3.10.103-oe64-g188ec80 #2\r\nAug  8 21:11:30 [kern.warning] [35733.887408] Hardware name: Intel Corporation S2600IP/S2600IP, BIOS SE5C600.86B.02.03.0003.041920141333 04/19/2014\r\nAug  8 21:11:30 [kern.warning] [35733.887409] task: ffff880fe2a0a260 ti: ffff880fe1128000 task.ti: ffff880fe1128000\r\nAug  8 21:11:30 [kern.warning] [35733.887410] RIP: 0010:[<ffffffff8131cbb0>]  [<ffffffff8131cbb0>] __delay+0x10/0x10\r\nAug  8 21:11:30 [kern.warning] [35733.887416] RSP: 0018:ffff880fff603e00  EFLAGS: 00000012\r\nAug  8 21:11:30 [kern.warning] [35733.887417] RAX: 0000000000001000 RBX: 0000000000001000 RCX: 0000000000000001\r\nAug  8 21:11:30 [kern.warning] [35733.887418] RDX: 0000000000000001 RSI: 0000000000000080 RDI: 0000000000068dbc\r\nAug  8 21:11:30 [kern.warning] [35733.887419] RBP: 00000000000003e9 R08: ffffffff81a57ef0 R09: 0000000000000d05\r\nAug  8 21:11:30 [kern.warning] [35733.887419] R10: ffff880fff60d7c0 R11: 0000000000000d04 R12: ffffffff81a57ef0\r\nAug  8 21:11:30 [kern.warning] [35733.887420] R13: 0000000000000400 R14: 0000000000000002 R15: 0000000000000002\r\nAug  8 21:11:30 [kern.warning] [35733.887422] FS:  0000000000000000(0000) GS:ffff880fff600000(0000) knlGS:0000000000000000\r\nAug  8 21:11:30 [kern.warning] [35733.887423] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nAug  8 21:11:30 [kern.warning] [35733.887423] CR2: 0000000001c2a980 CR3: 000000000198a000 CR4: 00000000000407f0\r\nAug  8 21:11:30 [kern.warning] [35733.887424] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\nAug  8 21:11:30 [kern.warning] [35733.887425] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400\r\nAug  8 21:11:30 [kern.warning] [35733.887426] Stack:\r\nAug  8 21:11:30 [kern.warning] [35733.887427]  ffffffff8102bdf2 0000000000000001 000000000000a04a ffffffff8102caed\r\nAug  8 21:11:30 [kern.warning] [35733.887428]  0000000000000082 0000000000002710 ffffffff81a57f00 ffffffff819b6e40\r\nAug  8 21:11:30 [kern.warning] [35733.887430]  ffffffff819b6d80 0000000000978460 ffff880fff60d7c0 ffffffff8102cc52\r\nAug  8 21:11:30 [kern.warning] [35733.887431] Call Trace:\r\nAug  8 21:11:30 [kern.warning] [35733.887432]  <IRQ> \r\nAug  8 21:11:30 [kern.warning] [35733.887433]  [<ffffffff8102bdf2>] ? native_safe_apic_wait_icr_idle+0x22/0x50\r\nAug  8 21:11:30 [kern.warning] [35733.887438]  [<ffffffff8102caed>] ? default_send_IPI_mask_sequence_phys+0xcd/0xf0\r\nAug  8 21:11:30 [kern.warning] [35733.887440]  [<ffffffff8102cc52>] ? arch_trigger_all_cpu_backtrace+0x52/0x90\r\nAug  8 21:11:30 [kern.warning] [35733.887452]  [<ffffffff810b2e9d>] ? rcu_check_callbacks+0x2ed/0x580\r\nAug  8 21:11:30 [kern.warning] [35733.887455]  [<ffffffff81051e9b>] ? update_process_times+0x3b/0x70\r\nAug  8 21:11:30 [kern.warning] [35733.887457]  [<ffffffff81085724>] ? tick_sched_timer+0x34/0x50\r\nAug  8 21:11:30 [kern.warning] [35733.887460]  [<ffffffff8106701c>] ? __run_hrtimer.isra.32+0x3c/0xc0\r\nAug  8 21:11:30 [kern.warning] [35733.887461]  [<ffffffff8106765f>] ? hrtimer_interrupt+0xef/0x240\r\nAug  8 21:11:30 [kern.warning] [35733.887463]  [<ffffffff8102bf12>] ? smp_apic_timer_interrupt+0x62/0xa0\r\nAug  8 21:11:30 [kern.warning] [35733.887466]  [<ffffffff816c35dd>] ? apic_timer_interrupt+0x6d/0x80\r\nAug  8 21:11:30 [kern.warning] [35733.887466]  <EOI> \r\nAug  8 21:11:30 [kern.warning] [35733.887470]  [<ffffffffa065396b>] ? unique_remove+0x5b/0x90 [zfs]\r\nAug  8 21:11:30 [kern.warning] [35733.887531]  [<ffffffffa0606af0>] ? compression_changed_cb+0x20/0x20 [zfs]\r\nAug  8 21:11:30 [kern.warning] [35733.887548]  [<ffffffffa06296fe>] ? dsl_prop_unregister+0x7e/0x190 [zfs]\r\nAug  8 21:11:30 [kern.warning] [35733.887563]  [<ffffffffa06296ba>] ? dsl_prop_unregister+0x3a/0x190 [zfs]\r\nAug  8 21:11:30 [kern.warning] [35733.887576]  [<ffffffffa0606af0>] ? compression_changed_cb+0x20/0x20 [zfs]\r\nAug  8 21:11:30 [kern.warning] [35733.887590]  [<ffffffffa060954f>] ? dmu_objset_evict+0x10f/0x580 [zfs]\r\nAug  8 21:11:30 [kern.warning] [35733.887604]  [<ffffffffa061863c>] ? dsl_dataset_evict+0x2c/0x160 [zfs]\r\nAug  8 21:11:30 [kern.warning] [35733.887609]  [<ffffffffa0550cb0>] ? taskq_thread+0x230/0x450 [spl]\r\nAug  8 21:11:30 [kern.warning] [35733.887611]  [<ffffffff8106f450>] ? wake_up_process+0x10/0x10\r\nAug  8 21:11:30 [kern.warning] [35733.887614]  [<ffffffffa0550a80>] ? taskq_cancel_id+0x110/0x110 [spl]\r\nAug  8 21:11:30 [kern.warning] [35733.887616]  [<ffffffff81063d82>] ? kthread+0xc2/0xd0\r\nAug  8 21:11:30 [kern.warning] [35733.887618]  [<ffffffff81070000>] ? build_sched_domains+0x500/0xb00\r\nAug  8 21:11:30 [kern.warning] [35733.887620]  [<ffffffff81063cc0>] ? kthread_create_on_node+0x110/0x110\r\nAug  8 21:11:30 [kern.warning] [35733.887622]  [<ffffffff816c2918>] ? ret_from_fork+0x58/0x90\r\nAug  8 21:11:30 [kern.warning] [35733.887624]  [<ffffffff81063cc0>] ? kthread_create_on_node+0x110/0x110\r\nAug  8 21:11:30 [kern.warning] [35733.887625] Code: 66 66 2e 0f 1f 84 00 00 00 00 00 48 ff c8 75 fb 48 ff c8 c3 0f 1f 80 00 00 00 00 48 8b 05 39 ef 6a 00 ff e0 0f 1f 80 00 00 00 00 <65> 48 8b 14 25 20 15 01 00 48 8d 14 92 48 8d 04 bd 00 00 00 00\r\n```\r\n\r\nCould you please help with analysis? Is this a known bug?\r\nPlease take a look at the attachment to see more details connected with call-trace: [call_trace.txt](https://github.com/zfsonlinux/zfs/files/1214058/call_trace.txt)", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6490/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bendikro": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6476", "title": "Problems getting kdump to work with ZFS", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 16.04\r\nLinux Kernel                 | 4.4.0\r\nArchitecture                 | x86\r\nZFS Version                  | 0.6.5.6-0ubuntu16\r\nSPL Version                  | 0.6.5.6-0ubuntu4\r\n\r\n\r\n### Describe the problem you're observing\r\n\r\nA lot of issues have been reported about kdump-tools package failing to install on Ubuntu, such as \r\nhttps://bugs.launchpad.net/ubuntu/+source/initramfs-tools/+bug/1661629\r\n\r\nAs far as I understand, the problem is that the bash function `dep_add_modules_mount` in `/usr/share/initramfs-tools/hook-functions` (initramsfs-tools-core package) doesn't support ZFS.\r\n\r\nBy adding `MODULES=most` to `/usr/share/initramfs-tools/conf-hooks.d/zfs`, the error goes away since the `dep_add_modules_mount` function is no longer called.\r\n\r\nHow should this be solved? Should `MODULES=most` be added by default in the ZFS package?\r\n\r\nAfter resolving this issue, I still have problems getting kdump to save a crash dump, presumably because of ZFS or lack of support for ZFS in kdump?\r\n\r\nI've basically followed these instructions: https://help.ubuntu.com/lts/serverguide/kernel-crash-dump.html\r\nDoing this on an Ubuntu VM with ext4 filesystem works, but natively with ZFS on root results only in a `/var/crash/kexec_cmd` file containing `/sbin/kexec -p --command-line=\"BOOT_IMAGE=/ROOT/ubuntu@/boot/vmlinuz-4.4.0-79-generic root=ZFS=rpool/ROOT/ubuntu ro boot=zfs bootfs=rpool/ROOT/ubuntu irqpoll nr_cpus=1 nousb systemd.unit=kdump-tools.service\" --initrd=/var/lib/kdump/initrd.img /var/lib/kdump/vmlinuz`\r\n\r\nI do understand that this may not be a bug in the ZFS code/package, but it would still be very beneficial for everyone to sort this issue out.\r\n\r\nAny hints on how to debug this further to figure out the problem?", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6476/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "omgold": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6473", "title": "Shared spares do not honor multihost parameter", "body": "### System information\r\nZFS Version                  | 0.7.0\r\nSPL Version                  | 0.7.0\r\n\r\n### Describe the problem you're observing\r\n\r\nWhen using hotspares which are shared between pool, and both pools have set multihost=on, it is still possible to activate the hotspare on both pools at the same time.\r\n\r\n### Describe how to reproduce the problem\r\n\r\n- use a system with disks shared between two servers.\r\n- create 2 zpools\r\n- set multihost=on on both zpools\r\n- add a hotspare to both pools (the same disk for both pools)\r\n- import pool 1 on server A and pool 2 on server B\r\n- on server A, replace a disk in pool 1 with the hotspare with 'zpool replace'\r\n- on server B, replace a disk in pool 2 with the hotspare with 'zpool replace -f'\r\n\r\nRemark: for this test, it only works when giving the '-f' option on 'zpool replace', when trying to use the active hotspare again, but I assume there is a race condition, if one would do the 'zpool replace' on both hosts at the same time.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6473/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mcr-ksh": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6472", "title": "memmove crashdump zcommon", "body": "\r\n### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 16.04LTS\r\nLinux Kernel                 | 4.4.0-77-generic #98-Ubuntu SMP Wed Apr 26 08:34:02 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.10-1ubuntu2 16.04.york0\r\nSPL Version                  | 0.6.5.10-1 16.04.york0\r\n\r\n### Describe the problem you're observing\r\n```\r\n[7856344.969893] BUG: unable to handle kernel paging request at ffff88012f6c5000\r\n[7856344.970236] IP: [<ffffffff814065f4>] __memmove+0x24/0x1a0\r\n[7856344.970490] PGD 1e0b067 PUD 1ff824067 PMD 1ff6a8067 PTE 0\r\n[7856344.970763] Oops: 0002 [#1] SMP \r\n[7856344.970928] Modules linked in: udp_diag tcp_diag inet_diag netlink_diag af_packet_diag unix_diag btrfs ufs qnx4 hfsplus hfs minix ntfs msdos jfs xfs cpuid nfnetlink_queue nfnetlink_log nfnetlink bluetooth xt_physdev br_netfilter appletalk ipx p8023 p8022 psnap veth xen_pciback xen_netback xen_blkback xen_gntalloc xen_gntdev xen_evtchn xenfs xen_privcmd ipt_REJECT nf_reject_ipv4 xt_recent xt_conntrack xt_nat ip6table_filter xt_CHECKSUM iptable_mangle xt_tcpudp ip6t_MASQUERADE nf_nat_masquerade_ipv6 ip6table_nat nf_conntrack_ipv6 nf_defrag_ipv6 nf_nat_ipv6 ip6_tables ipt_MASQUERADE nf_nat_masquerade_ipv4 xt_comment iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack bridge stp llc iptable_filter ip_tables x_tables binfmt_misc zfs(POE) zunicode(PO) zcommon(PO) znvpair(POE) spl(OE)\r\n[7856344.974533]  zavl(POE) intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp crct10dif_pclmul crc32_pclmul ghash_clmulni_intel aesni_intel aes_x86_64 input_leds joydev lrw ipmi_ssif gf128mul glue_helper ablk_helper cryptd soc_button_array 8250_fintek ie31200_edac lpc_ich shpchp ipmi_si ipmi_msghandler edac_core mac_hid ib_iser rdma_cm iw_cm ib_cm ib_sa ib_mad ib_core ib_addr iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi msr lp parport nfsd auth_rpcgss nfs_acl lockd grace sunrpc autofs4 raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c multipath linear dm_mirror dm_region_hash dm_log raid1 raid0 hid_generic usbhid hid ast ttm drm_kms_helper igb dca ahci i2c_algo_bit libahci syscopyarea sysfillrect sysimgblt fb_sys_fops drm e1000e ptp fjes video\r\n[7856344.992331]  pps_core [last unloaded: ebtables]\r\n[7856344.995442] CPU: 0 PID: 24547 Comm: loop11 Tainted: P           OE   4.4.0-77-generic #98-Ubuntu\r\n[7856344.998749] Hardware name: Supermicro X10SLL-F/X10SLL-SF/X10SLL-F/X10SLL-SF, BIOS 1.00.T201305201729 05/20/2013\r\n[7856345.000482] task: ffff88011b4ab800 ti: ffff880153214000 task.ti: ffff880153214000\r\n[7856345.001192] RIP: e030:[<ffffffff814065f4>]  [<ffffffff814065f4>] __memmove+0x24/0x1a0\r\n[7856345.001913] RSP: e02b:ffff880153217b20  EFLAGS: 00010206\r\n[7856345.002605] RAX: ffff88012f6c5000 RBX: ffff880153217c80 RCX: 0000000000001000\r\n[7856345.003326] RDX: 0000000000001000 RSI: ffffc9007625f000 RDI: ffff88012f6c5000\r\n[7856345.004028] RBP: ffff880153217b70 R08: 000060fe09617180 R09: ffffffffc04e94c5\r\n[7856345.004731] R10: ffff880153217e18 R11: 000000000000b077 R12: 0000000000001000\r\n[7856345.005431] R13: ffffc9007625f000 R14: 0000000000000000 R15: ffff88011b4ab800\r\n[7856345.006124] FS:  0000000000000000(0000) GS:ffff8801f6600000(0000) knlGS:ffff8801f6600000\r\n[7856345.006826] CS:  e033 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[7856345.007545] CR2: ffff88012f6c5000 CR3: 0000000043f84000 CR4: 0000000000042660\r\n[7856345.008236] DR0: 00007fbd545b3030 DR1: 0000000000000000 DR2: 0000000000000000\r\n[7856345.008924] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000600\r\n[7856345.009608] Stack:\r\n[7856345.010244]  ffffffffc055c7c7 ffff880153217e18 0000000000001000 ffff88011b4ab800\r\n[7856345.010934]  0000000000000001 0000000000001000 0000000000001000 ffff880153217c80\r\n[7856345.011620]  0000000000000000 ffff8801634781f8 ffff880153217bd0 ffffffffc0622d96\r\n[7856345.012302] Call Trace:\r\n[7856345.012940]  [<ffffffffc055c7c7>] ? uiomove+0x197/0x2a0 [zcommon]\r\n[7856345.013628]  [<ffffffffc0622d96>] dmu_read_uio_dnode+0xa6/0xf0 [zfs]\r\n[7856345.014300]  [<ffffffffc0623d3f>] dmu_read_uio_dbuf+0x3f/0x60 [zfs]\r\n[7856345.014998]  [<ffffffffc06a5a39>] zfs_read+0x129/0x380 [zfs]\r\n[7856345.015683]  [<ffffffffc06c0930>] zpl_read_common_iovec+0x80/0xd0 [zfs]\r\n[7856345.016372]  [<ffffffffc06c1451>] zpl_iter_read+0xa1/0xe0 [zfs]\r\n[7856345.017033]  [<ffffffff8120e775>] vfs_iter_read+0x75/0xc0\r\n[7856345.017672]  [<ffffffff81582099>] loop_queue_work+0x7b9/0xee0\r\n[7856345.018295]  [<ffffffff810a9bcd>] ? finish_task_switch+0x7d/0x220\r\n[7856345.018925]  [<ffffffff810a1150>] kthread_worker_fn+0x50/0x180\r\n[7856345.019558]  [<ffffffff810a1100>] ? flush_kthread_work+0x140/0x140\r\n[7856345.020189]  [<ffffffff810a0be8>] kthread+0xd8/0xf0\r\n[7856345.020803]  [<ffffffff810a0b10>] ? kthread_create_on_node+0x1e0/0x1e0\r\n[7856345.021438]  [<ffffffff8183bd0f>] ret_from_fork+0x3f/0x70\r\n[7856345.022056]  [<ffffffff810a0b10>] ? kthread_create_on_node+0x1e0/0x1e0\r\n[7856345.022713] Code: 88 0c 17 88 0f c3 90 48 89 f8 48 83 fa 20 0f 82 03 01 00 00 48 39 fe 7d 0f 49 89 f0 49 01 d0 49 39 f8 0f 8f 9f 00 00 00 48 89 d1 <f3> a4 c3 48 81 fa a8 02 00 00 72 05 40 38 fe 74 3b 48 83 ea 20 \r\n[7856345.024112] RIP  [<ffffffff814065f4>] __memmove+0x24/0x1a0\r\n[7856345.024712]  RSP <ffff880153217b20>\r\n[7856345.025283] CR2: ffff88012f6c5000\r\n[7856345.027765] ---[ end trace 82465d325986cc6b ]---\r\n```\r\n\r\n### Describe how to reproduce the problem\r\njust occurs\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6472/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jxiong": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6464", "title": "zdb fails to locate datasets within exporting zfs pool with '-e -p' options", "body": "zdb prior to zfs 0.7.0 is able to get information of a dataset in zfs pool by '-e -p' as following:\r\n\r\n```zdb -e -p /tmp -u lustre-ost1/ost1```\r\n\r\nwhere 'ost1' is a dataset within zfs pool 'lustre-ost1'.\r\n\r\nThe information of zfs pool is as follows:\r\n```\r\n[root@centos7 tests]# zpool list lustre-ost1 -v\r\nNAME   SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT\r\nlustre-ost1   384M  1.92M   382M         -     1%     0%  1.00x  ONLINE  -\r\n  /tmp/lustre-ost1   384M  1.92M   382M         -     1%     0%\r\n```\r\n\r\nit doesn't work any more in zfs 0.7.0:\r\n```\r\n[root@centos7 tests]# zdb -e -p /tmp -u lustre-ost1/ost1\r\nfailed to own dataset 'lustre-ost1/ost1': No such file or directory\r\nzdb: can't open 'lustre-ost1/ost1': No such file or directory\r\n```\r\nHowever, it works to check zfs pool directly:\r\n```\r\n[root@centos7 tests]# zdb -e -p /tmp -u lustre-ost1\r\n\r\nUberblock:\r\n\tmagic = 0000000000bab10c\r\n\tversion = 5000\r\n\ttxg = 126\r\n\tguid_sum = 8587760490132740705\r\n\ttimestamp = 1501874937 UTC = Fri Aug  4 12:28:57 2017\r\n\tmmp_magic = 00000000a11cea11\r\n\tmmp_delay = 0\r\n```\r\nThe workaround solution seems to add the zfs pool into cache file, as follows:\r\n```\r\n[root@centos7 tests]# zpool set cachefile=/tmp/zpool.cache lustre-ost1\r\n[root@centos7 tests]# zdb -U /tmp/zpool.cache -u lustre-ost1/ost1\r\nDataset lustre-ost1/ost1 [ZPL], ID 74, cr_txg 6, 1.79M, 224 objects\r\n```\r\n\r\nSince this is a not designed change, I would suppose this would be a bug.\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Centos\r\nDistribution Version    | 7.3\r\nLinux Kernel                 | 3.10.0-514.16.1.el7_lustre.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | zfs-0.7.0-13-g1e1c398\r\nSPL Version                  | spl-0.7.0-6-g6ecfd2b\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6464/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38775177", "body": "Sure, I will get rid of this change on the next revision. Meanwhile, please inspect the rest of this patch if you've got some time. Thanks.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38775177/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ITWOI": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6459", "title": "Potential data races when accessing variable di.zerr in function zfs_show_diffs", "body": "\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | \r\nDistribution Version    | \r\nLinux Kernel                 | \r\nArchitecture                 | \r\nZFS Version                  |  master branch\r\nSPL Version                  |  master branch\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nHi all,\r\n\r\nOur bug scanner has reported a data race issue at [libzfs_diff.c#L461](https://github.com/zfsonlinux/zfs/blob/master/lib/libzfs/libzfs_diff.c#L461) (or [libzfs_diff.c#L446](https://github.com/zfsonlinux/zfs/blob/master/lib/libzfs/libzfs_diff.c#L446) ) and [libzfs_diff.c#L806.](https://github.com/zfsonlinux/zfs/blob/master/lib/libzfs/libzfs_diff.c#L806) \r\n\r\nFollowings are code snippets.\r\n\r\n```c++\r\nstatic void *\r\ndiffer(void *arg)\r\n{\r\n    ...\r\n    if (rv < 0 || (rv == 0 && len != sizeof (dr))) {\r\n        di->zerr = EPIPE;\r\n        break;\r\n    }\r\n    ...\r\n    default:\r\n        di->zerr = EPIPE;\r\n        break;\r\n```\r\n\r\nand \r\n\r\n```c++\r\nint\r\nzfs_show_diffs(zfs_handle_t *zhp, int outfd, const char *fromsnap,\r\n    const char *tosnap, int flags)\r\n{\r\n    ...\r\n        iocerr = ioctl(zhp->zfs_hdl->libzfs_fd, ZFS_IOC_DIFF, &zc);\r\n\tif (iocerr != 0) {\r\n            ...\r\n\t    if (errno == EPERM) {\r\n\t\t...\r\n\t    } else if (errno == EXDEV) {\r\n\t\t...\r\n\t    } else if (errno != EPIPE || di.zerr == 0) {\r\n\t\tzfs_error_aux(zhp->zfs_hdl, strerror(errno));\r\n\t    }\r\n    ...\r\n```\r\n\r\nThe call trace is as follows:\r\n\r\nzfs_show_diffs->pthread_create(&tid, NULL, differ, &di)->di->zerr = EPIPE;\r\n\r\nand\r\n\r\nzfs_show_diffs->errno != EPIPE || di.zerr == 0;\r\n\r\n\r\nSourceBrella Inc.,\r\nYu\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6459/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wangdbang": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6458", "title": "zfs 0.7.0 release version created pool failed with libudev-147", "body": "Hi all\uff0c\r\n\r\n    I tested zfs 0.7.0 release version with libudev-147, and sure it used libudev, \r\nWhen i tested it with command \"zpool create -f pool00 raidz sdb sdc sdd sde\" to create a pool,\r\nit failed with \"cannot label 'sdb': failed to detect device partitions on '/dev/sdb1': 19\", \r\nI read the code, and guess that caused by the udev related functions, am i right? \r\nwhich version of libudev should use?", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6458/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "redmop": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6455", "title": "Kernel hang on reboot with other mounted filesystems", "body": "I have /boot and /boot/efi mounted in fstab, and if I reboot or halt the server while they are mounted, the kernel hangs right before finishing the shutdown. If I \"umount /boot/efi /boot\" then restart, it works perfectly. This does not happen with the same install not on zfs.\r\n\r\n### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian/Proxmox\r\nDistribution Version    | 8.9/4.4\r\nLinux Kernel                 | 4.4.67-1-pve\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.9-pve15 (0.6.5.9-1)\r\nSPL Version                  | 0.6.5.9-pve8 (0.6.5.9-1)\r\n\r\n### Describe the problem you're observing\r\nWhen something not-zfs is mounted in fstab (/boot and /boot/efi* in this case) the server does not restart or shutdown properly, and gives a kernel dump.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\n```\r\ncat /etc/fstab\r\n# <file system> <mount point> <type> <options> <dump> <pass>\r\n/dev/zvol/rpool/swap none swap sw 0 0\r\nproc /proc proc defaults 0 0\r\n\r\n/dev/md0        /boot           ext4    defaults        0 0\r\n/dev/sda3       /boot/efi       vfat    defaults        0 0\r\n/dev/sdb3       /boot/efi2      vfat    defaults        0 0\r\n/dev/sdc3       /boot/efi3      vfat    defaults        0 0\r\n/dev/sdd3       /boot/efi4      vfat    defaults        0 0\r\n```\r\n```\r\n cat /proc/mdstat\r\nPersonalities : [raid1]\r\nmd0 : active raid1 sda4[0] sdd4[3] sdc4[2] sdb4[1]\r\n      1047552 blocks super 1.2 [4/4] [UUUU]\r\n\r\nunused devices: <none>\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6455/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "quentinleburel": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6453", "title": "Read performance in 0.7.0", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS\r\nDistribution Version    | 7.3\r\nLinux Kernel                 | 3.10.0-514.26.2.el7\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.0-1\r\nSPL Version                  | 0.7.0-1\r\n\r\n### Describe the problem you're observing\r\nRead performance seems much slower in 0.7.0 compared to 0.6.5.11\r\n\r\n### Describe how to reproduce the problem\r\nI have a RAIDz2 array of 10 disks, created with the following commands:\r\n```\r\nzpool create -f tank raidz2 disk{0..9}\r\nzfs set recordsize=1M tank\r\nzfs set atime=off tank\r\nzfs set xattr=sa tank\r\n```\r\n\r\nzfs,conf has some minor changes:\r\n```\r\noptions zfs zfs_vdev_async_write_max_active=15\r\noptions zfs zfs_vdev_async_write_min_active=5\r\noptions zfs zfs_dirty_data_max=16863235072\r\noptions zfs zfs_dirty_data_max_max=33726470144\r\n```\r\n\r\nTesting is done using iozone:\r\n```\r\niozone -t 10 -s 24G -r 1024k -i 0 -i 1 -e -+n -+m /root/clientlist.tank\r\n```\r\n\r\nResults in 0.6.5.11:\r\n```\r\nChildren see throughput for 10 initial writers  = 1167944.82 kB/sec\r\nChildren see throughput for 10 readers          = 1195458.09 kB/sec\r\n```\r\n\r\nResults in 0.7.0:\r\n```\r\nChildren see throughput for 10 initial writers  =  942196.48 kB/sec\r\nChildren see throughput for 10 readers          =  711960.76 kB/sec\r\n```\r\n\r\nAre there any parameters to adjust in order to get similar perfs as in the old version ?\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6453/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dinatale2": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6446", "title": "Test case: rsend_020_pos | rsend_021_pos", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | All\r\nDistribution Version    | All\r\nLinux Kernel                 | All\r\nArchitecture                 | All\r\nZFS Version                  | 0.7.0 (master)\r\nSPL Version                  | 0.7.0 (master)\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n`rsend_020_pos` and `rsend_021_pos` appear to occasionally hang causing zfs tests to be killed by the buildbot.\r\n\r\n### Describe how to reproduce the problem\r\nBuildbot.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nhttp://build.zfsonlinux.org/builders/Ubuntu%2016.04%20x86_64%20%28TEST%29/builds/241/steps/shell_8/logs/stdio\r\n```\r\nTest: /usr/share/zfs/zfs-tests/tests/functional/rsend/rsend_013_pos (run as root) [00:22] [PASS]\r\nTest: /usr/share/zfs/zfs-tests/tests/functional/rsend/rsend_014_pos (run as root) [00:09] [PASS]\r\nTest: /usr/share/zfs/zfs-tests/tests/functional/rsend/rsend_019_pos (run as root) [00:00] [SKIP]\r\n\r\ncommand timed out: 4200 seconds without output running ['runurl', 'https://raw.githubusercontent.com/zfsonlinux/zfs-buildbot/master/scripts/bb-test-zfstests.sh'], attempting to kill\r\n/usr/share/zfs/zfs-tests.sh: line 64:  7808 Terminated              ${TEST_RUNNER} ${QUIET} -c \"${RUNFILE}\" -i \"${STF_SUITE}\"\r\nTerminated\r\numount: /testpool/stream: target is busy\r\n        (In some cases useful info about processes that\r\n         use the device is found by lsof(8) or fuser(1).)\r\numount: /testpool: target is busy\r\n        (In some cases useful info about processes that\r\n         use the device is found by lsof(8) or fuser(1).)\r\numount: /testpool/stream: target is busy\r\n        (In some cases useful info about processes that\r\n         use the device is found by lsof(8) or fuser(1).)\r\numount: /testpool: target is busy\r\n        (In some cases useful info about processes that\r\n         use the device is found by lsof(8) or fuser(1).)\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6446/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6416", "title": "Test case: zfs_rollback_002_pos", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character -->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Amazon Linux\r\nDistribution Version    | \r\nLinux Kernel                 | \r\nArchitecture                 | x86\r\nZFS Version                  | 0.7.0-rc5\r\nSPL Version                  | 0.7.0-rc5\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n`zfs_rollback_002_pos` occasionally fails with the error `ERROR: ismounted /var/tmp/testdir exited 1` on the Amazon Linux builder. This usually occurs when the test right before it (`zfs_rollback_001_pos`) also fails.\r\n\r\n### Describe how to reproduce the problem\r\nReproduced by the Amazon Linux builder in buildbot.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nhttp://build.zfsonlinux.org/builders/Amazon%202015.09%20x86_64%20Release%20%28TEST%29/builds/3348/steps/shell_8/logs/log\r\n```\r\nTest: /usr/share/zfs/zfs-tests/tests/functional/cli_root/zfs_rollback/zfs_rollback_002_pos (run as root) [00:00] [FAIL]\r\n06:55:20.06 ASSERTION: 'zfs rollback -f' will force unmount any filesystems.\r\n06:55:20.15 SUCCESS: eval echo y | \t\t\tnewfs -v /dev/zvol/testpool/testvol > /dev/null 2>&1\r\n06:55:20.17 SUCCESS: mount /dev/zvol/testpool/testvol /var/tmp/testdir1\r\n06:55:20.17 SUCCESS: cp /etc/passwd /var/tmp/testdir/testfile0\r\n06:55:20.18 SUCCESS: sync\r\n06:55:20.19 SUCCESS: zfs snapshot testpool/testfs@testsnap\r\n06:55:20.20 SUCCESS: cp /etc/passwd /var/tmp/testdir1/testfile0\r\n06:55:20.21 SUCCESS: sync\r\n06:55:20.22 SUCCESS: zfs snapshot testpool/testvol@testsnap\r\n06:55:20.22 \r\n06:55:20.22 ERROR: ismounted /var/tmp/testdir exited 1\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6416/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6415", "title": "Test case: zfs_rollback_001_pos", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Amazon Linux\r\nDistribution Version    | \r\nLinux Kernel                 | \r\nArchitecture                 | x86\r\nZFS Version                  | 0.7.0-rc5\r\nSPL Version                  | 0.7.0-rc5\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n`zfs_rollback_001_pos` occasionally fails with the error `filesystem 'testpool/testfs' is already mounted cannot mount 'testpool/testfs': mountpoint or dataset is busy filesystem successfully created, but not mounted` on the Amazon Linux builder.\r\n\r\n### Describe how to reproduce the problem\r\nReproduced by the Amazon Linux builder in buildbot.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nhttp://build.zfsonlinux.org/builders/Amazon%202015.09%20x86_64%20Release%20%28TEST%29/builds/3348/steps/shell_8/logs/log\r\n```\r\nTest: /usr/share/zfs/zfs-tests/tests/functional/cli_root/zfs_rollback/zfs_rollback_001_pos (run as root) [00:00] [FAIL]\r\n06:55:19.06 ASSERTION: 'zfs rollback -r|-rf|-R|-Rf' will recursively destroy any  snapshots more recent than the one specified.\r\n06:55:19.06 NOTE: Create 3 snapshots, rollback to the 2nd snapshot  using -r.\r\n06:55:19.11 SUCCESS: zfs destroy -Rf testpool/testfs\r\n06:55:19.12 SUCCESS: zfs destroy -Rf testpool/testvol\r\n06:55:19.14 SUCCESS: zfs create testpool/testfs\r\n06:55:19.18 SUCCESS: zfs set mountpoint=/var/tmp/testdir testpool/testfs\r\n06:55:19.20 SUCCESS: zfs create -V 150m testpool/testvol\r\n06:55:19.51 SUCCESS: eval echo y | \t\t\tnewfs -v /dev/zvol/testpool/testvol > /dev/null 2>&1\r\n06:55:19.52 SUCCESS: mount /dev/zvol/testpool/testvol /var/tmp/testdir1\r\n06:55:19.53 SUCCESS: cp /etc/passwd /var/tmp/testdir/testfile0\r\n06:55:19.55 SUCCESS: sync\r\n06:55:19.56 SUCCESS: zfs snapshot testpool/testfs@testsnap\r\n06:55:19.56 SUCCESS: cp /etc/passwd /var/tmp/testdir/testfile1\r\n06:55:19.57 SUCCESS: sync\r\n06:55:19.58 SUCCESS: zfs snapshot testpool/testfs@testsnap1\r\n06:55:19.58 SUCCESS: cp /etc/passwd /var/tmp/testdir/testfile2\r\n06:55:19.59 SUCCESS: sync\r\n06:55:19.60 SUCCESS: zfs snapshot testpool/testfs@testsnap2\r\n06:55:19.61 SUCCESS: cp /etc/passwd /var/tmp/testdir1/testfile0\r\n06:55:19.61 SUCCESS: sync\r\n06:55:19.62 SUCCESS: zfs snapshot testpool/testvol@testsnap\r\n06:55:19.63 SUCCESS: cp /etc/passwd /var/tmp/testdir1/testfile1\r\n06:55:19.63 SUCCESS: sync\r\n06:55:19.64 SUCCESS: zfs snapshot testpool/testvol@testsnap1\r\n06:55:19.65 SUCCESS: cp /etc/passwd /var/tmp/testdir1/testfile2\r\n06:55:19.65 SUCCESS: sync\r\n06:55:19.66 SUCCESS: zfs snapshot testpool/testvol@testsnap2\r\n06:55:19.68 SUCCESS: zfs rollback -r testpool/testfs@testsnap1\r\n06:55:19.68 SUCCESS: datasetexists testpool/testfs@testsnap\r\n06:55:19.69 SUCCESS: datasetexists testpool/testfs@testsnap1\r\n06:55:19.69 SUCCESS: datasetnonexists testpool/testfs@testsnap2\r\n06:55:19.70 SUCCESS: datasetnonexists testpool/testfstestclone\r\n06:55:19.70 SUCCESS: datasetnonexists testpool/testfstestclone1\r\n06:55:19.70 SUCCESS: datasetnonexists testpool/testfstestclone2\r\n06:55:19.71 SUCCESS: files_exist /var/tmp/testdir/testfile0 /var/tmp/testdir/testfile1\r\n06:55:19.71 SUCCESS: files_nonexist /var/tmp/testdir/testfile2\r\n06:55:19.75 SUCCESS: umount -f /var/tmp/testdir1\r\n06:55:19.77 SUCCESS: zfs rollback -r testpool/testvol@testsnap1\r\n06:55:19.79 SUCCESS: mount /dev/zvol/testpool/testvol /var/tmp/testdir1\r\n06:55:19.79 SUCCESS: datasetexists testpool/testvol@testsnap\r\n06:55:19.80 SUCCESS: datasetexists testpool/testvol@testsnap1\r\n06:55:19.80 SUCCESS: datasetnonexists testpool/testvol@testsnap2\r\n06:55:19.81 SUCCESS: datasetnonexists testpool/testvoltestclone\r\n06:55:19.81 SUCCESS: datasetnonexists testpool/testvoltestclone1\r\n06:55:19.81 SUCCESS: datasetnonexists testpool/testvoltestclone2\r\n06:55:19.82 SUCCESS: files_exist /var/tmp/testdir1/testfile0 /var/tmp/testdir1/testfile1\r\n06:55:19.82 SUCCESS: files_nonexist /var/tmp/testdir1/testfile2\r\n06:55:19.83 NOTE: Create 3 snapshots and rollback to the 1st snapshot  using -r.\r\n06:55:19.87 SUCCESS: zfs destroy -Rf testpool/testfs\r\n06:55:19.90 SUCCESS: umount -f /var/tmp/testdir1\r\n06:55:19.92 SUCCESS: zfs destroy -Rf testpool/testvol\r\n06:55:19.95 filesystem 'testpool/testfs' is already mounted cannot mount 'testpool/testfs': mountpoint or dataset is busy filesystem successfully created, but not mounted\r\n06:55:19.95 ERROR: zfs create testpool/testfs exited 1\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6415/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6406", "title": "Test case: mmp_active_import", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | All\r\nDistribution Version    | All\r\nLinux Kernel                 | All\r\nArchitecture                 | All\r\nZFS Version                  | 0.7.0-rc5\r\nSPL Version                  | 0.7.0-rc5\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nmmp_active_import occasionally fails on buildbot builders. `pgrep` exits with a return code of 1 indicating that `ztest` stopped running.\r\n\r\n### Describe how to reproduce the problem\r\nReproducible by buildbot.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\nhttp://build.zfsonlinux.org/builders/Ubuntu%2014.04%20i686%20%28TEST%29/builds/3095/steps/shell_8/logs/log\r\n\r\n```\r\nTest: /usr/share/zfs/zfs-tests/tests/functional/mmp/mmp_active_import (run as root) [00:19] [FAIL]\r\n18:47:28.32 ASSERTION: multihost=on|off active pool activity checks\r\n18:47:28.35 SUCCESS: mkdir -p /var/tmp/mmp\r\n18:47:28.36 SUCCESS: truncate -s 268435456 /var/tmp/mmp/vdev1 /var/tmp/mmp/vdev2\r\n18:47:28.37 SUCCESS: mmp_clear_hostid\r\n18:47:28.39 SUCCESS: mmp_set_hostid 01234567\r\n18:47:28.56 SUCCESS: zpool create -f mmppool mirror /var/tmp/mmp/vdev1 /var/tmp/mmp/vdev2\r\n18:47:28.63 SUCCESS: zpool set multihost=on mmppool\r\n18:47:28.71 SUCCESS: zpool export mmppool\r\n18:47:28.73 SUCCESS: mmp_clear_hostid\r\n18:47:28.75 SUCCESS: mmp_set_hostid 89abcdef\r\n18:47:28.75 NOTE: Starting ztest in the background as hostid 01234567\r\n18:47:28.76 SUCCESS: eval ZFS_HOSTID=01234567 ztest -T120 -M -k0 -f /var/tmp/mmp -E -p mmppool >/dev/null 2>&1 &\r\n18:47:28.91 14636\r\n18:47:28.91 14640\r\n18:47:28.92 SUCCESS: pgrep ztest\r\n18:47:33.61 SUCCESS: sleep 5\r\n18:47:47.12 ERROR: pgrep ztest exited 1\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6406/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6194", "title": "Test case: zpool_create_024_pos (32-bit only)", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | All\r\nDistribution Version    | All\r\nLinux Kernel                 | All\r\nArchitecture                 | All\r\nZFS Version                  | 0.7.0-rc4\r\nSPL Version                  | 0.7.0-rc4\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nzpool_create_024_pos is occasionally killed due to timeout during automated testing.\r\n\r\n### Describe how to reproduce the problem\r\nReproducible by buildbot.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nhttp://build.zfsonlinux.org/builders/Ubuntu%2014.04%20i686%20%28TEST%29/builds/2252\r\n```\r\nTest: /usr/share/zfs/zfs-tests/tests/functional/cli_root/zpool_create/zpool_create_024_pos (run as root) [11:24] [KILLED]\r\n21:54:11.98 ASSERTION: Many 'zpool create' and 'zpool destroy' must succeed concurrently.\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6194/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6193", "title": "Test case: zvol_misc_002_pos (32-bit only)", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character -->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | All\r\nDistribution Version    | All\r\nLinux Kernel                 | All\r\nArchitecture                 | All\r\nZFS Version                  | 0.7.0-rc4\r\nSPL Version                  | 0.7.0-rc4\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nzvol_misc_002_pos is occasionally killed due to timeout during automated testing.\r\n\r\n### Describe how to reproduce the problem\r\nReproducible by buildbot.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nhttp://build.zfsonlinux.org/builders/Ubuntu%2014.04%20i686%20%28TEST%29/builds/2264/\r\n```\r\nTest: /usr/share/zfs/zfs-tests/tests/functional/zvol/zvol_misc/zvol_misc_002_pos (run as root) [10:00] [KILLED]\r\n20:22:10.81 ASSERTION: Verify that ZFS volume snapshot could be fscked\r\n20:22:10.87 SUCCESS: zfs set volsize=128m testpool/testvol\r\n20:22:11.18 SUCCESS: mkdir /var/tmp/testdir\r\n20:22:11.21 SUCCESS: mount /dev/zvol/testpool/testvol /var/tmp/testdir\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6193/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/89a66a0457cd392ab8c6ad6d9c138fedaa425067", "message": "Handle broken pipes in arc_summary\n\nUsing a command similar to 'arc_summary.py | head' causes\r\na broken pipe exception. Gracefully exit in the case of a\r\nbroken pipe in arc_summary.py.\r\n\r\nReviewed-by: Richard Elling <Richard.Elling@RichardElling.com>\r\nReviewed-by: loli10K <ezomori.nozomu@gmail.com>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6965 \r\nCloses #6969"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/eef005d882dfaa2ffddfeea5d70ff27aadee344a", "message": "Refresh TEST file to include new variables\n\nAdd any missing variables for CI control to TEST.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6846"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/87fbf4363631abb513f43ff4c02655828e8b561d", "message": "Provide tags in perf-regression.run\n\nA prior commit changed test-runner to enable tagging\r\nof testgroups within a test suite runfile. They must\r\nbe specified in each runfile. Update the runfile for\r\nperformance regressions so it is properly tagged.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6830"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/9a810efb02a646c59cd69e639d057022ccf3a9a9", "message": "Allow test-runner to filter test groups by tag\n\nEnable test-runner to accept a list of tags to identify\r\nwhich test groups the user wishes to run.\r\n\r\nAlso allow test-runner to perform multiple iterations\r\nof a test run.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: John Wren Kennedy <john.kennedy@delphix.com>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6788"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/a94d38c0f382c16244912de83a7356ae35e63322", "message": "Correct make mancheck recipe\n\nThe current make recipe for mancheck silently ignores errors. Correct\r\nthe recipe so errors cause the mancheck recipe fail.\r\n\r\nThe zpool reopen command in the zpool.8 manpage had a bullet list\r\nwithout an .El.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: loli10K <ezomori.nozomu@gmail.com>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6790"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/69b229bd60135838d606f5da41831d409d11de2d", "message": "commitcheck: Multiple OpenZFS ports in commit\n\nAllow commitcheck.sh to handle multiple OpenZFS ports in\r\na single commit. This is useful in the cases when a change\r\nupstream has bug fixes and it makes sense to port them with\r\nthe original patch.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Chris Dunlop <chris@onthe.net.au>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6780"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/8dcaf243d77b21c49092a4ffd3a641ac3407217c", "message": "Add Coverity defect fix commit checker support\n\nEnable commitcheck.sh to test if a commit message is\r\nin the expected format for a coverity defect fix.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6777"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/64b8c58e3e49dcca107c2f53dd5cc7208ee6f405", "message": "Ensure arc_size_break is filled in arc_summary.py\n\nUse mfu_size and mru_size pulled from the arcstats\r\nkstat file to calculate the mfu and mru percentages\r\nfor arc size breakdown.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Richard Elling <Richard.Elling@RichardElling.com>\r\nReviewed-by: AndCycle <andcycle@andcycle.idv.tw>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #5526 \r\nCloses #6770"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/63e5e960bad6f6c7d8eeb8273988ec7fd10f0a60", "message": "Correct flake8 errors after STYLE builder update\n\nFix new flake8 errors related to bare excepts and ambiguous\r\nvariable names due to a STYLE builder update.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6776"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/70c8a79446ce74db93f492312f4ce0a825c4dbf5", "message": "Provide commit message format for Coverity defects\n\nProvide details about the commit message format for Coverity defect\r\nfixes submitted.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6771"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/34d00e7aba13076e47dbd1791bd955c1e2f7b0c9", "message": "Correct cppcheck errors\n\nZFS buildbot STYLE builder was moved to Ubuntu 17.04\r\nwhich has a newer version of cppcheck. Handle the\r\nnew cppcheck errors.\r\n\r\nuu_* functions removed in this commit were unused\r\nand effectively dead code. They are now retired.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6653"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/ea49beba66106dcb272b43323fad147df1df13fd", "message": "Correct shellcheck errors\n\nThe ZFS buildbot moved to using Ubuntu 17.04 for the\r\nSTYLE builder which has a newer version of shellcheck.\r\nCorrect the new issues it discovers.\r\n\r\nReviewed-by: loli10K <ezomori.nozomu@gmail.com>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6647"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/787acae0b5cd139ea0f9fa60558cca28d4673b23", "message": "Linux 3.14 compat: IO acct, global_page_state, etc\n\ngeneric_start_io_acct/generic_end_io_acct in the master\r\nbranch of the linux kernel requires that the request_queue\r\nbe provided.\r\n\r\nMove the logic from freemem in the spl to arc_free_memory\r\nin arc.c. Do this so we can take advantage of global_page_state\r\ninterface checks in zfs.\r\n\r\nUpstream kernel replaced struct block_device with\r\nstruct gendisk in struct bio. Determine if the\r\nfunction bio_set_dev exists during configure\r\nand have zfs use that if it exists.\r\n\r\nbio_set_dev https://github.com/torvalds/linux/commit/74d4699\r\nglobal_node_page_state https://github.com/torvalds/linux/commit/75ef718\r\nio acct https://github.com/torvalds/linux/commit/d62e26b\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6635"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d7323e79a61fa7c6dd0b4bbcf4e5658c4fbea3dd", "message": "OpenZFS 8547 - update mandoc to 1.14.3\n\nAuthored by: Yuri Pankov <yuri.pankov@nexenta.com>\r\nReviewed by: Robert Mustacchi <rm@joyent.com>\r\nApproved by: Richard Lowe <richlowe@richlowe.net>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nPorted-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\n\r\nOpenZFS-issue: https://www.illumos.org/issues/8547\r\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/c66b804\r\nCloses #6549"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/4334df53539c280f847350cb5b11180993229ef9", "message": "Disable rsend_024_pos\n\nThe test case frequently hangs on buildbot\r\nTEST builders. Disable it for now.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6487"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/e3bdcb8ad88e7a90726193c9afe96a713729c928", "message": "Retry zfs destroy when busy in rsend tests\n\nrsend tests in the test suite frequently create and\r\ndestroy datasets. It is possible for zfs destroy to\r\nreturn an error code indicating the dataset is busy.\r\nSimply use a log_must_busy in these cases to retry\r\ndestroying those datasets. Other fixes to rsend test\r\ncases to avoid unmounting and remounting filesystems\r\nand some cleanup.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6418"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/af0f842883ceb7e88b8f4d1fe6ad80b6ad951827", "message": "mmp_on_uberblocks: Use kstat for uberblock counts\n\nUse kstat to get a more accurate count of uberblock updates.\r\nUsing a loop with zdb can potentially miss some uberblocks.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6407 \r\nCloses #6419"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/c1dd2f783a30d001f79648dab5ae5bbfe5614759", "message": "Disable zfs_send_007_pos\n\nTest case zfs_send_007_pos regularly is killed\r\nby test-runner during zfs-tests on buildbot. Disable\r\nit for now until further investigation can be done.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6422"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/bff245dd342f7318bfa88e471e584c2c46f8c4db", "message": "OpenZFS 8508 - Mounting a zpool on 32-bit platforms panics\n\nAuthored by: Justin Hibbits <chmeeedalf@gmail.com>\r\nReviewed by: Matt Ahrens <mahrens@delphix.com>\r\nApproved by: Dan McDonald <danmcd@joyent.com>\r\nReviewed-by: Tony Hutter <hutter2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nPorted-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\n\r\nOpenZFS-issue: https://www.illumos.org/issues/8508\r\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/15fc257\r\nCloses #6404"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/f6837d9b53d44547d640040d5db9e7d152664d4f", "message": "Increase delay for zed log in events tests\n\nIn zed event test cases, a brief delay was introduced\r\nto allow for events to make it to the zed log. On at least\r\none buildbot builder, the 1 second delay is not long enough.\r\nTherefore, increasing the delay should ensure the zed has\r\nmore than enough time to write to its log.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6395"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d6bcf7ff5e97df3195d34269b1b72952b4a00778", "message": "Restrict zpool iostat/status -c to search path\n\nzpool iostat/status -c is supposed to be restricted\r\nby its search path, but currently isn't. To prevent\r\narbitrary scripts from being executed, disallow '/'\r\nfrom commands.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Tony Hutter <hutter2@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Ned Bass <bass6@llnl.gov>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6353 \r\nCloses #6359"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/802ae562ed817af978e2636806db1952d29ee86d", "message": "Fix coverity defects: CID 165755\n\nCID 165755: Division or modulo by zero (DIVIDE_BY_ZERO)\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Tony Hutter <hutter2@llnl.gov>\r\nReviewed-by: Olaf Faaland <faaland1@llnl.gov>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6352"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/39554216dff2c8bd4a2fa296644b5e3723ce8ed3", "message": "zfs_mount_001_neg: use log_must_busy in cleanup\n\nUse log_must_busy when destroying the snapshot\r\nand dataset during cleanup in zfs_mount_001_neg.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6382"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/0c656a964da7993847943c438c5abee7f46aa06d", "message": "Disable nbmand tests on kernels w/o support\n\nThis change allows mountpoint_003_pos and send-c_props\r\nto run on Linux kernels that do not support mandatory\r\nlocking. Linux kernel versions greater than or equal to\r\n4.4 no longer support mandatory locking and the test\r\nsuite will now account for that.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Tony Hutter <hutter2@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6346 \r\nCloses #6347 \r\nCloses #6362"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/b24827ac1e327f763a8dd4ed60c44c2a5d918b42", "message": "Exit test-runner with non-zero if tests are KILLED\n\nfe46eeb introduced non-zero exit codes to test-runner.\r\nA non-zero exit code should be returned when test-runner\r\ndecided to kill a test and mark it as KILLED.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nCloses #6325"}], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6906", "title": "Add dbuf hash and dbuf cache kstats", "body": "### Description\r\n<!--- Describe your changes in detail -->\r\nIntroduce kstats about the dbuf hash and dbuf cache\r\nto make it easier to inspect state. This should help\r\nwith debugging and understanding of these portions\r\nof the codebase.\r\n\r\nCorrect format of dbuf kstat file.\r\n\r\nIntroduce a dbc column to dbufs kstat to indicate if\r\na dbuf is in the dbuf cache.\r\n\r\nIntroduce field filtering in the dbufstat python script.\r\n\r\nI will also be introducing some basic test cases to test the new dbufstats kstat and other basic scenarios.\r\n\r\n<!--- Provide a general summary of your changes in the Title above -->\r\n\r\n<!---\r\nDocumentation on ZFS Buildbot options can be found at\r\nhttps://github.com/zfsonlinux/zfs/wiki/Buildbot-Options\r\n-->\r\n\r\n### Motivation and Context\r\n<!--- Why is this change required? What problem does it solve? -->\r\n<!--- If it fixes an open issue, please link to the issue here. -->\r\nGain a better understanding how dbufs are cached and provide another useful tool for users/developer.\r\n\r\n### How Has This Been Tested?\r\n<!--- Please describe in detail how you tested your changes. -->\r\n<!--- Include details of your testing environment, and the tests you ran to -->\r\n<!--- see how your change affects other areas of the code, etc. -->\r\n<!--- If your change is a performance enhancement, please provide benchmarks here. -->\r\nLocally on a VM.\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [x] I have updated the documentation accordingly.\r\n- [x] I have read the **CONTRIBUTING** document.\r\n- [x] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [x] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6294", "title": "Enforce request limits on zvols", "body": "<!--- Provide a general summary of your changes in the Title above -->\r\n\r\n<!---\r\nDocumentation on ZFS Buildbot options can be found at\r\nhttps://github.com/zfsonlinux/zfs/wiki/Buildbot-Options\r\n-->\r\n\r\n### Description\r\nCurrently, zvols do not handle heavy random IO\r\nworkloads. zvols should limit the number of outstanding\r\nin-flight IO requests. This should improve performance.\r\n\r\n### Motivation and Context\r\n<!--- Why is this change required? What problem does it solve? -->\r\n<!--- If it fixes an open issue, please link to the issue here. -->\r\n#6127 \r\n#6278 \r\n\r\n### How Has This Been Tested?\r\nBuilds on my VM. Buildbot will help me test. Hoping to test on hardware soon.\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [x] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [x] I have updated the documentation accordingly.\r\n- [x] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [x] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nagisa": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6427", "title": "Creating a QEMU VM will fail with OOM when it could use memory used by ARC", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | nixos\r\nDistribution Version    | unstable\r\nLinux Kernel                 | 4.12\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.0-rc4\r\nSPL Version                  | 0.7.0-rc4\r\n\r\n### Describe the problem you're observing\r\n\r\nConsider a system with 16GB of RAM. ARC on such system is some 7.8G in size. Starting a QEMU VM with 10GB of RAM will fail with an error indicating allocation failure:\r\n\r\n```\r\n$ qemu-system-x86_64 -m 10000\r\nqemu-system-x86_64: cannot set up guest memory 'pc.ram': Cannot allocate memory\r\n```\r\n\r\nTo make this work, it is necessary to make ARC smaller, for which it is necessary to use incremental pressure (AFAICT anyway):\r\n\r\n```\r\n$ python -c 'i = [bytearray(11*1024*1024) for i in range(1024)]'\r\n$ qemu-system-x86_64 -m 10000\r\n// VM runs\r\n```\r\n\r\nThis does not happen on the traditional filesystems where the cache is accounted for in buff/cache. Furthermore, if the VM doesn\u2019t actually touch the memory, it will not even evict the caches.\r\n\r\n### Describe how to reproduce the problem\r\n\r\n* Fill ARC.\r\n* Run `qemu-system-x86_64 -m (freemem+some_arc)`", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6427/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dcrdev": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6413", "title": "0.7.0 Update kABI-tracking kmod Weak Modules", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS\r\nDistribution Version    | 7.3\r\nLinux Kernel                 | 3.10.0-514.26.2\r\nArchitecture                 | x86_64\r\nZFS Version                  | zfs-0.7.0-1\r\nSPL Version                  | spl-0.7.0-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nWhilst updating my system - I received several 'Permission Denied' messages whilst the post trans scripts were being run:\r\n\r\n```/sbin/weak-modules: line 174: /tmp/weak-modules.CWPkux/old_initramfs/./usr/lib/modules/3.10.0-514.26.2.el7.x86_64/extra/spl/spl/spl.ko: Permission denied\r\n/sbin/weak-modules: line 174: /tmp/weak-modules.CWPkux/old_initramfs/./usr/lib/modules/3.10.0-514.26.2.el7.x86_64/extra/spl/spl/spl.ko: Permission denied\r\n```\r\nThis results in an unbootable system - CentOS is on a ZFS root.\r\n\r\n### Describe how to reproduce the problem\r\nStart with a CentOS 7.3 install on a ZFS root, running ZoL 0.6.x - upgrade the system and receive the above errors.\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6413/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Low-power": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6391", "title": "Files created by ZoL have an empty NFSv4 ACL", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian GNU/Linux\r\nDistribution Version    | 8\r\nLinux Kernel                 | 3.16.0-4-686-pae\r\nArchitecture                 | i386\r\nZFS Version                  | 0.6.5.11\r\nSPL Version                  | 0.6.5.11\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nI known that the NFSv4 ACL for ZoL in currently unimplemented due to the leak of user land tools for Linux, but files on ZFS having empty ACL cases problem when interact with other platforms that support NFSv4 ACL for ZFS.\r\n\r\n### Describe how to reproduce the problem\r\nPut a file on a pool, and export this pool on Linux\r\nImport the pool on a Solaris OS (the pool is created with an version that both OpenZFS and Solaris supports)\r\nLooking the file by an non-root user\r\n```\r\n[whr@WHRsWorkPC]:[25]:[/mnt/1]:$ ls -l todo\r\n-rw-r--r-- 1 root root 11 2017-07-23 18:38 todo\r\n```\r\nLooks like I have the reading permission to this file, but...\r\n```\r\n[whr@WHRsWorkPC]:[26]:[/mnt/1]:$ cat todo\r\ncat: todo: Permission denied\r\n```\r\nBecause the file have empty ACL\r\n```\r\n[whr@WHRsWorkPC]:[27]:[/mnt/1]:$ /usr/bin/ls -lv todo\r\nls: can't read ACL on todo: Permission denied\r\n-rw-r--r--   1 root     root          11  7\u6708 23 18:38 todo\r\n[whr@WHRsWorkPC]:[28]:[/mnt/1]:$ sudo /usr/bin/ls -lv todo\r\n\u5bc6\u7801\uff1a\r\n-rw-r--r--   1 root     root          11  7\u6708 23 18:38 todo\r\n[whr@WHRsWorkPC]:[29]:[/mnt/1]:$ \r\n```\r\nCreate a default ACL from permissions mode for this file by using chmod command will fix this\r\n```\r\n[whr@WHRsWorkPC]:[29]:[/mnt/1]:$ sudo chmod 644 todo\r\n[whr@WHRsWorkPC]:[30]:[/mnt/1]:$ /usr/bin/ls -lv todo\r\n-rw-r--r--   1 root     root          11  7\u6708 23 18:38 todo\r\n     0:owner@:read_data/write_data/append_data/read_xattr/write_xattr\r\n         /read_attributes/write_attributes/read_acl/write_acl/write_owner\r\n         /synchronize:allow\r\n     1:group@:read_data/read_xattr/read_attributes/read_acl/synchronize:allow\r\n     2:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize\r\n         :allow\r\n[whr@WHRsWorkPC]:[31]:[/mnt/1]:$ cat todo\r\nReport bug\r\n[whr@WHRsWorkPC]:[32]:[/mnt/1]:$ \r\n```\r\n\r\n\r\nI think ZoL should add a module parameter or a property to turn on a behavior, that create the NFSv4 ACL from corresponding file mode when chmod(2) or creat(2) files, even this ACL is currently meaningless on Linux.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6391/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "batchuravi": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6381", "title": "OOM panic due to external fragmentation with zfs 0.7", "body": "We are seeing panic on oom after running a workload consisting of an av scan and light client \r\n workload for a couple of hours. \r\n\r\nWe are running with 32G memory. Test creates about 1 millions files spread accross 5 pools. After that test queries file attributes, change attributes, read the file and rewrites the first 4k bytes of the file.\r\n\r\nFrom the stack trace (below) it looks like fork cannot succeed because there is no contiguous 16k free chunk.\r\n\r\n```\r\ncrash> bt\r\nPID: 12218  TASK: ffff88010571ea40  CPU: 2   COMMAND: \"java\"\r\n #0 [ffff8804cce27910] machine_kexec at ffffffff8105d4f0\r\n #1 [ffff8804cce27980] crash_kexec at ffffffff8110c288\r\n #2 [ffff8804cce27a50] panic at ffffffff811825d9\r\n #3 [ffff8804cce27ad0] check_panic_on_oom at ffffffff81187526\r\n #4 [ffff8804cce27ae0] out_of_memory at ffffffff81187c80\r\n #5 [ffff8804cce27b80] __alloc_pages_slowpath at ffffffff8118d2e6\r\n #6 [ffff8804cce27c70] __alloc_pages_nodemask at ffffffff8118d619\r\n #7 [ffff8804cce27d40] alloc_kmem_pages_node at ffffffff8118e999\r\n #8 [ffff8804cce27d70] dup_task_struct at ffffffff8107ff8e\r\n #9 [ffff8804cce27db0] copy_process at ffffffff81081797\r\n#10 [ffff8804cce27e90] _do_fork at ffffffff81082e5e\r\n#11 [ffff8804cce27f40] sys_clone at ffffffff81083069\r\n#12 [ffff8804cce27f50] entry_SYSCALL_64_fastpath at ffffffff816cf76e\r\n    RIP: 00007fc05c0b2a71  RSP: 00007fc05cdb1428  RFLAGS: 00000206\r\n    RAX: ffffffffffffffda  RBX: 00007fc004402700  RCX: 00007fc05c0b2a71\r\n    RDX: 00007fc0044029d0  RSI: 00007fc004401ff0  RDI: 00000000003d0f00\r\n    RBP: 0000000000000000   R8: 00007fc004402700   R9: 00007fc004402700\r\n    R10: 00007fc0044029d0  R11: 0000000000000206  R12: 00007fc05cdb1500\r\n    R13: 00007fc0044029c0  R14: 0000000000000000  R15: 0000000000000003\r\n    ORIG_RAX: 0000000000000038  CS: 0033  SS: 002b\r\ncrash> \r\n```\r\n\r\nFrom vmcore-dmesg.txt there are no higher order free pages although there is a lot of free memory in solitary 4k pages:\r\n```\r\n[    0.453357] Node 0 DMA: 0*4kB 0*8kB 0*16kB 1*32kB (U) 1*64kB (U) 1*128kB (U) 1*256kB (U) 0*512kB 1*1024kB (U) 1*2048kB (M) 3*4096kB (M) = 15840kB\r\n[    0.453365] Node 0 DMA32: 6002*4kB (UMEH) 5015*8kB (UMEH) 3620*16kB (UMEH) 284*32kB (UME) 54*64kB (UME) 10*128kB (UME) 9*256kB (ME) 2*512kB (M) 0*1024kB 0*2048kB 0*4096kB = 139200kB\r\n[    0.453373] Node 0 Normal: 527308*4kB (UM) 0*8kB 0*16kB 0*32kB 0*64kB 0*128kB 0*256kB 0*512kB 0*1024kB 0*2048kB 0*4096kB = 2109232kB\r\n[    0.453386] Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=2048kB\r\n```\r\nNote that there as an issue (4953) filed last year but that was with an earlier version of ZFS (0.6.5.x) without ABD. The workload at that time did not have any scans. It was metadata intensive. It is possible though that in both cases there might have been a lot of small memory allocations.\r\n\r\nzfs version:        0.7.0-rc3 with (rc3 time frame) trim implementation and some minor changes. \r\nspl version:        0.7.0-rc3\r\nLinux kernel version: 4.4.14\r\nArchitecture:   x86_64\r\nCentos release 6.8\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6381/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "TeraHz": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6380", "title": "zpool stuck at 1/s resilver and zpool offline fails with no valid replicas", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS\r\nDistribution Version    | 7.3.1611\r\nLinux Kernel                 | 3.10.0-514.26.2.el7\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.11-1 from kmod epel 7.3 repo\r\nSPL Version                  | 0.6.5.11-1 from kmod epel 7.3 repo\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nAfter massive disk failure, zpool is stuck at 1/s resilver and `zpool offline` of  \"UNAVAIL\" devices fails with \"cannot offline ...: no valid replicas\" in a raidz2 vdev.\r\n\r\n### Describe how to reproduce the problem\r\nWe have a pool of 6 vdevs with 10drives each at  raidz2. A hardware issue caused 8 of its drives to go offline. Two vdevs lost 3 drives which shut down the pool. We relocated one of the three drives to working ports and we got the pool back to DEGRADED state. The pool thinks it is resilvering but no drives are listed as being resilvered (and no drives were really replaced) and the stats sit on \"1 scanned out of 10.3T at 1/s, (scan is slow, no estimated time)\"\r\n\r\nWe wanted to offline the UNAVAIL drives to possibly cancel the resilver and start a scrub but on four of the eight drives we get: \"cannot offline _uuid_: no valid replicas\"  were _uuid_ is the missing drive's uuid. \r\n\r\nWe have tried rebooting, clearing zfs cache, exporting/importing the pool - nothing affects this state. \r\nIt seems like the pool thinks the missing drives are going to fault the pool if offlined... \r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n\r\n```\r\n# zpool status\r\n  pool: tank\r\n state: DEGRADED\r\nstatus: One or more devices is currently being resilvered.  The pool will\r\n\tcontinue to function, possibly in a degraded state.\r\naction: Wait for the resilver to complete.\r\n  scan: resilver in progress since Thu Jul 20 16:42:40 2017\r\n    1 scanned out of 10.3T at 1/s, (scan is slow, no estimated time)\r\n    0 resilvered, 0.00% done\r\nconfig:\r\n\r\n\tNAME                                    STATE     READ WRITE CKSUM\r\n\ttank                                    DEGRADED     0     0     0\r\n\t  raidz2-0                              DEGRADED     0     0     0\r\n\t    ata-WDC_WD101KRYZ-01JPDB0_7JG9UAYC  ONLINE       0     0     0\r\n\t    ata-WDC_WD101KRYZ-01JPDB0_7JGAG69C  ONLINE       0     0     0\r\n\t    16937286732975812349                UNAVAIL      0     0     0  was /dev/disk/by-id/ata-WDC_WD101KRYZ-01JPDB0_7JGAHB6C-part1\r\n\t    451424496977972574                  UNAVAIL      0     0     0  was /dev/disk/by-id/ata-WDC_WD101KRYZ-01JPDB0_7JGAHLZC-part1\r\n\t    ata-WDC_WD101KRYZ-01JPDB0_7JGGYBKC  ONLINE       0     0     0\r\n\t    ata-WDC_WD101KRYZ-01JPDB0_7JGH7PAC  ONLINE       0     0     0\r\n\t    ata-WDC_WD101KRYZ-01JPDB0_7JGM3H8C  ONLINE       0     0     0\r\n\t    ata-WDC_WD101KRYZ-01JPDB0_7JGM3TMC  ONLINE       0     0     0\r\n\t    ata-WDC_WD101KRYZ-01JPDB0_7JGM688C  ONLINE       0     0     0\r\n\t    ata-WDC_WD101KRYZ-01JPDB0_7JGM6J8C  ONLINE       0     0     0\r\n\t  raidz2-1                              DEGRADED     0     0     0\r\n...\r\n# zpool offline tank 16937286732975812349\r\ncannot offline 16937286732975812349: no valid replicas\r\n\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ColinIanKing": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6372", "title": "zfs cloning takes longer to do as the number of clones increases", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\nDistribution Name       |   Ubuntu\r\nDistribution Version    |   Artful\r\nLinux Kernel                 | 4.12\r\nArchitecture                 |  x86-64\r\nZFS Version                  | 0.6.5.9\r\nSPL Version                  | 0.6.5.9\r\n\r\nCreating clones takes longer and longer as the number of clones increases.  Here are timings I get for time to create N clones:\r\n\r\n\r\nClones | Time (secs)\r\n100 | 7\r\n200 | 10\r\n300 | 19\r\n400 | 22\r\n500 | 29\r\n600 | 39\r\n700 | 46\r\n800 | 58\r\n900 | 74\r\n1000 | 90\r\n1100 | 98\r\n1200 | 102\r\n1300 | 113\r\n1400 | 143\r\n1500 | 145\r\n1600 | 165\r\n1700 | 187\r\n1800 | 210\r\n1900 | 226\r\n2000 | 256\r\n2200 | 311\r\n2400 | 373\r\n2600 | 487\r\n3000 | 619\r\n3400 | 915\r\n4000 | 1332\r\n\r\nSimple bash script to reproduce:\r\n\r\n```\r\n#!/bin/bash\r\n\r\nclone() {\r\n    echo \"Destroying existing clones..\"\r\n    zfs destroy -R testpool/testzfs\r\n    rm -Rf /tmp/testzfs\r\n    zfs create testpool/testzfs -o mountpoint=none\r\n    zfs snapshot testpool/testzfs@base\r\n    mkdir /tmp/testzfs\r\n    echo \"Creating ${1} clones\"\r\n    BASE=$(date +%s)\r\n    for i in $(seq ${1}); do\r\n        UUID=$(uuidgen)\r\n        zfs clone testpool/testzfs@base testpool/testzfs-$UUID -o mountpoint=/tmp/testzfs/${UUID}\r\n    done\r\n    END=$(date +%s)\r\n    TOTAL=$((END-BASE))\r\n    PER=$((${1}/${TOTAL}))\r\n    echo \"Took: $TOTAL seconds ($PER/s)\"\r\n}\r\n\r\nN=0\r\nwhile true\r\ndo\r\n        N=$((N + 100))\r\n        clone $N\r\ndone\r\n```\r\n\r\n------\r\n\r\nRunning strace against zfs create I see the following ioctl() taking the time:\r\n\r\n```\r\n1500475028.118005 ioctl(3, _IOC(0, 0x5a, 0x12, 0x00), 0x7ffc7c2184f0) = -1 ENOMEM (Cannot allocate memory) <0.390093>\r\n1500475028.508153 mmap(NULL, 290816, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fbfd487b000 <0.000017>\r\n1500475028.508201 ioctl(3, _IOC(0, 0x5a, 0x12, 0x00), 0x7ffc7c2184f0) = 0 <0.382304>\r\n```\r\n\r\nI believe this and ioctl on /dev/zfs, namely ZFS_IOC_OBJSET_STATS which is getting stats on *all* the zfs file systems. This ioctl takes longer to do as the number of clones increases.\r\n\r\nperf shows that over 99.9% of the zfs clone is indeed performing this ioctl:\r\n\r\n```\r\n-   99.39%     0.00%  zfs        [kernel.kallsyms]   [k] sys_ioctl                                                                     \r\n     sys_ioctl                                                                                                                         \r\n     do_vfs_ioctl                                                                                                                      \r\n   - zfsdev_ioctl                                                                                                                      \r\n      - 99.33% zfs_ioc_objset_stats                                                                                                    \r\n         - 99.30% zfs_ioc_objset_stats_impl.part.20                                                                                   \r\n            - 99.21% dmu_objset_stats                                                                                                 \r\n               - dsl_dataset_stats                                                                                                    \r\n                  - 99.18% get_clones_stat                                                                                             \r\n                     - 60.46% fnvlist_add_nvlist                                                                                       \r\n                          nvlist_add_nvlist                                                                                            \r\n                          nvlist_add_common.part.51                                                                                    \r\n                          nvlist_copy_embedded.isra.54                                                                                 \r\n                          nvlist_copy_pairs.isra.52                                                                                    \r\n                        - nvlist_add_common.part.51                                                                                    \r\n                           - 30.35% nvlist_copy_embedded.isra.54                                                                       \r\n                                nvlist_copy_pairs.isra.52                                                                              \r\n                              + nvlist_add_common.part.51                                                                           \r\n                           - 29.62% nvlist_remove_all.part.49                                                                       \r\n                                strcmp                                                                                               \r\n                     - 31.23% fnvlist_add_boolean                                                                                    \r\n                        - nvlist_add_boolean                                                                                          \r\n                        - nvlist_add_common.part.51                                                                             \r\n                           - 30.20% nvlist_remove_all.part.49                                                                         \r\n                                strcmp                                                                                                \r\n                             0.94% strcmp                                                                                              \r\n                     - 6.37% dsl_dataset_hold_obj                                                                                      \r\n                        - 6.28% dmu_bonus_hold                                                                                         \r\n                           - 5.89% dnode_hold                                                                                          \r\n                              - 5.83% dnode_hold_impl                                                                                  \r\n                                 - 5.40% dbuf_read                                                                                     \r\n                                    - dmu_zfetch                                                                                       \r\n                                       - 5.19% dmu_zfetch_dofetch.isra.7                                                               \r\n                                          - 4.76% dbuf_prefetch                                                                        \r\n                                             - 2.67% dbuf_find                                                                         \r\n                                                  mutex_lock                                                                           \r\n                                               0.91% mutex_unlock                                                                      \r\n                                               0.55% dnode_block_freed   \r\n```\r\n\r\nThis is a considerable bottleneck on something that seems to be a deficiency in the API between userspace and the zfs driver.  Is there anyway this can be optimized?  This is a time critical bottleneck when dealing with thousands of ZFS clones as backing store for containers.  ", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6372/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "matjohn2": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6367", "title": "Compile error libintl_gettext on Alpine linux w/0.7.0-RC5", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Alpine\r\nDistribution Version    | 3.5.2\r\nLinux Kernel                 | Linux fbba654516a3 4.9.31-moby\r\nArchitecture                 | x64\r\nZFS Version                  | 0.7.0-RC5\r\nSPL Version                  | 0.7.0-RC5\r\n\r\n### Describe the problem you're observing\r\n\r\nI'm having an issue which I beleive is related to alpine's libc vs glibc implementation issue. When compiling ZFS, the `zfs-tests` fail with the `libintl_gettext` error below.\r\n\r\nI've tried adding `LDFLAGS=-liconv -lintl` without success.\r\n\r\n```\r\nmake[5]: Entering directory '/zfs/tests/zfs-tests/cmd/mkbusy'\r\n  CC       mkbusy.o\r\n  CCLD     mkbusy\r\nmake[5]: Leaving directory '/zfs/tests/zfs-tests/cmd/mkbusy'\r\nMaking all in mkfile\r\nmake[5]: Entering directory '/zfs/tests/zfs-tests/cmd/mkfile'\r\n  CC       mkfile.o\r\n  CCLD     mkfile\r\nmkfile.o: In function `usage':\r\n/zfs/tests/zfs-tests/cmd/mkfile/mkfile.c:271: undefined reference to `libintl_gettext'\r\nmkfile.o: In function `main':\r\n/zfs/tests/zfs-tests/cmd/mkfile/mkfile.c:110: undefined reference to `libintl_gettext'\r\n/zfs/tests/zfs-tests/cmd/mkfile/mkfile.c:184: undefined reference to `libintl_gettext'\r\n/zfs/tests/zfs-tests/cmd/mkfile/mkfile.c:132: undefined reference to `libintl_gettext'\r\n/zfs/tests/zfs-tests/cmd/mkfile/mkfile.c:138: undefined reference to `libintl_gettext'\r\nmkfile.o:/zfs/tests/zfs-tests/cmd/mkfile/mkfile.c:220: more undefined references to `libintl_gettext' follow\r\ncollect2: error: ld returned 1 exit status\r\nmake[5]: *** [Makefile:585: mkfile] Error 1\r\nmake[5]: Leaving directory '/zfs/tests/zfs-tests/cmd/mkfile'\r\nmake[4]: *** [Makefile:562: all-recursive] Error 1\r\nmake[4]: Leaving directory '/zfs/tests/zfs-tests/cmd'\r\nmake[3]: *** [Makefile:541: all-recursive] Error 1\r\nmake[3]: Leaving directory '/zfs/tests/zfs-tests'\r\nmake[2]: *** [Makefile:541: all-recursive] Error 1\r\nmake[2]: Leaving directory '/zfs/tests'\r\nmake[1]: *** [Makefile:720: all-recursive] Error 1\r\nmake[1]: Leaving directory '/zfs'\r\nmake: *** [Makefile:589: all] Error 2\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6367/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lnicola": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6366", "title": "Invalid backup stream", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Arch Linux\r\nDistribution Version    | N/A\r\nLinux Kernel                 | 4.11.9-1-ARCH\r\nArchitecture                 | x86-64\r\nZFS Version                  | 0.7.0-rc4_95_g94b25662c\r\nSPL Version                  | 0.7.0-rc4_5_g7a35f2b\r\n\r\n### Describe the problem you're observing\r\nI have a box with two pools and I'm using `zxfer` to back up daily filesystem snapshots from one of the pools to the other. This fails for one of those filesystems with `cannot receive incremental stream: invalid backup stream`.  Note that I'm running `zfs send -Le`, not sure whether it makes a difference.\r\n\r\nI tried to destroy the destination and send everything again. It works for a couple of days, then sends start failing again:\r\n\r\n```\r\nbike/zroot                                                 21.8G  33.6G  13.6G  /\r\nbike/zroot@znap_2017-07-04-2100_daily                       460M      -  13.2G  -\r\nbike/zroot@znap_2017-07-05-2100_daily                       122M      -  13.2G  -\r\nbike/zroot@znap_2017-07-06-2100_daily                       119M      -  13.2G  -\r\nbike/zroot@znap_2017-07-07-2100_daily                       365M      -  13.4G  -\r\nbike/zroot@znap_2017-07-08-2100_daily                       131M      -  13.2G  -\r\nbike/zroot@znap_2017-07-09-2100_daily                       114M      -  13.2G  -\r\nbike/zroot@znap_2017-07-10-2100_daily                       133M      -  13.2G  -\r\nbike/zroot@znap_2017-07-11-2100_daily                       161M      -  13.3G  -\r\nbike/zroot@znap_2017-07-12-2100_daily                       158M      -  13.2G  -\r\nbike/zroot@znap_2017-07-13-2100_daily                       147M      -  13.2G  -\r\nbike/zroot@znap_2017-07-14-2100_daily                       514M      -  13.6G  -\r\nbike/zroot@znap_2017-07-15-2100_daily                       155M      -  13.2G  -\r\nbike/zroot@znap_2017-07-16-2100_daily                       150M      -  13.3G  -\r\nbike/zroot@znap_2017-07-17-2100_daily                       155M      -  13.2G  -\r\ntank/zroot                                                 6.04G  1.13T  5.39G  /tank/zroot\r\ntank/zroot@znap_2017-07-08-2100_daily                      57.8M      -  5.41G  -\r\ntank/zroot@znap_2017-07-09-2100_daily                      47.2M      -  5.41G  -\r\ntank/zroot@znap_2017-07-10-2100_daily                      56.8M      -  5.42G  -\r\ntank/zroot@znap_2017-07-11-2100_daily                      68.3M      -  5.44G  -\r\ntank/zroot@znap_2017-07-12-2100_daily                         0B      -  5.39G  -\r\n```\r\n\r\nNotice how the transfers stopped working after the July 12th snapshot. Ignore the `REFER` mismatches, `bike/zroot` is configured with `copies=2`.\r\n\r\nThis _might_ be related to https://github.com/zfsonlinux/zfs/commit/a9f0d7dd89fd9e09ef41ca98d6ba06bbd3d6ea88. I had enabled the `large_dnode` feature before that commit. It was only after that commit that I started seeing these errors for a couple of filesystems. Destroying the destination filesystems and the (older) source snapshots fixed it at that time with the exception of this filesystem here.\r\n\r\n### Describe how to reproduce the problem\r\nNot sure. Other filesystems work.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nNothing.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6366/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "don-brady": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6344", "title": "ZFS assigns 3 new metaslabs at import instead of reusing assigned ones", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  CentOS\r\nDistribution Version    |  7.2.1511\r\nLinux Kernel                 |  3.10\r\nArchitecture                 |  x86_64\r\nZFS Version                  |  0.7rc4 with allocation classes\r\nSPL Version                  |  0.7rc4\r\n\r\n### Describe the problem you're observing\r\nAt every pool import, ZFS will assign 3 new unassigned metaslabs (ones with no spacemap object created yet) to use for current allocations. With dRAID, and the use of special and normal (mirror vs draid), this will cause the code to reach the maximum special class threshold (20%) sooner than a organic working set would otherwise cause.\r\n\r\n### Describe how to reproduce the problem\r\nThe following scripts demonstrates the issue:\r\n```\r\n#!/bin/bash\r\n\r\nPOOL=metaslabs.$$\r\nLEAF=/tmp/metaslab-pool-leaf.$$\r\nZPOOL=/sbin/zpool\r\nZDB=/sbin/zdb\r\nDEVSIZE=${DEVSIZE-3g}\r\n\r\ntruncate -s \"$DEVSIZE\" $LEAF\r\n$ZPOOL create $POOL $LEAF\r\n\r\nfree=`$ZDB -m $POOL | grep \"spacemap      0   free\" | wc -l`\r\n\r\nwhile [ $free -gt 0 ]\r\ndo\r\n    echo $free free on $POOL\r\n    $ZPOOL export $POOL\r\n    $ZPOOL import -d /tmp $POOL\r\n    current=`$ZDB -m $POOL | grep \"spacemap      0   free\" | wc -l`\r\n    if [ \"$current\" -eq \"$free\" ]\r\n    then\r\n        break\r\n    fi\r\n    free=$current\r\ndone\r\n\r\n$ZPOOL destroy $POOL\r\nrm $LEAF\r\n```\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nExample Output from script above:\r\n```\r\n-sh-4.2$ sudo ~/src/metaslabs.sh\r\n188 free on metaslabs.81163\r\n185 free on metaslabs.81163\r\n182 free on metaslabs.81163\r\n179 free on metaslabs.81163\r\n176 free on metaslabs.81163\r\n173 free on metaslabs.81163\r\n170 free on metaslabs.81163\r\n. . .\r\n20 free on metaslabs.81163\r\n17 free on metaslabs.81163\r\n14 free on metaslabs.81163\r\n11 free on metaslabs.81163\r\n8 free on metaslabs.81163\r\n5 free on metaslabs.81163\r\n2 free on metaslabs.81163\r\nfinished\r\n-sh-4.2$\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6344/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/31df97cdab40545e2d854a9c4ce2fece29e3951b", "message": "Build regression in c89 cleanups\n\nFixed build regression in non-debug builds from recent cleanups of\r\nc89 workarounds.\r\n\r\nReviewed-by: Tim Chase <tim@chase2k.com>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Don Brady <don.brady@delphix.com>\r\nCloses #6832"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/1c27024e22af4386b592b30d40e6a0820ceb48c1", "message": "Undo c89 workarounds to match with upstream\n\nWith PR 5756 the zfs module now supports c99 and the\r\nremaining past c89 workarounds can be undone.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Don Brady <don.brady@delphix.com>\r\nCloses #6816"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/5df5d06a8d86af475e90fe7dd86ea044f937be66", "message": "Cleanup zloop working directory after each pass\n\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed by: John Kennedy <jwk404@gmail.com>\r\nReviewed-by: Olaf Faaland <faaland1@llnl.gov>\r\nSigned-off-by: Don Brady <don.brady@delphix.com>\r\nIssue #6595 \r\nCloses #6663"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d977122da92e870a6a8990437ced845a04c39cfc", "message": "Add corruption failure option to zinject(8)\n\nAdded a 'corrupt' error option that will flip a bit in the data\r\nafter a read operation.  This is useful for generating checksum\r\nerrors at the device layer (in a mirror config for example). It\r\nis also used to validate the diagnosis of checksum errors from\r\nthe zfs diagnosis engine.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Don Brady <don.brady@intel.com>\r\nCloses #6345"}], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6558", "title": "OpenZFS 7431 - ZFS Channel Programs", "body": "Authored by: Chris Williamson <chris.williamson@delphix.com>\r\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\r\nReviewed by: George Wilson <george.wilson@delphix.com>\r\nReviewed by: John Kennedy <john.kennedy@delphix.com>\r\nReviewed by: Dan Kimmel <dan.kimmel@delphix.com>\r\nApproved by: Garrett D'Amore <garrett@damore.org>\r\nPorted-by: Don Brady <don.brady@delphix.com>\r\nPorted-by: John Kennedy <john.kennedy@delphix.com>\r\n\r\nOpenZFS-issue: https://www.illumos.org/issues/7431\r\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/dfc11533\r\n\r\nPorting Notes:\r\n* The CLI long option arguments for '-t' and '-m' don't parse on linux\r\n* Switched from kmem_alloc to vmem_alloc in zcp_lua_alloc\r\n* Lua implementation is built as its own module (zlua.ko)\r\n* Lua headers consumed directly by zfs code moved to 'include/sys/lua/'\r\n* There is no native setjmp/longjump available in stock Linux kernel.  Brought over implementation from illumos and FreeBSD\r\n* The get_temporary_prop() was adapted due to VFS platform differences\r\n* Use of in-lining functions in lua parser code to reduce stack usage per nested C call\r\n\r\n### How Has This Been Tested?\r\n#### Manual tests\r\n- running basic get-props channel programs from CLI\r\n- exercised the zfs property get CLI with the envr *ZFS_PROP_DEBUG=1* set\r\n#### Automated tests\r\n- ztest runs that exercise the new ZCP destroy snapshots path\r\n- new ZTS channel_program functional tests\r\n\r\n### Types of changes\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [x] I have updated the documentation accordingly.\r\n- [x] I have read the **CONTRIBUTING** document.\r\n- [x] I have added tests to cover my changes.\r\n- [x] All new and existing tests passed.\r\n- [ ] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/5182", "title": "Metadata Allocation Classes", "body": "# ZFS Allocation Classes [WIP]\r\n\r\n*Note:* there was a recent consolidation effort and the various allocation classes were merged into a single *special* class that serves metadata, DDT, and small file blocks (optional). Addtionally, the segregation feature was removed.\r\n\r\n## The Special Allocation Class\r\nAllocation classes can be thought of as allocation tiers that are dedicated to specific block categories. Now in addtion to general data (normal class), and Intent Log data (log class), we introduce a Special class (metadata, DDT, and optionally small file blocks).\r\n\r\n### Feature Flag Encapsulation\r\nThe `feature@allocation_classes` becomes active when a special allocation class is instantiated for a VDEV.  Activating this feature makes the pool read-only on builds that don't support allocation classes.\r\n\r\n### Allocation Class Granularity\r\nThe class allocation bias occurs at a VDEV granularity. Each top-level VDEV now has an allocation bias.\r\n\r\n## Special VDEVs\r\nAll metaslabs in the VDEV are dedicated to the special allocation class category. A pool must always have at least one general (non-specified) VDEV when using special VDEVs. Opt-in using the special VDEV class designation when creating the VDEV. \r\n\r\n_Example Syntax:_\r\n\t`zpool create demo raidz <disks> special mirror <disks>`\r\n\t`zpool add demo special <disk>`\r\n\r\n### New Ditto Block Policy\r\nWhen there is only one VDEV available and more than one DVA is required (ditto copies > 1), the traditional ditto placement policy was to place the allocation a distance of 1/8th of total vdev allocation space away from the other DVAs.  This policy has been simplified to a simple guarantee that the other DVAs land in a different metaslab.  To validate that the new policy is honored, a zdb(8) block audit will also report any DVAs that landed in the same metaslab (expecting none). If there is a policy failure it will be manifest as follows in zdb output:\r\n`Dittoed blocks in same metaslab: 21`\r\n\r\n### VDEV Changes\r\nClasses show up in run-time VDEV instance as an allocation bias:\r\n```\r\ntypedef enum vdev_alloc_bias {\r\n    VDEV_BIAS_NONE,\r\n    VDEV_BIAS_LOG,    /* dedicated to ZIL data (SLOG) */\r\n    VDEV_BIAS_SPECIAL /* dedicated to metadata and small blocks */\r\n\t} vdev_alloc_bias_t;\r\n```\r\nThis vdev allocation class bias is stored in the per-vdev zap object as a string value:\r\n```\r\n/* vdev metaslab allocation bias */\r\n#define VDEV_ALLOC_BIAS_LOG.           \"log\"\r\n#define VDEV_ALLOC_BIAS_SPECIAL        \"special\"\r\n```\r\nThe bias is also passed internally in the pool config nvlist during a pool create and any internal pool config query.  This is used by functions in the zpool(8) command.\r\n\r\n### Allocation Stats\r\nThere is additional allocation stats that is recorded. This metadata is kept in a _class_alloc_stats_phys_t_ structure, one per allocation class, that tracks the allocation stats by category.\r\n\r\n```\r\n/*\r\n * Persistent per-class block allocation stats\r\n *\r\n * Note since this object is stored as a generitc DMU_OTN_UINT64_METADATA\r\n * type, we include a magic value (blockallocatestat)\r\n */\r\ntypedef struct class_alloc_stats_phys {\r\n\tuint64_t\tcas_magic;\r\n\tuint64_t\tcas_birth_txg;\t\t/* start of stat collection */\r\n\r\n\tstruct alloc_stat cas_dedup_data;\t/* dedup specific data */\r\n\tstruct alloc_stat cas_dmu_dnodes;\t/* dmu dnode block */\r\n\tstruct alloc_stat cas_file_data;\t/* file leaf blocks */\r\n\tstruct alloc_stat cas_file_indirect;\t/* file metadata */\r\n\tstruct alloc_stat cas_zvol_data;\t/* zvol leaf blocks */\r\n\tstruct alloc_stat cas_zvol_indirect;\t/* zvol metadata */\r\n\r\n\t/*\r\n\t * These histograms track allocated blocks by size.  Each\r\n\t * bucket contains the number of allocated blocks whose\r\n\t * size is >= to a power-of-two and < the next power-of two\r\n\t * for the range of 2^9 through 2^17.\r\n\t */\r\n\tuint64_t\tcas_metadata_histogram[CAS_HISTOGRAM_SIZE];\r\n\tuint64_t\tcas_smallblk_histogram[CAS_HISTOGRAM_SIZE];\r\n} ms_alloc_phys_t;\r\n```\r\n\r\n## Observing Allocation Classes\r\nThere are several ways to observe aspects of allocation classes using the zpool(8), zdb(8) and kstat\r\n### ZPOOL\r\n```\r\n\t$ zpool status demo\r\n\t  pool: demo\r\n\t state: ONLINE\r\n\t  scan: none requested\r\n\tconfig:\r\n\t        NAME                        STATE     READ WRITE CKSUM\r\n\t        demo                        ONLINE       0     0     0\r\n\t          raidz1-0                  ONLINE       0     0     0\r\n\t            scsi-35000c5007adc15a5  ONLINE       0     0     0\r\n\t            scsi-35000c5007adc6d2f  ONLINE       0     0     0\r\n\t            scsi-35000c5007adcf3f4  ONLINE       0     0     0\r\n\t            scsi-35000c5007add017e  ONLINE       0     0     0\r\n\t            scsi-35000c5007addaf56  ONLINE       0     0     0\r\n\t          special:mirror-1          ONLINE       0     0     0\r\n\t            wwn-0x55cd2e404c033dab  ONLINE       0     0     0\r\n\t            wwn-0x55cd2e404c033da3  ONLINE       0     0     0\r\n```\t\r\n\r\n### ZDB\r\nVarious portions of ZDB have been adapted to accommodate allocation class information:\r\n* '-m' Shows the bias assigned to a VDEV\r\n* '-MM' Shows the allocation summary by category for each allocation class\r\n\r\n`zdb -MM [-P] <pool>` -- per class allocation info by block categories:\r\n```\r\n\tSpecial Class Allocations\r\n\t      SIZE     BLOCKS    AVERAGE   %TOTAL    CATEGORY        \r\n\t     4.88M      2.51K      1.95K     0.44    dedup data      \r\n\t     53.5M      22.1K      2.42K     4.80    dmu dnode       \r\n\t     1012M       209K      4.83K    90.86    file user data  \r\n\t     15.1M      8.37K      1.80K     1.35    file indirect   \r\n\t         0          0          -        -    zvol user data  \r\n\t     6.72M        172      40.0K     0.60    zvol indirect   \r\n\t       other metadata\r\n\t\t\t 512:  19319 ****************************************\r\n\t\t\t  1K:   4974 ***********\r\n\t\t\t  2K:    727 **\r\n\t\t\t  4K:    746 **\r\n\t\t\t  8K:     74 *\r\n\t\t\t 16K:     10 *\r\n\t    small file blocks\r\n\t\t\t 512:  32424 ***********************\r\n\t\t\t  1K:  46889 *********************************\r\n\t\t\t  2K:  57726 ****************************************\r\n\t\t\t  4K:  38505 ***************************\r\n\t\t\t  8K:  23505 *****************\r\n\t\t\t 16K:  15068 ***********\r\n\t\t\t 32K:    240 *\r\n```\r\n`zdb -b <pool>` -- class summary and ditto policy (shows any same-vdev ditto blocks that landed in the same metaslab).\r\n```\r\n\t        No leaks (block sum matches space maps exactly)\r\n\t\r\n\t        bp count:                602424\r\n\t        ganged count:                 0\r\n\t        bp logical:         18845081600      avg:  31282\r\n\t        bp physical:        18475993600      avg:  30669     compression:   1.02\r\n\t        bp allocated:       19405235712      avg:  32211     compression:   0.97\r\n\t        bp deduped:                   0    ref>1:      0   deduplication:   1.00\r\n\t        Normal class:       18499772416     used:  4.63%\r\n\t        Special Class:      905463296     used: 87.05%\r\n\t\r\n\t        additional, non-pointer bps of type 0:        642\r\n\t        Dittoed blocks on same vdev: 27127\r\n```\r\n### KSTAT\r\n```\r\n[root@ssu1_oss2]# cat /proc/spl/kstat/zfs/alloc_class_stats\r\n \r\nname                            type data\r\nnormal_allocated                4    61325031915520\r\nnormal_highest_allocated        4    61325037486080\r\nspecial_allocated               4    233487310848\r\nspecial_highest_allocated       4    233536917504\r\nslog_allocated                  4    0\r\nslog_highest_allocated          4    0\r\n```\r\n\r\n## ZTEST Coverage\r\n* Occasionally add a special dedicated vdev\r\n* Occasionally disable small blocks in special class\r\n* a new '-C' option allows control over special vdev creation (see ztest(8) -?)\r\n\t\r\n## Caveats and TBD\r\n\r\n- At this point there are no custom allocation policies and all classes use the default DNF allocator.", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "edillmann": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6334", "title": "BUG: unable to handle kernel paging request", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Proxmox\r\nDistribution Version    | 5.0\r\nLinux Kernel                 | 4.10.15-15\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.7.0-rc4_96_g0ea05c64f\r\nSPL Version                  | 0.7.0-rc4_5_g7a35f2b\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nI'm observing the following stack trace on the receiving side of zfs send but this time receiving a regular dataset.\r\nI don't know if this is related to #6330 \r\n\r\n### Describe how to reproduce the problem\r\nSetup a regular send to remote system (1/hour)\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n```\r\n[192656.325619] BUG: unable to handle kernel paging request at 0000000004ed4fe1\r\n[192656.326546] IP: dbuf_find+0x92/0x1a0 [zfs]\r\n[192656.327948] PGD 0 \r\n\r\n[192656.330805] Oops: 0000 [#1] SMP\r\n[192656.332225] Modules linked in: sch_sfq cls_u32 sch_htb xt_REDIRECT nf_nat_redirect iptable_nat nf_nat_ipv4 nf_nat nf_log_ipv6 xt_hl ip6t_rt nf_log_ipv4 nf_log_common xt_LOG xt_limit veth ip6t_REJECT nf_reject_ipv6 nf_conntrack_ipv6 nf_defrag_ipv6 ip6table_filter ip6_tables ipt_REJECT nf_reject_ipv4 xt_physdev xt_comment nf_conntrack_ipv4 nf_defrag_ipv4 xt_tcpudp xt_mark xt_addrtype xt_multiport xt_conntrack ip_set_hash_net ip_set tpm_rng iptable_filter softdog nfnetlink_log nfnetlink arc4 intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel snd_hda_codec_hdmi pcbc iwlmvm mac80211 aesni_intel iwlwifi aes_x86_64 crypto_simd glue_helper cryptd btusb btrtl btbcm intel_cstate btintel i915 intel_rapl_perf bluetooth snd_hda_codec_realtek\r\n[192656.343321]  snd_hda_codec_generic cfg80211 drm_kms_helper snd_hda_intel pcspkr snd_hda_codec mei_me drm snd_hda_core i2c_algo_bit fb_sys_fops syscopyarea lpc_ich sysfillrect mei sysimgblt snd_hwdep shpchp snd_soc_rt5640 snd_soc_ssm4567 snd_soc_rl6231 snd_soc_core snd_compress ac97_bus dw_dmac snd_pcm_dmaengine snd_soc_sst_acpi video dw_dmac_core snd_pcm snd_soc_sst_match snd_timer elan_i2c snd soundcore acpi_als 8250_dw kfifo_buf mac_hid spi_pxa2xx_platform tpm_crb industrialio acpi_pad vhost_net vhost macvtap macvlan ib_iser rdma_cm iw_cm ib_cm ib_core sunrpc configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi ip_vs_wrr ip_vs nf_conntrack libcrc32c ip_tables x_tables autofs4 zfs(PO) zunicode(PO) zavl(PO) icp(PO) zcommon(PO) znvpair(PO) spl(O) i2c_i801 ahci libahci r8169 mii sdhci_acpi\r\n[192656.356619]  sdhci fjes i2c_hid hid i2c_designware_platform i2c_designware_core\r\n[192656.358622] CPU: 0 PID: 13036 Comm: receive_writer Tainted: P           O    4.10.15-1-pve #1\r\n[192656.360700] Hardware name: GIGABYTE GB-BXi3-5010/MQLP3AP-00, BIOS F4 12/01/2015\r\n[192656.362788] task: ffff88c47cc98000 task.stack: ffffb11cc91c8000\r\n[192656.364922] RIP: 0010:dbuf_find+0x92/0x1a0 [zfs]\r\n[192656.367043] RSP: 0018:ffffb11cc91cbb68 EFLAGS: 00010206\r\n[192656.369142] RAX: ffff88c47cc98000 RBX: 0000000000006010 RCX: 0000000000006000\r\n[192656.371289] RDX: ffffb11ccabf0000 RSI: ffff88c47cc98000 RDI: ffffffffc066ebd0\r\n[192656.373773] RBP: ffffb11cc91cbbc0 R08: 000000000000008c R09: ffff88c60e403040\r\n[192656.376475] R10: 0000000000000000 R11: ffff88c46d1189c0 R12: ffffffffc066ebd0\r\n[192656.378565] R13: 0000000000000000 R14: ffff88c30a0dd800 R15: 0000000004ed4fe1\r\n[192656.380638] FS:  0000000000000000(0000) GS:ffff88c61ec00000(0000) knlGS:0000000000000000\r\n[192656.382663] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[192656.384668] CR2: 0000000004ed4fe1 CR3: 0000000292a09000 CR4: 00000000003406f0\r\n[192656.386634] Call Trace:\r\n[192656.388614]  ? dbuf_rele_and_unlock+0x27b/0x470 [zfs]\r\n[192656.390548]  __dbuf_hold_impl+0x42/0x5f0 [zfs]\r\n[192656.392423]  ? spl_kmem_alloc+0x9b/0x170 [spl]\r\n[192656.394266]  dbuf_hold_impl+0xa8/0xd0 [zfs]\r\n[192656.396111]  dnode_next_offset_level+0xb7/0x400 [zfs]\r\n[192656.397989]  dnode_next_offset+0xf0/0x1e0 [zfs]\r\n[192656.399883]  dmu_object_next+0xc2/0x100 [zfs]\r\n[192656.401724]  ? dnode_hold+0x1b/0x20 [zfs]\r\n[192656.403573]  receive_freeobjects.isra.14+0x70/0xd0 [zfs]\r\n[192656.405384]  ? spl_kmem_free+0x33/0x40 [spl]\r\n[192656.407193]  ? mutex_lock+0x12/0x40\r\n[192656.409015]  ? bqueue_dequeue+0xc7/0xf0 [zfs]\r\n[192656.410889]  receive_writer_thread+0x262/0x6e0 [zfs]\r\n[192656.412669]  ? set_curr_task_fair+0x2b/0x60\r\n[192656.414427]  ? spl_kmem_free+0x33/0x40 [spl]\r\n[192656.416224]  ? receive_freeobjects.isra.14+0xd0/0xd0 [zfs]\r\n[192656.417979]  thread_generic_wrapper+0x72/0x80 [spl]\r\n[192656.419769]  kthread+0x109/0x140\r\n[192656.421538]  ? __thread_exit+0x20/0x20 [spl]\r\n[192656.423294]  ? kthread_create_on_node+0x60/0x60\r\n[192656.424989]  ret_from_fork+0x2c/0x40\r\n[192656.426658] Code: 04 25 00 d3 00 00 48 89 83 e8 8b 66 c0 48 89 c6 4e 8b 3c fa 4d 85 ff 75 12 e9 a8 00 00 00 4d 8b 7f 38 4d 85 ff 0f 84 9b 00 00 00 <4d> 3b 2f 75 ee 4d 3b 77 20 75 e8 0f b6 45 cf 41 3a 47 50 75 de \r\n[192656.430546] RIP: dbuf_find+0x92/0x1a0 [zfs] RSP: ffffb11cc91cbb68\r\n[192656.432228] CR2: 0000000004ed4fe1\r\n[192656.433827] ---[ end trace 64f65c6a692083d5 ]---\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6334/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6330", "title": "z_vol task hung on receiving side of zfs send", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Proxmox\r\nDistribution Version    | 5.0\r\nLinux Kernel                 | 4.10.15-15\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.7.0-rc4_96_g0ea05c64f\r\nSPL Version                  | 0.7.0-rc4_5_g7a35f2b\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nI'm observing the following stack trace on the receiving side of zfs send.\r\n\r\n### Describe how to reproduce the problem\r\nSetup a regular send to remote system (1/hour)\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n```\r\n[27671.358618] INFO: task z_zvol:348 blocked for more than 120 seconds.\r\n[27671.360473]       Tainted: P           O    4.10.15-1-pve #1\r\n[27671.362479] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[27671.363379] z_zvol          D    0   348      2 0x00000000\r\n[27671.364409] Call Trace:\r\n[27671.365169]  __schedule+0x233/0x6f0\r\n[27671.366083]  schedule+0x36/0x80\r\n[27671.366877]  taskq_wait_outstanding+0x8c/0xd0 [spl]\r\n[27671.367720]  ? wake_atomic_t_function+0x60/0x60\r\n[27671.368698]  zvol_task_cb+0x2fd/0x490 [zfs]\r\n[27671.369449]  ? __schedule+0x23b/0x6f0\r\n[27671.370574]  taskq_thread+0x25e/0x460 [spl]\r\n[27671.371725]  ? wake_up_q+0x80/0x80\r\n[27671.372447]  kthread+0x109/0x140\r\n[27671.373306]  ? taskq_cancel_id+0x130/0x130 [spl]\r\n[27671.374005]  ? kthread_create_on_node+0x60/0x60\r\n[27671.374736]  ret_from_fork+0x2c/0x40\r\n[27913.020358] INFO: task z_zvol:348 blocked for more than 120 seconds.\r\n[27913.021135]       Tainted: P           O    4.10.15-1-pve #1\r\n[27913.021804] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[27913.022507] z_zvol          D    0   348      2 0x00000000\r\n[27913.023489] Call Trace:\r\n[27913.024566]  __schedule+0x233/0x6f0\r\n[27913.025821]  schedule+0x36/0x80\r\n[27913.026760]  taskq_wait_outstanding+0x8c/0xd0 [spl]\r\n[27913.027658]  ? wake_atomic_t_function+0x60/0x60\r\n[27913.029015]  zvol_task_cb+0x2fd/0x490 [zfs]\r\n[27913.030207]  ? __schedule+0x23b/0x6f0\r\n[27913.031317]  taskq_thread+0x25e/0x460 [spl]\r\n[27913.032461]  ? wake_up_q+0x80/0x80\r\n[27913.033602]  kthread+0x109/0x140\r\n[27913.034545]  ? taskq_cancel_id+0x130/0x130 [spl]\r\n[27913.035439]  ? kthread_create_on_node+0x60/0x60\r\n[27913.036467]  ret_from_fork+0x2c/0x40\r\n\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6330/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/2807959", "body": "i forgot to remove this one :-(\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/2807959/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "jspiros": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6332", "title": "PANIC at zfs_vfsops.c:426:zfs_space_delta_cb()", "body": "**NOTE:** I have some thoughts about what may be causing this problem, and I'm not sure that my logs in this first post are immediately useful. You may want to skip down to [my updates after this first post](#issuecomment-316974318) for more context and thoughts on what might be happening.\r\n \r\n### System information\r\n<!--  add version after \"|\" character -->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian\r\nDistribution Version    | testing/unstable\r\nLinux Kernel                 | 4.9.6-3\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.6.5.9-2\r\nSPL Version                  | 0.6.5.9-1\r\nPool Version                  | 28\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nI have experienced two panics in a short period of time seemingly related to rsync and/or send/recv. I was running with two pools, poolA and poolB, and I was in the process of moving everything from poolA to poolB using send/recv so that I could run with poolB only. One of the filesystem hierarchies being moved is the destination of some rsync cronjobs used for backup purposes. When I was send/recv'ing that hierarchy (using `zfs send -vR poolA/backups@snap | zfs receive -uv poolB/backups`), before it completed, one of the cronjobs started to run (with poolA as the effective destination). With both the send/recv and the rsync running at the same time, I got a panic which brought everything related to ZFS or any ZFS filesystems to a halt, necessitating a reboot. This is the panic from my kern.log:\r\n\r\n```\r\nJul  8 02:10:30 system kernel: [20231.348800] VERIFY3(sa.sa_magic == 0x2F505A) failed (1499494230 == 3100762)\r\nJul  8 02:10:30 system kernel: [20231.348853] PANIC at zfs_vfsops.c:426:zfs_space_delta_cb()\r\nJul  8 02:10:30 system kernel: [20231.348880] Showing stack for process 3506\r\nJul  8 02:10:30 system kernel: [20231.348883] CPU: 4 PID: 3506 Comm: txg_sync Tainted: P           OE   4.9.0-1-amd64 #1 Debian 4.9.6-3\r\nJul  8 02:10:30 system kernel: [20231.348884] Hardware name: Supermicro X9SCL/X9SCM/X9SCL/X9SCM, BIOS 2.2 02/20/2015\r\nJul  8 02:10:30 system kernel: [20231.348886]  0000000000000000 ffffffff98128e34 ffffffffc0ad8713 ffffab7762447a10\r\nJul  8 02:10:30 system kernel: [20231.348889]  ffffffffc095088b ffff8b64afd18300 ffffab7700000030 ffffab7762447a20\r\nJul  8 02:10:30 system kernel: [20231.348891]  ffffab77624479c0 2833594649524556 616d5f61732e6173 30203d3d20636967\r\nJul  8 02:10:30 system kernel: [20231.348893] Call Trace:\r\nJul  8 02:10:30 system kernel: [20231.348900]  [<ffffffff98128e34>] ? dump_stack+0x5c/0x78\r\nJul  8 02:10:30 system kernel: [20231.348916]  [<ffffffffc095088b>] ? spl_panic+0xbb/0xf0 [spl]\r\nJul  8 02:10:30 system kernel: [20231.348921]  [<ffffffff97ea8773>] ? update_load_avg+0x73/0x360\r\nJul  8 02:10:30 system kernel: [20231.348924]  [<ffffffff97fddd6b>] ? free_block+0x12b/0x1c0\r\nJul  8 02:10:30 system kernel: [20231.348926]  [<ffffffff97fe1962>] ? ___cache_free+0x1c2/0x2e0\r\nJul  8 02:10:30 system kernel: [20231.349020]  [<ffffffffc0a052ce>] ? dbuf_rele_and_unlock+0x2ce/0x3c0 [zfs]\r\nJul  8 02:10:30 system kernel: [20231.349054]  [<ffffffffc0a8c6de>] ? zfs_space_delta_cb+0xbe/0x190 [zfs]\r\nJul  8 02:10:30 system kernel: [20231.349082]  [<ffffffffc0a1402b>] ? dmu_objset_userquota_get_ids+0x10b/0x3a0 [zfs]\r\nJul  8 02:10:30 system kernel: [20231.349107]  [<ffffffffc0a2271c>] ? dnode_sync+0xcc/0x830 [zfs]\r\nJul  8 02:10:30 system kernel: [20231.349130]  [<ffffffffc0a22916>] ? dnode_sync+0x2c6/0x830 [zfs]\r\nJul  8 02:10:30 system kernel: [20231.349151]  [<ffffffffc0a12eb1>] ? dmu_objset_sync_dnodes+0x91/0xb0 [zfs]\r\nJul  8 02:10:30 system kernel: [20231.349175]  [<ffffffffc0a130ba>] ? dmu_objset_sync+0x1ea/0x370 [zfs]\r\nJul  8 02:10:30 system kernel: [20231.349199]  [<ffffffffc09fb4c0>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nJul  8 02:10:30 system kernel: [20231.349221]  [<ffffffffc09fa180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nJul  8 02:10:30 system kernel: [20231.349244]  [<ffffffffc09fd1f0>] ? l2arc_read_done+0x400/0x400 [zfs]\r\nJul  8 02:10:30 system kernel: [20231.349274]  [<ffffffffc0a2aae9>] ? dsl_dataset_sync+0x49/0x80 [zfs]\r\nJul  8 02:10:30 system kernel: [20231.349299]  [<ffffffffc0a335bf>] ? dsl_pool_sync+0x9f/0x440 [zfs]\r\nJul  8 02:10:30 system kernel: [20231.349329]  [<ffffffffc0a4d46b>] ? spa_sync+0x37b/0xb10 [zfs]\r\nJul  8 02:10:30 system kernel: [20231.349332]  [<ffffffff97eb8774>] ? __wake_up+0x34/0x50\r\nJul  8 02:10:30 system kernel: [20231.349362]  [<ffffffffc0a5d718>] ? txg_sync_thread+0x3b8/0x610 [zfs]\r\nJul  8 02:10:30 system kernel: [20231.349391]  [<ffffffffc0a5d360>] ? txg_delay+0x160/0x160 [zfs]\r\nJul  8 02:10:30 system kernel: [20231.349405]  [<ffffffffc094dbfd>] ? thread_generic_wrapper+0x6d/0x80 [spl]\r\nJul  8 02:10:30 system kernel: [20231.349410]  [<ffffffffc094db90>] ? __thread_exit+0x20/0x20 [spl]\r\nJul  8 02:10:30 system kernel: [20231.349413]  [<ffffffff97e9655e>] ? kthread+0xce/0xf0\r\nJul  8 02:10:30 system kernel: [20231.349417]  [<ffffffff97e24701>] ? __switch_to+0x2c1/0x6c0\r\nJul  8 02:10:30 system kernel: [20231.349419]  [<ffffffff97e96490>] ? kthread_park+0x60/0x60\r\nJul  8 02:10:30 system kernel: [20231.349422]  [<ffffffff983fbff5>] ? ret_from_fork+0x25/0x30\r\nJul  8 02:13:40 system kernel: [20420.944200] INFO: task txg_sync:3506 blocked for more than 120 seconds.\r\nJul  8 02:13:40 system kernel: [20420.944226]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 02:13:40 system kernel: [20420.944242] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 02:13:40 system kernel: [20420.944263] txg_sync        D    0  3506      2 0x00000000\r\nJul  8 02:13:40 system kernel: [20420.944266]  ffff8b6476555800 ffff8b6434bf1c00 ffff8b647962c080 ffff8b64afd18300\r\nJul  8 02:13:40 system kernel: [20420.944268]  ffff8b621ad950c0 ffffab7762447878 ffffffff983f6fe3 ffffffff988137ca\r\nJul  8 02:13:40 system kernel: [20420.944269]  00203d3d20636967 ffff8b64afd18300 0000000000000286 ffff8b647962c080\r\nJul  8 02:13:40 system kernel: [20420.944271] Call Trace:\r\nJul  8 02:13:40 system kernel: [20420.944277]  [<ffffffff983f6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 02:13:40 system kernel: [20420.944280]  [<ffffffff983f74b2>] ? schedule+0x32/0x80\r\nJul  8 02:13:40 system kernel: [20420.944286]  [<ffffffffc09508b1>] ? spl_panic+0xe1/0xf0 [spl]\r\nJul  8 02:13:40 system kernel: [20420.944289]  [<ffffffff97ea8773>] ? update_load_avg+0x73/0x360\r\nJul  8 02:13:40 system kernel: [20420.944291]  [<ffffffff97fddd6b>] ? free_block+0x12b/0x1c0\r\nJul  8 02:13:40 system kernel: [20420.944293]  [<ffffffff97fe1962>] ? ___cache_free+0x1c2/0x2e0\r\nJul  8 02:13:40 system kernel: [20420.944320]  [<ffffffffc0a052ce>] ? dbuf_rele_and_unlock+0x2ce/0x3c0 [zfs]\r\nJul  8 02:13:40 system kernel: [20420.944346]  [<ffffffffc0a8c6de>] ? zfs_space_delta_cb+0xbe/0x190 [zfs]\r\nJul  8 02:13:40 system kernel: [20420.944366]  [<ffffffffc0a1402b>] ? dmu_objset_userquota_get_ids+0x10b/0x3a0 [zfs]\r\nJul  8 02:13:40 system kernel: [20420.944387]  [<ffffffffc0a2271c>] ? dnode_sync+0xcc/0x830 [zfs]\r\nJul  8 02:13:40 system kernel: [20420.944406]  [<ffffffffc0a22916>] ? dnode_sync+0x2c6/0x830 [zfs]\r\nJul  8 02:13:40 system kernel: [20420.944425]  [<ffffffffc0a12eb1>] ? dmu_objset_sync_dnodes+0x91/0xb0 [zfs]\r\nJul  8 02:13:40 system kernel: [20420.944443]  [<ffffffffc0a130ba>] ? dmu_objset_sync+0x1ea/0x370 [zfs]\r\nJul  8 02:13:40 system kernel: [20420.944459]  [<ffffffffc09fb4c0>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nJul  8 02:13:40 system kernel: [20420.944474]  [<ffffffffc09fa180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nJul  8 02:13:40 system kernel: [20420.944490]  [<ffffffffc09fd1f0>] ? l2arc_read_done+0x400/0x400 [zfs]\r\nJul  8 02:13:40 system kernel: [20420.944511]  [<ffffffffc0a2aae9>] ? dsl_dataset_sync+0x49/0x80 [zfs]\r\nJul  8 02:13:40 system kernel: [20420.944532]  [<ffffffffc0a335bf>] ? dsl_pool_sync+0x9f/0x440 [zfs]\r\nJul  8 02:13:40 system kernel: [20420.944556]  [<ffffffffc0a4d46b>] ? spa_sync+0x37b/0xb10 [zfs]\r\nJul  8 02:13:40 system kernel: [20420.944558]  [<ffffffff97eb8774>] ? __wake_up+0x34/0x50\r\nJul  8 02:13:40 system kernel: [20420.944583]  [<ffffffffc0a5d718>] ? txg_sync_thread+0x3b8/0x610 [zfs]\r\nJul  8 02:13:40 system kernel: [20420.944607]  [<ffffffffc0a5d360>] ? txg_delay+0x160/0x160 [zfs]\r\nJul  8 02:13:40 system kernel: [20420.944611]  [<ffffffffc094dbfd>] ? thread_generic_wrapper+0x6d/0x80 [spl]\r\nJul  8 02:13:40 system kernel: [20420.944614]  [<ffffffffc094db90>] ? __thread_exit+0x20/0x20 [spl]\r\nJul  8 02:13:40 system kernel: [20420.944616]  [<ffffffff97e9655e>] ? kthread+0xce/0xf0\r\nJul  8 02:13:40 system kernel: [20420.944618]  [<ffffffff97e24701>] ? __switch_to+0x2c1/0x6c0\r\nJul  8 02:13:40 system kernel: [20420.944619]  [<ffffffff97e96490>] ? kthread_park+0x60/0x60\r\nJul  8 02:13:40 system kernel: [20420.944621]  [<ffffffff983fbff5>] ? ret_from_fork+0x25/0x30\r\nJul  8 02:15:41 system kernel: [20541.771313] INFO: task txg_sync:3506 blocked for more than 120 seconds.\r\nJul  8 02:15:41 system kernel: [20541.771341]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 02:15:41 system kernel: [20541.771358] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 02:15:41 system kernel: [20541.771385] txg_sync        D    0  3506      2 0x00000000\r\nJul  8 02:15:41 system kernel: [20541.771391]  ffff8b6476555800 ffff8b6434bf1c00 ffff8b647962c080 ffff8b64afd18300\r\nJul  8 02:15:41 system kernel: [20541.771393]  ffff8b621ad950c0 ffffab7762447878 ffffffff983f6fe3 ffffffff988137ca\r\nJul  8 02:15:41 system kernel: [20541.771395]  00203d3d20636967 ffff8b64afd18300 0000000000000286 ffff8b647962c080\r\nJul  8 02:15:41 system kernel: [20541.771397] Call Trace:\r\nJul  8 02:15:41 system kernel: [20541.771404]  [<ffffffff983f6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 02:15:41 system kernel: [20541.771407]  [<ffffffff983f74b2>] ? schedule+0x32/0x80\r\nJul  8 02:15:41 system kernel: [20541.771413]  [<ffffffffc09508b1>] ? spl_panic+0xe1/0xf0 [spl]\r\nJul  8 02:15:41 system kernel: [20541.771416]  [<ffffffff97ea8773>] ? update_load_avg+0x73/0x360\r\nJul  8 02:15:41 system kernel: [20541.771419]  [<ffffffff97fddd6b>] ? free_block+0x12b/0x1c0\r\nJul  8 02:15:41 system kernel: [20541.771420]  [<ffffffff97fe1962>] ? ___cache_free+0x1c2/0x2e0\r\nJul  8 02:15:41 system kernel: [20541.771454]  [<ffffffffc0a052ce>] ? dbuf_rele_and_unlock+0x2ce/0x3c0 [zfs]\r\nJul  8 02:15:41 system kernel: [20541.771483]  [<ffffffffc0a8c6de>] ? zfs_space_delta_cb+0xbe/0x190 [zfs]\r\nJul  8 02:15:41 system kernel: [20541.771505]  [<ffffffffc0a1402b>] ? dmu_objset_userquota_get_ids+0x10b/0x3a0 [zfs]\r\nJul  8 02:15:41 system kernel: [20541.771528]  [<ffffffffc0a2271c>] ? dnode_sync+0xcc/0x830 [zfs]\r\nJul  8 02:15:41 system kernel: [20541.771550]  [<ffffffffc0a22916>] ? dnode_sync+0x2c6/0x830 [zfs]\r\nJul  8 02:15:41 system kernel: [20541.771573]  [<ffffffffc0a12eb1>] ? dmu_objset_sync_dnodes+0x91/0xb0 [zfs]\r\nJul  8 02:15:41 system kernel: [20541.771602]  [<ffffffffc0a130ba>] ? dmu_objset_sync+0x1ea/0x370 [zfs]\r\nJul  8 02:15:41 system kernel: [20541.771620]  [<ffffffffc09fb4c0>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nJul  8 02:15:41 system kernel: [20541.771638]  [<ffffffffc09fa180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nJul  8 02:15:41 system kernel: [20541.771655]  [<ffffffffc09fd1f0>] ? l2arc_read_done+0x400/0x400 [zfs]\r\nJul  8 02:15:41 system kernel: [20541.771678]  [<ffffffffc0a2aae9>] ? dsl_dataset_sync+0x49/0x80 [zfs]\r\nJul  8 02:15:41 system kernel: [20541.771702]  [<ffffffffc0a335bf>] ? dsl_pool_sync+0x9f/0x440 [zfs]\r\nJul  8 02:15:41 system kernel: [20541.771729]  [<ffffffffc0a4d46b>] ? spa_sync+0x37b/0xb10 [zfs]\r\nJul  8 02:15:41 system kernel: [20541.771731]  [<ffffffff97eb8774>] ? __wake_up+0x34/0x50\r\nJul  8 02:15:41 system kernel: [20541.771759]  [<ffffffffc0a5d718>] ? txg_sync_thread+0x3b8/0x610 [zfs]\r\nJul  8 02:15:41 system kernel: [20541.771786]  [<ffffffffc0a5d360>] ? txg_delay+0x160/0x160 [zfs]\r\nJul  8 02:15:41 system kernel: [20541.771790]  [<ffffffffc094dbfd>] ? thread_generic_wrapper+0x6d/0x80 [spl]\r\nJul  8 02:15:41 system kernel: [20541.771794]  [<ffffffffc094db90>] ? __thread_exit+0x20/0x20 [spl]\r\nJul  8 02:15:41 system kernel: [20541.771796]  [<ffffffff97e9655e>] ? kthread+0xce/0xf0\r\nJul  8 02:15:41 system kernel: [20541.771798]  [<ffffffff97e24701>] ? __switch_to+0x2c1/0x6c0\r\nJul  8 02:15:41 system kernel: [20541.771800]  [<ffffffff97e96490>] ? kthread_park+0x60/0x60\r\nJul  8 02:15:41 system kernel: [20541.771801]  [<ffffffff983fbff5>] ? ret_from_fork+0x25/0x30\r\nJul  8 02:17:42 system kernel: [20662.598452] INFO: task txg_sync:3506 blocked for more than 120 seconds.\r\nJul  8 02:17:42 system kernel: [20662.598482]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 02:17:42 system kernel: [20662.598501] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 02:17:42 system kernel: [20662.598527] txg_sync        D    0  3506      2 0x00000000\r\nJul  8 02:17:42 system kernel: [20662.598530]  ffff8b6476555800 ffff8b6434bf1c00 ffff8b647962c080 ffff8b64afd18300\r\nJul  8 02:17:42 system kernel: [20662.598533]  ffff8b621ad950c0 ffffab7762447878 ffffffff983f6fe3 ffffffff988137ca\r\nJul  8 02:17:42 system kernel: [20662.598535]  00203d3d20636967 ffff8b64afd18300 0000000000000286 ffff8b647962c080\r\nJul  8 02:17:42 system kernel: [20662.598537] Call Trace:\r\nJul  8 02:17:42 system kernel: [20662.598543]  [<ffffffff983f6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 02:17:42 system kernel: [20662.598547]  [<ffffffff983f74b2>] ? schedule+0x32/0x80\r\nJul  8 02:17:42 system kernel: [20662.598553]  [<ffffffffc09508b1>] ? spl_panic+0xe1/0xf0 [spl]\r\nJul  8 02:17:42 system kernel: [20662.598556]  [<ffffffff97ea8773>] ? update_load_avg+0x73/0x360\r\nJul  8 02:17:42 system kernel: [20662.598559]  [<ffffffff97fddd6b>] ? free_block+0x12b/0x1c0\r\nJul  8 02:17:42 system kernel: [20662.598560]  [<ffffffff97fe1962>] ? ___cache_free+0x1c2/0x2e0\r\nJul  8 02:17:42 system kernel: [20662.598590]  [<ffffffffc0a052ce>] ? dbuf_rele_and_unlock+0x2ce/0x3c0 [zfs]\r\nJul  8 02:17:42 system kernel: [20662.598622]  [<ffffffffc0a8c6de>] ? zfs_space_delta_cb+0xbe/0x190 [zfs]\r\nJul  8 02:17:42 system kernel: [20662.598646]  [<ffffffffc0a1402b>] ? dmu_objset_userquota_get_ids+0x10b/0x3a0 [zfs]\r\nJul  8 02:17:42 system kernel: [20662.598671]  [<ffffffffc0a2271c>] ? dnode_sync+0xcc/0x830 [zfs]\r\nJul  8 02:17:42 system kernel: [20662.598694]  [<ffffffffc0a22916>] ? dnode_sync+0x2c6/0x830 [zfs]\r\nJul  8 02:17:42 system kernel: [20662.598717]  [<ffffffffc0a12eb1>] ? dmu_objset_sync_dnodes+0x91/0xb0 [zfs]\r\nJul  8 02:17:42 system kernel: [20662.598739]  [<ffffffffc0a130ba>] ? dmu_objset_sync+0x1ea/0x370 [zfs]\r\nJul  8 02:17:42 system kernel: [20662.598758]  [<ffffffffc09fb4c0>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nJul  8 02:17:42 system kernel: [20662.598777]  [<ffffffffc09fa180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nJul  8 02:17:42 system kernel: [20662.598797]  [<ffffffffc09fd1f0>] ? l2arc_read_done+0x400/0x400 [zfs]\r\nJul  8 02:17:42 system kernel: [20662.598822]  [<ffffffffc0a2aae9>] ? dsl_dataset_sync+0x49/0x80 [zfs]\r\nJul  8 02:17:42 system kernel: [20662.598847]  [<ffffffffc0a335bf>] ? dsl_pool_sync+0x9f/0x440 [zfs]\r\nJul  8 02:17:42 system kernel: [20662.598885]  [<ffffffffc0a4d46b>] ? spa_sync+0x37b/0xb10 [zfs]\r\nJul  8 02:17:42 system kernel: [20662.598887]  [<ffffffff97eb8774>] ? __wake_up+0x34/0x50\r\nJul  8 02:17:42 system kernel: [20662.598926]  [<ffffffffc0a5d718>] ? txg_sync_thread+0x3b8/0x610 [zfs]\r\nJul  8 02:17:42 system kernel: [20662.598955]  [<ffffffffc0a5d360>] ? txg_delay+0x160/0x160 [zfs]\r\nJul  8 02:17:42 system kernel: [20662.598959]  [<ffffffffc094dbfd>] ? thread_generic_wrapper+0x6d/0x80 [spl]\r\nJul  8 02:17:42 system kernel: [20662.598963]  [<ffffffffc094db90>] ? __thread_exit+0x20/0x20 [spl]\r\nJul  8 02:17:42 system kernel: [20662.598966]  [<ffffffff97e9655e>] ? kthread+0xce/0xf0\r\nJul  8 02:17:42 system kernel: [20662.598968]  [<ffffffff97e24701>] ? __switch_to+0x2c1/0x6c0\r\nJul  8 02:17:42 system kernel: [20662.598970]  [<ffffffff97e96490>] ? kthread_park+0x60/0x60\r\nJul  8 02:17:42 system kernel: [20662.598972]  [<ffffffff983fbff5>] ? ret_from_fork+0x25/0x30\r\nJul  8 02:19:43 system kernel: [20783.425575] INFO: task txg_sync:3506 blocked for more than 120 seconds.\r\nJul  8 02:19:43 system kernel: [20783.425609]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 02:19:43 system kernel: [20783.425631] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 02:19:43 system kernel: [20783.425661] txg_sync        D    0  3506      2 0x00000000\r\nJul  8 02:19:43 system kernel: [20783.425665]  ffff8b6476555800 ffff8b6434bf1c00 ffff8b647962c080 ffff8b64afd18300\r\nJul  8 02:19:43 system kernel: [20783.425667]  ffff8b621ad950c0 ffffab7762447878 ffffffff983f6fe3 ffffffff988137ca\r\nJul  8 02:19:43 system kernel: [20783.425670]  00203d3d20636967 ffff8b64afd18300 0000000000000286 ffff8b647962c080\r\nJul  8 02:19:43 system kernel: [20783.425672] Call Trace:\r\nJul  8 02:19:43 system kernel: [20783.425680]  [<ffffffff983f6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 02:19:43 system kernel: [20783.425684]  [<ffffffff983f74b2>] ? schedule+0x32/0x80\r\nJul  8 02:19:43 system kernel: [20783.425692]  [<ffffffffc09508b1>] ? spl_panic+0xe1/0xf0 [spl]\r\nJul  8 02:19:43 system kernel: [20783.425697]  [<ffffffff97ea8773>] ? update_load_avg+0x73/0x360\r\nJul  8 02:19:43 system kernel: [20783.425700]  [<ffffffff97fddd6b>] ? free_block+0x12b/0x1c0\r\nJul  8 02:19:43 system kernel: [20783.425702]  [<ffffffff97fe1962>] ? ___cache_free+0x1c2/0x2e0\r\nJul  8 02:19:43 system kernel: [20783.425736]  [<ffffffffc0a052ce>] ? dbuf_rele_and_unlock+0x2ce/0x3c0 [zfs]\r\nJul  8 02:19:43 system kernel: [20783.425788]  [<ffffffffc0a8c6de>] ? zfs_space_delta_cb+0xbe/0x190 [zfs]\r\nJul  8 02:19:43 system kernel: [20783.425825]  [<ffffffffc0a1402b>] ? dmu_objset_userquota_get_ids+0x10b/0x3a0 [zfs]\r\nJul  8 02:19:43 system kernel: [20783.425853]  [<ffffffffc0a2271c>] ? dnode_sync+0xcc/0x830 [zfs]\r\nJul  8 02:19:43 system kernel: [20783.425880]  [<ffffffffc0a22916>] ? dnode_sync+0x2c6/0x830 [zfs]\r\nJul  8 02:19:43 system kernel: [20783.425907]  [<ffffffffc0a12eb1>] ? dmu_objset_sync_dnodes+0x91/0xb0 [zfs]\r\nJul  8 02:19:43 system kernel: [20783.425932]  [<ffffffffc0a130ba>] ? dmu_objset_sync+0x1ea/0x370 [zfs]\r\nJul  8 02:19:43 system kernel: [20783.425954]  [<ffffffffc09fb4c0>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nJul  8 02:19:43 system kernel: [20783.425976]  [<ffffffffc09fa180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nJul  8 02:19:43 system kernel: [20783.425998]  [<ffffffffc09fd1f0>] ? l2arc_read_done+0x400/0x400 [zfs]\r\nJul  8 02:19:43 system kernel: [20783.426026]  [<ffffffffc0a2aae9>] ? dsl_dataset_sync+0x49/0x80 [zfs]\r\nJul  8 02:19:43 system kernel: [20783.426056]  [<ffffffffc0a335bf>] ? dsl_pool_sync+0x9f/0x440 [zfs]\r\nJul  8 02:19:43 system kernel: [20783.426089]  [<ffffffffc0a4d46b>] ? spa_sync+0x37b/0xb10 [zfs]\r\nJul  8 02:19:43 system kernel: [20783.426091]  [<ffffffff97eb8774>] ? __wake_up+0x34/0x50\r\nJul  8 02:19:43 system kernel: [20783.426126]  [<ffffffffc0a5d718>] ? txg_sync_thread+0x3b8/0x610 [zfs]\r\nJul  8 02:19:43 system kernel: [20783.426160]  [<ffffffffc0a5d360>] ? txg_delay+0x160/0x160 [zfs]\r\nJul  8 02:19:43 system kernel: [20783.426165]  [<ffffffffc094dbfd>] ? thread_generic_wrapper+0x6d/0x80 [spl]\r\nJul  8 02:19:43 system kernel: [20783.426170]  [<ffffffffc094db90>] ? __thread_exit+0x20/0x20 [spl]\r\nJul  8 02:19:43 system kernel: [20783.426172]  [<ffffffff97e9655e>] ? kthread+0xce/0xf0\r\nJul  8 02:19:43 system kernel: [20783.426175]  [<ffffffff97e24701>] ? __switch_to+0x2c1/0x6c0\r\nJul  8 02:19:43 system kernel: [20783.426177]  [<ffffffff97e96490>] ? kthread_park+0x60/0x60\r\nJul  8 02:19:43 system kernel: [20783.426179]  [<ffffffff983fbff5>] ? ret_from_fork+0x25/0x30\r\nJul  8 02:21:43 system kernel: [20904.252643] INFO: task txg_sync:3506 blocked for more than 120 seconds.\r\nJul  8 02:21:43 system kernel: [20904.252667]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 02:21:43 system kernel: [20904.252682] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 02:21:43 system kernel: [20904.252703] txg_sync        D    0  3506      2 0x00000000\r\nJul  8 02:21:43 system kernel: [20904.252706]  ffff8b6476555800 ffff8b6434bf1c00 ffff8b647962c080 ffff8b64afd18300\r\nJul  8 02:21:43 system kernel: [20904.252708]  ffff8b621ad950c0 ffffab7762447878 ffffffff983f6fe3 ffffffff988137ca\r\nJul  8 02:21:43 system kernel: [20904.252709]  00203d3d20636967 ffff8b64afd18300 0000000000000286 ffff8b647962c080\r\nJul  8 02:21:43 system kernel: [20904.252711] Call Trace:\r\nJul  8 02:21:43 system kernel: [20904.252717]  [<ffffffff983f6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 02:21:43 system kernel: [20904.252719]  [<ffffffff983f74b2>] ? schedule+0x32/0x80\r\nJul  8 02:21:43 system kernel: [20904.252726]  [<ffffffffc09508b1>] ? spl_panic+0xe1/0xf0 [spl]\r\nJul  8 02:21:43 system kernel: [20904.252728]  [<ffffffff97ea8773>] ? update_load_avg+0x73/0x360\r\nJul  8 02:21:43 system kernel: [20904.252730]  [<ffffffff97fddd6b>] ? free_block+0x12b/0x1c0\r\nJul  8 02:21:43 system kernel: [20904.252732]  [<ffffffff97fe1962>] ? ___cache_free+0x1c2/0x2e0\r\nJul  8 02:21:43 system kernel: [20904.252757]  [<ffffffffc0a052ce>] ? dbuf_rele_and_unlock+0x2ce/0x3c0 [zfs]\r\nJul  8 02:21:43 system kernel: [20904.252782]  [<ffffffffc0a8c6de>] ? zfs_space_delta_cb+0xbe/0x190 [zfs]\r\nJul  8 02:21:43 system kernel: [20904.252801]  [<ffffffffc0a1402b>] ? dmu_objset_userquota_get_ids+0x10b/0x3a0 [zfs]\r\nJul  8 02:21:43 system kernel: [20904.252820]  [<ffffffffc0a2271c>] ? dnode_sync+0xcc/0x830 [zfs]\r\nJul  8 02:21:43 system kernel: [20904.252839]  [<ffffffffc0a22916>] ? dnode_sync+0x2c6/0x830 [zfs]\r\nJul  8 02:21:43 system kernel: [20904.252857]  [<ffffffffc0a12eb1>] ? dmu_objset_sync_dnodes+0x91/0xb0 [zfs]\r\nJul  8 02:21:43 system kernel: [20904.252874]  [<ffffffffc0a130ba>] ? dmu_objset_sync+0x1ea/0x370 [zfs]\r\nJul  8 02:21:43 system kernel: [20904.252890]  [<ffffffffc09fb4c0>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nJul  8 02:21:43 system kernel: [20904.252905]  [<ffffffffc09fa180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nJul  8 02:21:43 system kernel: [20904.252920]  [<ffffffffc09fd1f0>] ? l2arc_read_done+0x400/0x400 [zfs]\r\nJul  8 02:21:43 system kernel: [20904.252940]  [<ffffffffc0a2aae9>] ? dsl_dataset_sync+0x49/0x80 [zfs]\r\nJul  8 02:21:43 system kernel: [20904.252960]  [<ffffffffc0a335bf>] ? dsl_pool_sync+0x9f/0x440 [zfs]\r\nJul  8 02:21:43 system kernel: [20904.252983]  [<ffffffffc0a4d46b>] ? spa_sync+0x37b/0xb10 [zfs]\r\nJul  8 02:21:43 system kernel: [20904.252984]  [<ffffffff97eb8774>] ? __wake_up+0x34/0x50\r\nJul  8 02:21:43 system kernel: [20904.253008]  [<ffffffffc0a5d718>] ? txg_sync_thread+0x3b8/0x610 [zfs]\r\nJul  8 02:21:43 system kernel: [20904.253031]  [<ffffffffc0a5d360>] ? txg_delay+0x160/0x160 [zfs]\r\nJul  8 02:21:43 system kernel: [20904.253035]  [<ffffffffc094dbfd>] ? thread_generic_wrapper+0x6d/0x80 [spl]\r\nJul  8 02:21:43 system kernel: [20904.253038]  [<ffffffffc094db90>] ? __thread_exit+0x20/0x20 [spl]\r\nJul  8 02:21:43 system kernel: [20904.253039]  [<ffffffff97e9655e>] ? kthread+0xce/0xf0\r\nJul  8 02:21:43 system kernel: [20904.253041]  [<ffffffff97e24701>] ? __switch_to+0x2c1/0x6c0\r\nJul  8 02:21:43 system kernel: [20904.253043]  [<ffffffff97e96490>] ? kthread_park+0x60/0x60\r\nJul  8 02:21:43 system kernel: [20904.253044]  [<ffffffff983fbff5>] ? ret_from_fork+0x25/0x30\r\nJul  8 02:23:44 system kernel: [21025.079776] INFO: task txg_sync:3506 blocked for more than 120 seconds.\r\nJul  8 02:23:44 system kernel: [21025.079810]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 02:23:44 system kernel: [21025.079825] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 02:23:44 system kernel: [21025.079843] txg_sync        D    0  3506      2 0x00000000\r\nJul  8 02:23:44 system kernel: [21025.079845]  ffff8b6476555800 ffff8b6434bf1c00 ffff8b647962c080 ffff8b64afd18300\r\nJul  8 02:23:44 system kernel: [21025.079847]  ffff8b621ad950c0 ffffab7762447878 ffffffff983f6fe3 ffffffff988137ca\r\nJul  8 02:23:44 system kernel: [21025.079849]  00203d3d20636967 ffff8b64afd18300 0000000000000286 ffff8b647962c080\r\nJul  8 02:23:44 system kernel: [21025.079850] Call Trace:\r\nJul  8 02:23:44 system kernel: [21025.079856]  [<ffffffff983f6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 02:23:44 system kernel: [21025.079858]  [<ffffffff983f74b2>] ? schedule+0x32/0x80\r\nJul  8 02:23:44 system kernel: [21025.079864]  [<ffffffffc09508b1>] ? spl_panic+0xe1/0xf0 [spl]\r\nJul  8 02:23:44 system kernel: [21025.079866]  [<ffffffff97ea8773>] ? update_load_avg+0x73/0x360\r\nJul  8 02:23:44 system kernel: [21025.079868]  [<ffffffff97fddd6b>] ? free_block+0x12b/0x1c0\r\nJul  8 02:23:44 system kernel: [21025.079870]  [<ffffffff97fe1962>] ? ___cache_free+0x1c2/0x2e0\r\nJul  8 02:23:44 system kernel: [21025.079894]  [<ffffffffc0a052ce>] ? dbuf_rele_and_unlock+0x2ce/0x3c0 [zfs]\r\nJul  8 02:23:44 system kernel: [21025.079928]  [<ffffffffc0a8c6de>] ? zfs_space_delta_cb+0xbe/0x190 [zfs]\r\nJul  8 02:23:44 system kernel: [21025.079947]  [<ffffffffc0a1402b>] ? dmu_objset_userquota_get_ids+0x10b/0x3a0 [zfs]\r\nJul  8 02:23:44 system kernel: [21025.079967]  [<ffffffffc0a2271c>] ? dnode_sync+0xcc/0x830 [zfs]\r\nJul  8 02:23:44 system kernel: [21025.079995]  [<ffffffffc0a22916>] ? dnode_sync+0x2c6/0x830 [zfs]\r\nJul  8 02:23:44 system kernel: [21025.080015]  [<ffffffffc0a12eb1>] ? dmu_objset_sync_dnodes+0x91/0xb0 [zfs]\r\nJul  8 02:23:44 system kernel: [21025.080040]  [<ffffffffc0a130ba>] ? dmu_objset_sync+0x1ea/0x370 [zfs]\r\nJul  8 02:23:44 system kernel: [21025.080076]  [<ffffffffc09fb4c0>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nJul  8 02:23:44 system kernel: [21025.080099]  [<ffffffffc09fa180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nJul  8 02:23:44 system kernel: [21025.080116]  [<ffffffffc09fd1f0>] ? l2arc_read_done+0x400/0x400 [zfs]\r\nJul  8 02:23:44 system kernel: [21025.080136]  [<ffffffffc0a2aae9>] ? dsl_dataset_sync+0x49/0x80 [zfs]\r\nJul  8 02:23:44 system kernel: [21025.080165]  [<ffffffffc0a335bf>] ? dsl_pool_sync+0x9f/0x440 [zfs]\r\nJul  8 02:23:44 system kernel: [21025.080187]  [<ffffffffc0a4d46b>] ? spa_sync+0x37b/0xb10 [zfs]\r\nJul  8 02:23:44 system kernel: [21025.080188]  [<ffffffff97eb8774>] ? __wake_up+0x34/0x50\r\nJul  8 02:23:44 system kernel: [21025.080210]  [<ffffffffc0a5d718>] ? txg_sync_thread+0x3b8/0x610 [zfs]\r\nJul  8 02:23:44 system kernel: [21025.080241]  [<ffffffffc0a5d360>] ? txg_delay+0x160/0x160 [zfs]\r\nJul  8 02:23:44 system kernel: [21025.080244]  [<ffffffffc094dbfd>] ? thread_generic_wrapper+0x6d/0x80 [spl]\r\nJul  8 02:23:44 system kernel: [21025.080247]  [<ffffffffc094db90>] ? __thread_exit+0x20/0x20 [spl]\r\nJul  8 02:23:44 system kernel: [21025.080249]  [<ffffffff97e9655e>] ? kthread+0xce/0xf0\r\nJul  8 02:23:44 system kernel: [21025.080250]  [<ffffffff97e24701>] ? __switch_to+0x2c1/0x6c0\r\nJul  8 02:23:44 system kernel: [21025.080252]  [<ffffffff97e96490>] ? kthread_park+0x60/0x60\r\nJul  8 02:23:44 system kernel: [21025.080253]  [<ffffffff983fbff5>] ? ret_from_fork+0x25/0x30\r\nJul  8 02:25:45 system kernel: [21145.910895] INFO: task txg_sync:3506 blocked for more than 120 seconds.\r\nJul  8 02:25:45 system kernel: [21145.910920]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 02:25:45 system kernel: [21145.910935] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 02:25:45 system kernel: [21145.910954] txg_sync        D    0  3506      2 0x00000000\r\nJul  8 02:25:45 system kernel: [21145.910957]  ffff8b6476555800 ffff8b6434bf1c00 ffff8b647962c080 ffff8b64afd18300\r\nJul  8 02:25:45 system kernel: [21145.910959]  ffff8b621ad950c0 ffffab7762447878 ffffffff983f6fe3 ffffffff988137ca\r\nJul  8 02:25:45 system kernel: [21145.910961]  00203d3d20636967 ffff8b64afd18300 0000000000000286 ffff8b647962c080\r\nJul  8 02:25:45 system kernel: [21145.910962] Call Trace:\r\nJul  8 02:25:45 system kernel: [21145.910968]  [<ffffffff983f6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 02:25:45 system kernel: [21145.910971]  [<ffffffff983f74b2>] ? schedule+0x32/0x80\r\nJul  8 02:25:45 system kernel: [21145.910976]  [<ffffffffc09508b1>] ? spl_panic+0xe1/0xf0 [spl]\r\nJul  8 02:25:45 system kernel: [21145.910979]  [<ffffffff97ea8773>] ? update_load_avg+0x73/0x360\r\nJul  8 02:25:45 system kernel: [21145.910981]  [<ffffffff97fddd6b>] ? free_block+0x12b/0x1c0\r\nJul  8 02:25:45 system kernel: [21145.910982]  [<ffffffff97fe1962>] ? ___cache_free+0x1c2/0x2e0\r\nJul  8 02:25:45 system kernel: [21145.911008]  [<ffffffffc0a052ce>] ? dbuf_rele_and_unlock+0x2ce/0x3c0 [zfs]\r\nJul  8 02:25:45 system kernel: [21145.911032]  [<ffffffffc0a8c6de>] ? zfs_space_delta_cb+0xbe/0x190 [zfs]\r\nJul  8 02:25:45 system kernel: [21145.911051]  [<ffffffffc0a1402b>] ? dmu_objset_userquota_get_ids+0x10b/0x3a0 [zfs]\r\nJul  8 02:25:45 system kernel: [21145.911070]  [<ffffffffc0a2271c>] ? dnode_sync+0xcc/0x830 [zfs]\r\nJul  8 02:25:45 system kernel: [21145.911088]  [<ffffffffc0a22916>] ? dnode_sync+0x2c6/0x830 [zfs]\r\nJul  8 02:25:45 system kernel: [21145.911106]  [<ffffffffc0a12eb1>] ? dmu_objset_sync_dnodes+0x91/0xb0 [zfs]\r\nJul  8 02:25:45 system kernel: [21145.911123]  [<ffffffffc0a130ba>] ? dmu_objset_sync+0x1ea/0x370 [zfs]\r\nJul  8 02:25:45 system kernel: [21145.911137]  [<ffffffffc09fb4c0>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nJul  8 02:25:45 system kernel: [21145.911152]  [<ffffffffc09fa180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nJul  8 02:25:45 system kernel: [21145.911167]  [<ffffffffc09fd1f0>] ? l2arc_read_done+0x400/0x400 [zfs]\r\nJul  8 02:25:45 system kernel: [21145.911186]  [<ffffffffc0a2aae9>] ? dsl_dataset_sync+0x49/0x80 [zfs]\r\nJul  8 02:25:45 system kernel: [21145.911209]  [<ffffffffc0a335bf>] ? dsl_pool_sync+0x9f/0x440 [zfs]\r\nJul  8 02:25:45 system kernel: [21145.911232]  [<ffffffffc0a4d46b>] ? spa_sync+0x37b/0xb10 [zfs]\r\nJul  8 02:25:45 system kernel: [21145.911234]  [<ffffffff97eb8774>] ? __wake_up+0x34/0x50\r\nJul  8 02:25:45 system kernel: [21145.911260]  [<ffffffffc0a5d718>] ? txg_sync_thread+0x3b8/0x610 [zfs]\r\nJul  8 02:25:45 system kernel: [21145.911300]  [<ffffffffc0a5d360>] ? txg_delay+0x160/0x160 [zfs]\r\nJul  8 02:25:45 system kernel: [21145.911305]  [<ffffffffc094dbfd>] ? thread_generic_wrapper+0x6d/0x80 [spl]\r\nJul  8 02:25:45 system kernel: [21145.911309]  [<ffffffffc094db90>] ? __thread_exit+0x20/0x20 [spl]\r\nJul  8 02:25:45 system kernel: [21145.911312]  [<ffffffff97e9655e>] ? kthread+0xce/0xf0\r\nJul  8 02:25:45 system kernel: [21145.911314]  [<ffffffff97e24701>] ? __switch_to+0x2c1/0x6c0\r\nJul  8 02:25:45 system kernel: [21145.911316]  [<ffffffff97e96490>] ? kthread_park+0x60/0x60\r\nJul  8 02:25:45 system kernel: [21145.911318]  [<ffffffff983fbff5>] ? ret_from_fork+0x25/0x30\r\nJul  8 02:27:46 system kernel: [21266.734082] INFO: task txg_sync:3506 blocked for more than 120 seconds.\r\nJul  8 02:27:46 system kernel: [21266.734122]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 02:27:46 system kernel: [21266.734150] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 02:27:46 system kernel: [21266.734187] txg_sync        D    0  3506      2 0x00000000\r\nJul  8 02:27:46 system kernel: [21266.734191]  ffff8b6476555800 ffff8b6434bf1c00 ffff8b647962c080 ffff8b64afd18300\r\nJul  8 02:27:46 system kernel: [21266.734195]  ffff8b621ad950c0 ffffab7762447878 ffffffff983f6fe3 ffffffff988137ca\r\nJul  8 02:27:46 system kernel: [21266.734197]  00203d3d20636967 ffff8b64afd18300 0000000000000286 ffff8b647962c080\r\nJul  8 02:27:46 system kernel: [21266.734200] Call Trace:\r\nJul  8 02:27:46 system kernel: [21266.734210]  [<ffffffff983f6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 02:27:46 system kernel: [21266.734214]  [<ffffffff983f74b2>] ? schedule+0x32/0x80\r\nJul  8 02:27:46 system kernel: [21266.734224]  [<ffffffffc09508b1>] ? spl_panic+0xe1/0xf0 [spl]\r\nJul  8 02:27:46 system kernel: [21266.734229]  [<ffffffff97ea8773>] ? update_load_avg+0x73/0x360\r\nJul  8 02:27:46 system kernel: [21266.734232]  [<ffffffff97fddd6b>] ? free_block+0x12b/0x1c0\r\nJul  8 02:27:46 system kernel: [21266.734234]  [<ffffffff97fe1962>] ? ___cache_free+0x1c2/0x2e0\r\nJul  8 02:27:46 system kernel: [21266.734275]  [<ffffffffc0a052ce>] ? dbuf_rele_and_unlock+0x2ce/0x3c0 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.734321]  [<ffffffffc0a8c6de>] ? zfs_space_delta_cb+0xbe/0x190 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.734356]  [<ffffffffc0a1402b>] ? dmu_objset_userquota_get_ids+0x10b/0x3a0 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.734391]  [<ffffffffc0a2271c>] ? dnode_sync+0xcc/0x830 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.734424]  [<ffffffffc0a22916>] ? dnode_sync+0x2c6/0x830 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.734457]  [<ffffffffc0a12eb1>] ? dmu_objset_sync_dnodes+0x91/0xb0 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.734488]  [<ffffffffc0a130ba>] ? dmu_objset_sync+0x1ea/0x370 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.734516]  [<ffffffffc09fb4c0>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.734543]  [<ffffffffc09fa180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.734571]  [<ffffffffc09fd1f0>] ? l2arc_read_done+0x400/0x400 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.734607]  [<ffffffffc0a2aae9>] ? dsl_dataset_sync+0x49/0x80 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.734643]  [<ffffffffc0a335bf>] ? dsl_pool_sync+0x9f/0x440 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.734685]  [<ffffffffc0a4d46b>] ? spa_sync+0x37b/0xb10 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.734688]  [<ffffffff97eb8774>] ? __wake_up+0x34/0x50\r\nJul  8 02:27:46 system kernel: [21266.734730]  [<ffffffffc0a5d718>] ? txg_sync_thread+0x3b8/0x610 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.734773]  [<ffffffffc0a5d360>] ? txg_delay+0x160/0x160 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.734779]  [<ffffffffc094dbfd>] ? thread_generic_wrapper+0x6d/0x80 [spl]\r\nJul  8 02:27:46 system kernel: [21266.734785]  [<ffffffffc094db90>] ? __thread_exit+0x20/0x20 [spl]\r\nJul  8 02:27:46 system kernel: [21266.734788]  [<ffffffff97e9655e>] ? kthread+0xce/0xf0\r\nJul  8 02:27:46 system kernel: [21266.734790]  [<ffffffff97e24701>] ? __switch_to+0x2c1/0x6c0\r\nJul  8 02:27:46 system kernel: [21266.734793]  [<ffffffff97e96490>] ? kthread_park+0x60/0x60\r\nJul  8 02:27:46 system kernel: [21266.734795]  [<ffffffff983fbff5>] ? ret_from_fork+0x25/0x30\r\nJul  8 02:27:46 system kernel: [21266.734835] INFO: task zfs:8112 blocked for more than 120 seconds.\r\nJul  8 02:27:46 system kernel: [21266.734865]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 02:27:46 system kernel: [21266.734892] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 02:27:46 system kernel: [21266.734928] zfs             D    0  8112   8108 0x00000000\r\nJul  8 02:27:46 system kernel: [21266.734931]  ffff8b64507c8800 0000000000000000 ffff8b63b886a0c0 ffff8b64afd58300\r\nJul  8 02:27:46 system kernel: [21266.734934]  ffff8b648bf57100 ffffab7761c33b60 ffffffff983f6fe3 0000000016a799ea\r\nJul  8 02:27:46 system kernel: [21266.734936]  0000000000000286 ffff8b64afd58300 ffffab7761c33b80 ffff8b63b886a0c0\r\nJul  8 02:27:46 system kernel: [21266.734939] Call Trace:\r\nJul  8 02:27:46 system kernel: [21266.734943]  [<ffffffff983f6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 02:27:46 system kernel: [21266.734946]  [<ffffffff983f74b2>] ? schedule+0x32/0x80\r\nJul  8 02:27:46 system kernel: [21266.734953]  [<ffffffffc095263b>] ? cv_wait_common+0x10b/0x120 [spl]\r\nJul  8 02:27:46 system kernel: [21266.734955]  [<ffffffff97eb8cc0>] ? prepare_to_wait_event+0xf0/0xf0\r\nJul  8 02:27:46 system kernel: [21266.734999]  [<ffffffffc0a5cfe5>] ? txg_wait_synced+0xd5/0x120 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.735041]  [<ffffffffc0ab2b20>] ? dsl_dataset_user_hold_sync+0x170/0x170 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.735082]  [<ffffffffc0ab2280>] ? dsl_dataset_user_hold_sync_one_impl+0x290/0x290 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.735121]  [<ffffffffc0a3b287>] ? dsl_sync_task+0x157/0x240 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.735162]  [<ffffffffc0ab2b20>] ? dsl_dataset_user_hold_sync+0x170/0x170 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.735202]  [<ffffffffc0ab2280>] ? dsl_dataset_user_hold_sync_one_impl+0x290/0x290 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.735241]  [<ffffffffc0ab26fc>] ? dsl_dataset_user_release_impl+0x19c/0x260 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.735280]  [<ffffffffc0ab2520>] ? dsl_dataset_user_release_sync+0x2a0/0x2a0 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.735283]  [<ffffffff97ea0824>] ? ttwu_do_wakeup+0x14/0xd0\r\nJul  8 02:27:46 system kernel: [21266.735326]  [<ffffffffc0a587fa>] ? spa_name_compare+0xa/0x30 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.735330]  [<ffffffffc08c2141>] ? avl_find+0x51/0x90 [zavl]\r\nJul  8 02:27:46 system kernel: [21266.735371]  [<ffffffffc0a5765e>] ? spa_lookup+0x5e/0x80 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.735373]  [<ffffffff983f921e>] ? mutex_lock+0xe/0x30\r\nJul  8 02:27:46 system kernel: [21266.735415]  [<ffffffffc0a4fdd3>] ? spa_open_common+0x2b3/0x450 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.735418]  [<ffffffff97ef8e8a>] ? do_futex+0x2ea/0xb00\r\nJul  8 02:27:46 system kernel: [21266.735459]  [<ffffffffc0ab2891>] ? dsl_dataset_user_release_onexit+0xd1/0x110 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.735501]  [<ffffffffc0a874cd>] ? zfs_onexit_destroy+0x6d/0xf0 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.735543]  [<ffffffffc0a81b9c>] ? zfsdev_release+0x2c/0x60 [zfs]\r\nJul  8 02:27:46 system kernel: [21266.735546]  [<ffffffff9800487d>] ? __fput+0xcd/0x1e0\r\nJul  8 02:27:46 system kernel: [21266.735549]  [<ffffffff97e94a89>] ? task_work_run+0x79/0xa0\r\nJul  8 02:27:46 system kernel: [21266.735551]  [<ffffffff97e03284>] ? exit_to_usermode_loop+0xa4/0xb0\r\nJul  8 02:27:46 system kernel: [21266.735554]  [<ffffffff97e03a94>] ? syscall_return_slowpath+0x54/0x60\r\nJul  8 02:27:46 system kernel: [21266.735556]  [<ffffffff983fbe08>] ? system_call_fast_compare_end+0x99/0x9b\r\nJul  8 02:29:47 system kernel: [21387.561230] INFO: task txg_sync:3506 blocked for more than 120 seconds.\r\nJul  8 02:29:47 system kernel: [21387.561269]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 02:29:47 system kernel: [21387.561296] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 02:29:47 system kernel: [21387.561334] txg_sync        D    0  3506      2 0x00000000\r\nJul  8 02:29:47 system kernel: [21387.561338]  ffff8b6476555800 ffff8b6434bf1c00 ffff8b647962c080 ffff8b64afd18300\r\nJul  8 02:29:47 system kernel: [21387.561341]  ffff8b621ad950c0 ffffab7762447878 ffffffff983f6fe3 ffffffff988137ca\r\nJul  8 02:29:47 system kernel: [21387.561344]  00203d3d20636967 ffff8b64afd18300 0000000000000286 ffff8b647962c080\r\nJul  8 02:29:47 system kernel: [21387.561347] Call Trace:\r\nJul  8 02:29:47 system kernel: [21387.561356]  [<ffffffff983f6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 02:29:47 system kernel: [21387.561361]  [<ffffffff983f74b2>] ? schedule+0x32/0x80\r\nJul  8 02:29:47 system kernel: [21387.561370]  [<ffffffffc09508b1>] ? spl_panic+0xe1/0xf0 [spl]\r\nJul  8 02:29:47 system kernel: [21387.561375]  [<ffffffff97ea8773>] ? update_load_avg+0x73/0x360\r\nJul  8 02:29:47 system kernel: [21387.561378]  [<ffffffff97fddd6b>] ? free_block+0x12b/0x1c0\r\nJul  8 02:29:47 system kernel: [21387.561380]  [<ffffffff97fe1962>] ? ___cache_free+0x1c2/0x2e0\r\nJul  8 02:29:47 system kernel: [21387.561421]  [<ffffffffc0a052ce>] ? dbuf_rele_and_unlock+0x2ce/0x3c0 [zfs]\r\nJul  8 02:29:47 system kernel: [21387.561467]  [<ffffffffc0a8c6de>] ? zfs_space_delta_cb+0xbe/0x190 [zfs]\r\nJul  8 02:29:47 system kernel: [21387.561502]  [<ffffffffc0a1402b>] ? dmu_objset_userquota_get_ids+0x10b/0x3a0 [zfs]\r\nJul  8 02:29:47 system kernel: [21387.561538]  [<ffffffffc0a2271c>] ? dnode_sync+0xcc/0x830 [zfs]\r\nJul  8 02:29:47 system kernel: [21387.561571]  [<ffffffffc0a22916>] ? dnode_sync+0x2c6/0x830 [zfs]\r\nJul  8 02:29:47 system kernel: [21387.561604]  [<ffffffffc0a12eb1>] ? dmu_objset_sync_dnodes+0x91/0xb0 [zfs]\r\nJul  8 02:29:47 system kernel: [21387.561635]  [<ffffffffc0a130ba>] ? dmu_objset_sync+0x1ea/0x370 [zfs]\r\nJul  8 02:29:47 system kernel: [21387.561663]  [<ffffffffc09fb4c0>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nJul  8 02:29:47 system kernel: [21387.561690]  [<ffffffffc09fa180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nJul  8 02:29:47 system kernel: [21387.561718]  [<ffffffffc09fd1f0>] ? l2arc_read_done+0x400/0x400 [zfs]\r\nJul  8 02:29:47 system kernel: [21387.561754]  [<ffffffffc0a2aae9>] ? dsl_dataset_sync+0x49/0x80 [zfs]\r\nJul  8 02:29:47 system kernel: [21387.561790]  [<ffffffffc0a335bf>] ? dsl_pool_sync+0x9f/0x440 [zfs]\r\nJul  8 02:29:47 system kernel: [21387.561833]  [<ffffffffc0a4d46b>] ? spa_sync+0x37b/0xb10 [zfs]\r\nJul  8 02:29:47 system kernel: [21387.561835]  [<ffffffff97eb8774>] ? __wake_up+0x34/0x50\r\nJul  8 02:29:47 system kernel: [21387.561879]  [<ffffffffc0a5d718>] ? txg_sync_thread+0x3b8/0x610 [zfs]\r\nJul  8 02:29:47 system kernel: [21387.561921]  [<ffffffffc0a5d360>] ? txg_delay+0x160/0x160 [zfs]\r\nJul  8 02:29:47 system kernel: [21387.561927]  [<ffffffffc094dbfd>] ? thread_generic_wrapper+0x6d/0x80 [spl]\r\nJul  8 02:29:47 system kernel: [21387.561933]  [<ffffffffc094db90>] ? __thread_exit+0x20/0x20 [spl]\r\nJul  8 02:29:47 system kernel: [21387.561936]  [<ffffffff97e9655e>] ? kthread+0xce/0xf0\r\nJul  8 02:29:47 system kernel: [21387.561939]  [<ffffffff97e24701>] ? __switch_to+0x2c1/0x6c0\r\nJul  8 02:29:47 system kernel: [21387.561941]  [<ffffffff97e96490>] ? kthread_park+0x60/0x60\r\nJul  8 02:29:47 system kernel: [21387.561944]  [<ffffffff983fbff5>] ? ret_from_fork+0x25/0x30\r\n```\r\n\r\nBefore I rebooted, I confirmed that both the rsync and the zfs commands were still running. I was logging the output of the zfs commands, and it did log the final \"receiving incremental stream\" line relating to the last snapshot of the last filesystem that was to be sent per the initial estimate when the commands first started. But, the processes did not exit.\r\n\r\nAfter reboot, it looked like the final snapshot of the final filesystem was indeed on poolB, though `zpool history` for poolA or poolB did not show the send/recv commands.\r\n\r\nEventually I did the final move by creating new snapshots with everything unmounted, did an incremental update with `zfs send -vRI @snap poolA/backups@final-snap | zfs receive -uFv poolB/backups`. After this completed, I again confirmed (using `zfs list`) that everything was showing up on the poolB side, before exporting poolA, and reconfiguring mounts and rebooting the system to run only with poolB.\r\n\r\nAfter the system was running with poolB only for a while, one of the rsync backup cronjobs ran, with a destination in the same hierarchy of filesystems that I ran into the first panic with (but this time with poolB as the actual pool involved), and I got another panic. This time, there was no send/recv taking place at the same time.\r\n\r\nThere are other processes appearing in this log, like dropbox, youtube-dl, etc., and those are all processes that were interacting with other ZFS filesystems at the time. But, those same processes have been running with no issue otherwise, so I suspect it's related to rsync and the filesystems I had issues with during the send/recv process.\r\n\r\n```\r\nJul  8 06:00:20 system kernel: [ 2215.962287] VERIFY3(sa.sa_magic == 0x2F505A) failed (1499508020 == 3100762)\r\nJul  8 06:00:20 system kernel: [ 2215.962347] PANIC at zfs_vfsops.c:426:zfs_space_delta_cb()\r\nJul  8 06:00:20 system kernel: [ 2215.962378] Showing stack for process 2351\r\nJul  8 06:00:20 system kernel: [ 2215.962382] CPU: 2 PID: 2351 Comm: txg_sync Tainted: P           OE   4.9.0-1-amd64 #1 Debian 4.9.6-3\r\nJul  8 06:00:20 system kernel: [ 2215.962383] Hardware name: Supermicro X9SCL/X9SCM/X9SCL/X9SCM, BIOS 2.2 02/20/2015\r\nJul  8 06:00:20 system kernel: [ 2215.962384]  0000000000000000 ffffffff8fd28e34 ffffffffc0972713 ffffa98d041bfa10\r\nJul  8 06:00:20 system kernel: [ 2215.962388]  ffffffffc078b88b 0000000000000001 ffff8ef100000030 ffffa98d041bfa20\r\nJul  8 06:00:20 system kernel: [ 2215.962391]  ffffa98d041bf9c0 2833594649524556 616d5f61732e6173 30203d3d20636967\r\nJul  8 06:00:20 system kernel: [ 2215.962393] Call Trace:\r\nJul  8 06:00:20 system kernel: [ 2215.962401]  [<ffffffff8fd28e34>] ? dump_stack+0x5c/0x78\r\nJul  8 06:00:20 system kernel: [ 2215.962411]  [<ffffffffc078b88b>] ? spl_panic+0xbb/0xf0 [spl]\r\nJul  8 06:00:20 system kernel: [ 2215.962415]  [<ffffffff8faa8700>] ? update_cfs_rq_load_avg+0x490/0x490\r\nJul  8 06:00:20 system kernel: [ 2215.962417]  [<ffffffff8fbddd6b>] ? free_block+0x12b/0x1c0\r\nJul  8 06:00:20 system kernel: [ 2215.962420]  [<ffffffff8fbe1962>] ? ___cache_free+0x1c2/0x2e0\r\nJul  8 06:00:20 system kernel: [ 2215.962459]  [<ffffffffc089f2ce>] ? dbuf_rele_and_unlock+0x2ce/0x3c0 [zfs]\r\nJul  8 06:00:20 system kernel: [ 2215.962501]  [<ffffffffc09266de>] ? zfs_space_delta_cb+0xbe/0x190 [zfs]\r\nJul  8 06:00:20 system kernel: [ 2215.962534]  [<ffffffffc08ae02b>] ? dmu_objset_userquota_get_ids+0x10b/0x3a0 [zfs]\r\nJul  8 06:00:20 system kernel: [ 2215.962560]  [<ffffffffc08bc71c>] ? dnode_sync+0xcc/0x830 [zfs]\r\nJul  8 06:00:20 system kernel: [ 2215.962582]  [<ffffffffc08aceb1>] ? dmu_objset_sync_dnodes+0x91/0xb0 [zfs]\r\nJul  8 06:00:20 system kernel: [ 2215.962602]  [<ffffffffc08ad0ba>] ? dmu_objset_sync+0x1ea/0x370 [zfs]\r\nJul  8 06:00:20 system kernel: [ 2215.962621]  [<ffffffffc08954c0>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nJul  8 06:00:20 system kernel: [ 2215.962648]  [<ffffffffc0894180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nJul  8 06:00:20 system kernel: [ 2215.962676]  [<ffffffffc08971f0>] ? l2arc_read_done+0x400/0x400 [zfs]\r\nJul  8 06:00:20 system kernel: [ 2215.962712]  [<ffffffffc08c4ae9>] ? dsl_dataset_sync+0x49/0x80 [zfs]\r\nJul  8 06:00:20 system kernel: [ 2215.962756]  [<ffffffffc08cd5bf>] ? dsl_pool_sync+0x9f/0x440 [zfs]\r\nJul  8 06:00:20 system kernel: [ 2215.962793]  [<ffffffffc08e746b>] ? spa_sync+0x37b/0xb10 [zfs]\r\nJul  8 06:00:20 system kernel: [ 2215.962796]  [<ffffffff8fab8774>] ? __wake_up+0x34/0x50\r\nJul  8 06:00:20 system kernel: [ 2215.962841]  [<ffffffffc08f7718>] ? txg_sync_thread+0x3b8/0x610 [zfs]\r\nJul  8 06:00:20 system kernel: [ 2215.962880]  [<ffffffffc08f7360>] ? txg_delay+0x160/0x160 [zfs]\r\nJul  8 06:00:20 system kernel: [ 2215.962883]  [<ffffffff8fa7ba4a>] ? do_group_exit+0x3a/0xa0\r\nJul  8 06:00:20 system kernel: [ 2215.962890]  [<ffffffffc0788bfd>] ? thread_generic_wrapper+0x6d/0x80 [spl]\r\nJul  8 06:00:20 system kernel: [ 2215.962896]  [<ffffffffc0788b90>] ? __thread_exit+0x20/0x20 [spl]\r\nJul  8 06:00:20 system kernel: [ 2215.962899]  [<ffffffff8fa9655e>] ? kthread+0xce/0xf0\r\nJul  8 06:00:20 system kernel: [ 2215.962901]  [<ffffffff8fa24701>] ? __switch_to+0x2c1/0x6c0\r\nJul  8 06:00:20 system kernel: [ 2215.962903]  [<ffffffff8fa96490>] ? kthread_park+0x60/0x60\r\nJul  8 06:00:20 system kernel: [ 2215.962906]  [<ffffffff8fffbff5>] ? ret_from_fork+0x25/0x30\r\nJul  8 06:03:42 system kernel: [ 2417.672875] INFO: task txg_sync:2351 blocked for more than 120 seconds.\r\nJul  8 06:03:42 system kernel: [ 2417.672914]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 06:03:42 system kernel: [ 2417.672941] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 06:03:42 system kernel: [ 2417.672978] txg_sync        D    0  2351      2 0x00000000\r\nJul  8 06:03:42 system kernel: [ 2417.672982]  ffff8ef0f2da1800 0000000000000000 ffff8ef1fc1e40c0 ffff8ef22fc98300\r\nJul  8 06:03:42 system kernel: [ 2417.672986]  ffff8ef206494040 ffffa98d041bf878 ffffffff8fff6fe3 0000000000000001\r\nJul  8 06:03:42 system kernel: [ 2417.672988]  0000000000000000 ffff8ef22fc98300 0000000000000286 ffff8ef1fc1e40c0\r\nJul  8 06:03:42 system kernel: [ 2417.672991] Call Trace:\r\nJul  8 06:03:42 system kernel: [ 2417.673001]  [<ffffffff8fff6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 06:03:42 system kernel: [ 2417.673010]  [<ffffffff8fff74b2>] ? schedule+0x32/0x80\r\nJul  8 06:03:42 system kernel: [ 2417.673023]  [<ffffffffc078b8b1>] ? spl_panic+0xe1/0xf0 [spl]\r\nJul  8 06:03:42 system kernel: [ 2417.673029]  [<ffffffff8faa8700>] ? update_cfs_rq_load_avg+0x490/0x490\r\nJul  8 06:03:42 system kernel: [ 2417.673034]  [<ffffffff8fbddd6b>] ? free_block+0x12b/0x1c0\r\nJul  8 06:03:42 system kernel: [ 2417.673037]  [<ffffffff8fbe1962>] ? ___cache_free+0x1c2/0x2e0\r\nJul  8 06:03:42 system kernel: [ 2417.673086]  [<ffffffffc089f2ce>] ? dbuf_rele_and_unlock+0x2ce/0x3c0 [zfs]\r\nJul  8 06:03:42 system kernel: [ 2417.673140]  [<ffffffffc09266de>] ? zfs_space_delta_cb+0xbe/0x190 [zfs]\r\nJul  8 06:03:42 system kernel: [ 2417.673192]  [<ffffffffc08ae02b>] ? dmu_objset_userquota_get_ids+0x10b/0x3a0 [zfs]\r\nJul  8 06:03:42 system kernel: [ 2417.673228]  [<ffffffffc08bc71c>] ? dnode_sync+0xcc/0x830 [zfs]\r\nJul  8 06:03:42 system kernel: [ 2417.673261]  [<ffffffffc08aceb1>] ? dmu_objset_sync_dnodes+0x91/0xb0 [zfs]\r\nJul  8 06:03:42 system kernel: [ 2417.673292]  [<ffffffffc08ad0ba>] ? dmu_objset_sync+0x1ea/0x370 [zfs]\r\nJul  8 06:03:42 system kernel: [ 2417.673320]  [<ffffffffc08954c0>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nJul  8 06:03:42 system kernel: [ 2417.673347]  [<ffffffffc0894180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nJul  8 06:03:42 system kernel: [ 2417.673375]  [<ffffffffc08971f0>] ? l2arc_read_done+0x400/0x400 [zfs]\r\nJul  8 06:03:42 system kernel: [ 2417.673411]  [<ffffffffc08c4ae9>] ? dsl_dataset_sync+0x49/0x80 [zfs]\r\nJul  8 06:03:42 system kernel: [ 2417.673448]  [<ffffffffc08cd5bf>] ? dsl_pool_sync+0x9f/0x440 [zfs]\r\nJul  8 06:03:42 system kernel: [ 2417.673490]  [<ffffffffc08e746b>] ? spa_sync+0x37b/0xb10 [zfs]\r\nJul  8 06:03:42 system kernel: [ 2417.673493]  [<ffffffff8fab8774>] ? __wake_up+0x34/0x50\r\nJul  8 06:03:42 system kernel: [ 2417.673536]  [<ffffffffc08f7718>] ? txg_sync_thread+0x3b8/0x610 [zfs]\r\nJul  8 06:03:42 system kernel: [ 2417.673579]  [<ffffffffc08f7360>] ? txg_delay+0x160/0x160 [zfs]\r\nJul  8 06:03:42 system kernel: [ 2417.673581]  [<ffffffff8fa7ba4a>] ? do_group_exit+0x3a/0xa0\r\nJul  8 06:03:42 system kernel: [ 2417.673588]  [<ffffffffc0788bfd>] ? thread_generic_wrapper+0x6d/0x80 [spl]\r\nJul  8 06:03:42 system kernel: [ 2417.673594]  [<ffffffffc0788b90>] ? __thread_exit+0x20/0x20 [spl]\r\nJul  8 06:03:42 system kernel: [ 2417.673596]  [<ffffffff8fa9655e>] ? kthread+0xce/0xf0\r\nJul  8 06:03:42 system kernel: [ 2417.673600]  [<ffffffff8fa24701>] ? __switch_to+0x2c1/0x6c0\r\nJul  8 06:03:42 system kernel: [ 2417.673602]  [<ffffffff8fa96490>] ? kthread_park+0x60/0x60\r\nJul  8 06:03:42 system kernel: [ 2417.673605]  [<ffffffff8fffbff5>] ? ret_from_fork+0x25/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.498942] INFO: task txg_sync:2351 blocked for more than 120 seconds.\r\nJul  8 06:05:43 system kernel: [ 2538.498980]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 06:05:43 system kernel: [ 2538.499007] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 06:05:43 system kernel: [ 2538.499044] txg_sync        D    0  2351      2 0x00000000\r\nJul  8 06:05:43 system kernel: [ 2538.499049]  ffff8ef0f2da1800 0000000000000000 ffff8ef1fc1e40c0 ffff8ef22fc98300\r\nJul  8 06:05:43 system kernel: [ 2538.499052]  ffff8ef206494040 ffffa98d041bf878 ffffffff8fff6fe3 0000000000000001\r\nJul  8 06:05:43 system kernel: [ 2538.499055]  0000000000000000 ffff8ef22fc98300 0000000000000286 ffff8ef1fc1e40c0\r\nJul  8 06:05:43 system kernel: [ 2538.499058] Call Trace:\r\nJul  8 06:05:43 system kernel: [ 2538.499066]  [<ffffffff8fff6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 06:05:43 system kernel: [ 2538.499071]  [<ffffffff8fff74b2>] ? schedule+0x32/0x80\r\nJul  8 06:05:43 system kernel: [ 2538.499081]  [<ffffffffc078b8b1>] ? spl_panic+0xe1/0xf0 [spl]\r\nJul  8 06:05:43 system kernel: [ 2538.499085]  [<ffffffff8faa8700>] ? update_cfs_rq_load_avg+0x490/0x490\r\nJul  8 06:05:43 system kernel: [ 2538.499088]  [<ffffffff8fbddd6b>] ? free_block+0x12b/0x1c0\r\nJul  8 06:05:43 system kernel: [ 2538.499091]  [<ffffffff8fbe1962>] ? ___cache_free+0x1c2/0x2e0\r\nJul  8 06:05:43 system kernel: [ 2538.499132]  [<ffffffffc089f2ce>] ? dbuf_rele_and_unlock+0x2ce/0x3c0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.499177]  [<ffffffffc09266de>] ? zfs_space_delta_cb+0xbe/0x190 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.499212]  [<ffffffffc08ae02b>] ? dmu_objset_userquota_get_ids+0x10b/0x3a0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.499248]  [<ffffffffc08bc71c>] ? dnode_sync+0xcc/0x830 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.499281]  [<ffffffffc08aceb1>] ? dmu_objset_sync_dnodes+0x91/0xb0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.499312]  [<ffffffffc08ad0ba>] ? dmu_objset_sync+0x1ea/0x370 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.499340]  [<ffffffffc08954c0>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.499367]  [<ffffffffc0894180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.499395]  [<ffffffffc08971f0>] ? l2arc_read_done+0x400/0x400 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.499431]  [<ffffffffc08c4ae9>] ? dsl_dataset_sync+0x49/0x80 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.499467]  [<ffffffffc08cd5bf>] ? dsl_pool_sync+0x9f/0x440 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.499509]  [<ffffffffc08e746b>] ? spa_sync+0x37b/0xb10 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.499512]  [<ffffffff8fab8774>] ? __wake_up+0x34/0x50\r\nJul  8 06:05:43 system kernel: [ 2538.499555]  [<ffffffffc08f7718>] ? txg_sync_thread+0x3b8/0x610 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.499597]  [<ffffffffc08f7360>] ? txg_delay+0x160/0x160 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.499599]  [<ffffffff8fa7ba4a>] ? do_group_exit+0x3a/0xa0\r\nJul  8 06:05:43 system kernel: [ 2538.499606]  [<ffffffffc0788bfd>] ? thread_generic_wrapper+0x6d/0x80 [spl]\r\nJul  8 06:05:43 system kernel: [ 2538.499611]  [<ffffffffc0788b90>] ? __thread_exit+0x20/0x20 [spl]\r\nJul  8 06:05:43 system kernel: [ 2538.499614]  [<ffffffff8fa9655e>] ? kthread+0xce/0xf0\r\nJul  8 06:05:43 system kernel: [ 2538.499617]  [<ffffffff8fa24701>] ? __switch_to+0x2c1/0x6c0\r\nJul  8 06:05:43 system kernel: [ 2538.499620]  [<ffffffff8fa96490>] ? kthread_park+0x60/0x60\r\nJul  8 06:05:43 system kernel: [ 2538.499623]  [<ffffffff8fffbff5>] ? ret_from_fork+0x25/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.499633] INFO: task dropbox:4911 blocked for more than 120 seconds.\r\nJul  8 06:05:43 system kernel: [ 2538.499665]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 06:05:43 system kernel: [ 2538.499691] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 06:05:43 system kernel: [ 2538.499728] dropbox         D    0  4911      1 0x00000000\r\nJul  8 06:05:43 system kernel: [ 2538.499730]  ffff8ef1f37dd400 0000000000000000 ffff8ef1fb5a4040 ffff8ef22fc58300\r\nJul  8 06:05:43 system kernel: [ 2538.499733]  ffff8ef20bf3c000 ffffa98d2b57fa68 ffffffff8fff6fe3 ffff8eef5630e040\r\nJul  8 06:05:43 system kernel: [ 2538.499736]  00000000d4c5c916 ffff8ef22fc58300 ffffa98d2b57fa88 ffff8ef1fb5a4040\r\nJul  8 06:05:43 system kernel: [ 2538.499738] Call Trace:\r\nJul  8 06:05:43 system kernel: [ 2538.499743]  [<ffffffff8fff6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 06:05:43 system kernel: [ 2538.499746]  [<ffffffff8fff74b2>] ? schedule+0x32/0x80\r\nJul  8 06:05:43 system kernel: [ 2538.499753]  [<ffffffffc078d63b>] ? cv_wait_common+0x10b/0x120 [spl]\r\nJul  8 06:05:43 system kernel: [ 2538.499755]  [<ffffffff8fab8cc0>] ? prepare_to_wait_event+0xf0/0xf0\r\nJul  8 06:05:43 system kernel: [ 2538.499791]  [<ffffffffc08b5936>] ? dmu_tx_wait+0x96/0x340 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.499793]  [<ffffffff8fff921e>] ? mutex_lock+0xe/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.499825]  [<ffffffffc08b5c67>] ? dmu_tx_assign+0x87/0x470 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.499860]  [<ffffffffc08be582>] ? dsl_dataset_block_freeable+0x42/0x60 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.499903]  [<ffffffffc092e0ea>] ? zfs_write+0x3ba/0xb40 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.499955]  [<ffffffffc0928596>] ? zfs_open+0x76/0xf0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.500015]  [<ffffffffc0942ad1>] ? zpl_write_common_iovec+0x91/0xf0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.500073]  [<ffffffffc09433e0>] ? zpl_iter_write+0xb0/0xe0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.500078]  [<ffffffff8fc0205a>] ? new_sync_write+0xda/0x130\r\nJul  8 06:05:43 system kernel: [ 2538.500082]  [<ffffffff8fc027c0>] ? vfs_write+0xb0/0x190\r\nJul  8 06:05:43 system kernel: [ 2538.500085]  [<ffffffff8fc03ba2>] ? SyS_write+0x52/0xc0\r\nJul  8 06:05:43 system kernel: [ 2538.500089]  [<ffffffff8fffbd7b>] ? system_call_fast_compare_end+0xc/0x9b\r\nJul  8 06:05:43 system kernel: [ 2538.500096] INFO: task dropbox:4979 blocked for more than 120 seconds.\r\nJul  8 06:05:43 system kernel: [ 2538.500152]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 06:05:43 system kernel: [ 2538.500199] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 06:05:43 system kernel: [ 2538.500262] dropbox         D    0  4979      1 0x00000000\r\nJul  8 06:05:43 system kernel: [ 2538.500266]  ffff8ef1f4ec5000 0000000000000000 ffff8ef1cb692080 ffff8ef22fd58300\r\nJul  8 06:05:43 system kernel: [ 2538.500270]  ffff8ef20bf57100 ffffa98d0409bad0 ffffffff8fff6fe3 000000006ec76cdb\r\nJul  8 06:05:43 system kernel: [ 2538.500274]  00ff8ef1dc24c3e0 ffff8ef22fd58300 ffffa98d0409baf0 ffff8ef1cb692080\r\nJul  8 06:05:43 system kernel: [ 2538.500278] Call Trace:\r\nJul  8 06:05:43 system kernel: [ 2538.500283]  [<ffffffff8fff6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 06:05:43 system kernel: [ 2538.500287]  [<ffffffff8fff74b2>] ? schedule+0x32/0x80\r\nJul  8 06:05:43 system kernel: [ 2538.500298]  [<ffffffffc078d63b>] ? cv_wait_common+0x10b/0x120 [spl]\r\nJul  8 06:05:43 system kernel: [ 2538.500301]  [<ffffffff8fab8cc0>] ? prepare_to_wait_event+0xf0/0xf0\r\nJul  8 06:05:43 system kernel: [ 2538.500351]  [<ffffffffc08b5936>] ? dmu_tx_wait+0x96/0x340 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.500354]  [<ffffffff8fff921e>] ? mutex_lock+0xe/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.500406]  [<ffffffffc08b5c67>] ? dmu_tx_assign+0x87/0x470 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.500450]  [<ffffffffc092d38a>] ? zfs_dirty_inode+0xfa/0x2f0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.500452]  [<ffffffff8fbfe8ea>] ? __check_object_size+0xfa/0x1d8\r\nJul  8 06:05:43 system kernel: [ 2538.500495]  [<ffffffffc0923456>] ? zfs_range_unlock+0x156/0x250 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.500497]  [<ffffffff8fd47899>] ? list_del+0x9/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.500539]  [<ffffffffc0923495>] ? zfs_range_unlock+0x195/0x250 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.500580]  [<ffffffffc09448b5>] ? zpl_dirty_inode+0x25/0x40 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.500583]  [<ffffffff8fc30f55>] ? __mark_inode_dirty+0x165/0x350\r\nJul  8 06:05:43 system kernel: [ 2538.500586]  [<ffffffff8fc1e949>] ? generic_update_time+0x79/0xd0\r\nJul  8 06:05:43 system kernel: [ 2538.500589]  [<ffffffff8fc1eaa6>] ? current_time+0x36/0x70\r\nJul  8 06:05:43 system kernel: [ 2538.500592]  [<ffffffff8fc205f9>] ? touch_atime+0xa9/0xd0\r\nJul  8 06:05:43 system kernel: [ 2538.500632]  [<ffffffffc09434ce>] ? zpl_iter_read+0xbe/0xe0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.500635]  [<ffffffff8fc01f07>] ? new_sync_read+0xd7/0x120\r\nJul  8 06:05:43 system kernel: [ 2538.500637]  [<ffffffff8fc02671>] ? vfs_read+0x91/0x130\r\nJul  8 06:05:43 system kernel: [ 2538.500639]  [<ffffffff8fc03ae2>] ? SyS_read+0x52/0xc0\r\nJul  8 06:05:43 system kernel: [ 2538.500642]  [<ffffffff8fffbd7b>] ? system_call_fast_compare_end+0xc/0x9b\r\nJul  8 06:05:43 system kernel: [ 2538.500671] INFO: task afpd:4938 blocked for more than 120 seconds.\r\nJul  8 06:05:43 system kernel: [ 2538.500702]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 06:05:43 system kernel: [ 2538.500729] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 06:05:43 system kernel: [ 2538.500765] afpd            D    0  4938   3895 0x00000004\r\nJul  8 06:05:43 system kernel: [ 2538.500768]  ffff8ef1fb848400 0000000000000000 ffff8ef1fb869100 ffff8ef22fc18300\r\nJul  8 06:05:43 system kernel: [ 2538.500770]  ffffffff9060e500 ffffa98d2b6c7ad0 ffffffff8fff6fe3 00000000306826a7\r\nJul  8 06:05:43 system kernel: [ 2538.500773]  00ff8ef14ec40620 ffff8ef22fc18300 ffffa98d2b6c7af0 ffff8ef1fb869100\r\nJul  8 06:05:43 system kernel: [ 2538.500776] Call Trace:\r\nJul  8 06:05:43 system kernel: [ 2538.500779]  [<ffffffff8fff6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 06:05:43 system kernel: [ 2538.500782]  [<ffffffff8fff74b2>] ? schedule+0x32/0x80\r\nJul  8 06:05:43 system kernel: [ 2538.500789]  [<ffffffffc078d63b>] ? cv_wait_common+0x10b/0x120 [spl]\r\nJul  8 06:05:43 system kernel: [ 2538.500792]  [<ffffffff8fab8cc0>] ? prepare_to_wait_event+0xf0/0xf0\r\nJul  8 06:05:43 system kernel: [ 2538.500827]  [<ffffffffc08b5936>] ? dmu_tx_wait+0x96/0x340 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.500829]  [<ffffffff8fff921e>] ? mutex_lock+0xe/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.500861]  [<ffffffffc08b5c67>] ? dmu_tx_assign+0x87/0x470 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.500904]  [<ffffffffc092d38a>] ? zfs_dirty_inode+0xfa/0x2f0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.500907]  [<ffffffff8fbfe8ea>] ? __check_object_size+0xfa/0x1d8\r\nJul  8 06:05:43 system kernel: [ 2538.500960]  [<ffffffffc0923456>] ? zfs_range_unlock+0x156/0x250 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.500963]  [<ffffffff8fd47899>] ? list_del+0x9/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.501003]  [<ffffffffc0923495>] ? zfs_range_unlock+0x195/0x250 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.501005]  [<ffffffff8fff921e>] ? mutex_lock+0xe/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.501046]  [<ffffffffc09448b5>] ? zpl_dirty_inode+0x25/0x40 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.501048]  [<ffffffff8fc30f55>] ? __mark_inode_dirty+0x165/0x350\r\nJul  8 06:05:43 system kernel: [ 2538.501051]  [<ffffffff8fc1e949>] ? generic_update_time+0x79/0xd0\r\nJul  8 06:05:43 system kernel: [ 2538.501053]  [<ffffffff8fc1eaa6>] ? current_time+0x36/0x70\r\nJul  8 06:05:43 system kernel: [ 2538.501056]  [<ffffffff8fc205f9>] ? touch_atime+0xa9/0xd0\r\nJul  8 06:05:43 system kernel: [ 2538.501095]  [<ffffffffc09434ce>] ? zpl_iter_read+0xbe/0xe0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.501098]  [<ffffffff8fc01f07>] ? new_sync_read+0xd7/0x120\r\nJul  8 06:05:43 system kernel: [ 2538.501101]  [<ffffffff8fc02671>] ? vfs_read+0x91/0x130\r\nJul  8 06:05:43 system kernel: [ 2538.501103]  [<ffffffff8fc03ca0>] ? SyS_pread64+0x90/0xb0\r\nJul  8 06:05:43 system kernel: [ 2538.501105]  [<ffffffff8fffbd7b>] ? system_call_fast_compare_end+0xc/0x9b\r\nJul  8 06:05:43 system kernel: [ 2538.501109] INFO: task irssi:4965 blocked for more than 120 seconds.\r\nJul  8 06:05:43 system kernel: [ 2538.501141]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 06:05:43 system kernel: [ 2538.501167] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 06:05:43 system kernel: [ 2538.501204] irssi           D    0  4965   4964 0x00000000\r\nJul  8 06:05:43 system kernel: [ 2538.501206]  ffff8ef1b5f14400 0000000000000000 ffff8ef1fca550c0 ffff8ef22fc18300\r\nJul  8 06:05:43 system kernel: [ 2538.501209]  ffffffff9060e500 ffffa98d04063a68 ffffffff8fff6fe3 ffff8ef144940a38\r\nJul  8 06:05:43 system kernel: [ 2538.501211]  000000005fdfe230 ffff8ef22fc18300 ffffa98d04063a88 ffff8ef1fca550c0\r\nJul  8 06:05:43 system kernel: [ 2538.501214] Call Trace:\r\nJul  8 06:05:43 system kernel: [ 2538.501217]  [<ffffffff8fff6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 06:05:43 system kernel: [ 2538.501220]  [<ffffffff8fff74b2>] ? schedule+0x32/0x80\r\nJul  8 06:05:43 system kernel: [ 2538.501227]  [<ffffffffc078d63b>] ? cv_wait_common+0x10b/0x120 [spl]\r\nJul  8 06:05:43 system kernel: [ 2538.501230]  [<ffffffff8fab8cc0>] ? prepare_to_wait_event+0xf0/0xf0\r\nJul  8 06:05:43 system kernel: [ 2538.501264]  [<ffffffffc08b5936>] ? dmu_tx_wait+0x96/0x340 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.501266]  [<ffffffff8fff921e>] ? mutex_lock+0xe/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.501298]  [<ffffffffc08b5c67>] ? dmu_tx_assign+0x87/0x470 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.501333]  [<ffffffffc08be582>] ? dsl_dataset_block_freeable+0x42/0x60 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.501373]  [<ffffffffc092e0ea>] ? zfs_write+0x3ba/0xb40 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.501377]  [<ffffffff8fc0dc62>] ? lookup_fast+0x52/0x2e0\r\nJul  8 06:05:43 system kernel: [ 2538.501380]  [<ffffffff8fc2493e>] ? legitimize_mnt+0xe/0x50\r\nJul  8 06:05:43 system kernel: [ 2538.501420]  [<ffffffffc0942ad1>] ? zpl_write_common_iovec+0x91/0xf0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.501459]  [<ffffffffc09433e0>] ? zpl_iter_write+0xb0/0xe0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.501462]  [<ffffffff8fc0205a>] ? new_sync_write+0xda/0x130\r\nJul  8 06:05:43 system kernel: [ 2538.501464]  [<ffffffff8fc027c0>] ? vfs_write+0xb0/0x190\r\nJul  8 06:05:43 system kernel: [ 2538.501467]  [<ffffffff8fc03ba2>] ? SyS_write+0x52/0xc0\r\nJul  8 06:05:43 system kernel: [ 2538.501469]  [<ffffffff8fffbd7b>] ? system_call_fast_compare_end+0xc/0x9b\r\nJul  8 06:05:43 system kernel: [ 2538.501481] INFO: task rsync:16446 blocked for more than 120 seconds.\r\nJul  8 06:05:43 system kernel: [ 2538.501513]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 06:05:43 system kernel: [ 2538.501539] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 06:05:43 system kernel: [ 2538.501575] rsync           D    0 16446  16444 0x00000000\r\nJul  8 06:05:43 system kernel: [ 2538.501578]  ffff8ef1b5f14c00 0000000000000000 ffff8ef0f78bc100 ffff8ef22fdd8300\r\nJul  8 06:05:43 system kernel: [ 2538.501580]  ffff8ef20bf61000 ffffa98d34717a68 ffffffff8fff6fe3 ffff8ef0dd71b920\r\nJul  8 06:05:43 system kernel: [ 2538.501583]  00ff8ef0dd71b5d0 ffff8ef22fdd8300 ffffa98d34717a88 ffff8ef0f78bc100\r\nJul  8 06:05:43 system kernel: [ 2538.501586] Call Trace:\r\nJul  8 06:05:43 system kernel: [ 2538.501589]  [<ffffffff8fff6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 06:05:43 system kernel: [ 2538.501592]  [<ffffffff8fff74b2>] ? schedule+0x32/0x80\r\nJul  8 06:05:43 system kernel: [ 2538.501599]  [<ffffffffc078d63b>] ? cv_wait_common+0x10b/0x120 [spl]\r\nJul  8 06:05:43 system kernel: [ 2538.501601]  [<ffffffff8fab8cc0>] ? prepare_to_wait_event+0xf0/0xf0\r\nJul  8 06:05:43 system kernel: [ 2538.501636]  [<ffffffffc08b5936>] ? dmu_tx_wait+0x96/0x340 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.501637]  [<ffffffff8fff921e>] ? mutex_lock+0xe/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.501670]  [<ffffffffc08b5c67>] ? dmu_tx_assign+0x87/0x470 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.501675]  [<ffffffffc0786aea>] ? spl_kmem_zalloc+0x8a/0x160 [spl]\r\nJul  8 06:05:43 system kernel: [ 2538.501710]  [<ffffffffc08bd80e>] ? dsl_dataset_prev_snap_txg+0x2e/0x60 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.501743]  [<ffffffffc08be557>] ? dsl_dataset_block_freeable+0x17/0x60 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.501783]  [<ffffffffc092e0ea>] ? zfs_write+0x3ba/0xb40 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.501824]  [<ffffffffc0942ad1>] ? zpl_write_common_iovec+0x91/0xf0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.501863]  [<ffffffffc09433e0>] ? zpl_iter_write+0xb0/0xe0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.501865]  [<ffffffff8fc0205a>] ? new_sync_write+0xda/0x130\r\nJul  8 06:05:43 system kernel: [ 2538.501867]  [<ffffffff8fc027c0>] ? vfs_write+0xb0/0x190\r\nJul  8 06:05:43 system kernel: [ 2538.501870]  [<ffffffff8fc03ba2>] ? SyS_write+0x52/0xc0\r\nJul  8 06:05:43 system kernel: [ 2538.501872]  [<ffffffff8fffbd7b>] ? system_call_fast_compare_end+0xc/0x9b\r\nJul  8 06:05:43 system kernel: [ 2538.501877] INFO: task youtube-dl:17543 blocked for more than 120 seconds.\r\nJul  8 06:05:43 system kernel: [ 2538.501910]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 06:05:43 system kernel: [ 2538.501936] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 06:05:43 system kernel: [ 2538.501972] youtube-dl      D    0 17543  16556 0x00000000\r\nJul  8 06:05:43 system kernel: [ 2538.501974]  ffff8ef1f3525c00 0000000000000000 ffff8ef0b7c8e100 ffff8ef22fd58300\r\nJul  8 06:05:43 system kernel: [ 2538.501977]  ffff8ef20bf57100 ffffa98d34767ad0 ffffffff8fff6fe3 00000000e10abe9f\r\nJul  8 06:05:43 system kernel: [ 2538.501980]  00ff8ef1008313e0 ffff8ef22fd58300 ffffa98d34767af0 ffff8ef0b7c8e100\r\nJul  8 06:05:43 system kernel: [ 2538.501982] Call Trace:\r\nJul  8 06:05:43 system kernel: [ 2538.501986]  [<ffffffff8fff6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 06:05:43 system kernel: [ 2538.501989]  [<ffffffff8fff74b2>] ? schedule+0x32/0x80\r\nJul  8 06:05:43 system kernel: [ 2538.501996]  [<ffffffffc078d63b>] ? cv_wait_common+0x10b/0x120 [spl]\r\nJul  8 06:05:43 system kernel: [ 2538.501998]  [<ffffffff8fab8cc0>] ? prepare_to_wait_event+0xf0/0xf0\r\nJul  8 06:05:43 system kernel: [ 2538.502033]  [<ffffffffc08b5936>] ? dmu_tx_wait+0x96/0x340 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.502034]  [<ffffffff8fff921e>] ? mutex_lock+0xe/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.502066]  [<ffffffffc08b5c67>] ? dmu_tx_assign+0x87/0x470 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.502106]  [<ffffffffc092d38a>] ? zfs_dirty_inode+0xfa/0x2f0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.502108]  [<ffffffff8fbfe8ea>] ? __check_object_size+0xfa/0x1d8\r\nJul  8 06:05:43 system kernel: [ 2538.502149]  [<ffffffffc0923456>] ? zfs_range_unlock+0x156/0x250 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.502151]  [<ffffffff8fd47899>] ? list_del+0x9/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.502191]  [<ffffffffc0923495>] ? zfs_range_unlock+0x195/0x250 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.502192]  [<ffffffff8fff921e>] ? mutex_lock+0xe/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.502232]  [<ffffffffc09448b5>] ? zpl_dirty_inode+0x25/0x40 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.502234]  [<ffffffff8fc30f55>] ? __mark_inode_dirty+0x165/0x350\r\nJul  8 06:05:43 system kernel: [ 2538.502237]  [<ffffffff8fc1e949>] ? generic_update_time+0x79/0xd0\r\nJul  8 06:05:43 system kernel: [ 2538.502239]  [<ffffffff8fc1eaa6>] ? current_time+0x36/0x70\r\nJul  8 06:05:43 system kernel: [ 2538.502242]  [<ffffffff8fc205f9>] ? touch_atime+0xa9/0xd0\r\nJul  8 06:05:43 system kernel: [ 2538.502280]  [<ffffffffc09434ce>] ? zpl_iter_read+0xbe/0xe0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.502282]  [<ffffffff8fc01f07>] ? new_sync_read+0xd7/0x120\r\nJul  8 06:05:43 system kernel: [ 2538.502285]  [<ffffffff8fc02671>] ? vfs_read+0x91/0x130\r\nJul  8 06:05:43 system kernel: [ 2538.502287]  [<ffffffff8fc03ae2>] ? SyS_read+0x52/0xc0\r\nJul  8 06:05:43 system kernel: [ 2538.502289]  [<ffffffff8fc598c1>] ? SyS_flock+0x121/0x190\r\nJul  8 06:05:43 system kernel: [ 2538.502292]  [<ffffffff8fffbd7b>] ? system_call_fast_compare_end+0xc/0x9b\r\nJul  8 06:05:43 system kernel: [ 2538.502293] INFO: task pisg:17546 blocked for more than 120 seconds.\r\nJul  8 06:05:43 system kernel: [ 2538.502324]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 06:05:43 system kernel: [ 2538.502350] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 06:05:43 system kernel: [ 2538.502386] pisg            D    0 17546  16452 0x00000000\r\nJul  8 06:05:43 system kernel: [ 2538.502388]  ffff8ef1f308e000 0000000000000000 ffff8ef0b87c9080 ffff8ef22fcd8300\r\nJul  8 06:05:43 system kernel: [ 2538.502391]  ffff8ef20bf4b080 ffffa98d3475fad0 ffffffff8fff6fe3 000000003cbc3061\r\nJul  8 06:05:43 system kernel: [ 2538.502394]  00ff8ef0f3d4f1a0 ffff8ef22fcd8300 ffffa98d3475faf0 ffff8ef0b87c9080\r\nJul  8 06:05:43 system kernel: [ 2538.502396] Call Trace:\r\nJul  8 06:05:43 system kernel: [ 2538.502400]  [<ffffffff8fff6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 06:05:43 system kernel: [ 2538.502402]  [<ffffffff8fff74b2>] ? schedule+0x32/0x80\r\nJul  8 06:05:43 system kernel: [ 2538.502409]  [<ffffffffc078d63b>] ? cv_wait_common+0x10b/0x120 [spl]\r\nJul  8 06:05:43 system kernel: [ 2538.502412]  [<ffffffff8fab8cc0>] ? prepare_to_wait_event+0xf0/0xf0\r\nJul  8 06:05:43 system kernel: [ 2538.502446]  [<ffffffffc08b5936>] ? dmu_tx_wait+0x96/0x340 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.502448]  [<ffffffff8fff921e>] ? mutex_lock+0xe/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.502480]  [<ffffffffc08b5c67>] ? dmu_tx_assign+0x87/0x470 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.502521]  [<ffffffffc092d38a>] ? zfs_dirty_inode+0xfa/0x2f0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.502522]  [<ffffffff8fbfe8ea>] ? __check_object_size+0xfa/0x1d8\r\nJul  8 06:05:43 system kernel: [ 2538.502563]  [<ffffffffc0923456>] ? zfs_range_unlock+0x156/0x250 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.502565]  [<ffffffff8fd47899>] ? list_del+0x9/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.502604]  [<ffffffffc0923495>] ? zfs_range_unlock+0x195/0x250 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.502606]  [<ffffffff8fff921e>] ? mutex_lock+0xe/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.502645]  [<ffffffffc09448b5>] ? zpl_dirty_inode+0x25/0x40 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.502647]  [<ffffffff8fc30f55>] ? __mark_inode_dirty+0x165/0x350\r\nJul  8 06:05:43 system kernel: [ 2538.502650]  [<ffffffff8fc1e949>] ? generic_update_time+0x79/0xd0\r\nJul  8 06:05:43 system kernel: [ 2538.502652]  [<ffffffff8fc1eaa6>] ? current_time+0x36/0x70\r\nJul  8 06:05:43 system kernel: [ 2538.502655]  [<ffffffff8fc205f9>] ? touch_atime+0xa9/0xd0\r\nJul  8 06:05:43 system kernel: [ 2538.502693]  [<ffffffffc09434ce>] ? zpl_iter_read+0xbe/0xe0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.502695]  [<ffffffff8fc01f07>] ? new_sync_read+0xd7/0x120\r\nJul  8 06:05:43 system kernel: [ 2538.502698]  [<ffffffff8fc02671>] ? vfs_read+0x91/0x130\r\nJul  8 06:05:43 system kernel: [ 2538.502700]  [<ffffffff8fc03ae2>] ? SyS_read+0x52/0xc0\r\nJul  8 06:05:43 system kernel: [ 2538.502702]  [<ffffffff8fffbd7b>] ? system_call_fast_compare_end+0xc/0x9b\r\nJul  8 06:05:43 system kernel: [ 2538.502705] INFO: task sh:17636 blocked for more than 120 seconds.\r\nJul  8 06:05:43 system kernel: [ 2538.502735]       Tainted: P           OE   4.9.0-1-amd64 #1\r\nJul  8 06:05:43 system kernel: [ 2538.502762] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJul  8 06:05:43 system kernel: [ 2538.502814] sh              D    0 17636  17633 0x00000000\r\nJul  8 06:05:43 system kernel: [ 2538.502825]  ffff8ef0ff5bfc00 0000000000000000 ffff8eef51968040 ffff8ef22fc18300\r\nJul  8 06:05:43 system kernel: [ 2538.502855]  ffffffff9060e500 ffffa98d346d7a28 ffffffff8fff6fe3 00000000e3fb1787\r\nJul  8 06:05:43 system kernel: [ 2538.502864]  00ff8ef1f0b0f620 ffff8ef22fc18300 ffffa98d346d7a48 ffff8eef51968040\r\nJul  8 06:05:43 system kernel: [ 2538.502873] Call Trace:\r\nJul  8 06:05:43 system kernel: [ 2538.502879]  [<ffffffff8fff6fe3>] ? __schedule+0x233/0x6d0\r\nJul  8 06:05:43 system kernel: [ 2538.502884]  [<ffffffff8fff74b2>] ? schedule+0x32/0x80\r\nJul  8 06:05:43 system kernel: [ 2538.502894]  [<ffffffffc078d63b>] ? cv_wait_common+0x10b/0x120 [spl]\r\nJul  8 06:05:43 system kernel: [ 2538.502899]  [<ffffffff8fab8cc0>] ? prepare_to_wait_event+0xf0/0xf0\r\nJul  8 06:05:43 system kernel: [ 2538.502934]  [<ffffffffc08b5936>] ? dmu_tx_wait+0x96/0x340 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.502938]  [<ffffffff8fff921e>] ? mutex_lock+0xe/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.502972]  [<ffffffffc08b5c67>] ? dmu_tx_assign+0x87/0x470 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.503013]  [<ffffffffc092d38a>] ? zfs_dirty_inode+0xfa/0x2f0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.503021]  [<ffffffff8fff921e>] ? mutex_lock+0xe/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.503054]  [<ffffffffc08a88bf>] ? dmu_buf_hold_array_by_dnode+0x2ff/0x460 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.503094]  [<ffffffffc0923456>] ? zfs_range_unlock+0x156/0x250 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.503100]  [<ffffffff8fd47899>] ? list_del+0x9/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.503142]  [<ffffffffc0923495>] ? zfs_range_unlock+0x195/0x250 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.503146]  [<ffffffff8fff921e>] ? mutex_lock+0xe/0x30\r\nJul  8 06:05:43 system kernel: [ 2538.503185]  [<ffffffffc09448b5>] ? zpl_dirty_inode+0x25/0x40 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.503192]  [<ffffffff8fc30f55>] ? __mark_inode_dirty+0x165/0x350\r\nJul  8 06:05:43 system kernel: [ 2538.503200]  [<ffffffff8fc1e949>] ? generic_update_time+0x79/0xd0\r\nJul  8 06:05:43 system kernel: [ 2538.503204]  [<ffffffff8fc1eaa6>] ? current_time+0x36/0x70\r\nJul  8 06:05:43 system kernel: [ 2538.503209]  [<ffffffff8fc205f9>] ? touch_atime+0xa9/0xd0\r\nJul  8 06:05:43 system kernel: [ 2538.503247]  [<ffffffffc09434ce>] ? zpl_iter_read+0xbe/0xe0 [zfs]\r\nJul  8 06:05:43 system kernel: [ 2538.503252]  [<ffffffff8fc01f07>] ? new_sync_read+0xd7/0x120\r\nJul  8 06:05:43 system kernel: [ 2538.503257]  [<ffffffff8fc02671>] ? vfs_read+0x91/0x130\r\nJul  8 06:05:43 system kernel: [ 2538.503263]  [<ffffffff8fc08de1>] ? prepare_binprm+0x101/0x1e0\r\nJul  8 06:05:43 system kernel: [ 2538.503271]  [<ffffffff8fc0a593>] ? do_execveat_common.isra.37+0x4c3/0x790\r\nJul  8 06:05:43 system kernel: [ 2538.503276]  [<ffffffff8fd59f00>] ? strncpy_from_user+0x30/0x160\r\nJul  8 06:05:43 system kernel: [ 2538.503279]  [<ffffffff8fc0aa85>] ? SyS_execve+0x35/0x40\r\nJul  8 06:05:43 system kernel: [ 2538.503281]  [<ffffffff8fa03b1c>] ? do_syscall_64+0x7c/0xf0\r\nJul  8 06:05:43 system kernel: [ 2538.503284]  [<ffffffff8fffbe2f>] ? entry_SYSCALL64_slow_path+0x25/0x25\r\n```\r\n\r\nThis is a production system, and since the backups aren't as essential as the other processes that depend on other filesystems on this system, and I hoped that the issue was with something in those filesystems that were send/recv'ing during the first panic, I decided to try and avoid further panics by simply unmounting the entire suspect filesystem hierarchy (/backups), and disabling the cronjobs that expected it to be mounted. The system has now been running for another two days (after a reboot after the last panic) without any further panics.\r\n\r\nI still have poolA, though attached to a different system, and all of the data that was sent to poolB is still on poolA, if that might make it possible to run tests without taking my production system down repeatedly.\r\n\r\nThis seems possibly related to #3968, #1303, and more. Any ideas?", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6332/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tuomari": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6327", "title": "General protection fault on avl_destroy_nodes", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Debian\r\nDistribution Version    | 8.7 (Jessie)\r\nLinux Kernel                 | 4.9.20\r\nArchitecture                 | amd64\r\nZFS Version                  | 7.0-RC3 84c07ad +ntrim2\r\nSPL Version                  | 7.0-RC3 bf8abea\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n```\r\nJul  7 19:58:59 helvi kernel: [2241775.274959] general protection fault: 0000 [#1] SMP\r\nJul  7 19:58:59 helvi kernel: [2241775.275031] Modules linked in: binfmt_misc iscsi_target_mod target_core_mod 8021q garp ipt_MASQUERADE nf_nat_masquerade_ipv4 xfrm_user xfrm_algo iptable_nat xt_addrtype iptable_filter ip_tables xt_conntrack x_tables br_netfilter bridge stp llc dm_thin_pool dm_persistent_data dm_bio_prison dm_bufio loop openvswitch nf_conntrack_ipv6 nf_nat_ipv6 nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_defrag_ipv6 nf_nat nf_conntrack configfs nfsd auth_rpcgss oid_registry nfs_acl nfs lockd grace sunrpc zfs(PO) zunicode(PO) zavl(PO) icp(PO) iTCO_wdt gpio_ich iTCO_vendor_support crct10dif_pclmul crc32_pclmul ghash_clmulni_intel aesni_intel aes_x86_64 ablk_helper cryptd lrw gf128mul glue_helper serio_raw pcspkr i2c_i801 i2c_smbus ftdi_sio usbserial ast snd_hda_intel bttv tveeprom ttm snd_hda_codec videobuf_dma_sg\r\nJul  7 19:58:59 helvi kernel: [2241775.275834]  drm_kms_helper syscopyarea tea575x videobuf_core rc_core sysfillrect sysimgblt v4l2_common fb_sys_fops snd_hda_core zcommon(PO) videodev lpc_ich znvpair(PO) media snd_pcm drm btrfs snd_timer agpgart mfd_core spl(O) snd soundcore zlib_deflate i7core_edac ioatdma i5500_temp dca edac_core shpchp ipmi_si ipmi_msghandler evdev acpi_cpufreq tpm_tis tpm_tis_core tpm w83795 w83627ehf hwmon_vid coretemp autofs4 usb_storage hid_generic usbhid hid crc32c_intel psmouse uhci_hcd ehci_pci ehci_hcd usbcore usb_common sg sd_mod\r\nJul  7 19:58:59 helvi kernel: [2241775.276437] CPU: 10 PID: 8422 Comm: txg_sync Tainted: P           O    4.9.20.iudex.kvm.ovs.1 #1\r\nJul  7 19:58:59 helvi kernel: [2241775.276559] Hardware name: System manufacturer System Product Name/Z8NA-D6(C), BIOS 1303    05/10/2012\r\nJul  7 19:58:59 helvi kernel: [2241775.276658] task: ffff8804105b2f00 task.stack: ffffc9002b720000\r\nJul  7 19:58:59 helvi kernel: [2241775.276730] RIP: 0010:[<ffffffffa020428a>]  [<ffffffffa020428a>] avl_destroy_nodes+0x6a/0xf0 [zavl]\r\nJul  7 19:58:59 helvi kernel: [2241775.276853] RSP: 0018:ffffc9002b723c30  EFLAGS: 00010282\r\nJul  7 19:58:59 helvi kernel: [2241775.276917] RAX: 95846296855c907f RBX: ffff88027c4a1240 RCX: ffff88005d887747\r\nJul  7 19:58:59 helvi kernel: [2241775.277013] RDX: 95846296855c907f RSI: ffffc9002b723c40 RDI: ffff88082933f000\r\nJul  7 19:58:59 helvi kernel: [2241775.277111] RBP: ffffc9002b723c30 R08: ffff88027c4a1a40 R09: 0000000000000000\r\nJul  7 19:58:59 helvi kernel: [2241775.277218] R10: ffff88042fd9b200 R11: ffffea0009f12840 R12: 0000000000000000\r\nJul  7 19:58:59 helvi kernel: [2241775.277332] R13: ffff88082933f000 R14: 0000000000000000 R15: ffff880826075000\r\nJul  7 19:58:59 helvi kernel: [2241775.277425] FS:  0000000000000000(0000) GS:ffff88042fd80000(0000) knlGS:0000000000000000\r\nJul  7 19:58:59 helvi kernel: [2241775.277539] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nJul  7 19:58:59 helvi kernel: [2241775.277602] CR2: 00007fe4af804000 CR3: 0000000001c08000 CR4: 00000000000006e0\r\nJul  7 19:58:59 helvi kernel: [2241775.277705] Stack:\r\nJul  7 19:58:59 helvi kernel: [2241775.277757]  ffffc9002b723c68 ffffffffa108bed3 ffff88027c4a1a40 ffff880826075000\r\nJul  7 19:58:59 helvi kernel: [2241775.277855]  000000000157af16 000000000157af16 0000000000000000 ffffc9002b723c80\r\nJul  7 19:58:59 helvi kernel: [2241775.277974]  ffffffffa10878c8 ffff880826075008 ffffc9002b723cd8 ffffffffa10887f2\r\nJul  7 19:58:59 helvi kernel: [2241775.278088] Call Trace:\r\nJul  7 19:58:59 helvi kernel: [2241775.278196]  [<ffffffffa108bed3>] range_tree_vacate+0x63/0x90 [zfs]\r\nJul  7 19:58:59 helvi kernel: [2241775.278301]  [<ffffffffa10878c8>] metaslab_unload+0x18/0x50 [zfs]\r\nJul  7 19:58:59 helvi kernel: [2241775.278416]  [<ffffffffa10887f2>] metaslab_sync_done+0x492/0x630 [zfs]\r\nJul  7 19:58:59 helvi kernel: [2241775.278536]  [<ffffffffa10a9cb9>] vdev_sync_done+0x39/0x70 [zfs]\r\nJul  7 19:58:59 helvi kernel: [2241775.278626]  [<ffffffffa1094f3e>] spa_sync+0x5ce/0xdc0 [zfs]\r\nJul  7 19:58:59 helvi kernel: [2241775.278690]  [<ffffffff810d114d>] ? default_wake_function+0xd/0x10\r\nJul  7 19:58:59 helvi kernel: [2241775.278797]  [<ffffffffa10a6810>] txg_sync_thread+0x2c0/0x450 [zfs]\r\nJul  7 19:58:59 helvi kernel: [2241775.278909]  [<ffffffffa10a6550>] ? txg_init+0x230/0x230 [zfs]\r\nJul  7 19:58:59 helvi kernel: [2241775.278966]  [<ffffffffa0228356>] thread_generic_wrapper+0x76/0x90 [spl]\r\nJul  7 19:58:59 helvi kernel: [2241775.279040]  [<ffffffffa02282e0>] ? __thread_exit+0x20/0x20 [spl]\r\nJul  7 19:58:59 helvi kernel: [2241775.279109]  [<ffffffff810c7069>] kthread+0xe9/0x100\r\nJul  7 19:58:59 helvi kernel: [2241775.279172]  [<ffffffff810c6f80>] ? kthread_park+0x60/0x60\r\nJul  7 19:58:59 helvi kernel: [2241775.279237]  [<ffffffff810c6f80>] ? kthread_park+0x60/0x60\r\nJul  7 19:58:59 helvi kernel: [2241775.279299]  [<ffffffff816d75d2>] ret_from_fork+0x22/0x30\r\nJul  7 19:58:59 helvi kernel: [2241775.279348] Code: 49 8b 50 08 48 85 d2 75 22 49 8b 50 10 4c 89 c0 48 83 e2 f8 48 85 d2 75 28 4c 29 c8 48 c7 06 01 00 00 00 5d c3 48 89 d1 48 89 c2 <48> 8b 02 48 85 c0 75 f2 48 8b 42 08 48 85 c0 74 58 48 85 d2 74 \r\nJul  7 19:58:59 helvi kernel: [2241775.279674] RIP  [<ffffffffa020428a>] avl_destroy_nodes+0x6a/0xf0 [zavl]\r\nJul  7 19:58:59 helvi kernel: [2241775.279729]  RSP <ffffc9002b723c30>\r\nJul  7 19:58:59 helvi kernel: [2241775.280097] ---[ end trace 1a42da5330e1784a ]---\r\nJul  7 19:58:59 helvi kernel: [2241775.280213] BUG: unable to handle kernel NULL pointer dereference at 000000000000000b\r\nJul  7 19:58:59 helvi kernel: [2241775.280436] IP: [<ffffffff810e4885>] __wake_up_common+0x25/0x80\r\nJul  7 19:58:59 helvi kernel: [2241775.280579] PGD 0 \r\nJul  7 19:58:59 helvi kernel: [2241775.280629] \r\nJul  7 19:58:59 helvi kernel: [2241775.280752] Oops: 0000 [#2] SMP\r\nJul  7 19:58:59 helvi kernel: [2241775.280839] Modules linked in: binfmt_misc iscsi_target_mod target_core_mod 8021q garp ipt_MASQUERADE nf_nat_masquerade_ipv4 xfrm_user xfrm_algo iptable_nat xt_addrtype iptable_filter ip_tables xt_conntrack x_tables br_netfilter bridge stp llc dm_thin_pool dm_persistent_data dm_bio_prison dm_bufio loop openvswitch nf_conntrack_ipv6 nf_nat_ipv6 nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_defrag_ipv6 nf_nat nf_conntrack configfs nfsd auth_rpcgss oid_registry nfs_acl nfs lockd grace sunrpc zfs(PO) zunicode(PO) zavl(PO) icp(PO) iTCO_wdt gpio_ich iTCO_vendor_support crct10dif_pclmul crc32_pclmul ghash_clmulni_intel aesni_intel aes_x86_64 ablk_helper cryptd lrw gf128mul glue_helper serio_raw pcspkr i2c_i801 i2c_smbus ftdi_sio usbserial ast snd_hda_intel bttv tveeprom ttm snd_hda_codec videobuf_dma_sg\r\nJul  7 19:58:59 helvi kernel: [2241775.284118]  drm_kms_helper syscopyarea tea575x videobuf_core rc_core sysfillrect sysimgblt v4l2_common fb_sys_fops snd_hda_core zcommon(PO) videodev lpc_ich znvpair(PO) media snd_pcm drm btrfs snd_timer agpgart mfd_core spl(O) snd soundcore zlib_deflate i7core_edac ioatdma i5500_temp dca edac_core shpchp ipmi_si ipmi_msghandler evdev acpi_cpufreq tpm_tis tpm_tis_core tpm w83795 w83627ehf hwmon_vid coretemp autofs4 usb_storage hid_generic usbhid hid crc32c_intel psmouse uhci_hcd ehci_pci ehci_hcd usbcore usb_common sg sd_mod\r\nJul  7 19:58:59 helvi kernel: [2241775.286717] CPU: 10 PID: 8422 Comm: txg_sync Tainted: P      D    O    4.9.20.iudex.kvm.ovs.1 #1\r\nJul  7 19:58:59 helvi kernel: [2241775.286871] Hardware name: System manufacturer System Product Name/Z8NA-D6(C), BIOS 1303    05/10/2012\r\nJul  7 19:58:59 helvi kernel: [2241775.287018] task: ffff8804105b2f00 task.stack: ffffc9002b720000\r\nJul  7 19:58:59 helvi kernel: [2241775.287122] RIP: 0010:[<ffffffff810e4885>]  [<ffffffff810e4885>] __wake_up_common+0x25/0x80\r\nJul  7 19:58:59 helvi kernel: [2241775.287309] RSP: 0018:ffffc9002b723e50  EFLAGS: 00010086\r\nJul  7 19:58:59 helvi kernel: [2241775.287391] RAX: 0000000000000286 RBX: ffffc9002b723f08 RCX: 0000000000000000\r\nJul  7 19:58:59 helvi kernel: [2241775.287543] RDX: 000000000000000b RSI: 0000000000000003 RDI: ffffc9002b723f08\r\nJul  7 19:58:59 helvi kernel: [2241775.287694] RBP: ffffc9002b723e88 R08: 0000000000000000 R09: 0000000000000000\r\nJul  7 19:58:59 helvi kernel: [2241775.287812] R10: 00000000000006f4 R11: ffffffff81882500 R12: ffffc9002b723f08\r\nJul  7 19:58:59 helvi kernel: [2241775.287960] R13: 0000000000000286 R14: 0000000000000000 R15: 0000000000000003\r\nJul  7 19:58:59 helvi kernel: [2241775.288106] FS:  0000000000000000(0000) GS:ffff88042fd80000(0000) knlGS:0000000000000000\r\nJul  7 19:58:59 helvi kernel: [2241775.288239] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nJul  7 19:58:59 helvi kernel: [2241775.288342] CR2: 000000000000000b CR3: 0000000001c08000 CR4: 00000000000006e0\r\nJul  7 19:58:59 helvi kernel: [2241775.288482] Stack:\r\nJul  7 19:58:59 helvi kernel: [2241775.288550]  0000000181e86858 0000000000000000 ffffc9002b723f08 ffffc9002b723f00\r\nJul  7 19:58:59 helvi kernel: [2241775.288852]  0000000000000286 0000000000000000 ffff880826075000 ffffc9002b723e98\r\nJul  7 19:58:59 helvi kernel: [2241775.289136]  ffffffff810e48ee ffffc9002b723ec0 ffffffff810e52c2 ffff8804105b3540\r\nJul  7 19:58:59 helvi kernel: [2241775.289421] Call Trace:\r\nJul  7 19:58:59 helvi kernel: [2241775.289505]  [<ffffffff810e48ee>] __wake_up_locked+0xe/0x10\r\nJul  7 19:58:59 helvi kernel: [2241775.289613]  [<ffffffff810e52c2>] complete+0x32/0x50\r\nJul  7 19:58:59 helvi kernel: [2241775.289695]  [<ffffffff810a75f0>] mm_release+0xb0/0x130\r\nJul  7 19:58:59 helvi kernel: [2241775.289796]  [<ffffffff810ad4d2>] do_exit+0x132/0xb10\r\nJul  7 19:58:59 helvi kernel: [2241775.289897]  [<ffffffff816d9297>] rewind_stack_do_exit+0x17/0x20\r\nJul  7 19:58:59 helvi kernel: [2241775.290001] Code: 00 00 00 00 00 90 55 48 89 e5 41 57 41 89 f7 41 56 41 89 ce 41 55 41 54 49 89 fc 53 48 83 ec 10 89 55 cc 48 8b 57 18 4c 89 45 d0 <4c> 8b 02 48 8d 42 e8 48 39 c7 4d 8d 68 e8 75 05 eb 34 49 89 d5 \r\nJul  7 19:58:59 helvi kernel: [2241775.292808] RIP  [<ffffffff810e4885>] __wake_up_common+0x25/0x80\r\nJul  7 19:58:59 helvi kernel: [2241775.292956]  RSP <ffffc9002b723e50>\r\nJul  7 19:58:59 helvi kernel: [2241775.293048] CR2: 000000000000000b\r\nJul  7 19:58:59 helvi kernel: [2241775.293131] ---[ end trace 1a42da5330e1784b ]---\r\nJul  7 19:58:59 helvi kernel: [2241775.293221] Fixing recursive fault but reboot is needed!\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6327/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6220", "title": "Unable to handle kernel paging request on avl_walk", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Debian\r\nDistribution Version    | 8.7 (Jessie)\r\nLinux Kernel                 | 4.9.20\r\nArchitecture                 | amd64\r\nZFS Version                  | 7.0-RC3 84c07ad +ntrim2\r\nSPL Version                  | 7.0-RC3 bf8abea\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nI was hitting some strange stacktraces and zfs problems with my system and payload (see #4895, #5071, #5226, #5429 and #5910). The system was up for maximum of two days before crashing. After upgrading to these versions the system was stable for almost two months, after which I got the following stacktrace:\r\n\r\n```\r\nJun 10 16:11:07 helvi kernel: [5767162.001696] BUG: unable to handle kernel paging request at 000000009b9da09d\r\nJun 10 16:11:07 helvi kernel: [5767162.001781] IP: [<ffffffffa00e202e>] avl_walk+0x2e/0x60 [zavl]\r\nJun 10 16:11:07 helvi kernel: [5767162.001865] PGD 0 \r\nJun 10 16:11:07 helvi kernel: [5767162.001878] \r\nJun 10 16:11:07 helvi kernel: [5767162.001946] Oops: 0000 [#1] SMP\r\nJun 10 16:11:07 helvi kernel: [5767162.002013] Modules linked in: usb_storage binfmt_misc iscsi_target_mod target_core_mod 8021q garp ipt_MASQUERADE nf_nat_masquerade_ipv4 xfrm_user xfrm_algo iptable_nat xt_addrtype iptable_filter ip_tables xt_conntrack x_tables br_netfilter bridge stp llc dm_thin_pool dm_persistent_data dm_bio_prison dm_bufio loop openvswitch nf_conntrack_ipv6 nf_nat_ipv6 nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_defrag_ipv6 nf_nat nf_conntrack configfs nfsd auth_rpcgss oid_registry nfs_acl nfs lockd grace sunrpc zfs(PO) zunicode(PO) zavl(PO) icp(PO) iTCO_wdt gpio_ich iTCO_vendor_support crct10dif_pclmul crc32_pclmul ghash_clmulni_intel aesni_intel aes_x86_64 ablk_helper cryptd lrw gf128mul glue_helper serio_raw pcspkr i2c_i801 i2c_smbus ast ftdi_sio bttv usbserial ttm tveeprom videobuf_dma_sg drm_kms_helper\r\nJun 10 16:11:07 helvi kernel: [5767162.003034]  tea575x syscopyarea videobuf_core sysfillrect rc_core sysimgblt v4l2_common fb_sys_fops drm videodev snd_hda_intel agpgart media snd_hda_codec lpc_ich mfd_core snd_hda_core snd_pcm btrfs snd_timer zcommon(PO) snd soundcore znvpair(PO) i7core_edac ioatdma spl(O) dca zlib_deflate i5500_temp edac_core ipmi_si ipmi_msghandler shpchp evdev acpi_cpufreq tpm_tis tpm_tis_core tpm w83795 w83627ehf hwmon_vid coretemp autofs4 hid_generic usbhid hid crc32c_intel psmouse uhci_hcd ehci_pci ehci_hcd usbcore usb_common sg sd_mod\r\nJun 10 16:11:07 helvi kernel: [5767162.003769] CPU: 2 PID: 1446 Comm: z_wr_iss Tainted: P           O    4.9.20.iudex.kvm.ovs.1 #1\r\nJun 10 16:11:07 helvi kernel: [5767162.003888] Hardware name: System manufacturer System Product Name/Z8NA-D6(C), BIOS 1303    05/10/2012\r\nJun 10 16:11:07 helvi kernel: [5767162.004011] task: ffff880421450000 task.stack: ffffc90005fa0000\r\nJun 10 16:11:07 helvi kernel: [5767162.004078] RIP: 0010:[<ffffffffa00e202e>]  [<ffffffffa00e202e>] avl_walk+0x2e/0x60 [zavl]\r\nJun 10 16:11:07 helvi kernel: [5767162.004175] RSP: 0018:ffffc90005fa3b38  EFLAGS: 00010202\r\nJun 10 16:11:07 helvi kernel: [5767162.004246] RAX: 000000009b9da09d RBX: ffff88002a903700 RCX: 0000000000000000\r\nJun 10 16:11:07 helvi kernel: [5767162.004350] RDX: 000000009b9da09d RSI: ffff88002a903700 RDI: ffff88002a903700\r\nJun 10 16:11:07 helvi kernel: [5767162.004458] RBP: ffffc90005fa3b38 R08: 0000000000000000 R09: 0000000000000000\r\nJun 10 16:11:07 helvi kernel: [5767162.004543] R10: 0000000000000000 R11: 0000000000000001 R12: 0000000000028000\r\nJun 10 16:11:07 helvi kernel: [5767162.004665] R13: ffff88082858a800 R14: 0000064ccbbc6000 R15: ffff88082ba44b00\r\nJun 10 16:11:07 helvi kernel: [5767162.007623] FS:  0000000000000000(0000) GS:ffff88042fc80000(0000) knlGS:0000000000000000\r\nJun 10 16:11:07 helvi kernel: [5767162.007725] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nJun 10 16:11:07 helvi kernel: [5767162.007790] CR2: 000000009b9da09d CR3: 0000000001c08000 CR4: 00000000000006e0\r\nJun 10 16:11:07 helvi kernel: [5767162.007896] Stack:\r\nJun 10 16:11:07 helvi kernel: [5767162.007940]  ffffc90005fa3bc0 ffffffffa10d5653 ffff88082ba44800 ffffc90005fa3b70\r\nJun 10 16:11:07 helvi kernel: [5767162.008051]  0000000000000246 ffffc90005fa3ba0 ffffffff816d47e9 ffffffff811cd06c\r\nJun 10 16:11:07 helvi kernel: [5767162.008175]  0000000000000246 ffff88042f803040 0000064cab64e000 0000064cab676000\r\nJun 10 16:11:07 helvi kernel: [5767162.008305] Call Trace:\r\nJun 10 16:11:07 helvi kernel: [5767162.008432]  [<ffffffffa10d5653>] metaslab_block_picker.constprop.19+0xa3/0x100 [zfs]\r\nJun 10 16:11:07 helvi kernel: [5767162.008560]  [<ffffffff816d47e9>] ? __mutex_unlock_slowpath+0xa9/0x140\r\nJun 10 16:11:07 helvi kernel: [5767162.008632]  [<ffffffff811cd06c>] ? __kmalloc_node+0x20c/0x2c0\r\nJun 10 16:11:07 helvi kernel: [5767162.008734]  [<ffffffffa10d61f0>] metaslab_df_alloc+0x90/0xd0 [zfs]\r\nJun 10 16:11:07 helvi kernel: [5767162.008821]  [<ffffffffa10d8529>] metaslab_alloc+0x649/0xe60 [zfs]\r\nJun 10 16:11:07 helvi kernel: [5767162.008928]  [<ffffffffa1143f6c>] zio_dva_allocate+0xac/0x560 [zfs]\r\nJun 10 16:11:07 helvi kernel: [5767162.008997]  [<ffffffff816d7129>] ? _raw_spin_unlock+0x9/0x10\r\nJun 10 16:11:07 helvi kernel: [5767162.009070]  [<ffffffffa01bcc20>] ? tsd_hash_search.isra.0+0x70/0x90 [spl]\r\nJun 10 16:11:07 helvi kernel: [5767162.009143]  [<ffffffffa01bccb9>] ? tsd_get_by_thread+0x29/0x40 [spl]\r\nJun 10 16:11:07 helvi kernel: [5767162.009197]  [<ffffffffa01b7633>] ? taskq_member+0x13/0x20 [spl]\r\nJun 10 16:11:07 helvi kernel: [5767162.009307]  [<ffffffffa1140507>] zio_execute+0x97/0x100 [zfs]\r\nJun 10 16:11:07 helvi kernel: [5767162.009372]  [<ffffffffa01b84df>] taskq_thread+0x25f/0x490 [spl]\r\nJun 10 16:11:07 helvi kernel: [5767162.009435]  [<ffffffff810d1140>] ? wake_up_q+0x70/0x70\r\nJun 10 16:11:07 helvi kernel: [5767162.009502]  [<ffffffffa01b8280>] ? taskq_cancel_id+0x110/0x110 [spl]\r\nJun 10 16:11:07 helvi kernel: [5767162.009555]  [<ffffffff810c7069>] kthread+0xe9/0x100\r\nJun 10 16:11:07 helvi kernel: [5767162.009609]  [<ffffffff810c6f80>] ? kthread_park+0x60/0x60\r\nJun 10 16:11:07 helvi kernel: [5767162.009705]  [<ffffffff816d75d2>] ret_from_fork+0x22/0x30\r\nJun 10 16:11:07 helvi kernel: [5767162.009776] Code: 47 10 b9 01 00 00 00 29 d1 48 89 e5 4c 01 c6 48 85 f6 74 40 48 63 d2 48 89 f7 48 8b 04 d6 48 85 c0 74 1a 48 63 c9 eb 03 48 89 d0 <48> 8b 14 c8 48 85 d2 75 f4 4c 29 c0 5d c3 39 ca 74 f7 48 8b 47 \r\nJun 10 16:11:07 helvi kernel: [5767162.010097] RIP  [<ffffffffa00e202e>] avl_walk+0x2e/0x60 [zavl]\r\nJun 10 16:11:07 helvi kernel: [5767162.010165]  RSP <ffffc90005fa3b38>\r\nJun 10 16:11:07 helvi kernel: [5767162.010221] CR2: 000000009b9da09d\r\nJun 10 16:11:07 helvi kernel: [5767162.010587] ---[ end trace df26484b3ebdf0b3 ]---\r\nJun 10 16:11:07 helvi kernel: [5767162.010700] BUG: unable to handle kernel NULL pointer dereference at 0000000000000009\r\nJun 10 16:11:07 helvi kernel: [5767162.010920] IP: [<ffffffff810e4885>] __wake_up_common+0x25/0x80\r\nJun 10 16:11:07 helvi kernel: [5767162.011063] PGD 0 \r\nJun 10 16:11:07 helvi kernel: [5767162.011105] \r\nJun 10 16:11:07 helvi kernel: [5767162.011221] Oops: 0000 [#2] SMP\r\nJun 10 16:11:07 helvi kernel: [5767162.011307] Modules linked in: usb_storage binfmt_misc iscsi_target_mod target_core_mod 8021q garp ipt_MASQUERADE nf_nat_masquerade_ipv4 xfrm_user xfrm_algo iptable_nat xt_addrtype iptable_filter ip_tables xt_conntrack x_tables br_netfilter bridge stp llc dm_thin_pool dm_persistent_data dm_bio_prison dm_bufio loop openvswitch nf_conntrack_ipv6 nf_nat_ipv6 nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_defrag_ipv6 nf_nat nf_conntrack configfs nfsd auth_rpcgss oid_registry nfs_acl nfs lockd grace sunrpc zfs(PO) zunicode(PO) zavl(PO) icp(PO) iTCO_wdt gpio_ich iTCO_vendor_support crct10dif_pclmul crc32_pclmul ghash_clmulni_intel aesni_intel aes_x86_64 ablk_helper cryptd lrw gf128mul glue_helper serio_raw pcspkr i2c_i801 i2c_smbus ast ftdi_sio bttv usbserial ttm tveeprom videobuf_dma_sg drm_kms_helper\r\nJun 10 16:11:07 helvi kernel: [5767162.014671]  tea575x syscopyarea videobuf_core sysfillrect rc_core sysimgblt v4l2_common fb_sys_fops drm videodev snd_hda_intel agpgart media snd_hda_codec lpc_ich mfd_core snd_hda_core snd_pcm btrfs snd_timer zcommon(PO) snd soundcore znvpair(PO) i7core_edac ioatdma spl(O) dca zlib_deflate i5500_temp edac_core ipmi_si ipmi_msghandler shpchp evdev acpi_cpufreq tpm_tis tpm_tis_core tpm w83795 w83627ehf hwmon_vid coretemp autofs4 hid_generic usbhid hid crc32c_intel psmouse uhci_hcd ehci_pci ehci_hcd usbcore usb_common sg sd_mod\r\nJun 10 16:11:07 helvi kernel: [5767162.017216] CPU: 2 PID: 1446 Comm: z_wr_iss Tainted: P      D    O    4.9.20.iudex.kvm.ovs.1 #1\r\nJun 10 16:11:07 helvi kernel: [5767162.017369] Hardware name: System manufacturer System Product Name/Z8NA-D6(C), BIOS 1303    05/10/2012\r\nJun 10 16:11:07 helvi kernel: [5767162.017522] task: ffff880421450000 task.stack: ffffc90005fa0000\r\nJun 10 16:11:07 helvi kernel: [5767162.017617] RIP: 0010:[<ffffffff810e4885>]  [<ffffffff810e4885>] __wake_up_common+0x25/0x80\r\nJun 10 16:11:07 helvi kernel: [5767162.017800] RSP: 0018:ffffc90005fa3e50  EFLAGS: 00010086\r\nJun 10 16:11:07 helvi kernel: [5767162.017894] RAX: 0000000000000286 RBX: ffffc90005fa3f08 RCX: 0000000000000000\r\nJun 10 16:11:07 helvi kernel: [5767162.018030] RDX: 0000000000000009 RSI: 0000000000000003 RDI: ffffc90005fa3f08\r\nJun 10 16:11:07 helvi kernel: [5767162.018174] RBP: ffffc90005fa3e88 R08: 0000000000000000 R09: 0000000000000000\r\nJun 10 16:11:07 helvi kernel: [5767162.018321] R10: 0000000000000740 R11: ffffffff81882500 R12: ffffc90005fa3f08\r\nJun 10 16:11:07 helvi kernel: [5767162.018440] R13: 0000000000000286 R14: 0000000000000000 R15: 0000000000000003\r\nJun 10 16:11:07 helvi kernel: [5767162.018575] FS:  0000000000000000(0000) GS:ffff88042fc80000(0000) knlGS:0000000000000000\r\nJun 10 16:11:07 helvi kernel: [5767162.018720] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nJun 10 16:11:07 helvi kernel: [5767162.018805] CR2: 0000000000000009 CR3: 0000000001c08000 CR4: 00000000000006e0\r\nJun 10 16:11:07 helvi kernel: [5767162.018951] Stack:\r\nJun 10 16:11:07 helvi kernel: [5767162.019030]  0000000100000740 0000000000000000 ffffc90005fa3f08 ffffc90005fa3f00\r\nJun 10 16:11:07 helvi kernel: [5767162.019331]  0000000000000286 0000000000000000 000000000000000b ffffc90005fa3e98\r\nJun 10 16:11:07 helvi kernel: [5767162.019617]  ffffffff810e48ee ffffc90005fa3ec0 ffffffff810e52c2 ffff880421450640\r\nJun 10 16:11:07 helvi kernel: [5767162.019911] Call Trace:\r\nJun 10 16:11:07 helvi kernel: [5767162.019991]  [<ffffffff810e48ee>] __wake_up_locked+0xe/0x10\r\nJun 10 16:11:07 helvi kernel: [5767162.020094]  [<ffffffff810e52c2>] complete+0x32/0x50\r\nJun 10 16:11:07 helvi kernel: [5767162.020199]  [<ffffffff810a75f0>] mm_release+0xb0/0x130\r\nJun 10 16:11:07 helvi kernel: [5767162.020295]  [<ffffffff810ad4d2>] do_exit+0x132/0xb10\r\nJun 10 16:11:07 helvi kernel: [5767162.020386]  [<ffffffff816d9297>] rewind_stack_do_exit+0x17/0x20\r\nJun 10 16:11:07 helvi kernel: [5767162.020502] Code: 00 00 00 00 00 90 55 48 89 e5 41 57 41 89 f7 41 56 41 89 ce 41 55 41 54 49 89 fc 53 48 83 ec 10 89 55 cc 48 8b 57 18 4c 89 45 d0 <4c> 8b 02 48 8d 42 e8 48 39 c7 4d 8d 68 e8 75 05 eb 34 49 89 d5 \r\nJun 10 16:11:07 helvi kernel: [5767162.023297] RIP  [<ffffffff810e4885>] __wake_up_common+0x25/0x80\r\nJun 10 16:11:07 helvi kernel: [5767162.023436]  RSP <ffffc90005fa3e50>\r\nJun 10 16:11:07 helvi kernel: [5767162.023523] CR2: 0000000000000009\r\nJun 10 16:11:07 helvi kernel: [5767162.023617] ---[ end trace df26484b3ebdf0b4 ]---\r\nJun 10 16:11:07 helvi kernel: [5767162.023722] Fixing recursive fault but reboot is needed!\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6220/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jazzl0ver": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6322", "title": "Feature request: permit nfs-sharing folders or choosing i/o scheduler", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  CentOS\r\nDistribution Version    | 7.2\r\nLinux Kernel                 | 3.10.0-514.26.2\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.10-1\r\nSPL Version                  | 0.6.5.10-1\r\n\r\n### Describe the problem you're observing\r\nThere's ZFS pool of mirrored drives:\r\n```\r\n# zpool list -v\r\nNAME   SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT\r\nzmir  1.81T   966G   890G      372M    42%    52%  1.05x  ONLINE  -\r\n  mirror  1.81T   966G   890G      372M    42%    52%\r\n    sdb      -      -      -         -      -      -\r\n    sda      -      -      -         -      -      -\r\n```\r\nZFS volume (same-titled) is created on top of it:\r\n```\r\n# zfs list\r\nNAME           USED  AVAIL  REFER  MOUNTPOINT\r\nzmir          1021G   822G   915G  /zmir\r\n```\r\nI need to share some folders under /zmir over NFS, so I created appropriate entries in /etc/exports (like /zmir/folder1 ...). The issue comes  up when there's an intensive i/o load: the latency of NFS server's replies becomes high (minutes!). It appears the problem lays in the scheduler that ZFS overrides to its favorite (```noop```) for the specific drives - it just eats all i/o without sharing with the NFS daemon.\r\n\r\n### Describe how to reproduce the problem\r\n1. Create a ZFS volume\r\n2. Share it with system NFS daemon\r\n3. Create high i/o from another server (NFS client)\r\n4. Try to run, for example, ```df -h``` serveral times on the NFS client\r\n5. You gonna see significant delays\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nSometimes the following is showed it the log on the server:\r\n```\r\nJun 28 22:36:19 filer-1 kernel: INFO: task nfsd:2128 blocked for more than 120 seconds.\r\nJun 28 22:36:19 filer-1 kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\nJun 28 22:36:19 filer-1 kernel: nfsd            D 0000000000000000     0  2128      2 0x00000000\r\nJun 28 22:36:19 filer-1 kernel: ffff8803e4e676a0 0000000000000046 ffff8803fa056dd0 ffff8803e4e67fd8\r\nJun 28 22:36:19 filer-1 kernel: ffff8803e4e67fd8 ffff8803e4e67fd8 ffff8803fa056dd0 ffff88040673d178\r\nJun 28 22:36:19 filer-1 kernel: ffff88040673d148 ffff88040673d180 ffff88040673d170 0000000000000000\r\nJun 28 22:36:19 filer-1 kernel: Call Trace:\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffff8168c739>] schedule+0x29/0x70\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffffa04ad6ad>] cv_wait_common+0x10d/0x130 [spl]\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffff810b1b20>] ? wake_up_atomic_t+0x30/0x30\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffffa04ad6e5>] __cv_wait+0x15/0x20 [spl]\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffffa069eb93>] dmu_tx_wait+0xa3/0x360 [zfs]\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffffa069eee1>] dmu_tx_assign+0x91/0x4b0 [zfs]\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffffa069e45b>] ? dmu_tx_count_dnode+0x5b/0xa0 [zfs]\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffffa071d438>] zfs_write+0x3f8/0xc20 [zfs]\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffffa0686c40>] ? dbuf_rele_and_unlock+0x2f0/0x3e0 [zfs]\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffffa0733c85>] zpl_write_common_iovec.constprop.8+0x95/0x100 [zfs]\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffffa0733dee>] zpl_aio_write+0xfe/0x120 [zfs]\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffff811fe1c9>] do_sync_readv_writev+0x79/0xd0\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffff811ff86e>] do_readv_writev+0xce/0x260\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffffa0733cf0>] ? zpl_write_common_iovec.constprop.8+0x100/0x100 [zfs]\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffff811fe080>] ? do_sync_read+0xd0/0xd0\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffff812caa03>] ? ima_get_action+0x23/0x30\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffff812ca03e>] ? process_measurement+0x8e/0x250\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffff811fc644>] ? do_dentry_open+0x1e4/0x2e0\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffff811ffa95>] vfs_writev+0x35/0x60\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffffa0367cd0>] nfsd_vfs_write+0xc0/0x3a0 [nfsd]\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffffa036a5ab>] nfsd_write+0x10b/0x290 [nfsd]\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffffa0370ca0>] nfsd3_proc_write+0xc0/0x160 [nfsd]\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffffa03626fb>] nfsd_dispatch+0xbb/0x200 [nfsd]\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffffa0309ea3>] svc_process_common+0x453/0x6f0 [sunrpc]\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffffa030a24b>] svc_process+0x10b/0x1a0 [sunrpc]\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffffa036207f>] nfsd+0xdf/0x150 [nfsd]\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffffa0361fa0>] ? nfsd_destroy+0x80/0x80 [nfsd]\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffff810b0a4f>] kthread+0xcf/0xe0\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffff810b0980>] ? kthread_create_on_node+0x140/0x140\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffff81697698>] ret_from_fork+0x58/0x90\r\nJun 28 22:36:19 filer-1 kernel: [<ffffffff810b0980>] ? kthread_create_on_node+0x140/0x140\r\n```\r\n\r\nI'm currently testing these cronjobs as a workaround:\r\n```\r\n* * * * * echo deadline> /sys/block/sda/queue/scheduler\r\n* * * * * echo deadline> /sys/block/sdb/queue/scheduler\r\n```\r\n\r\nOne of the possible solutions is to add ability to specify certain folders for NFS sharing (currently I'm able to share the whole volume only) by means of ZFS. This actually seems to be possible in Oracle ZFS implementation (https://docs.oracle.com/cd/E23824_01/html/821-1448/gayne.html, see ```New ZFS Sharing Syntax```).\r\n\r\nAnother solution is to add a property to choose the i/o scheduler for a ZFS pool.\r\n\r\nThank you in advance!", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6322/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "seanjnkns": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6321", "title": "general protection fault in __spl_cache_flush+0xa2/0x140 [spl]", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS\r\nDistribution Version    | 7.3.1611\r\nLinux Kernel                 | 4.9.32\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.10\r\nSPL Version                  | 0.6.5.10\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nGPF that in turn causes our SAN to crash requiring a hard reboot\r\n\r\n### Describe how to reproduce the problem\r\nFirst we've seen it, but it was while our 2 SAN setup that establishes a MD for servers hosted over iscsi was syncing data.  So, higher IO than normal, but still minimal compared to its hardware profile\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n[  866.483956] general protection fault: 0000 [#1] SMP\r\n[  866.484727] Modules linked in: ib_isert target_core_user uio target_core_pscsi target_core_file target_core_iblock ip_vs nf_conntrack macvlan bonding iptable_filter ib_iser rdma_ucm ib_ucm ib_uverbs ib_umad ipmi_devintf sb_edac edac_core zfs(PO) x86_pkg_temp_thermal intel_powerclamp raid10 coretemp zunicode(PO) kvm_intel zavl(PO) zcommon(PO) znvpair(PO) kvm iTCO_wdt iTCO_vendor_support spl(O) irqbypass pcspkr joydev i2c_i801 i2c_smbus sg mei_me lpc_ich mei ioatdma shpchp mfd_core ipmi_si ipmi_msghandler acpi_power_meter acpi_pad ip_tables xfs libcrc32c mlx4_en mlx4_ib raid1 rdma_cm iw_cm ib_cm mlx5_ib ib_core sd_mod 8021q garp mrp crct10dif_pclmul crc32_pclmul ghash_clmulni_intel aesni_intel lrw gf128mul glue_helper ablk_helper cryptd ast drm_kms_helper syscopyarea sysfillrect sysimgblt fb_sys_fops\r\n[  866.488479]  mlx5_core ahci ttm igb libahci mlx4_core drm dca ptp libata pps_core i2c_algo_bit wmi sunrpc dm_mirror dm_region_hash dm_log dm_mod\r\n[  866.489755] CPU: 8 PID: 1551 Comm: arc_reclaim Tainted: P           O    4.9.32-4.el7.centos.x86_64 #1\r\n[  866.490414] Hardware name: Supermicro SYS-6028TP-HTFR/X10DRT-PIBF, BIOS 1.1 08/03/2015\r\n[  866.491090] task: ffff887f4c614b80 task.stack: ffffc900338b8000\r\n[  866.491812] RIP: 0010:[<ffffffffa07c2ef2>]  [<ffffffffa07c2ef2>] __spl_cache_flush+0xa2/0x140 [spl]\r\n[  866.492511] RSP: 0018:ffffc900338bbc38  EFLAGS: 00010086\r\n[  866.493184] RAX: 0000000000020000 RBX: 202e746e656d6f6d RCX: 0000000000000010\r\n[  866.493873] RDX: 000000000001ffff RSI: 000000000001ffff RDI: ffffc9038da44018\r\n[  866.494573] RBP: ffffc900338bbc78 R08: ffff887cb989b578 R09: 00000001802e002a\r\n[  866.495281] R10: 0000000090ebf901 R11: ffff887c90ebf478 R12: ffff887f63dd2600\r\n[  866.496000] R13: 0000000000000040 R14: 0000000000000000 R15: ffff883f5d2c5800\r\n[  866.496724] FS:  0000000000000000(0000) GS:ffff887f7f200000(0000) knlGS:0000000000000000\r\n[  866.497463] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[  866.498203] CR2: 00007fc5f41dd000 CR3: 0000000001c07000 CR4: 00000000001406e0\r\n[  866.498963] Stack:\r\n[  866.499720]  0000000800000001 ffffffffa07c3a43 ffff883f5d2c58a0 ffff883f5d2c58b8\r\n[  866.500486]  ffff883f5d2c5800 ffff887f63dd2600 0000000000000008 0000000000000000\r\n[  866.501289]  ffffc900338bbca8 ffffffffa07c3216 ffff887f63dd2600 ffff883f5d2c5800\r\n[  866.502215] Call Trace:\r\n[  866.503037]  [<ffffffffa07c3a43>] ? spl_slab_reclaim+0x53/0x210 [spl]\r\n[  866.503848]  [<ffffffffa07c3216>] spl_cache_flush+0x36/0x50 [spl]\r\n[  866.504635]  [<ffffffffa07c408e>] spl_kmem_cache_free+0x1ae/0x1c0 [spl]\r\n[  866.505457]  [<ffffffffa09e016a>] zio_data_buf_free+0x5a/0x60 [zfs]\r\n[  866.506243]  [<ffffffffa093713b>] arc_buf_data_free.isra.16+0x1b/0x40 [zfs]\r\n[  866.507038]  [<ffffffffa09389fa>] arc_buf_destroy+0x5a/0x170 [zfs]\r\n[  866.507839]  [<ffffffffa093aa2f>] arc_evict_state+0x41f/0x6f0 [zfs]\r\n[  866.508629]  [<ffffffffa093ada0>] arc_adjust_impl.constprop.25+0x30/0x40 [zfs]\r\n[  866.509413]  [<ffffffffa093b124>] arc_adjust+0x374/0x450 [zfs]\r\n[  866.510181]  [<ffffffffa093c680>] arc_reclaim_thread+0xb0/0x210 [zfs]\r\n[  866.510938]  [<ffffffffa093c5d0>] ? arc_shrink+0xc0/0xc0 [zfs]\r\n[  866.511745]  [<ffffffffa07c4790>] ? __thread_exit+0x20/0x20 [spl]\r\n[  866.511747]  [<ffffffffa07c4802>] thread_generic_wrapper+0x72/0x80 [spl]\r\n[  866.511752]  [<ffffffff810a36b6>] kthread+0xe6/0x100", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6321/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "waltersteinlein": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6320", "title": "zpool iostat still shows implausible numbers for cache drives", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  CentOS\r\nDistribution Version    |  7.3.1611\r\nLinux Kernel                 |  3.10.0\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.0-rc4\r\nSPL Version                  | 0.7.0-rc4\r\n\r\n\r\nThe issue described in https://github.com/zfsonlinux/zfs/issues/5583 seems still present in 0.7.0-rc4. <code>zpool iostat <pool> -v</code> still shows 16EiB when a cache drive is full:\r\n\r\n```\r\ncache                         -      -      -      -      -      -\r\n  wwn-0x55cd2e404c2814d7   187G  16,0E      0      8  19,3K   434K\r\n```\r\nThe reported available space seems to be correct as long as the cache drive is not filled up.\r\n<code>zpool list -v</code> also report an <code>ALLOC</code> value that's bigger than the size of the drive (which seems odd):\r\n\r\n```\r\ncache      -      -      -         -      -      -\r\n  wwn-0x55cd2e404c2814d7   186G   187G  16,0E         -     0%   100%\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6320/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "TyberiusPrime": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6318", "title": "ZFS Diff outputs spaces in filename as \\0040", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 16.04\r\nLinux Kernel                 | 4.4.0-66-generic #87-Ubuntu SMP Fri Mar 3 15:29:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.6.5.6-0ubuntu15\r\nSPL Version                  | 0.6.5.6-0ubuntu4\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\"zfs diff\" mangles file names with spaces in them.\r\n### Describe how to reproduce the problem\r\n```sudo zfs create pool/temp\r\nsudo zfs snapshot pool/temp@1\r\nsudo touch \"/pool/temp/A with spaces\"\r\nsudo zfs snapshot pool/temp@2\r\nsudo zfs diff pool/temp@1 pool/temp/@2\r\n```\r\nOutput\r\n```+\t/pool/temp/A\\0040with\\0040space```\r\n\r\nExpected Output\r\n```+\t/pool/temp/A with space```\r\n\r\n\r\nI can't make sense of the mangling - if it's 'escaping space' it would be 32 or 0x20... Is this octal of all things? Any other characters that get escaped? \r\n\r\nThanks.\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6318/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "spacelama": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6317", "title": "zfs receive soft deadlock", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian \r\nDistribution Version    | Debian GNU/Linux 9 (stretch/stable)\r\nLinux Kernel                 | 4.9.0-3-amd64 (4.9.30-1 (2017-06-04))\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.6.5.9-5\r\nSPL Version                  | 0.6.5.9-1\r\n\r\n### Describe the problem you're observing\r\n\r\n\"zfs receive tank8TBext/backuppc@$snapname < $new\"\r\nfrom an ~90GB incremental generated on another system hangs up frequently on the first transfer of the day, straight after an import of tank8TBext, with read IOPs on $new and write IOPs on tank8TBext dropping to zero.\r\n\r\nIf I run a `zfs list`, zfs disk churns for a bit, finishes the listing, and IOPs briefly goes back into the hundreds then back to 0.  Repeat the zfs list, and disk churns a bit more, IOPs increases then back to 0.  Typically, the third zfs list then allows reading to proceed at 50-80MB/s and IOPs back into a reasonable range.  30 minutes later, I notice zfs receive has locked up again, and IOPs back to 0.  Repeat process until entire zfs receive finishes.\r\n\r\nI had the zfs receive in a loop for all the daily incrementals shipped on the disk from remote location, and all subsequent incrementals (smaller in size) read correctly in one go.  At end of process, zfs pool exported again.\r\n\r\nThis is not an isolated incident - this happened to me last week as well.  That time, same pattern - first incremental after a zfs import kept halting (no progress overnight), `zfs list` revived it, second and subsequent incrementals after zfs import read fine, even though some of those incrementals were larger than the initial failing one.\r\n\r\nIncrementals on remote system were generated with zfs=spl=0.6.5.9-1, but with a Debian 4.9.25-1~bpo8+1 (2017-05-19) kernel.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6317/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "RubenKelevra": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6307", "title": "Slow TRIM-performance on ZVOLs", "body": "Hey Guys,\r\n\r\nI know this might be not a major problem, but I just added the trim-capability to my virtual machines.\r\n\r\nI use KVM based VMs which are switched from VIRTIO to VIRTIO-SCSI. You can add manually a flag called discard=\"unmap\" which then export the SCSI-command unmap to the underlaying file-system as discard.\r\n\r\nThose VMs run on ZVOLs with a volblocksize of 8k and inside the VM a ext4 with some optimizations (usually used for RAIDs) to create writes in larger sizes to enhance the performance on those 8K blocks (different story!).\r\n\r\nI started a live-cd and mounted this EXT4 inside it. I did a full trim, while the filesystem has never been trimmed before and is 314 G in size but use only 9 G.\r\n\r\nI learned that fstrim is working 10 times faster if sync is disabled (which should be pretty safe when \"writing\" empty blocks). So the whole operation only took 9 minutes when sync is disabled.\r\n\r\nI started with sync=standard, where it just made 100 MB in 4-6 seconds (reduction of usedbydataset) while with sync=disabled it managed to make 1000 MB in 4-6 seconds (reduction of usedbydataset).\r\n\r\nSo I guess there's a lot of room for improvements. I guess it's somewhat safe to do the discard-command async and just return the request as fast as possible.\r\n\r\n\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  ArchLinux\r\nLinux Kernel Hypervisor                | 4.9.34-1-lts\r\nLinux Kernel VM (LiveCD)               | 4.11.3-1-ARCH\r\nArchitecture                 |  x64\r\nZFS Version                  | 0.6.5.10\r\nSPL Version                  | 0.6.5.10\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6307/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mailinglists35": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6304", "title": "feature request: able to decrease the vdev capacity (shrink vdev size)", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | -\r\nDistribution Version    | -\r\nLinux Kernel                 | -\r\nArchitecture                 | -\r\nZFS Version                  | -\r\nSPL Version                  | -\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nA vdev is configured to use all block device capacity. I need to reclaim some of the block device space to use with something else than zfs.\r\nI don't want to destroy and recreate the pool and restore from backup.\r\nEven if I don't have any high hopes, it would be cute if I could issue a `zpool` command to shrink the vdev by cutting space from either beginning or end. I'm pretty sure many other people wish for this, so I'm filing this feature request issue which should belong to openzfs upstream repo.\r\nWondering if work on https://github.com/openzfs/openzfs/pull/251 could be used to build this feature.\r\n\r\n### Describe how to reproduce the problem\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6304/reactions", "total_count": 4, "+1": 4, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6266", "title": "WARNING: kernel stack frame pointer at [...] in [some zfs kernel thread name] has bad value           (null)", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 16.04\r\nLinux Kernel                 | 4.10.0-24-generic\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.7.0-rc4_65_gd9ad3fe\r\nSPL Version                  | 0.7.0-rc4_4_gac48361\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n$title message printed by kernel \r\n### Describe how to reproduce the problem\r\nimport a pool, generate some io (find, rsync, ncdu, etc)\r\nrun `perf top -ag`\r\nafter few seconds kernel wil print some messages (only once per lifetime of kernel. if I open perf top -ag a second time, it no longer prints)\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n```\r\n[81275.892383] WARNING: kernel stack frame pointer at ffffbc9d4332bf50 in z_rd_int_2:1924 has bad value           (null)\r\n[81275.892385] unwind stack type:0 next_sp:          (null) mask:2 graph_idx:0\r\n[81275.892387] ffffbc9d4332bbf8: 0000000000000282 (0x282)\r\n[81275.892388] ffffbc9d4332bc00: 0000000000000000 ...\r\n[81275.892457] ffffbc9d4332bc08: ffffffffc06f5970 (zio_taskq_member.isra.9.constprop.13+0x70/0x70 [zfs])\r\n[81275.892458] ffffbc9d4332bc10: ffff96bb8ed2d868 (0xffff96bb8ed2d868)\r\n[81275.892459] ffffbc9d4332bc18: ffffbc9d4332bca8 (0xffffbc9d4332bca8)\r\n[81275.892460] ffffbc9d4332bc20: ffff96bbebe9fe00 (0xffff96bbebe9fe00)\r\n[81275.892460] ffffbc9d4332bc28: 0000000000000000 ...\r\n[81275.892461] ffffbc9d4332bc30: ffff96bb61b1d0f0 (0xffff96bb61b1d0f0)\r\n[81275.892461] ffffbc9d4332bc38: 0000000000000000 ...\r\n[81275.892462] ffffbc9d4332bc48: 0000000000000001 (0x1)\r\n[81275.892462] ffffbc9d4332bc50: 0000000000000000 ...\r\n[81275.892463] ffffbc9d4332bc60: 0000000000000282 (0x282)\r\n[81275.892463] ffffbc9d4332bc68: 0000000000000282 (0x282)\r\n[81275.892464] ffffbc9d4332bc70: ffffffffffffff10 (0xffffffffffffff10)\r\n[81275.892467] ffffbc9d4332bc78: ffffffffaa8d38c5 (_raw_spin_unlock_irqrestore+0x15/0x20)\r\n[81275.892468] ffffbc9d4332bc80: 0000000000000010 (0x10)\r\n[81275.892469] ffffbc9d4332bc88: 0000000000000282 (0x282)\r\n[81275.892469] ffffbc9d4332bc90: ffffbc9d4332bca8 (0xffffbc9d4332bca8)\r\n[81275.892470] ffffbc9d4332bc98: 0000000000000018 (0x18)\r\n[81275.892470] ffffbc9d4332bca0: 0000000000000282 (0x282)\r\n[81275.892471] ffffbc9d4332bca8: ffffbc9d4332bce8 (0xffffbc9d4332bce8)\r\n[81275.892479] ffffbc9d4332bcb0: ffffffffc04cc8aa (taskq_dispatch_ent+0x6a/0x1e0 [spl])\r\n[81275.892479] ffffbc9d4332bcb8: ffff96bb8ed2d460 (0xffff96bb8ed2d460)\r\n[81275.892480] ffffbc9d4332bcc0: ffff96bbdf228020 (0xffff96bbdf228020)\r\n[81275.892523] ffffbc9d4332bcc8: ffffffffc06f5970 (zio_taskq_member.isra.9.constprop.13+0x70/0x70 [zfs])\r\n[81275.892523] ffffbc9d4332bcd0: ffff96bb8ed2d460 (0xffff96bb8ed2d460)\r\n[81275.892524] ffffbc9d4332bcd8: 0000000000000000 ...\r\n[81275.892524] ffffbc9d4332bce0: ffff96bb8ed2d868 (0xffff96bb8ed2d868)\r\n[81275.892525] ffffbc9d4332bce8: ffffbc9d4332bd40 (0xffffbc9d4332bd40)\r\n[81275.892566] ffffbc9d4332bcf0: ffffffffc0684062 (spa_taskq_dispatch_ent+0xa2/0x130 [zfs])\r\n[81275.892567] ffffbc9d4332bcf8: ffff96bbea736a00 (0xffff96bbea736a00)\r\n[81275.892568] ffffbc9d4332bd00: ffff96bb8ed29770 (0xffff96bb8ed29770)\r\n[81275.892568] ffffbc9d4332bd08: ffff96bb61b1d510 (0xffff96bb61b1d510)\r\n[81275.892569] ffffbc9d4332bd10: 0000000096998cb7 (0x96998cb7)\r\n[81275.892569] ffffbc9d4332bd18: ffff96bb8ed2d460 (0xffff96bb8ed2d460)\r\n[81275.892570] ffffbc9d4332bd20: 0000000000000000 ...\r\n[81275.892570] ffffbc9d4332bd28: 0000000000000002 (0x2)\r\n[81275.892571] ffffbc9d4332bd30: ffff96bb8ed2d868 (0xffff96bb8ed2d868)\r\n[81275.892571] ffffbc9d4332bd38: 0000000000000000 ...\r\n[81275.892572] ffffbc9d4332bd40: ffffbc9d4332bd88 (0xffffbc9d4332bd88)\r\n[81275.892614] ffffbc9d4332bd48: ffffffffc06f3a85 (zio_taskq_dispatch+0xa5/0x130 [zfs])\r\n[81275.892615] ffffbc9d4332bd50: ffff96bb8ed2d868 (0xffff96bb8ed2d868)\r\n[81275.892615] ffffbc9d4332bd58: ffff96bbdf228000 (0xffff96bbdf228000)\r\n[81275.892616] ffffbc9d4332bd60: ffff96bb8ed29770 (0xffff96bb8ed29770)\r\n[81275.892616] ffffbc9d4332bd68: 0000000000000000 ...\r\n[81275.892617] ffffbc9d4332bd70: 0000000000000002 (0x2)\r\n[81275.892617] ffffbc9d4332bd78: ffff96bb8ed2d7e0 (0xffff96bb8ed2d7e0)\r\n[81275.892618] ffffbc9d4332bd80: ffff96bb8ed2d460 (0xffff96bb8ed2d460)\r\n[81275.892618] ffffbc9d4332bd88: ffffbc9d4332be08 (0xffffbc9d4332be08)\r\n[81275.892660] ffffbc9d4332bd90: ffffffffc06ff282 (zio_done+0x942/0x1940 [zfs])\r\n[81275.892661] ffffbc9d4332bd98: ffff96bb8ed29770 (0xffff96bb8ed29770)\r\n[81275.892661] ffffbc9d4332bda0: ffff96bb8ed29b10 (0xffff96bb8ed29b10)\r\n[81275.892662] ffffbc9d4332bda8: ffff96bb8ed29af0 (0xffff96bb8ed29af0)\r\n[81275.892662] ffffbc9d4332bdb0: ffff96bb8ed2d800 (0xffff96bb8ed2d800)\r\n[81275.892663] ffffbc9d4332bdb8: ffffbc9d4332bdd0 (0xffffbc9d4332bdd0)\r\n[81275.892664] ffffbc9d4332bdc0: ffff96bbdf1c8000 (0xffff96bbdf1c8000)\r\n[81275.892664] ffffbc9d4332bdc8: ffff96bbdf1c8000 (0xffff96bbdf1c8000)\r\n[81275.892664] ffffbc9d4332bdd0: 0000000000000000 ...\r\n[81275.892665] ffffbc9d4332bdd8: 0000000096998cb7 (0x96998cb7)\r\n[81275.892666] ffffbc9d4332bde0: 0000000000800000 (0x800000)\r\n[81275.892666] ffffbc9d4332bde8: ffff96bbdf1c8000 (0xffff96bbdf1c8000)\r\n[81275.892667] ffffbc9d4332bdf0: 00000000ffffffff (0xffffffff)\r\n[81275.892668] ffffbc9d4332bdf8: 0000000000bc0002 (0xbc0002)\r\n[81275.892668] ffffbc9d4332be00: ffff96bb8ed29770 (0xffff96bb8ed29770)\r\n[81275.892669] ffffbc9d4332be08: ffffbc9d4332be50 (0xffffbc9d4332be50)\r\n[81275.892710] ffffbc9d4332be10: ffffffffc06f5a5c (zio_execute+0xec/0x2f0 [zfs])\r\n[81275.892710] ffffbc9d4332be18: ffff96bbdf1c8000 (0xffff96bbdf1c8000)\r\n[81275.892711] ffffbc9d4332be20: 000a000000000046 (0xa000000000046)\r\n[81275.892711] ffffbc9d4332be28: 0000000000000000 ...\r\n[81275.892712] ffffbc9d4332be30: ffff96bb8ed29b78 (0xffff96bb8ed29b78)\r\n[81275.892713] ffffbc9d4332be38: ffff96bbdf039490 (0xffff96bbdf039490)\r\n[81275.892713] ffffbc9d4332be40: ffff96bbdf039480 (0xffff96bbdf039480)\r\n[81275.892714] ffffbc9d4332be48: ffff96bbebe9fc00 (0xffff96bbebe9fc00)\r\n[81275.892714] ffffbc9d4332be50: ffffbc9d4332bef8 (0xffffbc9d4332bef8)\r\n[81275.892719] ffffbc9d4332be58: ffffffffc04cd2b2 (taskq_thread+0x292/0x590 [spl])\r\n[81275.892720] ffffbc9d4332be60: ffff96bbebe9fcd8 (0xffff96bbebe9fcd8)\r\n[81275.892720] ffffbc9d4332be68: ffff96bbdf1c8000 (0xffff96bbdf1c8000)\r\n[81275.892721] ffffbc9d4332be70: ffff96bbdf1c8000 (0xffff96bbdf1c8000)\r\n[81275.892722] ffffbc9d4332be78: ffff96bbebe9fc80 (0xffff96bbebe9fc80)\r\n[81275.892722] ffffbc9d4332be80: 0000000000000246 (0x246)\r\n[81275.892722] ffffbc9d4332be88: 0000000000000000 ...\r\n[81275.892723] ffffbc9d4332be98: 0000000000000001 (0x1)\r\n[81275.892724] ffffbc9d4332bea0: ffff96bbdf1c8000 (0xffff96bbdf1c8000)\r\n[81275.892726] ffffbc9d4332bea8: ffffffffaa0b4d00 (wake_up_q+0x70/0x70)\r\n[81275.892727] ffffbc9d4332beb0: dead000000000100 (0xdead000000000100)\r\n[81275.892727] ffffbc9d4332beb8: dead000000000200 (0xdead000000000200)\r\n[81275.892728] ffffbc9d4332bec0: ffffffffffffffff (0xffffffffffffffff)\r\n[81275.892728] ffffbc9d4332bec8: 0000000096998cb7 (0x96998cb7)\r\n[81275.892729] ffffbc9d4332bed0: ffff96bbdf0399c0 (0xffff96bbdf0399c0)\r\n[81275.892730] ffffbc9d4332bed8: ffff96bc012cf7c0 (0xffff96bc012cf7c0)\r\n[81275.892730] ffffbc9d4332bee0: ffffbc9d42d9fab0 (0xffffbc9d42d9fab0)\r\n[81275.892731] ffffbc9d4332bee8: ffff96bbdf039480 (0xffff96bbdf039480)\r\n[81275.892731] ffffbc9d4332bef0: ffff96bbdf1c8000 (0xffff96bbdf1c8000)\r\n[81275.892732] ffffbc9d4332bef8: ffffbc9d4332bf40 (0xffffbc9d4332bf40)\r\n[81275.892734] ffffbc9d4332bf00: ffffffffaa0a8d99 (kthread+0x109/0x140)\r\n[81275.892739] ffffbc9d4332bf08: ffffffffc04cd020 (taskq_thread_should_stop+0x90/0x90 [spl])\r\n[81275.892739] ffffbc9d4332bf10: ffff96bbdf0399f8 (0xffff96bbdf0399f8)\r\n[81275.892742] ffffbc9d4332bf18: ffffffffaa0a8c90 (kthread_create_on_node+0x60/0x60)\r\n[81275.892742] ffffbc9d4332bf20: ffff96bc012cf7c0 (0xffff96bc012cf7c0)\r\n[81275.892742] ffffbc9d4332bf28: 0000000000000000 ...\r\n[81275.892743] ffffbc9d4332bf30: 0000100200008086 (0x100200008086)\r\n[81275.892744] ffffbc9d4332bf38: 0000102700008086 (0x102700008086)\r\n[81275.892744] ffffbc9d4332bf40: ffffbc9d4332bf50 (0xffffbc9d4332bf50)\r\n[81275.892746] ffffbc9d4332bf48: ffffffffaa8d3fbc (ret_from_fork+0x2c/0x40)\r\n[81275.892747] ffffbc9d4332bf50: 0000000000000000 ...\r\n\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6266/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6242", "title": "taking a drive offline during too much data written to pool triggers full pool resilver scanning instead of just updating the offlined drive with differences", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 16.04\r\nLinux Kernel                 | 4.10.0-22-generic #24~16.04.1-Ubuntu SMP\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.9-2\r\nSPL Version                  | 0.6.5.9-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\non md if I enable the 'bitmap internal' feature I can keep a drive offline for lots of time/data then when I online it the sync is very fast, bringing the drive up to date very quickly by writing to it only the changes.\r\non zfs if I online a drive for too much data, a full resilver scan starts instead of just updating the drive with current pool data. if I offline it for a little while, the resilver happens as expected like md (drive is brought in sync very quickly)\r\nit would be useful if zfs allowed unrestricted back in time synchronization of an onlined drive instead of re-scanning the entire pool. \r\n\r\n### Describe how to reproduce the problem\r\ncreate mirror pool\r\noffline one drive\r\nwrite little data\r\nonline the drive\r\nresilver is fast, only the changes are sent to onlined drive, drive is updated very fast\r\nwrite some data to the pool \r\noffline it again\r\nwrite data (I received 100GB of datasets into a 10TB pool)\r\nonline it\r\nresilver starts scanning the entire pool.\r\nthis is not what I expected.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6242/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6041", "title": "feature request: add new option to `zfs snapshot` subcommand to skip creation of zero sized snapshots", "body": "Let's have this scenario\r\n```\r\nroot@linux:~# zfs list -r -t all test\r\nNAME           USED  AVAIL  REFER  MOUNTPOINT\r\ntest           126K  55.9M    23K  /test\r\ntest/fs1        46K  55.9M    23K  /test/fs1\r\ntest/fs1/fs2    23K  55.9M    23K  /test/fs1/fs2\r\n\r\nroot@linux:~# zfs snap -r test@snap1\r\n\r\nroot@linux:~# zfs list -r -t all test\r\nNAME                 USED  AVAIL  REFER  MOUNTPOINT\r\ntest                 126K  55.9M    23K  /test\r\ntest@snap1              0      -    23K  -\r\ntest/fs1              46K  55.9M    23K  /test/fs1\r\ntest/fs1@snap1          0      -    23K  -\r\ntest/fs1/fs2          23K  55.9M    23K  /test/fs1/fs2\r\ntest/fs1/fs2@snap1      0      -    23K  -\r\n\r\nroot@linux:~# touch /test/fs1/file1\r\n```\r\nI would like to have a new option, let's call it `-z` for this example,  that when used would produce the new situation:\r\n\r\n```\r\nroot@linux:~# zfs snap -r -z test@snap2\r\n\r\nroot@linux:~# zfs list -r -t all test\r\nNAME                 USED  AVAIL  REFER  MOUNTPOINT\r\ntest                 290K  55.7M    23K  /test\r\ntest@snap1              0      -    23K  -\r\ntest/fs1              59K  55.7M    23K  /test/fs1\r\ntest/fs1@snap1        13K      -    23K  -\r\ntest/fs1@snap2          0      -    23K  -\r\ntest/fs1/fs2          23K  55.7M    23K  /test/fs1/fs2\r\ntest/fs1/fs2@snap1      0      -    23K  -\r\n```\r\n\r\nBasically what I'd like to be able to ask 'zfs snapshot' **when `-z` parameter is present** is:\r\n\r\n\"_before you create a snapshot, regardless if is requested recursively or not, check if any previous snapshot exists and if it does then check if the newly to-be-created snapshot would reference any data that the previous snapshot doesn't ._\r\n\r\n_if it doesn't reference new data since previous snapshot, silently skip creation of the requested snapshot and report a successful operation_\"\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6041/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bunder2015": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6299", "title": "Test case: zpool_scrub_003_pos", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Gentoo\r\nDistribution Version    | n/a\r\nLinux Kernel                 | 4.9.16\r\nArchitecture                 | amd64\r\nZFS Version                  | head 0.7.0-rc4_86_gfe46eebe6\r\nSPL Version                  | head 0.7.0-rc4_5_g7a35f2b\r\n\r\n### Describe the problem you're observing\r\nTest case zpool_scrub_003_pos causes next test to fail\r\n### Describe how to reproduce the problem\r\nRun tests, 004 will fail, disable 004 and 005 will fail, disable 003 and 004+005 pass\r\n### Include any warning/errors/backtraces from the system logs\r\n003 causing 004 to fail\r\n```\r\nTest: /home/testuser/zfs/tests/zfs-tests/tests/functional/cli_root/zpool_scrub/zpool_scrub_004_pos (run as root) [00:09] [FAIL]\r\n03:37:40.74 ASSERTION: Resilver prevent scrub from starting until the resilver completes\r\n03:37:40.82 SUCCESS: zpool detach testpool loop1\r\n03:37:40.83 Added handler 3 with the following properties:\r\n03:37:40.83   pool: testpool\r\n03:37:40.83   vdev: d25058a08418a571\r\n03:37:40.83 SUCCESS: zinject -d loop0 -D10:1 testpool\r\n03:37:50.56 SUCCESS: zpool attach testpool loop0 loop1\r\n03:37:50.57 ERROR: is_pool_resilvering testpool exited 1\r\n03:37:50.57\r\n03:37:50.57 NOTE: Performing test-fail callback (/home/testuser/zfs/tests/zfs-tests/callbacks/zfs_dbgmsg.ksh)\r\n03:37:50.57 =================================================================\r\n03:37:50.57  Tailing last 200 lines of zfs_dbgmsg log\r\n03:37:50.57 =================================================================\r\n03:37:50.59 1498981040   command: zfs set mountpoint=/var/tmp/testdir testpool/testfs\r\n03:37:50.59 1498981040   os=ffff88002cb1f800 obj=72 txg=9 blocksize=4096 ibs=17 dn_slots=1\r\n03:37:50.59 1498981040   os=ffff88002cb1f800 obj=73 txg=9 blocksize=4096 ibs=17 dn_slots=1\r\n03:37:50.59 1498981040   os=ffff88002cb1f800 obj=74 txg=9 blocksize=4096 ibs=17 dn_slots=1\r\n03:37:50.59 1498981040   os=ffff88002cb1f800 obj=75 txg=9 blocksize=4096 ibs=17 dn_slots=1\r\n03:37:50.59 1498981040   ds=          (null) obj=3d num=1\r\n03:37:50.59 1498981041   waiting; tx_synced=9 waiting=8 dp=ffff8800a387c000\r\n03:37:50.59 1498981041   txg=10 quiesce_txg=10 sync_txg=10\r\n03:37:50.59 1498981041   broadcasting sync more tx_synced=9 waiting=10 dp=ffff8800a387c000\r\n03:37:50.59 1498981041   txg=10 quiesce_txg=11 sync_txg=10\r\n03:37:50.59 1498981041   quiesce done, handing off txg 10\r\n03:37:50.59 1498981041   txg=10 quiesce_txg=11 sync_txg=10\r\n03:37:50.59 1498981044   os=ffff88002cb1f800 obj=135 txg=10 blocksize=512 ibs=17 dn_slots=1\r\n03:37:50.59 1498981044   txg 10 scan setup func=1 mintxg=0 maxtxg=10\r\n03:37:50.59 1498981044   doing scan sync txg 10; ddt bm=0/0/0/0\r\n03:37:50.59 1498981044   scanned 0 ddt entries with class_max = 1; pausing=0\r\n03:37:50.59 1498981044   scanned dataset 131 (testpool/testfs) with min=1 max=10; pausing=0\r\n03:37:50.59 1498981045   pausing at bookmark 33/80/0/75\r\n03:37:50.59 1498981045   pausing at DDT bookmark 3/0/0/0\r\n03:37:50.59 1498981045   scanned dataset 51 (testpool) with min=1 max=10; pausing=1\r\n03:37:50.59 1498981045   visited 203 blocks in 1001ms\r\n03:37:50.59 1498981045   os=ffff88002cb1f800 obj=76 txg=10 blocksize=4096 ibs=17 dn_slots=1\r\n03:37:50.59 1498981045   os=ffff88002cb1f800 obj=77 txg=10 blocksize=4096 ibs=17 dn_slots=1\r\n03:37:50.59 1498981045   os=ffff88002cb1f800 obj=78 txg=10 blocksize=4096 ibs=17 dn_slots=1\r\n03:37:50.59 1498981045   os=ffff88002cb1f800 obj=79 txg=10 blocksize=4096 ibs=17 dn_slots=1\r\n03:37:50.59 1498981045   os=ffff88002cb1f800 obj=80 txg=10 blocksize=4096 ibs=17 dn_slots=1\r\n03:37:50.59 1498981045   ds=          (null) obj=3d num=1\r\n03:37:50.59 1498981046   txg=11 quiesce_txg=12 sync_txg=10\r\n03:37:50.59 1498981046   quiesce done, handing off txg 11\r\n03:37:50.59 1498981046   txg=11 quiesce_txg=12 sync_txg=10\r\n03:37:50.59 1498981046   doing scan sync txg 11; bm=51/128/0/117\r\n03:37:50.59 1498981046   resuming at 33/80/0/75\r\n03:37:50.59 1498981046   txg=12 quiesce_txg=12 sync_txg=12\r\n03:37:50.59 1498981046   broadcasting sync more tx_synced=10 waiting=12 dp=ffff8800a387c000\r\n03:37:50.59 1498981047   pausing at bookmark 33/80/0/f9\r\n03:37:50.59 1498981047   pausing at DDT bookmark 3/0/0/0\r\n03:37:50.59 1498981047   scanned dataset 51 (testpool) with min=1 max=10; pausing=1\r\n03:37:50.59 1498981047   visited 142 blocks in 1003ms\r\n03:37:50.59 1498981047   ds=          (null) obj=3d num=1\r\n03:37:50.59 1498981048   txg=12 quiesce_txg=13 sync_txg=12\r\n03:37:50.59 1498981048   quiesce done, handing off txg 12\r\n03:37:50.59 1498981048   broadcasting sync more tx_synced=11 waiting=12 dp=ffff8800a387c000\r\n03:37:50.59 1498981048   txg=12 quiesce_txg=13 sync_txg=12\r\n03:37:50.59 1498981048   command: zpool scrub testpool\r\n03:37:50.59 1498981048   txg 12 scan cancelled errors=0\r\n03:37:50.59 1498981048   spa=testpool async request task=8\r\n03:37:50.59 1498981048   os=ffff88002cb1f800 obj=136 txg=12 blocksize=4096 ibs=17 dn_slots=1\r\n03:37:50.59 1498981048   txg 12, spa testpool, DTL old object 0, new object 136\r\n03:37:50.59 1498981048   os=ffff88002cb1f800 obj=137 txg=12 blocksize=4096 ibs=17 dn_slots=1\r\n03:37:50.59 1498981048   txg 12, spa testpool, DTL old object 0, new object 137\r\n03:37:50.59 1498981048   ds=          (null) obj=89 num=1\r\n03:37:50.59 1498981048   ds=          (null) obj=3d num=1\r\n03:37:50.59 1498981048   ds=          (null) obj=88 num=1\r\n03:37:50.59 1498981048   ds=          (null) obj=87 num=1\r\n03:37:50.59 1498981048   waiting; tx_synced=12 waiting=12 dp=ffff8800a387c000\r\n03:37:50.59 1498981049   txg=13 quiesce_txg=13 sync_txg=13\r\n03:37:50.59 1498981049   broadcasting sync more tx_synced=12 waiting=13 dp=ffff8800a387c000\r\n03:37:50.59 1498981049   txg=13 quiesce_txg=14 sync_txg=13\r\n03:37:50.59 1498981049   quiesce done, handing off txg 13\r\n03:37:50.59 1498981049   txg=13 quiesce_txg=14 sync_txg=13\r\n03:37:50.59 1498981049   command: zpool scrub -s testpool\r\n03:37:50.59 1498981049   os=ffff88002cb1f800 obj=138 txg=13 blocksize=512 ibs=17 dn_slots=1\r\n03:37:50.59 1498981049   txg 13 scan setup func=1 mintxg=0 maxtxg=13\r\n03:37:50.59 1498981049   doing scan sync txg 13; ddt bm=0/0/0/0\r\n03:37:50.59 1498981049   scanned 0 ddt entries with class_max = 1; pausing=0\r\n03:37:50.59 1498981049   scanned dataset 48 (testpool/$ORIGIN@$ORIGIN) with min=0 max=13; pausing=0\r\n03:37:50.59 1498981049   scanned dataset 131 (testpool/testfs) with min=1 max=13; pausing=0\r\n03:37:50.59 1498981049   scanned dataset 45 (testpool/$ORIGIN) with min=1 max=13; pausing=0\r\n03:37:50.59 1498981050   pausing at bookmark 33/80/0/6e\r\n03:37:50.59 1498981050   pausing at DDT bookmark 3/0/0/0\r\n03:37:50.59 1498981050   scanned dataset 51 (testpool) with min=1 max=13; pausing=1\r\n03:37:50.59 1498981050   visited 201 blocks in 1081ms\r\n03:37:50.59 1498981050   ds=          (null) obj=3d num=1\r\n03:37:50.59 1498981050   txg=14 quiesce_txg=15 sync_txg=13\r\n03:37:50.59 1498981050   quiesce done, handing off txg 14\r\n03:37:50.59 1498981050   txg=14 quiesce_txg=15 sync_txg=13\r\n03:37:50.59 1498981050   command: zpool scrub testpool\r\n03:37:50.59 1498981050   doing scan sync txg 14; bm=51/128/0/110\r\n03:37:50.59 1498981050   resuming at 33/80/0/6e\r\n03:37:50.59 1498981055   pausing at bookmark 33/80/0/282\r\n03:37:50.59 1498981055   pausing at DDT bookmark 3/0/0/0\r\n03:37:50.59 1498981055   scanned dataset 51 (testpool) with min=1 max=13; pausing=1\r\n03:37:50.59 1498981055   visited 542 blocks in 5083ms\r\n03:37:50.59 1498981056   ds=          (null) obj=3d num=1\r\n03:37:50.59 1498981056   txg=15 quiesce_txg=16 sync_txg=13\r\n03:37:50.59 1498981056   quiesce done, handing off txg 15\r\n03:37:50.59 1498981056   txg=15 quiesce_txg=16 sync_txg=13\r\n03:37:50.59 1498981056   command: zpool status -v testpool\r\n03:37:50.59 1498981056   doing scan sync txg 15; bm=51/128/0/642\r\n03:37:50.59 1498981058   scanned dataset 51 (testpool) with min=1 max=13; pausing=0\r\n03:37:50.59 1498981058   visited 168 blocks in 1341ms\r\n03:37:50.59 1498981058   txg 15 traversal complete, waiting till txg 16\r\n03:37:50.59 1498981058   ds=          (null) obj=3d num=1\r\n03:37:50.59 1498981058   txg=16 quiesce_txg=17 sync_txg=13\r\n03:37:50.59 1498981058   quiesce done, handing off txg 16\r\n03:37:50.59 1498981058   txg=16 quiesce_txg=17 sync_txg=13\r\n03:37:50.59 1498981058   txg 16 scan complete\r\n03:37:50.59 1498981058   txg 16 scan done errors=0\r\n03:37:50.59 1498981058   spa=testpool async request task=8\r\n03:37:50.59 1498981058   ds=          (null) obj=8a num=1\r\n03:37:50.59 1498981058   ds=          (null) obj=89 num=1\r\n03:37:50.59 1498981058   ds=          (null) obj=3d num=1\r\n03:37:50.59 1498981058   ds=          (null) obj=88 num=1\r\n03:37:50.59 1498981058   txg=17 quiesce_txg=17 sync_txg=17\r\n03:37:50.59 1498981058   broadcasting sync more tx_synced=15 waiting=17 dp=ffff8800a387c000\r\n03:37:50.59 1498981059   txg=17 quiesce_txg=18 sync_txg=17\r\n03:37:50.59 1498981059   quiesce done, handing off txg 17\r\n03:37:50.59 1498981059   txg=17 quiesce_txg=18 sync_txg=17\r\n03:37:50.59 1498981059   broadcasting sync more tx_synced=16 waiting=17 dp=ffff8800a387c000\r\n03:37:50.59 1498981059   os=ffff88002cb1f800 obj=139 txg=17 blocksize=512 ibs=17 dn_slots=1\r\n03:37:50.59 1498981059   txg 17 scan setup func=1 mintxg=0 maxtxg=17\r\n03:37:50.59 1498981059   doing scan sync txg 17; ddt bm=0/0/0/0\r\n03:37:50.59 1498981059   scanned 0 ddt entries with class_max = 1; pausing=0\r\n03:37:50.59 1498981059   scanned dataset 48 (testpool/$ORIGIN@$ORIGIN) with min=0 max=17; pausing=0\r\n03:37:50.59 1498981059   scanned dataset 45 (testpool/$ORIGIN) with min=1 max=17; pausing=0\r\n03:37:50.59 1498981060   pausing at bookmark 33/80/0/5d\r\n03:37:50.59 1498981060   pausing at DDT bookmark 3/0/0/0\r\n03:37:50.59 1498981060   scanned dataset 51 (testpool) with min=1 max=17; pausing=1\r\n03:37:50.59 1498981060   visited 167 blocks in 1052ms\r\n03:37:50.59 1498981060   ds=          (null) obj=3d num=1\r\n03:37:50.59 1498981060   txg=18 quiesce_txg=19 sync_txg=17\r\n03:37:50.59 1498981060   quiesce done, handing off txg 18\r\n03:37:50.59 1498981060   txg=18 quiesce_txg=19 sync_txg=17\r\n03:37:50.59 1498981060   command: zpool scrub testpool\r\n03:37:50.59 1498981060   doing scan sync txg 18; bm=51/128/0/93\r\n03:37:50.59 1498981060   resuming at 33/80/0/5d\r\n03:37:50.59 1498981060   scanned dataset 51 (testpool) with min=1 max=17; pausing=0\r\n03:37:50.59 1498981060   scanned dataset 131 (testpool/testfs) with min=1 max=17; pausing=0\r\n03:37:50.59 1498981060   visited 734 blocks in 127ms\r\n03:37:50.59 1498981060   txg 18 traversal complete, waiting till txg 19\r\n03:37:50.59 1498981060   ds=          (null) obj=3d num=1\r\n03:37:50.59 1498981060   txg=19 quiesce_txg=20 sync_txg=17\r\n03:37:50.59 1498981060   quiesce done, handing off txg 19\r\n03:37:50.59 1498981060   txg=19 quiesce_txg=20 sync_txg=17\r\n03:37:50.59 1498981060   txg=19 quiesce_txg=20 sync_txg=19\r\n03:37:50.59 1498981060   broadcasting sync more tx_synced=18 waiting=19 dp=ffff8800a387c000\r\n03:37:50.59 1498981060   txg 19 scan complete\r\n03:37:50.59 1498981060   txg 19 scan done errors=0\r\n03:37:50.59 1498981060   spa=testpool async request task=8\r\n03:37:50.59 1498981060   ds=          (null) obj=43 num=1\r\n03:37:50.59 1498981060   ds=          (null) obj=89 num=1\r\n03:37:50.59 1498981060   ds=          (null) obj=3d num=1\r\n03:37:50.59 1498981060   ds=          (null) obj=88 num=1\r\n03:37:50.59 1498981060   ds=          (null) obj=8b num=1\r\n03:37:50.59 1498981060   waiting; tx_synced=19 waiting=19 dp=ffff8800a387c000\r\n03:37:50.59 1498981060   restarting resilver txg=23\r\n03:37:50.59 1498981060   txg=23 quiesce_txg=20 sync_txg=23\r\n03:37:50.59 1498981060   broadcasting sync more tx_synced=19 waiting=23 dp=ffff8800a387c000\r\n03:37:50.59 1498981060   txg=20 quiesce_txg=21 sync_txg=23\r\n03:37:50.59 1498981060   quiesce done, handing off txg 20\r\n03:37:50.59 1498981060   txg=20 quiesce_txg=21 sync_txg=23\r\n03:37:50.59 1498981060   os=ffff88002cb1f800 obj=81 txg=20 blocksize=512 ibs=17 dn_slots=1\r\n03:37:50.59 1498981061   txg 20 detach vdev=/dev/loop1\r\n03:37:50.59 1498981061   command: zpool detach testpool loop1\r\n03:37:50.59 1498981061   os=ffff88002cb1f800 obj=82 txg=20 blocksize=4096 ibs=17 dn_slots=1\r\n03:37:50.59 1498981061   txg 20, spa testpool, DTL old object 0, new object 82\r\n03:37:50.59 1498981061   ds=          (null) obj=3d num=1\r\n03:37:50.59 1498981061   txg=21 quiesce_txg=22 sync_txg=23\r\n03:37:50.59 1498981061   quiesce done, handing off txg 21\r\n03:37:50.59 1498981061   txg=21 quiesce_txg=22 sync_txg=23\r\n03:37:50.59 1498981061   broadcasting sync more tx_synced=20 waiting=23 dp=ffff8800a387c000\r\n03:37:50.59 1498981061   txg=22 quiesce_txg=23 sync_txg=23\r\n03:37:50.59 1498981061   quiesce done, handing off txg 22\r\n03:37:50.59 1498981061   txg=22 quiesce_txg=23 sync_txg=23\r\n03:37:50.59 1498981061   broadcasting sync more tx_synced=21 waiting=23 dp=ffff8800a387c000\r\n03:37:50.59 1498981061   txg=23 quiesce_txg=24 sync_txg=23\r\n03:37:50.59 1498981061   txg=23 quiesce_txg=24 sync_txg=23\r\n03:37:50.59 1498981061   broadcasting sync more tx_synced=22 waiting=23 dp=ffff8800a387c000\r\n03:37:50.59 1498981061   restarting scan func=2 txg=23\r\n03:37:50.59 1498981061   os=ffff88002cb1f800 obj=140 txg=23 blocksize=512 ibs=17 dn_slots=1\r\n03:37:50.59 1498981061   txg 23 scan setup func=2 mintxg=3 maxtxg=23\r\n03:37:50.59 1498981061   doing scan sync txg 23; ddt bm=0/0/0/0\r\n03:37:50.59 1498981061   scanned 0 ddt entries with class_max = 1; pausing=0\r\n03:37:50.59 1498981061   scanned dataset 48 (testpool/$ORIGIN@$ORIGIN) with min=3 max=23; pausing=0\r\n03:37:50.59 1498981061   scanned dataset 131 (testpool/testfs) with min=3 max=23; pausing=0\r\n03:37:50.59 1498981064   pausing at bookmark 33/80/0/13e\r\n03:37:50.59 1498981064   pausing at DDT bookmark 3/0/0/0\r\n03:37:50.59 1498981064   scanned dataset 51 (testpool) with min=3 max=23; pausing=1\r\n03:37:50.59 1498981064   visited 410 blocks in 3081ms\r\n03:37:50.59 1498981064   ds=          (null) obj=3d num=1\r\n03:37:50.59 1498981065   txg=24 quiesce_txg=25 sync_txg=23\r\n03:37:50.59 1498981065   quiesce done, handing off txg 24\r\n03:37:50.59 1498981065   txg=24 quiesce_txg=25 sync_txg=23\r\n03:37:50.59 1498981065   doing scan sync txg 24; bm=51/128/0/318\r\n03:37:50.59 1498981065   resuming at 33/80/0/13e\r\n03:37:50.59 1498981069   scanned dataset 51 (testpool) with min=3 max=23; pausing=0\r\n03:37:50.59 1498981069   scanned dataset 45 (testpool/$ORIGIN) with min=3 max=23; pausing=0\r\n03:37:50.59 1498981069   visited 492 blocks in 4583ms\r\n03:37:50.59 1498981069   txg 24 traversal complete, waiting till txg 25\r\n03:37:50.59 1498981070   ds=          (null) obj=3d num=1\r\n03:37:50.59 1498981070   txg=25 quiesce_txg=26 sync_txg=23\r\n03:37:50.59 1498981070   quiesce done, handing off txg 25\r\n03:37:50.59 1498981070   txg=25 quiesce_txg=26 sync_txg=23\r\n03:37:50.59 1498981070   txg 25 scan complete\r\n03:37:50.59 1498981070   txg 25 scan done errors=0\r\n03:37:50.59 1498981070   spa=testpool async request task=8\r\n03:37:50.59 1498981070   ds=          (null) obj=89 num=1\r\n03:37:50.59 1498981070   ds=          (null) obj=52 num=1\r\n03:37:50.59 1498981070   ds=          (null) obj=3d num=1\r\n03:37:50.59 1498981070 ds= (null) obj=8c num=1\r\n```\r\n\r\n003 causing 005 to fail\r\n\r\n```\r\nTest: /home/testuser/zfs/tests/zfs-tests/tests/functional/cli_root/zpool_scrub/zpool_scrub_005_pos (run as root) [00:00] [FAIL]\r\n08:56:52.38 ASSERTION: When scrubbing, detach device should not break system.\r\n08:56:52.41 ERROR: zpool scrub testpool exited 1\r\n08:56:52.41 cannot scrub testpool: currently scrubbing; use 'zpool scrub -s' to cancel current scrub\r\n08:56:52.41 NOTE: Performing test-fail callback (/home/testuser/zfs/tests/zfs-tests/callbacks/zfs_dbgmsg.ksh)\r\n08:56:52.41 =================================================================\r\n08:56:52.41  Tailing last 200 lines of zfs_dbgmsg log\r\n08:56:52.41 =================================================================\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=7fffffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=30d endblk=30d\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=1fffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=7fffffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=30e endblk=30e\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=1fffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=7fffffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=30f endblk=30f\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=1fffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=7fffffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=310 endblk=310\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=1fffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=7fffffffffff\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=1fffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=7fffffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=312 endblk=312\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=1fffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=7fffffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=313 endblk=313\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=1fffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=7fffffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=314 endblk=314\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=1fffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=7fffffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=315 endblk=315\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=1fffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=7fffffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=316 endblk=316\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=1fffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=7fffffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=317 endblk=317\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=1fffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=7fffffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=318 endblk=318\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=1fffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=7fffffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=319 endblk=319\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=1fffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=7fffffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=31a endblk=31a\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=1fffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=7fffffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=31b endblk=31b\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=1fffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=7fffffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=31c endblk=31c\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=1fffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=7fffffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=31d endblk=31d\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=1fffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=7fffffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=31e endblk=31e\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=1fffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=7fffffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=31f endblk=31f\r\n08:56:52.44 1499000192   found txh type 2 beginblk=0 endblk=1fffffffff\r\n08:56:52.44 1499000192   found txh type 1 beginblk=0 endblk=0\r\n08:56:52.44 1499000192   command: zfs set mountpoint=/var/tmp/testdir testpool/testfs\r\n08:56:52.44 1499000192   os=ffff8800c2591800 obj=71 txg=9 blocksize=4096 ibs=17 dn_slots=1\r\n08:56:52.44 1499000192   os=ffff8800c2591800 obj=72 txg=9 blocksize=4096 ibs=17 dn_slots=1\r\n08:56:52.44 1499000192   os=ffff8800c2591800 obj=73 txg=9 blocksize=4096 ibs=17 dn_slots=1\r\n08:56:52.44 1499000192   os=ffff8800c2591800 obj=74 txg=9 blocksize=4096 ibs=17 dn_slots=1\r\n08:56:52.44 1499000192   ds=          (null) obj=3d num=1\r\n08:56:52.44 1499000193   waiting; tx_synced=9 waiting=8 dp=ffff8800ca03b000\r\n08:56:52.44 1499000193   txg=10 quiesce_txg=10 sync_txg=10\r\n08:56:52.44 1499000193   broadcasting sync more tx_synced=9 waiting=10 dp=ffff8800ca03b000\r\n08:56:52.44 1499000193   txg=10 quiesce_txg=11 sync_txg=10\r\n08:56:52.44 1499000193   quiesce done, handing off txg 10\r\n08:56:52.44 1499000193   txg=10 quiesce_txg=11 sync_txg=10\r\n08:56:52.44 1499000196   os=ffff8800c2591800 obj=75 txg=10 blocksize=512 ibs=17 dn_slots=1\r\n08:56:52.44 1499000196   doing scan sync txg 10; ddt bm=0/0/0/0\r\n08:56:52.44 1499000196   scanned 0 ddt entries with class_max = 1; pausing=0\r\n08:56:52.44 1499000196   scanned dataset 48 (testpool/$ORIGIN@$ORIGIN) with min=0 max=10; pausing=0\r\n08:56:52.44 1499000196   scanned dataset 67 (testpool/testfs) with min=1 max=10; pausing=0\r\n08:56:52.44 1499000197   pausing at bookmark 33/80/0/76\r\n08:56:52.44 1499000197   pausing at DDT bookmark 3/0/0/0\r\n08:56:52.44 1499000197   scanned dataset 51 (testpool) with min=1 max=10; pausing=1\r\n08:56:52.44 1499000197   visited 204 blocks in 1001ms\r\n08:56:52.44 1499000197   os=ffff8800c2591800 obj=76 txg=10 blocksize=4096 ibs=17 dn_slots=1\r\n08:56:52.44 1499000197   os=ffff8800c2591800 obj=77 txg=10 blocksize=4096 ibs=17 dn_slots=1\r\n08:56:52.44 1499000197   os=ffff8800c2591800 obj=78 txg=10 blocksize=4096 ibs=17 dn_slots=1\r\n08:56:52.44 1499000197   os=ffff8800c2591800 obj=79 txg=10 blocksize=4096 ibs=17 dn_slots=1\r\n08:56:52.44 1499000197   os=ffff8800c2591800 obj=80 txg=10 blocksize=4096 ibs=17 dn_slots=1\r\n08:56:52.44 1499000197   ds=          (null) obj=3d num=1\r\n08:56:52.44 1499000198   txg=11 quiesce_txg=12 sync_txg=10\r\n08:56:52.44 1499000198   quiesce done, handing off txg 11\r\n08:56:52.44 1499000198   txg=11 quiesce_txg=12 sync_txg=10\r\n08:56:52.44 1499000198   doing scan sync txg 11; bm=51/128/0/118\r\n08:56:52.44 1499000198   resuming at 33/80/0/76\r\n08:56:52.44 1499000198   txg=12 quiesce_txg=12 sync_txg=12\r\n08:56:52.44 1499000198   broadcasting sync more tx_synced=10 waiting=12 dp=ffff8800ca03b000\r\n08:56:52.44 1499000199   pausing at bookmark 33/80/0/fa\r\n08:56:52.44 1499000199   pausing at DDT bookmark 3/0/0/0\r\n08:56:52.44 1499000199   scanned dataset 51 (testpool) with min=1 max=10; pausing=1\r\n08:56:52.44 1499000199   visited 142 blocks in 1002ms\r\n08:56:52.44 1499000199   ds=          (null) obj=3d num=1\r\n08:56:52.44 1499000200   txg=12 quiesce_txg=13 sync_txg=12\r\n08:56:52.44 1499000200   broadcasting sync more tx_synced=11 waiting=12 dp=ffff8800ca03b000\r\n08:56:52.44 1499000200   quiesce done, handing off txg 12\r\n08:56:52.44 1499000200   txg=12 quiesce_txg=13 sync_txg=12\r\n08:56:52.44 1499000200   command: zpool scrub testpool\r\n08:56:52.44 1499000200   txg 12 scan cancelled errors=0\r\n08:56:52.44 1499000200   spa=testpool async request task=8\r\n08:56:52.44 1499000200   os=ffff8800c2591800 obj=136 txg=12 blocksize=4096 ibs=17 dn_slots=1\r\n08:56:52.44 1499000200   txg 12, spa testpool, DTL old object 0, new object 136\r\n08:56:52.44 1499000200   os=ffff8800c2591800 obj=137 txg=12 blocksize=4096 ibs=17 dn_slots=1\r\n08:56:52.44 1499000200   txg 12, spa testpool, DTL old object 0, new object 137\r\n08:56:52.44 1499000200   ds=          (null) obj=89 num=1\r\n08:56:52.44 1499000200   ds=          (null) obj=4b num=1\r\n08:56:52.44 1499000200   ds=          (null) obj=3d num=1\r\n08:56:52.44 1499000200   ds=          (null) obj=88 num=1\r\n08:56:52.44 1499000200   waiting; tx_synced=12 waiting=12 dp=ffff8800ca03b000\r\n08:56:52.44 1499000201   txg=13 quiesce_txg=13 sync_txg=13\r\n08:56:52.44 1499000201   broadcasting sync more tx_synced=12 waiting=13 dp=ffff8800ca03b000\r\n08:56:52.44 1499000201   txg=13 quiesce_txg=14 sync_txg=13\r\n08:56:52.44 1499000201   quiesce done, handing off txg 13\r\n08:56:52.44 1499000201   txg=13 quiesce_txg=14 sync_txg=13\r\n08:56:52.44 1499000201   command: zpool scrub -s testpool\r\n08:56:52.44 1499000201   os=ffff8800c2591800 obj=138 txg=13 blocksize=512 ibs=17 dn_slots=1\r\n08:56:52.44 1499000201   txg 13 scan setup func=1 mintxg=0 maxtxg=13\r\n08:56:52.44 1499000201   doing scan sync txg 13; ddt bm=0/0/0/0\r\n08:56:52.44 1499000201   scanned 0 ddt entries with class_max = 1; pausing=0\r\n08:56:52.44 1499000201   scanned dataset 48 (testpool/$ORIGIN@$ORIGIN) with min=0 max=13; pausing=0\r\n08:56:52.44 1499000201   scanned dataset 67 (testpool/testfs) with min=1 max=13; pausing=0\r\n08:56:52.44 1499000202   pausing at bookmark 33/80/0/70\r\n08:56:52.44 1499000202   pausing at DDT bookmark 3/0/0/0\r\n08:56:52.44 1499000202   scanned dataset 51 (testpool) with min=1 max=13; pausing=1\r\n08:56:52.44 1499000202   visited 203 blocks in 1091ms\r\n08:56:52.44 1499000202   ds=          (null) obj=3d num=1\r\n08:56:52.44 1499000202   txg=14 quiesce_txg=15 sync_txg=13\r\n08:56:52.44 1499000202   quiesce done, handing off txg 14\r\n08:56:52.44 1499000202   txg=14 quiesce_txg=15 sync_txg=13\r\n08:56:52.44 1499000202   command: zpool scrub testpool\r\n08:56:52.44 1499000202   doing scan sync txg 14; bm=51/128/0/112\r\n08:56:52.44 1499000202   resuming at 33/80/0/70\r\n08:56:52.44 1499000207   pausing at bookmark 33/80/0/284\r\n08:56:52.44 1499000207   pausing at DDT bookmark 3/0/0/0\r\n08:56:52.44 1499000207   scanned dataset 51 (testpool) with min=1 max=13; pausing=1\r\n08:56:52.44 1499000207   visited 542 blocks in 5081ms\r\n08:56:52.44 1499000208   ds=          (null) obj=3d num=1\r\n08:56:52.44 1499000208   txg=15 quiesce_txg=16 sync_txg=13\r\n08:56:52.44 1499000208   quiesce done, handing off txg 15\r\n08:56:52.44 1499000208   txg=15 quiesce_txg=16 sync_txg=13\r\n08:56:52.44 1499000208   resuming at 33/80/0/284\r\n08:56:52.44 1499000209   scanned dataset 51 (testpool) with min=1 max=13; pausing=0\r\n08:56:52.44 1499000209   scanned dataset 45 (testpool/$ORIGIN) with min=1 max=13; pausing=0\r\n08:56:52.44 1499000209   visited 166 blocks in 1321ms\r\n08:56:52.44 1499000209   txg 15 traversal complete, waiting till txg 16\r\n08:56:52.44 1499000210   ds=          (null) obj=3d num=1\r\n08:56:52.44 1499000210   txg=16 quiesce_txg=17 sync_txg=13\r\n08:56:52.44 1499000210   quiesce done, handing off txg 16\r\n08:56:52.44 1499000210   txg=16 quiesce_txg=17 sync_txg=13\r\n08:56:52.44 1499000210   txg 16 scan complete\r\n08:56:52.44 1499000210   txg 16 scan done errors=0\r\n08:56:52.44 1499000210   spa=testpool async request task=8\r\n08:56:52.44 1499000210   ds=          (null) obj=8a num=1\r\n08:56:52.44 1499000210   ds=          (null) obj=89 num=1\r\n08:56:52.44 1499000210   ds=          (null) obj=3d num=1\r\n08:56:52.44 1499000210   ds=          (null) obj=88 num=1\r\n08:56:52.44 1499000210   txg=17 quiesce_txg=17 sync_txg=17\r\n08:56:52.44 1499000210   broadcasting sync more tx_synced=15 waiting=17 dp=ffff8800ca03b000\r\n08:56:52.44 1499000210   broadcasting sync more tx_synced=16 waiting=17 dp=ffff8800ca03b000\r\n08:56:52.44 1499000210   txg=17 quiesce_txg=18 sync_txg=17\r\n08:56:52.44 1499000210   quiesce done, handing off txg 17\r\n08:56:52.44 1499000210   txg=17 quiesce_txg=18 sync_txg=17\r\n08:56:52.44 1499000210   os=ffff8800c2591800 obj=139 txg=17 blocksize=512 ibs=17 dn_slots=1\r\n08:56:52.44 1499000210   txg 17 scan setup func=1 mintxg=0 maxtxg=17\r\n08:56:52.44 1499000210   doing scan sync txg 17; ddt bm=0/0/0/0\r\n08:56:52.44 1499000210   scanned 0 ddt entries with class_max = 1; pausing=0\r\n08:56:52.44 1499000210   scanned dataset 48 (testpool/$ORIGIN@$ORIGIN) with min=0 max=17; pausing=0\r\n08:56:52.44 1499000211   pausing at bookmark 33/80/0/5e\r\n08:56:52.44 1499000211   pausing at DDT bookmark 3/0/0/0\r\n08:56:52.44 1499000211   scanned dataset 51 (testpool) with min=1 max=17; pausing=1\r\n08:56:52.44 1499000211   visited 168 blocks in 1050ms\r\n08:56:52.44 1499000211   ds=          (null) obj=3d num=1\r\n08:56:52.44 1499000212   txg=18 quiesce_txg=19 sync_txg=17\r\n08:56:52.44 1499000212   quiesce done, handing off txg 18\r\n08:56:52.44 1499000212   txg=18 quiesce_txg=19 sync_txg=17\r\n08:56:52.44 1499000212   command: zpool scrub testpool\r\n08:56:52.44 1499000212   doing scan sync txg 18; bm=51/128/0/94\r\n08:56:52.44 1499000212   resuming at 33/80/0/5e\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6299/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6296", "title": "Test case: zfs_copies_006_pos", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Gentoo\r\nDistribution Version    | n/a\r\nLinux Kernel                 | 4.9.16\r\nArchitecture                 | amd64\r\nZFS Version                  | head 0.7.0-rc4_86_gfe46eebe6\r\nSPL Version                  | head 0.7.0-rc4_5_g7a35f2b\r\n\r\n### Describe the problem you're observing\r\nzfs_copies_006_pos never finishes running on my test VM, unsure when this started occurring.  Unsure if this is related to zfs_copies_003_pos as this test appears to a standard zvol.\r\n### Describe how to reproduce the problem\r\nRun the test and wait forever (I cut this one off early after 10 minutes but it'll run forever if given the opportunity)\r\n### Include any warning/errors/backtraces from the system logs\r\n```\r\nTest: /home/testuser/zfs/tests/zfs-tests/tests/functional/cli_root/zfs_copies/zfs_copies_006_pos (run as root) [10:30] [KILLED]\r\n02:25:02.50 ASSERTION: Verify that ZFS volume space used by multiple copies is charged correctly.\r\n02:25:02.56 SUCCESS: zfs create -V 150m -o copies=1 testpool/testvol1\r\n02:25:02.59 SUCCESS: zfs set refreservation=none testpool/testvol1\r\n02:25:02.95 SUCCESS: mount -o rw /dev/zvol/testpool/testvol1 /testdir_nfs_mntpoint\r\n02:25:02.96 5569536\r\n02:25:02.97 SUCCESS: mkfile 10m /testdir_nfs_mntpoint/file.copies\r\n02:25:02.98 5569536\r\n(trimmed for brevity)\r\n02:25:38.37 5569536\r\n02:25:39.38 16103424\r\n(trimmed)\r\n02:35:31.94 16103424\r\n(I got bored and killed the test)\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6296/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6295", "title": "Test case: zfs_copies_003_pos", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Gentoo\r\nDistribution Version    | n/a\r\nLinux Kernel                 | 4.9.16\r\nArchitecture                 | amd64\r\nZFS Version                  | head 0.7.0-rc4_86_gfe46eebe6\r\nSPL Version                  | head 0.7.0-rc4_5_g7a35f2b\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nzfs_copies_003_pos never finishes running on my test VM, unsure when this started occurring.  This test appears to employ a pool-on-pool setup, which was previously described as being problematic due to a deadlock situation.  I don't see this problem on any of the buildbots, but I felt it was worth reporting as its reproducible on my test VM.\r\n### Describe how to reproduce the problem\r\nRun the test and wait forever\r\n### Include any warning/errors/backtraces from the system logs\r\n```\r\nTest: /home/testuser/zfs/tests/zfs-tests/tests/functional/cli_root/zfs_copies/zfs_copies_003_pos (run as root) [49:05] [KILLED]\r\n00:16:18.63 ASSERTION: Verify that ZFS volume space used by multiple copies is charged correctly.\r\n00:16:18.68 SUCCESS: zfs create -V 150m -o copies=1 testpool/testvol1\r\n00:16:18.70 SUCCESS: zfs set refreservation=none testpool/testvol1\r\n00:16:18.97 SUCCESS: zpool create testpool1 /dev/zvol/testpool/testvol1\r\n00:16:19.00 SUCCESS: zfs create testpool1/testfs1\r\n00:16:19.01 20480\r\n00:16:19.05 SUCCESS: mkfile 10m /testpool1/testfs1/file.copies\r\n00:16:19.06 20480\r\n(4 entries trimmed for brevity)\r\n00:16:24.11 20480\r\n00:16:25.13 15262720\r\n(3 entries trimmed for brevity)\r\n00:16:29.17 15262720\r\n00:16:30.18 15469568\r\n(many entries trimmed for brevity)\r\n00:24:15.40 15469568\r\n00:24:16.42 15874048\r\n(trimmed)\r\n00:24:25.51 16181248\r\n00:24:26.53 16471040\r\n(trimmed)\r\n00:50:26.81 16471040\r\n00:50:27.82 16761856\r\n(trimmed)\r\n00:50:36.92 16761856\r\n00:50:37.94 17050624\r\n(trimmed)\r\n00:50:52.09 17050624\r\n00:50:53.10 17374208\r\n(trimmed)\r\n00:55:54.47 17374208\r\n00:55:55.48 18679808\r\n(trimmed)\r\n00:55:59.53 18679808\r\n00:56:00.54 18993152\r\n(trimmed)\r\n01:05:23.99 18993152\r\n(i got bored and killed the test)\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6295/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6086", "title": "Test case: rsend_019_pos", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Gentoo\r\nDistribution Version    | \r\nLinux Kernel                 | 4.4.52-gentoo\r\nArchitecture                 | amd64 (intel haswell)\r\nZFS Version                  | 0.7.0-rc3 (git head + #6074)\r\nSPL Version                  | 0.7.0-rc3 (git head)\r\n\r\n### Describe the problem you're observing\r\nrsend_019_pos occasionally takes way way longer than it should, and eventually gets killed off by the test scripts.  Average run time is less than a minute, sometimes can run longer than 45 minutes on my VM despite the 10 minute cutoff timer.  Culprit appears to be a zfs recv operation.  ZFS version doesn't seem to matter much, anything recent seems to exhibit the same behaviour.\r\n\r\n> Test: /home/testuser/zfs/tests/zfs-tests/tests/functional/rsend/rsend_019_pos (run as root) [46:49] [KILLED]\r\n\r\n### Describe how to reproduce the problem\r\nUsually can get triggered by running tests repeatedly on the same system without rebooting, doesn't appear to happen on kernel 4.9, but it does on 4.4.  Seems to happen on occasion to the Ubuntu 14.04 i686 testbot recently as well. [eg1](http://build.zfsonlinux.org/builders/Ubuntu%2014.04%20i686%20%28TEST%29/builds/1904/steps/shell_9/logs/stdio) [eg2](http://build.zfsonlinux.org/builders/Ubuntu%2014.04%20i686%20%28TEST%29/builds/1902/steps/shell_9/logs/stdio) [eg3](http://build.zfsonlinux.org/builders/Ubuntu%2014.04%20i686%20%28TEST%29/builds/1898/steps/shell_9/logs/stdio)\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nI grabbed these stack traces while the test was spinning its wheels, I hope they are useful.\r\n\r\n[zfs.16456](https://gist.github.com/bunder2015/e79863e449adf4abbccde7fa65cdcdd6)\r\n[txg_quiesce.28243](https://gist.github.com/bunder2015/3e924a3564edc7c2015f8c0bbe148a0b)\r\n[txg_sync.28244](https://gist.github.com/bunder2015/5231b1a0478d82dcee697871cf823570)\r\n[txg_quiesce.28087](https://gist.github.com/bunder2015/1e2fa65ec8e29e8a56c53b49596d34b9)\r\n[txg_sync.28088](https://gist.github.com/bunder2015/8521e5b3e69f1528005f27acdc7ef04c)\r\n[receive_writer.16519](https://gist.github.com/bunder2015/888ad71306af98c5532e2642fb839779)", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6086/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/2917956841362b10af8b3afe7274968ddcc3f6e2", "message": "zfs(8) manpage corrections\n\nCorrected indent of the note located at the bottom of the options for\r\nzfs send as well as remove an extra whitespace\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: bunder2015 <omfgbunder@gmail.com>\r\nCloses #6590"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/0f69f42b43637548bec225ed25fa71b032de114e", "message": "Correct man page generation\n\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: bunder2015 <omfgbunder@gmail.com>\r\nCloses #6409\r\nCloses #6410"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ioquatix": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6265", "title": "task mv:11533 blocked for more than 120 seconds", "body": "### System information\r\n\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Arch Linux\r\nDistribution Version    | Up to date rolling release\r\nLinux Kernel                 | Linux hana 4.11.6-1-ARCH #1 SMP PREEMPT Sat Jun 17 08:19:42 CEST 2017 x86_64 GNU/Linux\r\nArchitecture                 | Intel Pentium G4560\r\nZFS Version                  | 0.7.0-rc4_65_gd9ad3fea3\r\nSPL Version                  | 0.7.0-rc4_4_gac48361\r\n\r\n### Describe the problem you're observing\r\n\r\nThis is an entirely new system with 6x 4TB WD RED drives on a SAS3 backplane with 32Gb of memory with about 12 free at the time this happened.\r\n\r\n```\r\n[16655.091457] perf: interrupt took too long (3144 > 3132), lowering kernel.perf_event_max_sample_rate to 63600\r\n[23030.236885] perf: interrupt took too long (3946 > 3930), lowering kernel.perf_event_max_sample_rate to 50400\r\n[37846.703532] INFO: task mv:11533 blocked for more than 120 seconds.\r\n[37846.703601]       Tainted: P           O    4.11.6-1-ARCH #1\r\n[37846.703651] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[37846.703719] mv              D    0 11533  16785 0x00000000\r\n[37846.703725] Call Trace:\r\n[37846.703738]  __schedule+0x22e/0x8e0\r\n[37846.703744]  schedule+0x3d/0x90\r\n[37846.703755]  cv_wait_common+0x11c/0x130 [spl]\r\n[37846.703763]  ? wake_bit_function+0x60/0x60\r\n[37846.703790]  __cv_wait+0x15/0x20 [spl]\r\n[37846.703863]  txg_wait_open+0xb0/0x100 [zfs]\r\n[37846.703919]  dmu_free_long_range+0x295/0x410 [zfs]\r\n[37846.703985]  zfs_rmnode+0x245/0x330 [zfs]\r\n[37846.704042]  ? zfs_znode_hold_exit+0xf4/0x130 [zfs]\r\n[37846.704098]  zfs_zinactive+0xd4/0xe0 [zfs]\r\n[37846.704155]  zfs_inactive+0x7e/0x210 [zfs]\r\n[37846.704164]  ? truncate_pagecache+0x5a/0x70\r\n[37846.704218]  zpl_evict_inode+0x43/0x60 [zfs]\r\n[37846.704227]  evict+0xc5/0x190\r\n[37846.704234]  iput+0x1ae/0x240\r\n[37846.704239]  do_unlinkat+0x1a7/0x310\r\n[37846.704246]  SyS_unlinkat+0x1b/0x30\r\n[37846.704252]  entry_SYSCALL_64_fastpath+0x1a/0xa9\r\n[37846.704257] RIP: 0033:0x7f6829416a27\r\n[37846.704260] RSP: 002b:00007ffefe01f278 EFLAGS: 00000246 ORIG_RAX: 0000000000000107\r\n[37846.704266] RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 00007f6829416a27\r\n[37846.704269] RDX: 0000000000000000 RSI: 0000000001f0df18 RDI: 0000000000000004\r\n[37846.704272] RBP: 0000000001f0da60 R08: 0000000000000000 R09: 0000000000000000\r\n[37846.704275] R10: 70303830312e3130 R11: 0000000000000246 R12: 0000000000000004\r\n[37846.704279] R13: 00007ffefe01f354 R14: 00007ffefe01f3b0 R15: 0000000001f0e970\r\n[37969.583533] INFO: task mv:11533 blocked for more than 120 seconds.\r\n[37969.583617]       Tainted: P           O    4.11.6-1-ARCH #1\r\n[37969.583694] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[37969.583777] mv              D    0 11533  16785 0x00000000\r\n[37969.583783] Call Trace:\r\n[37969.583796]  __schedule+0x22e/0x8e0\r\n[37969.583802]  schedule+0x3d/0x90\r\n[37969.583812]  cv_wait_common+0x11c/0x130 [spl]\r\n[37969.583820]  ? wake_bit_function+0x60/0x60\r\n[37969.583827]  __cv_wait+0x15/0x20 [spl]\r\n[37969.583880]  txg_wait_open+0xb0/0x100 [zfs]\r\n[37969.583917]  dmu_free_long_range+0x295/0x410 [zfs]\r\n[37969.583961]  zfs_rmnode+0x245/0x330 [zfs]\r\n[37969.584001]  ? zfs_znode_hold_exit+0xf4/0x130 [zfs]\r\n[37969.584040]  zfs_zinactive+0xd4/0xe0 [zfs]\r\n[37969.584079]  zfs_inactive+0x7e/0x210 [zfs]\r\n[37969.584085]  ? truncate_pagecache+0x5a/0x70\r\n[37969.584128]  zpl_evict_inode+0x43/0x60 [zfs]\r\n[37969.584137]  evict+0xc5/0x190\r\n[37969.584144]  iput+0x1ae/0x240\r\n[37969.584149]  do_unlinkat+0x1a7/0x310\r\n[37969.584156]  SyS_unlinkat+0x1b/0x30\r\n[37969.584162]  entry_SYSCALL_64_fastpath+0x1a/0xa9\r\n[37969.584166] RIP: 0033:0x7f6829416a27\r\n[37969.584169] RSP: 002b:00007ffefe01f278 EFLAGS: 00000246 ORIG_RAX: 0000000000000107\r\n[37969.584173] RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 00007f6829416a27\r\n[37969.584175] RDX: 0000000000000000 RSI: 0000000001f0df18 RDI: 0000000000000004\r\n[37969.584177] RBP: 0000000001f0da60 R08: 0000000000000000 R09: 0000000000000000\r\n[37969.584179] R10: 70303830312e3130 R11: 0000000000000246 R12: 0000000000000004\r\n[37969.584181] R13: 00007ffefe01f354 R14: 00007ffefe01f3b0 R15: 0000000001f0e970\r\n[38092.463546] INFO: task mv:11533 blocked for more than 120 seconds.\r\n[38092.463614]       Tainted: P           O    4.11.6-1-ARCH #1\r\n[38092.463681] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[38092.463749] mv              D    0 11533  16785 0x00000000\r\n[38092.463755] Call Trace:\r\n[38092.463768]  __schedule+0x22e/0x8e0\r\n[38092.463774]  schedule+0x3d/0x90\r\n[38092.463785]  cv_wait_common+0x11c/0x130 [spl]\r\n[38092.463801]  ? wake_bit_function+0x60/0x60\r\n[38092.463808]  __cv_wait+0x15/0x20 [spl]\r\n[38092.463864]  txg_wait_open+0xb0/0x100 [zfs]\r\n[38092.463900]  dmu_free_long_range+0x295/0x410 [zfs]\r\n[38092.464209]  zfs_rmnode+0x245/0x330 [zfs]\r\n[38092.464250]  ? zfs_znode_hold_exit+0xf4/0x130 [zfs]\r\n[38092.464289]  zfs_zinactive+0xd4/0xe0 [zfs]\r\n[38092.464328]  zfs_inactive+0x7e/0x210 [zfs]\r\n[38092.464335]  ? truncate_pagecache+0x5a/0x70\r\n[38092.464374]  zpl_evict_inode+0x43/0x60 [zfs]\r\n[38092.464382]  evict+0xc5/0x190\r\n[38092.464387]  iput+0x1ae/0x240\r\n[38092.464391]  do_unlinkat+0x1a7/0x310\r\n[38092.464395]  SyS_unlinkat+0x1b/0x30\r\n[38092.464399]  entry_SYSCALL_64_fastpath+0x1a/0xa9\r\n[38092.464403] RIP: 0033:0x7f6829416a27\r\n[38092.464406] RSP: 002b:00007ffefe01f278 EFLAGS: 00000246 ORIG_RAX: 0000000000000107\r\n[38092.464410] RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 00007f6829416a27\r\n[38092.464412] RDX: 0000000000000000 RSI: 0000000001f0df18 RDI: 0000000000000004\r\n[38092.464414] RBP: 0000000001f0da60 R08: 0000000000000000 R09: 0000000000000000\r\n[38092.464416] R10: 70303830312e3130 R11: 0000000000000246 R12: 0000000000000004\r\n[38092.464418] R13: 00007ffefe01f354 R14: 00007ffefe01f3b0 R15: 0000000001f0e970\r\n```\r\n\r\nI have two ZFS pools:\r\n\r\n```\r\n# zpool status\r\n  pool: old-tank\r\n state: ONLINE\r\nstatus: Some supported features are not enabled on the pool. The pool can\r\n        still be used, but some features are unavailable.\r\naction: Enable all features using 'zpool upgrade'. Once this is done,\r\n        the pool may no longer be accessible by software that does not support\r\n        the features. See zpool-features(5) for details.\r\n  scan: scrub repaired 0B in 19h6m with 0 errors on Tue Apr  4 16:31:03 2017\r\nconfig:\r\n\r\n        NAME                        STATE     READ WRITE CKSUM\r\n        old-tank                    ONLINE       0     0     0\r\n          raidz1-0                  ONLINE       0     0     0\r\n            wwn-0x50014ee20702ef26  ONLINE       0     0     0\r\n            wwn-0x50014ee15a0a0c46  ONLINE       0     0     0\r\n            wwn-0x50014ee20702e46a  ONLINE       0     0     0\r\n            wwn-0x50014ee15a0a11ce  ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n\r\n  pool: tank\r\n state: ONLINE\r\n  scan: none requested\r\nconfig:\r\n\r\n        NAME                        STATE     READ WRITE CKSUM\r\n        tank                        ONLINE       0     0     0\r\n          mirror-0                  ONLINE       0     0     0\r\n            wwn-0x50014ee20eba1695  ONLINE       0     0     0\r\n            wwn-0x50014ee20eba337e  ONLINE       0     0     0\r\n          mirror-1                  ONLINE       0     0     0\r\n            wwn-0x50014ee2640efb3a  ONLINE       0     0     0\r\n            wwn-0x50014ee2b964ef3f  ONLINE       0     0     0\r\n          mirror-2                  ONLINE       0     0     0\r\n            wwn-0x50014ee2b964f2d5  ONLINE       0     0     0\r\n            wwn-0x50014ee2b964f4f4  ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n```\r\n\r\nI am copying a lot of data from the `old-tank` to `tank`.\r\n\r\nWhen copying, from `zfs iostat 5` I was getting about 250Mbytes/s READ from `old-tank` and 550Mbyte/s WRITE to `tank`. It was humming along quite nicely for a long time.\r\n\r\nI noticed some odd pauses and from another machine, copying via `scp`, it said \"stalled\" which indicates no data transfer for a few moments.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nCopy lots of data. Perhaps this issue is isolated to the `old-tank`, or operations that involve it? Perhaps those drives have issues and they may not have TLER so are blocking the system?\r\n\r\n### drives\r\n\r\n```\r\nlrwxrwxrwx 1 root root    9 Jun 25 19:41 ata-Samsung_SSD_850_PRO_256GB_S39KNWAJ313793W -> ../../sda\r\nlrwxrwxrwx 1 root root    9 Jun 25 23:25 ata-WDC_WD30EZRX-00MMMB0_WD-WCAWZ2144377 -> ../../sdh\r\nlrwxrwxrwx 1 root root    9 Jun 25 23:26 ata-WDC_WD30EZRX-00MMMB0_WD-WCAWZ2263503 -> ../../sdj\r\nlrwxrwxrwx 1 root root    9 Jun 25 23:26 ata-WDC_WD30EZRX-00MMMB0_WD-WMAWZ0353148 -> ../../sdk\r\nlrwxrwxrwx 1 root root    9 Jun 25 23:26 ata-WDC_WD30EZRX-00MMMB0_WD-WMAWZ0365295 -> ../../sdi\r\nlrwxrwxrwx 1 root root    9 Jun 25 19:41 ata-WDC_WD40EFRX-68N32N0_WD-WCC7K0EZ4UKS -> ../../sdc\r\nlrwxrwxrwx 1 root root    9 Jun 25 19:41 ata-WDC_WD40EFRX-68N32N0_WD-WCC7K0RLAYP1 -> ../../sdf\r\nlrwxrwxrwx 1 root root    9 Jun 25 19:41 ata-WDC_WD40EFRX-68N32N0_WD-WCC7K1AT8T4C -> ../../sdd\r\nlrwxrwxrwx 1 root root    9 Jun 25 19:41 ata-WDC_WD40EFRX-68N32N0_WD-WCC7K2NYU749 -> ../../sdg\r\nlrwxrwxrwx 1 root root    9 Jun 25 19:41 ata-WDC_WD40EFRX-68N32N0_WD-WCC7K4VYNKV0 -> ../../sde\r\nlrwxrwxrwx 1 root root    9 Jun 25 19:41 ata-WDC_WD40EFRX-68N32N0_WD-WCC7K4YSVVTC -> ../../sdb\r\nlrwxrwxrwx 1 root root    9 Jun 25 23:26 wwn-0x50014ee15a0a0c46 -> ../../sdk\r\nlrwxrwxrwx 1 root root    9 Jun 25 23:26 wwn-0x50014ee15a0a11ce -> ../../sdi\r\nlrwxrwxrwx 1 root root    9 Jun 25 23:25 wwn-0x50014ee20702e46a -> ../../sdh\r\nlrwxrwxrwx 1 root root    9 Jun 25 23:26 wwn-0x50014ee20702ef26 -> ../../sdj\r\nlrwxrwxrwx 1 root root    9 Jun 25 19:41 wwn-0x50014ee20eba1695 -> ../../sde\r\nlrwxrwxrwx 1 root root    9 Jun 25 19:41 wwn-0x50014ee20eba337e -> ../../sdf\r\nlrwxrwxrwx 1 root root    9 Jun 25 19:41 wwn-0x50014ee2640efb3a -> ../../sdb\r\nlrwxrwxrwx 1 root root    9 Jun 25 19:41 wwn-0x50014ee2b964ef3f -> ../../sdd\r\nlrwxrwxrwx 1 root root    9 Jun 25 19:41 wwn-0x50014ee2b964f2d5 -> ../../sdc\r\nlrwxrwxrwx 1 root root    9 Jun 25 19:41 wwn-0x50014ee2b964f4f4 -> ../../sdg\r\nlrwxrwxrwx 1 root root    9 Jun 25 19:41 wwn-0x5002538d704e9ff1 -> ../../sda\r\n```\r\n\r\n### iotop while mv\r\n\r\n```\r\nTotal DISK READ :      99.58 M/s | Total DISK WRITE :      51.15 M/s\r\nActual DISK READ:      49.47 M/s | Actual DISK WRITE:     119.54 M/s\r\n  TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO>    COMMAND                                                                                                                         \r\n 5702 be/0 root        0.00 B/s    0.00 B/s  0.00 % 99.99 % [z_null_int]\r\n  474 be/0 root        0.00 B/s    0.00 B/s  0.00 % 99.99 % [z_null_int]\r\n 5704 be/0 root        4.90 M/s    0.00 B/s  0.00 %  0.00 % [z_rd_int_0]\r\n 5705 be/0 root        5.25 M/s    0.00 B/s  0.00 %  0.00 % [z_rd_int_1]\r\n 5706 be/0 root        4.62 M/s    0.00 B/s  0.00 %  0.00 % [z_rd_int_2]\r\n 5707 be/0 root        4.97 M/s    0.00 B/s  0.00 %  0.00 % [z_rd_int_3]\r\n 5708 be/0 root        6.32 M/s    0.00 B/s  0.00 %  0.00 % [z_rd_int_4]\r\n 5709 be/0 root        6.21 M/s    0.00 B/s  0.00 %  0.00 % [z_rd_int_5]\r\n 5710 be/0 root        4.89 M/s    0.00 B/s  0.00 %  0.00 % [z_rd_int_6]\r\n 5711 be/0 root        4.97 M/s    0.00 B/s  0.00 %  0.00 % [z_rd_int_7]\r\n26859 be/4 samuel     57.45 M/s   51.15 M/s  0.00 %  0.00 % mv /old-tank/media/Western/Animation/stuff /tank/media/video/Cartoons/\r\n```\r\n\r\n### smartctl --all\r\n\r\nFor the four older drives, I've attached the output of smartctl. I think one of the drives may have started to go bad (wwn-0x50014ee20702e46a) - or it might have been a one off failure for some reason. I'll investigate further.\r\n\r\n[wwn-0x50014ee15a0a0c46-smart.txt](https://github.com/zfsonlinux/zfs/files/1100267/wwn-0x50014ee15a0a0c46-smart.txt)\r\n[wwn-0x50014ee15a0a11ce-smart.txt](https://github.com/zfsonlinux/zfs/files/1100270/wwn-0x50014ee15a0a11ce-smart.txt)\r\n[wwn-0x50014ee20702e46a-smart.txt](https://github.com/zfsonlinux/zfs/files/1100268/wwn-0x50014ee20702e46a-smart.txt)\r\n[wwn-0x50014ee20702ef26-smart.txt](https://github.com/zfsonlinux/zfs/files/1100269/wwn-0x50014ee20702ef26-smart.txt)\r\n\r\n### iostat\r\n\r\n```\r\n# iostat -mx 10\r\nLinux 4.11.6-1-ARCH (hana)      26/06/17        _x86_64_        (4 CPU)\r\n\r\navg-cpu:  %user   %nice %system %iowait  %steal   %idle\r\n           7.35    0.00    7.85   15.41    0.00   69.38\r\n\r\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\r\nsda               0.00     0.55    0.39    0.81     0.01     0.01    21.15     0.00    0.55    0.22    0.71   0.45   0.05\r\nsdb               0.00     0.00    1.38  128.88     0.02    12.98   204.30     0.25    1.92   48.12    1.43   1.20  15.64\r\nsdc               0.00     0.00    1.43  125.11     0.02    12.53   203.16     0.21    1.68   27.86    1.38   1.01  12.72\r\nsdd               0.00     0.00    1.37  128.45     0.02    12.98   205.00     0.21    1.64   28.27    1.35   0.98  12.67\r\nsde               0.00     0.00    1.39  130.68     0.02    13.24   205.57     0.24    1.77   35.04    1.42   1.06  14.03\r\nsdf               0.00     0.00    1.41  130.46     0.02    13.24   205.89     0.22    1.64   27.91    1.35   0.98  12.87\r\nsdg               0.00     0.00    1.35  125.29     0.02    12.53   202.95     0.29    2.26   73.42    1.49   1.50  18.93\r\nsdh               0.00     0.00  163.08   21.99     9.34     0.43   108.03     1.31    7.08    7.92    0.81   2.03  37.52\r\nsdi               0.00     0.00  164.02   18.43     9.33     0.43   109.55     1.27    6.98    7.65    1.08   2.01  36.75\r\nsdj               0.00     0.00  160.24   21.91     9.34     0.43   109.75     1.32    7.23    8.14    0.61   2.04  37.12\r\nsdk               0.00     0.00  163.52   19.62     9.33     0.42   109.13     1.33    7.25    8.02    0.83   2.00  36.71\r\n\r\navg-cpu:  %user   %nice %system %iowait  %steal   %idle\r\n           0.10    0.00    0.28    0.55    0.00   99.07\r\n\r\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\r\nsda               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\r\nsdb               0.00     0.00    0.40   10.10     0.00     0.05     9.75     0.02    1.65   38.25    0.20   1.59   1.67\r\nsdc               0.00     0.00    0.40   10.10     0.00     0.05     9.75     0.02    1.69   40.00    0.17   1.62   1.70\r\nsdd               0.00     0.00    0.40    9.80     0.00     0.05    10.04     0.02    1.54   35.00    0.17   1.47   1.50\r\nsde               0.00     0.00    0.40    8.70     0.00     0.04     7.91     0.01    1.62   35.00    0.08   1.57   1.43\r\nsdf               0.00     0.00    0.40    8.70     0.00     0.04     7.91     0.02    1.90   41.75    0.07   1.87   1.70\r\nsdg               0.00     0.00    0.40   10.00     0.00     0.05     9.85     0.02    2.28   55.00    0.17   2.21   2.30\r\nsdh               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\r\nsdi               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\r\nsdj               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\r\nsdk               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\r\n```\r\n\r\n### free -h\r\n\r\n```\r\n# free -h\r\n              total        used        free      shared  buff/cache   available\r\nMem:            31G         12G         18G        1.3M        462M         18G\r\nSwap:          8.0G          0B        8.0G\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6265/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "MyPod": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6263", "title": "PANIC at zvol.c:1165:zvol_resume()", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian\r\nDistribution Version    | Stretch\r\nLinux Kernel                 | 4.9.30-2+deb9u1\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.7.0-rc4_64_g0241e491a\r\nSPL Version                  | 0.7.0-rc4_4_gac48361\r\n\r\n### Describe the problem you're observing\r\nWhile receiving a zvol from another machine, I obtained this:\r\n\r\n```\r\n[238797.610918] VERIFY3(zv->zv_objset->os_dsl_dataset->ds_owner == zv) failed (          (null) == ffff931c97dc3400)\r\n[238797.612947] PANIC at zvol.c:1165:zvol_resume()\r\n[238797.614940] Showing stack for process 10784\r\n[238797.616951] CPU: 0 PID: 10784 Comm: zfs Tainted: G           O    4.9.0-3-amd64 #1 Debian 4.9.30-2+deb9u1\r\n[238797.619117] Hardware name: Gigabyte Technology Co., Ltd. GA-78LMT-USB3 6.0/GA-78LMT-USB3 6.0, BIOS F2 11/25/2014\r\n[238797.621219]  0000000000000000 ffffffff8dd286e4 ffffffffc0dcbfec ffffb083029079e0\r\n[238797.623331]  ffffffffc052535b ffff931c78582140 ffffffff00000030 ffffb083029079f0\r\n[238797.625468]  ffffb08302907990 2833594649524556 6f5f767a3e2d767a 6f3e2d7465736a62\r\n[238797.627629] Call Trace:\r\n[238797.629779]  [<ffffffff8dd286e4>] ? dump_stack+0x5c/0x78\r\n[238797.631967]  [<ffffffffc052535b>] ? spl_panic+0xbb/0xf0 [spl]\r\n[238797.634118]  [<ffffffff8e00380e>] ? mutex_lock+0xe/0x30\r\n[238797.636379]  [<ffffffffc0cb1bd0>] ? rrw_exit+0x40/0x130 [zfs]\r\n[238797.638516]  [<ffffffff8e00380e>] ? mutex_lock+0xe/0x30\r\n[238797.640741]  [<ffffffffc0c79902>] ? dmu_objset_from_ds+0x62/0x140 [zfs]\r\n[238797.642967]  [<ffffffffc0c79a53>] ? dmu_objset_hold+0x73/0xc0 [zfs]\r\n[238797.645203]  [<ffffffffc0d3a2d8>] ? zvol_resume+0x108/0x1b0 [zfs]\r\n[238797.647449]  [<ffffffffc0d073d8>] ? zfs_ioc_recv_impl+0x828/0x10a0 [zfs]\r\n[238797.649726]  [<ffffffffc0d07e61>] ? zfs_ioc_recv+0x211/0x340 [zfs]\r\n[238797.651991]  [<ffffffffc0cc82da>] ? spa_name_compare+0xa/0x20 [zfs]\r\n[238797.654171]  [<ffffffffc01c1141>] ? avl_find+0x51/0x90 [zavl]\r\n[238797.656408]  [<ffffffffc0cc717e>] ? spa_lookup+0x5e/0x80 [zfs]\r\n[238797.658579]  [<ffffffff8dbdf99e>] ? __kmalloc+0x17e/0x560\r\n[238797.660718]  [<ffffffff8dca2171>] ? security_capable+0x41/0x60\r\n[238797.662876]  [<ffffffff8da80b33>] ? ns_capable_common+0x63/0x80\r\n[238797.665121]  [<ffffffffc0cb08cd>] ? priv_policy.isra.1.part.2+0xd/0x20 [zfs]\r\n[238797.667369]  [<ffffffffc0d052fe>] ? zfsdev_ioctl+0x73e/0x890 [zfs]\r\n[238797.669555]  [<ffffffff8dbb384b>] ? handle_mm_fault+0xf7b/0x1390\r\n[238797.671748]  [<ffffffff8dc161cf>] ? do_vfs_ioctl+0x9f/0x600\r\n[238797.673943]  [<ffffffff8dc167a4>] ? SyS_ioctl+0x74/0x80\r\n[238797.676131]  [<ffffffff8e00627b>] ? system_call_fast_compare_end+0xc/0x9b\r\n```\r\n\r\n### Describe how to reproduce the problem\r\nThis seems to happen infrequently on zfs receive. I don't have a 100% way to reproduce it, but it has happened a few times and forced me to reboot the machine involved as the pool affected would become unresponsive (zfs and zpool commands becoming unresponsive, no I/O possible from VMs running on the same pool)\r\n\r\nThe command used was\r\n`zfs send -Rv mov/stretch-git@send | ssh root@192.168.2.7 zfs recv -uv data/virt/git`\r\nhowever I have had this happen even locally (between two different pools on the same machine)\r\n\r\nIf anything else is needed, I have not yet rebooted this system, but will have to soon.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6263/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "devZer0": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6261", "title": "zpool labelclear /dev/sda and /dev/sdb wiped partitions !!!", "body": "I have setup two old hdd`s for a new OS installation (raid1 mdadm mirror).\r\nThe disks had been used in a zpool before.\r\n\r\nAfter new CentOS installation (i.e. new partitioning / formatting) some \"ghost\" pool showed up i tried to get rid of with zpool labelclear, as i thought that would wipe the \"remaining\" zfs information from the disks.\r\n\r\nIIrc, i wiped the disks befor OS installation with \"dd\", but as they are 2TB i did not wait dd to finish.\r\n\r\nApparently, the system did not boot anymore after labelclear operation. \r\nThe partition tables seem to be wiped completely and i was not able to recover the partitions.\r\n\r\nI`m not sure if that was my mistake in how i used labelclear (maybe i overread something),  why had the partion tables been removed when there (i assume) was only some zfs information at the end or the middle of the disk ?\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6261/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/3938404", "body": "wouldn`t it be more flexible if that database would be read from a textfile within /etc/zfs/ ?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/3938404/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "azeemism": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6251", "title": "[0.6.5.9-5] Debian Jessie Native ZFS fails to load sub filesystems when rpool/ROOT/debian-8/var is set to canmount=off", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n\r\n```root@skynet:/proc# uname -a\r\nLinux skynet 3.16.0-4-amd64 #1 SMP Debian 3.16.43-2+deb8u1 (2017-06-18) x86_64 GNU/Linux\r\n```\r\n```\r\nroot@skynet:/# dpkg -l | grep zfs\r\nii  libzfs2linux                         0.6.5.9-5                            amd64        OpenZFS filesystem library for Linux\r\nii  zfs-dkms                             0.6.5.9-5                            all          OpenZFS filesystem kernel modules for Linux\r\nii  zfs-initramfs                        0.6.5.9-5                            all          OpenZFS root filesystem capabilities for Linux - initramfs\r\nii  zfs-zed                              0.6.5.9-5                            amd64        OpenZFS Event Daemon\r\nii  zfsutils-linux                       0.6.5.9-5                            amd64        command-line tools to manage OpenZFS filesystems\r\n```\r\n```\r\nroot@skynet:/# dpkg -l | grep spl\r\nii  spl                                  0.6.5.9-1                            amd64        Solaris Porting Layer user-space utilities for Linux\r\nii  spl-dkms                             0.6.5.9-1                            all          Solaris Porting Layer kernel modules for Linux\r\n```\r\n```\r\nroot@skynet:/boot# cat /etc/*-release\r\nPRETTY_NAME=\"Debian GNU/Linux 8 (jessie)\"\r\nNAME=\"Debian GNU/Linux\"\r\nVERSION_ID=\"8\"\r\nVERSION=\"8 (jessie)\"\r\nID=debian\r\n```\r\n```\r\nroot@skynet:/boot# modinfo zfs | grep -iw version\r\nversion:        0.6.5.9-5\r\nroot@skynet:/boot# modinfo spl | grep -iw version\r\nversion:        0.6.5.9-1\r\n\r\n```\r\n\r\n\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n```\r\nroot@skynet:/boot# zpool status\r\n  pool: rpool\r\n state: ONLINE\r\n  scan: none requested\r\nconfig:\r\n\r\n        NAME                                                STATE     READ WRITE CKSUM\r\n        rpool                                               ONLINE       0     0     0\r\n          mirror-0                                          ONLINE       0     0     0\r\n            ata-Samsung_SSD_840_PRO_Series_S1ATNSAF520000X  ONLINE       0     0     0\r\n            ata-Samsung_SSD_840_PRO_Series_S1ATNSAF520000D  ONLINE       0     0     0\r\n          mirror-1                                          ONLINE       0     0     0\r\n            ata-Samsung_SSD_840_PRO_Series_S1ATNSAF520000E  ONLINE       0     0     0\r\n            ata-Samsung_SSD_840_PRO_Series_S1ATNSAF520000L  ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n```\r\n\r\n```\r\nroot@skynet:/# mount\r\nsysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime)\r\nproc on /proc type proc (rw,nosuid,nodev,noexec,relatime)\r\nudev on /dev type devtmpfs (rw,relatime,size=10240k,nr_inodes=8258008,mode=755)\r\ndevpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000)\r\ntmpfs on /run type tmpfs (rw,nosuid,relatime,size=13216864k,mode=755)\r\nrpool/ROOT/debian-8 on / type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/bck on /bck type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/home on /home type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/home/jails on /home/jails type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/root on /root type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/srv on /srv type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/tmp on /tmp type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/usr on /usr type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/usr/share on /usr/share type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var on /var type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/backups on /var/backups type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/cache on /var/cache type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/lib on /var/lib type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/lib/nfs on /var/lib/nfs type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/log on /var/log type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/mail on /var/mail type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/pgdump on /var/pgdump type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/spool on /var/spool type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/tmp on /var/tmp type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/virusmails on /var/virusmails type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/vmail on /var/vmail type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/www on /var/www type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/www/example.com on /var/www/example.com type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/www/example.com/2.x_pub_static on /var/www/example.com/magento/2.x/2.x/pub/static type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/www/example.com/2.x_var on /var/www/example.com/magento/2.x/2.x/var type zfs (rw,relatime,xattr,noacl)\r\nsecurityfs on /sys/kernel/security type securityfs (rw,nosuid,nodev,noexec,relatime)\r\ntmpfs on /dev/shm type tmpfs (rw,nosuid,nodev)\r\ntmpfs on /run/lock type tmpfs (rw,nosuid,nodev,noexec,relatime,size=5120k)\r\ntmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)\r\ncgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd)\r\npstore on /sys/fs/pstore type pstore (rw,nosuid,nodev,noexec,relatime)\r\ncgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)\r\ncgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)\r\ncgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)\r\ncgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)\r\ncgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)\r\ncgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)\r\ncgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)\r\nsystemd-1 on /proc/sys/fs/binfmt_misc type autofs (rw,relatime,fd=23,pgrp=1,timeout=300,minproto=5,maxproto=5,direct)\r\nmqueue on /dev/mqueue type mqueue (rw,relatime)\r\nhugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime)\r\ndebugfs on /sys/kernel/debug type debugfs (rw,relatime)\r\ntmpfs on /var/lib/varnish type tmpfs (rw,nosuid,noatime,nodiratime,size=512000k)\r\ntmpfs on /var/cache/nginx/ngx_pagespeed/vws type tmpfs (rw,relatime,size=1048576k)\r\ntmpfs on /var/cache/nginx/fastcgi/vws type tmpfs (rw,relatime,size=1048576k)\r\ntmpfs on /var/cache/nginx/proxy/vws type tmpfs (rw,relatime,size=2097152k)\r\n/dev/md0 on /boot type ext4 (rw,noatime,data=ordered)\r\n/dev/sde1 on /boot/efi type vfat (rw,relatime,fmask=0022,dmask=0022,codepage=437,iocharset=utf8,shortname=mixed,errors=remount-ro)\r\ntmpfs on /run/user/0 type tmpfs (rw,nosuid,nodev,relatime,size=6608432k,mode=700)\r\n\r\n```\r\n```\r\nroot@skynet:~# lsblk\r\nNAME    MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT\r\nsda       8:0    0   204G  0 disk\r\n\u251c\u2500sda1    8:1    0   204G  0 part\r\n\u2514\u2500sda9    8:9    0     8M  0 part\r\nsdb       8:16   0   204G  0 disk\r\n\u251c\u2500sdb1    8:17   0   204G  0 part\r\n\u2514\u2500sdb9    8:25   0     8M  0 part\r\nsdc       8:32   0   204G  0 disk\r\n\u251c\u2500sdc1    8:33   0   204G  0 part\r\n\u2514\u2500sdc9    8:41   0     8M  0 part\r\nsdd       8:48   0   204G  0 disk\r\n\u251c\u2500sdd1    8:49   0   204G  0 part\r\n\u2514\u2500sdd9    8:57   0     8M  0 part\r\nsde       8:64   0   1.8T  0 disk\r\n\u251c\u2500sde1    8:65   0   487M  0 part  /boot/efi\r\n\u251c\u2500sde2    8:66   0   4.7G  0 part\r\n\u2502 \u2514\u2500md0   9:0    0   4.7G  0 raid1 /boot\r\n\u251c\u2500sde3    8:67   0   3.7G  0 part\r\n\u2502 \u2514\u2500md1   9:1    0   3.7G  0 raid1\r\n\u251c\u2500sde4    8:68   0   4.7G  0 part\r\n\u2502 \u2514\u2500md2   9:2    0   4.7G  0 raid1\r\n\u251c\u2500sde5    8:69   0   3.7G  0 part\r\n\u2502 \u2514\u2500md3   9:3    0   3.7G  0 raid1\r\n\u251c\u2500sde6    8:70   0 186.3G  0 part\r\n\u2502 \u2514\u2500md4   9:4    0 186.1G  0 raid1\r\n\u2514\u2500sde7    8:71   0   1.6T  0 part\r\nsdf       8:80   0   1.8T  0 disk\r\n\u251c\u2500sdf1    8:81   0   487M  0 part\r\n\u251c\u2500sdf2    8:82   0   4.7G  0 part\r\n\u2502 \u2514\u2500md0   9:0    0   4.7G  0 raid1 /boot\r\n\u251c\u2500sdf3    8:83   0   3.7G  0 part\r\n\u2502 \u2514\u2500md1   9:1    0   3.7G  0 raid1\r\n\u251c\u2500sdf4    8:84   0   4.7G  0 part\r\n\u2502 \u2514\u2500md2   9:2    0   4.7G  0 raid1\r\n\u251c\u2500sdf5    8:85   0   3.7G  0 part\r\n\u2502 \u2514\u2500md3   9:3    0   3.7G  0 raid1\r\n\u251c\u2500sdf6    8:86   0 186.3G  0 part\r\n\u2502 \u2514\u2500md4   9:4    0 186.1G  0 raid1\r\n\u2514\u2500sdf7    8:87   0   1.6T  0 part\r\nsdg       8:96   1  28.8G  0 disk\r\n\u251c\u2500sdg1    8:97   1   487M  0 part\r\n\u2514\u2500sdg2    8:98   1   4.7G  0 part\r\nzd0     230:0    0     4G  0 disk  [SWAP]\r\n\r\n```\r\n\r\n### Describe the problem you're observing\r\n\r\nDebian Jessie Native ZFS fails to load sub filesystems under `/var` when `rpool/ROOT/debian-8/var` is set to `canmount=off`.\r\n\r\n![2017-06-19 14_22_45-2017-06-18 21_26_12-java ikvm viewer v1 69 21 00 00 00 00 - resolution 1024](https://user-images.githubusercontent.com/10731034/27306586-fb3413f2-54fa-11e7-92f7-c85d0b184699.png)\r\n\r\n\r\n### Describe how to reproduce the problem\r\nFollow zfs wiki to install with the exception of putting `/boot` an `/boot/efi` partitions on a different disk.\r\n`https://github.com/zfsonlinux/zfs/wiki/Debian-Jessie-Root-on-ZFS`\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n\r\nThanks!", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6251/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6250", "title": "[0.6.5.9-5] Debian Jessie Native ZFS (except for /boot /boot/efi) Hangs on Reboot or Shutdown", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n\r\n```root@skynet:/proc# uname -a\r\nLinux skynet 3.16.0-4-amd64 #1 SMP Debian 3.16.43-2+deb8u1 (2017-06-18) x86_64 GNU/Linux\r\n```\r\n```\r\nroot@skynet:/# dpkg -l | grep zfs\r\nii  libzfs2linux                         0.6.5.9-5                            amd64        OpenZFS filesystem library for Linux\r\nii  zfs-dkms                             0.6.5.9-5                            all          OpenZFS filesystem kernel modules for Linux\r\nii  zfs-initramfs                        0.6.5.9-5                            all          OpenZFS root filesystem capabilities for Linux - initramfs\r\nii  zfs-zed                              0.6.5.9-5                            amd64        OpenZFS Event Daemon\r\nii  zfsutils-linux                       0.6.5.9-5                            amd64        command-line tools to manage OpenZFS filesystems\r\n```\r\n```\r\nroot@skynet:/# dpkg -l | grep spl\r\nii  spl                                  0.6.5.9-1                            amd64        Solaris Porting Layer user-space utilities for Linux\r\nii  spl-dkms                             0.6.5.9-1                            all          Solaris Porting Layer kernel modules for Linux\r\n```\r\n```\r\nroot@skynet:/boot# cat /etc/*-release\r\nPRETTY_NAME=\"Debian GNU/Linux 8 (jessie)\"\r\nNAME=\"Debian GNU/Linux\"\r\nVERSION_ID=\"8\"\r\nVERSION=\"8 (jessie)\"\r\nID=debian\r\n```\r\n```\r\nroot@skynet:/boot# modinfo zfs | grep -iw version\r\nversion:        0.6.5.9-5\r\nroot@skynet:/boot# modinfo spl | grep -iw version\r\nversion:        0.6.5.9-1\r\n\r\n```\r\n\r\n\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n```\r\nroot@skynet:/boot# zpool status\r\n  pool: rpool\r\n state: ONLINE\r\n  scan: none requested\r\nconfig:\r\n\r\n        NAME                                                STATE     READ WRITE CKSUM\r\n        rpool                                               ONLINE       0     0     0\r\n          mirror-0                                          ONLINE       0     0     0\r\n            ata-Samsung_SSD_840_PRO_Series_S1ATNSAF520000X  ONLINE       0     0     0\r\n            ata-Samsung_SSD_840_PRO_Series_S1ATNSAF520000D  ONLINE       0     0     0\r\n          mirror-1                                          ONLINE       0     0     0\r\n            ata-Samsung_SSD_840_PRO_Series_S1ATNSAF520000E  ONLINE       0     0     0\r\n            ata-Samsung_SSD_840_PRO_Series_S1ATNSAF520000L  ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n```\r\n\r\n```\r\nroot@skynet:/# mount\r\nsysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime)\r\nproc on /proc type proc (rw,nosuid,nodev,noexec,relatime)\r\nudev on /dev type devtmpfs (rw,relatime,size=10240k,nr_inodes=8258008,mode=755)\r\ndevpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000)\r\ntmpfs on /run type tmpfs (rw,nosuid,relatime,size=13216864k,mode=755)\r\nrpool/ROOT/debian-8 on / type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/bck on /bck type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/home on /home type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/home/jails on /home/jails type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/root on /root type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/srv on /srv type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/tmp on /tmp type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/usr on /usr type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/usr/share on /usr/share type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var on /var type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/backups on /var/backups type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/cache on /var/cache type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/lib on /var/lib type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/lib/nfs on /var/lib/nfs type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/log on /var/log type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/mail on /var/mail type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/pgdump on /var/pgdump type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/spool on /var/spool type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/tmp on /var/tmp type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/virusmails on /var/virusmails type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/vmail on /var/vmail type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/www on /var/www type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/www/example.com on /var/www/example.com type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/www/example.com/2.x_pub_static on /var/www/example.com/magento/2.x/2.x/pub/static type zfs (rw,relatime,xattr,noacl)\r\nrpool/ROOT/debian-8/var/www/example.com/2.x_var on /var/www/example.com/magento/2.x/2.x/var type zfs (rw,relatime,xattr,noacl)\r\nsecurityfs on /sys/kernel/security type securityfs (rw,nosuid,nodev,noexec,relatime)\r\ntmpfs on /dev/shm type tmpfs (rw,nosuid,nodev)\r\ntmpfs on /run/lock type tmpfs (rw,nosuid,nodev,noexec,relatime,size=5120k)\r\ntmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)\r\ncgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd)\r\npstore on /sys/fs/pstore type pstore (rw,nosuid,nodev,noexec,relatime)\r\ncgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)\r\ncgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)\r\ncgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)\r\ncgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)\r\ncgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)\r\ncgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)\r\ncgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)\r\nsystemd-1 on /proc/sys/fs/binfmt_misc type autofs (rw,relatime,fd=23,pgrp=1,timeout=300,minproto=5,maxproto=5,direct)\r\nmqueue on /dev/mqueue type mqueue (rw,relatime)\r\nhugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime)\r\ndebugfs on /sys/kernel/debug type debugfs (rw,relatime)\r\ntmpfs on /var/lib/varnish type tmpfs (rw,nosuid,noatime,nodiratime,size=512000k)\r\ntmpfs on /var/cache/nginx/ngx_pagespeed/vws type tmpfs (rw,relatime,size=1048576k)\r\ntmpfs on /var/cache/nginx/fastcgi/vws type tmpfs (rw,relatime,size=1048576k)\r\ntmpfs on /var/cache/nginx/proxy/vws type tmpfs (rw,relatime,size=2097152k)\r\n/dev/md0 on /boot type ext4 (rw,noatime,data=ordered)\r\n/dev/sde1 on /boot/efi type vfat (rw,relatime,fmask=0022,dmask=0022,codepage=437,iocharset=utf8,shortname=mixed,errors=remount-ro)\r\ntmpfs on /run/user/0 type tmpfs (rw,nosuid,nodev,relatime,size=6608432k,mode=700)\r\n\r\n```\r\n```\r\nroot@skynet:~# lsblk\r\nNAME    MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT\r\nsda       8:0    0   204G  0 disk\r\n\u251c\u2500sda1    8:1    0   204G  0 part\r\n\u2514\u2500sda9    8:9    0     8M  0 part\r\nsdb       8:16   0   204G  0 disk\r\n\u251c\u2500sdb1    8:17   0   204G  0 part\r\n\u2514\u2500sdb9    8:25   0     8M  0 part\r\nsdc       8:32   0   204G  0 disk\r\n\u251c\u2500sdc1    8:33   0   204G  0 part\r\n\u2514\u2500sdc9    8:41   0     8M  0 part\r\nsdd       8:48   0   204G  0 disk\r\n\u251c\u2500sdd1    8:49   0   204G  0 part\r\n\u2514\u2500sdd9    8:57   0     8M  0 part\r\nsde       8:64   0   1.8T  0 disk\r\n\u251c\u2500sde1    8:65   0   487M  0 part  /boot/efi\r\n\u251c\u2500sde2    8:66   0   4.7G  0 part\r\n\u2502 \u2514\u2500md0   9:0    0   4.7G  0 raid1 /boot\r\n\u251c\u2500sde3    8:67   0   3.7G  0 part\r\n\u2502 \u2514\u2500md1   9:1    0   3.7G  0 raid1\r\n\u251c\u2500sde4    8:68   0   4.7G  0 part\r\n\u2502 \u2514\u2500md2   9:2    0   4.7G  0 raid1\r\n\u251c\u2500sde5    8:69   0   3.7G  0 part\r\n\u2502 \u2514\u2500md3   9:3    0   3.7G  0 raid1\r\n\u251c\u2500sde6    8:70   0 186.3G  0 part\r\n\u2502 \u2514\u2500md4   9:4    0 186.1G  0 raid1\r\n\u2514\u2500sde7    8:71   0   1.6T  0 part\r\nsdf       8:80   0   1.8T  0 disk\r\n\u251c\u2500sdf1    8:81   0   487M  0 part\r\n\u251c\u2500sdf2    8:82   0   4.7G  0 part\r\n\u2502 \u2514\u2500md0   9:0    0   4.7G  0 raid1 /boot\r\n\u251c\u2500sdf3    8:83   0   3.7G  0 part\r\n\u2502 \u2514\u2500md1   9:1    0   3.7G  0 raid1\r\n\u251c\u2500sdf4    8:84   0   4.7G  0 part\r\n\u2502 \u2514\u2500md2   9:2    0   4.7G  0 raid1\r\n\u251c\u2500sdf5    8:85   0   3.7G  0 part\r\n\u2502 \u2514\u2500md3   9:3    0   3.7G  0 raid1\r\n\u251c\u2500sdf6    8:86   0 186.3G  0 part\r\n\u2502 \u2514\u2500md4   9:4    0 186.1G  0 raid1\r\n\u2514\u2500sdf7    8:87   0   1.6T  0 part\r\nsdg       8:96   1  28.8G  0 disk\r\n\u251c\u2500sdg1    8:97   1   487M  0 part\r\n\u2514\u2500sdg2    8:98   1   4.7G  0 part\r\nzd0     230:0    0     4G  0 disk  [SWAP]\r\n```\r\n\r\n### Describe the problem you're observing\r\n\r\n`shutdown -h now` `shutdown -r now` or `reboot` are not success full and the system hangs when, using native zfs on Jessie (except for `/boot`, `/boot/efi`). Shutdown does not go past `All DM devices detached.`\r\n\r\n![2017-06-19 13_55_59-2017-06-14 15_30_00-java ikvm viewer v1 69 21 00 00 00 00 - resolution 1024](https://user-images.githubusercontent.com/10731034/27305666-6b3d71f6-54f7-11e7-9b80-5b98ddad5763.png)\r\n\r\n\r\nAlso, I am not sure why `/var` does not unmount. See picture above.\r\n\r\nNOTE: If unmount `/boot/efi` and `/boot` before rebooting, the system reboots normally\r\n\r\n\r\n### Describe how to reproduce the problem\r\nFollow zfs wiki to install with the exception of putting `/boot` an `/boot/efi` partitions on a different disk.\r\n`https://github.com/zfsonlinux/zfs/wiki/Debian-Jessie-Root-on-ZFS`\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n\r\nThanks!", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6250/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mrqwer88": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6247", "title": "Feature request: add support zstd compression.", "body": "Hello!\r\n\r\n[zstd](https://github.com/facebook/zstd) is more faster then gzip with same compression(without overhead for used resourses). It be good if zfs will have support zstd for compression dataset like gzip, lz4 and etc.\r\n\r\nBenchmark from zstd -\r\n\r\n| Compressor name         | Ratio | Compression| Decompress.|\r\n| ---------------         | ------| -----------| ---------- |\r\n| **zstd 1.1.3 -1**       | 2.877 |   430 MB/s |  1110 MB/s |\r\n| zlib 1.2.8 -1           | 2.743 |   110 MB/s |   400 MB/s |\r\n| brotli 0.5.2 -0         | 2.708 |   400 MB/s |   430 MB/s |\r\n| quicklz 1.5.0 -1        | 2.238 |   550 MB/s |   710 MB/s |\r\n| lzo1x 2.09 -1           | 2.108 |   650 MB/s |   830 MB/s |\r\n| lz4 1.7.5               | 2.101 |   720 MB/s |  3600 MB/s |\r\n| snappy 1.1.3            | 2.091 |   500 MB/s |  1650 MB/s |\r\n| lzf 3.6 -1              | 2.077 |   400 MB/s |   860 MB/s |\r\n\r\nThanks in advance!", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6247/reactions", "total_count": 5, "+1": 5, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rschlaikjer": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6237", "title": "Stat of directory causes segfault, kernel BUG at fs/namei.c:1199", "body": "### System information\r\nType                                | Version/Name\r\n----- | -----\r\nDistribution Name       |  Debian \r\nDistribution Version    |  Jessie/Testing\r\nLinux Kernel                 | 4.10.12\r\nArchitecture                 |  amd64\r\nZFS Version                  |  0.6.5.9-5\r\nSPL Version                  | 0.6.5.9-1\r\n\r\n### Describe the problem you're observing\r\nI have a build output directory that has become un`stat`able - attempting to remove the directory or inspect it with most programs causes the following userspace error:\r\n```\r\nross@stirrup:/h/r/j/c/s/out$ strace stat Production/gen/chrome/app/policy/android/values-v21 2>&1 | tail -n 10\r\nread(3, \"\", 4096)                       = 0\r\nclose(3)                                = 0\r\nopen(\"/usr/share/locale/en_US.UTF-8/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\r\nopen(\"/usr/share/locale/en_US.utf8/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\r\nopen(\"/usr/share/locale/en_US/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\r\nopen(\"/usr/share/locale/en.UTF-8/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\r\nopen(\"/usr/share/locale/en.utf8/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\r\nopen(\"/usr/share/locale/en/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\r\nlstat(\"Production/gen/chrome/app/policy/android/values-v21\",  <unfinished ...>) = ?\r\n+++ killed by SIGSEGV +++\r\n```\r\n\r\nThis causes a stacktrace to be emitted to syslog:\r\n```\r\nJun 16 14:15:03 stirrup kernel: [688028.854162] ------------[ cut here ]------------\r\nJun 16 14:15:03 stirrup kernel: [688028.855562] kernel BUG at fs/namei.c:1199!\r\nJun 16 14:15:03 stirrup kernel: [688028.857379] invalid opcode: 0000 [#41] SMP\r\nJun 16 14:15:03 stirrup kernel: [688028.859192] Modules linked in: nls_utf8(E) nls_cp437(E) vfat(E) fat(E) uas(E) usb_storage(E) bnep(E) ipt_MASQUERADE(E) nf_nat_masquerade_ipv4(E) xfrm_user(E) xfrm_algo(E) iptable_nat(E) nf_conntrack_ipv4(E) nf_defrag_ipv4(E) nf_nat_ipv4(E) xt_addrtype(E) iptable_filter(E) xt_conntrack(E) nf_nat(E) nf_conntrack(E) libcrc32c(E) crc32c_generic(E) br_netfilter(E) bridge(E) stp(E) llc(E) overlay(E) fuse(E) hid_generic(E) intel_rapl(E) usbhid(E) x86_pkg_temp_thermal(E) intel_powerclamp(E) kvm_intel(E) kvm(E) irqbypass(E) crct10dif_pclmul(E) crc32_pclmul(E) ghash_clmulni_intel(E) pcbc(E) zfs(POE) snd_hda_codec_realtek(E) nouveau(E) snd_hda_codec_generic(E) zunicode(POE) ttm(E) zavl(POE) zcommon(POE) snd_hda_intel(E) drm_kms_helper(E) znvpair(POE) eeepc_wmi(E) aesni_intel(E) snd_hda_codec(E) drm(E) asus_wmi(E)\r\nJun 16 14:15:03 stirrup kernel: [688028.865126]  aes_x86_64(E) spl(OE) sparse_keymap(E) mxm_wmi(E) evdev(E) i2c_algo_bit(E) crypto_simd(E) snd_hda_core(E) glue_helper(E) snd_hwdep(E) cryptd(E) snd_pcm(E) snd_timer(E) iTCO_wdt(E) snd(E) iTCO_vendor_support(E) soundcore(E) sg(E) mei_me(E) shpchp(E) mei(E) serio_raw(E) pcspkr(E) hci_uart(E) btbcm(E) btqca(E) btintel(E) bluetooth(E) battery(E) rfkill(E) wmi(E) video(E) intel_lpss_acpi(E) intel_lpss(E) mfd_core(E) acpi_als(E) tpm_tis(E) kfifo_buf(E) tpm_tis_core(E) tpm(E) industrialio(E) button(E) acpi_pad(E) sunrpc(E) coretemp(E) adt7475(E) hwmon_vid(E) ip_tables(E) x_tables(E) autofs4(E) ext4(E) crc16(E) jbd2(E) mbcache(E) sd_mod(E) crc32c_intel(E) psmouse(E) ahci(E) libahci(E) i2c_i801(E) libata(E) r8169(E) xhci_pci(E) mii(E) xhci_hcd(E) scsi_mod(E) usbcore(E) fan(E) thermal(E) i2c_hid(E)\r\nJun 16 14:15:03 stirrup kernel: [688028.874815]  hid(E) fjes(E)\r\nJun 16 14:15:03 stirrup kernel: [688028.874818] CPU: 0 PID: 16538 Comm: stat Tainted: P      D W  OE   4.10.12-stirrup #1\r\nJun 16 14:15:03 stirrup kernel: [688028.874818] Hardware name: System manufacturer System Product Name/Z170-P, BIOS 0601 11/16/2015\r\nJun 16 14:15:03 stirrup kernel: [688028.874819] task: ffff9a09d972a080 task.stack: ffffa53d2f800000\r\nJun 16 14:15:03 stirrup kernel: [688028.874821] RIP: 0010:follow_managed+0x2e0/0x310\r\nJun 16 14:15:03 stirrup kernel: [688028.874821] RSP: 0018:ffffa53d2f803c30 EFLAGS: 00010246\r\nJun 16 14:15:03 stirrup kernel: [688028.874822] RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000000000010\r\nJun 16 14:15:03 stirrup kernel: [688028.874822] RDX: ffff9a1030a12520 RSI: ffffa53d2f803d50 RDI: ffffa53d2f803cd8\r\nJun 16 14:15:03 stirrup kernel: [688028.874823] RBP: ffffa53d2f803cd8 R08: ffff9a0f40000000 R09: ffff9a05204ab045\r\nJun 16 14:15:03 stirrup kernel: [688028.874823] R10: ffff9a0f40000038 R11: 00000000ce645327 R12: 0000000003255777\r\nJun 16 14:15:03 stirrup kernel: [688028.874823] R13: ffff9a0f40000000 R14: ffffa53d2f803d50 R15: 0000000000000000\r\nJun 16 14:15:03 stirrup kernel: [688028.874824] FS:  00007feb25c9d3c0(0000) GS:ffff9a107e400000(0000) knlGS:0000000000000000\r\nJun 16 14:15:03 stirrup kernel: [688028.874824] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nJun 16 14:15:03 stirrup kernel: [688028.874825] CR2: 0000556c271ba108 CR3: 0000000e0bb5f000 CR4: 00000000003406f0\r\nJun 16 14:15:03 stirrup kernel: [688028.874825] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\r\nJun 16 14:15:03 stirrup kernel: [688028.874825] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\r\nJun 16 14:15:03 stirrup kernel: [688028.874826] Call Trace:\r\nJun 16 14:15:03 stirrup kernel: [688028.874827]  ? lookup_fast+0x1da/0x300\r\nJun 16 14:15:03 stirrup kernel: [688028.874828]  ? walk_component+0x48/0x450\r\nJun 16 14:15:03 stirrup kernel: [688028.874829]  ? path_lookupat+0x52/0x110\r\nJun 16 14:15:03 stirrup kernel: [688028.874829]  ? filename_lookup+0xb1/0x180\r\nJun 16 14:15:03 stirrup kernel: [688028.874831]  ? schedule+0x32/0x80\r\nJun 16 14:15:03 stirrup kernel: [688028.874833]  ? kmem_cache_alloc+0xf6/0x200\r\nJun 16 14:15:03 stirrup kernel: [688028.874834]  ? getname_flags+0x6f/0x1e0\r\nJun 16 14:15:03 stirrup kernel: [688028.874835]  ? vfs_fstatat+0x59/0xb0\r\nJun 16 14:15:03 stirrup kernel: [688028.874836]  ? SYSC_newlstat+0x2d/0x60\r\nJun 16 14:15:03 stirrup kernel: [688028.874837]  ? syscall_trace_enter+0x8c/0x2f0\r\nJun 16 14:15:03 stirrup kernel: [688028.874838]  ? do_sys_open+0x193/0x210\r\nJun 16 14:15:03 stirrup kernel: [688028.874839]  ? do_syscall_64+0x5c/0x170\r\nJun 16 14:15:03 stirrup kernel: [688028.874840]  ? entry_SYSCALL64_slow_path+0x25/0x25\r\nJun 16 14:15:03 stirrup kernel: [688028.874840] Code: e8 36 58 01 00 48 8b 14 24 e9 d3 fe ff ff 48 8b 7d 00 e8 b4 73 01 00 48 8b 14 24 e9 76 ff ff ff 48 83 f8 eb 74 12 44 89 e3 eb 9e <0f> 0b 4c 8b 6d 08 31 db e9 38 fd ff ff 41 f6 46 38 10 74 e7 45 \r\nJun 16 14:15:03 stirrup kernel: [688028.874849] RIP: follow_managed+0x2e0/0x310 RSP: ffffa53d2f803c30\r\nJun 16 14:15:03 stirrup kernel: [688028.874878] ---[ end trace e984190633c92eaa ]---\r\n```\r\n\r\nzdb output for the directory:\r\n```\r\nross@stirrup:/h/ross$ sudo zdb -dddd tank/chromium/out 76606\r\nDataset tank/chromium/out [ZPL], ID 156, cr_txg 1395, 3.89G, 59458 objects, rootbp DVA[0]=<0:2353b50600:200> DVA[1]=<0:c01389800:200> [L0 DMU objset] fletcher4 lz4 LE contiguous unique double size=800L/200P birth=391202L/391202P fill=59458 cksum=13b51e2c97:6b776dcec2a:1341f7cc8a49e:268342585fcfff\r\n\r\n    Object  lvl   iblk   dblk  dsize  lsize   %full  type\r\n     76606    1    16K    512      0    512  100.00  ZFS directory\r\n                                        168   bonus  System attributes\r\n\tdnode flags: USED_BYTES USERUSED_ACCOUNTED \r\n\tdnode maxblkid: 0\r\n\tpath\t/Production/gen/chrome/app/policy/android/values-v21\r\n\tuid     1000\r\n\tgid     1000\r\n\tatime\tThu Jun  1 14:06:30 2017\r\n\tmtime\tThu Jun  1 14:07:19 2017\r\n\tctime\tThu Jun  1 14:07:19 2017\r\n\tcrtime\tThu Jun  1 14:06:30 2017\r\n\tgen\t137660\r\n\tmode\t40755\r\n\tsize\t3\r\n\tparent\t76605\r\n\tlinks\t2\r\n\tpflags\t40800000144\r\n\tmicrozap: 512 bytes, 1 entries\r\n\r\n\t\trestriction_values.xml = 78223 (type: Regular File)\r\n```\r\n\r\nThe BUG at `fs/namei.c:1199` is this assert here:\r\nhttp://elixir.free-electrons.com/linux/v4.10.12/source/fs/namei.c#L1199\r\n\r\nUnfortunately I do not know of a way to reproduce this issue.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6237/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tokiwinter": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6230", "title": "High load, processes in uninterruptable sleep state, ZFS-backed LXD", "body": "## System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 16.04.2 LTS\r\nLinux Kernel                 | 4.4.0-78-generic\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.6-0ubuntu16 and 0.6.5.9-1\r\nSPL Version                  | 0.6.5.6-0 and 0.6.5.9-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nAfter around 36-48 hrs continuous operation, we are seeing very high system load (most usage in sys). Nil ZFS tuning has been performed. This is ZFS-backed LXD, with 41 running containers.\r\n\r\nHappy to provide further information at your request, although the system may not be responsive for much longer. I suspect that for our workload some considerable tuning needs to occur, but require guidance on where we should start.\r\n\r\nThis is reproducable using the packages in the xenial repos, as well as building debs from the latest zfs/spl release.\r\n\r\nThis is a c4.4xlarge ec2 instance (30GB RAM, 16 vCPU).\r\n\r\nThanks\r\n```\r\nroot@node-1:~# top\r\ntop - 07:41:25 up 1 day, 23:52,  1 user,  load average: 253.31, 269.41, 256.79\r\nTasks: 2474 total,  11 running, 2446 sleeping,   0 stopped,  17 zombie\r\n%Cpu(s): 27.9 us, 70.0 sy,  0.0 ni,  0.0 id,  2.0 wa,  0.0 hi,  0.1 si,  0.0 st\r\nKiB Mem : 30865928 total,  1322136 free, 16159704 used, 13384088 buff/cache\r\nKiB Swap:        0 total,        0 free,        0 used.  2454908 avail Mem\r\n\r\n   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\r\n 27678 100000    20   0 2384496 655028   7552 S  58.4  2.1   9445:48 jujud\r\n 81617 root      20   0  240308 108540      0 S  50.3  0.4 237:58.58 kube-apiserver\r\n   959 root      20   0       0      0      0 R  49.0  0.0 326:19.32 arc_reclaim\r\n 22476 root      20   0   95648   5276      0 S  45.8  0.0  35:08.24 kube-apiserver\r\n 25289 root      20   0 1969728  53940   1016 S  41.6  0.2 197:46.95 kubelet\r\n   120 root      20   0       0      0      0 S  41.2  0.0 203:10.15 kswapd0\r\n  3810 root      20   0  250704 107844      0 S  40.3  0.3 224:02.54 kube-apiserver\r\n  5491 root      20   0 1799492  54972      0 S  36.0  0.2 229:32.35 kubelet\r\n 39048 root      20   0  236464  90268      0 S  35.7  0.3 197:58.90 kube-apiserver\r\n 20521 root      20   0  246520 128232      0 S  31.2  0.4 201:59.71 kube-apiserver\r\n 66705 root      20   0  215188 101368      0 R  30.5  0.3 286:38.13 kube-apiserver\r\n 88496 root      20   0  394720  20884  16104 S  30.5  0.1   0:00.94 juju-log\r\n120891 root      20   0   96416   5212      0 R  30.2  0.0  28:35.34 kube-apiserver\r\n 67602 root      20   0 1759376  46408      0 S  29.5  0.2 300:32.30 kubelet\r\n 67549 root      20   0   56900  14156   1296 S  27.6  0.0  31:30.73 kube-proxy\r\n  3069 root      20   0 3404720   6508   1280 S  25.3  0.0 146:24.38 lxcfs\r\n 88499 root      20   0  232592  10884   9508 D  23.7  0.0   0:00.73 relation-set\r\n 88547 root      20   0  250040  12444  10508 S  22.7  0.0   0:00.70 relation-set\r\n 88417 root      20   0   75620   6920   6388 R  22.4  0.0   0:00.69 relation-set\r\n 25862 root      20   0 1780412  50228    728 S  21.1  0.2 147:57.40 kubelet\r\n 83933 root      20   0 1724704  54740   1924 S  20.5  0.2 195:43.08 kubelet\r\n 88539 root      20   0  319932  19168  15212 R  19.5  0.1   0:00.60 relation-set\r\n 13747 root      20   0  577564   5024      0 S  17.2  0.0   7:47.55 kubelet\r\n 41003 root      20   0   19864   1384   1052 D  16.6  0.0  20:28.45 squashfuse\r\n 82763 root      20   0   51344  12716   5920 D  16.6  0.0   0:03.90 kubectl\r\n 31717 root      20   0 1477924  47568      0 S  16.2  0.2 213:46.48 kubelet\r\n 57841 root      20   0   96704   6528      0 R  15.9  0.0  61:25.80 kube-apiserver\r\n 85322 root      20   0   19880   1524   1176 D  15.9  0.0   6:30.97 squashfuse\r\n  9718 root      20   0   55076  13172   1500 S  15.6  0.0  21:49.62 kube-proxy\r\n 17712 root      20   0   19880   1636   1284 D  15.6  0.0  18:23.11 squashfuse\r\n 88493 root      20   0  235056  12972  10712 D  15.6  0.0   0:00.48 relation-set\r\n 25282 root      20   0   54784  12204    500 S  15.3  0.0  21:11.25 kube-proxy\r\n 88557 root      20   0  259644  12424  10496 R  15.3  0.0   0:00.47 config-get\r\n 29678 root      20   0   19884   1424   1068 D  14.9  0.0  20:24.76 squashfuse\r\n 48616 root      20   0   96960   6304      0 S  14.3  0.0  51:00.09 kube-apiserver\r\n 65530 root      20   0   20200   1684   1168 D  14.3  0.0  17:20.34 squashfuse\r\n 88590 root      20   0  158860  11048   9640 D  14.0  0.0   0:00.43 juju-log \r\n```\r\nFurther info:\r\n```\r\nroot@node-1:~# uptime\r\n 07:49:05 up 1 day, 23:59,  1 user,  load average: 231.48, 252.61, 255.19\r\nroot@node-1:~# free -m\r\n              total        used        free      shared  buff/cache   available\r\nMem:          30142       16085        1112        9421       12944        2075\r\nSwap:             0           0           0\r\nroot@node-1:~# vmstat 1 5\r\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\r\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\r\n69 87      0 1407088   5688 13142436    0    0  2316     1   12    0 29 24 43  4  0\r\n63 94      0 1170964   5688 13179624    0    0 445242     0 63838 1540814 23 72  0  4  0\r\n59 114      0 1197220   5688 13203916    0    0 310349     0 46968 1362719 22 74  0  3  0\r\n71 102      0 1185708   5688 13227240    0    0 248368     0 40640 974430 22 76  0  2  0\r\n62 91      0 1219612   5688 13252276    0    0 230607     0 39560 546441 23 75  0  2  0\r\nroot@node-1:~# arcstat.py 1 5\r\n    time  read  miss  miss%  dmis  dm%  pmis  pm%  mmis  mm%  arcsz     c\r\n07:46:58     3     3    100     3  100     0    0     0    0   181M   32M\r\n07:46:59  9.7K  6.0K     61  5.2K   58   788   96  1.4K   39   246M   32M\r\n07:47:00   12K  6.8K     52  5.7K   49  1.1K   86  1.4K   32   241M   32M\r\n07:47:01  9.0K  5.2K     57  4.2K   53   938   82  1.3K   36   234M   32M\r\n07:47:02  7.1K  4.2K     59  3.7K   58   555   71  1.4K   45   237M   32M\r\nroot@node-1:~# cat /proc/spl/kstat/zfs/arcstats\r\n6 1 0x01 91 4368 4709499310 172686102123891\r\nname                            type data\r\nhits                            4    232325291\r\nmisses                          4    151884718\r\ndemand_data_hits                4    111534579\r\ndemand_data_misses              4    76533037\r\ndemand_metadata_hits            4    84693482\r\ndemand_metadata_misses          4    45899616\r\nprefetch_data_hits              4    1976296\r\nprefetch_data_misses            4    10151144\r\nprefetch_metadata_hits          4    34120934\r\nprefetch_metadata_misses        4    19300921\r\nmru_hits                        4    45754394\r\nmru_ghost_hits                  4    4241793\r\nmfu_hits                        4    150602999\r\nmfu_ghost_hits                  4    35488439\r\ndeleted                         4    103194103\r\nmutex_miss                      4    17053838\r\nevict_skip                      4    69750963751\r\nevict_not_enough                4    433533004\r\nevict_l2_cached                 4    0\r\nevict_l2_eligible               4    9683688122880\r\nevict_l2_ineligible             4    1123216037888\r\nevict_l2_skip                   4    0\r\nhash_elements                   4    8296\r\nhash_elements_max               4    318036\r\nhash_collisions                 4    924112\r\nhash_chains                     4    7\r\nhash_chain_max                  4    4\r\np                               4    30822912\r\nc                               4    33554432\r\nc_min                           4    33554432\r\nc_max                           4    15803355136\r\nsize                            4    286715056\r\nhdr_size                        4    3231144\r\ndata_size                       4    110027264\r\nmetadata_size                   4    86627328\r\nother_size                      4    86829320\r\nanon_size                       4    20538368\r\nanon_evictable_data             4    0\r\nanon_evictable_metadata         4    0\r\nmru_size                        4    172940288\r\nmru_evictable_data              4    88866816\r\nmru_evictable_metadata          4    3522560\r\nmru_ghost_size                  4    131072\r\nmru_ghost_evictable_data        4    0\r\nmru_ghost_evictable_metadata    4    0\r\nmfu_size                        4    3175936\r\nmfu_evictable_data              4    0\r\nmfu_evictable_metadata          4    1146880\r\nmfu_ghost_size                  4    33350144\r\nmfu_ghost_evictable_data        4    1179648\r\nmfu_ghost_evictable_metadata    4    31985152\r\nl2_hits                         4    0\r\nl2_misses                       4    0\r\nl2_feeds                        4    0\r\nl2_rw_clash                     4    0\r\nl2_read_bytes                   4    0\r\nl2_write_bytes                  4    0\r\nl2_writes_sent                  4    0\r\nl2_writes_done                  4    0\r\nl2_writes_error                 4    0\r\nl2_writes_lock_retry            4    0\r\nl2_evict_lock_retry             4    0\r\nl2_evict_reading                4    0\r\nl2_evict_l1cached               4    0\r\nl2_free_on_write                4    0\r\nl2_cdata_free_on_write          4    0\r\nl2_abort_lowmem                 4    0\r\nl2_cksum_bad                    4    0\r\nl2_io_error                     4    0\r\nl2_size                         4    0\r\nl2_asize                        4    0\r\nl2_hdr_size                     4    0\r\nl2_compress_successes           4    0\r\nl2_compress_zeros               4    0\r\nl2_compress_failures            4    0\r\nmemory_throttle_count           4    0\r\nduplicate_buffers               4    0\r\nduplicate_buffers_size          4    0\r\nduplicate_reads                 4    1221703\r\nmemory_direct_count             4    20189497\r\nmemory_indirect_count           4    2182\r\narc_no_grow                     4    1\r\narc_tempreserve                 4    0\r\narc_loaned_bytes                4    0\r\narc_prune                       4    0\r\narc_meta_used                   4    176687792\r\narc_meta_limit                  4    11852516352\r\narc_meta_max                    4    1631862600\r\narc_meta_min                    4    16777216\r\narc_need_free                   4    0\r\narc_sys_free                    4    493854720\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6230/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zrav": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6224", "title": "Silent corruption in incremental send without -L flag with large block pool", "body": "### System information\r\n\r\n| Type | Version/Name |\r\n| --- | --- |\r\n| Distribution Name | Ubuntu |\r\n| Distribution Version | 17.04 |\r\n| Linux Kernel | 4.10.0-22-generic |\r\n| Architecture | x86_64 |\r\n| ZFS Version | 0.6.5.9 |\r\n| SPL Version | 0.6.5.9 |\r\n\r\n### Describe the problem you're observing\r\n\r\nI have found a data corruption issue in zfs send. In pools using 1M recordsize, incremental sends without the -L flag sometimes silently zero out some files. The results are repeatable. Scrub does not find any errors.\r\n\r\nTested on Ubuntu Xenial with 0.6.5.6 and Zesty with 0.6.5.9 on the same systems.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nSource pool:\r\n```\r\nroot@oddity:~# modinfo zfs\r\nfilename:       /lib/modules/4.10.0-22-generic/kernel/zfs/zfs/zfs.ko\r\nversion:        0.6.5.9-2\r\nlicense:        CDDL\r\nauthor:         OpenZFS on Linux\r\ndescription:    ZFS\r\nsrcversion:     42C4AB70887EA26A9970936\r\ndepends:        spl,znvpair,zcommon,zunicode,zavl\r\nvermagic:       4.10.0-22-generic SMP mod_unload\r\n...\r\nroot@oddity:~# zpool status\r\n  pool: tank\r\n state: ONLINE\r\n  scan: scrub canceled on Sun Jun 11 09:52:38 2017\r\nconfig:\r\n\r\n        NAME                                 STATE     READ WRITE CKSUM\r\n        tank                                 ONLINE       0     0     0\r\n          raidz2-0                           ONLINE       0     0     0\r\n            ata-ST4000VN000-1H4168_XXXXXXXX  ONLINE       0     0     0\r\n            ata-ST4000VN000-1H4168_XXXXXXXX  ONLINE       0     0     0\r\n            ata-ST4000VN000-2AH166_XXXXXXXX  ONLINE       0     0     0\r\n            ata-ST4000VN000-1H4168_XXXXXXXX  ONLINE       0     0     0\r\n            ata-ST4000VN000-1H4168_XXXXXXXX  ONLINE       0     0     0\r\n            ata-ST4000VN000-1H4168_XXXXXXXX  ONLINE       0     0     0\r\n            ata-ST4000VN000-1H4168_XXXXXXXX  ONLINE       0     0     0\r\n            ata-ST4000VN000-1H4168_XXXXXXXX  ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n\r\nroot@oddity:~# zpool get all tank\r\nNAME  PROPERTY                    VALUE                       SOURCE\r\ntank  size                        29T                         -\r\ntank  capacity                    57%                         -\r\ntank  altroot                     -                           default\r\ntank  health                      ONLINE                      -\r\ntank  guid                        18319514605431597227        default\r\ntank  version                     -                           default\r\ntank  bootfs                      -                           default\r\ntank  delegation                  on                          default\r\ntank  autoreplace                 off                         default\r\ntank  cachefile                   -                           default\r\ntank  failmode                    wait                        default\r\ntank  listsnapshots               off                         default\r\ntank  autoexpand                  off                         default\r\ntank  dedupditto                  0                           default\r\ntank  dedupratio                  1.00x                       -\r\ntank  free                        12.3T                       -\r\ntank  allocated                   16.7T                       -\r\ntank  readonly                    off                         -\r\ntank  ashift                      12                          local\r\ntank  comment                     -                           default\r\ntank  expandsize                  -                           -\r\ntank  freeing                     0                           default\r\ntank  fragmentation               5%                          -\r\ntank  leaked                      0                           default\r\ntank  feature@async_destroy       enabled                     local\r\ntank  feature@empty_bpobj         active                      local\r\ntank  feature@lz4_compress        active                      local\r\ntank  feature@spacemap_histogram  active                      local\r\ntank  feature@enabled_txg         active                      local\r\ntank  feature@hole_birth          active                      local\r\ntank  feature@extensible_dataset  active                      local\r\ntank  feature@embedded_data       active                      local\r\ntank  feature@bookmarks           enabled                     local\r\ntank  feature@filesystem_limits   enabled                     local\r\ntank  feature@large_blocks        active                      local\r\n\r\nroot@oddity:~# zfs get all tank\r\nNAME  PROPERTY              VALUE                  SOURCE\r\ntank  type                  filesystem             -\r\ntank  creation              Fri May 13 19:22 2016  -\r\ntank  used                  11.8T                  -\r\ntank  available             8.13T                  -\r\ntank  referenced            222K                   -\r\ntank  compressratio         1.03x                  -\r\ntank  mounted               yes                    -\r\ntank  quota                 none                   default\r\ntank  reservation           none                   default\r\ntank  recordsize            1M                     local\r\ntank  mountpoint            /tank                  default\r\ntank  sharenfs              off                    default\r\ntank  checksum              on                     default\r\ntank  compression           lz4                    local\r\ntank  atime                 off                    local\r\ntank  devices               on                     default\r\ntank  exec                  on                     default\r\ntank  setuid                on                     default\r\ntank  readonly              off                    default\r\ntank  zoned                 off                    default\r\ntank  snapdir               hidden                 default\r\ntank  aclinherit            passthrough            local\r\ntank  canmount              on                     default\r\ntank  xattr                 sa                     local\r\ntank  copies                1                      default\r\ntank  version               5                      -\r\ntank  utf8only              off                    -\r\ntank  normalization         none                   -\r\ntank  casesensitivity       mixed                  -\r\ntank  vscan                 off                    default\r\ntank  nbmand                off                    default\r\ntank  sharesmb              off                    default\r\ntank  refquota              none                   default\r\ntank  refreservation        none                   default\r\ntank  primarycache          all                    default\r\ntank  secondarycache        all                    default\r\ntank  usedbysnapshots       154K                   -\r\ntank  usedbydataset         222K                   -\r\ntank  usedbychildren        11.8T                  -\r\ntank  usedbyrefreservation  0                      -\r\ntank  logbias               latency                default\r\ntank  dedup                 off                    default\r\ntank  mlslabel              none                   default\r\ntank  sync                  standard               default\r\ntank  refcompressratio      1.00x                  -\r\ntank  written               0                      -\r\ntank  logicalused           12.8T                  -\r\ntank  logicalreferenced     41K                    -\r\ntank  filesystem_limit      none                   default\r\ntank  snapshot_limit        none                   default\r\ntank  filesystem_count      none                   default\r\ntank  snapshot_count        none                   default\r\ntank  snapdev               hidden                 default\r\ntank  acltype               posixacl               local\r\ntank  context               none                   default\r\ntank  fscontext             none                   default\r\ntank  defcontext            none                   default\r\ntank  rootcontext           none                   default\r\ntank  relatime              off                    default\r\ntank  redundant_metadata    all                    default\r\ntank  overlay               off                    default\r\n```\r\nThe target pool:\r\n```\r\nroot@ubackup:~# modinfo zfs\r\nfilename:       /lib/modules/4.10.0-22-generic/kernel/zfs/zfs/zfs.ko\r\nversion:        0.6.5.9-2\r\nlicense:        CDDL\r\nauthor:         OpenZFS on Linux\r\ndescription:    ZFS\r\nsrcversion:     42C4AB70887EA26A9970936\r\ndepends:        spl,znvpair,zcommon,zunicode,zavl\r\nvermagic:       4.10.0-22-generic SMP mod_unload\r\n...\r\nroot@ubackup:~# zpool status\r\n  pool: btank\r\n state: ONLINE\r\n  scan: scrub repaired 0 in 3h36m with 0 errors on Tue Jun 13 13:34:08 2017\r\nconfig:\r\n\r\n        NAME                                          STATE     READ WRITE CKSUM\r\n        btank                                         ONLINE       0     0     0\r\n          raidz1-0                                    ONLINE       0     0     0\r\n            ata-WDC_WD30EZRX-00MMMB0_WD-XXXXXXXXXXXX  ONLINE       0     0     0\r\n            ata-WDC_WD30EZRX-00MMMB0_WD-XXXXXXXXXXXX  ONLINE       0     0     0\r\n            ata-WDC_WD30EZRX-00MMMB0_WD-XXXXXXXXXXXX  ONLINE       0     0     0\r\n            ata-WDC_WD30EZRX-00MMMB0_WD-XXXXXXXXXXXX  ONLINE       0     0     0\r\n            ata-WDC_WD30EZRX-00MMMB0_WD-XXXXXXXXXXXX  ONLINE       0     0     0\r\n            ata-WDC_WD30EZRX-00MMMB0_WD-XXXXXXXXXXXX  ONLINE       0     0     0\r\n            ata-WDC_WD30EZRX-00MMMB0_WD-XXXXXXXXXXXX  ONLINE       0     0     0\r\n            ata-WDC_WD30EZRX-00MMMB0_WD-XXXXXXXXXXXX  ONLINE       0     0     0\r\n            ata-WDC_WD30EZRX-00MMMB0_WD-XXXXXXXXXXXX  ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\nroot@ubackup:~# zpool get all btank\r\nNAME   PROPERTY                    VALUE                       SOURCE\r\nbtank  size                        24.5T                       -\r\nbtank  capacity                    23%                         -\r\nbtank  altroot                     -                           default\r\nbtank  health                      ONLINE                      -\r\nbtank  guid                        14601555808903550874        default\r\nbtank  version                     -                           default\r\nbtank  bootfs                      -                           default\r\nbtank  delegation                  on                          default\r\nbtank  autoreplace                 off                         default\r\nbtank  cachefile                   -                           default\r\nbtank  failmode                    wait                        default\r\nbtank  listsnapshots               off                         default\r\nbtank  autoexpand                  off                         default\r\nbtank  dedupditto                  0                           default\r\nbtank  dedupratio                  1.00x                       -\r\nbtank  free                        18.9T                       -\r\nbtank  allocated                   5.64T                       -\r\nbtank  readonly                    off                         -\r\nbtank  ashift                      12                          local\r\nbtank  comment                     -                           default\r\nbtank  expandsize                  -                           -\r\nbtank  freeing                     0                           default\r\nbtank  fragmentation               9%                          -\r\nbtank  leaked                      0                           default\r\nbtank  feature@async_destroy       enabled                     local\r\nbtank  feature@empty_bpobj         active                      local\r\nbtank  feature@lz4_compress        active                      local\r\nbtank  feature@spacemap_histogram  active                      local\r\nbtank  feature@enabled_txg         active                      local\r\nbtank  feature@hole_birth          active                      local\r\nbtank  feature@extensible_dataset  active                      local\r\nbtank  feature@embedded_data       active                      local\r\nbtank  feature@bookmarks           enabled                     local\r\nbtank  feature@filesystem_limits   enabled                     local\r\nbtank  feature@large_blocks        active                      local\r\nroot@ubackup:~# zfs get all btank\r\nNAME   PROPERTY              VALUE                  SOURCE\r\nbtank  type                  filesystem             -\r\nbtank  creation              Mon Jun 12 18:41 2017  -\r\nbtank  used                  5.01T                  -\r\nbtank  available             16.1T                  -\r\nbtank  referenced            171K                   -\r\nbtank  compressratio         1.03x                  -\r\nbtank  mounted               yes                    -\r\nbtank  quota                 none                   default\r\nbtank  reservation           none                   default\r\nbtank  recordsize            1M                     local\r\nbtank  mountpoint            /btank                 default\r\nbtank  sharenfs              off                    default\r\nbtank  checksum              on                     default\r\nbtank  compression           lz4                    local\r\nbtank  atime                 off                    local\r\nbtank  devices               on                     default\r\nbtank  exec                  on                     default\r\nbtank  setuid                on                     default\r\nbtank  readonly              off                    default\r\nbtank  zoned                 off                    default\r\nbtank  snapdir               hidden                 default\r\nbtank  aclinherit            passthrough            local\r\nbtank  canmount              on                     default\r\nbtank  xattr                 sa                     local\r\nbtank  copies                1                      default\r\nbtank  version               5                      -\r\nbtank  utf8only              on                     -\r\nbtank  normalization         formD                  -\r\nbtank  casesensitivity       mixed                  -\r\nbtank  vscan                 off                    default\r\nbtank  nbmand                off                    default\r\nbtank  sharesmb              off                    default\r\nbtank  refquota              none                   default\r\nbtank  refreservation        none                   default\r\nbtank  primarycache          all                    default\r\nbtank  secondarycache        all                    default\r\nbtank  usedbysnapshots       0                      -\r\nbtank  usedbydataset         171K                   -\r\nbtank  usedbychildren        5.01T                  -\r\nbtank  usedbyrefreservation  0                      -\r\nbtank  logbias               latency                default\r\nbtank  dedup                 off                    default\r\nbtank  mlslabel              none                   default\r\nbtank  sync                  disabled               local\r\nbtank  refcompressratio      1.00x                  -\r\nbtank  written               171K                   -\r\nbtank  logicalused           5.18T                  -\r\nbtank  logicalreferenced     40K                    -\r\nbtank  filesystem_limit      none                   default\r\nbtank  snapshot_limit        none                   default\r\nbtank  filesystem_count      none                   default\r\nbtank  snapshot_count        none                   default\r\nbtank  snapdev               hidden                 default\r\nbtank  acltype               posixacl               local\r\nbtank  context               none                   default\r\nbtank  fscontext             none                   default\r\nbtank  defcontext            none                   default\r\nbtank  rootcontext           none                   default\r\nbtank  relatime              off                    default\r\nbtank  redundant_metadata    all                    default\r\nbtank  overlay               off                    default\r\n```\r\nWhile the issue was observed with multiple datasets, I'll focus on a smaller one. First, the source (some uneventful snapshots omitted):\r\n```\r\nroot@oddity:~# zfs list -o space -r tank/dataz/Backup\r\nNAME               AVAIL   USED  USEDSNAP  USEDDS  USEDREFRESERV  USEDCHILD\r\ntank/dataz/Backup  8.13T   126G      461K    126G              0          0\r\nroot@oddity:~# zfs list -t snapshot -r tank/dataz/Backup\r\nNAME                                                      USED  AVAIL  REFER  MOUNTPOINT\r\ntank/dataz/Backup@zfs-auto-snap_monthly-2017-06-01-0100      0      -   125G  -\r\ntank/dataz/Backup@zfs-auto-snap_hourly-2017-06-12-1900       0      -   125G  -\r\ntank/dataz/Backup@zfs-auto-snap_hourly-2017-06-12-2000    205K      -   125G  -\r\ntank/dataz/Backup@zfs-auto-snap_hourly-2017-06-12-2100       0      -   126G  -\r\ntank/dataz/Backup@zfs-auto-snap_hourly-2017-06-13-0900       0      -   126G  -\r\n```\r\nThe initial send to ubackup has been performed with the -L and -e flags (not sure if this is relevant). Then performing an incremental send with the -L flag produces the expected result (size differences are due to different pool geometry):\r\n```\r\nroot@oddity:~# zfs send -L -e -I tank/dataz/Backup@zfs-auto-snap_monthly-2017-06-01-0100 tank/dataz/Backup@zfs-auto-snap_hourly-2017-06-13-0900 | ssh ubackup \"zfs receive btank/oddity/tank/dataz/Backup\"\r\n\r\nroot@ubackup:~# zfs list -o space -r btank/oddity/tank/dataz/Backup\r\nNAME                            AVAIL   USED  USEDSNAP  USEDDS  USEDREFRESERV  USEDCHILD\r\nbtank/oddity/tank/dataz/Backup  16.0T   133G      754K    133G              0          0\r\nroot@ubackup:~# zfs list -t snapshot -r btank/oddity/tank/dataz/Backup\r\nNAME                                                                   USED  AVAIL  REFER  MOUNTPOINT\r\nbtank/oddity/tank/dataz/Backup@zfs-auto-snap_monthly-2017-06-01-0100  14.2K      -   131G  -\r\nbtank/oddity/tank/dataz/Backup@zfs-auto-snap_hourly-2017-06-12-1900   14.2K      -   131G  -\r\nbtank/oddity/tank/dataz/Backup@zfs-auto-snap_hourly-2017-06-12-2000    156K      -   131G  -\r\nbtank/oddity/tank/dataz/Backup@zfs-auto-snap_hourly-2017-06-12-2100   14.2K      -   133G  -\r\nbtank/oddity/tank/dataz/Backup@zfs-auto-snap_hourly-2017-06-13-0900       0      -   133G  -\r\n```\r\nBut repeating the same send without the -L flag results in corruption:\r\n```\r\nroot@ubackup:~# zfs rollback -r btank/oddity/tank/dataz/Backup@zfs-auto-snap_monthly-2017-06-01-0100\r\n\r\nroot@oddity:~# zfs send -e -I tank/dataz/Backup@zfs-auto-snap_monthly-2017-06-01-0100 tank/dataz/Backup@zfs-auto-snap_hourly-2017-06-13-0900 | ssh ubackup \"zfs receive btank/oddity/tank/dataz/Backup\"\r\n\r\nroot@ubackup:~# zfs list -o space -r btank/oddity/tank/dataz/Backup\r\nNAME                            AVAIL   USED  USEDSNAP  USEDDS  USEDREFRESERV  USEDCHILD\r\nbtank/oddity/tank/dataz/Backup  16.0T   133G     19.0G    114G              0          0\r\nroot@ubackup:~# zfs list -t snapshot -r btank/oddity/tank/dataz/Backup\r\nNAME                                                                   USED  AVAIL  REFER  MOUNTPOINT\r\nbtank/oddity/tank/dataz/Backup@zfs-auto-snap_monthly-2017-06-01-0100  14.2K      -   131G  -\r\nbtank/oddity/tank/dataz/Backup@zfs-auto-snap_hourly-2017-06-12-1900   14.2K      -   131G  -\r\nbtank/oddity/tank/dataz/Backup@zfs-auto-snap_hourly-2017-06-12-2000    156K      -   112G  -\r\nbtank/oddity/tank/dataz/Backup@zfs-auto-snap_hourly-2017-06-12-2100   14.2K      -   114G  -\r\nbtank/oddity/tank/dataz/Backup@zfs-auto-snap_hourly-2017-06-13-0900       0      -   114G  -\r\n```\r\nNotice how the REFER size drops and the USEDSNAP increase between the two runs. I looked around to find where the data had gone missing and found several random files whose reported sizes on disk were reduced to 512 bytes. Reading these files seems to return the full original size but with content all zeros.\r\n\r\nI repeated the whole process multiple times, also recreating the target pool, with the same result. The affected files are always the same, but I haven't found a common characteristic among them. Scrubbing both pools finds no errors. Syslog shows nothing uncommon. I have never noted something similar before. The only notable change I made recently was that I started using sa based xattrs in May, but that might be unrelated.\r\n\r\nI am happy to provide more info as far as I'm able.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6224/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chrisrd": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6223", "title": "arc_prune: high load and soft lockups", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian\r\nDistribution Version    | 8.8\r\nLinux Kernel                 | 4.9.31\r\nArchitecture                 | x64\r\nZFS Version                  | 0.7.0+ (zfsonlinux/zfs@8264410)\r\nSPL Version                  | 0.7.0+ (zfsonlinux/spl@ac48361)\r\n\r\n### Describe the problem you're observing\r\n\r\nWe're experiencing high CPU load (spiking to 60-70, occasionally 90+) and soft lockups multiple times a day involving `arc_prune`, e.g.:\r\n```\r\n kernel:[2584566.976329] NMI watchdog: BUG: soft lockup - CPU#0 stuck for 21s! [arc_prune:10762]\r\n kernel:[2584567.126329] NMI watchdog: BUG: soft lockup - CPU#1 stuck for 21s! [arc_prune:30403]\r\n kernel:[2584568.636340] NMI watchdog: BUG: soft lockup - CPU#11 stuck for 23s! [arc_prune:2218]\r\n kernel:[2584568.786343] NMI watchdog: BUG: soft lockup - CPU#12 stuck for 23s! [arc_prune:20348]\r\n kernel:[2584568.936343] NMI watchdog: BUG: soft lockup - CPU#13 stuck for 22s! [arc_prune:18709]\r\n kernel:[2584569.086344] NMI watchdog: BUG: soft lockup - CPU#14 stuck for 22s! [arc_prune:31076]\r\n kernel:[2584571.276359] NMI watchdog: BUG: soft lockup - CPU#2 stuck for 22s! [arc_prune:30756]\r\n kernel:[2584571.426359] NMI watchdog: BUG: soft lockup - CPU#3 stuck for 21s! [arc_prune:29186]\r\n kernel:[2584571.586364] NMI watchdog: BUG: soft lockup - CPU#4 stuck for 22s! [arc_prune:29185]\r\n kernel:[2584571.736363] NMI watchdog: BUG: soft lockup - CPU#5 stuck for 23s! [arc_prune:30272]\r\n kernel:[2584571.886363] NMI watchdog: BUG: soft lockup - CPU#6 stuck for 23s! [arc_prune:312]\r\n kernel:[2584572.036363] NMI watchdog: BUG: soft lockup - CPU#7 stuck for 23s! [arc_prune:15481]\r\n kernel:[2584572.336366] NMI watchdog: BUG: soft lockup - CPU#9 stuck for 22s! [migration/9:65]\r\n kernel:[2584573.236374] NMI watchdog: BUG: soft lockup - CPU#15 stuck for 24s! [arc_prune:30402]\r\n```\r\n\r\nThe box has 192G RAM, Xeon E5620 x 2 (16 cores) and, possibly crucially, 2K+ mounted zfs filesystems (base filesystems + snapshots).\r\n\r\nNon-default settings:\r\n```\r\nzfs_arc_max:120259084288\r\nzfs_arc_meta_adjust_restarts:4\r\nzfs_arc_meta_limit:120259084288\r\nzfs_arc_meta_min:6442450944\r\nzfs_arc_meta_prune:100000\r\nzfs_arc_min:12884901888\r\nzfs_dirty_data_max:4294967296\r\nzfs_vdev_async_write_min_active:3\r\n```\r\n\r\nSome of these settings were put in place to see if they might help the issue e.g. `zfs_arc_meta_prune` and `zfs_arc_meta_adjust_restarts` (https://github.com/zfsonlinux/zfs/issues/3303#issuecomment-93598260).\r\n\r\nUsing `perf` during times of trouble, I see the major call chains being:\r\n```\r\narc_prune\r\n  + ret_from_fork\r\n    + kthread\r\n      + taskq_thread\r\n        + arc_prune_task\r\n          + zpl_prune_sb\r\n            + zfs_prune\r\n              + igrab\r\n              | + _raw_spin_lock\r\n              |   + lock_aquire\r\n              |\r\n              + d_prune_aliases\r\n                + _raw_spin_lock\r\n                  + lock_aquire\r\n\r\nkthreadd\r\n  + ret_from_fork\r\n    + kthread\r\n      + taskq_thread\r\n        + arc_prune_task\r\n          + zpl_prune_sb\r\n            + zfs_prune\r\n              + igrab\r\n              | + _raw_spin_lock\r\n              |   + lock_aquire\r\n              |\r\n              + d_prune_aliases\r\n                + _raw_spin_lock\r\n                  + lock_aquire\r\n```\r\n\r\nThe problem seems to lie in `zfs_prune()`.\r\n\r\nWith this kernel, we have:\r\n```\r\n$ egrep -h 'HAVE_SHRINK|HAVE_SPLIT_SHRINKER_CALLBACK|SHRINK_CONTROL_HAS_NID|SHRINKER_NUMA_AWARE|HAVE_D_PRUNE_ALIASES' ../{spl/spl,zfs/zfs}_config.h ~/git/linux/include/linux/shrinker.h\r\n#define HAVE_SHRINK_CONTROL_STRUCT 1\r\n#define HAVE_SPLIT_SHRINKER_CALLBACK 1\r\n#define HAVE_D_PRUNE_ALIASES 1\r\n#define SHRINK_CONTROL_HAS_NID 1\r\n#define SHRINKER_NUMA_AWARE     (1 << 0)\r\n```\r\n...which means `zfs_prune()` becomes:\r\n```\r\n/*\r\n * The ARC has requested that the filesystem drop entries from the dentry\r\n * and inode caches.  This can occur when the ARC needs to free meta data\r\n * blocks but can't because they are all pinned by entries in these caches.\r\n */\r\nint\r\nzfs_prune(struct super_block *sb, unsigned long nr_to_scan, int *objects)\r\n{\r\n        zfsvfs_t *zfsvfs = sb->s_fs_info;\r\n        int error = 0;\r\n        struct shrinker *shrinker = &sb->s_shrink;\r\n        struct shrink_control sc = {\r\n                .nr_to_scan = nr_to_scan,\r\n                .gfp_mask = GFP_KERNEL,\r\n        };\r\n        ZFS_ENTER(zfsvfs);\r\n        if (sb->s_shrink.flags & SHRINKER_NUMA_AWARE) {\r\n                *objects = 0;\r\n                for_each_online_node(sc.nid) {\r\n                        *objects += (*shrinker->scan_objects)(shrinker, &sc);\r\n                }\r\n        } else {\r\n                        *objects = (*shrinker->scan_objects)(shrinker, &sc);\r\n        }\r\n        /*\r\n         * Fall back to zfs_prune_aliases if the kernel's per-superblock\r\n         * shrinker couldn't free anything, possibly due to the inodes being\r\n         * allocated in a different memcg.\r\n         */\r\n        if (*objects == 0)\r\n                *objects = zfs_prune_aliases(zfsvfs, nr_to_scan);\r\n\r\n        ZFS_EXIT(zfsvfs);\r\n\r\n        dprintf_ds(zfsvfs->z_os->os_dsl_dataset,\r\n            \"pruning, nr_to_scan=%lu objects=%d error=%d\\n\",\r\n            nr_to_scan, *objects, error);\r\n\r\n        return (error);\r\n}\r\n```\r\n\r\n(The `*objects = 0` appears redundant as this function is only called from `zpl_prune_sb()` which sets `*objects` to zero first.)\r\n\r\nNote that `zfs_prune()` is called for each zfs superblock (i.e. zfs mount) on the system, and `zfs_prune()` in turn calls, \"for_each_online_node\", `(*shrinker->scan_objects)(shrinker, &sc)`.\r\n\r\nThrough the magic of `SPL_SHRINKER_CALLBACK_WRAPPER` and friends, the call to `shrinker->scan_objects()` ends up as a call to `__arc_shrinker_func(NULL, &sc)`.\r\n\r\nHowever for the life of me I can't see where `__arc_shrinker_func()` is either node-specific or superblock-specific. I.e. it seems for our 16 cores and 2K+ mounted file systems, we're calling `__arc_shrinker_func()` 32000+ times (16 * 2K) when once would have produced the same result?!  \r\n\r\nLooking at the issue from another direction, I used a systemtap script to log the calls to `zfs_prune()`, and during those calls, when something was actually able to be pruned (i.e. `*objects > 0`).  Over a 30 second period during \"moderate\" load average (~20-30) I observed 60886 calls to `zfs_prune()` (which amplifies to 970K+ calls to `__arc_shrinker_func()`), and only 1081 of those `zfs_prune()` calls returned with `*objects` set to non-zero, i.e. it was actually able to prune something. So that supports the code analysis that the vast majority of the time, `zfs_prune()` is simply spinning the wheels.\r\n\r\nWhat am I missing, or is pruning really this badly in need of some TLC?", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6223/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/e71cade67d48495db46fb6eed29b88b895bcb2d8", "message": "Scale the dbuf cache with arc_c\n\nCommit d3c2ae1 introduced a dbuf cache with a default size of the\r\nminimum of 100M or 1/32 maximum ARC size. (These figures may be adjusted\r\nusing dbuf_cache_max_bytes and dbuf_cache_max_shift.) The dbuf cache\r\nis counted as metadata for the purposes of ARC size calculations.\r\n\r\nOn a 1GB box the ARC maximum size defaults to c_max 493M which gives a\r\ndbuf cache default minimum size of 15.4M, and the ARC metadata defaults\r\nto minimum 16M. I.e. the dbuf cache is an significant proportion of the\r\nminimum metadata size. With other overheads involved this actually means\r\nthe ARC metadata doesn't get down to the minimum.\r\n\r\nThis patch dynamically scales the dbuf cache to the target ARC size\r\ninstead of statically scaling it to the maximum ARC size. (The scale is\r\nstill set by dbuf_cache_max_shift and the maximum size is still fixed by\r\ndbuf_cache_max_bytes.) Using the target ARC size rather than the current\r\nARC size is done to help the ARC reach the target rather than simply\r\nfocusing on the current size.\r\n\r\nReviewed-by: Chunwei Chen <tuxoko@gmail.com>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Chris Dunlop <chris@onthe.net.au>\r\nIssue #6506 \r\nCloses #6561"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/2fb1a234ab6912399336c8d459b1e40f67bc823e", "message": "dbuf_cons: deduplicate multilist_link_init()\n\nRemove harmless duplicate multilist_link_init() introduced by\r\ncommit d3c2ae1.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: loli10K <ezomori.nozomu@gmail.com>\r\nSigned-off-by: Chris Dunlop <chris@onthe.net.au>\r\nCloses #6552"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/1828902", "body": "Are these allocation failures considered \"mostly harmless\" or \"completely harmless\", and how \"unlikely\" are they expected to be?\n\nE.g. without this patch, minimal system/zfs turning (set zfs_arc_meta_limit to zfs_arc_max, no explicit vm.min_free_kbytes) and a light workload of a single receiving rsync and a simultaneous resilver:\n\n```\nSep  8 01:46:19 b5 kernel: [47880.015793] rsync: page allocation failure: order:5, mode:0x4030\nSep  8 01:46:21 b5 kernel: [47884.618058] rsync: page allocation failure: order:5, mode:0x4030\nSep  8 01:50:54 b5 kernel: [47892.526902] rsync: page allocation failure: order:5, mode:0x4030\nSep  8 01:50:54 b5 kernel: [47892.676713] rsync: page allocation failure: order:5, mode:0x4030\nSep  8 01:50:54 b5 kernel: [47892.845738] rsync: page allocation failure: order:5, mode:0x4030\nSep  8 01:50:54 b5 kernel: [47930.190093] rsync: page allocation failure: order:5, mode:0x4030\nSep  8 01:50:54 b5 kernel: [47930.349216] rsync: page allocation failure: order:5, mode:0x4030\nSep  8 01:50:54 b5 kernel: [47930.520605] rsync: page allocation failure: order:5, mode:0x4030\nSep  8 01:50:54 b5 kernel: [47930.719995] rsync: page allocation failure: order:5, mode:0x4030\nSep  8 01:50:54 b5 kernel: [47930.898777] rsync: page allocation failure: order:5, mode:0x4030\nSep  8 01:50:54 b5 kernel: [47931.378268] rsync: page allocation failure: order:5, mode:0x4030\nSep  8 01:50:54 b5 kernel: [47931.537419] rsync: page allocation failure: order:5, mode:0x4030\nSep  8 01:50:54 b5 kernel: [47931.667836] txg_sync: page allocation failure: order:5, mode:0x4030\nSep  8 01:50:54 b5 kernel: [47931.717222] rsync: page allocation failure: order:5, mode:0x4030\nSep  8 01:50:54 b5 kernel: [47931.919181] rsync: page allocation failure: order:5, mode:0x4030\n```\n\nThe box seemed to trundle along without any issues after this.\n\nGiven this patch makes the notifications go away altogether, are they completely uninteresting or is it worth leaving some kind of less-threatening notification in there?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/1828902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10250808", "body": "Commit comment:\n\"space it consumed\" => \"space is consumed\"\n\"This ensure's\" => \"This ensures\"\n\nLooks good to me. For what it's worth:\n\nReviewed-by: Chris Dunlop chris@onthe.net.au\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10250808/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10250986", "body": "Acheiving => Achieving\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10250986/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10251002", "body": "being exceeded.  Then request => being exceeded then request\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10251002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10251003", "body": "Commit comment:\n\n\"Acheiving\" => \"Achieving\"\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10251003/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/5022697", "body": "On Wed, Jul 03, 2013 at 05:33:14AM -0700, Richard Yao wrote:\n\n> > @@ -0,0 +1,29 @@\n> > +dnl #\n> > +dnl # 2.10.x API change\n> \n> This should say 3.10.x.\n\nThanks. Fix pushed, but I'm not sure how that works with a\npotential pull from Brian. Do I need to flatten the commits\nsomehow?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/5022697/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "Ektorus": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6218", "title": "General protection fault: 0000 [#1] SMP in arc_released+0x29/0x60 [zfs]", "body": "### System information\r\nType                                | Debian\r\nDistribution Name       |  stretch\r\nDistribution Version    | 9.0\r\nLinux Kernel                 | 4.9.0-3\r\nArchitecture                 | amd64\r\nZFS Version                  | 0.6.5.9-5\r\nSPL Version                  | 0.6.5.9-1\r\n\r\n### Describe the problem you're observing\r\nA general protection fault crashes the kernel in arc_released+0x29/0x60 [zfs].\r\nThis problem has occurred many times, but in different instructions but always general protection fault.\r\n\r\n### Describe how to reproduce the problem\r\nThe problem mostly occurs when a lot of I/O operations happens on zfs pools. For example when running multiple rsync operations or as in the example given when updatedb.mlocate runs.\r\n\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n\r\nThe last bit for dmesg (the full log has been attached):\r\n```\r\n[83929.227751] general protection fault: 0000 [#1] SMP\r\n[83929.227786] Modules linked in: dm_snapshot dm_bufio vhost_net vhost macvtap macvlan tun ebtable_filter ebtables ip6table_filter ip6_tables iptable_filter bridge stp llc binfmt_misc iTCO_wdt iTCO_vendor_support joydev zfs(PO) zunicode(PO) zavl(PO) zcommon(PO) znvpair(PO) spl(O) intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel snd_hda_codec_hdmi intel_cstate snd_hda_codec_realtek snd_hda_codec_generic mei_me mei lpc_ich snd_hda_intel snd_hda_codec mfd_core snd_hda_core snd_hwdep snd_pcm intel_uncore snd_timer shpchp sg intel_rapl_perf pcspkr serio_raw ie31200_edac edac_core tpm_infineon snd evdev soundcore nfsd auth_rpcgss oid_registry nfs_acl lockd grace sunrpc loop ip_tables x_tables autofs4 ext4 crc16 jbd2 fscrypto\r\n[83929.228306]  ecb glue_helper lrw gf128mul ablk_helper cryptd aes_x86_64 mbcache btrfs raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c crc32c_generic raid0 multipath linear dm_mod raid1 md_mod sd_mod hid_logitech ff_memless ata_generic usbhid hid mxm_wmi crc32c_intel ata_piix libata i2c_i801 i2c_smbus i915 xhci_pci ehci_pci ehci_hcd xhci_hcd video i2c_algo_bit usbcore drm_kms_helper usb_common megaraid_sas r8169 mii drm scsi_mod wmi button\r\n[83929.228637] CPU: 1 PID: 22427 Comm: updatedb.mlocat Tainted: P        W  O    4.9.0-3-amd64 #1 Debian 4.9.25-1\r\n[83929.228688] Hardware name: Gigabyte Technology Co., Ltd. Z68A-D3H-B3/Z68A-D3H-B3, BIOS F13 03/20/2012\r\n[83929.228735] task: ffff92dcb622e000 task.stack: ffffac4c31808000\r\n[83929.228766] RIP: 0010:[<ffffffffc10061b9>]  [<ffffffffc10061b9>] arc_released+0x29/0x60 [zfs]\r\n[83929.228852] RSP: 0018:ffffac4c3180b7d8  EFLAGS: 00010286\r\n[83929.228880] RAX: fdff92d9b7996a80 RBX: ffff92d9b7998630 RCX: 0000000000000000\r\n[83929.228916] RDX: ffffffffc11706e0 RSI: ffff92d9b7998630 RDI: ffff92d9b7998640\r\n[83929.228953] RBP: ffff92d9b6caf240 R08: ffff92d9b7996b78 R09: 000000000100ff71\r\n[83929.228989] R10: ffff92d9b6ca9d58 R11: ffff92ddeb60c000 R12: ffff92d9b7998640\r\n[83929.229025] R13: 0000008000000000 R14: ffff92d9b7998630 R15: ffff92d9b7996a80\r\n[83929.229063] FS:  00007f355c050700(0000) GS:ffff92de1fa40000(0000) knlGS:0000000000000000\r\n[83929.229104] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[83929.229134] CR2: 00007f61d9a83000 CR3: 00000006c1102000 CR4: 00000000001426e0\r\n[83929.229170] Stack:\r\n[83929.229183]  ffff92d9b7998630 ffff92d9b6caf240 ffff92d9b7998630 ffffffffc1009904\r\n[83929.229230]  ffff92d9b6caf240 0000000000000000 ffffffffc100a404 0000000000000000\r\n[83929.229276]  ffffac4c3180b8d0 ffff92d9b9e6bf00 ffffffffc1005050 ffffffffc10a798e\r\n[83929.229322] Call Trace:\r\n[83929.229367]  [<ffffffffc1009904>] ? dbuf_set_data+0x24/0x40 [zfs]\r\n[83929.229426]  [<ffffffffc100a404>] ? dbuf_read_done+0x44/0xf0 [zfs]\r\n[83929.229485]  [<ffffffffc1005050>] ? arc_read+0x720/0xa30 [zfs]\r\n[83929.229556]  [<ffffffffc10a798e>] ? zio_create+0x3be/0x470 [zfs]\r\n[83929.229615]  [<ffffffffc100a3c0>] ? dbuf_rele_and_unlock+0x3c0/0x3c0 [zfs]\r\n[83929.229677]  [<ffffffffc100bb2e>] ? dbuf_read+0x29e/0x7d0 [zfs]\r\n[83929.229741]  [<ffffffffc1024a08>] ? dnode_hold_impl+0xc8/0x4c0 [zfs]\r\n[83929.229813]  [<ffffffffc109c0f2>] ? zfs_znode_hold_enter+0xf2/0x140 [zfs]\r\n[83929.229878]  [<ffffffffc10143c3>] ? dmu_bonus_hold+0x33/0x290 [zfs]\r\n[83929.229948]  [<ffffffffc109faa4>] ? zfs_zget+0x114/0x200 [zfs]\r\n[83929.230018]  [<ffffffffc107efc8>] ? zfs_dirent_lock+0x508/0x5b0 [zfs]\r\n[83929.230090]  [<ffffffffc1079862>] ? zfs_zaccess_aces_check+0x182/0x310 [zfs]\r\n[83929.230163]  [<ffffffffc107f103>] ? zfs_dirlook+0x93/0x2f0 [zfs]\r\n[83929.230233]  [<ffffffffc1093dbb>] ? zfs_lookup+0x2db/0x330 [zfs]\r\n[83929.230302]  [<ffffffffc10aedf0>] ? zpl_lookup+0xc0/0x210 [zfs]\r\n[83929.230337]  [<ffffffff8d0a1f91>] ? security_capable+0x41/0x60\r\n[83929.230370]  [<ffffffff8d00df93>] ? lookup_slow+0xa3/0x170\r\n[83929.230400]  [<ffffffff8d00e693>] ? walk_component+0x1f3/0x320\r\n[83929.230433]  [<ffffffff8d00f797>] ? path_lookupat+0x67/0x120\r\n[83929.230464]  [<ffffffff8d012101>] ? filename_lookup+0xb1/0x180\r\n[83929.230496]  [<ffffffff8cffe03a>] ? __check_object_size+0xfa/0x1d8\r\n[83929.230531]  [<ffffffff8d156838>] ? strncpy_from_user+0x48/0x160\r\n[83929.230565]  [<ffffffff8d011d3a>] ? getname_flags+0x6a/0x1e0\r\n[83929.230595]  [<ffffffff8d006eb9>] ? vfs_fstatat+0x59/0xb0\r\n[83929.230625]  [<ffffffff8d0a1f91>] ? security_capable+0x41/0x60\r\n[83929.230657]  [<ffffffff8d00746d>] ? SYSC_newlstat+0x2d/0x60\r\n[83929.230687]  [<ffffffff8d00bf42>] ? path_put+0x12/0x20\r\n[83929.230716]  [<ffffffff8d01890b>] ? SyS_poll+0x6b/0x120\r\n[83929.230746]  [<ffffffff8d40413b>] ? system_call_fast_compare_end+0xc/0x9b\r\n[83929.230781] Code: 00 00 0f 1f 44 00 00 41 54 4c 8d 67 10 55 53 48 89 fb 4c 89 e7 e8 08 b5 3f cc 48 83 7b 40 00 74 3c 48 8b 03 48 8b 15 2f a3 16 00 <48> 39 90 f0 00 00 00 40 0f 94 c5 48 83 c3 38 48 89 df e8 00 de \r\n[83929.231020] RIP  [<ffffffffc10061b9>] arc_released+0x29/0x60 [zfs]\r\n[83929.231082]  RSP <ffffac4c3180b7d8>\r\n```\r\n\r\nThe backtrace from the dump file (also attached):\r\n```\r\nWARNING: kernel relocated [190MB]: patching 76353 gdb minimal_symbol values\r\n\r\n      KERNEL: /usr/lib/debug/boot/vmlinux-4.9.0-3-amd64\r\n    DUMPFILE: dump.201706100641  [PARTIAL DUMP]\r\n        CPUS: 4\r\n        DATE: Sat Jun 10 06:40:42 2017\r\n      UPTIME: 05:53:26\r\nLOAD AVERAGE: 1.11, 1.14, 0.85\r\n       TASKS: 694\r\n    NODENAME: storage-server\r\n     RELEASE: 4.9.0-3-amd64\r\n     VERSION: #1 SMP Debian 4.9.25-1 (2017-05-02)\r\n     MACHINE: x86_64  (3392 Mhz)\r\n      MEMORY: 31.9 GB\r\n       PANIC: \"general protection fault: 0000 [#1] SMP\"\r\n         PID: 22427\r\n     COMMAND: \"updatedb.mlocat\"\r\n        TASK: ffff92dcb622e000  [THREAD_INFO: ffff92dcb622e000]\r\n         CPU: 1\r\n       STATE: TASK_RUNNING (PANIC)\r\n\r\ncrash> bt\r\nPID: 22427  TASK: ffff92dcb622e000  CPU: 1   COMMAND: \"updatedb.mlocat\"\r\n #0 [ffffac4c3180b5d0] machine_kexec at ffffffff8ce51e58\r\n #1 [ffffac4c3180b628] __crash_kexec at ffffffff8cf03389\r\n #2 [ffffac4c3180b6e8] crash_kexec at ffffffff8cf034a8\r\n #3 [ffffac4c3180b700] oops_end at ffffffff8ce28973\r\n #4 [ffffac4c3180b720] general_protection at ffffffff8d4054b8\r\n    [exception RIP: arc_released+41]\r\n    RIP: ffffffffc10061b9  RSP: ffffac4c3180b7d8  RFLAGS: 00010286\r\n    RAX: fdff92d9b7996a80  RBX: ffff92d9b7998630  RCX: 0000000000000000\r\n    RDX: ffffffffc11706e0  RSI: ffff92d9b7998630  RDI: ffff92d9b7998640\r\n    RBP: ffff92d9b6caf240   R8: ffff92d9b7996b78   R9: 000000000100ff71\r\n    R10: ffff92d9b6ca9d58  R11: ffff92ddeb60c000  R12: ffff92d9b7998640\r\n    R13: 0000008000000000  R14: ffff92d9b7998630  R15: ffff92d9b7996a80\r\n    ORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018\r\n #5 [ffffac4c3180b7f0] dbuf_set_data at ffffffffc1009904 [zfs]\r\n #6 [ffffac4c3180b808] dbuf_read_done at ffffffffc100a404 [zfs]\r\n #7 [ffffac4c3180b828] arc_read at ffffffffc1005050 [zfs]\r\n #8 [ffffac4c3180b8d8] dbuf_read at ffffffffc100bb2e [zfs]\r\n #9 [ffffac4c3180b970] dnode_hold_impl at ffffffffc1024a08 [zfs]\r\n#10 [ffffac4c3180b9d8] dmu_bonus_hold at ffffffffc10143c3 [zfs]\r\n#11 [ffffac4c3180ba18] zfs_zget at ffffffffc109faa4 [zfs]\r\n#12 [ffffac4c3180baa0] zfs_dirent_lock at ffffffffc107efc8 [zfs]\r\n#13 [ffffac4c3180bb48] zfs_dirlook at ffffffffc107f103 [zfs]\r\n#14 [ffffac4c3180bbb0] zfs_lookup at ffffffffc1093dbb [zfs]\r\n#15 [ffffac4c3180bc18] zpl_lookup at ffffffffc10aedf0 [zfs]\r\n#16 [ffffac4c3180bca0] lookup_slow at ffffffff8d00df93\r\n#17 [ffffac4c3180bd00] walk_component at ffffffff8d00e693\r\n#18 [ffffac4c3180bd50] path_lookupat at ffffffff8d00f797\r\n#19 [ffffac4c3180bd78] filename_lookup at ffffffff8d012101\r\n#20 [ffffac4c3180be88] vfs_fstatat at ffffffff8d006eb9\r\n#21 [ffffac4c3180bed0] SYSC_newlstat at ffffffff8d00746d\r\n#22 [ffffac4c3180bf50] system_call_fast_compare_end at ffffffff8d40413b\r\n    RIP: 00007f355bb7b125  RSP: 00007ffe3b691488  RFLAGS: 00000246\r\n    RAX: ffffffffffffffda  RBX: 0000000000000010  RCX: 00007f355bb7b125\r\n    RDX: 00007ffe3b691500  RSI: 00007ffe3b691500  RDI: 0000556a676b8129\r\n    RBP: 0000556a676b80a9   R8: 000000000000006e   R9: 000000000000006e\r\n    R10: 0000000000000000  R11: 0000000000000246  R12: 0000556a676b81c9\r\n    R13: 0000556a676b81c0  R14: 0000000000000000  R15: 0000556a676af850\r\n    ORIG_RAX: 0000000000000006  CS: 0033  SS: 002b\r\n```\r\n\r\nAttached files:\r\n[dmesg.201706100641.txt](https://github.com/zfsonlinux/zfs/files/1066425/dmesg.201706100641.txt)\r\n[gdb_bt.201706100641.txt](https://github.com/zfsonlinux/zfs/files/1066426/gdb_bt.201706100641.txt)", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6218/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alek-p": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6211", "title": "rare race around .zfs contorl dir", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | \r\nDistribution Version    | \r\nLinux Kernel                 | \r\nArchitecture                 | \r\nZFS Version                  | \r\nSPL Version                  | \r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\nI think this should be present on all system with the \"new\" autounmount.\r\nThat's anything post zfs-0.6.5\r\n\r\n### Describe the problem you're observing\r\nsnapentry_expire() and zfsctl_snapshot_unmount_cancel() could modify\r\nzfs_snapentry_t's se_taskid while holding the snapshot lock as readers.\r\nThis leads to a race on se_taskid manifested in hitting the se_taskqid\r\nassert in zfsctl_snapshot_unmount_delay_impl().\r\nI believe the impact on non-debug builds could be early auto-unmount\r\nand a leak of ref a snapshot's refcount where it doesn't get decremented.\r\n\r\nHere is the discussion about the bellow patch https://github.com/zfsonlinux/zfs/pull/6062\r\n\r\n### Describe how to reproduce the problem\r\nrun newer debug code with zfs-0.6.5 or newer and do .zfs dir access\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nWe discovered this in the Datto lab when this assert was hit (once in all testing):\r\n```\r\n[2985333.968482] VERIFY3(se->se_taskqid == ((taskqid_t)0)) failed (6290 == 0)\r\n[2985333.979621] PANIC at zfs_ctldir.c:375:zfsctl_snapshot_unmount_delay_impl()\r\n[2985333.990973] Showing stack for process 17335\r\n[2985333.990977] CPU: 9 PID: 17335 Comm: java Tainted: P           OE   4.4.0-67-generic #88-Ubuntu\r\n[2985333.990979] Hardware name: Supermicro X9DRD-7LN4F(-JBOD)/X9DRD-EF/X9DRD-7LN4F, BIOS 3.2 01/16/2015\r\n[2985333.990981]  0000000000000286 0000000008a44152 ffff88293db8b9d8 ffffffff813f86d3\r\n[2985333.990984]  ffffffffc1088eff 0000000000000177 ffff88293db8b9e8 ffffffffc0438872\r\n[2985333.990986]  ffff88293db8bb70 ffffffffc043893b 0000000008a44152 ffff882d00000030\r\n[2985333.990988] Call Trace:\r\n[2985333.990998]  [<ffffffff813f86d3>] dump_stack+0x63/0x90\r\n[2985333.991010]  [<ffffffffc0438872>] spl_dumpstack+0x42/0x50 [spl]\r\n[2985333.991016]  [<ffffffffc043893b>] spl_panic+0xbb/0xf0 [spl]\r\n[2985333.991086]  [<ffffffffc0f219d6>] ? zfs_refcount_add+0x16/0x20 [zfs]\r\n[2985333.991142]  [<ffffffffc0f6e848>] ? zfsctl_snapshot_find_by_objsetid+0x68/0xc0 [zfs]\r\n[2985333.991192]  [<ffffffffc0f60fb7>] ? fzap_add_cd+0x147/0x200 [zfs]\r\n[2985333.991202]  [<ffffffff8183c497>] ? _raw_spin_lock_irqsave+0x37/0x40\r\n[2985333.991212]  [<ffffffffc0436630>] ? taskq_cancel_id+0xd0/0x130 [spl]\r\n[2985333.991259]  [<ffffffffc0f6eeb2>] zfsctl_snapshot_unmount_delay_impl+0x82/0x90 [zfs]\r\n[2985333.991310]  [<ffffffffc0f6efa2>] zfsctl_snapshot_unmount_delay+0x52/0xd0 [zfs]\r\n[2985333.991360]  [<ffffffffc0fb40f7>] zpl_revalidate+0xe7/0x110 [zfs]\r\n[2985333.991366]  [<ffffffff8121a0af>] lookup_fast+0x24f/0x330\r\n[2985333.991368]  [<ffffffff8121b779>] walk_component+0x49/0x300\r\n[2985333.991371]  [<ffffffff8121cd01>] link_path_walk+0x191/0x5b0\r\n[2985333.991373]  [<ffffffff8121b16b>] ? path_init+0x1eb/0x3c0\r\n[2985333.991376]  [<ffffffff8121d5e9>] path_openat+0xa9/0x1330\r\n[2985333.991381]  [<ffffffff81101d9e>] ? futex_wake_op+0x2ee/0x620\r\n[2985333.991385]  [<ffffffff810b5269>] ? update_curr+0x79/0x160\r\n[2985333.991387]  [<ffffffff8121fa61>] do_filp_open+0x91/0x100\r\n[2985333.991390]  [<ffffffff8122d366>] ? __alloc_fd+0x46/0x190\r\n[2985333.991393]  [<ffffffff8120df08>] do_sys_open+0x138/0x2a0\r\n[2985333.991398]  [<ffffffff8106b544>] ? __do_page_fault+0x1b4/0x400\r\n[2985333.991401]  [<ffffffff8120e08e>] SyS_open+0x1e/0x20\r\n[2985333.991404]  [<ffffffff8183c672>] entry_SYSCALL_64_fastpath+0x16/0x7\r\n```\r\nI tried the obvious fix of switching se_refcount to a write lock\r\nwhich looks to lead to a deadlock: \r\n```\r\ndiff --git a/module/zfs/zfs_ctldir.c b/module/zfs/zfs_ctldir.c\r\nindex eea1bb2e80..fb0e88b3a4 100644\r\n--- a/module/zfs/zfs_ctldir.c\r\n+++ b/module/zfs/zfs_ctldir.c\r\n@@ -192,7 +193,7 @@ static void\r\n zfsctl_snapshot_add(zfs_snapentry_t *se)\r\n {\r\n \tASSERT(RW_WRITE_HELD(&zfs_snapshot_lock));\r\n-\trefcount_add(&se->se_refcount, NULL);\r\n+\tzfsctl_snapshot_hold(se);\r\n \tavl_add(&zfs_snapshots_by_name, se);\r\n \tavl_add(&zfs_snapshots_by_objsetid, se);\r\n }\r\n@@ -269,7 +270,7 @@ zfsctl_snapshot_find_by_name(char *snapname)\r\n \tsearch.se_name = snapname;\r\n \tse = avl_find(&zfs_snapshots_by_name, &search, NULL);\r\n \tif (se)\r\n-\t\trefcount_add(&se->se_refcount, NULL);\r\n+\t\tzfsctl_snapshot_hold(se);\r\n \r\n \treturn (se);\r\n }\r\n@@ -290,7 +291,7 @@ zfsctl_snapshot_find_by_objsetid(spa_t *spa, uint64_t objsetid)\r\n \tsearch.se_objsetid = objsetid;\r\n \tse = avl_find(&zfs_snapshots_by_objsetid, &search, NULL);\r\n \tif (se)\r\n-\t\trefcount_add(&se->se_refcount, NULL);\r\n+\t\tzfsctl_snapshot_hold(se);\r\n \r\n \treturn (se);\r\n }\r\n@@ -334,15 +335,15 @@ snapentry_expire(void *data)\r\n \t\treturn;\r\n \t}\r\n \r\n-\tse->se_taskqid = TASKQID_INVALID;\r\n \t(void) zfsctl_snapshot_unmount(se->se_name, MNT_EXPIRE);\r\n-\tzfsctl_snapshot_rele(se);\r\n \r\n \t/*\r\n \t * Reschedule the unmount if the zfs_snapentry_t wasn't removed.\r\n \t * This can occur when the snapshot is busy.\r\n \t */\r\n-\trw_enter(&zfs_snapshot_lock, RW_READER);\r\n+\trw_enter(&zfs_snapshot_lock, RW_WRITER);\r\n+\tse->se_taskqid = TASKQID_INVALID;\r\n+\tzfsctl_snapshot_rele(se);\r\n \tif ((se = zfsctl_snapshot_find_by_objsetid(spa, objsetid)) != NULL) {\r\n \t\tzfsctl_snapshot_unmount_delay_impl(se, zfs_expire_snapshot);\r\n \t\tzfsctl_snapshot_rele(se);\r\n@@ -358,7 +359,7 @@ snapentry_expire(void *data)\r\n static void\r\n zfsctl_snapshot_unmount_cancel(zfs_snapentry_t *se)\r\n {\r\n-\tASSERT(RW_LOCK_HELD(&zfs_snapshot_lock));\r\n+\tASSERT(RW_WRITE_HELD(&zfs_snapshot_lock));\r\n \r\n \tif (taskq_cancel_id(system_delay_taskq, se->se_taskqid) == 0) {\r\n \t\tse->se_taskqid = TASKQID_INVALID;\r\n@@ -372,6 +373,7 @@ zfsctl_snapshot_unmount_cancel(zfs_snapentry_t *se)\r\n static void\r\n zfsctl_snapshot_unmount_delay_impl(zfs_snapentry_t *se, int delay)\r\n {\r\n+\tASSERT(RW_WRITE_HELD(&zfs_snapshot_lock));\r\n \tASSERT3S(se->se_taskqid, ==, TASKQID_INVALID);\r\n \r\n \tif (delay <= 0)\r\n@@ -394,7 +396,7 @@ zfsctl_snapshot_unmount_delay(spa_t *spa, uint64_t objsetid, int delay)\r\n \tzfs_snapentry_t *se;\r\n \tint error = ENOENT;\r\n \r\n-\trw_enter(&zfs_snapshot_lock, RW_READER);\r\n+\trw_enter(&zfs_snapshot_lock, RW_WRITER);\r\n \tif ((se = zfsctl_snapshot_find_by_objsetid(spa, objsetid)) != NULL) {\r\n \t\tzfsctl_snapshot_unmount_cancel(se);\r\n \t\tzfsctl_snapshot_unmount_delay_impl(se, delay);\r\n```\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6211/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/01ff0d7540b21c461c19b90b1e715df26cba3ff2", "message": "Update the default for zfs_txg_history\n\nIt's often useful to have access to txg history for debugging\r\npurposes. This patch changes the default from 0 to 100 TXGs\r\nworth of history preserved.\r\n\r\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed by: Richard Elling <Richard.Elling@RichardElling.com>\r\nReviewed by: Prakash Surya <prakash.surya@delphix.com>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Alek Pinchuk <apinchuk@datto.com>\r\nCloses #6691"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/e4b6b2db124eac51580833d45d83dfde05cbc55c", "message": "OpenZFS 8414 - Implemented zpool scrub pause/resume\n\nAuthored by: Alek Pinchuk <apinchuk@datto.com>\r\nReviewed by: George Melikov <mail@gmelikov.ru>\r\nReviewed by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed by: Brad Lewis <brad.lewis@delphix.com>\r\nReviewed by: Serapheim Dimitropoulos <serapheim@delphix.com>\r\nReviewed by: Matt Ahrens <mahrens@delphix.com>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nApproved by: Dan McDonald <danmcd@joyent.com>\r\nPorted-by: Alek Pinchuk <apinchuk@datto.com>\r\n\r\nOpenZFS-issue: https://www.illumos.org/issues/8414\r\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/c29616076\r\nCloses #6538"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/0ea05c64f8d08c20439dd2a06e949a2aa4115101", "message": "Implemented zpool scrub pause/resume\n\nCurrently, there is no way to pause a scrub. Pausing may\r\nbe useful when the pool is busy with other I/O to preserve\r\nbandwidth.\r\n\r\nThis patch adds the ability to pause and resume scrubbing.\r\nThis is achieved by maintaining a persistent on-disk scrub state.\r\nWhile the state is 'paused' we do not scrub any more blocks.\r\nWe do however perform regular scan housekeeping such as\r\nfreeing async destroyed and deadlist blocks while paused.\r\n\r\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\r\nReviewed by: Thomas Caputi <tcaputi@datto.com>\r\nReviewed-by: Serapheim Dimitropoulos <serapheimd@gmail.com>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Alek Pinchuk <apinchuk@datto.com>\r\nCloses #6167"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rugubara": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6196", "title": "kernel panic with uncomressed sparse ZVOL", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |   Gentoo\r\nDistribution Version    |    current/latest\r\nLinux Kernel                 |   4.10.17\r\nArchitecture                 |   amd64\r\nZFS Version                  |  0.7.0-rc4_44_g9f7b066b\r\nSPL Version                  |  0.7.0-rc4_4_gac48361\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\ncopying a sparse VM image file to an uncompressed ZVOL floods memory and leads to a panic\r\n### Describe how to reproduce the problem\r\ncreate a ZPOOL with no compression. Create a sparse ZVOL. I have an image with virtual size 320GB and allocated size of 32GB. This is a real windows 10 image for KVM.\r\n```\r\ndd if=image.raw of=/dev/zvol/test/tst01 bs=4M\r\n```\r\nwatch the system burn in flames. This behaviour can be delayed by capping zfs_arc_max. I have 16GB RAM. If I cap arc max to 2GB I can then observe very high utilization in z_wr_iss and arc_prune (close to 95% of all available CPU power). I can also then observe that zvol grows far beyond the size of 320GB but in the end the system panics before the copy operation is complete.\r\nThe swap usage skyrockets and the system quickly become a standstill. \r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n[latest panic from pstore](https://pastebin.com/LW4dsBsq)\r\n\r\nThis problem can be worked around by enabling zle compression on zvol. \r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6196/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ovaistariq": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6187", "title": "CPU lockup with arc_reclaim responsible for most of the CPU time", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Debian GNU/Linux\r\nDistribution Version    | 8 (jessie)\r\nLinux Kernel                 | 3.18.44\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.7-8-jessie\r\nSPL Version                  | 0.6.5.7-5-jessie\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nWe are seeing periodic stalls due to CPU stalls. The stalls last for a couple of minute and the system becomes unresponsive. There is no increase in IO activity during the time with most of the CPU time being reported spent in %sys.\r\n\r\nThe hosts where we see stalls run MySQL. The stall happens with no change in the workload or no noticeable change in the workload. We have not been able to correlate it to increase in IO activity or other changes in activity.\r\n\r\n### Describe how to reproduce the problem\r\nThe problem appears to happen without any change in workload so I am not yet sure how to reproduce it. On one of the hosts it is very regular and that host performs large sequential reads.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nThere are couple of different types of messages in dmesg when the stall occurs.\r\n\r\nOne example is the following:\r\n```\r\n[Thu May 18 09:38:42 2017] INFO: task mysqld:59024 blocked for more than 120 seconds.\r\n[Thu May 18 09:38:42 2017]       Tainted: P           OE  3.18.44 #1\r\n[Thu May 18 09:38:42 2017] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\r\n[Thu May 18 09:38:42 2017] mysqld          D ffff883ffe7b36c0     0 59024 118853 0x00000000\r\n[Thu May 18 09:38:42 2017]  ffff881e9991bc48 0000000000000086 ffff881ef6815a00 00000000000136c0\r\n[Thu May 18 09:38:42 2017]  ffff881e9991bfd8 00000000000136c0 ffff881fa60c5a00 ffff881ef6815a00\r\n[Thu May 18 09:38:42 2017]  000000009991bc28 ffff883efc823f90 ffff883efc823f94 ffff881ef6815a00\r\n[Thu May 18 09:38:42 2017] Call Trace:\r\n[Thu May 18 09:38:42 2017]  [<ffffffff8179c779>] schedule_preempt_disabled+0x29/0x70\r\n[Thu May 18 09:38:42 2017]  [<ffffffff8179e9d5>] __mutex_lock_slowpath+0x95/0x100\r\n[Thu May 18 09:38:42 2017]  [<ffffffff81200790>] ? lookup_fast+0x170/0x2f0\r\n[Thu May 18 09:38:42 2017]  [<ffffffff8179ea63>] mutex_lock+0x23/0x40\r\n[Thu May 18 09:38:42 2017]  [<ffffffff81205b46>] path_lookupat+0x896/0xd60\r\n[Thu May 18 09:38:42 2017]  [<ffffffff811ce2d2>] ? mpol_misplaced+0xe2/0x1c0\r\n[Thu May 18 09:38:42 2017]  [<ffffffff811d76e5>] ? kmem_cache_alloc+0x35/0x200\r\n[Thu May 18 09:38:42 2017]  [<ffffffff81206aef>] ? getname_flags+0x4f/0x1a0\r\n[Thu May 18 09:38:42 2017]  [<ffffffff8120603a>] filename_lookup+0x2a/0xd0\r\n[Thu May 18 09:38:42 2017]  [<ffffffff81207b64>] user_path_at_empty+0x54/0xa0\r\n[Thu May 18 09:38:42 2017]  [<ffffffff81061ad0>] ? __do_page_fault+0x2a0/0x5b0\r\n[Thu May 18 09:38:42 2017]  [<ffffffff81207bc1>] user_path_at+0x11/0x20\r\n[Thu May 18 09:38:42 2017]  [<ffffffff811fa682>] vfs_fstatat+0x52/0xa0\r\n[Thu May 18 09:38:42 2017]  [<ffffffff810e0bb4>] ? hrtimer_start_range_ns+0x14/0x20\r\n[Thu May 18 09:38:42 2017]  [<ffffffff811fab1f>] SYSC_newstat+0x1f/0x40\r\n[Thu May 18 09:38:42 2017]  [<ffffffff81061e11>] ? do_page_fault+0x31/0x70\r\n[Thu May 18 09:38:42 2017]  [<ffffffff817a3478>] ? page_fault+0x28/0x30\r\n[Thu May 18 09:38:42 2017]  [<ffffffff811fad9e>] SyS_newstat+0xe/0x10\r\n[Thu May 18 09:38:42 2017]  [<ffffffff817a144d>] system_call_fastpath+0x16/0x1b\r\n```\r\n\r\nI was able to capture profiling information using `perf` from during the time of the stall.\r\nHere is one of the perf reports:\r\n```\r\nSamples: 18K of event 'cycles', Event count (approx.): 170290002289\r\n  Children      Self  Command          Shared Object                         Symbol                                                                                                                                                                                            \u25c6\r\n-   42.17%     0.00%  arc_reclaim      [kernel.kallsyms]                     [k] ret_from_fork                                                                                                                                                                                 \u2592\r\n     ret_from_fork                                                                                                                                                                                                                                                             \u2592\r\n-   42.17%     0.00%  arc_reclaim      [kernel.kallsyms]                     [k] kthread                                                                                                                                                                                       \u2592\r\n     kthread                                                                                                                                                                                                                                                                   \u2592\r\n     ret_from_fork                                                                                                                                                                                                                                                             \u2592\r\n-   42.17%     0.00%  arc_reclaim      [spl]                                 [k] thread_generic_wrapper                                                                                                                                                                        \u2592\r\n     thread_generic_wrapper                                                                                                                                                                                                                                                    \u2592\r\n     kthread                                                                                                                                                                                                                                                                   \u2592\r\n     ret_from_fork                                                                                                                                                                                                                                                             \u2592\r\n-   42.17%     0.00%  arc_reclaim      [zfs]                                 [k] arc_reclaim_thread                                                                                                                                                                            \u2592\r\n     arc_reclaim_thread                                                                                                                                                                                                                                                        \u2592\r\n     thread_generic_wrapper                                                                                                                                                                                                                                                    \u2592\r\n     kthread                                                                                                                                                                                                                                                                   \u2592\r\n     ret_from_fork                                                                                                                                                                                                                                                             \u2592\r\n-   42.15%     0.00%  arc_reclaim      [zfs]                                 [k] arc_adjust                                                                                                                                                                                    \u2592\r\n     arc_adjust                                                                                                                                                                                                                                                                \u2592\r\n     arc_reclaim_thread                                                                                                                                                                                                                                                        \u2592\r\n     thread_generic_wrapper                                                                                                                                                                                                                                                    \u2592\r\n     kthread                                                                                                                                                                                                                                                                   \u2592\r\n     ret_from_fork                                                                                                                                                                                                                                                             \u2592\r\n-   41.92%     0.00%  arc_reclaim      [zfs]                                 [k] arc_adjust_impl.constprop.27                                                                                                                                                                  \u2592\r\n     arc_adjust_impl.constprop.27                                                                                                                                                                                                                                              \u2592\r\n     arc_adjust                                                                                                                                                                                                                                                                \u2592\r\n     arc_reclaim_thread                                                                                                                                                                                                                                                        \u2592\r\n     thread_generic_wrapper                                                                                                                                                                                                                                                    \u2592\r\n     kthread                                                                                                                                                                                                                                                                   \u2592\r\n     ret_from_fork                                                                                                                                                                                                                                                             \u2592\r\n-   36.81%    20.38%  arc_reclaim      [zfs]                                 [k] arc_evict_state                                                                                                                                                                               \u2592\r\n     arc_evict_state                                                                                                                                                                                                                                                           \u2592\r\n     arc_adjust_impl.constprop.27                                                                                                                                                                                                                                              \u2592\r\n     arc_adjust                                                                                                                                                                                                                                                                \u2592\r\n     arc_reclaim_thread                                                                                                                                                                                                                                                        \u2592\r\n     thread_generic_wrapper                                                                                                                                                                                                                                                    \u2592\r\n     kthread                                                                                                                                                                                                                                                                   \u2592\r\n     ret_from_fork                                                                                                                                                                                                                                                             \u2592\r\n+   13.93%     3.14%  python           python2.7                             [.] PyEval_EvalFrameEx                                                                                                                                                                            \u2592\r\n+   13.91%     0.03%  python           python2.7                             [.] PyEval_EvalCodeEx                                                                                                                                                                             \u2592\r\n+   12.25%     0.00%  python           python2.7                             [.] PyEval_EvalCode                                                                                                                                                                               \u2592\r\n+   11.69%     0.00%  python           python2.7                             [.] 0x00000000000b5322                                                                                                                                                                            \u2592\r\n+   11.64%     0.00%  python           python2.7                             [.] PyImport_ExecCodeModuleEx                                                                                                                                                                     \u2592\r\n+   11.61%     0.00%  python           python2.7                             [.] 0x00000000000c6ac6                                                                                                                                                                            \u2592\r\n+   11.32%     0.00%  python           python2.7                             [.] 0x00000000000b7bb6                                                                                                                                                                            \u2592\r\n+   11.10%     0.02%  python           python2.7                             [.] PyEval_CallObjectWithKeywords                                                                                                                                                                 \u2592\r\n-    8.93%     8.93%  arc_reclaim      [kernel.kallsyms]                     [k] mutex_trylock                                                                                                                                                                                 \u2592\r\n   - mutex_trylock                                                                                                                                                                                                                                                             \u2592\r\n      - 99.24% arc_evict_state                                                                                                                                                                                                                                                 \u2592\r\n           arc_adjust_impl.constprop.27                                                                                                                                                                                                                                        \u2592\r\n           arc_adjust                                                                                                                                                                                                                                                          \u2592\r\n           arc_reclaim_thread                                                                                                                                                                                                                                                  \u2592\r\n           thread_generic_wrapper                                                                                                                                                                                                                                              \u2592\r\n           kthread                                                                                                                                                                                                                                                             \u2592\r\n           ret_from_fork                                                                                                                                                                                                                                                       \u2592\r\n      - 0.76% arc_adjust_impl.constprop.27                                                                                                                                                                                                                                     \u2592\r\n           arc_adjust                                                                                                                                                                                                                                                          \u2592\r\n           arc_reclaim_thread                                                                                                                                                                                                                                                  \u2592\r\n           thread_generic_wrapper                                                                                                                                                                                                                                              \u2592\r\n           kthread                                                                                                                                                                                                                                                             \u2592\r\n           ret_from_fork                                                                                                                                                                                                                                                       \u2592\r\n+    8.09%     0.00%  python           python2.7                             [.] 0x000000000001ce39                                                                                                                                                                            \u2592\r\n+    5.71%     0.00%  python           python2.7                             [.] 0x00000000000b4ca3                                                                                                                                                                            \u2592\r\n-    5.45%     0.04%  mysqld           [kernel.kallsyms]                     [k] system_call                                                                                                                                                                                   \u2592\r\n   + system_call                                                                                                                                                                                                                                                               \u2592\r\n+    4.46%     4.46%  arc_reclaim      [kernel.kallsyms]                     [k] _raw_spin_lock\r\n```\r\n\r\n**CPU utilization stats from around the time of stall**\r\n```\r\n00:00:01  cpu  %usr %nice %sys %irq %softirq  %steal %guest  %wait %idle  _cpu_\r\n00:01:01  all    85     5   39    0        2       0      0      8  3061\r\n21:15:01  all    48     6 2733    0        1       0      0      3   409\r\n            0     1     0   86    0        0       0      0      0    12\r\n            1     2     1   71    0        0       0      0      0    25\r\n            2     3     0   87    0        0       0      0      0    10\r\n            3     1     0   87    0        0       0      0      0    11\r\n            4     7     1   82    0        0       0      0      0    10\r\n            5     2     0   87    0        0       0      0      0    11\r\n            6     2     1   87    0        0       0      0      0    10\r\n            7     2     0   86    0        0       0      0      0    11\r\n            8     2     0   88    0        0       0      0      0    10\r\n            9     2     0   78    0        0       0      0      0    20\r\n           10     2     0   88    0        0       0      0      0    11\r\n           11     3     0   84    0        0       0      0      0    13\r\n           12     1     1   85    0        0       0      0      0    13\r\n           13     2     0   88    0        0       0      0      0    11\r\n           14     2     0   88    0        0       0      0      0    10\r\n           15     1     0   88    0        0       0      0      0    10\r\n           16     1     0   88    0        0       0      0      0    11\r\n           17     1     0   86    0        0       0      0      0    13\r\n           18     1     0   87    0        0       0      0      0    12\r\n           19     0     0   87    0        0       0      0      0    13\r\n           20     0     0   87    0        0       0      0      0    12\r\n           21     1     0   87    0        0       0      0      0    13\r\n           22     0     0   87    0        0       0      0      0    12\r\n           23     0     0   87    0        0       0      0      0    13\r\n           24     0     0   87    0        0       0      0      0    12\r\n           25     1     0   88    0        0       0      0      0    11\r\n           26     1     0   87    0        0       0      0      0    12\r\n           27     2     1   85    0        0       0      0      0    12\r\n           28     1     0   85    0        0       0      0      0    14\r\n           29     2     0   86    0        0       0      0      0    13\r\n           30     3     0   85    0        0       0      0      0    12\r\n           31     1     0   75    0        0       0      0      0    24\r\n21:16:01  all    79     6 1628    0        2       0      0     16  1469\r\n           0     4     0   52    0        1       0      0      1    41\r\n           1     3     0   51    0        0       0      0      0    46\r\n           2     5     0   53    0        0       0      0      2    40\r\n           3     3     0   52    0        0       0      0      0    45\r\n           4     5     1   52    0        0       0      0      1    42\r\n           5     2     0   51    0        0       0      0      0    47\r\n           6     5     0   47    0        0       0      0      1    46\r\n           7     3     0   50    0        0       0      0      0    46\r\n           8     5     0   52    0        0       0      0      1    42\r\n           9     1     2   51    0        0       0      0      0    46\r\n          10     4     0   53    0        0       0      0      1    42\r\n          11     2     0   51    0        0       0      0      0    46\r\n          12     4     0   47    0        0       0      0      1    48\r\n          13     2     0   52    0        0       0      0      0    45\r\n          14     5     0   53    0        0       0      0      1    41\r\n          15     2     1   52    0        0       0      0      0    46\r\n          16     3     0   50    0        0       0      0      1    46\r\n          17     2     0   51    0        0       0      0      0    47\r\n          18     2     0   52    0        0       0      0      1    46\r\n          19     2     0   49    0        0       0      0      0    48\r\n          20     2     0   52    0        0       0      0      1    45\r\n          21     1     0   51    0        0       0      0      0    48\r\n          22     2     0   51    0        0       0      0      1    46\r\n          23     0     0   51    0        0       0      0      0    49\r\n          24     2     0   51    0        0       0      0      1    46\r\n          25     1     0   52    0        0       0      0      0    48\r\n          26     2     0   51    0        0       0      0      0    46\r\n          27     1     0   51    0        0       0      0      0    48\r\n          28     2     0   44    0        0       0      0      1    54\r\n          29     1     0   52    0        0       0      0      0    48\r\n          30     2     0   50    0        0       0      0      0    47\r\n          31     0     1   51    0        0       0      0      0    48\r\n```\r\n\r\n**Memory stats from during the stall**\r\n```\r\n21:00:01  memtotal memfree buffers cached dirty slabmem  swptotal swpfree _mem_\r\n21:15:01   257843M 205334M    342M 10073M    0M   6413M        0M      0M\r\n21:16:01   257843M 205506M    342M 10073M    0M   6289M        0M      0M\r\n21:17:01   257843M 205500M    342M 10073M    0M   6287M        0M      0M\r\n21:18:01   257843M 204808M    342M 10074M    0M   7004M        0M      0M\r\n22:04:01   257843M 204185M    342M 10102M    0M   6984M        0M      0M\r\n22:05:01   257843M 204579M    342M 10102M    0M   6983M        0M      0M\r\n22:06:01   257843M 205284M    342M 10102M    0M   6485M        0M      0M\r\n22:07:01   257843M 205335M    342M 10102M    0M   6466M        0M      0M\r\n22:08:01   257843M 205270M    342M 10103M    0M   6463M        0M      0M\r\n```\r\n\r\n**Disk stats from during the stall**\r\n```\r\n21:15:01  disk           busy read/s KB/read  writ/s KB/writ avque avserv _dsk_\r\n21:16:01  sda              2%  109.6     9.4   282.2     6.3   1.1   0.06 ms\r\n          sdb              3%  111.0     9.4   313.9     5.8   1.0   0.06 ms\r\n          sdc              3%  105.0     9.3   360.8     5.2   1.1   0.06 ms\r\n          sdd              3%  105.7     9.1   284.4     6.6   1.1   0.07 ms\r\n          sde              3%  115.0     9.2   271.4     6.8   1.1   0.07 ms\r\n21:17:01  sda              2%   65.1     9.2   231.0    11.7   1.0   0.08 ms\r\n          sdb              2%   62.6     9.1   265.6    10.1   1.0   0.07 ms\r\n          sdc              3%   63.6     9.2   351.6     7.6   1.0   0.06 ms\r\n          sdd              2%   67.5     9.3   238.3    11.8   1.0   0.07 ms\r\n          sde              2%   71.1     9.3   209.0    13.0   1.0   0.09 ms\r\n22:05:01  disk           busy read/s KB/read  writ/s KB/writ avque avserv _dsk_\r\n22:06:01  sda              1%   35.3     8.7    45.8    15.9   1.1   0.11 ms\r\n          sdb              1%   29.7     8.1    59.9    11.1   1.0   0.06 ms\r\n          sdc              1%   32.4     8.0    69.9    10.2   1.0   0.08 ms\r\n          sdd              1%   32.0     8.1    68.5    10.0   1.0   0.08 ms\r\n          sde              1%   31.8     8.1    35.6    20.6   1.0   0.09 ms\r\n22:07:01  sda              3%  116.8     9.3   311.9     7.2   1.0   0.06 ms\r\n          sdb              3%  125.9     9.3   330.2     6.5   1.1   0.07 ms\r\n          sdc              3%  132.2     9.3   357.4     6.0   1.0   0.06 ms\r\n          sdd              4%  131.8     9.5   413.8     5.4   1.0   0.06 ms\r\n          sde              3%  124.7     9.3   287.9     7.7   1.1   0.07 ms\r\n```\r\n\r\n**Below are some other perf report output from different hosts that show the same issue**\r\n```\r\nSamples: 512K of event 'cycles', Event count (approx.): 857895049913\r\nOverhead  Command          Shared Object                         Symbol\r\n  56.05%  mysqld           [kernel.kallsyms]                     [k] _raw_spin_lock_irqsave\r\n   1.90%  swapper          [kernel.kallsyms]                     [k] intel_idle\r\n   1.01%  opsless-agent    [kernel.kallsyms]                     [k] _raw_spin_lock_irqsave\r\n   0.80%  mysqld           [kernel.kallsyms]                     [k] up_read\r\n   0.71%  mysqld           [kernel.kallsyms]                     [k] down_read\r\n   0.69%  mysqld           [zfs]                                 [k] dnode_hold_impl\r\n   0.62%  z_wr_iss         [kernel.kallsyms]                     [k] _raw_spin_lock_irqsave\r\n   0.56%  mysqld           [kernel.kallsyms]                     [k] _raw_spin_lock\r\n   0.56%  z_wr_iss         [zfs]                                 [k] lz4_compress_zfs\r\n   0.50%  mysqld           [kernel.kallsyms]                     [k] mutex_lock\r\n   0.43%  python           [kernel.kallsyms]                     [k] system_call\r\n   0.42%  mysqld           [kernel.kallsyms]                     [k] mutex_unlock\r\n   0.35%  mysqld           [kernel.kallsyms]                     [k] _raw_spin_unlock_irqrestore\r\n   0.33%  mysqld           [zfs]                                 [k] zrl_add\r\n   0.27%  mysqld           mysqld                                [.] 0x0000000000746563\r\n```\r\n\r\n```\r\nSamples: 959K of event 'cycles', Event count (approx.): 2203503838470\r\nOverhead  Command          Shared Object                  Symbol\r\n  81.44%  mysqld           [kernel.kallsyms]              [k] _raw_spin_lock_irqsave\r\n   2.86%  opsless-agent    [kernel.kallsyms]              [k] _raw_spin_lock_irqsave\r\n   1.08%  mysqld           [kernel.kallsyms]              [k] up_read\r\n   0.95%  mysqld           [kernel.kallsyms]              [k] down_read\r\n   0.94%  mysqld           [zfs]                          [k] dnode_hold_impl\r\n   0.47%  mysqld           [kernel.kallsyms]              [k] _raw_spin_lock\r\n   0.41%  mysqld           [kernel.kallsyms]              [k] _raw_spin_unlock_irqrestore\r\n   0.40%  mysqld           [kernel.kallsyms]              [k] mutex_unlock\r\n   0.39%  mysqld           [zfs]                          [k] zrl_add\r\n   0.37%  mysqld           [kernel.kallsyms]              [k] mutex_lock\r\n   0.27%  mysqld           [zfs]                          [k] dbuf_read\r\n   0.17%  mysqld           [zavl]                         [k] avl_find\r\n   0.13%  mysqld           [zfs]                          [k] zrl_remove\r\n   0.13%  python           [kernel.kallsyms]              [k] system_call\r\n   0.11%  mysqld           [zavl]                         [k] avl_remove\r\n   0.11%  mysqld           [kernel.kallsyms]              [k] memset\r\n   0.11%  mysqld           [zfs]                          [k] zfs_znode_hold_compare\r\n   0.11%  mysqld           [zfs]                          [k] zfs_znode_hold_exit\r\n   0.10%  python           python2.7                      [.] PyEval_EvalFrameEx\r\n   0.09%  z_wr_iss         [zfs]                          [k] lz4_compress_zfs\r\n```\r\n\r\n```\r\nSamples: 959K of event 'cycles', Event count (approx.): 2204173457265\r\nOverhead  Command          Shared Object           Symbol\r\n  84.97%  mysqld           [kernel.kallsyms]       [k] _raw_spin_lock_irqsave\r\n   2.83%  opsless-agent    [kernel.kallsyms]       [k] _raw_spin_lock_irqsave\r\n   1.09%  mysqld           [kernel.kallsyms]       [k] up_read\r\n   0.94%  mysqld           [zfs]                   [k] dnode_hold_impl\r\n   0.93%  mysqld           [kernel.kallsyms]       [k] down_read\r\n   0.47%  mysqld           [kernel.kallsyms]       [k] _raw_spin_lock\r\n   0.42%  mysqld           [kernel.kallsyms]       [k] mutex_unlock\r\n   0.41%  mysqld           [kernel.kallsyms]       [k] _raw_spin_unlock_irqrestore\r\n   0.39%  mysqld           [zfs]                   [k] zrl_add\r\n   0.38%  mysqld           [kernel.kallsyms]       [k] mutex_lock\r\n   0.26%  mysqld           [zfs]                   [k] dbuf_read\r\n   0.17%  mysqld           [zavl]                  [k] avl_find\r\n   0.12%  mysqld           [zfs]                   [k] zrl_remove\r\n   0.12%  mysqld           [zfs]                   [k] zfs_znode_hold_compare\r\n   0.12%  mysqld           [zavl]                  [k] avl_remove\r\n   0.11%  mysqld           [kernel.kallsyms]       [k] memset\r\n   0.10%  mysqld           [zfs]                   [k] zfs_znode_hold_exit\r\n   0.09%  z_wr_iss         [zfs]                   [k] lz4_compress_zfs\r\n\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6187/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thegreatgazoo": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6157", "title": "More stringent checks in zfs_blkptr_verify()", "body": "zfs_blkptr_verify() already checks PSIZE is no more than SPA_MAXBLOCKSIZE. But for embedded BPs, PSIZE cannot be over BPE_PAYLOAD_SIZE. We can add such a check for embedded BPs.\r\n\r\nAlso, in the DVA checking part, _offset + asize > vd->vdev_asize_ can be made more stringent by _offset >> vd->vdev_ms_shift == offset + asize - 1 >> vd->vdev_ms_shift_, i.e. the DVA cannot go over metaslab boundary.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6157/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/eea2e2413220b34a3da0911dee554df20bdd0c47", "message": "Use linear abd in vdev_copy_uberblocks()\n\nThe vdev_copy_uberblocks() function should use abd_alloc_linear() to\r\nallocate ub_abd, because abd_to_buf(ub_abd)) is used later.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Isaac Huang <he.huang@intel.com>\r\nCloses #6718 \r\nCloses #6713"}], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/5841", "title": "dRAID vdev driver", "body": "This patch implements the dRAID vdev driver and a new rebuild mechanism (#3497). Most features are complete, except:\r\n- Rebuild stop and resume, and persistant rebuild state.\r\n- Support for HW-accelerated parity routines: The dRAID code uses the raidz parity functions (generation, and reconstruction) but needs a small change. Currently the change has been made only to the original raidz parity functions, i.e. not the new HW-accelerated ones.\r\n\r\nHowever, this is still work in progress: user interface may change, on-disk format may change as well. Also, there's still some crufty hacks I'm going to clean up.\r\n\r\nI've added a [dRAID howto](https://github.com/zfsonlinux/zfs/wiki/dRAID-HOWTO). It contains only basics for now, but I'll continue to update the document.\r\n\r\nPlease report bugs to [the dRAID project](https://github.com/thegreatgazoo/zfs/issues).\r\n\r\nComments, testing, fixes, and porting are greatly appreciated!\r\n\r\nCode structure:\r\n- New code\r\n  - module/zfs/vdev_draid.c: vdev driver for draid and draid spare\r\n  - module/zfs/spa_scan.c: sequential rebuild, for both draid and mirror vdevs\r\n  - cmd/draidcfg/*.[ch]: user space tools, mainly to create permutations\r\n- Changes to existing code\r\n  - module/zfs/vdev_raidz.c: the parity functions need to include draid skip sectors for computation and reconstruction.\r\n  - module/zfs/vdev_mirror.c: minor changes to support mirror_map_t allocated by draid vdev (for hybrid mirror support)\r\n  - module/zfs/metaslab.c: to add support for draid hybrid mirror, also disallow block allocation during rebuild\r\n  - Other changes:\r\n    - Add knowledge about the new vdev types and the new rebuild mechanism\r\n    - draid spare pretends to be a leaf but is actually not. Some code needs to be aware of that, e.g. handling labels on leaf vdevs.\r\n\r\nThe goal is to change existing code in a way that when draid is not in use the effective change is none. Though there's still some cleanups needed.", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/5153", "title": "Sequential SPA-level rebuild for mirror and dRAID", "body": "This patch implements SPA-level rebuild for mirror and the upcoming dRAID (#3497) vdev's. It works by scanning the metaslabs and rebuilding each used segment of DVA. The redundancy layout (i.e. where the copies, data/parity blocks are) must be determined by the DVA only. That's why it does not work with raidz. Since the scan happens at SPA/metaslab level, it doesn't traverse the block pointer tree. The downside is that block checksum cannot be verified during the scan. The upside is that the IO is perfectly sequential and IO size is often larger than block size. Note that this is different from #3625, which is still resilver.\n\nThe following tests compared rebuild and resilver on the same hardware populated with exactly the same data:\n\n```\n# modprobe zfs zfs_vdev_scrub_min_active=8 zfs_vdev_scrub_max_active=10 zfs_vdev_async_write_min_active=10\n# zpool create -f tank mirror sdc sde\n# for i in `seq 1 20`; do cat ubuntu-14.04.4-desktop-amd64.iso > /tank/$i.iso; done\n# tar xf linux-4.7.3.tar.xz -C /tank/\n# tar xf linux-4.5.tar.xz -C /tank/\n# df -h /tank/\nFilesystem      Size  Used Avail Use% Mounted on\ntank            1.8T   22G  1.8T   2% /tank\n# zpool attach -f tank sdc sdg\n# zpool status\n  pool: tank\n state: ONLINE\n  scan: rebuilt 21.5G in 0h1m46s with 0 errors on Thu Sep 22 19:57:46 2016\nconfig: \n\n        NAME        STATE     READ WRITE CKSUM\n        tank        ONLINE       0     0     0\n          mirror-0  ONLINE       0     0     0\n            sdc     ONLINE       0     0     0\n            sde     ONLINE       0     0     0\n            sdg     ONLINE       0     0     0\n\nerrors: No known data errors\n# zpool export tank\n# zpool create -f tank mirror sdc sde\n# for i in `seq 1 20`; do cat ubuntu-14.04.4-desktop-amd64.iso > /tank/$i.iso; done\n# tar xf linux-4.7.3.tar.xz -C /tank/\n# tar xf linux-4.5.tar.xz -C /tank/\n# df -h /tank/\nFilesystem      Size  Used Avail Use% Mounted on\ntank            1.8T   22G  1.8T   2% /tank\n# echo 0 > /sys/module/zfs/parameters/vdev_draid_max_rebuild # Disable Rebuild\n# zpool attach -f tank sdc sdg\n# zpool status\n  pool: tank\n state: ONLINE\n  scan: resilvered 21.5G in 0h3m0s with 0 errors on Thu Sep 22 20:05:18 2016\nconfig: \n\n        NAME        STATE     READ WRITE CKSUM\n        tank        ONLINE       0     0     0\n          mirror-0  ONLINE       0     0     0\n            sdc     ONLINE       0     0     0\n            sde     ONLINE       0     0     0\n            sdg     ONLINE       0     0     0\n\nerrors: No known data errors\n```\n\nThe rebuild took 106 seconds while the resilver took 180 seconds:\n- scan: **rebuilt** 21.5G in 0h1m46s with 0 errors on Thu Sep 22 19:57:46 2016\n- scan: **resilvered** 21.5G in 0h3m0s with 0 errors on Thu Sep 22 20:05:18 2016\n\nThe resilver took about 70% more time to complete. Note that this is a very ideal workload for resilver: new and nearly empty pool, mostly large files. But rebuild is sequential by design and is not that allergic to aging, fragmentation, or file sizes. So this test actually favors resilver.\n\nThe following two pictures shows graphics of IO data during the rebuild and the resilver:\n![rebuild](https://cloud.githubusercontent.com/assets/6722662/18766970/f4482f10-80d9-11e6-96c2-2af0c404dc9f.png)\n![resilver](https://cloud.githubusercontent.com/assets/6722662/18766969/f4446768-80d9-11e6-8d04-2df9e1ecc2b1.png)\nIn each picture, there are 4 graphs:\n- X axis shows wall clock time. Each graph shows 120 seconds of IO data\n- The 1st graph shows read bw in MB/s for the individual drives\n- The 2nd graph shows write bw in MB/s for the individual drives\n- The 3rd and 4th graphs show average read/write request sizes in KB, respectively.\n\nThe most interesting part is the 3rd and 4th graphs showing average IO request sizes. For rebuild, the r/w request sizes averaged at about 1M, but for resilver the sizes were at around 128K, which is the _recordsize_. This is because rebuild scans metaslabs, and fixes used segments each of which may contain many blocks.\n\nComments and suggestions are very welcome. The code at this point is rough and hacky. It may panic and/or destroy your data. That said, I was able to test rebuilds successfully, and scrub found no errors.\n\nHow to use:\n1. Apply this patch.\n2. Then all attach and replace operations of a mirror vdev will use the sequential rebuild mechanism. To use resilver instead: `echo 0 > /sys/module/zfs/parameters/vdev_draid_max_rebuild`\n", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10502715", "body": "The default value is now 50 \\* 1024 \\* 1024.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10502715/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/9854012", "body": "If we return from here, nvl and cb hasn't been saved in a zevent_t. In this case, how does the caller know when to free nvl? Looking at call sites, it seemed that nvl would be freed in the cb callback, but since cb is not saved it appeared to me that cb would not be called. I'm new to the code, so I guess I must have missed something here.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/9854012/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/9854061", "body": "So cb is saved in ev->ev_cb and called later in zfs_zevent_free():\n/\\* Run provided cleanup callback */\nev->ev_cb(ev->ev_nvl, ev->ev_detector);\n\nBut it seemed OK to call zfs_zevent_post(cb==NULL):\nzfs_ereport_send_interim_checksum(zio_cksum_report_t *report)  \n{\n#ifdef _KERNEL\n        zfs_zevent_post(report->zcr_ereport, report->zcr_detector, NULL);\n\nThen would zfs_zevent_free() dereference a NULL pointer when calling ev_cb without checking it against NULL?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/9854061/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/9855334", "body": "OK, will open an issue and come up with a patch.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/9855334/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "serg-ku": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6156", "title": "unexpected IO and/or resilver restarts without any errors or warnings after zpool layout changes", "body": "\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  CentOS\r\nDistribution Version    |  7.3\r\nLinux Kernel                 |  3.10.0-514.16.1.el7.x86_64\r\nArchitecture                 |  x86_64\r\nZFS Version                  |  0.7.0-rc4\r\nSPL Version                  |  0.7.0-rc4\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nThis is \"SSD only\" test system, I was trying to replace zpool mirror devices from partitions (~210GiB, located on sys SSDs) with separate SSD (~220GiB, they were brand new, empty, just inserted in hotswap slots)\r\nThere was only 1 mirror vdev in pool, both devices was ONLINE, then I issued commands:\r\n```\r\nzpool set autoexpand=on ssd1\r\nzpool replace -o ashift=12 ssd1 wwn-0x5002538c40396e3d-part2 wwn-0x5002538c40396d69\r\n```\r\nAnd tried to watch zpool status, but every invocation of zpool status was hanging 5-10 secs.\r\nIt was saying, that resilver in progress at 249M/s, but at 3-4G scanned it was always restarted, approx 15s between restarts(It can kill SSD very fast with such speeds and restarts).\r\nIn iostat src drives were reading ~150MB/s + ~60MB/s and dst drive writing ~230MB/s\r\nI've decided to offline new ssd with:\r\n```\r\nzpool offline ssd1 wwn-0x5002538c40396d69\r\n```\r\nAccording to events device went to offline and immeadetly to online, after that zpool status started to work fast and resilver restarted and ended without problem.\r\nSame behavior was with second replacement, also offline helped, this is very strange.\r\nThere was NO errors in dmesg, and messages/events.\r\nI've found open issues for restarting resilver only with IO errors and dRAID, but this is another situation I think.\r\n\r\n### Describe how to reproduce the problem\r\nTry to replace partition in mirror vdev with full device.\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\nHere you can see restarts and device offline:\r\n```\r\nMay 23 2017 17:20:18.204124796 sysevent.fs.zfs.vdev_attach\r\nMay 23 2017 17:20:18.224124439 sysevent.fs.zfs.resilver_start\r\nMay 23 2017 17:20:21.253070341 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:20:33.385853647 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:20:33.387853611 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:20:36.440799085 sysevent.fs.zfs.resilver_start\r\nMay 23 2017 17:20:45.511637078 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:20:45.512637060 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:20:48.535583069 sysevent.fs.zfs.resilver_start\r\nMay 23 2017 17:20:57.610420991 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:20:57.612420955 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:21:05.688276718 sysevent.fs.zfs.resilver_start\r\nMay 23 2017 17:21:14.754114800 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:21:14.755114782 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:21:19.802024643 sysevent.fs.zfs.resilver_start\r\nMay 23 2017 17:21:31.914808306 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:21:31.916808271 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:21:39.998663926 sysevent.fs.zfs.resilver_start\r\nMay 23 2017 17:21:49.100501366 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:21:49.102501330 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:21:52.122447393 sysevent.fs.zfs.resilver_start\r\n\r\n...\r\n\r\nMay 23 2017 17:29:34.329067375 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:29:34.331067339 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:29:37.397011685 sysevent.fs.zfs.resilver_start\r\nMay 23 2017 17:29:46.471846956 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:29:46.471846956 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:29:51.522755270 resource.fs.zfs.statechange\r\nMay 23 2017 17:29:51.557754635 resource.fs.zfs.statechange\r\nMay 23 2017 17:29:51.571754381 sysevent.fs.zfs.vdev_online\r\nMay 23 2017 17:29:54.593699525 sysevent.fs.zfs.resilver_start\r\nMay 23 2017 17:30:03.659534960 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:30:03.660534942 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:30:03.662534905 sysevent.fs.zfs.config_sync\r\nMay 23 2017 17:30:08.681443801 sysevent.fs.zfs.resilver_start\r\nMay 23 2017 17:32:11.151244943 sysevent.fs.zfs.resilver_finish\r\nMay 23 2017 17:32:11.173244551 sysevent.fs.zfs.vdev_remove\r\nMay 23 2017 17:32:11.182244391 sysevent.fs.zfs.config_sync\r\n\r\n```\r\n\r\nMore verbose:\r\n```\r\nMay 23 2017 17:29:34.329067375 sysevent.fs.zfs.config_sync\r\n        version = 0x0\r\n        class = \"sysevent.fs.zfs.config_sync\"\r\n        pool = \"ssd1\"\r\n        pool_guid = 0x7607cb2bb1356617\r\n        pool_state = 0x0\r\n        pool_context = 0x0\r\n        time = 0x5924474e 0x139d2b6f \r\n        eid = 0x6e\r\n\r\nMay 23 2017 17:29:34.331067339 sysevent.fs.zfs.config_sync\r\n        version = 0x0\r\n        class = \"sysevent.fs.zfs.config_sync\"\r\n        pool = \"ssd1\"\r\n        pool_guid = 0x7607cb2bb1356617\r\n        pool_state = 0x0\r\n        pool_context = 0x0\r\n        time = 0x5924474e 0x13bbafcb \r\n        eid = 0x6f\r\n\r\nMay 23 2017 17:29:37.397011685 sysevent.fs.zfs.resilver_start\r\n        version = 0x0\r\n        class = \"sysevent.fs.zfs.resilver_start\"\r\n        pool = \"ssd1\"\r\n        pool_guid = 0x7607cb2bb1356617\r\n        pool_state = 0x0\r\n        pool_context = 0x0\r\n        time = 0x59244751 0x17a9eae5 \r\n        eid = 0x70\r\n\r\nMay 23 2017 17:29:46.471846956 sysevent.fs.zfs.config_sync\r\n        version = 0x0\r\n        class = \"sysevent.fs.zfs.config_sync\"\r\n        pool = \"ssd1\"\r\n        pool_guid = 0x7607cb2bb1356617\r\n        pool_state = 0x0\r\n        pool_context = 0x0\r\n        time = 0x5924475a 0x1c1fd02c \r\n        eid = 0x71\r\n\r\nMay 23 2017 17:29:46.471846956 sysevent.fs.zfs.config_sync\r\n        version = 0x0\r\n        class = \"sysevent.fs.zfs.config_sync\"\r\n        pool = \"ssd1\"\r\n        pool_guid = 0x7607cb2bb1356617\r\n        pool_state = 0x0\r\n        pool_context = 0x0\r\n        time = 0x5924475a 0x1c1fd02c \r\n        eid = 0x72\r\n\r\nMay 23 2017 17:29:51.522755270 resource.fs.zfs.statechange\r\n        version = 0x0\r\n        class = \"resource.fs.zfs.statechange\"\r\n        pool = \"ssd1\"\r\n        pool_guid = 0x7607cb2bb1356617\r\n        pool_state = 0x0\r\n        pool_context = 0x0\r\n        vdev_guid = 0x4d155a09ce8c6d60\r\n        vdev_state = \"OFFLINE\" (0x2)\r\n        vdev_path = \"/dev/disk/by-id/wwn-0x5002538c40396d69-part1\"\r\n        vdev_devid = \"ata-SAMSUNG_MZ7LM240HCGR-00003_S1YFNX0H701741-part1\"\r\n        vdev_physpath = \"pci-0000:00:1f.2-ata-3.0\"\r\n        vdev_laststate = \"ONLINE\" (0x7)\r\n        time = 0x5924475f 0x1f289cc6 \r\n        eid = 0x73\r\n\r\nMay 23 2017 17:29:51.557754635 resource.fs.zfs.statechange\r\n        version = 0x0\r\n        class = \"resource.fs.zfs.statechange\"\r\n        pool = \"ssd1\"\r\n        pool_guid = 0x7607cb2bb1356617\r\n        pool_state = 0x0\r\n        pool_context = 0x0\r\n        vdev_guid = 0x4d155a09ce8c6d60\r\n        vdev_state = \"ONLINE\" (0x7)\r\n        vdev_path = \"/dev/disk/by-id/wwn-0x5002538c40396d69-part1\"\r\n        vdev_devid = \"ata-SAMSUNG_MZ7LM240HCGR-00003_S1YFNX0H701741-part1\"\r\n        vdev_physpath = \"pci-0000:00:1f.2-ata-3.0\"\r\n        vdev_laststate = \"OFFLINE\" (0x2)\r\n        time = 0x5924475f 0x213ea90b \r\n        eid = 0x74\r\n\r\nMay 23 2017 17:29:51.571754381 sysevent.fs.zfs.vdev_online\r\n        version = 0x0\r\n        class = \"sysevent.fs.zfs.vdev_online\"\r\n        pool = \"ssd1\"\r\n        pool_guid = 0x7607cb2bb1356617\r\n        pool_state = 0x0\r\n        pool_context = 0x0\r\n        vdev_guid = 0x4d155a09ce8c6d60\r\n        vdev_state = \"ONLINE\" (0x7)\r\n        vdev_path = \"/dev/disk/by-id/wwn-0x5002538c40396d69-part1\"\r\n        vdev_devid = \"ata-SAMSUNG_MZ7LM240HCGR-00003_S1YFNX0H701741-part1\"\r\n        time = 0x5924475f 0x2214478d \r\n        eid = 0x75\r\n\r\nMay 23 2017 17:29:54.593699525 sysevent.fs.zfs.resilver_start\r\n        version = 0x0\r\n        class = \"sysevent.fs.zfs.resilver_start\"\r\n        pool = \"ssd1\"\r\n        pool_guid = 0x7607cb2bb1356617\r\n        pool_state = 0x0\r\n        pool_context = 0x0\r\n        time = 0x59244762 0x236322c5 \r\n        eid = 0x76\r\n\r\nMay 23 2017 17:30:03.659534960 sysevent.fs.zfs.config_sync\r\n        version = 0x0\r\n        class = \"sysevent.fs.zfs.config_sync\"\r\n        pool = \"ssd1\"\r\n        pool_guid = 0x7607cb2bb1356617\r\n        pool_state = 0x0\r\n        pool_context = 0x0\r\n        time = 0x5924476b 0x274fb470 \r\n        eid = 0x77\r\n\r\nMay 23 2017 17:30:03.660534942 sysevent.fs.zfs.config_sync\r\n        version = 0x0\r\n        class = \"sysevent.fs.zfs.config_sync\"\r\n        pool = \"ssd1\"\r\n        pool_guid = 0x7607cb2bb1356617\r\n        pool_state = 0x0\r\n        pool_context = 0x0\r\n        time = 0x5924476b 0x275ef69e \r\n        eid = 0x78\r\n\r\nMay 23 2017 17:30:03.662534905 sysevent.fs.zfs.config_sync\r\n        version = 0x0\r\n        class = \"sysevent.fs.zfs.config_sync\"\r\n        pool = \"ssd1\"\r\n        pool_guid = 0x7607cb2bb1356617\r\n        pool_state = 0x0\r\n        pool_context = 0x0\r\n        time = 0x5924476b 0x277d7af9 \r\n        eid = 0x79\r\n\r\nMay 23 2017 17:30:08.681443801 sysevent.fs.zfs.resilver_start\r\n        version = 0x0\r\n        class = \"sysevent.fs.zfs.resilver_start\"\r\n        pool = \"ssd1\"\r\n        pool_guid = 0x7607cb2bb1356617\r\n        pool_state = 0x0\r\n        pool_context = 0x0\r\n        time = 0x59244770 0x289e01d9 \r\n        eid = 0x7a\r\n\r\nMay 23 2017 17:32:11.151244943 sysevent.fs.zfs.resilver_finish\r\n        version = 0x0\r\n        class = \"sysevent.fs.zfs.resilver_finish\"\r\n        pool = \"ssd1\"\r\n        pool_guid = 0x7607cb2bb1356617\r\n        pool_state = 0x0\r\n        pool_context = 0x0\r\n        time = 0x592447eb 0x903d08f \r\n        eid = 0x7b\r\n\r\nMay 23 2017 17:32:11.173244551 sysevent.fs.zfs.vdev_remove\r\n        version = 0x0\r\n        class = \"sysevent.fs.zfs.vdev_remove\"\r\n        pool = \"ssd1\"\r\n        pool_guid = 0x7607cb2bb1356617\r\n        pool_state = 0x0\r\n        pool_context = 0x0\r\n        vdev_guid = 0x222fb09dae7c1011\r\n        vdev_state = \"ONLINE\" (0x7)\r\n        vdev_path = \"/dev/disk/by-id/wwn-0x5002538c40396e3d-part2\"\r\n        time = 0x592447eb 0xa538087 \r\n        eid = 0x7c\r\n```\r\n\r\nzpool status for second disk with the same problem:\r\n```\r\n#time zpool status\r\npool: ssd1\r\n state: ONLINE\r\nstatus: One or more devices is currently being resilvered.  The pool will\r\n        continue to function, possibly in a degraded state.\r\naction: Wait for the resilver to complete.\r\n  scan: resilver in progress since Tue May 23 17:39:42 2017\r\n        2.19G scanned out of 28.7G at 249M/s, 0h1m to go\r\n        2.19G resilvered, 7.64% done\r\nconfig:\r\n\r\n        NAME                                STATE     READ WRITE CKSUM\r\n        ssd1                                ONLINE       0     0     0\r\n          mirror-0                          ONLINE       0     0     0\r\n            wwn-0x5002538c40396d69          ONLINE       0     0     0\r\n            replacing-1                     ONLINE       0     0     0\r\n              wwn-0x5002538c40396e44-part2  ONLINE       0     0     0\r\n              wwn-0x5002538c40396d63        ONLINE       0     0     0  (resilvering)\r\n\r\nerrors: No known data errors\r\n\r\nreal    0m10.335s\r\nuser    0m0.002s\r\nsys     0m0.005s\r\n```\r\n\r\niostat -xk 1:\r\n```\r\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\r\nsda               0.00     0.00 2407.00  129.00 157256.00  1928.00   125.54     0.84    0.33    0.34    0.12   0.22  56.70\r\nsdb               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\r\nsdc               0.00     0.00  956.00  123.00 61912.00  1928.00   118.33     0.27    0.25    0.27    0.10   0.23  24.60\r\nsdd               0.00     2.00    0.00 4912.00     0.00 231856.00    94.40     1.73    0.35    0.00    0.35   0.18  89.60\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6156/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nonooo": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6155", "title": "Zpool history not working.", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Debian jessie (XEN dom0) \r\nDistribution Version    | jessie fully uptodate. \r\nLinux Kernel                 | #1 SMP Debian 3.16.43-2 (2017-04-30         \r\nArchitecture                 | 64 bits\r\nZFS Version                  | 0.6.5.9-2~bpo8+1         \r\nSPL Version                  | 0.6.5.9-1~bpo8+1  \r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nI can't zpool history.\r\n\r\n### Describe how to reproduce the problem\r\n```\r\nroot@g8server:/usr/share/grub# zpool  history patapouf\r\nHistory for 'patapouf':\r\nzpool: libzfs_pool.c :3853 :  l'assertion \u00ab nvlist_add_nvlist_array(*nvhisp, ZPOOL_HIST_RECORD, records, numrecords) == 0 \u00bb a \u00e9chou\u00e9.\r\nAbandon  \r\n```\r\n\r\nplease let me know if you need more info.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Bronek": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6149", "title": "soft lock and RCU stall in 0.7.0-rc4", "body": "Type                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Arch\r\nLinux Kernel                 |  4.9.28\r\nArchitecture                 |  x64\r\nZFS Version                  |  0.7.0-rc4\r\nSPL Version                  |  0.7.0-rc4\r\n\r\nThis might be bug in ZOL, or in kernel itself.\r\n\r\nMy computer reported soft lock when starting from cold state. I captured as much information as I could think of, via serial console. This has happened first time and I suspect it is unlikely to happen again soon.\r\n\r\nFor actual soft lock do `grep \"BUG: soft lockup\" bug.txt` , for example: \r\n\r\n```\r\n[  108.737749] NMI watchdog: BUG: soft lockup - CPU#2 stuck for 23s! [libvirtd:10467]\r\n[  108.745321] Modules linked in: ebtable_filter ebtables ip6table_filter ip6_tables xt_multiport iptable_filter nls_iso8859_1 nls_cp437 joydev vfat mousedev mxm_wmi ext4 fat crc16 jbd2 fscrypto mbcache intel_rapl sb_edac edac_core x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm crct10dif_pclmul crc32_pclmul crc32c_intel ghash_clmulni_intel aesni_intel aes_x86_64 lrw gf128mul glue_helper ablk_helper cryptd evdev intel_cstate igb input_leds led_class ptp intel_rapl_perf mac_hid pcspkr pps_core i2c_algo_bit mei_me hid_generic ioatdma i2c_i801 mei i2c_smbus shpchp lpc_ich dca wmi fjes tpm_tis tpm_tis_core tpm button sch_fq_codel ip_tables x_tables sd_mod usbhid hid zfs(PO) zunicode(PO) zavl(PO) icp(PO) zcommon(PO) znvpair(PO) spl(O) isci ahci libsas libahci ehci_pci xhci_pci serio_raw mpt3sas xhci_hcd atkbd ehci_hcd raid_class libps2 libata scsi_transport_sas usbcore usb_common scsi_mod i8042 serio nvme nvme_core bridge stp llc vhost_net tun vhost macvtap macvlan vfio_pci irqbypass vfio_virqfd vfio_iommu_type1 vfio\r\n[  108.837930] CPU: 2 PID: 10467 Comm: libvirtd Tainted: P           O L  4.9.28-1-ARCH #1\r\n[  108.845972] Hardware name: Supermicro X9DA7/E/X9DA7/E, BIOS 3.0a 07/02/2014\r\n[  108.852923] task: ffff881614ac4600 task.stack: ffffc90041768000\r\n[  108.858842] RIP: 0010:[<ffffffff810fe13a>]  [<ffffffff810fe13a>] smp_call_function_many+0x1ea/0x240\r\n[  108.867924] RSP: 0018:ffffc9004176bc38  EFLAGS: 00000202\r\n[  108.873226] RAX: 0000000000000003 RBX: 0000000000000040 RCX: 0000000000000001\r\n[  108.880368] RDX: ffff88103fa5c0e0 RSI: 0000000000000040 RDI: ffff88103fa98f48\r\n[  108.887493] RBP: ffffc9004176bc70 R08: fffffffffffffffe R09: 00000000fffffffb\r\n[  108.894624] R10: 0000000000001688 R11: 0000000000006071 R12: ffff88103fa98f40\r\n[  108.901749] R13: ffff88103fa98f48 R14: ffffffff81034790 R15: 0000000000000000\r\n[  108.908934] FS:  00007f0553fac700(0000) GS:ffff88103fa80000(0000) knlGS:0000000000000000\r\n[  108.917072] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[  108.922842] CR2: 00007f056a0ed3e0 CR3: 000000061ab1c000 CR4: 00000000001406e0\r\n[  108.929966] Stack:\r\n[  108.932033]  0000000000018f00 01ffc90000000001 ffffffff811d605a ffffffff81034790\r\n[  108.939552]  0000000000000000 ffffffff811d605b ffffffff8185c9c0 ffffc9004176bc98\r\n[  108.947040]  ffffffff810fe1ed ffffffff811d605a 0000000000000005 ffffc9004176bceb\r\n[  108.954552] Call Trace:\r\n[  108.957040]  [<ffffffff811d605a>] ? ___slab_alloc+0x36a/0x560\r\n[  108.962798]  [<ffffffff81034790>] ? arch_unregister_cpu+0x30/0x30\r\n[  108.968932]  [<ffffffff811d605b>] ? ___slab_alloc+0x36b/0x560\r\n[  108.974687]  [<ffffffff810fe1ed>] on_each_cpu+0x2d/0x60\r\n[  108.979929]  [<ffffffff811d605a>] ? ___slab_alloc+0x36a/0x560\r\n[  108.985683]  [<ffffffff81035674>] text_poke_bp+0x94/0xf0\r\n[  108.991037]  [<ffffffff811d605a>] ? ___slab_alloc+0x36a/0x560\r\n[  108.996795]  [<ffffffff810323db>] arch_jump_label_transform+0x9b/0x120\r\n[  109.003312]  [<ffffffff8116d8a7>] __jump_label_update+0x77/0x90\r\n[  109.009230]  [<ffffffff8116d948>] jump_label_update+0x88/0x90\r\n[  109.014967]  [<ffffffff8116dde5>] static_key_slow_inc+0x95/0xa0\r\n[  109.020880]  [<ffffffff81115466>] cpuset_css_online+0x66/0x180\r\n[  109.026746]  [<ffffffff8110a9ef>] online_css+0x1f/0x60\r\n[  109.031875]  [<ffffffff8110f1d4>] cgroup_apply_control_enable+0x1f4/0x300\r\n[  109.038661]  [<ffffffff811123de>] cgroup_mkdir+0x29e/0x310\r\n[  109.044139]  [<ffffffff81278b7a>] kernfs_iop_mkdir+0x5a/0x90\r\n[  109.049842]  [<ffffffff812064f4>] vfs_mkdir+0xc4/0x110\r\n[  109.054990]  [<ffffffff8120ba7d>] SyS_mkdirat+0xcd/0x100\r\n[  109.060311]  [<ffffffff8120bac9>] SyS_mkdir+0x19/0x20\r\n[  109.065354]  [<ffffffff815e4bb7>] entry_SYSCALL_64_fastpath+0x1a/0xa9\r\n[  109.071792] Code: 2b 20 00 3b 05 8c 04 80 00 89 c1 0f 8d a3 fe ff ff 48 98 49 8b 14 24 48 03 14 c5 c0 33 78 81 8b 42 18 a8 01 74 09 f3 90 8b 42 18 <a8> 01 75 f7 eb bf 48 c7 c2 58 d4 8f 81 48 89 de 44 89 e7 e8 4e \r\n```\r\n\r\nFor RCU stall do `grep \"rcu_sched self-detected stall\" bug.txt` , for example:\r\n\r\n```\r\n[  114.015638] INFO: rcu_sched self-detected stall on CPU[  114.016642] INFO: rcu_sched detected stalls on CPUs/tasks:\r\n[  114.016646] \t1-...: (0 ticks this GP) idle=181/140000000000000/0 softirq=12047/12047 fqs=14252 \r\n[  114.016647] \t2-...: (59292 ticks this GP) idle=009/140000000000001/0 softirq=10979/10980 fqs=14252 \r\n[  114.016647] \t\r\n[  114.016649] (detected by 16, t=60002 jiffies, g=3323, c=3322, q=3539)\r\n[  114.016649] Task dump for CPU 1:\r\n[  114.016650] systemd         R\r\n[  114.016650]   running task        0     1      0 0x00000008\r\n ffffffff81300cc8\r\n[  114.016652]  ffffc900000079c8 ffffffff810acb8e ffff881000000001 ffff88103fa18028\r\n[  114.016653]  ffff881034a4c680 ffff88103fa18028 ffffc900000079a0 ffffffff810a9c0c\r\n[  114.016654]  ffffc90000007a20 ffffffff810b16fa ffffc90000007a20Call Trace:\r\n[  114.016658]  [<ffffffff81300cc8>] ? find_next_bit+0x18/0x20\r\n[  114.016661]  [<ffffffff810acb8e>] ? select_idle_sibling+0x31e/0x3a0\r\n[  114.016662]  [<ffffffff810a9c0c>] ? __enqueue_entity+0x6c/0x70\r\n[  114.016663]  [<ffffffff810b16fa>] ? enqueue_entity+0x21a/0xdd0\r\n[  114.016665]  [<ffffffff810b237c>] ? enqueue_task_fair+0xcc/0x960\r\n[  114.016666]  [<ffffffff810acb8e>] ? select_idle_sibling+0x31e/0x3a0\r\n[  114.016667]  [<ffffffff810b237c>] ? enqueue_task_fair+0xcc/0x960\r\n[  114.016668]  [<ffffffff810a89e9>] ? sched_clock_cpu+0x99/0xb0\r\n[  114.016669]  [<ffffffff810a25e2>] ? check_preempt_curr+0x52/0x90\r\n[  114.016670]  [<ffffffff810a2639>] ? ttwu_do_wakeup+0x19/0xe0\r\n[  114.016671]  [<ffffffff810a276f>] ? ttwu_do_activate+0x6f/0x80\r\n[  114.016672]  [<ffffffff810c465c>] ? mutex_optimistic_spin+0x13c/0x1a0\r\n[  114.016674]  [<ffffffff81212930>] ? dput+0x40/0x2a0\r\n[  114.016676]  [<ffffffff8121c172>] ? legitimize_mnt+0x12/0x60\r\n[  114.016677]  [<ffffffff812129ef>] ? dput+0xff/0x2a0\r\n[  114.016678]  [<ffffffff8120749e>] ? follow_managed+0x28e/0x330\r\n[  114.016679]  [<ffffffff81207b14>] ? lookup_fast+0x1e4/0x310\r\n[  114.016680]  [<ffffffff81277048>] ? kernfs_refresh_inode+0xc8/0xe0\r\n[  114.016682]  [<ffffffff811d612b>] ? ___slab_alloc+0x43b/0x560\r\n[  114.016683]  [<ffffffff81213be7>] ? __d_alloc+0x27/0x1d0\r\n[  114.016684]  [<ffffffff8121bc34>] ? mntput+0x24/0x40\r\n[  114.016685]  [<ffffffff81205180>] ? terminate_walk+0xe0/0xf0\r\n[  114.016686]  [<ffffffff812087ec>] ? path_parentat+0x3c/0x80\r\n[  114.016687]  [<ffffffff8120a7f0>] ? filename_parentat+0xe0/0x150\r\n[  114.016688]  [<ffffffff81213be7>] ? __d_alloc+0x27/0x1d0\r\n[  114.016689]  [<ffffffff811d6270>] ? __slab_alloc+0x20/0x40\r\n[  114.016690]  [<ffffffff811d63ee>] ? kmem_cache_alloc+0x15e/0x1a0\r\n[  114.016691]  [<ffffffff81213be7>] ? __d_alloc+0x27/0x1d0\r\n[  114.016692]  [<ffffffff81213daa>] ? d_alloc+0x1a/0x90\r\n[  114.016693]  [<ffffffff81208bd5>] ? __lookup_hash+0x45/0xa0\r\n[  114.016694]  [<ffffffff8120ac61>] ? filename_create+0x91/0x160\r\n[  114.016695]  [<ffffffff8120ba02>] ? SyS_mkdirat+0x52/0x100\r\n[  114.016696]  [<ffffffff8120bac9>] ? SyS_mkdir+0x19/0x20\r\n[  114.016697]  [<ffffffff815e4bb7>] ? entry_SYSCALL_64_fastpath+0x1a/0xa9\r\n[  114.016698] Task dump for CPU 2:\r\n[  114.016698] libvirtd        R\r\n[  114.016698]   running task        0 10467      1 0x0000000a\r\n ffff881038e2d400\r\n[  114.016700]  ffff88103fa97fc0 ffffc9004176b9c8 ffffffff812f3514 ffffc9004176ba40\r\n[  114.016701]  ffff88103fa92080 ffffc9004176b9f8 ffffffff810e92ec ffffc9004176ba40\r\n[  114.016702]  ffffffff811d6363 ffffffffa01d79f2 ffff882035f79a00Call Trace:\r\n[  114.016704]  [<ffffffff812f3514>] ? timerqueue_del+0x24/0x70\r\n[  114.016706]  [<ffffffff810e92ec>] ? __remove_hrtimer+0x3c/0x90\r\n[  114.016706]  [<ffffffff811d6363>] ? kmem_cache_alloc+0xd3/0x1a0\r\n[  114.016711]  [<ffffffffa01d79f2>] ? spl_kmem_cache_alloc+0x72/0x8d0 [spl]\r\n[  114.016712]  [<ffffffffa01d79f2>] ? spl_kmem_cache_alloc+0x72/0x8d0 [spl]\r\n[  114.016713]  [<ffffffff815e3b0e>] ? schedule_hrtimeout_range_clock+0xae/0x120\r\n[  114.016714]  [<ffffffff815e20e2>] ? mutex_lock+0x12/0x30\r\n[  114.016741]  [<ffffffffa045678e>] ? dbuf_find+0x12e/0x140 [zfs]\r\n[  114.016751]  [<ffffffffa04567ce>] ? dbuf_cache_multilist_index_func+0x2e/0x40 [zfs]\r\n[  114.016760]  [<ffffffffa04567ce>] ? dbuf_cache_multilist_index_func+0x2e/0x40 [zfs]\r\n[  114.016760]  [<ffffffff815e20e2>] ? mutex_lock+0x12/0x30\r\n[  114.016762]  [<ffffffffa01de526>] ? tsd_hash_search.isra.0+0x46/0xa0 [spl]\r\n[  114.016763]  [<ffffffffa01de5b7>] ? tsd_get+0x37/0x50 [spl]\r\n[  114.016773]  [<ffffffffa045786a>] ? dbuf_rele_and_unlock+0x30a/0x490 [zfs]\r\n[  114.016774]  [<ffffffff81034790>] ? arch_unregister_cpu+0x30/0x30\r\n[  114.016776]  [<ffffffff810fe137>] ? smp_call_function_many+0x1e7/0x240\r\n[  114.016777]  [<ffffffff810fe112>] ? smp_call_function_many+0x1c2/0x240\r\n[  114.016778]  [<ffffffff811d605a>] ? ___slab_alloc+0x36a/0x560\r\n[  114.016779]  [<ffffffff81034790>] ? arch_unregister_cpu+0x30/0x30\r\n[  114.016780]  [<ffffffff811d605b>] ? ___slab_alloc+0x36b/0x560\r\n[  114.016781]  [<ffffffff810fe1ed>] ? on_each_cpu+0x2d/0x60\r\n[  114.016782]  [<ffffffff811d605a>] ? ___slab_alloc+0x36a/0x560\r\n[  114.016783]  [<ffffffff81035674>] ? text_poke_bp+0x94/0xf0\r\n[  114.016784]  [<ffffffff811d605a>] ? ___slab_alloc+0x36a/0x560\r\n[  114.016785]  [<ffffffff810323db>] ? arch_jump_label_transform+0x9b/0x120\r\n[  114.016786]  [<ffffffff8116d8a7>] ? __jump_label_update+0x77/0x90\r\n[  114.016787]  [<ffffffff8116d948>] ? jump_label_update+0x88/0x90\r\n[  114.016788]  [<ffffffff8116dde5>] ? static_key_slow_inc+0x95/0xa0\r\n[  114.016789]  [<ffffffff81115466>] ? cpuset_css_online+0x66/0x180\r\n[  114.016791]  [<ffffffff8110a9ef>] ? online_css+0x1f/0x60\r\n[  114.016792]  [<ffffffff8110f1d4>] ? cgroup_apply_control_enable+0x1f4/0x300\r\n[  114.016793]  [<ffffffff811123de>] ? cgroup_mkdir+0x29e/0x310\r\n[  114.016794]  [<ffffffff81278b7a>] ? kernfs_iop_mkdir+0x5a/0x90\r\n[  114.016795]  [<ffffffff812064f4>] ? vfs_mkdir+0xc4/0x110\r\n[  114.016796]  [<ffffffff8120ba7d>] ? SyS_mkdirat+0xcd/0x100\r\n[  114.016796]  [<ffffffff8120bac9>] ? SyS_mkdir+0x19/0x20\r\n[  114.016797]  [<ffffffff815e4bb7>] ? entry_SYSCALL_64_fastpath+0x1a/0xa9\r\n[  114.542572] \t2-...: (59292 ticks this GP) idle=009/140000000000001/0 softirq=10979/10980 fqs=14378 \r\n[  114.551631] \t (t=60538 jiffies g=3323 c=3322 q=3539)\r\n[  114.556630] Task dump for CPU 1:\r\n[  114.559909] systemd         R  running task        0     1      0 0x00000008\r\n[  114.567014]  ffffffff81300cc8 ffffc900000079c8 ffffffff810acb8e ffff881000000001\r\n[  114.574500]  ffff88103fa18028 ffff881034a4c680 ffff88103fa18028 ffffc900000079a0\r\n[  114.581980]  ffffffff810a9c0c ffffc90000007a20 ffffffff810b16fa ffffc90000007a20\r\n[  114.589459] Call Trace:\r\n[  114.591910]  [<ffffffff81300cc8>] ? find_next_bit+0x18/0x20\r\n[  114.597502]  [<ffffffff810acb8e>] ? select_idle_sibling+0x31e/0x3a0\r\n[  114.603759]  [<ffffffff810a9c0c>] ? __enqueue_entity+0x6c/0x70\r\n[  114.609593]  [<ffffffff810b16fa>] ? enqueue_entity+0x21a/0xdd0\r\n[  114.615460]  [<ffffffff810b237c>] ? enqueue_task_fair+0xcc/0x960\r\n[  114.621510]  [<ffffffff810acb8e>] ? select_idle_sibling+0x31e/0x3a0\r\n[  114.627810]  [<ffffffff810b237c>] ? enqueue_task_fair+0xcc/0x960\r\n[  114.633851]  [<ffffffff810a89e9>] ? sched_clock_cpu+0x99/0xb0\r\n[  114.639587]  [<ffffffff810a25e2>] ? check_preempt_curr+0x52/0x90\r\n[  114.645592]  [<ffffffff810a2639>] ? ttwu_do_wakeup+0x19/0xe0\r\n[  114.651287]  [<ffffffff810a276f>] ? ttwu_do_activate+0x6f/0x80\r\n[  114.657164]  [<ffffffff810c465c>] ? mutex_optimistic_spin+0x13c/0x1a0\r\n[  114.663629]  [<ffffffff81212930>] ? dput+0x40/0x2a0\r\n[  114.668524]  [<ffffffff8121c172>] ? legitimize_mnt+0x12/0x60\r\n[  114.674200]  [<ffffffff812129ef>] ? dput+0xff/0x2a0\r\n[  114.679098]  [<ffffffff8120749e>] ? follow_managed+0x28e/0x330\r\n[  114.684930]  [<ffffffff81207b14>] ? lookup_fast+0x1e4/0x310\r\n[  114.690503]  [<ffffffff81277048>] ? kernfs_refresh_inode+0xc8/0xe0\r\n[  114.696698]  [<ffffffff811d612b>] ? ___slab_alloc+0x43b/0x560\r\n[  114.702452]  [<ffffffff81213be7>] ? __d_alloc+0x27/0x1d0\r\n[  114.707766]  [<ffffffff8121bc34>] ? mntput+0x24/0x40\r\n[  114.712739]  [<ffffffff81205180>] ? terminate_walk+0xe0/0xf0\r\n[  114.718416]  [<ffffffff812087ec>] ? path_parentat+0x3c/0x80\r\n[  114.724006]  [<ffffffff8120a7f0>] ? filename_parentat+0xe0/0x150\r\n[  114.730021]  [<ffffffff81213be7>] ? __d_alloc+0x27/0x1d0\r\n[  114.735333]  [<ffffffff811d6270>] ? __slab_alloc+0x20/0x40\r\n[  114.740854]  [<ffffffff811d63ee>] ? kmem_cache_alloc+0x15e/0x1a0\r\n[  114.746876]  [<ffffffff81213be7>] ? __d_alloc+0x27/0x1d0\r\n[  114.752198]  [<ffffffff81213daa>] ? d_alloc+0x1a/0x90\r\n[  114.757276]  [<ffffffff81208bd5>] ? __lookup_hash+0x45/0xa0\r\n[  114.762848]  [<ffffffff8120ac61>] ? filename_create+0x91/0x160\r\n[  114.768674]  [<ffffffff8120ba02>] ? SyS_mkdirat+0x52/0x100\r\n[  114.774159]  [<ffffffff8120bac9>] ? SyS_mkdir+0x19/0x20\r\n[  114.779394]  [<ffffffff815e4bb7>] ? entry_SYSCALL_64_fastpath+0x1a/0xa9\r\n[  114.786015] Task dump for CPU 2:\r\n[  114.789238] libvirtd        R  running task        0 10467      1 0x0000000a\r\n[  114.796361]  ffff88103fa83df0 ffffffff810a59d3 0000000000000002 ffffffff818fd460\r\n[  114.803813]  ffff88103fa83e08 ffffffff810a8417 0000000000000002 ffff88103fa83e40\r\n[  114.811301]  ffffffff8116eb5a ffff88103fa98d40 ffffffff81853140 ffffffff81853140\r\n[  114.818808] Call Trace:\r\n[  114.821261]  <IRQ> [  114.823195]  [<ffffffff810a59d3>] sched_show_task+0xd3/0x140\r\n[  114.828910]  [<ffffffff810a8417>] dump_cpu_task+0x37/0x40\r\n[  114.834330]  [<ffffffff8116eb5a>] rcu_dump_cpu_stacks+0x97/0xb9\r\n[  114.840277]  [<ffffffff810e3a07>] rcu_check_callbacks+0x6d7/0x820\r\n[  114.846393]  [<ffffffff8111d7fc>] ? acct_account_cputime+0x1c/0x20\r\n[  114.852580]  [<ffffffff810a8eb4>] ? account_system_time+0x94/0x120\r\n[  114.858762]  [<ffffffff810f8d10>] ? tick_sched_handle.isra.4+0x60/0x60\r\n[  114.865328]  [<ffffffff810e8baf>] update_process_times+0x2f/0x60\r\n[  114.871353]  [<ffffffff810f8cd1>] tick_sched_handle.isra.4+0x21/0x60\r\n[  114.877722]  [<ffffffff810f8d52>] tick_sched_timer+0x42/0x70\r\n[  114.883392]  [<ffffffff810e9563>] __hrtimer_run_queues+0xf3/0x270\r\n[  114.889499]  [<ffffffff810e9d6a>] hrtimer_interrupt+0x9a/0x180\r\n[  114.895368]  [<ffffffff81034790>] ? arch_unregister_cpu+0x30/0x30\r\n[  114.901478]  [<ffffffff81051b58>] local_apic_timer_interrupt+0x38/0x60\r\n[  114.908020]  [<ffffffff815e751d>] smp_apic_timer_interrupt+0x3d/0x50\r\n[  114.914390]  [<ffffffff815e6832>] apic_timer_interrupt+0x82/0x90\r\n[  114.920405]  <EOI> [  114.922329]  [<ffffffff81034790>] ? arch_unregister_cpu+0x30/0x30\r\n[  114.928447]  [<ffffffff810fe137>] ? smp_call_function_many+0x1e7/0x240\r\n[  114.934982]  [<ffffffff810fe112>] ? smp_call_function_many+0x1c2/0x240\r\n[  114.941543]  [<ffffffff811d605a>] ? ___slab_alloc+0x36a/0x560\r\n[  114.947323]  [<ffffffff81034790>] ? arch_unregister_cpu+0x30/0x30\r\n[  114.953408]  [<ffffffff811d605b>] ? ___slab_alloc+0x36b/0x560\r\n[  114.959153]  [<ffffffff810fe1ed>] on_each_cpu+0x2d/0x60\r\n[  114.964396]  [<ffffffff811d605a>] ? ___slab_alloc+0x36a/0x560\r\n[  114.970149]  [<ffffffff81035674>] text_poke_bp+0x94/0xf0\r\n[  114.975453]  [<ffffffff811d605a>] ? ___slab_alloc+0x36a/0x560\r\n[  114.981236]  [<ffffffff810323db>] arch_jump_label_transform+0x9b/0x120\r\n[  114.987753]  [<ffffffff8116d8a7>] __jump_label_update+0x77/0x90\r\n[  114.993670]  [<ffffffff8116d948>] jump_label_update+0x88/0x90\r\n[  114.999416]  [<ffffffff8116dde5>] static_key_slow_inc+0x95/0xa0\r\n[  115.005346]  [<ffffffff81115466>] cpuset_css_online+0x66/0x180\r\n[  115.011187]  [<ffffffff8110a9ef>] online_css+0x1f/0x60\r\n[  115.016324]  [<ffffffff8110f1d4>] cgroup_apply_control_enable+0x1f4/0x300\r\n[  115.023128]  [<ffffffff811123de>] cgroup_mkdir+0x29e/0x310\r\n[  115.028623]  [<ffffffff81278b7a>] kernfs_iop_mkdir+0x5a/0x90\r\n[  115.034326]  [<ffffffff812064f4>] vfs_mkdir+0xc4/0x110\r\n[  115.039499]  [<ffffffff8120ba7d>] SyS_mkdirat+0xcd/0x100\r\n[  115.044819]  [<ffffffff8120bac9>] SyS_mkdir+0x19/0x20\r\n[  115.049902]  [<ffffffff815e4bb7>] entry_SYSCALL_64_fastpath+0x1a/0xa9\r\n```\r\n\r\nThe reasons to suspect this might be ZOL bug:\r\n* at the top of each stacktrace you will see `SyS_mkdir` and my computer is using ZFS for all filesystems (except /boot )\r\n* process which soft locked on CPU2 is 10467 : libvirtd \r\n* this is the same process which is reported by RCU stall on CPU2, where I see in the call stack `dbuf_rele_and_unlock+0x30a/0x490 [zfs]`\r\n\r\nFor more diagnostic information do `grep \"sysrq: SysRq : \" bug.txt` You can find the file attached, alongside with my kernel configuration.\r\n\r\n[bug.txt](https://github.com/zfsonlinux/zfs/files/1016186/bug.txt)\r\n[config.txt](https://github.com/zfsonlinux/zfs/files/1016185/config.txt)\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6149/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6127", "title": "zvol write performance issue 0.7.0-rc4", "body": "Type                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Arch\r\nDistribution Version    | \r\nLinux Kernel                 | 4.9.26 (vanilla)\r\nArchitecture                 | x64\r\nZFS Version                  | 0.7.0-rc4\r\nSPL Version                  | 0.7.0-rc4\r\n\r\nI just encountered severe zvol write performance issue, where zvol is used as a backing store for a kvm virtual machines, running under qemu 2.9.0 and kernel 4.9.26 (vanilla , i.e. no patches) with ZFS/SPL version 0.7.0-rc4. Due to very high CPU utilization in kernel mode, the host became virtually unresponsive and had to be restarted. The host machine has 128GB RAM and 2 sockets, each with  Xeon E5-2667v2 and HT enabled, which means 16 cores (32 logical cores, i.e. hardware threads), while the guest was only assigned 2 logical cores and 2GB RAM. The system was luckily responsive to sysreq commands, which allowed me to dump blocked tasks (and several other statistics).  Here are interesting excerpts from kernel log:\r\n\r\n```\r\nroot@czersk ~ # grep -F \"[35224.445587] sysrq: SysRq : Show Blocked State\" total.log -A2860 | grep \"Call Trace\" | wc -l\r\n119\r\nroot@czersk ~ # grep -F \"[35224.445587] sysrq: SysRq : Show Blocked State\" total.log -A2860 | grep -E '^[^ ]+ zvol' | wc -l\r\n8\r\n\r\nroot@czersk ~ # grep -F \"[35270.367799] sysrq: SysRq : Show Blocked State\" total.log -A3220 | grep \"Call Trace\" | wc -l\r\n135\r\nroot@czersk ~ # grep -F \"[35270.367799] sysrq: SysRq : Show Blocked State\" total.log -A3220 | grep -E '^[^ ]+ zvol' | wc -l\r\n24\r\n\r\nroot@czersk ~ # grep -F \"[35343.395782] sysrq: SysRq : Show Blocked State\" total.log -A2860 | grep \"Call Trace\" | wc -l\r\n119\r\nroot@czersk ~ # grep -F \"[35343.395782] sysrq: SysRq : Show Blocked State\" total.log -A2860 | grep -E '^[^ ]+ zvol' | wc -l\r\n5\r\n\r\nroot@czersk ~ # grep -F \"[35393.369542] sysrq: SysRq : Show Blocked State\" total.log -A4170 | grep \"Call Trace\" | wc -l\r\n163\r\nroot@czersk ~ # grep -F \"[35393.369542] sysrq: SysRq : Show Blocked State\" total.log -A4170 | grep -E '^[^ ]+ zvol' | wc -l\r\n31\r\n\r\nroot@czersk ~ # grep -F \"[35468.219917] sysrq: SysRq : Show Blocked State\" total.log -A3660 | grep \"Call Trace\" | wc -l\r\n147\r\nroot@czersk ~ # grep -F \"[35468.219917] sysrq: SysRq : Show Blocked State\" total.log -A3655 | grep -E '^[^ ]+ zvol' | wc -l\r\n22\r\n\r\n[35287.590594] zvol            D    0 29199      2 0x00000000\r\n[35287.596136]  ffff881034603840 0000000000000000 ffff8809736f8e00 ffff88103fc97fc0\r\n[35287.603673]  ffff8810341c2a00 ffffc9003a48fba8 ffffffff815e03ec ffffffff815e0b3e\r\n[35287.611257]  00ffc9003a48fbc0 ffff88103fc97fc0 000000005ad0fc63 ffff8809736f8e00\r\n[35287.618831] Call Trace:\r\n[35287.621305]  [<ffffffff815e03ec>] ? __schedule+0x22c/0x6b0\r\n[35287.626831]  [<ffffffff815e0b3e>] ? schedule_preempt_disabled+0xe/0x10\r\n[35287.633391]  [<ffffffff815e08a6>] schedule+0x36/0x80\r\n[35287.638376]  [<ffffffffa01819d0>] cv_wait_common+0x110/0x130 [spl]\r\n[35287.644602]  [<ffffffff810bccd0>] ? wake_atomic_t_function+0x60/0x60\r\n[35287.651010]  [<ffffffffa0181a05>] __cv_wait+0x15/0x20 [spl]\r\n[35287.656625]  [<ffffffffa07764f8>] txg_wait_open+0xa8/0xf0 [zfs]\r\n[35287.662618]  [<ffffffffa0732691>] dmu_tx_wait+0x331/0x340 [zfs]\r\n[35287.668610]  [<ffffffffa0746717>] ? dsl_dir_tempreserve_space+0x97/0x140 [zfs]\r\n[35287.675843]  [<ffffffffa0732727>] dmu_tx_assign+0x87/0x3e0 [zfs]\r\n[35287.681973]  [<ffffffffa07d2ab7>] zvol_write+0x177/0x570 [zfs]\r\n[35287.687851]  [<ffffffffa0179f7b>] ? spl_kmem_alloc+0x9b/0x170 [spl]\r\n[35287.694177]  [<ffffffffa0182526>] ? tsd_hash_search.isra.0+0x46/0xa0 [spl]\r\n[35287.701083]  [<ffffffffa017da0e>] taskq_thread+0x25e/0x460 [spl]\r\n[35287.707151]  [<ffffffff815e03f4>] ? __schedule+0x234/0x6b0\r\n[35287.712694]  [<ffffffff810a3620>] ? wake_up_q+0x80/0x80\r\n[35287.717969]  [<ffffffffa017d7b0>] ? taskq_cancel_id+0x130/0x130 [spl]\r\n[35287.724430]  [<ffffffff81097dd6>] kthread+0xe6/0x100\r\n[35287.729402]  [<ffffffff81097cf0>] ? kthread_park+0x60/0x60\r\n[35287.734897]  [<ffffffff815e4d55>] ret_from_fork+0x25/0x30\r\n[35287.740330] zvol            D    0 29200      2 0x00000000\r\n[35287.745887]  ffff881035847080 0000000000000000 ffff8805440f2a00 ffff88103fc97fc0\r\n[35287.753406]  ffff88045ffab800 ffffc9003a497ba8 ffffffff815e03ec ffffffff815e0b3e\r\n[35287.760979]  00ffc9003a497bc0 ffff88103fc97fc0 000000002d4ba5b0 ffff8805440f2a00\r\n[35287.768504] Call Trace:\r\n[35287.770959]  [<ffffffff815e03ec>] ? __schedule+0x22c/0x6b0\r\n[35287.776469]  [<ffffffff815e0b3e>] ? schedule_preempt_disabled+0xe/0x10\r\n[35287.783036]  [<ffffffff815e08a6>] schedule+0x36/0x80\r\n[35287.788024]  [<ffffffffa01819d0>] cv_wait_common+0x110/0x130 [spl]\r\n[35287.794253]  [<ffffffff810bccd0>] ? wake_atomic_t_function+0x60/0x60\r\n[35287.800665]  [<ffffffffa0181a05>] __cv_wait+0x15/0x20 [spl]\r\n[35287.806303]  [<ffffffffa07764f8>] txg_wait_open+0xa8/0xf0 [zfs]\r\n[35287.812235]  [<ffffffffa0732691>] dmu_tx_wait+0x331/0x340 [zfs]\r\n[35287.818182]  [<ffffffffa0746717>] ? dsl_dir_tempreserve_space+0x97/0x140 [zfs]\r\n[35287.825451]  [<ffffffffa0732727>] dmu_tx_assign+0x87/0x3e0 [zfs]\r\n[35287.831484]  [<ffffffffa07d2ab7>] zvol_write+0x177/0x570 [zfs]\r\n[35287.837332]  [<ffffffffa0179f7b>] ? spl_kmem_alloc+0x9b/0x170 [spl]\r\n[35287.843614]  [<ffffffffa0182526>] ? tsd_hash_search.isra.0+0x46/0xa0 [spl]\r\n[35287.850538]  [<ffffffffa017da0e>] taskq_thread+0x25e/0x460 [spl]\r\n[35287.856597]  [<ffffffff815e03f4>] ? __schedule+0x234/0x6b0\r\n[35287.862081]  [<ffffffff810a3620>] ? wake_up_q+0x80/0x80\r\n[35287.867309]  [<ffffffffa017d7b0>] ? taskq_cancel_id+0x130/0x130 [spl]\r\n[35287.873757]  [<ffffffff81097dd6>] kthread+0xe6/0x100\r\n[35287.878720]  [<ffffffff81097cf0>] ? kthread_park+0x60/0x60\r\n[35287.884231]  [<ffffffff815e4d55>] ret_from_fork+0x25/0x30\r\n[35287.889668] zvol            D    0 29201      2 0x00000000\r\n[35287.895212]  ffff881035847080 0000000000000000 ffff8805440f3800 ffff88103fc97fc0\r\n[35287.902714]  ffff8805916e0e00 ffffc9003a49fba8 ffffffff815e03ec ffffffff815e0b3e\r\n[35287.910176]  00ffc9003a49fbc0 ffff88103fc97fc0 000000008f11df80 ffff8805440f3800\r\n[35287.917680] Call Trace:\r\n[35287.920139]  [<ffffffff815e03ec>] ? __schedule+0x22c/0x6b0\r\n[35287.925659]  [<ffffffff815e0b3e>] ? schedule_preempt_disabled+0xe/0x10\r\n[35287.932216]  [<ffffffff815e08a6>] schedule+0x36/0x80\r\n[35287.937219]  [<ffffffffa01819d0>] cv_wait_common+0x110/0x130 [spl]\r\n[35287.943517]  [<ffffffff810bccd0>] ? wake_atomic_t_function+0x60/0x60\r\n[35287.949973]  [<ffffffffa0181a05>] __cv_wait+0x15/0x20 [spl]\r\n[35287.955639]  [<ffffffffa07764f8>] txg_wait_open+0xa8/0xf0 [zfs]\r\n[35287.961612]  [<ffffffffa0732691>] dmu_tx_wait+0x331/0x340 [zfs]\r\n[35287.967618]  [<ffffffffa0746717>] ? dsl_dir_tempreserve_space+0x97/0x140 [zfs]\r\n[35287.974914]  [<ffffffffa0732727>] dmu_tx_assign+0x87/0x3e0 [zfs]\r\n[35287.981007]  [<ffffffffa07d2ab7>] zvol_write+0x177/0x570 [zfs]\r\n[35287.986925]  [<ffffffffa0179f7b>] ? spl_kmem_alloc+0x9b/0x170 [spl]\r\n[35287.993226]  [<ffffffffa0182526>] ? tsd_hash_search.isra.0+0x46/0xa0 [spl]\r\n[35288.000132]  [<ffffffffa017da0e>] taskq_thread+0x25e/0x460 [spl]\r\n[35288.006222]  [<ffffffff815e03f4>] ? __schedule+0x234/0x6b0\r\n[35288.011770]  [<ffffffff810a3620>] ? wake_up_q+0x80/0x80\r\n[35288.017057]  [<ffffffffa017d7b0>] ? taskq_cancel_id+0x130/0x130 [spl]\r\n[35288.023538]  [<ffffffff81097dd6>] kthread+0xe6/0x100\r\n[35288.028577]  [<ffffffff81097cf0>] ? kthread_park+0x60/0x60\r\n[35288.034093]  [<ffffffff815e4d55>] ret_from_fork+0x25/0x30\r\n[35288.039531] zvol            D    0 29202      2 0x00000000\r\n[35288.045082]  ffff881035fbf800 0000000000000000 ffff8805440f6200 ffff88207fc57fc0\r\n[35288.052641]  ffff8809736fd400 ffffc9003a4a7ba8 ffffffff815e03ec 00000000ffffffff\r\n[35288.060178]  0000000000000020 ffff88207fc57fc0 ffffc9003a4a7c48 ffff8805440f6200\r\n[35288.067778] Call Trace:\r\n[35288.070240]  [<ffffffff815e03ec>] ? __schedule+0x22c/0x6b0\r\n[35288.075734]  [<ffffffff815e08a6>] schedule+0x36/0x80\r\n[35288.080769]  [<ffffffffa01819d0>] cv_wait_common+0x110/0x130 [spl]\r\n[35288.086991]  [<ffffffff810bccd0>] ? wake_atomic_t_function+0x60/0x60\r\n[35288.093388]  [<ffffffffa0181a05>] __cv_wait+0x15/0x20 [spl]\r\n[35288.099017]  [<ffffffffa07764f8>] txg_wait_open+0xa8/0xf0 [zfs]\r\n[35288.105010]  [<ffffffffa0732691>] dmu_tx_wait+0x331/0x340 [zfs]\r\n[35288.111008]  [<ffffffffa0746717>] ? dsl_dir_tempreserve_space+0x97/0x140 [zfs]\r\n[35288.118320]  [<ffffffffa0732727>] dmu_tx_assign+0x87/0x3e0 [zfs]\r\n[35288.124405]  [<ffffffffa07d2ab7>] zvol_write+0x177/0x570 [zfs]\r\n[35288.130305]  [<ffffffffa0179f7b>] ? spl_kmem_alloc+0x9b/0x170 [spl]\r\n[35288.136656]  [<ffffffffa0182526>] ? tsd_hash_search.isra.0+0x46/0xa0 [spl]\r\n[35288.143564]  [<ffffffffa017da0e>] taskq_thread+0x25e/0x460 [spl]\r\n[35288.149631]  [<ffffffff815e03f4>] ? __schedule+0x234/0x6b0\r\n[35288.155201]  [<ffffffff810a3620>] ? wake_up_q+0x80/0x80\r\n[35288.160507]  [<ffffffffa017d7b0>] ? taskq_cancel_id+0x130/0x130 [spl]\r\n[35288.167049]  [<ffffffff81097dd6>] kthread+0xe6/0x100\r\n[35288.172083]  [<ffffffff81097cf0>] ? kthread_park+0x60/0x60\r\n[35288.177570]  [<ffffffff81097cf0>] ? kthread_park+0x60/0x60\r\n[35288.183047]  [<ffffffff815e4d55>] ret_from_fork+0x25/0x30\r\n\r\n```\r\n\r\nThe guest virtual machine where this was triggered is a very basic install of FreeBSD 10.3 (freshly installed). The issue was triggered when I started this command in guest:\r\n\r\n```\r\n# pkg install vim-8.0.0507\r\n```\r\n... which brings 116 dependencies, taking 2GB of disk space (as I've just learned, one would have to install \"vim-lite-8.0.0507\" on FreeBSD, for vim alone). The problem happened somewhere in the middle of guest installing these 2GB of dependencies. The guest does have sufficient free disk space, as seen here:\r\n\r\n```\r\n# df -h\r\nFilesystem      Size    Used   Avail Capacity  Mounted on\r\n/dev/vtbd0p2     12G    1.9G    8.8G    18%    /\r\ndevfs           1.0K    1.0K      0B   100%    /dev\r\n/dev/vtbd0p3     12G     32M     11G     0%    /home\r\n```\r\n\r\nThis is all hosted by a 24GB ZVOL, defined as below\r\n\r\n```\r\n\r\nroot@gdansk ~ # virsh dumpxml torun-spice | grep -A6 -B3 'vdis/torun'\r\n    <emulator>/usr/sbin/qemu-system-x86_64</emulator>\r\n    <disk type='block' device='disk'>\r\n      <driver name='qemu' type='raw' cache='none' io='threads'/>\r\n      <source dev='/dev/zvol/zdata/vdis/torun'/>\r\n      <backingStore/>\r\n      <target dev='vda' bus='virtio'/>\r\n      <boot order='1'/>\r\n      <alias name='virtio-disk0'/>\r\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>\r\n    </disk>\r\n\r\nroot@gdansk ~ # zfs list zdata/vdis/torun\r\nNAME               USED  AVAIL  REFER  MOUNTPOINT\r\nzdata/vdis/torun  26.8G  2.61T  2.05G  -\r\n\r\nroot@gdansk ~ # zfs get all zdata/vdis/torun\r\nNAME              PROPERTY              VALUE                  SOURCE\r\nzdata/vdis/torun  type                  volume                 -\r\nzdata/vdis/torun  creation              Fri May 12  0:36 2017  -\r\nzdata/vdis/torun  used                  26.8G                  -\r\nzdata/vdis/torun  available             2.61T                  -\r\nzdata/vdis/torun  referenced            2.05G                  -\r\nzdata/vdis/torun  compressratio         1.00x                  -\r\nzdata/vdis/torun  reservation           none                   default\r\nzdata/vdis/torun  volsize               24G                    local\r\nzdata/vdis/torun  volblocksize          8K                     default\r\nzdata/vdis/torun  checksum              on                     default\r\nzdata/vdis/torun  compression           off                    inherited from zdata/vdis\r\nzdata/vdis/torun  readonly              off                    default\r\nzdata/vdis/torun  copies                1                      default\r\nzdata/vdis/torun  refreservation        24.8G                  local\r\nzdata/vdis/torun  primarycache          all                    default\r\nzdata/vdis/torun  secondarycache        all                    default\r\nzdata/vdis/torun  usedbysnapshots       8.06M                  -\r\nzdata/vdis/torun  usedbydataset         2.05G                  -\r\nzdata/vdis/torun  usedbychildren        0B                     -\r\nzdata/vdis/torun  usedbyrefreservation  24.7G                  -\r\nzdata/vdis/torun  logbias               latency                default\r\nzdata/vdis/torun  dedup                 off                    default\r\nzdata/vdis/torun  mlslabel              none                   default\r\nzdata/vdis/torun  sync                  standard               default\r\nzdata/vdis/torun  refcompressratio      1.00x                  -\r\nzdata/vdis/torun  written               43.8M                  -\r\nzdata/vdis/torun  logicalused           2.04G                  -\r\nzdata/vdis/torun  logicalreferenced     2.03G                  -\r\nzdata/vdis/torun  snapshot_limit        none                   default\r\nzdata/vdis/torun  snapshot_count        none                   default\r\nzdata/vdis/torun  snapdev               hidden                 default\r\nzdata/vdis/torun  context               none                   default\r\nzdata/vdis/torun  fscontext             none                   default\r\nzdata/vdis/torun  defcontext            none                   default\r\nzdata/vdis/torun  rootcontext           none                   default\r\nzdata/vdis/torun  redundant_metadata    all                    default\r\n\r\n\r\nroot@gdansk ~ # zpool status zdata\r\n  pool: zdata\r\n state: ONLINE\r\nstatus: Some supported features are not enabled on the pool. The pool can\r\n        still be used, but some features are unavailable.\r\naction: Enable all features using 'zpool upgrade'. Once this is done,\r\n        the pool may no longer be accessible by software that does not support\r\n        the features. See zpool-features(5) for details.\r\n  scan: scrub repaired 0B in 57h30m with 0 errors on Fri May 13 16:50:16 2016\r\nconfig:\r\n\r\n        NAME                                          STATE     READ WRITE CKSUM\r\n        zdata                                         ONLINE       0     0     0\r\n          mirror-0                                    ONLINE       0     0     0\r\n            ata-WDC_WD40EFRX-68WT0N0_WD-WCC4E0178587  ONLINE       0     0     0\r\n            ata-WDC_WD40EFRX-68WT0N0_WD-WCC4E0181554  ONLINE       0     0     0\r\n          mirror-1                                    ONLINE       0     0     0\r\n            ata-WDC_WD40EFRX-68WT0N0_WD-WCC4E0196162  ONLINE       0     0     0\r\n            ata-WDC_WD40EFRX-68WT0N0_WD-WCC4E0182472  ONLINE       0     0     0\r\n        logs\r\n          nvme0n1p3                                   ONLINE       0     0     0\r\n        cache\r\n          nvme0n1p11                                  ONLINE       0     0     0\r\n\r\n\r\n```\r\n\r\n\r\nI think this should be easily reproducible and also I am happy to share more details (e.g. VM image , VM libvirt xml definition, my full kernel config etc). This is in fact why I raised this as a separate issue, so there is space for all these details (as the issue itself is not unique)", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6127/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "arachnist": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6146", "title": "zfs.ko fails to build against linux-next-20170519 - modpost: GPL-incompatible module zfs.ko uses GPL-only symbol 'rcu_all_qs'", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Gentoo\r\nDistribution Version    | \r\nLinux Kernel                 | linux-next-20170519\r\nArchitecture                 | amd64\r\nZFS Version                  | current git tip (f871ab6ea2dd9a3b9fae157ff0a7665bb269c565)\r\nSPL Version                  | \r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nModpost fails when trying to build zfs.ko against linux-next-20170519\r\n```\r\n  LD [M]  /var/tmp/portage/sys-fs/zfs-kmod-9999/work/zfs-kmod-9999/module/zfs/zfs.o\r\n  Building modules, stage 2.\r\n  MODPOST 6 modules\r\nFATAL: modpost: GPL-incompatible module zfs.ko uses GPL-only symbol 'rcu_all_qs'\r\nmake[5]: *** [/usr/src/linux-next/scripts/Makefile.modpost:91: __modpost] Error 1\r\nmake[4]: *** [/usr/src/linux-next/Makefile:1513: modules] Error 2\r\nmake[4]: Leaving directory '/usr/src/linux-next'\r\nmake[3]: *** [Makefile:152: sub-make] Error 2\r\nmake[3]: Leaving directory '/usr/src/linux-next'\r\nmake[2]: *** [Makefile:27: modules] Error 2\r\nmake[2]: Leaving directory '/var/tmp/portage/sys-fs/zfs-kmod-9999/work/zfs-kmod-9999/module'\r\nmake[1]: *** [Makefile:718: all-recursive] Error 1\r\nmake[1]: Leaving directory '/var/tmp/portage/sys-fs/zfs-kmod-9999/work/zfs-kmod-9999'\r\nmake: *** [Makefile:587: all] Error 2\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6146/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "brauner": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6119", "title": "Clones remounted by 'zfs rename' actions on the parent", "body": "### System information\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Ubuntu\r\nDistribution Version    |  Xenial (16.04)\r\nLinux Kernel                 |  4.4 - 4.9 (occurs on multiple versions)\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.8-0ubuntu7\r\nSPL Version                  |  0.6.5.8-2ubuntu1\r\n\r\n### Describe the problem you're observing\r\n\r\n- create a zfs dataset `/tank-test/test`\r\n- create a read-only snapshot of the dataset `/tank-test/test@readonly`\r\n- create a clone of the dataset `/tank-test/test@readonly` e.g. `/tank-test/test-clone`\r\n- mount the clone `/tank-test/test-clone`\r\n- rename the orginal dataset `/tank-test/test{@readonly}`\r\n- renaming the original dataset unmounts the clone\r\n\r\nThe clone should not be unmounted just because the original dataset got renamed.\r\nThe behavior I would expect is for the clone to stay mounted and the old dataset\r\nbe renamed.\r\nNote, that this is especially annoying since the umount can cause https://github.com/zfsonlinux/zfs/issues/5796 to be triggered.\r\n\r\n### Describe how to reproduce the problem\r\n\r\nSee the steps above.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6119/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mwm": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6118", "title": "zpool apparently quietly truncates pools over the zfs file system size limit", "body": "\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Ubuntu\r\nDistribution Version    | 16.10\r\nLinux Kernel                 | 4.8.0-51-generic\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.8-0ubuntu4.2\r\nSPL Version                  | 0.6.5.8-2\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nWith a large zpool: \r\n\r\n```\r\nchokfi% zpool list OMG\r\nNAME   SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT\r\nOMG   15.9E   120K  15.9E         -     0%     0%  1.00x  ONLINE  -\r\n```\r\nadd another chunk of space to push it past the 16EB limit on zfs file systems:\r\n\r\n```\r\nchokfi% ls -lh OMG5\r\n-rwxr-xr-x 1 mwm mwm 4.1E May 10 15:43 OMG5\r\nchokfi% ls -lh /home/mwm/tmp/OMG5\r\n-rwxr-xr-x 1 mwm mwm 4.1E May 10 15:43 /home/mwm/tmp/OMG5\r\nchokfi% zpool add OMG /home/mwm/tmp/OMG5\r\nchokfi% \r\n```\r\n\r\nAnd the zpool is now reported as being < 4EB in size:\r\n```\r\nchokfi% zpool list OMG\r\nNAME   SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT\r\nOMG   3.84E   133K  3.84E         -     0%     0%  1.00x  ONLINE  -\r\n```\r\n\r\n### Describe how to reproduce the problem\r\nI believe the above does the job. The one trick is:\r\n```\r\nchokfi% dd if=/dev/zero of=OMG6 bs=1 count=1 seek=8EB\r\n1+0 records in\r\n1+0 records out\r\n1 byte copied, 0.000221949 s, 4.5 kB/s\r\nchokfi% \r\n```\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nThere are none. That's the bug. Either the pool really is truncated, in which case this is a major bug, or the pool size is being misreported, in which case it's a minor bug.\r\n\r\nI'd expect at the very least a warning in either case, and preferably in the former a refusal to perform the operation.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6118/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ryao": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6097", "title": "Want ZIL Pipelining", "body": "Right now, concurrent synchronous operations block on ZIL. While we do batching, the blocking adds latency. If we could pipeline ZIL so that we can begin preparing new lwbs while ZIO is writing out the current commit, concurrent synchronous IO performance should improve, especially in terms of latencies.\r\n\r\nBy the way, I have written a preliminary patch at work. There is 1 space leak bug that I need to fix. After fixing it, the plan is to put it into a pull request. I am just filing this a little early to let people know that the patch is imminent. :)", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6097/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6042", "title": "Unexplained ARC metadata usage", "body": "In thinking about performance at work and also thinking about it because of #6040, it occurs to me that if we have a situation where we need to lookup an indirect block, we have multiple copies, one is in cache and the others are not, we can randomly select an uncached copy and generate an unnecessary IO. This hurts cache efficiency, which hurts performance on particularly large indirect block trees that do not fit entirely in RAM. This effect gets amortized by ARC on the upper levels of the indirect block tree and can be mitigated on the lowest level by doing `redundant_metadata=most`, but it would be nice if we modified the code to check for cached copies before making a random selection.\r\n\r\nI would like to hear what others think about this. I suspect there is an opportunity here to reduce unnecessary IOs, but it is possible that I made a mistake in reasoning. I have far bigger bottlenecks to tackle for work and given the easy way of mitigating this issue, this will not be a priority for a while.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6042/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/1d53657bf561564162e2ad6449f80fa0140f1dd6", "message": "Fix incompatibility with Reiser4 patched kernels\n\nIn ZFSOnLinux, our sources and build system are self contained such that\r\nwe do not need to make changes to the Linux kernel sources. Reiser4 on\r\nthe other hand exists solely as a kernel tree patch and opts to make\r\nchanges to the kernel rather than adapt to it. After Linux 4.1 made a\r\nVFS change that replaced new_sync_read with do_sync_read, Reiser4's\r\nmaintainer decided to modify the kernel VFS to export the old function.\r\nThis caused our autotools check to misidentify the kernel API as\r\npredating Linux 4.1 on kernels that have been patched with Reiser4\r\nsupport, which breaks our build.\r\n\r\nReiser4 really should be patched to stop doing this, but lets modify our\r\ncheck to be more strict to help the affected users of both filesystems.\r\n\r\nAlso, we were not checking the types of arguments and return value of\r\nnew_sync_read() and new_sync_write() . Lets fix that too.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Richard Yao <ryao@gentoo.org>\r\nCloses #6241 \r\nCloses #7021"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/0d3980acbcca00f495926a1d6b0886e0ea1f6589", "message": "Implement --enable-debuginfo to force debuginfo\n\nInspection of a Ubuntu 14.04 x64 system revealed that the config file\nused to build the kernel image differs from the config file used to\nbuild kernel modules by the presence of CONFIG_DEBUG_INFO=y:\n\nThis in itself is insufficient to show that the kernel is built with\ndebuginfo, but a cursory analysis of the debuginfo provided and the\nsize of the kernel strongly suggests that it was built with\nCONFIG_DEBUG_INFO=y while the modules were not. Installing\nlinux-image-$(uname -r)-dbgsym had no obvious effect on the debuginfo\nprovided by either the modules or the kernel.\n\nThe consequence is that issue reports from distributions such as Ubuntu\nand its derivatives build kernel modules without debuginfo contain\nnonsensical backtraces. It is therefore desireable to force generation\nof debuginfo, so we implement --enable-debuginfo. Since the build system\ncan build both userspace components and kernel modules, the generic\n--enable-debuginfo option will force debuginfo for both. However, it\nalso supports --enable-debuginfo=kernel and --enable-debuginfo=user for\nfiner grained control.\n\nEnabling debuginfo for the kernel modules works by injecting\nCONFIG_DEBUG_INFO=y into the make environment. This is enables\ngeneration of debuginfo by the kernel build systems on all Linux\nkernels, but the build environment is slightly different int hat\nCONFIG_DEBUG_INFO has not been in the CPP. Adding -DCONFIG_DEBUG_INFO\nwould fix that, but it would also cause build failures on kernels where\nCONFIG_DEBUG_INFO=y is already set. That would complicate its use in\nDKMS environments that support a range of kernels and is therefore\nundesireable. We could write a compatibility shim to enable\nCONFIG_DEBUG_INFO only when it is explicitly disabled, but we forgo\ndoing that because it is unnecessary. Nothing in ZoL or the kernel uses\nCONFIG_DEBUG_INFO in the CPP at this time and that is unlikely to\nchange.\n\nEnabling debuginfo for the userspace components is done by injecting -g\ninto CPPFLAGS. This is not necessary because the build system honors the\nenvironment's CPPFLAGS by appending them to the actual CPPFLAGS used,\nbut it is supported for consistency.\n\nReviewed-by: Chunwei Chen <tuxoko@gmail.com>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nSigned-off-by: Richard Yao <richard.yao@clusterhq.com>\nCloses #2734"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/6f174823cef105860a11f757f092942653c17869", "message": "Make --enable-debug fail when given bogus args\n\nCurrently, bogus options to --enable-debug become --disable-debug. That\nmeans that passing --enable-debug=true is analogous to --disable-debug,\nbut the result is counterintuitive. We switch to AS_CASE to allow us to\nfail when given a bogus option.\n\nAlso, we modify the text printed to clarify that --enable-debug enables\nassertions.\n\nReviewed-by: Chunwei Chen <tuxoko@gmail.com>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nSigned-off-by: Richard Yao <richard.yao@clusterhq.com>\nCloses #2734"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/1531002", "body": "When was support for 2.6.18 retired?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/1531002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2567065", "body": "This probably also applies to ARM.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2567065/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/3523784", "body": "Modular arithmetic and division are usually done in libgcc, which is unavailable in the kernel. It would be fairly easy to implement the libgcc functions responsible for those operators in the SPL by mapping them to the Linux kernel's own functions for doing division and modular arithmetic:\n\nhttp://lxr.free-electrons.com/source/include/linux/math64.h\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/3523784/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/7692659", "body": "It should.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/7692659/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/7703089", "body": "@ilovezfs It turns out that MySQL's innodb_use_native_aio also uses Direct IO, as @behlendorf had suspected. This is discussed in issue #224.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/7703089/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/9702234", "body": "I like this change. :)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/9702234/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11028356", "body": "This should have been wrapped by `#if defined(__linux__) ... #endif`\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11028356/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11248008", "body": "Agreed. I'll change this.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11248008/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/1515687", "body": "It might be better to say 'common' rather than 'no uncommon'.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/1515687/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/1517228", "body": "'not uncommon' is a double negative. It is better to say 'common'.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/1517228/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637371", "body": "If we use the hostname like I suggested, would this have any point?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637371/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637684", "body": "It says '(none)'. The code probably should try to handle this more gracefully by either explaining what is wrong or falling back to gethostname().\n\nBy the way, Solaris does not have getdomainname(). How does it do this?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637684/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/651379", "body": "I suggest filing a separate issue for this. Seeing the build failure would be helpful.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/651379/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4674025", "body": "This would be cleaner if you placed `return offset;` before `out:`. That would permit you to remove `if (error)`.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4674025/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4673743", "body": "mutex_lock() is not used anywhere in the ZFS code. You should replace this with the ZFS_ENTER(), ZFS_VERIFY() and ZFS_EXIT() macros as is done in other other zpl_\\* functions. In addition, this only needs to be done when zfs_holey() is called. You can get some idea of how this should look by looking at the Illumos gate code:\n\nhttps://github.com/illumos/illumos-gate/blob/master/usr/src/uts/common/fs/zfs/zfs_vnops.c#L241\n\nAlso, if you look at how Linux handles SEEK_SET, SEEK_CUR and SEEK_END, you would see that there is no use of mutex_lock() on those three cases. Solaris does the same.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4673743/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4675602", "body": "This check should return ENXIO, and zfs_holey does it for us, so it can be removed.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4675602/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4675623", "body": "This check should be done before calling zfs_holey to avoid unnecessary work.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4675623/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4715479", "body": "Failing to check for invalid whence values would appear to permit nonconformant behavior upon inspection of the code that implements generic_file_llseek() because undefined whence values appear to be handled as SEEK_SET. However the syscall functions for lseek() and llseek() will do the whence check for us, so we do not need to worry about this.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4715479/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/5007665", "body": "This should say 3.10.x.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/5007665/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/9393679", "body": "I think it would be better to make a clear definition between the on-disk zbookmark_t and the in-core zbookmark_t. Perhaps we could go with zbookmark_phys_t and zbookmark_core_t.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/9393679/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/9642924", "body": "To ensure that this is meant for reuse, we probably should pick a more general name for it and split it out into a separate commit.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/9642924/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/14440067", "body": "Fork will return -1. If the binary running this has other children, it will block until one of them exits, which is undesirable. Otherwise, it will pretend nothing happened. I have revised the pull request to address this.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/14440067/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/14440080", "body": "I have revised the pull request to fix this.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/14440080/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/14089599", "body": "This merits some comment. While temporary names are useful for both creation and import, the API for the import case is easier to use than the API for the create case. On import, the API already supported two names and import flags. Consequently, `-t` could just set a bit in the import flags to state that the old name is to be sent to disk. On create, the API only uses 1 name and opts for a nvlist does not support \"creation flags\", such that we are forced to use the nvlist. We have validation code in userland and a lingering question on my mind is if the import code should handle passing this flag via a nvlist when doing pool import. If it is, then some additional code should be added to spa_import() to handle this. If not, then this userland validation code should to change.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/14089599/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28430528", "body": "There is a race here between `zfs list` and `zfs destroy` that creates an edge case where `zfs list` says there is something and then the pipe says otherwise. An attempt to take the lock here and transfer ownership of the lock to another thread was messy, although it would close this race. Thoughts on whether this race should be closed or documented would be appreciated.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28430528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28723539", "body": "It appears that this is the case. The existing code should also suffer from this. Fixing this requires some refactoring to handle things like Illumos handles file attribute changes.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28723539/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28723670", "body": "The buffer size is dependent on the record header. It is not possible to use a cache here. It is possible to do an optimization where we try to allocate only once in this function and grow the allocation whenever it is too small.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28723670/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28723733", "body": "That didn't trip ZoL's cstyle, but you are right that it would be better to use `!= 0` explicitly for consistency. I will change this.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28723733/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28723777", "body": "Nice catch. :)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28723777/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28723816", "body": "Nice catch. I will change this to be \"defer\" as specifying that it is for a destroy in a destroy routine is redundant.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28723816/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28799305", "body": "Since the line comments disappeared as I pushed a revised pull request, I want to copy what I said in response to `[style] The above two if's should just be an if-else block.`:\n\n```\nIf we do that, then an error in `zfs_zevent_minor_to_state()` would be ignored.\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28799305/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28799311", "body": "Since the line comments disappeared as I pushed a revised pull request, I want to copy what I said in response to `[style] The above two if's should just be an if-else block.`:\n\n```\nIf we do that, then an error in `zfs_zevent_minor_to_state()` would be ignored.\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28799311/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28802366", "body": "I was thinking of the list and set cases when I said this. If `received` is set, the dataset can be considered inconsistent, so it should be okay to be racy here. The only way this would be a problem is if we snapshot during a receive and then want to do a rollback. That use case doesn't seem to make sense to me.\n\nOn the other hand, we probably want to think about how we cannot atomically inherit and set at the same time. We probably should unify the two so that the interactions are clear when the library function is executed.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28802366/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28814364", "body": "It is natural for API for `zfs send` to be called in a different thread than the API for `zfs recv`, but it is not natural for the API for `zfs list` to be called in a different thread than the code that reads it. I believe that doing this in the same thread would be prone to deadlocks from incorrect usage and I did not like the idea of subjecting the `spa_namespace` lock to that risk. With `zfs send`, the risk is limited to the snapshot being sent.\n\nThat said, it occurs to me that unprivileged users with access to this API could perform denial of service whenever it is possible for the pipe buffer to fill. That is not a problem on Linux where only root can access this API, but it would be a big problem for Illumos and FreeBSD. I will need to think of a way to avoid that. Doing CoW inside the kernel itself on the state of the SPA and DSL would make this work, although I would appreciate other ideas.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28814364/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28815902", "body": "On second thought, doing some thing similar to CoW inside the kernel on the state of the SPA and DSL would work nicely. I just need to have `zfs_stable_ioc_zfs_list` make a linked list with in-kernel references on the current state of the DSL for each pool in the SPA (when listing all) and then have the worker thread dump out data while releasing the holds. As long as I handle the system crashing, this ought to work nicely because we should be able to avoid holding locks should the pipe fill.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28815902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30647468", "body": "There should be an `else sched_yield();` here.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30647468/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30729263", "body": "A `do-while` loop is more readable than a `while` loop when you want the condition to only be checked after the first iteration.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30729263/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30742553", "body": "I suggest adding a `-F` here to match the behavior when the module auto-imports the pool itself to fix #3434.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30742553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30742569", "body": "I suggest adding a `-F` here to match the behavior when the module auto-imports the pool itself to fix #3434.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30742569/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30745132", "body": "That is fine.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30745132/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30551185", "body": "Using `sleep` in an init script is almost never allowed in OpenRC scripts because it is inefficient. It would be far better to do a busy wait like I did in genkernel:\n\nhttps://gitweb.gentoo.org/proj/genkernel.git/commit/?id=a21728ae287e988a1848435ab27f7ab503def784\nhttps://gitweb.gentoo.org/proj/genkernel.git/commit/?id=c812c35100771bb527f6b03853fa6d8ef66a48fe\nhttps://gitweb.gentoo.org/proj/genkernel.git/commit/?id=513021d90162ddeaeb0d308e7f27dd0a84b588db\n\nThe advantage of a busy-wait is that we do not introduce any `sleep` invocations into the code and we continue almost exactly after the module is loaded. This seems wasteful, but the total time spent booting is always less.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30551185/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30555789", "body": "The thread scheduler is able to enforce fairness. A busy wait will not lockup the system and it will not even run very long. If you want to be more efficient, you could try doing `date +%s%N` to get the time in nanoseconds and revert to `sleep 1` when a certain threshold is reached (e.g. 1 second). If we spin for more than 1 second, something is very wrong.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30555789/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30607033", "body": "That test is not representative of the actual boot process because no modules were loading when you ran it. The modules will typically finish initializing before the first check, so the typical case would have no busy wait at all. When they do lose the race, they are likely almost finished loading already, so it will not busy wait for very long. If the modules do take a long time to initialize such that we do several loops through the busy-wait routine, we probably should profile the initialization routine for optimization opportunities. We could likely output the number of times we looped in the situations where the modules lost the race so that people can tell us when the modules are loading slowly.\n\nSystems with very little memory typically use busybox, so the additional memory usage from doing this busy wait is negligible. The thread scheduler on slower systems is the same as it is on a faster system. Consequently, a slow system with very little memory should still be faster with the busy wait than a sleep. On a multicore system, we would be spending more time in the busy wait than we would otherwise spend, but system boot would still be faster.\n\nIf this is particularly controversial, we could make the option configurable so that the user can choose. However, I do not think that sleep should be the default. Gentoo's QA team will not be happy if I commit a sleep into Gentoo and I doubt that many ZoL users would be happy with a sleep on any distribution.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30607033/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30648638", "body": "The general idea in #3431 looks sane to me.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30648638/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30742748", "body": "I suggest adding a `-F` here to match the behavior when the module auto-imports the pool itself to fix #3434.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30742748/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31168790", "body": "This would not work in general. OpenRC is able to start things in parallel (although that is off by default). If we want zed to start before the import, we should put `before zfs-import` into the script zfs-zed script.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31168790/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31169052", "body": "Should I assume that this file is included into Debian's initramfs archives? The ZFS_INITRD options probably should be contained in a separate file for distributions that don't use an initramfs generator that hooks into this to make it easy to omit. Otherwise, it could confuse end users. I would say the same applies to DKMS, but hopefully that is self explanatory on systems that lack DKMS.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31169052/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31169525", "body": "`use mtab` is only needed for `zfs-mount`. It should be there.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31169525/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31169569", "body": "These negative keywords should be in the other scripts' `do_depend()` until delegation support is implemented.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31169569/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31169723", "body": "All `do_depend()` for scripts calling this should have `after sysfs`.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31169723/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31169888", "body": "Any script that calls this should have `after udev` in `do_depend()`.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31169888/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31170061", "body": "Any script calling this should have `after procfs` in `do_depend()`.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31170061/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31170216", "body": "All `do_depend()` for scripts calling this should have `after procfs`.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31170216/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31170416", "body": "After some more thought, zfs-mount should be in `boot`, not in `default`.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31170416/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31170716", "body": "Maybe we could just say which initramfs archive generators are known to support / not support it at this time. I don't think I will be adding this to genkernel because it has a busy-wait intended to eliminate the need for it.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31170716/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31173057", "body": "One last thought that is nagging me. I suspect that `sysinit` should be `boot` for consistency with Gentoo's LVM2 OpenRC script and basically everything other non-base system OpenRC script in Gentoo. However, it would be best to discuss these runlevel choices with @williamh in #openrc on freenode. He is the OpenRC maintainer.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31173057/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31173583", "body": "For reference:\nhttps://wiki.gentoo.org/wiki/LVM#openrc\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31173583/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31174050", "body": "OpenRC does away with numbered runlevels of the sysvrc scripts in favor of named run levels with actual dependency calculations. Consequently, it is much more forgiving than `sysvrc` in terms of choices and while we probably could get away with just 1 runlevel, we have 4. Specifically, sysinit, boot, default and shutdown.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31174050/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31174282", "body": "I should add that there is no sysinit binary for you to run. The /etc/inittab file is configured to start OpenRC, virtual terminals, serial lines and X (although there is a hack where an OpenRC script needs to kick it). Consequently, the services are started by scripts executed by sysvinit. The time intensive parts like dependency calculations are handled by binaries that were written in C. That said, it is possible to send signals to init via /sbin/init, but it is usually not needed.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31174282/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/35996826", "body": "This is unsafe on older kernels because interrupts must be disabled to prevent another thread from using the CPU's KM_USER1 slot. This is likely okay on newer kernels where the slots are dynamically allocated.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/35996826/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/36014168", "body": "When CONFIG_PREEMPT is set, the kernel scheduler should be able to preempt us. If the scheduler preempts us and schedules us to another CPU or runs something else on this CPU that uses this, the value will be clobbered. See the comment for `__schedule`:\n\nhttp://lxr.free-electrons.com/source/kernel/sched/core.c#L2693\n\nAll we need is an IRQ to occur when the scheduler thinks something else should run. As the comment for `__schedule()` indicates, the code in arch/x86/entry/entry_64.S will call into the scheduler on return from the IRQ and `preempt_schedule_irq()` will be called. That will call `schedule()` and something else can have the CPU.\n\nYou are correct that there is code in the mainline kernel doing this, but I think those routines will need to be changed for the same reason. The reason why they have not yet been changed is that these events are rare enough that no one has encountered them.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/36014168/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/36015196", "body": "@tuxoko It seems that I used the wrong term. It is unsafe to use kmap_atomic in a preemptible context. An interrupt context is definitely unsafe. Also, my statement about it being okay on newer kernels was wrong.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/36015196/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/36203857", "body": "@tuxoko Where does it do that?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/36203857/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38362133", "body": "Why was `procfs` removed? if `/etc/mtab` is a symlink to `/proc/mounts` or `/etc/mtab` is not there, `libzfs_mnttab_find()` will check `/proc/mounts` to see if things are already mounted before sending the request to the kernel. This should be used by `zfs mount -a`.\n\nAs for procps, I don't know why that is here. That looks like a typo that was never caught. It should be removed in a separate commit.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38362133/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38362389", "body": "Saying to run after zfs-mount should not be necessary. I do not think there is any need to enforce a strict ordering here. Leaving the order undefined would allow these scripts to run in parallel.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38362389/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38495116", "body": "Why does zfs-zed need to run after zfs-mount?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38495116/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38495255", "body": "You are correct. This is safe to drop.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38495255/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38497207", "body": "@MrStaticVoid My line of thinking when I asked that question was not one of my finer moments. You are completely right that we need to run after zfs-mount to ensure that /var is mounted when mounted by zfs-mount. You are also right about localmount possibly doing the mounting operation. The best thing here would be to specify `after zfs-mount localmount` in the zfs-zed.in script. The ordering of zfs-mount and localmount will depend on whether / is on ZFS or not, so this tackles both.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38497207/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38758761", "body": "Incrementing to ZPL_VERSION_6 is not the right thing to do here because Oracle used ZPL_VERSION_6 to mean something else. What we need to do is implement feature flags on ZPL versions (i.e. bump to version 5000) and then rely on the feature flags to indicate this change has been made. Otherwise, people might try doing send/recv from Oracle Solaris to ZoL and it will not do what people expect.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38758761/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "Rudd-O": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6085", "title": "zfs-import-cache fails on initramfs", "body": "I've tracked an issue on ZFS master in Fedora 25.  There is a `zfs-import-cache.service` failure during the initial RAM disk.\r\n\r\nThis error makes sense under some circumstances: when not all pools are necessarily available during early boot.  Some pools are not necessary to boot the root file system, so some devices are not decrypted during early boot, therefore some pools cannot be imported during early boot.  This error is evidently only triggered when the cache file is available in the initial RAM disk (*id est*, hostonly initial RAM disk).\r\n\r\nThe solution is to make it so that, within the initial RAM disk, a different unit file `zfs-import-cache-initramfs.service` is installed.  This unit would not execute `zpool import -aN`, but rather would only execute `zpool import -N <root pool>`, which can be determined ahead of time from the `root=` kernel command line parameter.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6085/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/333340", "body": "## Fix it in your tree, pullrequest, then I will commit later on. \n\nSent from my Android phone with K-9 Mail. Please excuse my brevity.\n\nnedbass reply@reply.github.com wrote:\n\nMixing tabs and spaces here -- Reply to this email directly or view it on GitHub: https://github.com/behlendorf/zfs/commit/6583dcacdcca2aad7eaec51f31797a3533845099#commitcomment-333338\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/333340/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/643311", "body": "Oh, so you're using the hostid file from the output of the hostid command.\n\nClever!  This is clearly better than my work.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/643311/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/643317", "body": "Sorry, my bad :-)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/643317/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/1854019", "body": "YEAH!\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/1854019/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/3571320", "body": "I am adding this to the list of awesome shit that fixes my life's problems to try.  You, sir, are a gentleman and a scholar.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/3571320/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/3617819", "body": "I will test it for you with a machine where this pool half-imported problem is endemic.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/3617819/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/4117481", "body": "I think it is fixed.  Haven't experienced problems anymore.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/4117481/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6052758", "body": "This will cause automatic pool import when this module is loaded, which may happen way before block devices are present.\n\nThe patch also doesn't seem to contemplate support for root on ZFS.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6052758/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6052775", "body": "I note there doesn't seem to be support for root on ZFS yet, and mounting file systems is done globally (-a) preventing interleaving of non-ZFS and ZFS file systems at different depths of the hierarchy of the mount tree.\n\n:-\\  My tree had a different implementation that has its problems, of course, but does support both features.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6052775/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/8252868", "body": "I find the suggestion \"No need to force-import on every reboot anymore.\" specious, since a crashed machine will still require a forced import.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/8252868/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11071", "body": "don't hardcode the paths, please.  otherwise it will fail depending on where the utilities are installed.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11071/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11072", "body": "we sync.  we expect no unmounting to happen here since it either will fail if core file systems are mounted and have files open, or successfully unmount the file systems only to make the later initscripts crap out horribly because core file systems are not available anymore.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11072/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11073", "body": "show better status here\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11251", "body": "what should I do with these two lines?  Remove them?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11251/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11252", "body": "should I re-add this file?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11252/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11289", "body": "I will add this right now.\n\nEl Wednesday, March 23, 2011, behlendorf escribi\u00f3:\n\n> > @@ -1,6 +0,0 @@\n> > \n> > ## -Stub file for 'make dist' distdir rule.\n> > \n> > -This file is directly referenced by ../Makefile.am as a source\n> > -file and thus will be expected by 'make dist'.  To avoid this\n> \n> This file should be readded exactly for the reasons mentioned in the file.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11289/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11290", "body": "The file has been re-added as of commit 9549dd1 and pushed too.\n\nNow onto the next revision.\n\nEl Wednesday, March 23, 2011, behlendorf escribi\u00f3:\n\n> > @@ -1,6 +0,0 @@\n> > \n> > ## -Stub file for 'make dist' distdir rule.\n> > \n> > -This file is directly referenced by ../Makefile.am as a source\n> > -file and thus will be expected by 'make dist'.  To avoid this\n> \n> This file should be readded exactly for the reasons mentioned in the file.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11290/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11291", "body": "Commit 5469b88, just pushed, removes those by simply cherry-picking your own \ncommit on top of the merge.\n\nEl Wednesday, March 23, 2011, behlendorf escribi\u00f3:\n\n> > @@ -204,6 +204,8 @@ const struct super_operations zpl_super_operations =\n> > {\n> > \n> > ```\n> > .put_super  = zpl_put_super,\n> > .write_super    = NULL,\n> > .sync_fs    = zpl_sync_fs,\n> > ```\n> > -   .freeze_fs  = NULL,\n> \n> Yes, please remove them.  They are currently unused hooks and they cause\n> compile errors on older platforms.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11291/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11294", "body": "File removed.  Commit pushed.  Let me refresh the page to see how the merge \ndiff will look like.  You should do the same.\n\nEl Wednesday, March 23, 2011, behlendorf escribi\u00f3:\n\n> > @@ -0,0 +1,59 @@\n> > +put the dracut/90zfs directory in /usr/share/dracut/modules.d (or\n> > symlink it)\n> \n> A version of this file was already added to the dracut subdirectory.  If\n> you want to make changes/rewrite it that's fine but let's just keep one\n> copy of it around with the other dracut code.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/11294/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "kcwitt": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6083", "title": "Create zfs.service to Load Kernel Modules", "body": "In the current implementation, the ZFS kernel drivers are loaded in the following two systemd unit files:\r\n\r\n- zfs-import-cache.service (won't run if /etc/zfs/zpool.cache doesn't exist)  \r\n- zfs-import-scan.service (disabled by default)\r\n\r\nSince an /etc/zfs/zpool.cache file doesn't exist after installation and the zfs-import-scan.service is disabled (by default), the kernel drivers are not loaded after reboot of a new install.\r\n\r\nRecommend to add a new oneshot systemd unit called zfs.service specifically to load the kernel modules which is enabled by default and has no conditions. This unit can be RequiredBy=zfs.target and Before:zfs-import-cache.service, zfs-import-scan.service, etc.\r\n\r\nIt is a source of great confusion when people install ZFS, restart the computer and then it doesn't work (because the kernel drivers weren't loaded because zfs-import-scan.service is disabled and zfs-import-cache.service doesn't run because it is conditional on the existence of the /etc/zfs/zpool.cache file).\r\n\r\nThis is a chicken and egg situation where the zfs kernel modules are not loaded because there are no pools, but the user can't create a new pool because the zfs kernel modules are not loaded. Of course the user can manually load the kernel drivers, but for me this turned into a two day R&D session because there is no documentation on the ZoL site for how the systemd modules work.\r\n\r\nThe benefit of the existing approach is that if there are no ZFS pools the kernel modules won't be loaded, but the downside is that it is very confusing for new users (there are alot of \"ZFS doesn't mount on reboot\" questions on the internet which are related to this issue).\r\n\r\nThe benefit of the proposed approach is that on a fresh install with no modifications to the upstream source the zfs kernel modules will be loaded every time.\r\n\r\nIf a package manager deems this is not the desired behavior they can customize it when they build their package, but for everyone else this is probably the expected behavior.\r\n\r\nAlso, if somebody is installing something it is a fair assumption that they want it loaded and working so there isn't much sense in installing ZFS and then not loading the kernel drivers (unless it is part of a linux distribution package, but again, in that case the package maintainer should document for the user whatever customizations they made and how it should be used).", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6083/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tritnaha": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6071", "title": "Add note for GRUB-part for Debian on Wiki", "body": "To avoid possible confusion, change the current order from;\r\n\r\n```\r\n# apt install --yes -t jessie-backports zfs-dkms zfs-initramfs\r\n\r\n# vi /usr/share/initramfs-tools/conf.d/zfs\r\nfor x in $(cat /proc/cmdline)\r\ndo\r\n    case $x in\r\n        root=ZFS=*)\r\n            BOOT=zfs\r\n            ;;\r\n    esac\r\ndone\r\n```\r\n\r\nto\r\n\r\n\r\n\r\n```\r\n# vi /usr/share/initramfs-tools/conf.d/zfs\r\nfor x in $(cat /proc/cmdline)\r\ndo\r\n    case $x in\r\n        root=ZFS=*)\r\n            BOOT=zfs\r\n            ;;\r\n    esac\r\ndone\r\n\r\n# apt install --yes -t jessie-backports zfs-dkms zfs-initramfs\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6071/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "koplover": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6069", "title": "zfs arc_freed causes kernel bug in 0.7.0 RC3 + PR5824 build", "body": "I created a build of 0.7.0 RC3 with PR5824 to investigate the PRs impact on our performance problems (see #4880 for performance impact)\r\n\r\nWe run a Xen virtualised system with ZFS hosted in our driver domain, so providing zvols for each of our hosted guests. This performance testing has been going for approximately one week, last night all access to the server through the virtualised networking was lost, and all VMs were 'frozen'.\r\n\r\nThis is akin to the lock up zfs errors we have seen for 0.6.4.2, but analysing the virtual CPUs assigned to our disk domain did not show any issues. However, the cause was found in the kern.log on next reboot, see trace below:\r\n\r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213467] BUG: unable to handle kernel NULL pointer dereference at 0000000000000007 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213482] IP: [<ffffffffc03f94c1>] buf_hash_find+0xc1/0x130 [zfs] \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213519] PGD 1a50bc067 PUD 25b0f4067 PMD 0 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213527] Oops: 0000 [#1] SMP \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213532] Modules linked in: xen_blkback xen_netback drbd xt_addrtype xt_owner xt_multiport xt_hl xt_tcpudp xt_conntrack xt_NFLOG nfnetlink_log nfnetlink ip6table_mangle iptable_mangle ip6table_nat nf_conntrack_ipv6 nf\r\n_defrag_ipv6 nf_nat_ipv6 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack ip6table_filter ip6_tables iptable_filter ip_tables x_tables crct10dif_pclmul crc32_pclmul ghash_clmulni_intel aesni_intel dm_multipath aes_x86_64 lrw gf128mul scsi_dh g\r\nlue_helper ablk_helper cryptd lru_cache libcrc32c xenfs xen_privcmd zfs(POE) zunicode(POE) icp(POE) zcommon(POE) znvpair(POE) spl(OE) zavl(POE) dm_mirror dm_region_hash dm_log raid0 multipath linear dm_raid raid456 async_raid6_recov async_memcpy async_pq async_xor async_\r\ntx raid1 raid10 xor raid6_pq nvme ahci libahci [last unloaded: drbd] \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213620] CPU: 2 PID: 23967 Comm: z_wr_int_5 Tainted: P \u00a0\u00a0\u00a0B \u00a0\u00a0\u00a0\u00a0\u00a0OE \u00a03.19.0-39-zdomu #44~14.04.1 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213627] task: ffff880284de93a0 ti: ffff8802265c4000 task.ti: ffff8802265c4000 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213632] RIP: e030:[<ffffffffc03f94c1>] \u00a0[<ffffffffc03f94c1>] buf_hash_find+0xc1/0x130 [zfs] \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213658] RSP: e02b:ffff8802265c7b18 \u00a0EFLAGS: 00010202 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213663] RAX: 0000000000000007 RBX: ffff8802265c7b70 RCX: 972d0935c2d85410 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213669] RDX: 0000000000000008 RSI: 0000000000025083 RDI: 000000009f7c4a10 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213675] RBP: ffff8802265c7b58 R08: 972d0935c2d85410 R09: 000000018040003a \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213680] R10: ffffea0009ee6e80 R11: ffffffffc02f1dca R12: ffff8801a005db18 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213686] R13: ffffffffc058d5f0 R14: 009e7e0ecceee9e6 R15: 000000000006ea7a \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213696] FS: \u00a00000000000000000(0000) GS:ffff880294d00000(0000) knlGS:0000000000504d80 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213703] CS: \u00a0e033 DS: 0000 ES: 0000 CR0: 000000008005003b \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213707] CR2: 0000000000000007 CR3: 00000002417ba000 CR4: 0000000000002660 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213712] Stack: \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213715] \u00a0ffff880286ee7400 0000000000025083 0000000000000000 ffff8801a005db18 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213723] \u00a0ffff8802866d4000 0000000000000000 00000000000254ac 0000000000000000 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213731] \u00a0ffff8802265c7b88 ffffffffc0400e65 ffff8802866d4000 0000000000000000 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213739] Call Trace: \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213768] \u00a0[<ffffffffc0400e65>] arc_freed+0x25/0x90 [zfs] \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213823] \u00a0[<ffffffffc04b63c5>] zio_free_sync+0x45/0x140 [zfs] \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213834] \u00a0[<ffffffff8177d86e>] ? __slab_free+0x101/0x2c1 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213882] \u00a0[<ffffffffc04b6cc3>] zio_free+0xc3/0x120 [zfs] \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213919] \u00a0[<ffffffffc043f3b1>] dsl_free+0x11/0x20 [zfs] \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213954] \u00a0[<ffffffffc04276d8>] dsl_dataset_block_kill+0x248/0x450 [zfs] \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.213984] \u00a0[<ffffffffc0409df2>] dbuf_write_done+0x172/0x1c0 [zfs] \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.214009] \u00a0[<ffffffffc03fe256>] arc_write_done+0x236/0x3d0 [zfs] \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.214055] \u00a0[<ffffffffc04b8c49>] zio_done+0x2f9/0xc10 [zfs] \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.214070] \u00a0[<ffffffffc02f1dca>] ? spl_kmem_free+0x2a/0x40 [spl] \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.214115] \u00a0[<ffffffffc04b3bfc>] zio_execute+0x9c/0x100 [zfs] \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.214128] \u00a0[<ffffffffc02f5783>] taskq_thread+0x243/0x480 [spl] \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.214137] \u00a0[<ffffffff8109ea40>] ? wake_up_state+0x20/0x20 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.214148] \u00a0[<ffffffffc02f5540>] ? taskq_cancel_id+0x120/0x120 [spl] \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.214155] \u00a0[<ffffffff81091539>] kthread+0xc9/0xe0 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.214162] \u00a0[<ffffffff81091470>] ? kthread_create_on_node+0x1c0/0x1c0 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.214171] \u00a0[<ffffffff817882d8>] ret_from_fork+0x58/0x90 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.214178] \u00a0[<ffffffff81091470>] ? kthread_create_on_node+0x1c0/0x1c0 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.214182] Code: 38 c1 48 8b 05 c9 a2 16 00 4a 8b 04 f8 48 85 c0 74 40 49 8b 14 24 48 8b 75 c8 eb 0f 66 0f 1f 44 00 00 48 8b 40 20 48 85 c0 74 27 <48> 39 10 75 f2 49 8b 7c 24 08 48 39 78 08 75 e7 48 3b 70 10 75 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.214247] RIP \u00a0[<ffffffffc03f94c1>] buf_hash_find+0xc1/0x130 [zfs] \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.214272] \u00a0RSP <ffff8802265c7b18> \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.214276] CR2: 0000000000000007 \r\nApr 25 20:08:19 zdiskdd0000-0026-00-00 kernel: [538931.214283] ---[ end trace 071f472bc78f6c36 ]---", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6069/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "allenzhao64": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6052", "title": "ZFS/NFS does not honor 'root_squash' on CentOS 7.2", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | CentOS\r\nDistribution Version    | 7.2\r\nLinux Kernel                 | 3.10.0-327.el7.x86_64\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.8-1\r\nSPL Version                  | 0.6.5.8-1\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nWhen a ZFS volume is shared via NFS, the root_squash option on /etc/exports is not working anymore, remote root user just functions like a local root. \r\n\r\nTested on the same server, if a non-ZFS volume is shared via NFS,  root_squash option will continue to work. \r\n\r\nWorse, the problem sometimes seem to be transient, occasionally, restarting of nfs may fix the issue (it is not 100% though).  \r\n\r\n### Describe how to reproduce the problem\r\nsay, the /etc/exports contains\r\n/share/Ydir *(ro,root_squash)\r\n\r\nthe share is mounted remotely on /mnt\r\n\r\n[root@winnt ~]# touch /mnt/test-me\r\n[root@winnt ~]# ls -al /mnt/test-me\r\n-rw-r--r-- 1 root root 0 Apr 21 10:14 /mnt/test-me\r\n\r\nWe also notice when we export a local directory  (non-ZFS), the issue reported above would be less likely to happen. \r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n<!-- \r\n*IMPORTANT* - Please mark logs and text output from terminal commands \r\nor else Github will not display them correctly. \r\nAn example is provided below.\r\n\r\nExample:\r\n```\r\nthis is an example how log text should be marked (wrap it with ```)\r\n```\r\n-->\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6052/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "OmenWild": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6036", "title": "ZFS send/receive failed after 96.3TB (87%)", "body": "### System information, sending system\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu \r\nDistribution Version    | 12.04.5 LTS\r\nLinux Kernel                 | 3.13.0-116-generic\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.4-1~precise\r\nSPL Version                  | 0.6.5.4-1~precise\r\n\r\n### System information, receiving system\r\n<!--  add version after \"|\" character -->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       | Ubuntu\r\nDistribution Version    | 16.04.2 LTS\r\nLinux Kernel                 | 4.4.0-72-generic\r\nArchitecture                 | x86_64\r\nZFS Version                  | 0.6.5.6-0ubuntu15\r\nSPL Version                  | 0.6.5.6-0ubuntu4\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nA ZFS send/receive between two hosts connected over InfiniBand stopped 87% of the way through. It moved 96.3TB of data over 4.5 days, then 2 days ago, the zfs process on the receive side exited, possibly due to `cannot share 'aqua-test': share(1M) failed` (see below).\r\n\r\nThe receive host is a very bare instance of Ubuntu 16.04 without the NFS packages installed.\r\n\r\nOddly enough, on the receive side, both mbuffer and netcat are still running, but ps shows no sign of zfs, so I am guessing it exited after it printed that error.\r\n\r\nIt seems to me that a soft error, that could be resolved at a later time, should not cause an operation that takes days to finish, to fail.\r\n\r\n### Describe how to reproduce the problem\r\nSender:\r\n```\r\nzfs send -R -e z0@1 | mbuffer -m 1G | nc.traditional 172.16.3.28 8023\r\n```\r\n\r\nReceiver:\r\n```\r\nnetcat -w 1200 -l -p 8023 | mbuffer -m 5G | zfs receive -d -F aqua-test\r\n```\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\nNo kernel warnings or backtraces on either end, but the receiver noted:\r\n```\r\nin @  400 MiB/s, out @  0.0 KiB/s, 97.1 TiB total, buffer   6% full cannot share 'aqua-test': share(1M) failed\r\nin @  0.0 KiB/s, out @  0.0 KiB/s, 97.1 TiB total, buffer 100% full\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6036/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vmp32k": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6019", "title": "Kernel Panic during 'zfs receive': list_add/del corruption. prev->next should be next", "body": "<!--\r\nThank you for reporting an issue.\r\n\r\n*IMPORTANT* - Please search our issue tracker *before* making a new issue.\r\nIf you cannot find a similar issue, then create a new issue.\r\nhttps://github.com/zfsonlinux/zfs/issues \r\n\r\n*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.\r\nPlease search the wiki and the mailing list archives before asking \r\nquestions on the mailing list.\r\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\r\n\r\nPlease fill in as much of the template as possible.\r\n-->\r\n\r\n### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nDistribution Name       |  Debian GNU/Linux \r\nDistribution Version    | 8.7 (jessie)\r\nLinux Kernel                 |  4.9.2-2~bpo8+1\r\nArchitecture                 |  amd64\r\nZFS Version                  | 0.6.5.9-2~bpo8+1\r\nSPL Version                  |  0.6.5.9-1~bpo8+\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\nI was restoring my pool with zfs receive (sending from a different server) via ethernet, went to eat dinner and got an email that my server wouldn't respond to pings not much later.\r\n\r\n### Describe how to reproduce the problem\r\nn/a\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n```\r\nApr 14 13:05:01 cloud CRON: (munin) CMD (if [ -x /usr/bin/munin-cron ]; then /usr/bin/munin-cron; fi)\r\nApr 14 13:05:01 cloud CRON: (root) CMD (if [ -x /etc/munin/plugins/apt_all ]; then /etc/munin/plugins/apt_all update 7200 12 >/dev/null; elif [ -x /etc/munin/plugins/apt ]; then /etc/munin/plugins/apt update 7200 12 >/dev/null; fi)\r\nApr 14 13:05:41 cloud kernel: [24821.238485] ------------[ cut here ]------------\r\nApr 14 13:05:41 cloud kernel: [24821.238520] WARNING: CPU: 0 PID: 3617 at /home/zumbi/linux-4.9.2/lib/list_debug.c:33 __list_add+0x86/0xb0\r\nApr 14 13:05:41 cloud kernel: [24821.238564] list_add corruption. prev->next should be next (ffff9e7322f1fa80), but was ffff9e72e53f5148. (prev=ffff9e72e53f5148).\r\nApr 14 13:05:41 cloud kernel: [24821.238614] Modules linked in: vmnet(OE) ppdev parport_pc parport binfmt_misc tun vmw_vsock_vmci_transport vsock vmw_vmci vmmon(OE) ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack xfrm_user xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 af_key xfrm_algo iptable_filter ip_tables x_tables nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi bridge stp llc iTCO_wdt iTCO_vendor_support evdev intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate intel_uncore intel_rapl_perf ast ttm drm_kms_helper lpc_ich pcspkr drm serio_raw sg mfd_core shpchp battery\r\nApr 14 13:05:41 cloud kernel: [24821.243708]  ipmi_msghandler video button tpm_infineon tpm_tis tpm_tis_core tpm zfs(POE) zunicode(POE) zavl(POE) zcommon(POE) znvpair(POE) spl(OE) fuse ecryptfs cbc hmac encrypted_keys autofs4 ext4 crc16 jbd2 fscrypto mbcache btrfs xor raid6_pq dm_mod md_mod hid_generic sd_mod usbhid hid crc32c_intel aesni_intel aes_x86_64 glue_helper lrw gf128mul ablk_helper cryptd ahci libahci psmouse libata scsi_mod igb ehci_pci ehci_hcd xhci_pci i2c_i801 xhci_hcd i2c_smbus i2c_algo_bit dca ptp pps_core usbcore thermal fan usb_common fjes\r\nApr 14 13:05:41 cloud kernel: [24821.250360] CPU: 0 PID: 3617 Comm: txg_sync Tainted: P           OE   4.9.0-0.bpo.1-amd64 #1 Debian 4.9.2-2~bpo8+1\r\nApr 14 13:05:41 cloud kernel: [24821.251740] Hardware name: ASUSTeK COMPUTER INC. P9D-V Series/P9D-V Series, BIOS 0704 03/28/2014\r\nApr 14 13:05:41 cloud kernel: [24821.253144]  0000000000000000 ffffffffa312a1f5 ffffab62481cb840 0000000000000000\r\nApr 14 13:05:41 cloud kernel: [24821.254561]  ffffffffa2e77884 ffff9e72e53f5148 ffffab62481cb898 ffff9e72e53f5148\r\nApr 14 13:05:41 cloud kernel: [24821.255982]  ffffffffc09ddea0 0000000000000000 ffff9e72dfeb0480 ffffffffa2e778ff\r\nApr 14 13:05:41 cloud kernel: [24821.257406] Call Trace:\r\nApr 14 13:05:41 cloud kernel: [24821.258820]  [<ffffffffa312a1f5>] ? dump_stack+0x5c/0x77\r\nApr 14 13:05:41 cloud kernel: [24821.260247]  [<ffffffffa2e77884>] ? __warn+0xc4/0xe0\r\nApr 14 13:05:41 cloud kernel: [24821.261705]  [<ffffffffc09ddea0>] ? zio_taskq_member.isra.6.constprop.14+0x70/0x70 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.263139]  [<ffffffffa2e778ff>] ? warn_slowpath_fmt+0x5f/0x80\r\nApr 14 13:05:41 cloud kernel: [24821.264589]  [<ffffffffc0938790>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.266037]  [<ffffffffc0937180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.267452]  [<ffffffffa3148e56>] ? __list_add+0x86/0xb0\r\nApr 14 13:05:41 cloud kernel: [24821.268867]  [<ffffffffc088749a>] ? taskq_dispatch_ent+0xea/0xf0 [spl]\r\nApr 14 13:05:41 cloud kernel: [24821.270316]  [<ffffffffc09ddea0>] ? zio_taskq_member.isra.6.constprop.14+0x70/0x70 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.271762]  [<ffffffffc098a556>] ? spa_taskq_dispatch_ent+0x86/0xc0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.273204]  [<ffffffffc09dd29e>] ? zio_taskq_dispatch+0x8e/0xa0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.274633]  [<ffffffffc09dd2be>] ? zio_issue_async+0xe/0x20 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.276052]  [<ffffffffc09e0b67>] ? zio_nowait+0x77/0xf0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.277462]  [<ffffffffc0945e10>] ? dbuf_sync_list+0xb0/0xc0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.278864]  [<ffffffffc0945ce7>] ? dbuf_sync_indirect+0xb7/0x130 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.280260]  [<ffffffffc0945df9>] ? dbuf_sync_list+0x99/0xc0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.281638]  [<ffffffffc0945ce7>] ? dbuf_sync_indirect+0xb7/0x130 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.283024]  [<ffffffffc0945df9>] ? dbuf_sync_list+0x99/0xc0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.284406]  [<ffffffffc0945ce7>] ? dbuf_sync_indirect+0xb7/0x130 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.285777]  [<ffffffffc0945df9>] ? dbuf_sync_list+0x99/0xc0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.287137]  [<ffffffffc095f513>] ? dnode_sync+0x2b3/0x810 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.288489]  [<ffffffffc094f441>] ? dmu_objset_sync_dnodes+0x91/0xb0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.289839]  [<ffffffffc094f673>] ? dmu_objset_sync+0x213/0x340 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.291162]  [<ffffffffc0938790>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.292488]  [<ffffffffc0937180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.293792]  [<ffffffffc093a1c0>] ? l2arc_read_done+0x420/0x420 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.295103]  [<ffffffffc0967809>] ? dsl_dataset_sync+0x49/0x80 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.296394]  [<ffffffffc097039f>] ? dsl_pool_sync+0x9f/0x450 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.297667]  [<ffffffffc0987d50>] ? spa_sync+0x370/0xb20 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.298884]  [<ffffffffa2ebb3c4>] ? __wake_up+0x34/0x50\r\nApr 14 13:05:41 cloud kernel: [24821.300099]  [<ffffffffc0999f46>] ? txg_sync_thread+0x3c6/0x620 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.301289]  [<ffffffffc0999b80>] ? txg_sync_stop+0xd0/0xd0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.302415]  [<ffffffffc0885ce6>] ? thread_generic_wrapper+0x76/0x90 [spl]\r\nApr 14 13:05:41 cloud kernel: [24821.303517]  [<ffffffffc0885c70>] ? __thread_exit+0x20/0x20 [spl]\r\nApr 14 13:05:41 cloud kernel: [24821.304587]  [<ffffffffa2e974e0>] ? kthread+0xe0/0x100\r\nApr 14 13:05:41 cloud kernel: [24821.305620]  [<ffffffffa2e2476b>] ? __switch_to+0x2bb/0x700\r\nApr 14 13:05:41 cloud kernel: [24821.306623]  [<ffffffffa2e97400>] ? kthread_park+0x60/0x60\r\nApr 14 13:05:41 cloud kernel: [24821.307592]  [<ffffffffa33fa435>] ? ret_from_fork+0x25/0x30\r\nApr 14 13:05:41 cloud kernel: [24821.308524] ---[ end trace 1391a191527a7790 ]---\r\nApr 14 13:05:41 cloud kernel: [24821.309426] ------------[ cut here ]------------\r\nApr 14 13:05:41 cloud kernel: [24821.310302] WARNING: CPU: 0 PID: 3617 at /home/zumbi/linux-4.9.2/lib/list_debug.c:36 __list_add+0xab/0xb0\r\nApr 14 13:05:41 cloud kernel: [24821.311187] list_add double add: new=ffff9e72e53f5148, prev=ffff9e72e53f5148, next=ffff9e7322f1fa80.\r\nApr 14 13:05:41 cloud kernel: [24821.312066] Modules linked in: vmnet(OE) ppdev parport_pc parport binfmt_misc tun vmw_vsock_vmci_transport vsock vmw_vmci vmmon(OE) ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack xfrm_user xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 af_key xfrm_algo iptable_filter ip_tables x_tables nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi bridge stp llc iTCO_wdt iTCO_vendor_support evdev intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate intel_uncore intel_rapl_perf ast ttm drm_kms_helper lpc_ich pcspkr drm serio_raw sg mfd_core shpchp battery\r\nApr 14 13:05:41 cloud kernel: [24821.317854]  ipmi_msghandler video button tpm_infineon tpm_tis tpm_tis_core tpm zfs(POE) zunicode(POE) zavl(POE) zcommon(POE) znvpair(POE) spl(OE) fuse ecryptfs cbc hmac encrypted_keys autofs4 ext4 crc16 jbd2 fscrypto mbcache btrfs xor raid6_pq dm_mod md_mod hid_generic sd_mod usbhid hid crc32c_intel aesni_intel aes_x86_64 glue_helper lrw gf128mul ablk_helper cryptd ahci libahci psmouse libata scsi_mod igb ehci_pci ehci_hcd xhci_pci i2c_i801 xhci_hcd i2c_smbus i2c_algo_bit dca ptp pps_core usbcore thermal fan usb_common fjes\r\nApr 14 13:05:41 cloud kernel: [24821.323277] CPU: 0 PID: 3617 Comm: txg_sync Tainted: P        W  OE   4.9.0-0.bpo.1-amd64 #1 Debian 4.9.2-2~bpo8+1\r\nApr 14 13:05:41 cloud kernel: [24821.324420] Hardware name: ASUSTeK COMPUTER INC. P9D-V Series/P9D-V Series, BIOS 0704 03/28/2014\r\nApr 14 13:05:41 cloud kernel: [24821.325580]  0000000000000000 ffffffffa312a1f5 ffffab62481cb840 0000000000000000\r\nApr 14 13:05:41 cloud kernel: [24821.326747]  ffffffffa2e77884 ffff9e72e53f5148 ffffab62481cb898 ffff9e72e53f5148\r\nApr 14 13:05:41 cloud kernel: [24821.327912]  ffffffffc09ddea0 0000000000000000 ffff9e72dfeb0480 ffffffffa2e778ff\r\nApr 14 13:05:41 cloud kernel: [24821.329086] Call Trace:\r\nApr 14 13:05:41 cloud kernel: [24821.330252]  [<ffffffffa312a1f5>] ? dump_stack+0x5c/0x77\r\nApr 14 13:05:41 cloud kernel: [24821.331430]  [<ffffffffa2e77884>] ? __warn+0xc4/0xe0\r\nApr 14 13:05:41 cloud kernel: [24821.332646]  [<ffffffffc09ddea0>] ? zio_taskq_member.isra.6.constprop.14+0x70/0x70 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.333848]  [<ffffffffa2e778ff>] ? warn_slowpath_fmt+0x5f/0x80\r\nApr 14 13:05:41 cloud kernel: [24821.335075]  [<ffffffffc0938790>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.336315]  [<ffffffffc0937180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.337531]  [<ffffffffa3148e7b>] ? __list_add+0xab/0xb0\r\nApr 14 13:05:41 cloud kernel: [24821.338749]  [<ffffffffc088749a>] ? taskq_dispatch_ent+0xea/0xf0 [spl]\r\nApr 14 13:05:41 cloud kernel: [24821.340007]  [<ffffffffc09ddea0>] ? zio_taskq_member.isra.6.constprop.14+0x70/0x70 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.341281]  [<ffffffffc098a556>] ? spa_taskq_dispatch_ent+0x86/0xc0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.342554]  [<ffffffffc09dd29e>] ? zio_taskq_dispatch+0x8e/0xa0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.343827]  [<ffffffffc09dd2be>] ? zio_issue_async+0xe/0x20 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.345090]  [<ffffffffc09e0b67>] ? zio_nowait+0x77/0xf0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.346347]  [<ffffffffc0945e10>] ? dbuf_sync_list+0xb0/0xc0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.347602]  [<ffffffffc0945ce7>] ? dbuf_sync_indirect+0xb7/0x130 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.348857]  [<ffffffffc0945df9>] ? dbuf_sync_list+0x99/0xc0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.350104]  [<ffffffffc0945ce7>] ? dbuf_sync_indirect+0xb7/0x130 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.351352]  [<ffffffffc0945df9>] ? dbuf_sync_list+0x99/0xc0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.352601]  [<ffffffffc0945ce7>] ? dbuf_sync_indirect+0xb7/0x130 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.353843]  [<ffffffffc0945df9>] ? dbuf_sync_list+0x99/0xc0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.355085]  [<ffffffffc095f513>] ? dnode_sync+0x2b3/0x810 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.356327]  [<ffffffffc094f441>] ? dmu_objset_sync_dnodes+0x91/0xb0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.357569]  [<ffffffffc094f673>] ? dmu_objset_sync+0x213/0x340 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.358802]  [<ffffffffc0938790>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.360044]  [<ffffffffc0937180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.361290]  [<ffffffffc093a1c0>] ? l2arc_read_done+0x420/0x420 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.362547]  [<ffffffffc0967809>] ? dsl_dataset_sync+0x49/0x80 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.363799]  [<ffffffffc097039f>] ? dsl_pool_sync+0x9f/0x450 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.365056]  [<ffffffffc0987d50>] ? spa_sync+0x370/0xb20 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.366261]  [<ffffffffa2ebb3c4>] ? __wake_up+0x34/0x50\r\nApr 14 13:05:41 cloud kernel: [24821.367473]  [<ffffffffc0999f46>] ? txg_sync_thread+0x3c6/0x620 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.368663]  [<ffffffffc0999b80>] ? txg_sync_stop+0xd0/0xd0 [zfs]\r\nApr 14 13:05:41 cloud kernel: [24821.369791]  [<ffffffffc0885ce6>] ? thread_generic_wrapper+0x76/0x90 [spl]\r\nApr 14 13:05:41 cloud kernel: [24821.370892]  [<ffffffffc0885c70>] ? __thread_exit+0x20/0x20 [spl]\r\nApr 14 13:05:41 cloud kernel: [24821.371961]  [<ffffffffa2e974e0>] ? kthread+0xe0/0x100\r\nApr 14 13:05:41 cloud kernel: [24821.372995]  [<ffffffffa2e2476b>] ? __switch_to+0x2bb/0x700\r\nApr 14 13:05:41 cloud kernel: [24821.374001]  [<ffffffffa2e97400>] ? kthread_park+0x60/0x60\r\nApr 14 13:05:41 cloud kernel: [24821.374971]  [<ffffffffa33fa435>] ? ret_from_fork+0x25/0x30\r\nApr 14 13:05:41 cloud kernel: [24821.375905] ---[ end trace 1391a191527a7791 ]---\r\nApr 14 13:05:42 cloud kernel: [24821.385266] ------------[ cut here ]------------\r\nApr 14 13:05:42 cloud kernel: [24821.385290] WARNING: CPU: 0 PID: 3617 at /home/zumbi/linux-4.9.2/lib/list_debug.c:33 __list_add+0x86/0xb0\r\nApr 14 13:05:42 cloud kernel: [24821.385296] list_add corruption. prev->next should be next (ffff9e7322f1fa80), but was ffff9e72e53f5148. (prev=ffff9e72e53f5148).\r\nApr 14 13:05:42 cloud kernel: [24821.385423] Modules linked in: vmnet(OE) ppdev parport_pc parport binfmt_misc tun vmw_vsock_vmci_transport vsock vmw_vmci vmmon(OE) ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack xfrm_user xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 af_key xfrm_algo iptable_filter ip_tables x_tables nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi bridge stp llc iTCO_wdt iTCO_vendor_support evdev intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate intel_uncore intel_rapl_perf ast ttm drm_kms_helper lpc_ich pcspkr drm serio_raw sg mfd_core shpchp battery\r\nApr 14 13:05:42 cloud kernel: [24821.385531]  ipmi_msghandler video button tpm_infineon tpm_tis tpm_tis_core tpm zfs(POE) zunicode(POE) zavl(POE) zcommon(POE) znvpair(POE) spl(OE) fuse ecryptfs cbc hmac encrypted_keys autofs4 ext4 crc16 jbd2 fscrypto mbcache btrfs xor raid6_pq dm_mod md_mod hid_generic sd_mod usbhid hid crc32c_intel aesni_intel aes_x86_64 glue_helper lrw gf128mul ablk_helper cryptd ahci libahci psmouse libata scsi_mod igb ehci_pci ehci_hcd xhci_pci i2c_i801 xhci_hcd i2c_smbus i2c_algo_bit dca ptp pps_core usbcore thermal fan usb_common fjes\r\nApr 14 13:05:42 cloud kernel: [24821.385537] CPU: 0 PID: 3617 Comm: txg_sync Tainted: P        W  OE   4.9.0-0.bpo.1-amd64 #1 Debian 4.9.2-2~bpo8+1\r\nApr 14 13:05:42 cloud kernel: [24821.385539] Hardware name: ASUSTeK COMPUTER INC. P9D-V Series/P9D-V Series, BIOS 0704 03/28/2014\r\nApr 14 13:05:42 cloud kernel: [24821.385555]  0000000000000000 ffffffffa312a1f5 ffffab62481cb840 0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.385559]  ffffffffa2e77884 ffff9e71b180f658 ffffab62481cb898 ffff9e72e53f5148\r\nApr 14 13:05:42 cloud kernel: [24821.385563]  ffffffffc09ddea0 0000000000000000 ffff9e72dfeb0480 ffffffffa2e778ff\r\nApr 14 13:05:42 cloud kernel: [24821.385569] Call Trace:\r\nApr 14 13:05:42 cloud kernel: [24821.385582]  [<ffffffffa312a1f5>] ? dump_stack+0x5c/0x77\r\nApr 14 13:05:42 cloud kernel: [24821.385599]  [<ffffffffa2e77884>] ? __warn+0xc4/0xe0\r\nApr 14 13:05:42 cloud kernel: [24821.385786]  [<ffffffffc09ddea0>] ? zio_taskq_member.isra.6.constprop.14+0x70/0x70 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.385798]  [<ffffffffa2e778ff>] ? warn_slowpath_fmt+0x5f/0x80\r\nApr 14 13:05:42 cloud kernel: [24821.385890]  [<ffffffffc0938790>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.385972]  [<ffffffffc0937180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.385978]  [<ffffffffa3148e56>] ? __list_add+0x86/0xb0\r\nApr 14 13:05:42 cloud kernel: [24821.386006]  [<ffffffffc088749a>] ? taskq_dispatch_ent+0xea/0xf0 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.386133]  [<ffffffffc09ddea0>] ? zio_taskq_member.isra.6.constprop.14+0x70/0x70 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.386260]  [<ffffffffc098a556>] ? spa_taskq_dispatch_ent+0x86/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.386373]  [<ffffffffc09dd29e>] ? zio_taskq_dispatch+0x8e/0xa0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.386481]  [<ffffffffc09dd2be>] ? zio_issue_async+0xe/0x20 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.386590]  [<ffffffffc09e0b67>] ? zio_nowait+0x77/0xf0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.386677]  [<ffffffffc0945e10>] ? dbuf_sync_list+0xb0/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.386764]  [<ffffffffc0945ce7>] ? dbuf_sync_indirect+0xb7/0x130 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.386852]  [<ffffffffc0945df9>] ? dbuf_sync_list+0x99/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.386938]  [<ffffffffc0945ce7>] ? dbuf_sync_indirect+0xb7/0x130 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.387018]  [<ffffffffc0945df9>] ? dbuf_sync_list+0x99/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.387105]  [<ffffffffc0945ce7>] ? dbuf_sync_indirect+0xb7/0x130 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.387191]  [<ffffffffc0945df9>] ? dbuf_sync_list+0x99/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.387294]  [<ffffffffc095f513>] ? dnode_sync+0x2b3/0x810 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.387388]  [<ffffffffc094f441>] ? dmu_objset_sync_dnodes+0x91/0xb0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.387466]  [<ffffffffc094f673>] ? dmu_objset_sync+0x213/0x340 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.387560]  [<ffffffffc0938790>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.387630]  [<ffffffffc0937180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.387700]  [<ffffffffc093a1c0>] ? l2arc_read_done+0x420/0x420 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.387790]  [<ffffffffc0967809>] ? dsl_dataset_sync+0x49/0x80 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.387893]  [<ffffffffc097039f>] ? dsl_pool_sync+0x9f/0x450 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.387996]  [<ffffffffc0987d50>] ? spa_sync+0x370/0xb20 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.388004]  [<ffffffffa2ebb3c4>] ? __wake_up+0x34/0x50\r\nApr 14 13:05:42 cloud kernel: [24821.388115]  [<ffffffffc0999f46>] ? txg_sync_thread+0x3c6/0x620 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.388221]  [<ffffffffc0999b80>] ? txg_sync_stop+0xd0/0xd0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.388232]  [<ffffffffc0885ce6>] ? thread_generic_wrapper+0x76/0x90 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.388247]  [<ffffffffc0885c70>] ? __thread_exit+0x20/0x20 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.388252]  [<ffffffffa2e974e0>] ? kthread+0xe0/0x100\r\nApr 14 13:05:42 cloud kernel: [24821.388267]  [<ffffffffa2e2476b>] ? __switch_to+0x2bb/0x700\r\nApr 14 13:05:42 cloud kernel: [24821.388271]  [<ffffffffa2e97400>] ? kthread_park+0x60/0x60\r\nApr 14 13:05:42 cloud kernel: [24821.388281]  [<ffffffffa33fa435>] ? ret_from_fork+0x25/0x30\r\nApr 14 13:05:42 cloud kernel: [24821.388289] ---[ end trace 1391a191527a7792 ]---\r\nApr 14 13:05:42 cloud kernel: [24821.388450] ------------[ cut here ]------------\r\nApr 14 13:05:42 cloud kernel: [24821.388455] WARNING: CPU: 0 PID: 3617 at /home/zumbi/linux-4.9.2/lib/list_debug.c:33 __list_add+0x86/0xb0\r\nApr 14 13:05:42 cloud kernel: [24821.388457] list_add corruption. prev->next should be next (ffff9e7322f1fa80), but was ffff9e72e53f4cf8. (prev=ffff9e72e53f4cf8).\r\nApr 14 13:05:42 cloud kernel: [24821.388535] Modules linked in: vmnet(OE) ppdev parport_pc parport binfmt_misc tun vmw_vsock_vmci_transport vsock vmw_vmci vmmon(OE) ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack xfrm_user xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 af_key xfrm_algo iptable_filter ip_tables x_tables nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi bridge stp llc iTCO_wdt iTCO_vendor_support evdev intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate intel_uncore intel_rapl_perf ast ttm drm_kms_helper lpc_ich pcspkr drm serio_raw sg mfd_core shpchp battery\r\nApr 14 13:05:42 cloud kernel: [24821.388596]  ipmi_msghandler video button tpm_infineon tpm_tis tpm_tis_core tpm zfs(POE) zunicode(POE) zavl(POE) zcommon(POE) znvpair(POE) spl(OE) fuse ecryptfs cbc hmac encrypted_keys autofs4 ext4 crc16 jbd2 fscrypto mbcache btrfs xor raid6_pq dm_mod md_mod hid_generic sd_mod usbhid hid crc32c_intel aesni_intel aes_x86_64 glue_helper lrw gf128mul ablk_helper cryptd ahci libahci psmouse libata scsi_mod igb ehci_pci ehci_hcd xhci_pci i2c_i801 xhci_hcd i2c_smbus i2c_algo_bit dca ptp pps_core usbcore thermal fan usb_common fjes\r\nApr 14 13:05:42 cloud kernel: [24821.388604] CPU: 0 PID: 3617 Comm: txg_sync Tainted: P        W  OE   4.9.0-0.bpo.1-amd64 #1 Debian 4.9.2-2~bpo8+1\r\nApr 14 13:05:42 cloud kernel: [24821.388605] Hardware name: ASUSTeK COMPUTER INC. P9D-V Series/P9D-V Series, BIOS 0704 03/28/2014\r\nApr 14 13:05:42 cloud kernel: [24821.388608]  0000000000000000 ffffffffa312a1f5 ffffab62481cb840 0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.388611]  ffffffffa2e77884 ffff9e72e53f4cf8 ffffab62481cb898 ffff9e72e53f4cf8\r\nApr 14 13:05:42 cloud kernel: [24821.388614]  ffffffffc09ddea0 0000000000000000 ffff9e72dfeb0480 ffffffffa2e778ff\r\nApr 14 13:05:42 cloud kernel: [24821.388618] Call Trace:\r\nApr 14 13:05:42 cloud kernel: [24821.388629]  [<ffffffffa312a1f5>] ? dump_stack+0x5c/0x77\r\nApr 14 13:05:42 cloud kernel: [24821.388634]  [<ffffffffa2e77884>] ? __warn+0xc4/0xe0\r\nApr 14 13:05:42 cloud kernel: [24821.388723]  [<ffffffffc09ddea0>] ? zio_taskq_member.isra.6.constprop.14+0x70/0x70 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.388738]  [<ffffffffa2e778ff>] ? warn_slowpath_fmt+0x5f/0x80\r\nApr 14 13:05:42 cloud kernel: [24821.388792]  [<ffffffffc0938790>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.388853]  [<ffffffffc0937180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.388856]  [<ffffffffa3148e56>] ? __list_add+0x86/0xb0\r\nApr 14 13:05:42 cloud kernel: [24821.388875]  [<ffffffffc088749a>] ? taskq_dispatch_ent+0xea/0xf0 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.388966]  [<ffffffffc09ddea0>] ? zio_taskq_member.isra.6.constprop.14+0x70/0x70 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.389046]  [<ffffffffc098a556>] ? spa_taskq_dispatch_ent+0x86/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.389119]  [<ffffffffc09dd29e>] ? zio_taskq_dispatch+0x8e/0xa0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.389199]  [<ffffffffc09dd2be>] ? zio_issue_async+0xe/0x20 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.389271]  [<ffffffffc09e0b67>] ? zio_nowait+0x77/0xf0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.389353]  [<ffffffffc0945e10>] ? dbuf_sync_list+0xb0/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.389410]  [<ffffffffc0945ce7>] ? dbuf_sync_indirect+0xb7/0x130 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.389491]  [<ffffffffc0945df9>] ? dbuf_sync_list+0x99/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.389558]  [<ffffffffc0945ce7>] ? dbuf_sync_indirect+0xb7/0x130 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.389611]  [<ffffffffc0945df9>] ? dbuf_sync_list+0x99/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.389675]  [<ffffffffc0945ce7>] ? dbuf_sync_indirect+0xb7/0x130 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.389742]  [<ffffffffc0945df9>] ? dbuf_sync_list+0x99/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.389810]  [<ffffffffc095f513>] ? dnode_sync+0x2b3/0x810 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.389875]  [<ffffffffc094f441>] ? dmu_objset_sync_dnodes+0x91/0xb0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.389940]  [<ffffffffc094f673>] ? dmu_objset_sync+0x213/0x340 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.390006]  [<ffffffffc0938790>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.390051]  [<ffffffffc0937180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.390111]  [<ffffffffc093a1c0>] ? l2arc_read_done+0x420/0x420 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.390180]  [<ffffffffc0967809>] ? dsl_dataset_sync+0x49/0x80 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.390243]  [<ffffffffc097039f>] ? dsl_pool_sync+0x9f/0x450 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.390314]  [<ffffffffc0987d50>] ? spa_sync+0x370/0xb20 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.390317]  [<ffffffffa2ebb3c4>] ? __wake_up+0x34/0x50\r\nApr 14 13:05:42 cloud kernel: [24821.390399]  [<ffffffffc0999f46>] ? txg_sync_thread+0x3c6/0x620 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.390472]  [<ffffffffc0999b80>] ? txg_sync_stop+0xd0/0xd0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.390495]  [<ffffffffc0885ce6>] ? thread_generic_wrapper+0x76/0x90 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.390501]  [<ffffffffc0885c70>] ? __thread_exit+0x20/0x20 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.390511]  [<ffffffffa2e974e0>] ? kthread+0xe0/0x100\r\nApr 14 13:05:42 cloud kernel: [24821.390515]  [<ffffffffa2e2476b>] ? __switch_to+0x2bb/0x700\r\nApr 14 13:05:42 cloud kernel: [24821.390518]  [<ffffffffa2e97400>] ? kthread_park+0x60/0x60\r\nApr 14 13:05:42 cloud kernel: [24821.390532]  [<ffffffffa33fa435>] ? ret_from_fork+0x25/0x30\r\nApr 14 13:05:42 cloud kernel: [24821.390533] ---[ end trace 1391a191527a7793 ]---\r\nApr 14 13:05:42 cloud kernel: [24821.390534] ------------[ cut here ]------------\r\nApr 14 13:05:42 cloud kernel: [24821.390538] WARNING: CPU: 0 PID: 3617 at /home/zumbi/linux-4.9.2/lib/list_debug.c:36 __list_add+0xab/0xb0\r\nApr 14 13:05:42 cloud kernel: [24821.390540] list_add double add: new=ffff9e72e53f4cf8, prev=ffff9e72e53f4cf8, next=ffff9e7322f1fa80.\r\nApr 14 13:05:42 cloud kernel: [24821.390603] Modules linked in: vmnet(OE) ppdev parport_pc parport binfmt_misc tun vmw_vsock_vmci_transport vsock vmw_vmci vmmon(OE) ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack xfrm_user xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 af_key xfrm_algo iptable_filter ip_tables x_tables nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi bridge stp llc iTCO_wdt iTCO_vendor_support evdev intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate intel_uncore intel_rapl_perf ast ttm drm_kms_helper lpc_ich pcspkr drm serio_raw sg mfd_core shpchp battery\r\nApr 14 13:05:42 cloud kernel: [24821.390663]  ipmi_msghandler video button tpm_infineon tpm_tis tpm_tis_core tpm zfs(POE) zunicode(POE) zavl(POE) zcommon(POE) znvpair(POE) spl(OE) fuse ecryptfs cbc hmac encrypted_keys autofs4 ext4 crc16 jbd2 fscrypto mbcache btrfs xor raid6_pq dm_mod md_mod hid_generic sd_mod usbhid hid crc32c_intel aesni_intel aes_x86_64 glue_helper lrw gf128mul ablk_helper cryptd ahci libahci psmouse libata scsi_mod igb ehci_pci ehci_hcd xhci_pci i2c_i801 xhci_hcd i2c_smbus i2c_algo_bit dca ptp pps_core usbcore thermal fan usb_common fjes\r\nApr 14 13:05:42 cloud kernel: [24821.390666] CPU: 0 PID: 3617 Comm: txg_sync Tainted: P        W  OE   4.9.0-0.bpo.1-amd64 #1 Debian 4.9.2-2~bpo8+1\r\nApr 14 13:05:42 cloud kernel: [24821.390667] Hardware name: ASUSTeK COMPUTER INC. P9D-V Series/P9D-V Series, BIOS 0704 03/28/2014\r\nApr 14 13:05:42 cloud kernel: [24821.390670]  0000000000000000 ffffffffa312a1f5 ffffab62481cb840 0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.390673]  ffffffffa2e77884 ffff9e72e53f4cf8 ffffab62481cb898 ffff9e72e53f4cf8\r\nApr 14 13:05:42 cloud kernel: [24821.390676]  ffffffffc09ddea0 0000000000000000 ffff9e72dfeb0480 ffffffffa2e778ff\r\nApr 14 13:05:42 cloud kernel: [24821.390679] Call Trace:\r\nApr 14 13:05:42 cloud kernel: [24821.390691]  [<ffffffffa312a1f5>] ? dump_stack+0x5c/0x77\r\nApr 14 13:05:42 cloud kernel: [24821.390696]  [<ffffffffa2e77884>] ? __warn+0xc4/0xe0\r\nApr 14 13:05:42 cloud kernel: [24821.390785]  [<ffffffffc09ddea0>] ? zio_taskq_member.isra.6.constprop.14+0x70/0x70 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.390797]  [<ffffffffa2e778ff>] ? warn_slowpath_fmt+0x5f/0x80\r\nApr 14 13:05:42 cloud kernel: [24821.390849]  [<ffffffffc0938790>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.390903]  [<ffffffffc0937180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.390913]  [<ffffffffa3148e7b>] ? __list_add+0xab/0xb0\r\nApr 14 13:05:42 cloud kernel: [24821.390921]  [<ffffffffc088749a>] ? taskq_dispatch_ent+0xea/0xf0 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.391007]  [<ffffffffc09ddea0>] ? zio_taskq_member.isra.6.constprop.14+0x70/0x70 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.391092]  [<ffffffffc098a556>] ? spa_taskq_dispatch_ent+0x86/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.391162]  [<ffffffffc09dd29e>] ? zio_taskq_dispatch+0x8e/0xa0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.391233]  [<ffffffffc09dd2be>] ? zio_issue_async+0xe/0x20 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.391318]  [<ffffffffc09e0b67>] ? zio_nowait+0x77/0xf0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.391379]  [<ffffffffc0945e10>] ? dbuf_sync_list+0xb0/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.391435]  [<ffffffffc0945ce7>] ? dbuf_sync_indirect+0xb7/0x130 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.391492]  [<ffffffffc0945df9>] ? dbuf_sync_list+0x99/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.391549]  [<ffffffffc0945ce7>] ? dbuf_sync_indirect+0xb7/0x130 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.391606]  [<ffffffffc0945df9>] ? dbuf_sync_list+0x99/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.391668]  [<ffffffffc0945ce7>] ? dbuf_sync_indirect+0xb7/0x130 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.391719]  [<ffffffffc0945df9>] ? dbuf_sync_list+0x99/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.391786]  [<ffffffffc095f513>] ? dnode_sync+0x2b3/0x810 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.391853]  [<ffffffffc094f441>] ? dmu_objset_sync_dnodes+0x91/0xb0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.391918]  [<ffffffffc094f673>] ? dmu_objset_sync+0x213/0x340 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.391977]  [<ffffffffc0938790>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.392027]  [<ffffffffc0937180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.392087]  [<ffffffffc093a1c0>] ? l2arc_read_done+0x420/0x420 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.392156]  [<ffffffffc0967809>] ? dsl_dataset_sync+0x49/0x80 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.392226]  [<ffffffffc097039f>] ? dsl_pool_sync+0x9f/0x450 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.392309]  [<ffffffffc0987d50>] ? spa_sync+0x370/0xb20 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.392313]  [<ffffffffa2ebb3c4>] ? __wake_up+0x34/0x50\r\nApr 14 13:05:42 cloud kernel: [24821.392399]  [<ffffffffc0999f46>] ? txg_sync_thread+0x3c6/0x620 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.392485]  [<ffffffffc0999b80>] ? txg_sync_stop+0xd0/0xd0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.392493]  [<ffffffffc0885ce6>] ? thread_generic_wrapper+0x76/0x90 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.392500]  [<ffffffffc0885c70>] ? __thread_exit+0x20/0x20 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.392507]  [<ffffffffa2e974e0>] ? kthread+0xe0/0x100\r\nApr 14 13:05:42 cloud kernel: [24821.392511]  [<ffffffffa2e2476b>] ? __switch_to+0x2bb/0x700\r\nApr 14 13:05:42 cloud kernel: [24821.392514]  [<ffffffffa2e97400>] ? kthread_park+0x60/0x60\r\nApr 14 13:05:42 cloud kernel: [24821.392527]  [<ffffffffa33fa435>] ? ret_from_fork+0x25/0x30\r\nApr 14 13:05:42 cloud kernel: [24821.392529] ---[ end trace 1391a191527a7794 ]---\r\nApr 14 13:05:42 cloud kernel: [24821.397578] ------------[ cut here ]------------\r\nApr 14 13:05:42 cloud kernel: [24821.397585] WARNING: CPU: 0 PID: 3617 at /home/zumbi/linux-4.9.2/lib/list_debug.c:33 __list_add+0x86/0xb0\r\nApr 14 13:05:42 cloud kernel: [24821.397588] list_add corruption. prev->next should be next (ffff9e7322f1fa80), but was ffff9e72e53f4cf8. (prev=ffff9e72e53f4cf8).\r\nApr 14 13:05:42 cloud kernel: [24821.397637] Modules linked in: vmnet(OE) ppdev parport_pc parport binfmt_misc tun vmw_vsock_vmci_transport vsock vmw_vmci vmmon(OE) ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack xfrm_user xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 af_key xfrm_algo iptable_filter ip_tables x_tables nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi bridge stp llc iTCO_wdt iTCO_vendor_support evdev intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate intel_uncore intel_rapl_perf ast ttm drm_kms_helper lpc_ich pcspkr drm serio_raw sg mfd_core shpchp battery\r\nApr 14 13:05:42 cloud kernel: [24821.397677]  ipmi_msghandler video button tpm_infineon tpm_tis tpm_tis_core tpm zfs(POE) zunicode(POE) zavl(POE) zcommon(POE) znvpair(POE) spl(OE) fuse ecryptfs cbc hmac encrypted_keys autofs4 ext4 crc16 jbd2 fscrypto mbcache btrfs xor raid6_pq dm_mod md_mod hid_generic sd_mod usbhid hid crc32c_intel aesni_intel aes_x86_64 glue_helper lrw gf128mul ablk_helper cryptd ahci libahci psmouse libata scsi_mod igb ehci_pci ehci_hcd xhci_pci i2c_i801 xhci_hcd i2c_smbus i2c_algo_bit dca ptp pps_core usbcore thermal fan usb_common fjes\r\nApr 14 13:05:42 cloud kernel: [24821.397681] CPU: 0 PID: 3617 Comm: txg_sync Tainted: P        W  OE   4.9.0-0.bpo.1-amd64 #1 Debian 4.9.2-2~bpo8+1\r\nApr 14 13:05:42 cloud kernel: [24821.397682] Hardware name: ASUSTeK COMPUTER INC. P9D-V Series/P9D-V Series, BIOS 0704 03/28/2014\r\nApr 14 13:05:42 cloud kernel: [24821.397686]  0000000000000000 ffffffffa312a1f5 ffffab62481cb840 0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.397689]  ffffffffa2e77884 ffff9e6f3115be38 ffffab62481cb898 ffff9e72e53f4cf8\r\nApr 14 13:05:42 cloud kernel: [24821.397692]  ffffffffc09ddea0 0000000000000000 ffff9e72dfeb0480 ffffffffa2e778ff\r\nApr 14 13:05:42 cloud kernel: [24821.397693] Call Trace:\r\nApr 14 13:05:42 cloud kernel: [24821.397701]  [<ffffffffa312a1f5>] ? dump_stack+0x5c/0x77\r\nApr 14 13:05:42 cloud kernel: [24821.397706]  [<ffffffffa2e77884>] ? __warn+0xc4/0xe0\r\nApr 14 13:05:42 cloud kernel: [24821.397772]  [<ffffffffc09ddea0>] ? zio_taskq_member.isra.6.constprop.14+0x70/0x70 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.397777]  [<ffffffffa2e778ff>] ? warn_slowpath_fmt+0x5f/0x80\r\nApr 14 13:05:42 cloud kernel: [24821.397817]  [<ffffffffc0938790>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.397852]  [<ffffffffc0937180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.397856]  [<ffffffffa3148e56>] ? __list_add+0x86/0xb0\r\nApr 14 13:05:42 cloud kernel: [24821.397865]  [<ffffffffc088749a>] ? taskq_dispatch_ent+0xea/0xf0 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.397917]  [<ffffffffc09ddea0>] ? zio_taskq_member.isra.6.constprop.14+0x70/0x70 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.397970]  [<ffffffffc098a556>] ? spa_taskq_dispatch_ent+0x86/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398020]  [<ffffffffc09dd29e>] ? zio_taskq_dispatch+0x8e/0xa0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398068]  [<ffffffffc09dd2be>] ? zio_issue_async+0xe/0x20 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398116]  [<ffffffffc09e0b67>] ? zio_nowait+0x77/0xf0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398152]  [<ffffffffc0945e10>] ? dbuf_sync_list+0xb0/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398188]  [<ffffffffc0945ce7>] ? dbuf_sync_indirect+0xb7/0x130 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398224]  [<ffffffffc0945df9>] ? dbuf_sync_list+0x99/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398259]  [<ffffffffc0945ce7>] ? dbuf_sync_indirect+0xb7/0x130 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398293]  [<ffffffffc0945df9>] ? dbuf_sync_list+0x99/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398329]  [<ffffffffc0945ce7>] ? dbuf_sync_indirect+0xb7/0x130 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398363]  [<ffffffffc0945df9>] ? dbuf_sync_list+0x99/0xc0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398405]  [<ffffffffc095f513>] ? dnode_sync+0x2b3/0x810 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398445]  [<ffffffffc094f441>] ? dmu_objset_sync_dnodes+0x91/0xb0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398484]  [<ffffffffc094f673>] ? dmu_objset_sync+0x213/0x340 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398517]  [<ffffffffc0938790>] ? arc_cksum_compute.isra.10+0xa0/0xa0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398549]  [<ffffffffc0937180>] ? arc_evictable_memory+0x80/0x80 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398582]  [<ffffffffc093a1c0>] ? l2arc_read_done+0x420/0x420 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398625]  [<ffffffffc0967809>] ? dsl_dataset_sync+0x49/0x80 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398669]  [<ffffffffc097039f>] ? dsl_pool_sync+0x9f/0x450 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398719]  [<ffffffffc0987d50>] ? spa_sync+0x370/0xb20 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398723]  [<ffffffffa2ebb3c4>] ? __wake_up+0x34/0x50\r\nApr 14 13:05:42 cloud kernel: [24821.398776]  [<ffffffffc0999f46>] ? txg_sync_thread+0x3c6/0x620 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398826]  [<ffffffffc0999b80>] ? txg_sync_stop+0xd0/0xd0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.398835]  [<ffffffffc0885ce6>] ? thread_generic_wrapper+0x76/0x90 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.398843]  [<ffffffffc0885c70>] ? __thread_exit+0x20/0x20 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.398847]  [<ffffffffa2e974e0>] ? kthread+0xe0/0x100\r\nApr 14 13:05:42 cloud kernel: [24821.398851]  [<ffffffffa2e2476b>] ? __switch_to+0x2bb/0x700\r\nApr 14 13:05:42 cloud kernel: [24821.398854]  [<ffffffffa2e97400>] ? kthread_park+0x60/0x60\r\nApr 14 13:05:42 cloud kernel: [24821.398858]  [<ffffffffa33fa435>] ? ret_from_fork+0x25/0x30\r\nApr 14 13:05:42 cloud kernel: [24821.398861] ---[ end trace 1391a191527a7795 ]---\r\nApr 14 13:05:42 cloud kernel: [24821.426739] ------------[ cut here ]------------\r\nApr 14 13:05:42 cloud kernel: [24821.426754] WARNING: CPU: 1 PID: 3119 at /home/zumbi/linux-4.9.2/lib/list_debug.c:62 taskq_thread+0x1e7/0x450 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.426756] list_del corruption. next->prev should be ffff9e71b6fc2418, but was ffff9e72e53f48a8\r\nApr 14 13:05:42 cloud kernel: [24821.426799] Modules linked in: vmnet(OE) ppdev parport_pc parport binfmt_misc tun vmw_vsock_vmci_transport vsock vmw_vmci vmmon(OE) ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack xfrm_user xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 af_key xfrm_algo iptable_filter ip_tables x_tables nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi bridge stp llc iTCO_wdt iTCO_vendor_support evdev intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate intel_uncore intel_rapl_perf ast ttm drm_kms_helper lpc_ich pcspkr drm serio_raw sg mfd_core shpchp battery\r\nApr 14 13:05:42 cloud kernel: [24821.426831]  ipmi_msghandler video button tpm_infineon tpm_tis tpm_tis_core tpm zfs(POE) zunicode(POE) zavl(POE) zcommon(POE) znvpair(POE) spl(OE) fuse ecryptfs cbc hmac encrypted_keys autofs4 ext4 crc16 jbd2 fscrypto mbcache btrfs xor raid6_pq dm_mod md_mod hid_generic sd_mod usbhid hid crc32c_intel aesni_intel aes_x86_64 glue_helper lrw gf128mul ablk_helper cryptd ahci libahci psmouse libata scsi_mod igb ehci_pci ehci_hcd xhci_pci i2c_i801 xhci_hcd i2c_smbus i2c_algo_bit dca ptp pps_core usbcore thermal fan usb_common fjes\r\nApr 14 13:05:42 cloud kernel: [24821.426835] CPU: 1 PID: 3119 Comm: z_wr_iss Tainted: P        W  OE   4.9.0-0.bpo.1-amd64 #1 Debian 4.9.2-2~bpo8+1\r\nApr 14 13:05:42 cloud kernel: [24821.426835] Hardware name: ASUSTeK COMPUTER INC. P9D-V Series/P9D-V Series, BIOS 0704 03/28/2014\r\nApr 14 13:05:42 cloud kernel: [24821.426840]  0000000000000000 ffffffffa312a1f5 ffffab6249fc7dd0 0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.426843]  ffffffffa2e77884 ffff9e731b451380 ffffab6249fc7e28 ffff9e73196dc100\r\nApr 14 13:05:42 cloud kernel: [24821.426845]  000000000000008f ffff9e71b6fc2418 ffff9e7322f1fa00 ffffffffa2e778ff\r\nApr 14 13:05:42 cloud kernel: [24821.426846] Call Trace:\r\nApr 14 13:05:42 cloud kernel: [24821.426852]  [<ffffffffa312a1f5>] ? dump_stack+0x5c/0x77\r\nApr 14 13:05:42 cloud kernel: [24821.426856]  [<ffffffffa2e77884>] ? __warn+0xc4/0xe0\r\nApr 14 13:05:42 cloud kernel: [24821.426859]  [<ffffffffa2e778ff>] ? warn_slowpath_fmt+0x5f/0x80\r\nApr 14 13:05:42 cloud kernel: [24821.426862]  [<ffffffffa2ebb3c4>] ? __wake_up+0x34/0x50\r\nApr 14 13:05:42 cloud kernel: [24821.426870]  [<ffffffffc0887147>] ? taskq_thread+0x1e7/0x450 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.426873]  [<ffffffffa2ea2b60>] ? wake_up_q+0x60/0x60\r\nApr 14 13:05:42 cloud kernel: [24821.426879]  [<ffffffffc0886f60>] ? taskq_thread_spawn+0x50/0x50 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.426882]  [<ffffffffa2e974e0>] ? kthread+0xe0/0x100\r\nApr 14 13:05:42 cloud kernel: [24821.426885]  [<ffffffffa2e2476b>] ? __switch_to+0x2bb/0x700\r\nApr 14 13:05:42 cloud kernel: [24821.426887]  [<ffffffffa2e97400>] ? kthread_park+0x60/0x60\r\nApr 14 13:05:42 cloud kernel: [24821.426890]  [<ffffffffa33fa435>] ? ret_from_fork+0x25/0x30\r\nApr 14 13:05:42 cloud kernel: [24821.426891] ---[ end trace 1391a191527a7796 ]---\r\nApr 14 13:05:42 cloud kernel: [24821.426920] BUG: unable to handle kernel NULL pointer dereference at           (null)\r\nApr 14 13:05:42 cloud kernel: [24821.426921] IP: [<          (null)>]           (null)\r\nApr 14 13:05:42 cloud kernel: [24821.426923] PGD 655877067 \r\nApr 14 13:05:42 cloud kernel: [24821.426923] PUD 655876067 \r\nApr 14 13:05:42 cloud kernel: [24821.426924] PMD 0 \r\nApr 14 13:05:42 cloud kernel: [24821.426924] \r\nApr 14 13:05:42 cloud kernel: [24821.426926] Oops: 0010 [#1] SMP\r\nApr 14 13:05:42 cloud kernel: [24821.426960] Modules linked in: vmnet(OE) ppdev parport_pc parport binfmt_misc tun vmw_vsock_vmci_transport vsock vmw_vmci vmmon(OE) ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack xfrm_user xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 af_key xfrm_algo iptable_filter ip_tables x_tables nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi bridge stp llc iTCO_wdt iTCO_vendor_support evdev intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate intel_uncore intel_rapl_perf ast ttm drm_kms_helper lpc_ich pcspkr drm serio_raw sg mfd_core shpchp battery\r\nApr 14 13:05:42 cloud kernel: [24821.426985]  ipmi_msghandler video button tpm_infineon tpm_tis tpm_tis_core tpm zfs(POE) zunicode(POE) zavl(POE) zcommon(POE) znvpair(POE) spl(OE) fuse ecryptfs cbc hmac encrypted_keys autofs4 ext4 crc16 jbd2 fscrypto mbcache btrfs xor raid6_pq dm_mod md_mod hid_generic sd_mod usbhid hid crc32c_intel aesni_intel aes_x86_64 glue_helper lrw gf128mul ablk_helper cryptd ahci libahci psmouse libata scsi_mod igb ehci_pci ehci_hcd xhci_pci i2c_i801 xhci_hcd i2c_smbus i2c_algo_bit dca ptp pps_core usbcore thermal fan usb_common fjes\r\nApr 14 13:05:42 cloud kernel: [24821.426988] CPU: 0 PID: 3118 Comm: z_wr_iss Tainted: P        W  OE   4.9.0-0.bpo.1-amd64 #1 Debian 4.9.2-2~bpo8+1\r\nApr 14 13:05:42 cloud kernel: [24821.426989] Hardware name: ASUSTeK COMPUTER INC. P9D-V Series/P9D-V Series, BIOS 0704 03/28/2014\r\nApr 14 13:05:42 cloud kernel: [24821.426990] task: ffff9e73193c70c0 task.stack: ffffab6249fbc000\r\nApr 14 13:05:42 cloud kernel: [24821.426992] RIP: 0010:[<0000000000000000>]  [<          (null)>]           (null)\r\nApr 14 13:05:42 cloud kernel: [24821.426993] RSP: 0018:ffffab6249fbfdc8  EFLAGS: 00010246\r\nApr 14 13:05:42 cloud kernel: [24821.426993] RAX: 0000000000000000 RBX: 0000000000000020 RCX: 0000000000020000\r\nApr 14 13:05:42 cloud kernel: [24821.426994] RDX: ffff9e700b46dd60 RSI: 0000000000020000 RDI: ffffab63bb4e0000\r\nApr 14 13:05:42 cloud kernel: [24821.426995] RBP: 00000000ffffffff R08: 0000000000000000 R09: ffff9e700b46dd00\r\nApr 14 13:05:42 cloud kernel: [24821.426996] R10: ffffab63bb4e0000 R11: ffff9e72237b58b0 R12: 00000000000a0000\r\nApr 14 13:05:42 cloud kernel: [24821.426997] R13: ffff9e71b6fc2000 R14: ffff9e71b6fc2418 R15: ffff9e7322f1fa00\r\nApr 14 13:05:42 cloud kernel: [24821.426998] FS:  0000000000000000(0000) GS:ffff9e733fc00000(0000) knlGS:0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.426999] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nApr 14 13:05:42 cloud kernel: [24821.427000] CR2: 0000000000000000 CR3: 00000006559d8000 CR4: 00000000001406f0\r\nApr 14 13:05:42 cloud kernel: [24821.427001] Stack:\r\nApr 14 13:05:42 cloud kernel: [24821.427004]  ffffffffc09e3140 ffff9e7322f1fac8 0000000000000046 0000000000000003\r\nApr 14 13:05:42 cloud kernel: [24821.427006]  0000000000000000 000000004d2f4e0b 0000000000000020 ffffffffc09ddc44\r\nApr 14 13:05:42 cloud kernel: [24821.427008]  ffffffffc09ddf2e ffff9e7320faf600 ffff9e7320faf610 ffff9e73193c70c0\r\nApr 14 13:05:42 cloud kernel: [24821.427008] Call Trace:\r\nApr 14 13:05:42 cloud kernel: [24821.427056]  [<ffffffffc09e3140>] ? zio_checksum_compute+0xf0/0x180 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.427092]  [<ffffffffc09ddc44>] ? zio_checksum_generate+0x44/0x60 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.427128]  [<ffffffffc09ddf2e>] ? zio_execute+0x8e/0xf0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.427134]  [<ffffffffc08871af>] ? taskq_thread+0x24f/0x450 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.427137]  [<ffffffffa2ea2b60>] ? wake_up_q+0x60/0x60\r\nApr 14 13:05:42 cloud kernel: [24821.427142]  [<ffffffffc0886f60>] ? taskq_thread_spawn+0x50/0x50 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.427145]  [<ffffffffa2e974e0>] ? kthread+0xe0/0x100\r\nApr 14 13:05:42 cloud kernel: [24821.427148]  [<ffffffffa2e2476b>] ? __switch_to+0x2bb/0x700\r\nApr 14 13:05:42 cloud kernel: [24821.427150]  [<ffffffffa2e97400>] ? kthread_park+0x60/0x60\r\nApr 14 13:05:42 cloud kernel: [24821.427153]  [<ffffffffa33fa435>] ? ret_from_fork+0x25/0x30\r\nApr 14 13:05:42 cloud kernel: [24821.427157] Code:  Bad RIP value.\r\nApr 14 13:05:42 cloud kernel: [24821.427158] RIP  [<          (null)>]           (null)\r\nApr 14 13:05:42 cloud kernel: [24821.427159]  RSP <ffffab6249fbfdc8>\r\nApr 14 13:05:42 cloud kernel: [24821.427160] CR2: 0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.432702] ---[ end trace 1391a191527a7797 ]---\r\nApr 14 13:05:42 cloud kernel: [24821.432704] divide error: 0000 [#2] SMP\r\nApr 14 13:05:42 cloud kernel: [24821.433557] Modules linked in: vmnet(OE) ppdev parport_pc parport binfmt_misc tun vmw_vsock_vmci_transport vsock vmw_vmci vmmon(OE) ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack xfrm_user xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 af_key xfrm_algo iptable_filter ip_tables x_tables nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi bridge stp llc iTCO_wdt iTCO_vendor_support evdev intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate intel_uncore intel_rapl_perf ast ttm drm_kms_helper lpc_ich pcspkr drm serio_raw sg mfd_core shpchp battery\r\nApr 14 13:05:42 cloud kernel: [24821.433590]  ipmi_msghandler video button tpm_infineon tpm_tis tpm_tis_core tpm zfs(POE) zunicode(POE) zavl(POE) zcommon(POE) znvpair(POE) spl(OE) fuse ecryptfs cbc hmac encrypted_keys autofs4 ext4 crc16 jbd2 fscrypto mbcache btrfs xor raid6_pq dm_mod md_mod hid_generic sd_mod usbhid hid crc32c_intel aesni_intel aes_x86_64 glue_helper lrw gf128mul ablk_helper cryptd ahci libahci psmouse libata scsi_mod igb ehci_pci ehci_hcd xhci_pci i2c_i801 xhci_hcd i2c_smbus i2c_algo_bit dca ptp pps_core usbcore thermal fan usb_common fjes\r\nApr 14 13:05:42 cloud kernel: [24821.433593] CPU: 2 PID: 3120 Comm: z_wr_iss Tainted: P      D W  OE   4.9.0-0.bpo.1-amd64 #1 Debian 4.9.2-2~bpo8+1\r\nApr 14 13:05:42 cloud kernel: [24821.433594] Hardware name: ASUSTeK COMPUTER INC. P9D-V Series/P9D-V Series, BIOS 0704 03/28/2014\r\nApr 14 13:05:42 cloud kernel: [24821.433595] task: ffff9e731971e140 task.stack: ffffab6249fcc000\r\nApr 14 13:05:42 cloud kernel: [24821.433651] RIP: 0010:[<ffffffffc0994720>]  [<ffffffffc0994720>] spa_get_random+0x30/0x60 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.433652] RSP: 0018:ffffab6249fcfcf0  EFLAGS: 00010246\r\nApr 14 13:05:42 cloud kernel: [24821.433653] RAX: f2ebf95f081b2d1c RBX: 0000000000000000 RCX: 0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.433654] RDX: 0000000000000000 RSI: 0000000000000000 RDI: ffffab6249fcfcc0\r\nApr 14 13:05:42 cloud kernel: [24821.433654] RBP: 0000000000000000 R08: 0000000013cfc79a R09: ffffab6249fcfc80\r\nApr 14 13:05:42 cloud kernel: [24821.433655] R10: 0000000087c458cf R11: 00000000f9ad25bd R12: ffff9e700b46dd00\r\nApr 14 13:05:42 cloud kernel: [24821.433656] R13: ffff9e7317828000 R14: 0000000000000000 R15: ffff9e71b6fc2000\r\nApr 14 13:05:42 cloud kernel: [24821.433658] FS:  0000000000000000(0000) GS:ffff9e733fd00000(0000) knlGS:0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.433659] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nApr 14 13:05:42 cloud kernel: [24821.433660] CR2: 00007faadfc7d000 CR3: 000000050d007000 CR4: 00000000001406e0\r\nApr 14 13:05:42 cloud kernel: [24821.433661] Stack:\r\nApr 14 13:05:42 cloud kernel: [24821.433664]  f2ebf95f081b2d1c 0000000021599c3f ffff9e6f105872e0 ffffffffc09a403a\r\nApr 14 13:05:42 cloud kernel: [24821.433667]  ffff9e7317828000 ffffab63bb4ffff4 ffffab65495e0004 ffffffffa33f770e\r\nApr 14 13:05:42 cloud kernel: [24821.433669]  ffff9e72dfeb0468 0000000021599c3f ffff9e71b6fc2000 0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.433670] Call Trace:\r\nApr 14 13:05:42 cloud kernel: [24821.433715]  [<ffffffffc09a403a>] ? vdev_mirror_map_alloc+0x24a/0x330 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.433719]  [<ffffffffa33f770e>] ? mutex_lock+0xe/0x30\r\nApr 14 13:05:42 cloud kernel: [24821.433762]  [<ffffffffc09a452b>] ? vdev_mirror_io_start+0x1b/0x180 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.433764]  [<ffffffffa33f770e>] ? mutex_lock+0xe/0x30\r\nApr 14 13:05:42 cloud kernel: [24821.433806]  [<ffffffffc09956ef>] ? spa_config_enter+0xbf/0x100 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.433848]  [<ffffffffc09dd721>] ? zio_vdev_io_start+0x1c1/0x2b0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.433889]  [<ffffffffc09ddf2e>] ? zio_execute+0x8e/0xf0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.433897]  [<ffffffffc08871af>] ? taskq_thread+0x24f/0x450 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.433901]  [<ffffffffa2ea2b60>] ? wake_up_q+0x60/0x60\r\nApr 14 13:05:42 cloud kernel: [24821.433907]  [<ffffffffc0886f60>] ? taskq_thread_spawn+0x50/0x50 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.433910]  [<ffffffffa2e974e0>] ? kthread+0xe0/0x100\r\nApr 14 13:05:42 cloud kernel: [24821.433913]  [<ffffffffa2e2476b>] ? __switch_to+0x2bb/0x700\r\nApr 14 13:05:42 cloud kernel: [24821.433916]  [<ffffffffa2e97400>] ? kthread_park+0x60/0x60\r\nApr 14 13:05:42 cloud kernel: [24821.433918]  [<ffffffffa33fa435>] ? ret_from_fork+0x25/0x30\r\nApr 14 13:05:42 cloud kernel: [24821.433948] Code: 53 be 08 00 00 00 48 89 fb 48 83 ec 10 48 89 e7 65 48 8b 04 25 28 00 00 00 48 89 44 24 08 31 c0 e8 76 d6 8a e2 48 8b 04 24 31 d2 <48> f7 f3 48 8b 4c 24 08 65 48 33 0c 25 28 00 00 00 75 09 48 83 \r\nApr 14 13:05:42 cloud kernel: [24821.433990] RIP  [<ffffffffc0994720>] spa_get_random+0x30/0x60 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.433992]  RSP <ffffab6249fcfcf0>\r\nApr 14 13:05:42 cloud kernel: [24821.433997] BUG: unable to handle kernel NULL pointer dereference at           (null)\r\nApr 14 13:05:42 cloud kernel: [24821.434052] IP: [<ffffffffc09a42cd>] vdev_mirror_io_done+0x1d/0x260 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.434053] ---[ end trace 1391a191527a7798 ]---\r\nApr 14 13:05:42 cloud kernel: [24821.434054] PGD 654034067 \r\nApr 14 13:05:42 cloud kernel: [24821.434055] PUD 654046067 \r\nApr 14 13:05:42 cloud kernel: [24821.434056] PMD 0 \r\nApr 14 13:05:42 cloud kernel: [24821.434056] \r\nApr 14 13:05:42 cloud kernel: [24821.434057] Oops: 0000 [#3] SMP\r\nApr 14 13:05:42 cloud kernel: [24821.434092] Modules linked in: vmnet(OE) ppdev parport_pc parport binfmt_misc tun vmw_vsock_vmci_transport vsock vmw_vmci vmmon(OE) ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack xfrm_user xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 af_key xfrm_algo iptable_filter ip_tables x_tables nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi bridge stp llc iTCO_wdt iTCO_vendor_support evdev intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate intel_uncore intel_rapl_perf ast ttm drm_kms_helper lpc_ich pcspkr drm serio_raw sg mfd_core shpchp battery\r\nApr 14 13:05:42 cloud kernel: [24821.434119]  ipmi_msghandler video button tpm_infineon tpm_tis tpm_tis_core tpm zfs(POE) zunicode(POE) zavl(POE) zcommon(POE) znvpair(POE) spl(OE) fuse ecryptfs cbc hmac encrypted_keys autofs4 ext4 crc16 jbd2 fscrypto mbcache btrfs xor raid6_pq dm_mod md_mod hid_generic sd_mod usbhid hid crc32c_intel aesni_intel aes_x86_64 glue_helper lrw gf128mul ablk_helper cryptd ahci libahci psmouse libata scsi_mod igb ehci_pci ehci_hcd xhci_pci i2c_i801 xhci_hcd i2c_smbus i2c_algo_bit dca ptp pps_core usbcore thermal fan usb_common fjes\r\nApr 14 13:05:42 cloud kernel: [24821.434121] CPU: 1 PID: 3119 Comm: z_wr_iss Tainted: P      D W  OE   4.9.0-0.bpo.1-amd64 #1 Debian 4.9.2-2~bpo8+1\r\nApr 14 13:05:42 cloud kernel: [24821.434122] Hardware name: ASUSTeK COMPUTER INC. P9D-V Series/P9D-V Series, BIOS 0704 03/28/2014\r\nApr 14 13:05:42 cloud kernel: [24821.434123] task: ffff9e73196dc100 task.stack: ffffab6249fc4000\r\nApr 14 13:05:42 cloud kernel: [24821.434163] RIP: 0010:[<ffffffffc09a42cd>]  [<ffffffffc09a42cd>] vdev_mirror_io_done+0x1d/0x260 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.434164] RSP: 0018:ffffab6249fc7d90  EFLAGS: 00010286\r\nApr 14 13:05:42 cloud kernel: [24821.434165] RAX: 0000000000000000 RBX: ffff9e71b6fc2000 RCX: 0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.434166] RDX: 0000000000000001 RSI: 0000000000000000 RDI: ffff9e71b6fc2000\r\nApr 14 13:05:42 cloud kernel: [24821.434167] RBP: ffff9e71b6fc2000 R08: 0000000000000000 R09: 0000000000000001\r\nApr 14 13:05:42 cloud kernel: [24821.434168] R10: 0000000000000021 R11: 0000000000000000 R12: 0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.434169] R13: ffff9e71b6fc2000 R14: ffff9e71b6fc2418 R15: ffff9e7322f1fa00\r\nApr 14 13:05:42 cloud kernel: [24821.434170] FS:  0000000000000000(0000) GS:ffff9e733fc80000(0000) knlGS:0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.434171] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nApr 14 13:05:42 cloud kernel: [24821.434172] CR2: 0000000000000000 CR3: 0000000655300000 CR4: 00000000001406e0\r\nApr 14 13:05:42 cloud kernel: [24821.434173] Stack:\r\nApr 14 13:05:42 cloud kernel: [24821.434176]  ffff9e71b6fc2418 000000000000008f ffff9e73196dc100 ffff9e731b451390\r\nApr 14 13:05:42 cloud kernel: [24821.434178]  ffffffffa33f770e ffff9e71b6fc2000 0000000000000000 ffffffffc09f9940\r\nApr 14 13:05:42 cloud kernel: [24821.434180]  ffff9e71b6fc2000 ffff9e71b6fc2418 ffff9e7322f1fa00 ffffffffc09de1ff\r\nApr 14 13:05:42 cloud kernel: [24821.434181] Call Trace:\r\nApr 14 13:05:42 cloud kernel: [24821.434184]  [<ffffffffa33f770e>] ? mutex_lock+0xe/0x30\r\nApr 14 13:05:42 cloud kernel: [24821.434224]  [<ffffffffc09de1ff>] ? zio_vdev_io_done+0x3f/0x160 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.434262]  [<ffffffffc09ddf2e>] ? zio_execute+0x8e/0xf0 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.434269]  [<ffffffffc08871af>] ? taskq_thread+0x24f/0x450 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.434272]  [<ffffffffa2ea2b60>] ? wake_up_q+0x60/0x60\r\nApr 14 13:05:42 cloud kernel: [24821.434277]  [<ffffffffc0886f60>] ? taskq_thread_spawn+0x50/0x50 [spl]\r\nApr 14 13:05:42 cloud kernel: [24821.434280]  [<ffffffffa2e974e0>] ? kthread+0xe0/0x100\r\nApr 14 13:05:42 cloud kernel: [24821.434282]  [<ffffffffa2e2476b>] ? __switch_to+0x2bb/0x700\r\nApr 14 13:05:42 cloud kernel: [24821.434284]  [<ffffffffa2e97400>] ? kthread_park+0x60/0x60\r\nApr 14 13:05:42 cloud kernel: [24821.434286]  [<ffffffffa33fa435>] ? ret_from_fork+0x25/0x30\r\nApr 14 13:05:42 cloud kernel: [24821.434315] Code: eb ed 31 c0 eb e9 0f 1f 84 00 00 00 00 00 0f 1f 44 00 00 41 57 41 56 41 55 41 54 55 53 48 89 fd 48 83 ec 28 4c 8b a7 18 02 00 00 <41> 8b 34 24 85 f6 0f 8e 61 01 00 00 8d 56 ff 49 8d 44 24 28 31 \r\nApr 14 13:05:42 cloud kernel: [24821.434355] RIP  [<ffffffffc09a42cd>] vdev_mirror_io_done+0x1d/0x260 [zfs]\r\nApr 14 13:05:42 cloud kernel: [24821.434356]  RSP <ffffab6249fc7d90>\r\nApr 14 13:05:42 cloud kernel: [24821.434356] CR2: 0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.434358] ---[ end trace 1391a191527a7799 ]---\r\nApr 14 13:05:42 cloud kernel: [24821.520319] BUG: unable to handle kernel paging request at 000004feffff35ac\r\nApr 14 13:05:42 cloud kernel: [24821.520333] IP: [<ffffffffa2ec3ef4>] native_queued_spin_lock_slowpath+0x104/0x190\r\nApr 14 13:05:42 cloud kernel: [24821.520340] PGD 0 \r\nApr 14 13:05:42 cloud kernel: [24821.520341] \r\nApr 14 13:05:42 cloud kernel: [24821.520348] Oops: 0002 [#4] SMP\r\nApr 14 13:05:42 cloud kernel: [24821.520593] Modules linked in: vmnet(OE) ppdev parport_pc parport binfmt_misc tun vmw_vsock_vmci_transport vsock vmw_vmci vmmon(OE) ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack xfrm_user xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 af_key xfrm_algo iptable_filter ip_tables x_tables nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi bridge stp llc iTCO_wdt iTCO_vendor_support evdev intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate intel_uncore intel_rapl_perf ast ttm drm_kms_helper lpc_ich pcspkr drm serio_raw sg mfd_core shpchp battery\r\nApr 14 13:05:42 cloud kernel: [24821.520656]  ipmi_msghandler video button tpm_infineon tpm_tis tpm_tis_core tpm zfs(POE) zunicode(POE) zavl(POE) zcommon(POE) znvpair(POE) spl(OE) fuse ecryptfs cbc hmac encrypted_keys autofs4 ext4 crc16 jbd2 fscrypto mbcache btrfs xor raid6_pq dm_mod md_mod hid_generic sd_mod usbhid hid crc32c_intel aesni_intel aes_x86_64 glue_helper lrw gf128mul ablk_helper cryptd ahci libahci psmouse libata scsi_mod igb ehci_pci ehci_hcd xhci_pci i2c_i801 xhci_hcd i2c_smbus i2c_algo_bit dca ptp pps_core usbcore thermal fan usb_common fjes\r\nApr 14 13:05:42 cloud kernel: [24821.520663] CPU: 0 PID: 3118 Comm: z_wr_iss Tainted: P      D W  OE   4.9.0-0.bpo.1-amd64 #1 Debian 4.9.2-2~bpo8+1\r\nApr 14 13:05:42 cloud kernel: [24821.520665] Hardware name: ASUSTeK COMPUTER INC. P9D-V Series/P9D-V Series, BIOS 0704 03/28/2014\r\nApr 14 13:05:42 cloud kernel: [24821.520668] task: ffff9e73193c70c0 task.stack: ffffab6249fbc000\r\nApr 14 13:05:42 cloud kernel: [24821.520681] RIP: 0010:[<ffffffffa2ec3ef4>]  [<ffffffffa2ec3ef4>] native_queued_spin_lock_slowpath+0x104/0x190\r\nApr 14 13:05:42 cloud kernel: [24821.520684] RSP: 0018:ffffab6249fbfe98  EFLAGS: 00010006\r\nApr 14 13:05:42 cloud kernel: [24821.520688] RAX: 000000000000134a RBX: 0000000000000282 RCX: ffff9e733fc19300\r\nApr 14 13:05:42 cloud kernel: [24821.520691] RDX: 000004feffff35ac RSI: 000000004d2f4e0b RDI: ffffab6249fbff18\r\nApr 14 13:05:42 cloud kernel: [24821.520693] RBP: ffffab6249fbff10 R08: 0000000000040000 R09: 0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.520696] R10: 0000000000000008 R11: 0000000000000000 R12: ffffab6249fbfd18\r\nApr 14 13:05:42 cloud kernel: [24821.520698] R13: 0000000000000000 R14: 0000000000000046 R15: 0000000000000001\r\nApr 14 13:05:42 cloud kernel: [24821.520703] FS:  0000000000000000(0000) GS:ffff9e733fc00000(0000) knlGS:0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.520706] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nApr 14 13:05:42 cloud kernel: [24821.520709] CR2: 000004feffff35ac CR3: 00000006559d8000 CR4: 00000000001406f0\r\nApr 14 13:05:42 cloud kernel: [24821.520711] Stack:\r\nApr 14 13:05:42 cloud kernel: [24821.520719]  ffffffffa33fa0e2 ffffab6249fbff18 ffffffffa2ebbc78 ffff9e73193c77f0\r\nApr 14 13:05:42 cloud kernel: [24821.520726]  ffff9e73193c70c0 ffffab6249fbfd18 ffffffffa2e74be0 ffff9e73193c70c0\r\nApr 14 13:05:42 cloud kernel: [24821.520733]  0000000000000009 ffffffffa2e7bc0b ffff9e7320faf600 0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.520734] Call Trace:\r\nApr 14 13:05:42 cloud kernel: [24821.520744]  [<ffffffffa33fa0e2>] ? _raw_spin_lock_irqsave+0x32/0x40\r\nApr 14 13:05:42 cloud kernel: [24821.520751]  [<ffffffffa2ebbc78>] ? complete+0x18/0x40\r\nApr 14 13:05:42 cloud kernel: [24821.520759]  [<ffffffffa2e74be0>] ? mm_release+0xb0/0x130\r\nApr 14 13:05:42 cloud kernel: [24821.520765]  [<ffffffffa2e7bc0b>] ? do_exit+0x14b/0xb50\r\nApr 14 13:05:42 cloud kernel: [24821.520774]  [<ffffffffa33fb997>] ? rewind_stack_do_exit+0x17/0x20\r\nApr 14 13:05:42 cloud kernel: [24821.520911] Code: c1 e0 10 45 31 c9 85 c0 74 44 48 89 c2 c1 e8 12 48 c1 ea 0c 83 e8 01 83 e2 30 48 98 48 81 c2 00 93 01 00 48 03 14 c5 c0 94 86 a3 <48> 89 0a 8b 41 08 85 c0 75 09 f3 90 8b 41 08 85 c0 74 f7 4c 8b \r\nApr 14 13:05:42 cloud kernel: [24821.520924] RIP  [<ffffffffa2ec3ef4>] native_queued_spin_lock_slowpath+0x104/0x190\r\nApr 14 13:05:42 cloud kernel: [24821.520928]  RSP <ffffab6249fbfe98>\r\nApr 14 13:05:42 cloud kernel: [24821.520932] CR2: 000004feffff35ac\r\nApr 14 13:05:42 cloud kernel: [24821.520941] ---[ end trace 1391a191527a779a ]---\r\nApr 14 13:05:42 cloud kernel: [24821.520953] BUG: unable to handle kernel paging request at ffffffffa38dca26\r\nApr 14 13:05:42 cloud kernel: [24821.523477] IP: [<ffffffffa2ec3ef4>] native_queued_spin_lock_slowpath+0x104/0x190\r\nApr 14 13:05:42 cloud kernel: [24821.523481] PGD 50d00a067 \r\nApr 14 13:05:42 cloud kernel: [24821.523484] PUD 50d00b063 \r\nApr 14 13:05:42 cloud kernel: [24821.523485] PMD 800000050ce001e1 \r\nApr 14 13:05:42 cloud kernel: [24821.523485] \r\nApr 14 13:05:42 cloud kernel: [24821.523489] Oops: 0003 [#5] SMP\r\nApr 14 13:05:42 cloud kernel: [24821.523715] Modules linked in: vmnet(OE) ppdev parport_pc parport binfmt_misc tun vmw_vsock_vmci_transport vsock vmw_vmci vmmon(OE) ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack xfrm_user xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 af_key xfrm_algo iptable_filter ip_tables x_tables nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi bridge stp llc iTCO_wdt iTCO_vendor_support evdev intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate intel_uncore intel_rapl_perf ast ttm drm_kms_helper lpc_ich pcspkr drm serio_raw sg mfd_core shpchp battery\r\nApr 14 13:05:42 cloud kernel: [24821.523786]  ipmi_msghandler video button tpm_infineon tpm_tis tpm_tis_core tpm zfs(POE) zunicode(POE) zavl(POE) zcommon(POE) znvpair(POE) spl(OE) fuse ecryptfs cbc hmac encrypted_keys autofs4 ext4 crc16 jbd2 fscrypto mbcache btrfs xor raid6_pq dm_mod md_mod hid_generic sd_mod usbhid hid crc32c_intel aesni_intel aes_x86_64 glue_helper lrw gf128mul ablk_helper cryptd ahci libahci psmouse libata scsi_mod igb ehci_pci ehci_hcd xhci_pci i2c_i801 xhci_hcd i2c_smbus i2c_algo_bit dca ptp pps_core usbcore thermal fan usb_common fjes\r\nApr 14 13:05:42 cloud kernel: [24821.523792] CPU: 1 PID: 3119 Comm: z_wr_iss Tainted: P      D W  OE   4.9.0-0.bpo.1-amd64 #1 Debian 4.9.2-2~bpo8+1\r\nApr 14 13:05:42 cloud kernel: [24821.523794] Hardware name: ASUSTeK COMPUTER INC. P9D-V Series/P9D-V Series, BIOS 0704 03/28/2014\r\nApr 14 13:05:42 cloud kernel: [24821.523797] task: ffff9e73196dc100 task.stack: ffffab6249fc4000\r\nApr 14 13:05:42 cloud kernel: [24821.523807] RIP: 0010:[<ffffffffa2ec3ef4>]  [<ffffffffa2ec3ef4>] native_queued_spin_lock_slowpath+0x104/0x190\r\nApr 14 13:05:42 cloud kernel: [24821.523810] RSP: 0018:ffffab6249fc7e98  EFLAGS: 00010082\r\nApr 14 13:05:42 cloud kernel: [24821.523813] RAX: 0000000000002a87 RBX: 0000000000000282 RCX: ffff9e733fc99300\r\nApr 14 13:05:42 cloud kernel: [24821.523816] RDX: ffffffffa38dca26 RSI: 00000000aa231e9c RDI: ffffab6249fc7f18\r\nApr 14 13:05:42 cloud kernel: [24821.523819] RBP: ffffab6249fc7f10 R08: 0000000000080000 R09: 0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.523821] R10: 0000000000000000 R11: 0000000000000004 R12: ffffab6249fc7ce8\r\nApr 14 13:05:42 cloud kernel: [24821.523824] R13: 0000000000000000 R14: 0000000000000046 R15: 0000000000000001\r\nApr 14 13:05:42 cloud kernel: [24821.523828] FS:  0000000000000000(0000) GS:ffff9e733fc80000(0000) knlGS:0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.523830] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nApr 14 13:05:42 cloud kernel: [24821.523833] CR2: ffffffffa38dca26 CR3: 0000000655300000 CR4: 00000000001406e0\r\nApr 14 13:05:42 cloud kernel: [24821.523835] Stack:\r\nApr 14 13:05:42 cloud kernel: [24821.523842]  ffffffffa33fa0e2 ffffab6249fc7f18 ffffffffa2ebbc78 ffff9e73196dc830\r\nApr 14 13:05:42 cloud kernel: [24821.523849]  ffff9e73196dc100 ffffab6249fc7ce8 ffffffffa2e74be0 ffff9e73196dc100\r\nApr 14 13:05:42 cloud kernel: [24821.523856]  0000000000000009 ffffffffa2e7bc0b ffff9e731b451380 0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.523857] Call Trace:\r\nApr 14 13:05:42 cloud kernel: [24821.523865]  [<ffffffffa33fa0e2>] ? _raw_spin_lock_irqsave+0x32/0x40\r\nApr 14 13:05:42 cloud kernel: [24821.523872]  [<ffffffffa2ebbc78>] ? complete+0x18/0x40\r\nApr 14 13:05:42 cloud kernel: [24821.523880]  [<ffffffffa2e74be0>] ? mm_release+0xb0/0x130\r\nApr 14 13:05:42 cloud kernel: [24821.523885]  [<ffffffffa2e7bc0b>] ? do_exit+0x14b/0xb50\r\nApr 14 13:05:42 cloud kernel: [24821.523893]  [<ffffffffa33fb997>] ? rewind_stack_do_exit+0x17/0x20\r\nApr 14 13:05:42 cloud kernel: [24821.523977] Code: c1 e0 10 45 31 c9 85 c0 74 44 48 89 c2 c1 e8 12 48 c1 ea 0c 83 e8 01 83 e2 30 48 98 48 81 c2 00 93 01 00 48 03 14 c5 c0 94 86 a3 <48> 89 0a 8b 41 08 85 c0 75 09 f3 90 8b 41 08 85 c0 74 f7 4c 8b \r\nApr 14 13:05:42 cloud kernel: [24821.523985] RIP  [<ffffffffa2ec3ef4>] native_queued_spin_lock_slowpath+0x104/0x190\r\nApr 14 13:05:42 cloud kernel: [24821.523987]  RSP <ffffab6249fc7e98>\r\nApr 14 13:05:42 cloud kernel: [24821.523989] CR2: ffffffffa38dca26\r\nApr 14 13:05:42 cloud kernel: [24821.523992] ---[ end trace 1391a191527a779b ]---\r\nApr 14 13:05:42 cloud kernel: [24821.523997] general protection fault: 0000 [#6] SMP\r\nApr 14 13:05:42 cloud kernel: [24821.524084] Modules linked in: vmnet(OE) ppdev parport_pc parport binfmt_misc tun vmw_vsock_vmci_transport vsock vmw_vmci vmmon(OE) ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack xfrm_user xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 af_key xfrm_algo iptable_filter ip_tables x_tables nfsd auth_rpcgss nfs_acl nfs lockd grace fscache sunrpc ib_iser rdma_cm iw_cm ib_cm ib_core configfs iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi bridge stp llc iTCO_wdt iTCO_vendor_support evdev intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass crct10dif_pclmul crc32_pclmul ghash_clmulni_intel intel_cstate intel_uncore intel_rapl_perf ast ttm drm_kms_helper lpc_ich pcspkr drm serio_raw sg mfd_core shpchp battery\r\nApr 14 13:05:42 cloud kernel: [24821.524153]  ipmi_msghandler video button tpm_infineon tpm_tis tpm_tis_core tpm zfs(POE) zunicode(POE) zavl(POE) zcommon(POE) znvpair(POE) spl(OE) fuse ecryptfs cbc hmac encrypted_keys autofs4 ext4 crc16 jbd2 fscrypto mbcache btrfs xor raid6_pq dm_mod md_mod hid_generic sd_mod usbhid hid crc32c_intel aesni_intel aes_x86_64 glue_helper lrw gf128mul ablk_helper cryptd ahci libahci psmouse libata scsi_mod igb ehci_pci ehci_hcd xhci_pci i2c_i801 xhci_hcd i2c_smbus i2c_algo_bit dca ptp pps_core usbcore thermal fan usb_common fjes\r\nApr 14 13:05:42 cloud kernel: [24821.524159] CPU: 2 PID: 3120 Comm: z_wr_iss Tainted: P      D W  OE   4.9.0-0.bpo.1-amd64 #1 Debian 4.9.2-2~bpo8+1\r\nApr 14 13:05:42 cloud kernel: [24821.524161] Hardware name: ASUSTeK COMPUTER INC. P9D-V Series/P9D-V Series, BIOS 0704 03/28/2014\r\nApr 14 13:05:42 cloud kernel: [24821.524165] task: ffff9e731971e140 task.stack: ffffab6249fcc000\r\nApr 14 13:05:42 cloud kernel: [24821.524178] RIP: 0010:[<ffffffffa2ec3ef4>]  [<ffffffffa2ec3ef4>] native_queued_spin_lock_slowpath+0x104/0x190\r\nApr 14 13:05:42 cloud kernel: [24821.524182] RSP: 0018:ffffab6249fcfe98  EFLAGS: 00010002\r\nApr 14 13:05:42 cloud kernel: [24821.524185] RAX: 0000000000000855 RBX: 0000000000000282 RCX: ffff9e733fd19300\r\nApr 14 13:05:42 cloud kernel: [24821.524188] RDX: 0070616d6573f27f RSI: 0000000021599c3f RDI: ffffab6249fcff18\r\nApr 14 13:05:42 cloud kernel: [24821.524190] RBP: ffffab6249fcff10 R08: 00000000000c0000 R09: 0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.524193] R10: 0000000000000000 R11: 0000000000000000 R12: ffffab6249fcfc48\r\nApr 14 13:05:42 cloud kernel: [24821.524195] R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000001\r\nApr 14 13:05:42 cloud kernel: [24821.524199] FS:  0000000000000000(0000) GS:ffff9e733fd00000(0000) knlGS:0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.524203] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\nApr 14 13:05:42 cloud kernel: [24821.524205] CR2: 00007faadfc7d000 CR3: 0000000655289000 CR4: 00000000001406e0\r\nApr 14 13:05:42 cloud kernel: [24821.524207] Stack:\r\nApr 14 13:05:42 cloud kernel: [24821.524215]  ffffffffa33fa0e2 ffffab6249fcff18 ffffffffa2ebbc78 ffff9e731971e870\r\nApr 14 13:05:42 cloud kernel: [24821.524223]  ffff9e731971e140 ffffab6249fcfc48 ffffffffa2e74be0 ffff9e731971e140\r\nApr 14 13:05:42 cloud kernel: [24821.524229]  000000000000000b ffffffffa2e7bc0b ffff9e73226b2580 0000000000000000\r\nApr 14 13:05:42 cloud kernel: [24821.524230] Call Trace:\r\nApr 14 13:05:42 cloud kernel: [24821.524239]  [<ffffffffa33fa0e2>] ? _raw_spin_lock_irqsave+0x32/0x40\r\nApr 14 13:05:42 cloud kernel: [24821.524245]  [<ffffffffa2ebbc78>] ? complete+0x18/0x40\r\nApr 14 13:05:42 cloud kernel: [24821.524254]  [<ffffffffa2e74be0>] ? mm_release+0xb0/0x130\r\nApr 14 13:05:42 cloud kernel: [24821.524259]  [<ffffffffa2e7bc0b>] ? do_exit+0x14b/0xb50\r\nApr 14 13:05:42 cloud kernel: [24821.524267]  [<ffffffffa33fb997>] ? rewind_stack_do_exit+0x17/0x20\r\nApr 14 13:05:42 cloud kernel: [24821.524350] Code: c1 e0 10 45 31 c9 85 c0 74 44 48 89 c2 c1 e8 12 48 c1 ea 0c 83 e8 01 83 e2 30 48 98 48 81 c2 00 93 01 00 48 03 14 c5 c0 94 86 a3 <48> 89 0a 8b 41 08 85 c0 75 09 f3 90 8b 41 08 85 c0 74 f7 4c 8b \r\nApr 14 13:05:42 cloud kernel: [24821.524359] RIP  [<ffffffffa2ec3ef4>] native_queued_spin_lock_slowpath+0x104/0x190\r\nApr 14 13:05:42 cloud kernel: [24821.524361]  RSP <ffffab6249fcfe98>\r\nApr 14 13:05:42 cloud kernel: [24821.524365] ---[ end trace 1391a191527a779c ]---\r\nApr 14 13:05:42 cloud kernel: [24821.637362] Fixing recursive fault but reboot is needed!\r\nApr 14 13:05:42 cloud kernel: [24821.637365] Fixing recursive fault but reboot is needed!\r\nApr 14 13:05:42 cloud kernel: [24821.637367] Fixing recursive fault but reboot is needed!\r\n```\r\nAlso available as a [gist](https://gist.github.com/vmp32k/4733c57a43b7969292ea5a2e625c857e).", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6019/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jn0": {"issues": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6018", "title": "lustre 2.9 and zfs 0.7-rc3", "body": "### System information\r\n<!--  add version after \"|\" character\u00a0-->\r\nType                                | Version/Name\r\n  ---                                  |     --- \r\nLinux       |  CentOS\r\nDistribution Version    |  7.3\r\nLinux Kernel                 |  3.10.0-514.el7_lustre\r\nArchitecture                 |  x86_64\r\nZFS Version                  |  0.7.0-rc3\r\nSPL Version                  |  0.7.0-rc3_8_g481762f\r\n<!-- \r\nCommands to find ZFS/SPL versions:\r\nmodinfo zfs | grep -iw version\r\nmodinfo spl | grep -iw version \r\n-->\r\n\r\n### Describe the problem you're observing\r\n\r\nmount.lustre fails for unresolved \"zrl_add\" symbol at osd_zfs insertion\r\n\r\n### Describe how to reproduce the problem\r\n\r\nbuild/install lustre from src rpm, build/install zfs/spl from github, try to mount a zfs volume\r\n\r\n### Include any warning/errors/backtraces from the system logs\r\n\r\nAt rpm install time:\r\n```\r\ndepmod: WARNING: /lib/modules/3.10.0-514.el7_lustre.x86_64/extra/lustre-osd-zfs/fs/osd_zfs.ko needs unknown symbol zrl_add\r\n```\r\n\r\nI'm trying to \"fix\" `zfs/module/zfs/zrlock.c` with\r\n\r\n```c\r\n#ifdef zrl_add\r\n#undef zrl_add\r\n#endif\r\n\r\nvoid zrl_add(zrlock_t *zrl, const char *zc)\r\n{\r\n        return zrl_add_impl(zrl, zc);\r\n}\r\nEXPORT_SYMBOL(zrl_add);\r\n```\r\nat the very end of the code (just before the last `#endif`)\r\n\r\nStill testing/experimenting...", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/6018/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "allanjude": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/6bc4a2376c694f813d2ee78af3d749aa2684f391", "message": "OpenZFS 8972 - zfs holds: In scripted mode, do not pad columns with spaces\n\nAuthored by: Allan Jude <allanjude@freebsd.org>\nApproved by: Dan McDonald <danmcd@joyent.com>\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\nReviewed by: George Melikov <mail@gmelikov.ru>\nPorted-by: Brian Behlendorf <behlendorf1@llnl.gov>\n\nOpenZFS-issue: https://www.illumos.org/issues/8972\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/3aace5c077\nCloses #7063"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "amotin": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/916384729e5d828e578c8f5c3003742360fb89ea", "message": "OpenZFS 8835 - Speculative prefetch in ZFS not working for misaligned reads\n\nIn case of misaligned I/O sequential requests are not detected as such\ndue to overlaps in logical block sequence:\n\n    dmu_zfetch(fffff80198dd0ae0, 27347, 9, 1)\n    dmu_zfetch(fffff80198dd0ae0, 27355, 9, 1)\n    dmu_zfetch(fffff80198dd0ae0, 27363, 9, 1)\n    dmu_zfetch(fffff80198dd0ae0, 27371, 9, 1)\n    dmu_zfetch(fffff80198dd0ae0, 27379, 9, 1)\n    dmu_zfetch(fffff80198dd0ae0, 27387, 9, 1)\n\nThis patch makes single block overlap to be counted as a stream hit,\nimproving performance up to several times.\n\nAuthored by: Alexander Motin <mav@FreeBSD.org>\nApproved by: Gordon Ross <gwr@nexenta.com>\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\nReviewed by: Allan Jude <allanjude@freebsd.org>\nReviewed by: Gvozden Neskovic <neskovic@gmail.com>\nReviewed by: George Melikov <mail@gmelikov.ru>\nPorted-by: Brian Behlendorf <behlendorf1@llnl.gov>\n\nOpenZFS-issue: https://www.illumos.org/issues/8835\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/aab6dd482a\nCloses #7062"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "loli10K": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/79c3270476b7140220c7946dd0a709a31bb9ed1b", "message": "Fix Debian packaging on ARMv7/ARM64\n\nWhen building packages on Debian-based systems specify the target\r\narchitecture used by 'alien' to convert .rpm packages into .deb: this\r\navoids detecting an incorrect value which results in the following\r\nerrors:\r\n\r\n<package>.aarch64.rpm is for architecture aarch64 ; the package cannot be built on this system\r\n<package>.armv7l.rpm is for architecture armel ; the package cannot be built on this system\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #7046 \r\nCloses #7058"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/390d679acdfa6a2498280a4dcd33b7600ace27ce", "message": "Fix 'zpool add' handling of nested interior VDEVs\n\nWhen replacing a faulted device which was previously handled by a spare\r\nmultiple levels of nested interior VDEVs will be present in the pool\r\nconfiguration; the following example illustrates one of the possible\r\nsituations:\r\n\r\n   NAME                          STATE     READ WRITE CKSUM\r\n   testpool                      DEGRADED     0     0     0\r\n     raidz1-0                    DEGRADED     0     0     0\r\n       spare-0                   DEGRADED     0     0     0\r\n         replacing-0             DEGRADED     0     0     0\r\n           /var/tmp/fault-dev    UNAVAIL      0     0     0  cannot open\r\n           /var/tmp/replace-dev  ONLINE       0     0     0\r\n         /var/tmp/spare-dev1     ONLINE       0     0     0\r\n       /var/tmp/safe-dev         ONLINE       0     0     0\r\n   spares\r\n     /var/tmp/spare-dev1         INUSE     currently in use\r\n\r\nThis is safe and allowed, but get_replication() needs to handle this\r\nsituation gracefully to let zpool add new devices to the pool.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6678 \r\nCloses #6996"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/c4ba46deade0a14d089228a56a5d0aa0ffd5fadd", "message": "Handle invalid options in arc_summary\n\nIf an invalid option is provided to arc_summary.py we handle any error\r\nthrown from the getopt Python module and print the usage help message.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6983"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/c30e34faa12e4b6e190edcccd4b2db185a286680", "message": "ZTS: Fix create-o_ashift test case\n\nThe function that fills the uberblock ring buffer on every device label\r\nhas been reworked to avoid occasional failures caused by a race\r\ncondition that prevents 'zpool sync' from writing some uberblock\r\nsequentially: this happens when the pool sync ioctl dispatch code calls\r\ntxg_wait_synced() while we're already waiting for a TXG to sync.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6924 \r\nCloses #6977"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/e2d936e0f893084384a0d29ddf2edfd9f68deaf2", "message": "Honor --with-mounthelperdir where applicable\n\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6962"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/ee410eefc2a9c0f0e77bd765894ee7767af647ea", "message": "Fix --with-systemd on Debian-based distributions (#6963)\n\nThese changes propagate the \"--with-systemd\" configure option to the\r\nRPM spec file, allowing Debian-based distributions to package\r\nsystemd-related files.\r\n\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6591 \r\nCloses #6963"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/4e9b156960562373e005798575a3fbc6d66e32ff", "message": "Various ZED fixes\n\n* Teach ZED to handle spares usingi the configured ashift: if the zpool\r\n   'ashift' property is set then ZED should use its value when kicking\r\n   in a hotspare; with this change 512e disks can be used as spares\r\n   for VDEVs that were created with ashift=9, even if ZFS natively\r\n   detects them as 4K block devices.\r\n\r\n * Introduce an additional auto_spare test case which verifies that in\r\n   the face of multiple device failures an appropiate number of spares\r\n   are kicked in.\r\n\r\n * Fix zed_stop() in \"libtest.shlib\" which did not correctly wait the\r\n   target pid.\r\n\r\n * Fix ZED crashing on startup caused by a race condition in libzfs\r\n   when used in multi-threaded context.\r\n\r\n * Convert ZED over to using the tpool library which is already present\r\n   in the Illumos FMA code.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #2562 \r\nCloses #6858"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/ed15d544812eb720d2fbec55d27e749d7a805bf6", "message": "Fix 'zfs get {user|group}objused@' functionality\n\nFix a regression accidentally introduced in 1b81ab4 that prevents\r\n'zfs get {user|group}objused@' from correctly reporting the requested\r\nvalue.\r\n\r\nUpdate \"userspace_003_pos.ksh\" and \"groupspace_003_pos.ksh\" to verify\r\nthis functionality.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6908"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/99834d1950f654d08585b61a1b0b4458d3d9e100", "message": "Fix truncate(2) mtime and ctime handling\n\nOn Linux, ftruncate(2) always changes the file timestamps, even if the\r\nfile size is not changed. However, in case of a successfull\r\ntruncate(2), the timestamps are updated only if the file size changes.\r\nThis translates to the VFS calling the ZFS Posix Layer \"setattr\"\r\nfunction (zpl_setattr) with ATTR_MTIME and ATTR_CTIME unconditionally\r\nset on the iattr mask only when doing a ftruncate(2), while the\r\ntruncate(2) is left to the filesystem implementation to be dealt with.\r\n\r\nThis behaviour is consistent with POSIX:2004/SUSv3 specifications\r\nwhere there's no explicit requirement for file size changes to update\r\nthe timestamps only for ftruncate(2):\r\n\r\nhttp://pubs.opengroup.org/onlinepubs/009695399/functions/truncate.html\r\nhttp://pubs.opengroup.org/onlinepubs/009695399/functions/ftruncate.html\r\n\r\nThis has been later updated in POSIX:2008/SUSv4 where, for both\r\ntruncate(2)/ftruncate(2), there's no mention of this size change\r\nrequirement:\r\n\r\nhttp://austingroupbugs.net/view.php?id=489\r\nhttp://pubs.opengroup.org/onlinepubs/9699919799/functions/truncate.html\r\nhttp://pubs.opengroup.org/onlinepubs/9699919799/functions/ftruncate.html\r\n\r\nUnfortunately the Linux VFS is still calling into the ZPL without\r\nATTR_MTIME/ATTR_CTIME set in the truncate(2) case: we fix this by\r\nexplicitly updating the timestamps when detecting the ATTR_SIZE bit,\r\nwhich is always set in do_truncate(), on the iattr mask.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6811 \r\nCloses #6819"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/011ef12c7a8d10da64d477d44a361de08e2d42e9", "message": "Fix undefined %{systemd_svcs} in RPM scriptlets\n\nThis allows RPM-based systems to properly control package installation\r\nand removal when using systemd.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6838 \r\nCloses #6841"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/271955da3eec30da42f0e67b63cd9ef5f003c55b", "message": "Fix zfs-tests.sh single test functionality\n\nWithout any tag specified into the runtime-generated runfile the\r\ntest-runner will not execute the test provided from the command line:\r\nfix this by adding tag information to the custom runfile.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6826"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/cb3b0419baeecd15cc6891cdccfe0d5d815edd36", "message": "contrib/initramfs: switch to automake\n\nUse automake to build initramfs scripts and hooks.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6761"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/ee45fbd89465f12b39e97173a088175d4b712b5f", "message": "ZFS send fails to dump objects larger than 128PiB\n\nWhen dumping objects larger than 128PiB it's possible for do_dump() to\r\nmiscalculate the FREE_RECORD offset due to an integer overflow\r\ncondition: this prevents the receiving end from correctly restoring\r\nthe dumped object.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Fabian Gr\u00fcnbichler <f.gruenbichler@proxmox.com>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6760"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/88f9c9396bcce596db56dc880260f95c49a51d67", "message": "Allow 'zpool events' filtering by pool name\n\nAdditionally add four new tests:\r\n\r\n * zpool_events_clear: verify 'zpool events -c' functionality\r\n * zpool_events_cliargs: verify command line options and arguments\r\n * zpool_events_follow: verify 'zpool events -f'\r\n * zpool_events_poolname: verify events filtering by pool name\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #3285 \r\nCloses #6762"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/aee1dd4d983c64db3c3155290d48f05243e85709", "message": "Fix intra-pool resumable 'zfs send -t <token>'\n\nBecause resuming from a token requires \"guid\" -> \"snapshot\" mapping\r\nwe have to walk the whole dataset hierarchy to find the right snapshot\r\nto send; when both source and destination exists, for an incremental\r\nresumable stream, libzfs gets confused and picks up the wrong snapshot\r\nto send from: this results in attempting to send\r\n\r\n   \"destination@snap1 -> source@snap2\"\r\n\r\ninstead of\r\n\r\n   \"source@snap1 -> source@snap2\"\r\n\r\nwhich fails with a \"Invalid cross-device link\" error (EXDEV).\r\n\r\nFix this by adjusting the logic behind dataset traversal in\r\nzfs_iter_children() to pick the right snapshot to send from.\r\n\r\nAdditionally update dry-run 'zfs send -t' to print its output to\r\nstderr: this is consistent with other dry-run commands.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6618\r\nCloses #6619\r\nCloses #6623"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/b59b22972db5913000ca157c24a254182df8d957", "message": "Add 'zfs diff' coverage to the ZFS Test Suite\n\nThis change adds four new tests to the ZTS:\r\n\r\n * zfs_diff_changes: verify type of changes diplayed (-, +, R and M)\r\n * zfs_diff_cliargs: verify command line options and arguments\r\n * zfs_diff_timestamp: verify 'zfs diff -t'\r\n * zfs_diff_types: verify type of objects (files, dirs, pipes...)\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: John Wren Kennedy <john.kennedy@delphix.com>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6686"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/3fd3e56cfd543d7d7a1bf502bfc0db6e24139668", "message": "Fix some ZFS Test Suite issues\n\n* Add 'zfs bookmark' coverage (zfs_bookmark_cliargs)\r\n\r\n * Add OpenZFS 8166 coverage (zpool_scrub_offline_device)\r\n\r\n * Fix \"busy\" zfs_mount_remount failures\r\n\r\n * Fix bootfs_003_pos, bootfs_004_neg, zdb_005_pos local cleanup\r\n\r\n * Update usage of $KEEP variable, add get_all_pools() function\r\n\r\n * Enable history_008_pos and rsend_019_pos (non-32bit builders)\r\n\r\n * Enable zfs_copies_005_neg, update local cleanup\r\n\r\n * Fix zfs_send_007_pos (large_dnode + OpenZFS 8199)\r\n\r\n * Fix rollback_003_pos (use dataset name, not mountpoint, to unmount)\r\n\r\n * Update default_raidz_setup() to work properly with more than 3 disks\r\n\r\n * Use $TEST_BASE_DIR instead of hardcoded (/var)/tmp for file VDEVs\r\n\r\n * Update usage of /dev/random to /dev/urandom\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nIssue #6086 \r\nCloses #5658 \r\nCloses #6143 \r\nCloses #6421 \r\nCloses #6627 \r\nCloses #6632"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/90cdf2833d163dd4db31aded1e6940cc9edb67f6", "message": "Add mdoc style checker\n\nAdd a new make 'mancheck' target which uses mandoc -Tlint to verify\r\nmanpage files: currently only zfs(8), zpool(8) zdb(8) and zgenhostid(8)\r\nare supported.\r\n\r\nAdditionally fix some outstanding manpage formatting issues.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6646"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/ded8f06a3cfee60b3a8ea5309e9c4d0e567ed3b5", "message": "Relax (ref)reservation constraints on ZVOLs\n\nThis change allow (ref)reservation to be set larger than the current\r\nZVOL size: this is safe as we normally set refreservation > volsize\r\nat ZVOL creation time when we account for metadata.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed by: Richard Elling <Richard.Elling@RichardElling.com>\r\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #2468 \r\nCloses #6610"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/835db58592d7d947e5818eb7281882e2a46073e0", "message": "Add -vnP support to 'zfs send' for bookmarks\n\nThis leverages the functionality introduced in cf7684b to expose\r\nverbose, dry-run and parsable 'zfs send' options for bookmarks.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #3666 \r\nCloses #6601"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/cf7684bc8d57ace26d086027e8059c725fd9ff92", "message": "Retire send space estimation via ZFS_IOC_SEND\n\nAdd a small wrapper around libzfs_core`lzc_send_space() to libzfs so\r\nthat every legacy ZFS_IOC_SEND consumer, along with their userland\r\ncounterpart estimate_ioctl(), can leverage ZFS_IOC_SEND_SPACE to\r\nrequest send space estimation.\r\n\r\nThe legacy functionality in zfs_ioc_send() is left untouched for\r\ncompatibility purposes.\r\n\r\nReviewed by: Thomas Caputi <tcaputi@datto.com>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6029"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/db4c1adaf82db7815baa675d7df3952f46b07ae9", "message": "Add support for DMU_OTN_* types in dbufstat.py\n\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6535"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/9000a9fac950d2e2c9578d760dd775ac1ceaa507", "message": "Disable mount(8) canonical paths in do_mount()\n\nBy default the mount(8) command, as invoked by 'zfs mount', will try\r\nto resolve any path parameter in its canonical form: this could lead\r\nto mount failures when the cwd contains a symlink having the same name\r\nof the dataset being mounted.\r\n\r\nFix this by explicitly disabling mount(8) path canonicalization.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #1791 \r\nCloses #6429 \r\nCloses #6437"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/f763c3d1df569a8d6b60bcb5e95cf07aa7a189e6", "message": "Fix range locking in ZIL commit codepath\n\nSince OpenZFS 7578 (1b7c1e5) if we have a ZVOL with logbias=throughput\r\nwe will force WR_INDIRECT itxs in zvol_log_write() setting itx->itx_lr\r\noffset and length to the offset and length of the BIO from\r\nzvol_write()->zvol_log_write(): these offset and length are later used\r\nto take a range lock in zillog->zl_get_data function: zvol_get_data().\r\n\r\nNow suppose we have a ZVOL with blocksize=8K and push 4K writes to\r\noffset 0: we will only be range-locking 0-4096. This means the\r\nASSERTion we make in dbuf_unoverride() is no longer valid because now\r\ndmu_sync() is called from zilog's get_data functions holding a partial\r\nlock on the dbuf.\r\n\r\nFix this by taking a range lock on the whole block in zvol_get_data().\r\n\r\nReviewed-by: Chunwei Chen <tuxoko@gmail.com>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6238 \r\nCloses #6315 \r\nCloses #6356 \r\nCloses #6477"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/08de8c16f5d322fb594742ea78958385d8ee5b50", "message": "Fix remounting snapshots read-write\n\nIt's not enough to preserve/restore MS_RDONLY on the superblock flags\r\nto avoid remounting a snapshot read-write: be explicit about our\r\nintentions to the VFS layer so the readonly bit is updated correctly\r\nin do_remount_sb().\r\n\r\nReviewed-by: Chunwei Chen <tuxoko@gmail.com>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6510 \r\nCloses #6515"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/b0bd8ffecd70b46e584e2e52ea74f2373b544217", "message": "Fix parsable 'zfs get' for compressratios\n\nThis is consistent with the change introduced in bc2d809 where\r\n'zpool get -p dedupratio' does not add a trailing \"x\" to the output.\r\n\r\nReviewed-by: Tony Hutter <hutter2@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6436 \r\nCloses #6449"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/c7a7601c08d3f7db42dfca46e0ad8aa287df43da", "message": "Fix volmode=none property behavior at import time\n\nAt import time spa_import() calls zvol_create_minors() directly: with\r\nthe current implementation we have no way to avoid device node\r\ncreation when volmode=none.\r\n\r\nFix this by enforcing volmode=none directly in zvol_alloc().\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6426"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/650258d7c786b8e62ca847a926f6f038cd3e5d94", "message": "zfs promote|rename .../%recv should be an error\n\nIf we are in the middle of an incremental 'zfs receive', the child\r\n.../%recv will exist. If we run 'zfs promote' .../%recv, it will \"work\",\r\nbut then zfs gets confused about the status of the new dataset.\r\nAttempting to do this promote should be an error.\r\n\r\nSimilarly renaming .../%recv datasets should not be allowed.\r\n\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #4843 \r\nCloses #6339"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/871e07321cc66fcea4dccc88fd2a754959ffa679", "message": "Fix buffer overflow in dsl_dataset_name()\n\nIf we're creating a pool with version >= SPA_VERSION_DSL_SCRUB (v11)\r\nwe need to account for additional space needed by the origin dataset\r\nwhich will also be snapshotted: \"poolname\"+\"/\"+\"$ORIGIN\"+\"@\"+\"$ORIGIN\".\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6374"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/cf8738d85374f51298a0872bcd58257bbb4fda6d", "message": "Add port of FreeBSD 'volmode' property\n\nThe volmode property may be set to control the visibility of ZVOL\r\nblock devices.\r\n\r\nThis allow switching ZVOL between three modes:\r\n   full - existing fully functional behaviour (default)\r\n   dev  - hide partitions on ZVOL block devices\r\n   none - not exposing volumes outside ZFS\r\n\r\nAdditionally the new zvol_volmode module parameter can be used to\r\ncontrol the default behaviour.\r\n\r\nThis functionality can be used, for instance, on \"backup\" pools to\r\navoid cluttering /dev with unneeded zd* devices.\r\n\r\nOriginal-patch-by: mav <mav@FreeBSD.org>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nPorted-by: loli10K <ezomori.nozomu@gmail.com>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\n\r\nFreeBSD-commit: https://github.com/freebsd/freebsd/commit/dd28e6bb\r\nCloses #1796 \r\nCloses #3438 \r\nCloses #6233"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/dda82a2eabefcf3e044c5d3bbe78954dfc0e4248", "message": "Fix chattr_001_pos\n\nCommands should be eval()ed if they involve a shell redirection,\r\notherwise we end up writing log_* functions messages to the output.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6300\r\nCloses #6323"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/92e43c17188d47f47b69318e4884096dec380e36", "message": "Fix 'zpool clear' on readonly pools\n\nIllumos 4080 inadvertently allows 'zpool clear' on readonly pools: fix\r\nthis by reintroducing a check (POOL_CHECK_READONLY) in zfs_ioc_clear\r\nregistration code.\r\n\r\nSigned-off-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: loli10K <ezomori.nozomu@gmail.com>\r\nCloses #6306"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kithrup": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/43cb30b3ce6ee3c3041276c93594ae61e7daaf86", "message": "OpenZFS 8959 - Add notifications when a scrub is paused or resumed\n\nAuthored by: Sean Eric Fagan <sef@ixsystems.com>\nReviewed by: Alek Pinchuk <pinchuk.alek@gmail.com>\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\nReviewed-by: George Melikov <mail@gmelikov.ru>\nReviewed-by: Tony Hutter <hutter2@llnl.gov>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nApproved by: Gordon Ross <gwr@nexenta.com>\nPorted-by: Giuseppe Di Natale <dinatale2@llnl.gov>\n\nPorting Notes:\n- Brought #defines in eventdefs.h in line with ZFS on Linux format.\n- Updated zfs-events.5 with the new events.\n\nOpenZFS-issue: https://www.illumos.org/issues/8959\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/c862b93eea\nCloses #7049"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yuripv": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/6df9f8ebd73c05da627144bcc3823e6fe980cd75", "message": "OpenZFS 8899 - zpool list property documentation doesn't match actual behaviour\n\nAuthored by: Yuri Pankov <yuri.pankov@nexenta.com>\nReviewed by: Alexander Pyhalov <alp@rsu.ru>\nReviewed-by: George Melikov <mail@gmelikov.ru>\nApproved by: Dan McDonald <danmcd@joyent.com>\nPorted-by: Brian Behlendorf <behlendorf1@llnl.gov>\n\nOpenZFS-issue: https://www.illumos.org/issues/8899\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/b0e142e57d\nCloses #7032"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/bcb1a8a25e4ee9a94478378710de53b45a9b1517", "message": "OpenZFS 8898 - creating fs with checksum=skein on the boot pools fails ungracefully\n\nAuthored by: Yuri Pankov <yuri.pankov@nexenta.com>\nReviewed by: Toomas Soome <tsoome@me.com>\nReviewed by: Andy Stormont <astormont@racktopsystems.com>\nReviewed-by: George Melikov <mail@gmelikov.ru>\nApproved by: Dan McDonald <danmcd@joyent.com>\nPorted-by: Brian Behlendorf <behlendorf1@llnl.gov>\n\nOpenZFS-issue: https://www.illumos.org/issues/8898\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/9fa2266d9a\nCloses #7031"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/8198c57b21d5e503f7e72221aa714aaabb2079cc", "message": "OpenZFS 8897 - zpool online -e fails assertion when run on non-leaf vdevs\n\nAuthored by: Yuri Pankov <yuri.pankov@nexenta.com>\nReviewed by: Toomas Soome <tsoome@me.com>\nReviewed by: Igor Kozhukhov <igor@dilos.org>\nReviewed-by: George Melikov <mail@gmelikov.ru>\nApproved by: Dan McDonald <danmcd@joyent.com>\nPorted-by: Brian Behlendorf <behlendorf1@llnl.gov>\n\nOpenZFS-issue: https://www.illumos.org/issues/8897\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/9a551dd645\nCloses #7030"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/e19572e4cc0b8df95ebf60053029e454592a92d4", "message": "OpenZFS 5428 - provide fts(), reallocarray(), and strtonum()\n\nAuthored by: Yuri Pankov <yuri.pankov@nexenta.com>\nReviewed by: Robert Mustacchi <rm@joyent.com>\nApproved by: Joshua M. Clulow <josh@sysmgr.org>\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\nReviewed-by: George Melikov <mail@gmelikov.ru>\nPorted-by: Brian Behlendorf <behlendorf1@llnl.gov>\n\nPorting Notes:\n* All hunks unrelated to ZFS were dropped.\n\nOpenZFS-issue: https://www.illumos.org/issues/5428\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/4585130\nCloses #6326"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "avg-I": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/6a2185660d000c99b14556c7eb1108c5609faf41", "message": "OpenZFS 8930 - zfs_zinactive: do not remove the node if the filesystem is readonly\n\nAuthored by: Andriy Gapon <avg@FreeBSD.org>\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\nReviewed-by: George Melikov <mail@gmelikov.ru>\nApproved by: Gordon Ross <gwr@nexenta.com>\nPorted-by: Brian Behlendorf <behlendorf1@llnl.gov>\n\nOpenZFS-issue: https://www.illumos.org/issues/8930\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/93c618e0f4\nCloses #7029"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/ecaebdbcf693d9f08844e04dc97f6859b7c37c80", "message": "OpenZFS 5778 - nvpair_type_is_array() does not recognize DATA_TYPE_INT8_ARRAY\n\nAuthored by: Andriy Gapon <avg@icyb.net.ua>\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\nReviewed by: Prakash Surya <prakash.surya@delphix.com>\nApproved by: Dan McDonald <danmcd@omniti.com>\nReviewed-by: Don Brady <dev.fs.zfs@gmail.com>\nReviewed-by: George Melikov <mail@gmelikov.ru>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nPorted-by: Giuseppe Di Natale <dinatale2@llnl.gov>\n\nOpenZFS-issue: https://www.illumos.org/issues/5778\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/bf4d553\nCloses #6580"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/f06f53fa3f566056fd3a03737032f1bd6bcf48bc", "message": "OpenZFS 7915 - checks in l2arc_evict could use some cleaning up\n\nAuthored by: Andriy Gapon <avg@FreeBSD.org>\nReviewed by: Dan Kimmel <dan.kimmel@delphix.com>\nReviewed by: Prakash Surya <prakash.surya@delphix.com>\nApproved by: Matthew Ahrens <mahrens@delphix.com>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nPorted-by: Giuseppe Di Natale <dinatale2@llnl.gov>\n\nOpenZFS-issue: https://www.illumos.org/issues/7915\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/836a00c\nCloses #6375"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/e98b6117252acb4931bbcc0ff6b164269273de4e", "message": "OpenZFS 8373 - TXG_WAIT in ZIL commit path\n\nAuthored by: Andriy Gapon <avg@FreeBSD.org>\nReviewed by: Prakash Surya <prakash.surya@delphix.com>\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\nApproved by: Dan McDonald <danmcd@joyent.com>\nReviewed-by: George Melikov <mail@gmelikov.ru>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nPorted-by: Giuseppe Di Natale <dinatale2@llnl.gov>\n\nOpenZFS-issue: https://www.illumos.org/issues/8373\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/7f04961\nCloses #6403"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28976438", "body": "I am not sure if we need to support `received` here at all, except maybe for legacy purposes.\nGiven that `lzc_receive` has an option to set received properties I think that `zc_set_props` should set just local properties.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28976438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "nwf": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/cba6fc61a2898395c47380a0c2303f19842a2ff0", "message": "Revert raidz_map and _col structure types\n\nAs part of the refactoring of ab9f4b0b824ab4cc64a4fa382c037f4154de12d6,\r\nseveral uint64_t-s and uint8_t-s were changed to other types.  This\r\ncaused ZoL github issue #6981, an overflow of a size_t on a 32-bit ARM\r\nmachine.  In absense of any strong motivation for the type changes, this\r\nsimply puts them back, modulo the changes accumulated for ABD.\r\n\r\nCompile-tested on amd64 and run-tested on armhf.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Gvozden Neskovic <neskovic@gmail.com>\r\nSigned-off-by: Nathaniel Wesley Filardo <nwf@cs.jhu.edu>\r\nCloses #6981 \r\nCloses #7023"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/8b20a9f996b90abe439ce14303fc440f26390e38", "message": "zhack: fix getopt return type\n\nThis fixes zhack's command processing on ARM.  On ARM char\r\nis unsigned, and so, in promotion to an int, it will never\r\ncompare equal to -1.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Nathaniel Wesley Filardo <nwf@cs.jhu.edu>\r\nCloses #7016"}], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6209", "title": "[RFC] new zscrub command for offline scrubs in userland", "body": "### Description\r\n\r\nThis PR adds a \"zhack scrub\" subcommand which, in user-land, finds and scrubs a pool.  This has proven useful for experimenting with the scan logic (especially the in-order-scrub patches) without having to reload the kernel module and seems like it may be useful to others.\r\n\r\nAt this point, it is not yet ready to merge -- there are no tests, no docs, &c... but I am curious for anyone's commentary and/or suggestions. :)\r\n\r\n### How Has This Been Tested?\r\n\r\nLimited testing against both files and actual block-device-backed pools.  Scrubs and resilvers appear to work just fine.\r\n\r\n### Types of changes\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [ ] I have updated the documentation accordingly.\r\n- [ ] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [ ] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gamanakis": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/be54a13c3e7db423ffdb3f7983d4dd1141cc94a0", "message": "Fix percentage styling in zfs-module-parameters.5\n\nReplace \"percent\" with \"%\", add bold to default values.\r\n\r\nReviewed-by: bunder2015 <omfgbunder@gmail.com>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: George Amanakis <gamanakis@gmail.com>\r\nCloses #7018"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lidongyang": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/823d48bfb182137c53b9432498f1f0564eaa8bfc", "message": "Call commit callbacks from the tail of the list\n\nOur zfs backed Lustre MDT had soft lockups while under heavy metadata\r\nworkloads while handling transaction callbacks from osd_zfs.\r\n\r\nThe problem is zfs is not taking advantage of the fast path in\r\nLustre's trans callback handling, where Lustre will skip the calls\r\nto ptlrpc_commit_replies() when it already saw a higher transaction\r\nnumber.\r\n\r\nThis patch corrects this, it also has a positive impact on metadata\r\nperformance on Lustre with osd_zfs, plus some cleanup in the headers.\r\n\r\nA similar issue for ext4/ldiskfs is described on:\r\nhttps://jira.hpdd.intel.com/browse/LU-6527\r\n\r\nReviewed-by: Olaf Faaland <faaland1@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Li Dongyang <dongyang.li@anu.edu.au>\r\nCloses #6986"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4692740", "body": "please have a look at lseek_execute(), should be EINVAL here.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4692740/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4692829", "body": "that's true, we can handle the SEEK_SET/CUR/END cases with generic_file_llseek and only take the mutex when dealing with SEEK_HOLE/DATA\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4692829/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4693009", "body": "offset is set by zfs_holey(), who takes the offset as uint64_t, so the check is necessary here.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4693009/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "tesujimath": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/993669a7bf17a26843630c547999be0b27483497", "message": "vdev_id: new slot type ses\n\nThis extends vdev_id to support a new slot type, ses, for SCSI Enclosure\r\nServices.  With slot type ses, the disk slot numbers are determined by\r\nusing the device slot number reported by sg_ses for the device with\r\nmatching SAS address, found by querying all available enclosures.\r\n\r\nThis is primarily of use on systems with a deficient driver omitting\r\nsupport for bay_identifier in /sys/devices.  In my testing, I found that\r\nthe existing slot types of port and id were not stable across disk\r\nreplacement, so an alternative was required.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Simon Guest <simon.guest@tesujimath.org>\r\nCloses #6956"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/269db7a4b3ef2bc14f3c2cf95f050479cbd69e72", "message": "vdev_id: extension for new scsi topology\n\nOn systems with SCSI rather than SAS disk topology, this change enables\r\nthe vdev_id script to match against the block device path, and therefore\r\ncreate a vdev alias in /dev/disk/by-vdev.\r\n\r\nReviewed-by: Tony Hutter <hutter2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Simon Guest <simon.guest@tesujimath.org>\r\nCloses #6592"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hadfl": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/2e7c1bb35a9858eff93feaf9132d3d89e756f6e0", "message": "OpenZFS 8794 - cstyle generates warnings with recent perl\n\nAuthored by: Dominik Hassler <hadfl@omniosce.org>\nReviewed by: Andy Fiddaman <andy@omniosce.org>\nReviewed by: Igor Kozhukhov <igor@dilos.org>\nReviewed by: Toomas Soome <tsoome@me.com>\nReviewed by: Brian Behlendorf <behlendorf1@llnl.gov>\nApproved by: Dan McDonald <danmcd@joyent.com>\nPorted-by: Giuseppe Di Natale <dinatale2@llnl.gov>\n\nOpenZFS-issue: https://www.illumos.org/issues/8794\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/578f67364c\nCloses #6973"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "daweiq": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/f6940bb9ea1ad6905101ba143859983189d32ac5", "message": "Enable QAT support in zfs-dkms RPM\n\nEnable QAT accelerated gzip compression in zfs-dkms RPM package when\nenvironment variant ICP_ROOT is set to QAT drive source code folder\nand QAT hardware presence.  Otherwise, use default gzip compression.\n\nReviewed-by: George Melikov <mail@gmelikov.ru>\nReviewed-by: David Qian <david.qian@intel.com>\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\nCloses #6932"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Lalufu": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/9920950ccbe032507663f5879721a80f14a83627", "message": "Add zfs-import.target services in spec file\n\nAdd missing zfs-import.target to list of systemd services in zfs\r\nRPM spec file.\r\n\r\nReviewed-by: Niklas Wagner <Skaro@Skaronator.com>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Ralf Ertzinger <ralf@skytale.net>\r\nIssue #6953 \r\nCloses #6955"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6222525", "body": "The basic idea was to replace the functionality offered by the SysV init script (and fix some low hanging issues in the process).\n\nI'm happy to look at booting from ZFS and the interleaving mount issue (the latter first, probably). If you file bugs for this please notify me.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6222525/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "tonyhutter": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/674b89342e43bd83659285d730b5b6a98f8b4b48", "message": "Fix segfault in zpool iostat when adding VDEVs\n\nFix a segfault when running 'zpool iostat -v 1' while adding\r\na VDEV.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Tony Hutter <hutter2@llnl.gov>\r\nCloses #6748 \r\nCloses #6872"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/6710381680b0f551c37627e3a5a4886ccf99983f", "message": "Only record zio->io_delay on reads and writes\n\nWhile investigating https://github.com/zfsonlinux/zfs/issues/6425 I\r\nnoticed that ioctl ZIOs were not setting zio->io_delay correctly.  They\r\nwould set the start time in zio_vdev_io_start(), but never set the end\r\ntime in zio_vdev_io_done(), since ioctls skip it and go straight to\r\nzio_done().  This was causing spurious \"delayed IO\" events to appear,\r\nwhich would eventually get rate-limited and displayed as\r\n\"Missed events\" messages in zed.\r\n\r\nTo get around the problem, this patch only sets zio->io_delay for read\r\nand write ZIOs, since that's all we care about anyway.\r\n\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Tony Hutter <hutter2@llnl.gov>\r\nCloses #6425 \r\nCloses #6440"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/c89a02a26ae8f314c621ffd5542a3502a196b7d0", "message": "Add new fsck return code to zvol_misc_002_pos\n\nzvol_misc_002_pos was failing on Fedora 26 because its newer version\r\nof fsck was returning a different code than previous versions.  The\r\nnew fsck error code is valid and is been added to the test in this\r\npatch.\r\n\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Olaf Faaland <faaland1@llnl.gov>\r\nSigned-off-by: Tony Hutter <hutter2@llnl.gov>\r\nCloses #6350"}], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/7051", "title": "zfs-0.7.6 patchset", "body": "<!--- Provide a general summary of your changes in the Title above -->\r\n\r\n<!---\r\nDocumentation on ZFS Buildbot options can be found at\r\nhttps://github.com/zfsonlinux/zfs/wiki/Buildbot-Options\r\n-->\r\n\r\n### Description\r\nTest 0.7.6 patchset in buildbot\r\n\r\n### Motivation and Context\r\n<!--- Why is this change required? What problem does it solve? -->\r\n<!--- If it fixes an open issue, please link to the issue here. -->\r\n\r\n### How Has This Been Tested?\r\n<!--- Please describe in detail how you tested your changes. -->\r\n<!--- Include details of your testing environment, and the tests you ran to -->\r\n<!--- see how your change affects other areas of the code, etc. -->\r\n<!--- If your change is a performance enhancement, please provide benchmarks here. -->\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [ ] My code follows the ZFS on Linux code style requirements.\r\n- [ ] I have updated the documentation accordingly.\r\n- [ ] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [ ] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "JKDingwall": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/9717fe052b6b7a43e497974817e392dffd8b939b", "message": "Add /usr/bin/env to COPY_EXEC_LIST initramfs hook\n\n5dc1ff29 changed the user space program to mount a zfs snapshot\r\nfrom /bin/sh to /usr/bin/env.  If the executable is not present\r\nin the initramfs then snapshots cannot be automounted.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Richard Laager <rlaager@wiktel.com>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: James Dingwall <james.dingwall@zynstra.com>\r\nCloses #5360 \r\nCloses #6913"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "markwright": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/56d8d8ace4724bde16145be25b75d8f551961813", "message": "Linux 4.14 compat: CONFIG_GCC_PLUGIN_RANDSTRUCT\n\nFix build errors with gcc 7.2.0 on Gentoo with kernel 4.14\r\nbuilt with CONFIG_GCC_PLUGIN_RANDSTRUCT=y such as:\r\n\r\nmodule/nvpair/nvpair.c:2810:2:error:\r\npositional initialization of field in ?struct? declared with\r\n'designated_init' attribute [-Werror=designated-init]\r\n  nvs_native_nvlist,\r\n  ^~~~~~~~~~~~~~~~~\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Mark Wright <gienah@gentoo.org>\r\nCloses #5390 \r\nCloses #6903"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rlaager": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/48ac22d8559313b2a33a80b9690aeea9acb9976c", "message": "initramfs: Honor canmount=off\n\nThe initramfs script was not honoring canmount=off.  With this change,\nit does.  If the administrator has asked that a filesystem not be\nmounted, that should be honored.\n\nAs an exception, the initramfs script ignores canmount=off on the\nrootfs.  The rootfs should not have canmount=off set either.  However,\nmounting it anyway seems harmless because it is being asked for\nexplicitly.  The point of this exception is to avoid the risk of\nbreaking existing systems, just in case someone has canmount=off set on\ntheir rootfs.\n\nThe initramfs still mounts filesystems with canmount=noauto.  This is\nnecessary because it is typical to set that on the rootfs so that it can\nbe cloned.  Without canmount=noauto, the clones' duplicate mountpoints\nwould conflict.\n\nThis is the remainder of the fix for:\nhttps://github.com/zfsonlinux/pkg-zfs/issues/221\n\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nReviewed-by: George Melikov <mail@gmelikov.ru>\nSigned-off-by: Richard Laager <rlaager@wiktel.com>\nCloses #6897"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/bd2958dea02f1126c4e907ab55c385ef24431f15", "message": "initramfs: Honor mountpoint=none/legacy\n\nFor filesystems that are children of the rootfs, when mountpoint=none or\nmountpoint=legacy, the initrafms script would assume a mountpoint based\non the dataset path.  Given that the rootfs should have mountpoint=/ and\nmountpoint inheritance is is the default behavior of ZFS, this behavior\nseems unnecessary.  In any event, it turns mountpoint=none into a no-op.\nThat removes this option from the administrator, and if someone uses it,\nit does not work as expected.  Worse yet, if the mountpoint directory\ndoes not exist (which is the typical case for mountpoint=none), the\nmounting and thus the boot process will fail.  For the case of\nmountpoint=legacy, the assumed mountpoint may not be the correct value\nset in /etc/fstab.\n\nThis change makes the initramfs script not mount the filesystem in\neither case.  For mountpoint=none, this means we are correctly honoring\nthe setting.  For mountpoint=legacy, there are two scenarios:  If\ncanmount=on, the filesystem will be mounted by the normal mechanisms\nlater in the boot process.  If canmount=noauto, the filesystem will not\nbe mounted at all, unless the administrator has done something special.\nIf they're not doing something special and they want it mounted by the\ninitramfs, they can simply not set mountpoint=legacy.\n\nThis is part of the fix for:\nhttps://github.com/zfsonlinux/pkg-zfs/issues/221\n\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nReviewed-by: George Melikov <mail@gmelikov.ru>\nSigned-off-by: Richard Laager <rlaager@wiktel.com>\nCloses #6897"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/4fc411f7a3ecee8a70fc8d6c687fae9a1cf20b31", "message": "initramfs: Set elevator=noop on the rpool's disks\n\nZFS already sets elevator=noop for wholedisk vdevs (for all pools), but\ntypical root-on-ZFS installations use partitions.  This sets\nelevator=noop on the disks in the root pool.\n\nUbuntu 16.04 and 16.10 had this.  It was lost in 17.04 due to Debian\nswitching to this upstream initramfs script.\n\nSigned-off-by: Richard Laager <rlaager@wiktel.com>"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/11b9dcfb2d30a8d05a424bb2a9a6d26ef39aabbb", "message": "initramfs: Fix a spelling error\n\nThis fixes a typo in a comment.\n\nSigned-off-by: Richard Laager <rlaager@wiktel.com>"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/4767c7a14e3b5f242746755d33d69edf81c6194f", "message": "initramfs: Fix inconsistent whitespace\n\nThis fixes one instance of inconsistent whitespace.\n\nSigned-off-by: Richard Laager <rlaager@wiktel.com>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/374212", "body": "What do you mean by this?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/374212/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/374494", "body": "I've found this approach works best: Start from a checkout of upstream trunk. Then `git branch TOPIC; git checkout TOPIC`. Make your changes and commit. Push that branch to github. Repeat as necessary for the other features, starting from a checkout of upstream trunk each time. Then, do a pull request for each branch.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/374494/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/374507", "body": "The easiest solution is probably to leave your master tracking upstream master (i.e. you should not commit anything to master). Use a separate branch to combine your topic branches.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/374507/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354255", "body": "domain should be 256 (255 + NUL). As a result, the other fields might need changing. I haven't looked closely.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354255/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354257", "body": "Is \"EPOH\" supposed to be \"epoch\"?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354257/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354259", "body": "This should be checked for NUL-termination correctness.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354259/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354260", "body": "This should be checked for NUL-termination correctness.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354260/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354264", "body": "This should probably const char *.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354264/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354267", "body": "By \"EOL\", you probably meant \"NUL\"?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354267/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354268", "body": "Like in the SMB patch, this usage of a function named file_is_executable() is really confusing.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354268/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354271", "body": "You're just blindly returning OK here. Should something be done?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354271/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354272", "body": "What should this function do? Maybe I or someone can help flesh it out.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354272/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354276", "body": "You shouldn't be calling strlen() in a loop like this. This should be rewritten more like this (untested):\n\n```\nfor (c = line ; *c ; c++) {\n    if (*c == '\\r' || *c == '\\n') {\n      c = '\\0';\n      break;\n    }\n}\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354276/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354281", "body": "The code inside this should be indented another level. (Is Github hiding that in the diff, maybe? I didn't check.)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354281/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354285", "body": "What's the purpose of this check? I don't understand why /dev/zvol is hardcoded.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354285/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354198", "body": "This line (and the one below) is unclear. How about `if (file_is_executable(NET_CMD_PATH) != SA_OK)` instead? Or, better, change the file_is_executable() function to return a boolean in the opposite sense (TRUE if executable, FALSE if not).\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354198/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354202", "body": "Why are you strdup()ing the sharename here? It looks like this is just a leak. There's no way libzfs_run_process() is freeing this, is there, as it'd crash trying to free() the const strings. Were you getting a compiler warning because sharename was const? argv should be const char \\* instead.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354202/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354204", "body": "Same leak as below.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354204/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354205", "body": "You need to ensure that name is NUL terminated after the strncpy() call:\nname [sizeof(name)-1] = '\\0';\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354205/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354206", "body": "argv should be const char *\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354206/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354208", "body": "argv should be const char *\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354208/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354210", "body": "What should this function do? Perhaps I or someone else can help flesh out the implementation?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354210/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354212", "body": "You shouldn't be calling strlen() in a loop like this. This should be rewritten more like this (untested):\n\n```\nfor (c = line ; *c ; c++) {\n    if (*c == '\\r' || *c == '\\n') {\n      c = '\\0';\n      break;\n    }\n}\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354212/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354215", "body": "Shouldn't this match the other error handling?\n\n```\nrc = SA_NO_MEMORY;\ngoto out;\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354215/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354218", "body": "I haven't looked closely, but these are almost surely more NUL-termination bugs.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354218/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354229", "body": "file_path should probably be sized PATH_MAX\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354229/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354234", "body": "Why are you hardcoding this, rather than using DT_REG? And really, you can't rely on d_type anyway (see the readdir man page). d_type can end up being DT_UNKNOWN for everything. So if you need to exclude non-regular files, you're going to have to stat() them and use S_ISREG().\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354234/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354275", "body": "I'd like to see the 255 in the name and comment fields (and the other similar ones below) be replaced with a #define like this:\n    /\\* The maximum SMB share name seems to be 254 characters, though good references are hard to find. */\n    #define SMB_NAME_MAX 255\n\nMicrosoft has recently released some new documentation that says 80 characters is the max, but I've seen some references to cifsmount on *nix being able to handle up to 255.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/354275/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/414018", "body": "If this can't be upstream'ed, is it really necessary, since you've got lintian.overrides?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/414018/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/623807", "body": "This command is never actually called.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/623807/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/623822", "body": "If this isn't necessary, it should be removed.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/623822/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/623829", "body": "This can never happen. At this point, pos = name (from the line above) and name is always unchanged from its static definition above. Did you mean to be checking sharename? Otherwise, this test can be removed.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/623829/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/623838", "body": "You're not passing NET_CMD_ARG_1 or _2, which I assume correspond to -S and 127.0.0.1 in your example. Either the code or the comment needs changing.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/623838/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/623841", "body": "The indentation looks wrong on this line...and several others; maybe github isn't handling mixed spaces and tabs well? I'm not a fan of them. Unless this is the Solaris or ZFS on Linux style, I'd suggest using just spaces or just tabs.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/623841/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/623875", "body": "I'm not sure we should expect users to understand \"ZoL\". Other parts of the man page say \"Linux\".  I don't think it's necessary to list possible scripting languages. The custom script should definitely not have a .sh extension. If the hook script is optional (which I think it should be), the man page should say that.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/623875/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637342", "body": "Why is tid_s 16 characters in size then?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637342/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637343", "body": "This bracing is inconsistent with the rest of the file.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637343/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637345", "body": "Also, it looks like it'd just be easier to do: if (!iscsi_available) exit(); and avoid a whole level of indentation. It seems that temp is always overwritten, which makes the malloc() above just a leak.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637345/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637346", "body": "Whatever this is needs to be figured out, or removed if it's unnecessary.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637346/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637350", "body": "Is that the convention for other ZFS code? If not, that style is, at least in my opinion, really a pain to work with.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637350/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637351", "body": "Is github also showing the indentation wrong here? It looks like there should be a whole pile of code indented another level, if you're adding a set of braces.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637351/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637353", "body": "There's another similar, possible, indentation issue here.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637353/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637400", "body": "Richard Yao's question reminded me that I wanted to ask about your handling of iSCSI naming in general. That lead me to look at this code...\n\nThis whole function's objective seems wrong. From what I understand, the date does not represent the current date, but the date when you first controlled the domain. While the former is okay as far as uniqueness is concerned, the latter is _at least_ the convention. That's why ZFS uses: \"iqn.1986-03.com.sun\".\n\nIs it possible to generate names using Sun's system (such that the behavior would match the same system running Solaris)?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637400/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/638904", "body": "The date is defined as the first month you controlled your domain name. (It SHOULD be the \"first full month\" to provide a better guarantee. Sun violated this SHOULD.) This is to ensure uniqueness, as domain names can change ownership. See http://en.wikipedia.org/wiki/ISCSI#Addressing as well as the RFCs that references.\n\nBut remember, the uniqueness of the date & domain is about establishing a unique naming _system_. So as long as we're using the same algorithm, it's fine to use the sun.com one from ZFS. From what I've seen online, Sun has two: iqn1986-03.com.sun:01:\\* and 1986-03.com.sun:02:*. The latter seems to be what they use for targets. I haven't yet figured out where they're getting the GUIDs that they use after the :02:.\n\nAlternatively, we could use some other, fixed, domain like \"iqn.2010-10.org.zfsonlinux:01:*\" and define our own algorithm. Datasets seem to have a unique ID (as seen in zdb). So if we combine the pool GUID and the dataset ID, we'd have a unique identifier. For example: iqn.2001-04.org.zfsonlinux:01:13104831232857787788:43214\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/638904/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/638933", "body": "Can you make this (and the one in line 48) tank/test instead of share/test1. Or, change the commands above. Either way, the examples should match. :)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/638933/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/639365", "body": "To muddy the waters further... patdk-wk in #zfs, who seems knowledgeable about iSCSI, said that your normally have one target per NIC, at most. Then you have multiple LUNs per target. If I'm reading this patch correctly, it says it's creating one target per zvol (which is why I was suggesting the naming scheme I did). Is that the case?\n\nUpdate: Is that because ietd only does permission checks on a per-target basis? For example, if I had 10 VMs with 3 disks each, I can create 30 targets with 1 LUN each, 10 targets with 3 LUNs each, or 1 target with 30 LUNs each. If ietd can only do permissions per target, then 10/3 is actually the best, but there's probably no easy way to make `zfs set shareiscsi` do that, so we have to do 30/1. If ietd has ACLs/views that let us do permissions per-LUN, then he's saying 1 target with 30 LUNs is the right answer.\n\nAlso, if you're using ietadm to create the target(s)/LUN(s), from what I read, they aren't persistent. In other words, if I restart ietd, do I lose all of my zvol target(s)/LUN(s), since they're not listed in /etc/ietd.conf?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/639365/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/640242", "body": "It seems we sent comments around the same time. We definitely need to determine whether the one-target-per-zvol is the best approach first. But, assuming it is...\nA) I'll agree with your logic on the date. You're violating a SHOULD, but not a MUST, so you're correct that it is allowed. I'll also agree that it doesn't create any uniqueness problems.\nB) I was assuming you'd set an alias to provide the readable name.\nC) I was thinking we needed more uniqueness than is necessary (or desirable). For example, if you `zfs send` a zvol to a backup server, I was thinking we'd want different IQNs, which your scheme does not provide. patdk said there's no need for them to be different in that case. And clearly, if they're the same, it makes failover easier.\n\nSo the question of target-or-lun per zvol remains, as does the issue of ensuring date stability.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/640242/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/645374", "body": "s/ietmadm/ietadm/\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/645374/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/645377", "body": "test1 or test? Likewise for two lines below.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/645377/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/645400", "body": "Why are you chopping off the last character of buffer here? (It might be legitimate; I just don't understand.)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/645400/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/645418", "body": "Is it really 8M on your system? It's 8 here.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/645418/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/645435", "body": "What are these changes for? Can they be part of a separate pull request?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/645435/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/645439", "body": "What is this change for? Can it be part of a separate pull request?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/645439/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/645443", "body": "What are these changes for? Can they be part of a separate pull request?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/645443/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/645445", "body": "What is this change for? Can it be part of a separate pull request?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/645445/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/645467", "body": "I'm hoping to test with Solaris today. But from what I can see online, it looks like Solaris does one target per zvol as well. I'm thinking maybe the best answer is to push forward with one target per zvol for now and worry about getting the rest of the patch in shape.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/645467/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "benrubson": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/7c351e31d5961a65ebf8ba2110f941391834df55", "message": "OpenZFS 7531 - Assign correct flags to prefetched buffers\n\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\nReviewed by: Prakash Surya <prakash.surya@delphix.com>\nAuthored by: abraunegg <alex.braunegg@gmail.com>\nApproved by: Dan McDonald <danmcd@joyent.com>\nPorted-by: Brian Behlendorf <behlendorf1@llnl.gov>\n\nOpenZFS-issue: https://www.illumos.org/issues/7531\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/468008cb"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dweeezil": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/71a24c3c52f6223f54e6351b57e089791aa3b2f1", "message": "Handle compressed buffers in __dbuf_hold_impl()\n\nIn __dbuf_hold_impl(), if a buffer is currently syncing and is still\r\nreferenced from db_data, a copy is made in case it is dirtied again in\r\nthe txg.  Previously, the buffer for the copy was simply allocated with\r\narc_alloc_buf() which doesn't handle compressed or encrypted buffers\r\n(which are a special case of a compressed buffer).  The result was\r\ntypically an invalid memory access because the newly-allocated buffer\r\nwas of the uncompressed size.\r\n\r\nThis commit fixes the problem by handling the 2 compressed cases,\r\nencrypted and unencrypted, respectively, with arc_alloc_raw_buf() and\r\narc_alloc_compressed_buf().\r\n\r\nAlthough using the proper allocation functions fixes the invalid memory\r\naccess by allocating a buffer of the compressed size, another unrelated\r\nissue made it impossible to properly detect compressed buffers in the\r\nfirst place.  The header's compression flag was set to ZIO_COMPRESS_OFF\r\nin arc_write() when it was possible that an attached buffer was actually\r\ncompressed.  This commit adds logic to only set ZIO_COMPRESS_OFF in\r\nthe non-ZIO_RAW case which wil handle both cases of compressed buffers\r\n(encrypted or unencrypted).\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Tim Chase <tim@chase2k.com>\r\nCloses #5742 \r\nCloses #6797"}], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6900", "title": "OpenZFS 7614 - zfs device evacuation/removal", "body": "### Description\r\n<!--- Describe your changes in detail -->\r\nThis project allows top-level vdevs to be removed from the storage pool\r\nwith \"zpool remove\", reducing the total amount of storage in the pool.\r\nThis operation copies all allocated regions of the device to be removed\r\nonto other devices, recording the mapping from old to new location.\r\nAfter the removal is complete, read and free operations to the removed\r\n(now \"indirect\") vdev must be remapped and performed at the new location\r\non disk.  The indirect mapping table is kept in memory whenever the pool\r\nis loaded, so there is minimal performance overhead when doing\r\noperations on the indirect vdev.\r\n\r\nThe size of the in-memory mapping table will be reduced when its entries\r\nbecome \"obsolete\" because they are no longer used by any block pointers\r\nin the pool.  An entry becomes obsolete when all the blocks that use it\r\nare freed.  An entry can also become obsolete when all the snapshots\r\nthat reference it are deleted, and the block pointers that reference it\r\nhave been \"remapped\" in all filesystems/zvols (and clones).  Whenever an\r\nindirect block is written, all the block pointers in it will be\r\n\"remapped\" to their new (concrete) locations if possible.  This process\r\ncan be accelerated by using the \"zfs remap\" command to proactively\r\nrewrite all indirect blocks that reference indirect (removed) vdevs.\r\n\r\nNote that when a device is removed, we do not verify the checksum of the\r\ndata that is copied.  This makes the process much faster, but if it were\r\nused on redundant vdevs (i.e. mirror or raidz vdevs), it would be\r\npossible to copy the wrong data, when we have the correct data on e.g.\r\nthe other side of the mirror.  Therefore, mirror and raidz devices can\r\nnot be removed.\r\n\r\n### Motivation and Context\r\nSee above.\r\n<!--- Why is this change required? What problem does it solve? -->\r\n<!--- If it fixes an open issue, please link to the issue here. -->\r\n\r\n### How Has This Been Tested?\r\nAdditions to the test suite in functional/removal.\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [x] I have updated the documentation accordingly.\r\n- [x] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [ ] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/5925", "title": "OpenZFS - 6363 Add UNMAP/TRIM functionality", "body": "<!--- Provide a general summary of your changes in the Title above -->\r\n\r\n### Description\r\nAdd TRIM support.  Replacement for #3656.\r\n\r\n### Motivation and Context\r\n<!--- Why is this change required? What problem does it solve? -->\r\n<!--- If it fixes an open issue, please link to the issue here. -->\r\n\r\n### How Has This Been Tested?\r\n<!--- Please describe in detail how you tested your changes. -->\r\n<!--- Include details of your testing environment, and the tests you ran to -->\r\n<!--- see how your change affects other areas of the code, etc. -->\r\n<!--- If your change is a performance enhancement, please provide benchmarks here. -->\r\nVarious stress testing with an assortment of vdev types.\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [x] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [ ] My code follows the ZFS on Linux code style requirements.\r\n- [ ] I have updated the documentation accordingly.\r\n- [ ] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n\r\nThis PR should integrate all the recent changes in the upstream patch set.  The stack also includes the separate fixes which were in #3656.  It seems stable so far during some fairly abusive testing on SSDs with various types of vdevs.  It does _not_ include the \"partial\" trim support of the previous PR.", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/5338921", "body": "Yes, indeed.  Nice catch.  I'll post a pull request for this shortly.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/5338921/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6064961", "body": "@pyavdr You need zfsonlinux/spl@545e9ac which adds them.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6064961/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/7102515", "body": "This is strictly a FYI and not a comment on the (now landed) commit, with which I've got no problem at all:\n\nUnnamed structs & unions are actually a C++ feature which wasn't introduced into C until c11, however, they've been so extensively used over the years (due, among other things to their convenience in applications such as this patch) that most compilers accept them.  With gcc, you need to use \"-Wpedantic\" to even get a (misleading in gcc) warning:\n\n```\n% gcc -std=c89 -c -Wpedantic test.c\nz.c:5:4: warning: ISO C90 doesn\u2019t support unnamed structs/unions [-Wpedantic]\n   };\n% gcc -std=c11 -c -Wpedantic test.c\n% clang -std=c89 -c -Wpedantic test.c\nz.c:3:3: warning: anonymous structs are a C11 extension [-Wc11-extensions]\n                struct {\n                ^\n% clang -std=c11 -c test.c\n% \n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/7102515/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/9095376", "body": "Why don't we set `.fs_flags = FS_REQUIRES_DEV` here?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/9095376/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/9236094", "body": "Since `zfs_add_option()` always returns zero, there's no need for \"error\" nor for its associated handling in this function; it can simply return zero as well.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/9236094/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4526399", "body": "The parameter change to zfs_destroy_snaps_nvl was accidentally picked up from illumos 3b2aab1.  There should be no problem with it in this commit, but it doesn't totally track the history of changes to Illumos.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4526399/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "aerusso": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/80b485246ab1acf5e2c5373c2739851aeb693fb4", "message": "Cleanup systemd dependencies\n\nSome redundancy is present in the systemd dependencies, as\r\nnoticed in PR#6764. Existing setups might rely on these quirks,\r\nso these cleanups have been moved to the development branch.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Antonio Russo <antonio.e.russo@gmail.com>\r\nCloses #6822"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/5c2552c56456191e8038efe25ed66270c3f1501a", "message": "systemd zfs-import.target and documentation\n\nzfs-import-{cache,scan}.service must complete before any mounting of\r\nfilesystems can occur. To simplify this dependency, create a target\r\nthat is reached After (in the systemd sense) the pool is imported.\r\n\r\nAdditionally, recommend that legacy zfs mounts use the option\r\n\r\nx-systemd.requires=zfs-import.target\r\n\r\nto codify this requirement.\r\n\r\nReviewed-by: Fabian Gr\u00fcnbichler <f.gruenbichler@proxmox.com>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Antonio Russo <antonio.e.russo@gmail.com>\r\nCloses #6764"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/085b501fb81654171bcc2ed76bff64eea408ca20", "message": "Explicitly depend on icp module in initramfs hook\n\nAutomatic dependency resolution is unreliable on many systems.\r\nFollow suit with existing code, and explicitly include icp\r\nin module dependencies.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Antonio Russo <antonio.e.russo@gmail.com>\r\nCloses #6751"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/c34efbebd577644674a7cf543b3e0540c9f0fa9f", "message": "Prevent dependencies on Debianized packages\n\nCall dpkg-shlibdeps with arguments excluding the Debianized packages\r\nlib{uutil1,nvpair1,zfs2,zpool2}linux from the auto-generated\r\ndependencies of generated .debs. A shim dh_shlibdeps that calls the\r\nreal dh_shlibdeps with corresponding arguments is installed into a\r\ntemporary directory, which is in turn pre-pended to the PATH for the\r\nalien call, working around alien's inability to directly alter the\r\ndependencies of its output debs. Resolves #6106.\r\n\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Antonio Russo <antonio.e.russo@gmail.com>\r\nCloses #6309 \r\nCloses #6106"}], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6974", "title": "fstab integration", "body": "Generate a tracked, updateable section in /etc/fstab for zfs filesystems\r\n\r\n### Description\r\nA contrib script fstab-generator is implemented that creates a trackable section in /etc/fstab (or another user-specified file) with fstab syntax reflecting the zfs mount and canmount parameters.\r\n\r\n### Motivation and Context\r\nWhile #4943 implements a per-pool granular import, the user will still \"need to add an entry like this in fstab:\r\n\r\n```rpool/home /home zfs rw,defaults,x-systemd.requires=zpool@rpool.service```\r\n\r\nThis script performs precisely that mechanical task, allowing for filesystem dependencies to be correctly identified, and mounted in time to guarantee their availability. A monolithic import of all zfs filesystems is not required to have system files on native zfs mountpoints. \r\n\r\nMoreover, by including this information in /etc/fstab, tools can fail appropriately if essential mountpoints are unavailable. This helps address the common annoyance where zfs fails to mount an important system directory, files then get placed on the zfs mountpoint, and then zfs will fail to mount on the subsequent boot (because overlay=off) even though the underlying problem was corrected. \r\n\r\n#### Why not a systemd-generator?\r\nBesides the obvious lack of integration for users without systemd, other tools may rely on /etc/fstab to determine what filesystems are present on a system. This approach immediately achieves integration with those tools--e.g., for analogous dependency tracking for other init systems that may develop in the future. Additionally, systemd generators may change syntax in the future, but they will have to remain compatible with /etc/fstab.\r\n\r\n### How Has This Been Tested?\r\nI'm running with the output of this script on a machine that has several `/var/` directories, and `/tmp` with purely zfs mountpoints.\r\n\r\n### RFC\r\nThis is a work in progress.\r\n1. Should this be converted to fstab-generator.in, and use `%sbindir%`, etc?\r\n2. Should this name be changed? Should this be installed elsewhere?\r\n3. How could/should this be integrated with the rest of the tools?\r\n4. Is there some reason `mount -ozfsutil` is ill-advised for zfs filesystems?\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [x] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [x] I have updated the documentation accordingly.\r\n- [x] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [x] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6964", "title": "Use zfs-import.target in contrib/dracut", "body": "### Description\r\nThe new zfs-import.target should be used in place of the zfs-import-*.service units in `contrib/dracut`.\r\n\r\n### Motivation and Context\r\nPR #6764 added `zfs-import.target` to simplify dependency on pool importing. #6822 did some cleanup. The recent #6955 (re: #6953) added RPM support for enabling this units. That bug report has prompted me to grep the code base for zfs-import. The last remaining code section to be updated is under `control/dracut/90zfs`.\r\n\r\nThis PR is  a **work in progress**. I don't think dracut users are exposed to any bug presently, because `sysroot.mount` is still ordered `After=zfs-import-*.service`\r\n\r\nTwo files are affected:\r\n1. `zfs-generator.sh.in` is straightforwardly modified to order `sysroot.mount` `After=zfs-import.target` (instead of each `zfs-import-*.service`). \r\n2. `module-setup.sh.in` is also modified. **I need input, because I don't know how precisely dracut works.** `zfs-import.target` (and each `zfs-import-*.service`) is `dracut_install`-ed (and *unconditionally* `mark_hostonly`-ed). Do we need to build a `zfs-import.target.wants` directory with `zfs-import-*.service` links? Or will that be inherited from the host system?\r\n\r\n### How Has This Been Tested?\r\nThis has NOT been tested. This is a place to centralize discussion about these changes.\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Performance enhancement (non-breaking change which improves efficiency)\r\n- [x] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [ ] I have updated the documentation accordingly.\r\n- [x] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [x] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wli5": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/a3df7fa79d7db857f67a38ced033764bd684ca71", "message": "Bug fix in qat_compress.c when compressed size is < 4KB\n\nWhen the 128KB block is compressed to less than 4KB, the pointer\r\nto the Footer is not in the end of the compressed buffer, that's\r\nbecause the Header offset was added twice for this case. So there\r\nis a gap between the Footer and the compressed buffer.\r\n1. Always compute the Footer pointer address from the start of the\r\nlast page.\r\n2. Remove the un-used workaroud code which has been verified fixed\r\nwith the latest driver and this fix.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Weigang Li <weigang.li@intel.com>\r\nCloses #6827"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/1cfdb0e6e401087778712fa893777b064a8afb6b", "message": "Support integration with new QAT products\n\nSupport integration with new QAT products: Intel(R) C62x Chipset,\r\nor Atom(R) C3000 Processor Product Family SoC:\r\n1. Detect new file name in auto-conf.\r\n2. Change MAX_INSTANCES to 48.\r\n3. Change \"num_inst\" to U16 to clean a build warning.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Weigang Li <weigang.li@intel.com>\r\nCloses #6767"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gg7": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/2df9ad1c075cfca3cf5acb64950a91e2284f6338", "message": "Fix column alignment with long zpool names\n\n`zpool status` normally aligns NAME/STATE/etc columns:\r\n\r\n    NAME                       STATE     READ WRITE CKSUM\r\n    dummy                      ONLINE       0     0     0\r\n      mirror-0                 ONLINE       0     0     0\r\n        /tmp/dummy-long-1.bin  ONLINE       0     0     0\r\n        /tmp/dummy-long-2.bin  ONLINE       0     0     0\r\n      mirror-1                 ONLINE       0     0     0\r\n        /tmp/dummy-long-3.bin  ONLINE       0     0     0\r\n        /tmp/dummy-long-4.bin  ONLINE       0     0     0\r\n\r\nHowever, if the zpool name is longer than the zvol names, alignment\r\nissues arise:\r\n\r\n    NAME                  STATE     READ WRITE CKSUM\r\n    dummy-very-very-long-zpool-name  ONLINE       0     0     0\r\n      mirror-0            ONLINE       0     0     0\r\n        /tmp/dummy-1.bin  ONLINE       0     0     0\r\n        /tmp/dummy-2.bin  ONLINE       0     0     0\r\n      mirror-1            ONLINE       0     0     0\r\n        /tmp/dummy-3.bin  ONLINE       0     0     0\r\n        /tmp/dummy-4.bin  ONLINE       0     0     0\r\n\r\n`zpool iostat` and `zpool import` are also affected:\r\n\r\n                  capacity     operations     bandwidth\r\n    pool        alloc   free   read  write   read  write\r\n    ----------  -----  -----  -----  -----  -----  -----\r\n    dummy        104K  1.97G      0      0    152  9.84K\r\n    dummy-very-very-long-zpool-name   152K  1.97G      0      1    144  13.1K\r\n    ----------  -----  -----  -----  -----  -----  -----\r\n\r\n    dummy-very-very-long-zpool-name  ONLINE\r\n      mirror-0            ONLINE\r\n        /tmp/dummy-1.bin  ONLINE\r\n        /tmp/dummy-2.bin  ONLINE\r\n      mirror-1            ONLINE\r\n        /tmp/dummy-3.bin  ONLINE\r\n        /tmp/dummy-4.bin  ONLINE\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Tony Hutter <hutter2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: George Gaydarov <git@gg7.io>\r\nCloses #6786"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Fabian-Gruenbichler": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/3ad59c015dce45965fa309a0364a46c6f8bdda9f", "message": "arcstat: flush stdout / outfile after each line\n\nOtherwise, if arcstat gets interrupted before the desired number of\r\niterations is reached, the output file will be empty (both if set via\r\n'-o' or via shell redirection).\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Fabian Gr\u00fcnbichler <f.gruenbichler@proxmox.com>\r\nCloses #6775"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/829e95c4dc74d7d6d31d01af9c39e03752499b15", "message": "Skip FREEOBJECTS for objects which can't exist\n\nWhen sending an incremental stream based on a snapshot, the receiving\nside must have the same base snapshot.  Thus we do not need to send\nFREEOBJECTS records for any objects past the maximum one which exists\nlocally.\n\nThis allows us to send incremental streams (again) to older ZFS\nimplementations (e.g. ZoL < 0.7) which actually try to free all objects\nin a FREEOBJECTS record, instead of bailing out early.\n\nReviewed by: Paul Dagnelie <pcd@delphix.com>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nSigned-off-by: Fabian Gr\u00fcnbichler <f.gruenbichler@proxmox.com>\nCloses #5699\nCloses #6507\nCloses #6616"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/48fbb9ddbf2281911560dfbc2821aa8b74127315", "message": "Free objects when receiving full stream as clone\n\nAll objects after the last written or freed object are not supposed to\nexist after receiving the stream.  Free them accordingly, as if a\nfreeobjects record for them had been included in the stream.\n\nReviewed by: Paul Dagnelie <pcd@delphix.com>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nSigned-off-by: Fabian Gr\u00fcnbichler <f.gruenbichler@proxmox.com>\nCloses #5699\nCloses #6507\nCloses #6616"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/c8811dec7044a126650c7e2d9f3404680ae115b5", "message": "Add man page reference to systemd units\n\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Fabian Gr\u00fcnbichler <f.gruenbichler@proxmox.com>\r\nCloses #6599"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/42a76fc8d757ab82fc1ce8e5e1f2079e07a5b9d4", "message": "dracut: make module-setup.sh shebang explicit\n\nwhile these are source by dracut (which is a bash script)\r\nthe practical difference is small, but it is more correct:\r\n\r\n/bin/sh is not bash on all systems (e.g. Debian and its\r\nderivatives use /bin/dash as /bin/sh by default).\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Fabian Gr\u00fcnbichler <f.gruenbichler@proxmox.com>\r\nCloses #6491"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/b58237e769a4fa57ab8846b61ce59e5c73035b26", "message": "Man page fixes\n\n* ztest.1 man page: fix typo\r\n* zfs-module-parameters.5 man page: fix grammar\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Fabian Gr\u00fcnbichler <f.gruenbichler@proxmox.com>\r\nCloses #6492"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dpquigl": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d9daa7abcf04f75ba013ec954c4f2d4854ba1cbc", "message": "ZTS: Add auto-spare tests\n\nThe ZED is expected to automatically kick in a hot spare device\r\nwhen there's one available in the pool and a sufficient number of\r\nread errors have been encountered.  Use zinject to simulate the\r\nfailure condition and verify the hot spare is used.\r\n\r\nauto_spare_001_pos.ksh: read IO errors, the vdev is FAULTED\r\nauto_spare_002_pos.ksh: read CHECKSUM errors, the vdev is DEGRADE\r\n\r\nReviewed by: Richard Elling <Richard.Elling@RichardElling.com>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: David Quigley <david.quigley@intel.com>\r\nCloses #6280"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/a9a2bf71520ffb5668c9204db5c7df0445dc912a", "message": "Remove FRU and LIBTOPO Support\n\nFRU and LIBTOPO support are illumos only features that will not be ported to\r\nLinux and make the code more complicated than necessary. This commit\r\nmakes way for further cleanups of the zed/FMA code.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: David Quigley <david.quigley@intel.com>\r\nCloses #6641"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/1f4e2c88fd6ca44ec50f178ca8baa18a9c764a98", "message": "ZTEST: Always enable asserts\n\nThe build for ztest always enabled debug information but does not enable\nasserts unless --enable-debug is used. This will always enable asserts\nin the ztest code.\n\nReviewed-by: George Melikov <mail@gmelikov.ru>\nReviewed-by Brian Behlendorf <behlendorf1@llnl.gov>\nSigned-off-by: David Quigley <david.quigley@intel.com>\nCloses #6640"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/b1490dd43e3c98649c7d23928d908f5bb019411b", "message": "Fix bug in distclean which removes needed files\n\nRunning distclean removes the following files because of an error\r\nin Makefile.am\r\n\r\ndeleted:    tests/zfs-tests/include/commands.cfg\r\ndeleted:    tests/zfs-tests/include/libtest.shlib\r\ndeleted:    tests/zfs-tests/include/math.shlib\r\ndeleted:    tests/zfs-tests/include/properties.shlib\r\ndeleted:    tests/zfs-tests/include/zpool_script.shlib\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: David Quigley <david.quigley@intel.com>\r\nCloses #6636"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "adisbladis": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/f8cd871a01a5e38e35cb4d44768dce09265265f4", "message": "Use ashift=12 by default on SSDSC2BW48 disks\n\nCurrently the 480GB models of this disk do not use ashift=12 by\r\ndefault.  SSDSC2BW48 is also optimized for 4k blocks.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: adisbladis <adis@blad.is>\r\nCloses #6774"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "johnramsden": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/6044cf59cdf80d4ba94a7b6c6736cace8a7de9db", "message": "Add convenience 'zfs_get' functions\n\nAdd get functions to match existing ones.\r\n\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: John Ramsden <johnramsden@riseup.net>\r\nCloses #6308"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tcharding": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/c721ba435fc7d2fb61a6e586c5cc11c27f67b480", "message": "Fix coverity defects: CID 161388\n\nCID 161388: Resource Leak (REASOURCE_LEAK)\r\n\r\nJump to errout so that file descriptor gets closed before returning\r\nfrom function.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Tobin C. Harding <me@tobin.cc>\r\nCloses #6755"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/ced28193b06b3d93f404a5d67713c124731a2a0d", "message": "Fix coverity defects: 147480, 147584\n\nCID 147480: Logically dead code (DEADCODE)\r\n\r\nRemove non-null check and subsequent function call. Add ASSERT to future\r\nproof the code.\r\n\r\nusage label is only jumped to before `zhp` is initialized.\r\n\r\nCID 147584: Out-of-bounds access (OVERRUN)\r\n\r\nSubtract length of current string from buffer length for `size` argument\r\nto `snprintf`.\r\n\r\nStarting address for the write is the start of the buffer + the current\r\nstring length. We need to subtract this string length else risk a buffer\r\noverflow.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Tobin C. Harding <me@tobin.cc>\r\nCloses #6745"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/c616dcf8bc3b2cb2dddc9b8f8bc64f307c9fb938", "message": "Fix function documentation to correctly mirror code\n\nCurrently the function documentation states that two strings are \r\nallocated, this is outdated. Only one char ** parameter is passed \r\ninto the function now, clearly only a pointer to a single string \r\nis returned and needs to be free'd.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Tobin C. Harding <me@tobin.cc>\r\nCloses #6754"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/523d5ce0f48a1b7127345d830e5a34079cace322", "message": "Fix coverity defects: CID 147474\n\nCID 147474: Logically dead code (DEADCODE)\r\n\r\nRemove ternary operator and return `error` directly.\r\n\r\nCurrently return value is derived from a ternary operator. The\r\nconditional is always true. The ternary operator is therefore\r\nredundant i.e dead code.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Tobin C. Harding <me@tobin.cc>\r\nCloses #6723"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/a0430cc5a913406cf471bcafac31c5118d89843f", "message": "Use bitwise '&' instead of logical '&&'\n\nMake two instances of the same change. Change bitwise AND (&) to logical\r\nAND (&&).\r\n\r\nCurrently the code uses a bitwise AND between two boolean values.\r\n\r\nIn the first instance;\r\n\r\nThe first operand is a flag that has been bitwise combined with a bit\r\nmask to get a boolean value as to whether a file has group write\r\npermissions set.\r\n\r\nThe second operand used is a struct member that is intended as a\r\nboolean flag not a bit mask.\r\n\r\nIn the second instance the argument is the same except with world write\r\npermissions instead of group write (S_IWOTH, S_IWGRP).\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Chris Dunlop <chris@onthe.net.au>\r\nSigned-off-by: Tobin C. Harding <me@tobin.cc>\r\nCloses #6684 \r\nCloses #6722"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d95a59805f8f00046bb449fb12161e1f0caf65fb", "message": "Remove unnecessary equality check\n\nCurrently `if` statement includes an assignment (from a function return\r\nvalue) and a equality check. The parenthesis are in the incorrect place,\r\ncurrently the code clobbers the function return value because of this.\r\n\r\nWe can fix this by simplifying the `if` statement.\r\n\r\n`if (foo != 0)`\r\n\r\ncan be more succinctly expressed as\r\n\r\n`if (foo)`\r\n\r\nRemove the equality check, add parenthesis to correct the statement.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Chris Dunlop <chris@onthe.net.au>\r\nSigned-off-by: Tobin C. Harding <me@tobin.cc>\r\nCloses #6685 \r\nClose #6719"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Conan-Kudo": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/7670f721fc82e6cdcdd31f83760a79b6f2f2b998", "message": "Add DKMS package on Debian-based distributions\n\n* config/deb.am: Enable building DKMS packages for Debian\r\n* rpm/generic/zfs-dkms.spec.in: Adjust spec to be Debian-compatible\r\n  * Condition kernel-devel Req to RPM distros\r\n  * Adjust the DKMS Req to have a minimum of a version only\r\n  * Ensure that --rpm_safe_upgrade isn't used on non-RPM distros\r\n* config/deb.am: Drop CONFIG_KERNEL and CONFIG_USER guards\r\n* Makefile.am: Add pkg-dkms target\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Neal Gompa <ngompa@datto.com>\r\nCloses #6044 \r\nCloses #6731"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "madwizard": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/cdc15a76045fa70743fb95a1fd450229e2b73fd3", "message": "Typo in dsl_dataset.h\n\nThe parameters dsl_dataset_t *os in function prototype should be\r\nrenamed to dsl_dataset_t *ds.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Damian Wojs\u0142aw <damian@wojslaw.pl>\r\nCloses #6756 \r\nCloses #6273"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "KireinaHoro": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d9ee0e2621d79c121b37e0a359d58834f37fbc78", "message": "Remove useless DEFAULT_INCLUDES in AM_CCASFLAGS\n\nCPPASCOMPILE and LTCPPASCOMPILE all include DEFAULT_INCLUDES,\nhence it's unnecessary to add the includes again.\n\nSigned-off-by: Pengcheng Xu <i@jsteward.moe>"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/e102b1b51521f8444b5512796dca92e1c0518d1f", "message": "Fix libspl assembler flags to respect cpu type\n\nIt's important to respect the user's CFLAGS as mismatched -mcpu\nwill directly result in the assembler not able to produce correct\ncode. Fixes #6733.\n\nSigned-off-by: Pengcheng Xu <i@jsteward.moe>"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/a7ec8c47e21c76624149beb1c9490c0e1bedf2a8", "message": "SPARC optimizations for Encode()\n\nNormally a SPARC processor runs in big endian mode. Save the extra labor\nneeded for little endian machines when the target is a big endian one\n(sparc).\n\nSigned-off-by: Pengcheng Xu <i@jsteward.moe>"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/46d4fe880e15848b272593ae68333577206d9c5b", "message": "SPARC optimizations for SHA1Transform()\n\nPassing arguments explicitly into SHA1Transform() increases the number of\nregisters abailable to the compiler, hence leaving more local and out registers\navailable. The missing symbol of sha1_consts[], which prevents compiling on\nSPARC, is added back, which speeds up the process of utilizing the relative\nconstants.\nThis should fix #6738.\n\nSigned-off-by: Pengcheng Xu <i@jsteward.moe>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alaraun": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d4404c3fdbe11f5220fd6ab429bb2f12ac93d1f4", "message": "Fix boot from ZFS issues\n\n* Correct ZFS snapshot listing\r\n* Disable \"lvm is not available\" message on quiet boot\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Alar Aun <spamtoaun@gmail.com>\r\nCloses #6700 \r\nCloses #6747"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "privb0x23": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/4f23c5d0c45649ba2bb7679c86561b006aee2dfe", "message": "Fix inclusion of libgcc_s.so on Void\n\nOn Void Linux (x86_64 musl) libgcc_s.so is located in \"/usr/lib\"\r\nso it is not found by dracut and it produces an error.\r\n\r\nAdd a simple additional path check for \"/usr/lib/libgcc_s.so*\"\r\nand install it in the initramfs.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: privb0x23 <privb0x23@users.noreply.github.com>\r\nCloses #6715"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ofaaland": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/b33d668ddb005b1072c26d36e1abff53aa39ca98", "message": "Fix ZTS MMP tests and ztest -M behavior\n\nQuote \"$MMP_IMPORT_MSG\" when it is passed as an argument, as it is a\r\nmulti-word string.  Some tests were passing when they should not have,\r\nbecause the grep was only testing for the first word.\r\n\r\nCorrect the message expected when no hostid is set and the test attempts\r\nto enable multihost.  It did not match the actual output in that\r\nsituation.\r\n\r\nDisable ztest_reguid() when ztest is invoked with the -M option.  If\r\nztest performs a reguid, a concurrent import attempt may fail with the\r\nerror \"one or more devices is currently unavailable\" if the guid sum is\r\ncalculated on the original device guids but compared against the guid\r\nsum ztest wrote based on the new device guids.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Olaf Faaland <faaland1@llnl.gov>\r\nCloses #6666"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d410c6d9fd4db812a1009e1603c89f1e222e1875", "message": "Reimplement vdev_random_leaf and rename it\n\nRename it as mmp_random_leaf() since it is defined in mmp.c.\r\n\r\nThe earlier implementation could end up spinning forever if a pool had a\r\nvdev marked writeable, none of whose children were writeable.  It also\r\ndid not guarantee that if a writeable leaf vdev existed, it would be\r\nfound.\r\n\r\nReimplement to recursively walk the device tree to select the leaf.  It\r\nsearches the entire tree, so that a return value of (NULL) indicates\r\nthere were no usable leaves in the pool; all were either not writeable\r\nor had pending mmp writes.\r\n\r\nIt still chooses the starting child randomly at each level of the tree,\r\nso if the pool's devices are healthy, the mmp writes go to random leaves\r\nwith an even distribution.  This was verified by testing using\r\nzfs_multihost_history enabled.\r\n\r\nReviewed by: Thomas Caputi <tcaputi@datto.com>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Olaf Faaland <faaland1@llnl.gov>\r\nCloses #6631 \r\nCloses #6665"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/4c5b89f59e4e5c8f5b4680040118ebde09598bbe", "message": "Improved dnode allocation and dmu_hold_impl()\n\nRefactor dmu_object_alloc_dnsize() and dnode_hold_impl() to simplify the\r\ncode, fix errors introduced by commit dbeb879 (PR #6117) interacting\r\nbadly with large dnodes, and improve performance.\r\n\r\n* When allocating a new dnode in dmu_object_alloc_dnsize(), update the\r\npercpu object ID for the core's metadnode chunk immediately.  This\r\neliminates most lock contention when taking the hold and creating the\r\ndnode.\r\n\r\n* Correct detection of the chunk boundary to work properly with large\r\ndnodes.\r\n\r\n* Separate the dmu_hold_impl() code for the FREE case from the code for\r\nthe ALLOCATED case to make it easier to read.\r\n\r\n* Fully populate the dnode handle array immediately after reading a\r\nblock of the metadnode from disk.  Subsequently the dnode handle array\r\nprovides enough information to determine which dnode slots are in use\r\nand which are free.\r\n\r\n* Add several kstats to allow the behavior of the code to be examined.\r\n\r\n* Verify dnode packing in large_dnode_008_pos.ksh.  Since the test is\r\npurely creates, it should leave very few holes in the metadnode.\r\n\r\n* Add test large_dnode_009_pos.ksh, which performs concurrent creates\r\nand deletes, to complement existing test which does only creates.\r\n\r\nWith the above fixes, there is very little contention in a test of about\r\n200,000 racing dnode allocations produced by tests 'large_dnode_008_pos'\r\nand 'large_dnode_009_pos'.\r\n\r\nname                            type data\r\ndnode_hold_dbuf_hold            4    0\r\ndnode_hold_dbuf_read            4    0\r\ndnode_hold_alloc_hits           4    3804690\r\ndnode_hold_alloc_misses         4    216\r\ndnode_hold_alloc_interior       4    3\r\ndnode_hold_alloc_lock_retry     4    0\r\ndnode_hold_alloc_lock_misses    4    0\r\ndnode_hold_alloc_type_none      4    0\r\ndnode_hold_free_hits            4    203105\r\ndnode_hold_free_misses          4    4\r\ndnode_hold_free_lock_misses     4    0\r\ndnode_hold_free_lock_retry      4    0\r\ndnode_hold_free_overflow        4    0\r\ndnode_hold_free_refcount        4    57\r\ndnode_hold_free_txg             4    0\r\ndnode_allocate                  4    203154\r\ndnode_reallocate                4    0\r\ndnode_buf_evict                 4    23918\r\ndnode_alloc_next_chunk          4    4887\r\ndnode_alloc_race                4    0\r\ndnode_alloc_next_block          4    18\r\n\r\nThe performance is slightly improved for concurrent creates with\r\n16+ threads, and unchanged for low thread counts.\r\n\r\nSigned-off-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Olaf Faaland <faaland1@llnl.gov>\r\nCloses #5396 \r\nCloses #6522 \r\nCloses #6414 \r\nCloses #6564"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/e889f0f520971fc2688189cdbe0efde2ccc8ec65", "message": "Report MMP_STATE_NO_HOSTID immediately\n\nThere is no need to perform the activity check before detecting that the\nuser must set the system hostid, because the pool's multihost property\nis on, but spa_get_hostid() returned 0.  The initial call to\nvdev_uberblock_load() provided the information required.\n\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nSigned-off-by: Olaf Faaland <faaland1@llnl.gov>\nCloses #6388"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/0582e403221008480657a88e8f50aecc88397c80", "message": "Add callback for zfs_multihost_interval\n\nAdd a callback to wake all running mmp threads when\nzfs_multihost_interval is changed.\n\nThis is necessary when the interval is changed from a very large value\nto a significantly lower one, while pools are imported that have the\nmultihost property enabled.\n\nWithout this commit, the mmp thread does not wake up and detect the new\ninterval until after it has waited the old multihost interval time.  A\nuser monitoring mmp writes via the provided kstat would be led to\nbelieve that the changed setting did not work.\n\nAdded a test in the ZTS under mmp to verify the new functionality is\nworking.\n\nAdded a test to ztest which starts and stops mmp threads, and calls into\nthe code to signal sleeping mmp threads, to test for deadlocks or\nsimilar locking issues.\n\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nSigned-off-by: Olaf Faaland <faaland1@llnl.gov>\nCloses #6387"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/60f510344596b1f2f890df2e96282d586edf6aaf", "message": "Skip activity check for zhack RO import\n\n\"zhack feature stat\" performs a read-only import, so the MMP activity\ncheck is not necessary.\n\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nSigned-off-by: Olaf Faaland <faaland1@llnl.gov>\nCloses #6388\nCloses #6389"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/b9373170e3e346733f5666dd80727cb6e05cd5d3", "message": "Add zgenhostid utility script\n\nTurning the multihost property on requires that a hostid be set to allow\nZFS to determine when a foreign system is attemping to import a pool.\nThe error message instructing the user to set a hostid refers to\ngenhostid(1).\n\nGenhostid(1) is not available on SUSE Linux.  This commit adds a script\nmodeled after genhostid(1) for those users.\n\nZgenhostid checks for an /etc/hostid file; if it does not exist, it\ncreates one and stores a value.  If the user has provided a hostid as an\nargument, that value is used.  Otherwise, a random hostid is generated\nand stored.\n\nThis differs from the CENTOS 6/7 versions of genhostid, which overwrite\nthe /etc/hostid file even though their manpages state otherwise.\n\nA man page for zgenhostid is added. The one for genhostid is in (1), but\nI put zgenhostid in (8) because I believe it's more appropriate.\n\nThe mmp tests are modified to use zgenhostid to set the hostid instead\nof using the spl_hostid module parameter.  zgenhostid will not replace\nan existing /etc/hostid file, so new mmp_clear_hostid calls are\nrequired.\n\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\nReviewed-by: Andreas Dilger <andreas.dilger@intel.com>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nSigned-off-by: Olaf Faaland <faaland1@llnl.gov>\nCloses #6358\nCloses #6379"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/ffb195c256f8a74a87c3834258ec90c513d66adb", "message": "Release SCL_STATE in map_write_done()\n\nThe config lock must be held for the duration of the MMP write.\nSince the I/Os are executed via map_nowait(), the done function\nis the only place where we know the write has completed.\n\nSince SCL_STATE is taken as reader, overlapping I/Os do not\ncreate a deadlock.  The refcount is simply increased when new\nI/Os are queued and decreased when I/Os complete.\n\nTest case added which exercises the probe IO call path to\nverify the fix and prevent a regression.\n\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nSigned-off-by: Olaf Faaland <faaland1@llnl.gov>\nCloses #6394"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/f43615d0cc22d7db496c1291c84e64e269ca51d9", "message": "Revert Fix vdev_probe() call wrt SCL_STATE_ALL\n\nThis reverts commit cc9c6bc, which has been causing intermittent\ntest failures on buildbot.  A correct fix for this locking issue\nhas been applied in a separate patch.\n\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nSigned-off-by: Olaf Faaland <faaland1@llnl.gov>"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/b6e5c40382a52206f48cb26cc20ed85294e1b0a9", "message": "Use correct macro for hz in mmp.c\n\nCommit 379ca9c Multi-modifier protection (MMP) used HZ to convert\r\nnanoseconds to ticks for use with cv_timedwait() and ddi_get_lbolt().\r\nThe correct macro is hz, which is defined within the SPL for kernel\r\nspace, and within zfs_context.h for user space.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Tony Hutter <hutter2@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Olaf Faaland <faaland1@llnl.gov>\r\nCloses #6357 \r\nCloses #6360"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/379ca9cf2beba802f096273e89e30914a2d6bafc", "message": "Multi-modifier protection (MMP)\n\nAdd multihost=on|off pool property to control MMP.  When enabled\na new thread writes uberblocks to the last slot in each label, at a\nset frequency, to indicate to other hosts the pool is actively imported.\nThese uberblocks are the last synced uberblock with an updated\ntimestamp.  Property defaults to off.\n\nDuring tryimport, find the \"best\" uberblock (newest txg and timestamp)\nrepeatedly, checking for change in the found uberblock.  Include the\nresults of the activity test in the config returned by tryimport.\nThese results are reported to user in \"zpool import\".\n\nAllow the user to control the period between MMP writes, and the\nduration of the activity test on import, via a new module parameter\nzfs_multihost_interval.  The period is specified in milliseconds.  The\nactivity test duration is calculated from this value, and from the\nmmp_delay in the \"best\" uberblock found initially.\n\nAdd a kstat interface to export statistics about Multiple Modifier\nProtection (MMP) updates. Include the last synced txg number, the\ntimestamp, the delay since the last MMP update, the VDEV GUID, the VDEV\nlabel that received the last MMP update, and the VDEV path.  Abbreviated\noutput below.\n\n$ cat /proc/spl/kstat/zfs/mypool/multihost\n31 0 0x01 10 880 105092382393521 105144180101111\ntxg   timestamp  mmp_delay   vdev_guid   vdev_label vdev_path\n20468    261337  250274925   68396651780       3    /dev/sda\n20468    261339  252023374   6267402363293     1    /dev/sdc\n20468    261340  252000858   6698080955233     1    /dev/sdx\n20468    261341  251980635   783892869810      2    /dev/sdy\n20468    261342  253385953   8923255792467     3    /dev/sdd\n20468    261344  253336622   042125143176      0    /dev/sdab\n20468    261345  253310522   1200778101278     2    /dev/sde\n20468    261346  253286429   0950576198362     2    /dev/sdt\n20468    261347  253261545   96209817917       3    /dev/sds\n20468    261349  253238188   8555725937673     3    /dev/sdb\n\nAdd a new tunable zfs_multihost_history to specify the number of MMP\nupdates to store history for. By default it is set to zero meaning that\nno MMP statistics are stored.\n\nWhen using ztest to generate activity, for automated tests of the MMP\nfunction, some test functions interfere with the test.  For example, the\npool is exported to run zdb and then imported again.  Add a new ztest\nfunction, \"-M\", to alter ztest behavior to prevent this.\n\nAdd new tests to verify the new functionality.  Tests provided by\nGiuseppe Di Natale.\n\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\nReviewed-by: Ned Bass <bass6@llnl.gov>\nReviewed-by: Andreas Dilger <andreas.dilger@intel.com>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nSigned-off-by: Olaf Faaland <faaland1@llnl.gov>\nCloses #745\nCloses #6279"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/34ae0ae1749f297c23c3c1680ea552df94ae2122", "message": "Make hostid consistent in user and kernel space\n\nIf no spl_hostid was set, and no /etc/hostid file existed, the user\nand kernel would have different values for the hostid.\n\nThe kernel's would be 0.  User space's would depend on the libc\nimplementation.  On systems with glibc, it would be a generated value,\nprobably the first 4 bytes of an IP address (see man 3 gethostid and\ncomments above hostid_read in SPL for details).\n\nThis then causes the hostid stored in the labels and in the pool\nconfig not to match the hostid userspace obtains from\nget_system_hostid().\n\nSince the kernel has no way to know the libc's generated hostid value,\nit serves no purpose for ZFS to use the value.\n\nThis patch changes user space's get_system_hostid() to conform to the\nkernel's method, first checking for the spl_hostid via sysfs, and then\nreading from /etc/hostid directly.\n\nIt does not look up spl_hostid_path, because if that is set and the\nfile it pointed to exists, spl_hostid will reflect its contents.\n\nIt eliminates the call to libc's gethostid().\n\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\nReviewed-by: Ned Bass <bass6@llnl.gov>\nReviewed-by: Andreas Dilger <andreas.dilger@intel.com>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nSigned-off-by: Olaf Faaland <faaland1@llnl.gov>\nCloses #745\nCloses #6279"}], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6479", "title": "Merge SPL into ZFS [WIP]", "body": "<!--- Provide a general summary of your changes in the Title above -->\r\n\r\n<!---\r\nDocumentation on ZFS Buildbot options can be found at\r\nhttps://github.com/zfsonlinux/zfs/wiki/Buildbot-Options\r\n-->\r\n\r\n### Description\r\n<!--- Describe your changes in detail -->\r\nMerge the SPL into ZFS to eliminate the extra work required when SPL code must change due to kernel or distro changes, and to simplify the build process.\r\n\r\nWork In Progress.\r\n\r\n### Motivation and Context\r\n<!--- Why is this change required? What problem does it solve? -->\r\n<!--- If it fixes an open issue, please link to the issue here. -->\r\n\r\n### How Has This Been Tested?\r\n<!--- Please describe in detail how you tested your changes. -->\r\n<!--- Include details of your testing environment, and the tests you ran to -->\r\n<!--- see how your change affects other areas of the code, etc. -->\r\n<!--- If your change is a performance enhancement, please provide benchmarks here. -->\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Performance enhancement (non-breaking change which improves efficiency)\r\n- [x] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [ ] My code follows the ZFS on Linux code style requirements.\r\n- [ ] I have updated the documentation accordingly.\r\n- [x] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [ ] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "loyou": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/18a2485fc8061f7393ca19f7291366bc46fe9bf7", "message": "misc: fix meaningless values\n\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Tim Chase <tim@chase2k.com>\r\nSigned-off-by: Feng Sun <loyou85@gmail.com>\r\nCloses #6658"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gaurkuma": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/0107f69898e3b6d39d974b272da1739cb2f39027", "message": "Modifying XATTRs doesnt change the ctime\n\nChanging any metadata, should modify the ctime.\r\n\r\nReviewed-by: Chunwei Chen <tuxoko@gmail.com>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: gaurkuma <gauravk.18@gmail.com>\r\nCloses #3644 \r\nCloses #6586"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/761b8ec6bf98f39550353173ad7bec5306073f9c", "message": "Allow longer SPA names in stats\n\nThe pool name can be 256 chars long. Today, in /proc/spl/kstat/zfs/\r\nthe name is limited to < 32 characters. This change is to allows\r\nbigger pool names.\r\n\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: loli10K <ezomori.nozomu@gmail.com>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: gaurkuma <gauravk.18@gmail.com>\r\nCloses #6481"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/520faf5ddcb1a9536f53438963d1c96678040466", "message": "Crash in dbuf_evict_one with DTRACE_PROBE\n\nUpdate the dbuf__evict__one() tracepoint so that it can safely\r\nhandle a NULL dmu_buf_impl_t pointer.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>    \r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: loli10K <ezomori.nozomu@gmail.com>\r\nSigned-off-by: gaurkuma <gauravk.18@gmail.com>\r\nCloses #6463"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chungy": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/57858fb5ca4b1d0f28b054cb5e15aaf3bc54c99e", "message": "Recommend compression=on in zfs(8) dedup section\n\ncompression=lz4 depends on the lz4 feature being enabled, while\r\ncompression=on will let ZFS use either lzjb or lz4 where appropriate.\r\nIt also allows the documentation to not go out of date if/when ZFS\r\npicks a new default in the future.\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Mike Swanson <mikeonthecomputer@gmail.com>\r\nCloses #6614"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "richlowe": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/1afc54f7f4f88902d0a3a3d88f6c7c6311c886ee", "message": "OpenZFS 2976 - remove useless offsetof() macros\n\nAuthored by: Richard Lowe <richlowe@richlowe.net>\nReviewed by: Josef 'Jeff' Sipek <jeffpc@josefsipek.net>\nReviewed by: Igor Kozhukhov <ikozhukhov@gmail.com>\nReviewed by: Andy Stormont <andyjstormont@gmail.com>\nApproved by: Dan McDonald <danmcd@omniti.com>\nReviewed-by: George Melikov <mail@gmelikov.ru>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nPorted-by: Giuseppe Di Natale <dinatale2@llnl.gov>\n\nOpenZFS-issue: https://www.illumos.org/issues/2976\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/5c5f137\nCloses #6582"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ironMann": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d22323e89f13975e1a60860aa78609584f00a606", "message": "dmu_objset: release bonus buffer in failure path\n\nReported by kmemleak during testing of a new patch:\r\n\r\n```\r\nunreferenced object 0xffff9f1c12e38800 (size 1024):\r\n  comm \"z_upgrade\", pid 17842, jiffies 4296870904 (age 8746.268s)\r\n  backtrace:\r\n    kmemleak_alloc+0x7a/0x100\r\n    __kmalloc_node+0x26c/0x510\r\n    range_tree_create+0x39/0xa0 [zfs]\r\n    dmu_zfetch_init+0x73/0xe0 [zfs]\r\n    dnode_create+0x12c/0x3b0 [zfs]\r\n    dnode_hold_impl+0x1096/0x1130 [zfs]\r\n    dnode_hold+0x23/0x30 [zfs]\r\n    dmu_bonus_hold_impl+0x6b/0x370 [zfs]\r\n    dmu_bonus_hold+0x1e/0x30 [zfs]\r\n    dmu_objset_space_upgrade+0x114/0x310 [zfs]\r\n    dmu_objset_userobjspace_upgrade_cb+0xd8/0x150 [zfs]\r\n    dmu_objset_upgrade_task_cb+0x136/0x1e0 [zfs]    \r\n    kthread+0x119/0x150\r\n```\r\n\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Gvozden Neskovic <neskovic@gmail.com>\r\nCloses #6575"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/551905dd475c6b4c4fa87d7734f018084a755af8", "message": "vdev_mirror: kstat observables for preferred vdev\n\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nSigned-off-by: Gvozden Neskovic <neskovic@gmail.com>\nCloses #6461"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d6c6590c5dd727832a58e77f7830049631b0c266", "message": "vdev_mirror: load balancing fixes\n\nvdev_queue:\n- Track the last position of each vdev, including the io size,\n  in order to detect linear access of the following zio.\n- Remove duplicate `vq_lastoffset`\n\nvdev_mirror:\n- Correctly calculate the zio offset (signedness issue)\n- Deprecate `vdev_queue_register_lastoffset()`\n- Add `VDEV_LABEL_START_SIZE` to zio offset of leaf vdevs\n\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nSigned-off-by: Gvozden Neskovic <neskovic@gmail.com>\nCloses #6461"}], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6568", "title": "[wip][test] Prefetch dmu", "body": "<!--- Provide a general summary of your changes in the Title above -->\r\nRun testers\r\n<!---\r\nDocumentation on ZFS Buildbot options can be found at\r\nhttps://github.com/zfsonlinux/zfs/wiki/Buildbot-Options\r\n-->\r\n\r\n### Description\r\n<!--- Describe your changes in detail -->\r\n\r\n### Motivation and Context\r\n<!--- Why is this change required? What problem does it solve? -->\r\n<!--- If it fixes an open issue, please link to the issue here. -->\r\n\r\n### How Has This Been Tested?\r\n<!--- Please describe in detail how you tested your changes. -->\r\n<!--- Include details of your testing environment, and the tests you ran to -->\r\n<!--- see how your change affects other areas of the code, etc. -->\r\n<!--- If your change is a performance enhancement, please provide benchmarks here. -->\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [ ] My code follows the ZFS on Linux code style requirements.\r\n- [ ] I have updated the documentation accordingly.\r\n- [ ] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [ ] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6107", "title": "LOCK tracking: disable tracking of ARC and dbuf hashmap locks (16384 mutexes)", "body": "Test for zfsonlinux/spl#587\r\n\r\nRequires-spl: refs/pull/587/head\r\n\r\n### Description\r\nDisable tracking of per-bucket locks.\r\n\r\n### Motivation and Context\r\n<!--- Why is this change required? What problem does it solve? -->\r\n<!--- If it fixes an open issue, please link to the issue here. -->\r\n\r\n### How Has This Been Tested?\r\n<!--- Please describe in detail how you tested your changes. -->\r\n<!--- Include details of your testing environment, and the tests you ran to -->\r\n<!--- see how your change affects other areas of the code, etc. -->\r\n<!--- If your change is a performance enhancement, please provide benchmarks here. -->\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [ ] I have updated the documentation accordingly.\r\n- [ ] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [ ] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/5095", "title": "RAID-Z: Increase read IOPS of metadata blocks", "body": "If IO fits in a single block, then corresponding raidz block consists of (nparity+1)\nblocks. Additionally, all parity blocks are equal to the data block, effectively\nproviding mirroring. This is expected and desired behavior, since it preserves\nreliability criteria of selected raid-z level, and should enable better IOPS for\nsmall metadata blocks.\n\nThis can be exploited in the read path. Such block are usually metadata and\nby selecting the least loaded child vdev from (nparity + 1) should yield more\nIOPS. The higher the RAID-Z level, the more \"mirror\" devices there are to choose from.\nTotal number of child devices does not matter.\n\nThis optimization seems similar to \"RAIDZ Hybrid Allocator\", but does not require\non-disk data change.\n\nAdditionally:\n- Heuristics from vdev_mirror 9f50093\n- mark last offset position of vdev in zio_vdev_child_io(). This should better\n  track offset position for all reads/writes, hopefully improving heuristics for\n  both mirror and raidz.\n", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ahrens": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/24ded86e8dd528b056d73630ff33e526f9540dbc", "message": "OpenZFS 7261 - nvlist code should enforce name length limit\n\nAuthored by: Matthew Ahrens <mahrens@delphix.com>\nReviewed by: Sebastien Roy <sebastien.roy@delphix.com>\nReviewed by: George Wilson <george.wilson@delphix.com>\nReviewed by: Robert Mustacchi <rm@joyent.com>\nApproved by: Dan McDonald <danmcd@omniti.com>\nReviewed-by: Don Brady <dev.fs.zfs@gmail.com>\nReviewed-by: George Melikov <mail@gmelikov.ru>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nPorted-by: Giuseppe Di Natale <dinatale2@llnl.gov>\n\nOpenZFS-issue: https://www.illumos.org/issues/7261\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/48dd5e6\nCloses #6579"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/006309e8d75b22efc7418095e408b3b3774ef8ea", "message": "OpenZFS 8375 - Kernel memory leak in nvpair code\n\nAuthored by: Matthew Ahrens <mahrens@delphix.com>\nReviewed by: Pavel Zakharov <pavel.zakharov@delphix.com>\nReviewed by: George Wilson <george.wilson@delphix.com>\nReviewed by: Prashanth Sreenivasa <pks@delphix.com>\nReviewed by: Robert Mustacchi <rm@joyent.com>\nApproved by: Dan McDonald <danmcd@joyent.com>\nReviewed-by: Don Brady <dev.fs.zfs@gmail.com>\nReviewed-by: George Melikov <mail@gmelikov.ru>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nPorted-by: Giuseppe Di Natale <dinatale2@llnl.gov>\n\nOpenZFS-issue: https://www.illumos.org/issues/8375\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/843c211\nCloses #6578"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/1e0457e7f5384b0328ea499083120dd191d80c90", "message": "Enhance comments for large dnode project\n\nFix a few nits in the comments from large dnodes. Also import\r\nsome of the commit message as a comment in the code, making\r\nit more accessible.\r\n\r\nReviewed-by: @rottegift \r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Wilson <george.wilson@delphix.com>\r\nSigned-off-by: Matt Ahrens <mahrens@delphix.com>\r\nCloses #6551"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/2ade4a99f0b995df6fb45138d04a0209b959ef63", "message": "OpenZFS 8126 - ztest assertion failed in dbuf_dirty due to dn_nlevels changing\n\nThe sync thread is concurrently modifying dn_phys->dn_nlevels\nwhile dbuf_dirty() is trying to assert something about it, without\nholding the necessary lock. We need to move this assertion further down\nin the function, after we have acquired the dn_struct_rwlock.\n\nAuthored by: Matthew Ahrens <mahrens@delphix.com>\nReviewed by: Pavel Zakharov <pavel.zakharov@delphix.com>\nReviewed by: Serapheim Dimitropoulos <serapheim@delphix.com>\nApproved by: Robert Mustacchi <rm@joyent.com>\nReviewed-by: George Melikov <mail@gmelikov.ru>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nPorted-by: Giuseppe Di Natale <dinatale2@llnl.gov>\n\nOpenZFS-issue: https://www.illumos.org/issues/8126\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/0ef125d\nCloses #6314"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/a896468c7872dc7277fd0171c65bd2c185bfcaab", "message": "OpenZFS 8067 - zdb should be able to dump literal embedded block pointer\n\nAuthored by: Matthew Ahrens <mahrens@delphix.com>\nReviewed by: George Wilson <george.wilson@delphix.com>\nReviewed by: Alex Reece <alex@delphix.com>\nReviewed by: Yuri Pankov <yuri.pankov@gmail.com>\nApproved by: Robert Mustacchi <rm@joyent.com>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nPorted-by: Giuseppe Di Natale <dinatale2@llnl.gov>\n\nOpenZFS-issue: https://www.illumos.org/issues/8067\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/8173085\nCloses #6319"}], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6536", "title": "diff and bookmark enhancements", "body": "\r\n<!--- Provide a general summary of your changes in the Title above -->\r\n\r\n<!---\r\nDocumentation on ZFS Buildbot options can be found at\r\nhttps://github.com/zfsonlinux/zfs/wiki/Buildbot-Options\r\n-->\r\n\r\n### Description\r\n<!--- Describe your changes in detail -->\r\nThis PR includes 3 related features that work well together:\r\n\r\n`zfs diff -a` shows which specific blocks were modified\r\n\r\n`zfs diff` from a bookmark (but it can't show renamed files)\r\n\r\n`zfs bookmark` from a filesystem, creating a bookmark which represents current point in time.  Not useful for `zfs send`, but can be used with `zfs diff`.\r\n\r\n### Motivation and Context\r\n<!--- Why is this change required? What problem does it solve? -->\r\n<!--- If it fixes an open issue, please link to the issue here. -->\r\n\r\nThis makes `zfs diff` useful in more situations.  For example, to find which blocks in a database or VDI file were changed.\r\n\r\n### How Has This Been Tested?\r\n<!--- Please describe in detail how you tested your changes. -->\r\n<!--- Include details of your testing environment, and the tests you ran to -->\r\n<!--- see how your change affects other areas of the code, etc. -->\r\n<!--- If your change is a performance enhancement, please provide benchmarks here. -->\r\n\r\nManual testing only at this point.  I'd like to add test cases to the test suite, but there aren't any tests for \"zfs diff\" at all, so it seems strange to add tests for just the new functionality I'm adding.  I'm open to input on what should be required for this PR.\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [x] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [x] I have updated the documentation accordingly.\r\n- [x] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [ ] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6759043", "body": "I do not believe that this fix resolves the issues marked as closed.  The actual problem is that there is a DVA with an invalid vdev ID.  This change may serve to mask the problem, or more likely just move it elsewhere, to the next function that tries to use the invalid vdev ID.\n\nThe correct way to address an incorrect pool is to ASSERT that the vdev ID is valid (or panic with a nicer error message, or fail with EIO but that would suppress bug reports when this happens).  This should probably happen in vdev_lookup_top(); currently only some callers do so (e.g. metaslab_free_dva()).\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6759043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/7163233", "body": "I'm pretty sure there can be bonus data stored in the middle here.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/7163233/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405455", "body": "use fnvlist_alloc(), it allows for simpler code\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405455/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405463", "body": "this could be a lot of realloc()-ing.  Hope it isn't too slow.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405463/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405470", "body": "code will be simpler (less casts) if you do:\n\nnv_dict_props = fnvlist_alloc();\n((nvlist_t**)cb->cb_data)[cb_nelem - 1] = nv_dict_props\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405470/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405478", "body": "style: all continuation lines should be indented +4 spaces compared to previous line\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405478/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405481", "body": "fix indentation\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405481/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405484", "body": "fix indentation\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405487", "body": "style: multi-line if/else body requires braces (even if only one statement)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405487/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405498", "body": "It would probably be easier to follow if you just created a new function, json_dataset() which is analogous to print_dataset() but is called when cb_json is set.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405498/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405507", "body": "again probably simpler to create a new func list_callback_json() to handle the json case.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405507/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405509", "body": "use fnvlist_alloc()\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405509/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405514", "body": "why does this need to be done by the callback, as opposed to by the caller?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405514/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405520", "body": "did you check all callbacks used with zfs_for_each() to make sure that they handle (and ignore, presumably) invocations with a NULL first argument?\n\nIf possible, it would be cleaner to remove this, and instead have the caller do whatever it needs after calling zfs_for_each().\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405520/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405552", "body": "I think you should use zfs_prop_to_name().  I don't think that this code (column_name()) will generate the output you have in your example (e.g. it will use \"REFER\" instead of \"referenced\").\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405552/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405573", "body": "would it be preferable for the output to have the dataset name as the key?  I think so.  Otherwise, something like \"zfs list -J -o used\" is basically useless (as it often is without the -J flag, but no reason to repeat that problem here).\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405573/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405582", "body": "style: no space after !\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405582/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405584", "body": "no space after !\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405584/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405585", "body": "no space after !\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405585/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405587", "body": "need \"break\"\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405587/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405603", "body": "seems like this should imply -p as well (i.e. set cb_literal), otherwise won't we get values like \"1.2MB\"?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20405603/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28807663", "body": "Why are you doing this in another thread, rather than in this thread?  Userland can easily create its own thread to do this if it wants.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28807663/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25310742", "body": "This is not C :-)\n\nAn \"inline static\" would be appropriate here, or just drop the assertions.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25310742/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25310752", "body": "Is this still true?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25310752/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25409189", "body": "why does this (and friends) need a length parameter (\"n\")?  Can't you use a->abd_size?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25409189/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25544274", "body": "Is that common enough that the performance benefit warrants the added\ncode?  If so, perhaps adding a 2nd function (abd_borrow_buf_len, or even\nthe fully-fledged abd_borrow_buf_offset_len) that takes the additional\nparameters would make sense.\n\nOn Fri, Feb 27, 2015 at 2:10 PM, tuxoko notifications@github.com wrote:\n\n> In include/sys/abd.h\n> https://github.com/zfsonlinux/zfs/pull/2129#discussion_r25543144:\n> \n> > +{                                  \\\n> > -   unsigned int ___ret;                        \\\n> > -   if (ABD_IS_LINEAR(abd))                     \\\n> > -       ___ret = bio_map(bio, ABD_TO_BUF(abd) + (off), size);   \\\n> > -   else                                \\\n> > -       ___ret = abd_scatter_bio_map_off(bio, abd, size, off);  \\\n> > -   ___ret;                             \\\n> >   +}                                  \\\n> >   +)\n> >   +#endif /\\* _KERNEL _/\n> >   +\n> >   +/_\n> > - \\* Borrow a linear buffer for an ABD\n> > - \\* Will allocate if ABD is scatter\n> > - */\n> >   +#define    abd_borrow_buf(a, n)            \\\n> \n> I'll retract that. They do need a length, becuase the user of these\n> function might be only accessing a part of the buffer.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/zfsonlinux/zfs/pull/2129/files#r25543144.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25544274/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25741349", "body": "adata may be NULL (e.g. if it's a zio_free).  in this case you need to set abuf to NULL; abd_borrow_buf_copy(NULL) won't work.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25741349/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "alaviss": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/1ea8942faa90c1787f3ad1bda44dc26bfc3186da", "message": "libtpool: don't clone affinity if not supported\n\npthread_attr_(get/set)affinity_np() is glibc-only. This commit\r\ndisable the code path that use those functions in non-glibc\r\nsystem. Fixes the following when building with musl:\r\n\r\nlibzfs.so: undefined reference to`pthread_attr_setaffinity_np'\r\nlibzfs.so: undefined reference to`pthread_attr_getaffinity_np'\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Leorize <alaviss@users.noreply.github.com>\r\nCloses #6571"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/478e3ecf8bc98687a336a64214c0e5e122454f52", "message": "Musl libc fixes\n\nMusl libc's <stdio.h> doesn't include <stdarg.h>, which cause\r\n`va_start` and `va_end` end up being undefined symbols.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Leorize <alaviss@users.noreply.github.com>\r\nCloses #6310"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/688c94c5c03c693f91d48adcf4b8826f8e16c6fd", "message": "Clang fixes\n\nClang doesn't support `/` as comment in assembly, this patch replaces\r\nthem with `#`.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Leorize <alaviss@users.noreply.github.com>\r\nCloses #6311"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tuxoko": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/05f85a6a64e999748ffbdf7a76b406f0f0ceb8b3", "message": "Fix zfs_ioc_pool_sync should not use fnvlist\n\nUse fnvlist on user input would allow user to easily panic zfs.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nReviewed-by: Alek Pinchuk <apinchuk@datto.com>\r\nSigned-off-by: Chunwei Chen <david.chen@osnexus.com>\r\nCloses #6529"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/376994828fd3753aba75d492859727ca76f6a293", "message": "Fix NULL pointer when O_SYNC read in snapshot\n\nWhen doing read on a file open with O_SYNC, it will trigger zil_commit.\r\nHowever for snapshot, there's no zil, so we shouldn't be doing that.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nSigned-off-by: Chunwei Chen <david.chen@osnexus.com>\r\nCloses #6478 \r\nCloses #6494"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/83a5e4d6b9c2509efa25ea4cfceba3cd313bf920", "message": "Fix don't zero_label when replace with spare\n\nWhen replacing a disk with non-wholedisk spare, we shouldn't zero_label\r\nit. The wholedisk case already skip it. In fact, zero_label function\r\nwill fail saying device busy because it's already opened exclusively,\r\nbut since there's no error checking, the replace command will succeed,\r\ncausing great confusion.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Chunwei Chen <david.chen@osnexus.com>\r\nCloses #6369"}], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6277", "title": "[WIP] Add pool prop `partition` to disable auto partition", "body": "### Description\r\n\r\nzfsonlinux always partition disk when it detects the device given is a\r\nwhole disk. This legacy behavior from Illumos, however, has no apparent\r\nbenefit on Linux, but has some down sides besides confusion. E.g.\r\nautoexpand, switching to dm device requires partprobe.\r\n\r\nWe add a pool property `partition` to be set during pool create. It\r\ncurrently has two values, legacy and raw. When setting it to legacy, it\r\nwill behave as it did. When setiing it to raw, it will always use the\r\ndevice as is without partitioning even if it's a whole disk.\r\n\r\nThis property applies to all commands that add disks to pool, so zpool\r\nadd/attach/replace will partition or not partition based on the property\r\non the target pool.\r\n    \r\nA pool without this property will be treated as legacy. Newly created\r\npool will by default have partition=legacy.\r\n\r\nSigned-off-by: Chunwei Chen <david.chen@osnexus.com>\r\n\r\n### Note\r\n\r\nI use PROP_ONETIME for the property, but it seems that this is not enforced at all, so you can still modify it after the fact. But you shouldn't change it after the fact, as it would cause device name appending wrong.", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/4943", "title": "[WIP] Prototype for systemd and fstab integration", "body": "Recently, I have been cooking some stuff for better integration with systemd and fstab integration. This is what I've got so far.\n\nWhat does this achieve:\n1. Event driven zpool import from udev, no more non sense like wait for udev-settle.\n2. Configurable selective import with fstab or explicit systemd service file.\n3. Using fstab for mounting to let systemd build a proper mounting dependency.\n\nSo how does this work?\nI create a `zready` command, which udev will fire upon discovering zfs devices. The `zready` command will read the label and build up the vdev tree in `/dev/zpool/<pool>/`. And when it figures the pool is ready to import. It will create a file called `/dev/zpool/<pool>/ready`. (Note, the current zready is just a hack I came up with. Since I'm not familiar with the label stuff, it might not be able to handle every cases.)\n\nThere's two systemd unit files, `zpool@.path` and `zpool@.service`. `zpool@<pool>.path` will listen on the `/dev/zpool/<pool>/ready`. And `zpool@<pool>.service` will do import with dependency on `zpool@<pool>.path`. Note, you don't need to do any configuration with these two unit files if you use fstab as described below, but you can still explicit enable a zpool@<pool>.service if you want.\n\nTo let systemd mount stuff on boot, you only need to add an entry like this in fstab:\n`rpool/home /home zfs rw,defaults,x-systemd.requires=zpool@rpool.service`\nThis will automatically make this mount depends on the service unit.\n\nFor using zfs as root, each distribution probably has its only way to do it. But if you use systemd in initrd, you can add `root=rpool/root rootfstype=zfs rootflags=x-systemd.requires=zpool@rpool.service` to kernel cmdline. systemd will be able to automatically build the dependency just as using fstab.\n\nSigned-off-by: Chunwei Chen david.chen@osnexus.com\n", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25452603", "body": "I forgot to modify the comment. Will change that\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25452603/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25452722", "body": "You are right it doesn't need it. I think it was a legacy from older revision. I'll change that.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25452722/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25543144", "body": "I'll retract that. They do need a length, becuase the user of these function might be only accessing a part of the buffer.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25543144/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25549930", "body": "It's not about the performance. It's that we don't want to copy over stuff that we shouldn't.\nBut I guess we can add a length argument to abd_get_offset and hopefully get around this problem.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25549930/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/26507348", "body": "Yeah, I already knew this. For now, I think you can just change pad[PAGE_SIZE] to pad[4096].\nBut I think, I'll just rip out this ugly thing when I push the next revision.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/26507348/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/35999480", "body": "This is not true, KM_USER\\* are not used in interrupt context since... well it's \"user\". So it's perfectly safe.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/35999480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/36063995", "body": "@ryao \nkmap_atomic does turn off preemption itself.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/36063995/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/36276957", "body": "@ryao \nhttp://lxr.free-electrons.com/source/include/linux/uaccess.h#L16\n\n```\n  7 /*\n  8  * These routines enable/disable the pagefault handler in that\n  9  * it will not take any locks and go straight to the fixup table.\n 10  *\n 11  * They have great resemblance to the preempt_disable/enable calls\n 12  * and in fact they are identical; this is because currently there is\n 13  * no other way to make the pagefault handlers do this. So we do\n 14  * disable preemption but we don't necessarily care about that.\n 15  */\n 16 static inline void pagefault_disable(void)\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/36276957/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "BtbN": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/a1f3a1c05fa9cb06334189813d0e0d556d479620", "message": "Use /sbin/openrc-run for openrc init scripts\n\nUsing /sbin/runscript is deprecated and throws a QA warning\r\nwhen still used in init scripts.\r\n\r\nReviewed-by: bunder2015 <omfgbunder@gmail.com>\r\nSigned-off-by: BtbN <btbn@btbn.de>\r\nCloses #6519"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sckobras": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d49d9c2bdc31263536d3b714d7e393b66321886a", "message": "vdev_id: implement slot numbering by port id\n\nWith HPE hardware and hpsa-driven SAS adapters, only a single phy is\r\nreported, but no individual per-port phys (ie. no phy* entry below\r\nport_dir), which breaks topology detection in the current sas_handler\r\ncode. Instead, slot information can be derived directly from the port\r\nnumber. This change implements a new slot keyword \"port\" similar to\r\n\"id\" and \"lun\", and assumes a default phy/port of 0 if no individual\r\nphy entry can be found. It allows to use the \"sas_direct\" topology with\r\ncurrent HPE Dxxxx and Apollo 45xx JBODs.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Daniel Kobras <d.kobras@science-computing.de>\r\nCloses #6484"}], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/7007", "title": "Allow to limit zed's syslog chattiness", "body": "### Description\r\n\r\nSome usage patterns like send/recv of replication streams can\r\nproduce a large number of events. In such a case, the current\r\nall-syslog.sh zedlet will hold up to its name, and flood the\r\nlogs with mostly redundant information. To mitigate this\r\nsituation, this changeset introduces two new variables\r\nZED_SYSLOG_SUBCLASS_INCLUDE and ZED_SYSLOG_SUBCLASS_EXCLUDE\r\nto zed.rc that give more control over which event classes end\r\nup in the syslog.\r\n\r\nSigned-off-by: Daniel Kobras <d.kobras@science-computing.de>\r\nCloses: #6886\r\n\r\n### Motivation and Context\r\nIt seems that each time a dataset that also uses =zfs-auto-snapshot= is replicated, a =history_event= for the =com.sun:auto-snapshot-desc= property in each snapshot is logged. This easily spams the logs with thousands of redundant, and rather useless messages as described in #6886, so adding a facility to trim down the noise without disabling the syslog feature altogether seems to be in order.\r\n\r\n### How Has This Been Tested?\r\nTested on EL7.4 with ZoL 0.7.2.\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [x] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [x] I have updated the documentation accordingly.\r\n- [x] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [x] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bprotopopov": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/5146d802b4e371cab1d6db79bea482c056be7bf2", "message": "zv_suspend_lock in zvol_open()/zvol_release()\n\nAcquire zv_suspend_lock on first open and last close only.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Boris Protopopov <boris.protopopov@actifio.com>\r\nCloses #6342"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kkretschmer": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/d19a6d5c80fb24451a7d76716eaf38d3a3f933c7", "message": "dracut: Install commands required for vdev_id\n\nThe vdev_id script requires awk, grep, and head.  Use dracut_install to\r\nensure that these commands are available in the initrd environment.\r\n\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Karsten Kretschmer <kkretschmer@gmail.com>\r\nCloses #6443\r\nCloses #6452"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "SenH": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/1e1c398033384106c8ee96435ba0683797b41a46", "message": "Fix zpool events scripted mode tab separator\n\nReviewed-by: George Melikov <mail@gmelikov.ru>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nReviewed-by: Tony Hutter <hutter2@llnl.gov>\r\nReviewed-by: Giuseppe Di Natale <dinatale2@llnl.gov>\r\nSigned-off-by: Sen Haerens <sen@senhaerens.be>\r\nCloses #6444 \r\nCloses #6445"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jbedo": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/f269060a24d1f43a30d647e0835ca4fcaac6bae9", "message": "Fix autoconf detection of super_setup_bdi_name\n\nThe previous autoconf test for the presence of super_setup_bdi_name()\r\nuses an invocation with an incorrect type signature, producing a\r\nwarning by the compiler when the test is run. This gets elevated to an\r\nerror when compiling with -Werror=format-security, causing autoconf to\r\nfalsely infer super_setup_bdi_name() is not present. This updates the\r\ntesting code to match the invocation used in\r\ninclude/linux/vfs_compat.h.\r\n\r\nReviewed-by: loli10K <ezomori.nozomu@gmail.com>\r\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\r\nSigned-off-by: Justin Bedo <cu@cua0.org>\r\nCloses #6398"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sdimitro": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/829f9251cf03f1b385a46960539380dd70555270", "message": "OpenZFS 8491 - uberblock on-disk padding to reserve space for smoothly merging zpool checkpoint & MMP in ZFS\n\nThe zpool checkpoint feature in DxOS added a new field in the uberblock.\nThe Multi-Modifier Protection Pull Request from ZoL adds three new fields\nin the uberblock (Reference: https://github.com/zfsonlinux/zfs/pull/6279).\nAs these two changes come from two different sources and once upstreamed\nand deployed will introduce an incompatibility with each other we want\nto upstream a change that will reserve the padding for both of them so\nintegration goes smoothly and everyone gets both features.\n\nPorting Notes: Preserved MMP comments in uberblock struct.\n\nAuthored by: Serapheim Dimitropoulos <serapheim@delphix.com>\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\nReviewed by: Brian Behlendorf <behlendorf1@llnl.gov>\nReviewed by: Olaf Faaland <faaland1@llnl.gov>\nApproved by: Gordon Ross <gwr@nexenta.com>\nPorted-by: Giuseppe Di Natale <dinatale2@llnl.gov>\n\nOpenZFS-issue: https://www.illumos.org/issues/8491\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/d84fa5f\nCloses #6390"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bahamas10": {"issues": [], "commits": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/commits/12fa0466df8773fc6151e8b55367a1c4f76ec407", "message": "OpenZFS 6939 - add sysevents to zfs core for commands\n\nAuthored by: Dave Eddy <dave@daveeddy.com>\nReviewed by: Patrick Mooney <patrick.mooney@joyent.com>\nReviewed by: Joshua M. Clulow <jmc@joyent.com>\nReviewed by: Josh Wilsdon <jwilsdon@joyent.com>\nReviewed by: Matthew Ahrens <mahrens@delphix.com>\nReviewed by: George Wilson <george.wilson@delphix.com>\nReviewed by: Richard Elling <Richard.Elling@RichardElling.com>\nReviewed by: Alan Somers <asomers@gmail.com>\nReviewed by: Andrew Stormont <andyjstormont@gmail.com>\nApproved by: Matthew Ahrens <mahrens@delphix.com>\nReviewed-by: Brian Behlendorf <behlendorf1@llnl.gov>\nReviewed-by: George Melikov <mail@gmelikov.ru>\nPorted-by: Giuseppe Di Natale <dinatale2@llnl.gov>\n\nOpenZFS-issue: https://www.illumos.org/issues/6939\nOpenZFS-commit: https://github.com/openzfs/openzfs/commit/ce1577b\nCloses #6328"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "avw1987": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/7055", "title": "Update README.initramfs.markdown", "body": "Fixed a typo\r\n\r\n<!--- Provide a general summary of your changes in the Title above -->\r\n\r\n<!---\r\nDocumentation on ZFS Buildbot options can be found at\r\nhttps://github.com/zfsonlinux/zfs/wiki/Buildbot-Options\r\n-->\r\n\r\n### Description\r\n<!--- Describe your changes in detail -->\r\n\r\n### Motivation and Context\r\n<!--- Why is this change required? What problem does it solve? -->\r\n<!--- If it fixes an open issue, please link to the issue here. -->\r\n\r\n### How Has This Been Tested?\r\n<!--- Please describe in detail how you tested your changes. -->\r\n<!--- Include details of your testing environment, and the tests you ran to -->\r\n<!--- see how your change affects other areas of the code, etc. -->\r\n<!--- If your change is a performance enhancement, please provide benchmarks here. -->\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [x] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [ ] My code follows the ZFS on Linux code style requirements.\r\n- [ ] I have updated the documentation accordingly.\r\n- [ ] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [ ] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dong-liuliu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6546", "title": "Use Multi-buffer sha256 support from SPL", "body": "<!--- Provide a general summary of your changes in the Title above -->\r\n\r\n<!---\r\nDocumentation on ZFS Buildbot options can be found at\r\nhttps://github.com/zfsonlinux/zfs/wiki/Buildbot-Options\r\n-->\r\n\r\n### Description\r\n<!--- Describe your changes in detail -->\r\nLet sha256 checksum using multi-buffer api if it is exported by SPL\r\n### Motivation and Context\r\n<!--- Why is this change required? What problem does it solve? -->\r\n<!--- If it fixes an open issue, please link to the issue here. -->\r\n Using multi-buffer type, performance of sha256 will be increased 2~7 times.\r\nNow a patch for multi-buffer sha256 facility in kernel space is implemented and submitted to SPL.\r\nIts userspace facility and sha512 parts will be following up after this patch is reviewed and commented.\r\n\r\n### How Has This Been Tested?\r\n<!--- Please describe in detail how you tested your changes. -->\r\n<!--- Include details of your testing environment, and the tests you ran to -->\r\n<!--- see how your change affects other areas of the code, etc. -->\r\n<!--- If your change is a performance enhancement, please provide benchmarks here. -->\r\nRun FIO sequential write test, on Intel Xeon server (Haswell E5-2699 v3, 18 core), with 6x SSD :\r\n\r\nSha256 | CPU-sys% | BW(MB/s)\r\n-- | -- | --\r\nmulti-buffer version | 27 | 1859\r\nicp version | 71 | 1876\r\n\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [x] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [ ] I have updated the documentation accordingly.\r\n- [x] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [x] All commit messages are properly formatted and contain `Signed-off-by`.\r\n- [ ] Change has been approved by a ZFS on Linux member.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "inkdot7": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/6078", "title": "Metadata classes wip no accounting", "body": "Please ignore this PR.\r\nI just want to see how the metadata allocation classes behave if the special accounting is removed.  (Which would allow the small-block-size limit to be changed after creation more easily.)\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "n1kl": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/5929", "title": "Quality of service for ZFS + improvement through compression", "body": "<!--- Provide a general summary of your changes in the Title above -->\r\nAs part of my master thesis at University of Hamburg I have targeted to improve ZFS through compression. Now I would like to share my 3 feature branches with the community.\r\n\r\n1. lz4fast #5927 \r\n2. autocompression #5928 \r\n3. qos (current)\r\n### Description\r\n<!--- Describe your changes in detail -->\r\nThis patch adds quality of service to ZFS datasets.\r\nzfs set compression=qos-[10,20,30,40,50,+50*n,1000]\r\nThe chosen value sets the throughput in MB/s.\r\nLow values will result in better compression ratio but less throughput.\r\n\r\n### Motivation1\r\n<!--- Why is this change required? What problem does it solve? -->\r\n<!--- If it fixes an open issue, please link to the issue here. -->\r\nQuality of service is an important aspect in dealing with limited resources.\r\nAt the moment the user can control storage requirement by choosing a compression algorithm like gzip for high compression. Depending on the hardware and the current CPU load the performance might be either poor or well.\r\nBy using the qos compression feature the desired write throughput can be chosen to meet the requirement for the application.\r\nThe qos algorithm keeps track of the compression speed and chooses either lz4 or gzip-[1-9] to speed up / slow down while compressing data. \r\n\r\n<!--- ### How Has This Been Tested? -->\r\n<!--- Please describe in detail how you tested your changes. -->\r\n<!--- Include details of your testing environment, and the tests you ran to -->\r\n<!--- see how your change affects other areas of the code, etc. -->\r\n<!--- If your change is a performance enhancement, please provide benchmarks here. -->\r\n### Benchmark1\r\n\r\nCopy file from Tempfs to ZFS into 1 Dataset:\r\n\r\nName | MB/s\t| Ratio\r\n ---   \t|  --- \t| ---\r\ngzip9\t| 11\t| 0.37\r\nqos10\t| 10\t| 0.38\r\nqos20\t| 22\t| 0.41\r\nqos30\t| 35\t| 0.45\r\nqos40\t| 45\t| 0.48\r\nqos50\t| 55\t| 0.50\r\nlz4\t| 71\t| 0.57\r\noff\t| 62\t| 1\r\n\r\n\r\n### Motivation2\r\n\r\nTransactiongroups in ZFS cause simultaneous writes into multiple datasets to wait for each other to complete. The slowest dataset is the limitation to the overall performance.\r\nThe qos feature can prevent this through dataset prioritisation.\r\nThe maximum bandwidth is limited by the disk throughput. Every dataset can request a part of this bandwidth by setting the qos property value.\r\nData can now be organised into low priority datasets with low quality of service requirements (but high compression, see Motivation1) and high priority to which also all non qos datasets belong.\r\nAll inheriting datasets and their parent share the same requested bandwidth. If the value of an inheriting dataset (lower hierarchy) is explicitly changed from \"inherit\" to \"qos\" then this dataset will request its own bandwidth.\r\n\r\n\r\n### Benchmark2\r\n\r\nCopy 2 files from Tempfs to ZFS into 2 Datasets:\r\n\r\nName     \t\t\t|MB/s\t|MB/s\t|Ratio\t|Ratio\t| Comment\r\n--- | --- | --- | --- | --- | ---\r\nqos10/qos10 - qos10/qos10_2\t|5\t|5\t|0.47\t|0.49\t|use of inheritance\r\nqos10 - qos10/qos10\t\t|5\t|5\t|0.49\t|0.47\t|use of inheritance\r\nqos10 - qos10_2\t\t\t|11\t|10\t|0.46\t|0.48\t| \r\nqos10/qos10 - qos10/qos10x\t|11\t|9\t|0.48\t|0.46\t|qos-10 explicit <br>set on qos10x\r\nqos10/qos20 - qos10\t\t|20\t|9\t|0.49\t|0.43\t| \r\nqos30 - qos10\t\t\t|29\t|9\t|0.52\t|0.40\t| \r\nqos40 - qos10\t\t\t|44\t|10\t|0.54\t|0.40\t| \r\nqos50 - qos10\t\t\t|51\t|9\t|0.53\t|0.40\t| \r\nlz4 - qos10\t\t\t|71\t|9\t|0.57\t|0.39\t| lz4 has high priority\r\noff - qos10\t\t\t|62\t|8\t|1\t|0.38\t| \r\ngzip9 - qos10\t\t\t|13\t|5\t|0.37\t|0.39\t| \r\nlz4 - gzip9\t\t\t|10\t|10\t|0.57\t|0.37\t|  lz4 waiting for gzip\r\n\r\n\r\n### Benchmark3\r\n\r\nCopy 2 files from Tempfs to ZFS into 1 Datasets:\r\n\r\nName     \t\t\t|MB/s\t|MB/s\t|Ratio\r\n--- | --- | --- | --- \r\nqos10 - qos10 |\t5\t|5|\t0,38\r\noff - off|\t22|\t22|\t1\r\nlz4 - lz4|\t30|\t27|\t0,57\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [x] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n\r\n#### Branch overlapping changes (feature, compress values)\r\nThe patch has read-only backward compatibility by using the new introduced SPA_FEATURE_COMPRESS_QOS feature. The feature activation procedure is equivalent to my other code branches.\r\nRegarding the limited namespace of BP_GET_COMPRESS() (128 values), the\r\nzio_compress enum's first part is for block pointer & dataset values, the second part for dataset values only. Or should I make use of a new property? This is an alternative suggestion to #3908.\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [ ] I have updated the documentation accordingly.\r\n- [x] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/5928", "title": "auto compression", "body": "<!--- Provide a general summary of your changes in the Title above -->\r\nAs part of my master thesis at University of Hamburg I have targeted to improve ZFS through compression. Now I would like to share my 3 feature branches with the community.\r\n\r\n1. lz4fast #5927 \r\n2. autocompression (current)\r\n3. qos #5929 \r\n\r\n### Description\r\n<!--- Describe your changes in detail -->\r\nThis patch adds auto as ZFS compression type.\r\nzfs set compression=auto\r\n\r\n### Motivation and Context\r\n<!--- Why is this change required? What problem does it solve? -->\r\n<!--- If it fixes an open issue, please link to the issue here. -->\r\nWhich compression algorithm is best for high throughput? The answer to this depends on the type of hardware in use.\r\nIf compression takes long then the disk remains idle. If compression is faster than the writing speed of the disk then the CPU remains idle as compression and writing to the disk happens in parallel.\r\nAuto compression tries to keep both as busy as possible.\r\nThe disk load is observed through the vdev queue. If the queue is empty a fast compression algorithm like lz4 with low compression rates is used and if the queue is full then gzip-[1-9] can require more CPU time for higher compression rates.\r\nThe already existing zio_dva_throttle might conflict with the concept described above. Therefore it is recommended to deactivate zio_dva_throttle.\r\n\r\n<!--- ### How Has This Been Tested? -->\r\n<!--- Please describe in detail how you tested your changes. -->\r\n<!--- Include details of your testing environment, and the tests you ran to -->\r\n<!--- see how your change affects other areas of the code, etc. -->\r\n<!--- If your change is a performance enhancement, please provide benchmarks here. -->\r\n### Benchmark\r\n\r\nCopy file from Tempfs to ZFS\r\n\r\n\r\n8 Cores:\r\n\r\nName\t|Ratio\t|MB/s\r\n---\t|---\t|---\r\nauto\t|0.44  \t|245\r\ngzip-1\t|0.43  \t|255\r\nlz4\t|0.58  \t|195\r\noff\t|1 \t|99\r\n\r\n\r\n1 Core:\r\n\r\nName\t|Ratio\t|MB/s\r\n---\t|---\t|---\r\nauto\t|0.56 \t|151\r\ngzip-1\t|0.43\t|51\r\nlz4\t|0.58\t|179\r\noff\t|1\t|99\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [x] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n\r\n#### Branch overlapping changes (feature, compress values)\r\n\r\nThe patch is has read-only backward compatibility by using the new introduced SPA_FEATURE_COMPRESS_AUTO feature. The feature activation procedure is equivalent to my other code branches.\r\nRegarding the limited namespace of BP_GET_COMPRESS() (128 values), the\r\nzio_compress enum's first part is for block pointer & dataset values, the second part for dataset values only. This is an alternative suggestion to #3908.\r\n\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [ ] I have updated the documentation accordingly.\r\n- [x] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/5927", "title": "lz4fast compression", "body": "<!--- Provide a general summary of your changes in the Title above -->\r\nAs part of my master thesis at University of Hamburg I have targeted to improve ZFS through compression. Now I would like to share my 3 feature branches with the community.\r\n\r\n1. lz4fast (current)\r\n2. autocompression #5928 \r\n3. qos #5929 \r\n\r\nThis patch updates the lz4 *1 code to version 1.7.3 to make use of lz4 fast compression.\r\nThe lz4 code is based on a seperate project for updating lz4 inside the linux kernel.\r\nThere a few changes were made for an clean implementation and to improve speed that are currently in review *2.\r\n\r\n*1: [https://github.com/lz4/lz4](https://github.com/lz4/lz4)\r\n*2: [https://patchwork.kernel.org/patch/9574745/](https://patchwork.kernel.org/patch/9574745/)\r\n\r\n\r\n### Description\r\n<!--- Describe your changes in detail -->\r\nLZ4-fast capability is now available.\r\nzfs set compression=lz4fast-[1-20,30,+10*n,100]\r\nHigher values result in improved compression speed and less ratio.\r\n\r\n\r\n### Motivation and Context\r\n<!--- Why is this change required? What problem does it solve? -->\r\n<!--- If it fixes an open issue, please link to the issue here. -->\r\nLz4 fast trades in compression ratio for speed. This gives us more flexibility in environments with either low computational power or fast and many SSDs/HDDs where the lz4 is the limiting factor.\r\nAutocompression and qos can also be improved by adding lz4fast algorithms.\r\n\r\n### How Has This Been Tested?\r\n<!--- Please describe in detail how you tested your changes. -->\r\n<!--- Include details of your testing environment, and the tests you ran to -->\r\n<!--- see how your change affects other areas of the code, etc. -->\r\n<!--- If your change is a performance enhancement, please provide benchmarks here. -->\r\nChecksums were made to proof full compatibility between the old and new lz4 compressed files.\r\n\r\n#### Benchmark\r\n\r\nCopy file from Tempfs to ZFS (ZFS also in Tempfs for high disk throughput simulation).\r\n\r\n\r\nName         |Ratio   |MB/s\r\n---          |---     |---\r\nlz4          |0.58    |228\r\nlz4fast-2    |0.62    |249\r\nlz4fast-3    |0.65    |266\r\nlz4fast-4    |0.68    |282\r\nlz4fast-5    |0.71    |298\r\nlz4fast-7    |0.76    |329\r\nlz4fast-10   |0.80    |370\r\nlz4fast-20   |0.97    |469\r\nlz4fast-30   |0.98    |546\r\nlz4fast-50   |0.98    |634\r\nlz4fast-100  |0.99    |690\r\noff          |1       |744\r\n\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [x] Performance enhancement (non-breaking change which improves efficiency)\r\n- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n\r\n#### Branch overlapping changes (feature, compress values)\r\n\r\nThe patch has read-only backward compatibility by using the new SPA_FEATURE_LZ4FAST_COMPRESS feature. The feature activation procedure is equivalent to my other code branches.\r\nRegarding the limited namespace of BP_GET_COMPRESS() (128 values), the\r\nzio_compress enum's first part is for block pointer & dataset values, the second part for dataset values only. Or should I make use of a new property? This is an alternative suggestion to #3908.\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the ZFS on Linux code style requirements.\r\n- [ ] I have updated the documentation accordingly.\r\n- [x] I have read the **CONTRIBUTING** document.\r\n- [ ] I have added tests to cover my changes.\r\n- [ ] All new and existing tests passed.\r\n- [ ] Change has been approved by a ZFS on Linux member.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "theairkit": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/4684", "title": "Per-filesystem IO accounting implementation through netlink", "body": "This patchset introduces per filesystem zfs io accounting.\nThis feature is required in my test and production environments for monitoring io per filesystem.\n\nSome details:\n- accounting implementation based on sending netlink multicast messages from zfs kernel module (and may by received in userspace);\n- **on each io operation zfs kernel module sends netlink multicast message, which contains full information about it: filesystem, pid, bytes, operation (read, write, mapped read, mapped write);**\n- **sending netlink messages from kernel fully non-blocking;**\n- accounting implemented on zpl/zfs-layers;\n- accounting managed by kernel mode parameter:\n\n```\n/sys/module/zfs/parameters/zfs_nl_ioacct # 0 - disable, any other - enable\n```\n- this PR consists of:\n  - patchset for kernel module;\n  - simple userspace utility, zfs_iostat (see below);\n- **zfs_iostat** utulity allow to get io statistics in realtime to stdout using next format:\n  \n  ```\n  <filesystem> <pid> <bytes> <operation_type>\n  ```\n  \n  Where _operation_type may_ be:\n  \n  ```\n  cr - common read\n  cw - common write\n  mr - mapped read\n  mw - mapped write\n  ```\n  \n   Example output:\n  \n  ```\n  $ sudo ./cmd/zfs_iostat/zfs_iostat\n  zroot/instances/fs-one 31333 4096 cr\n  zroot/instances/fs-one 31333 4096 cr\n  ```\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/4175", "title": "IO throttling implementation", "body": "@behlendorf\nHi!\nI wrote patchset - hierarchical io throttling.\n\nThis feature is required in my test and production environments (see https://github.com/zfsonlinux/zfs/issues/1952). So, I tries to implement it.\n\nSome details:\n- Throttling based on semaphores; when one process, who wants read/write, lock semaphore, another process(-es) for the same filesystem waiting (in D-state). Between up and down of this semaphore, there are usleep_range() to set rate.\n- Current implementation also 'smoothes' io-load: if set, for example, 100 max read iops for some filesystem, no one can read more often, than one operation per 0.01s.\n- Throttling implemented on zpl/zfs-layers.\n- Throttling mode and rates are **managed by**:\n  - **enabling/disabling throttling through module parameter**:\n  \n  ```\n    /sys/module/zfs/parameters/zfs_io_throttle # 0 - disable, any other - enable\n  ```\n  - **zfs properties**:\n  \n  ```\n    zfs get all | grep '_ops'\n    zroot         max_read_ops          150                    local\n    zroot         max_write_ops         230                    local\n    zroot/images  max_read_ops          10                     local\n    zroot/images  max_write_ops         20                     local\n    ...\n    zfs set max_read_ops=89 zroot/images\n    zfs set max_write_ops=shared zroot/images\n    ...\n  ```\n- There are **four modes** for reads and writes:\n  - **nolimit**: nolimit;\n  - **shared**: share rate with parent filesystem (it can't be set for root 'pool' filesystem);\n  - **none**: disable io on filesystem\n  - **iops**: limit to iops\n- **Example for read ops (write ops managed in the same manner and fully separate):**\n  \n  | filesystem | throttle property value | actual |\n  | --- | --- | --- |\n  | zroot | 100 | (probably share*) 100 |\n  | zroot/blah | shared | share 100 with zroot |\n  | zroot/blah/bar | 200 | 200 |\n  | zroot/bar | 300 | (probably share*) 300 |\n  | zroot/bar/foo | shared | share 300 with zroot/bar |\n  | zroot/test | none | disable io |\n  | zroot/woof | nolimit | nolimit |\n  \n  _*probably share - if rate for some children filesystems set to shared_\n- Current implementation does not take into accounting into time of the operation, so actual rate may be less.\n- Current implementation does not throttle operations on snaphots\n- **Current implementation does not care about whether read from ARC or not.**\n- Regular read/write throttles correctly.\n- mmap()'ed read/write:\n  - reads: throttles correctly; throttle only read page(s) from disk (one page - one io operation);\n  - writes: due to mapped writes (to filesystem) performs by linux kernel thread kworker, one per filesystem (if I understand correctly), this case more complex, and may require tuning some sysctls for more granular work of kworker between pages of different files/processes:\n  \n  ```\n  vm.dirty_background_bytes\n  vm.dirty_background_ratio\n  vm.dirty_bytes\n  vm.dirty_expire_centisecs\n  vm.dirty_ratio\n  vm.dirty_writeback_centisecs\n  vm.dirtytime_expire_seconds\n  ```\n\nWhat did you think about this (at least as idea etc.)?..\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vozhyk-": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/3908", "title": "Add lz4hc compression type", "body": "Adds LZ4HC compression, enabled with a `compression=lz4hc | lz4hc-{1..16}` property.\n\nStill doesn't add any feature flags and doesn't update the man-pages, as it hasn't been decided on the way it should be done. Currently 2 possibilities have been suggested:\n- Adding a feature flag\n- Using `compression=lz4` and adding a dataset property to choose between `lz4`/`lz4hc` (`lz4mode`, `compressionlevel`, etc.)\n\nHow the code can be further cleaned up:\n- By replacing `lz4` code with the new upstream code, which would decrease the amount of duplicated code (`lz4.c` and `lz4hc.c` have many common bits, but `lz4.c` has them as macros while in `lz4hc.c` they are functions).\n- By unifying `lz4[hc]_compress_zfs` functions, which contain almost the same code.\n\nIssue #1900\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ghost": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/1276681", "body": "Has this issue been resolved? I'm having the same problem on OpenSolaris with ZFS. The zpool-rpool process is writing on average at 400MB/h on an idle system. I can't seem to find an answer anywhere on the net.\n\nThanks for your help.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/1276681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/1277385", "body": "A quick DTrace on what the zpool-rpool process is doing reveals the following kernel function calls, accompanied by the number of times they have been called (the sampling lasted about a minute). Does this mean anything to you?\n\n``` DTrace\nrootnex`rootnex_coredma_allochdl          1\nrootnex`rootnex_dma_allochdl            1\nscsi`scsi_transport                     1\nzfs`buf_hash                            1\nzfs`arc_write_done                      1\nzfs`dbuf_find                           1\nzfs`dbuf_dirty                          1\nzfs`metaslab_ndf_alloc                  1\nzfs`spa_writeable                       1\nzfs`space_map_load                      1\nzfs`vdev_queue_deadline_compare          1\nzfs`vdev_queue_offset_compare           1\nzfs`vdev_queue_io                       1\nzfs`zio_buf_free                        1\nzfs`zio_remove_child                    1\nzfs`zio_destroy                         1\nzfs`zio_vdev_io_done                    1\nzfs`zrl_add                             1\nahci`ahci_check_ctl_handle              1\nsata`sata_scsi_start                    1\nsata`sata_txlt_write                    1\nsd`sdstrategy                           1\nsd`sd_core_iostart                      1\nsd`sd_initpkt_for_buf                   1\nsd`sd_start_cmds                        1\nunix`sep_save                           1\nunix`splr                               1\nunix`tsc_gethrtime                      1\nunix`tsc_scalehrtime                    1\nunix`bcopy                              1\nunix`gdt_update_usegd                   1\nunix`lock_set                           1\nunix`cmt_balance                        1\nunix`swtch                              1\nunix`disp_ratify                        1\nunix`default_lock_backoff               1\nunix`lock_set_spin                      1\ngenunix`avl_walk                        1\ngenunix`avl_rotation                    1\ngenunix`cv_broadcast                    1\ngenunix`ddi_fm_acc_err_get              1\ngenunix`disp_lock_enter                 1\ngenunix`thread_lock                     1\ngenunix`ldi_strategy                    1\ngenunix`copy_pattern                    1\ngenunix`kmem_zalloc                     1\ngenunix`list_create                     1\ngenunix`list_remove                     1\ngenunix`ddi_get_soft_state              1\ngenunix`restorectx                      1\nzfs`buf_hash_insert                     2\nzfs`dnode_diduse_space                  2\nzfs`zio_push_transform                  2\nzfs`zio_walk_parents                    2\nzfs`zio_done                            2\nzfs`zio_checksum_compute                2\nzfs`vdev_disk_io_start                  2\nsha2`SHA256TransformBlocks              2\nsd`sd_mapblockaddr_iostart              2\nsd`sd_add_buf_to_waitq                  2\nsd`ddi_xbuf_qstrategy                   2\nsd`xbuf_iostart                         2\nunix`rw_enter                           2\nunix`disp                               2\nunix`atomic_add_64_nv                   2\ngenunix`avl_remove                      2\ngenunix`avl_numnodes                    2\ngenunix`lbolt_event_driven              2\ngenunix`ddi_fm_dma_err_get              2\ngenunix`kmem_cache_free                 2\ngenunix`memcpy                          2\ngenunix`cpu_update_pct                  2\ngenunix`ndi_fmc_insert                  2\ngenunix`taskq_thread_wait               2\nzfs`arc_write_ready                     3\nzfs`metaslab_alloc_dva                  3\nzfs`vdev_accessible                     3\nzfs`zio_wait_for_children               3\nzfs`zio_notify_parent                   3\nzfs`zio_vdev_io_start                   3\nsd`sd_setup_rw_pkt                      3\nunix`mutex_owner_running                3\nunix`rw_exit                            3\nunix`mutex_vector_enter                 3\nunix`vsnprintf                          3\ngenunix`avl_last                        3\nzfs`space_map_remove                    4\nzfs`zio_execute                         4\nsd`sdinfo                               4\nunix`mutex_exit                         4\nzfs`metaslab_group_alloc                5\nzfs`vdev_queue_io_to_issue              5\nunix`bzero                              5\nunix`disp_getwork                       5\ngenunix`avl_insert                      5\nunix`0xfffffffffb85                     6\nunix`tsc_read                           6\ngenunix`kmem_cache_alloc                6\nunix`do_splx                            7\nzfs`space_map_seg_compare               9\nzfs`metaslab_segsize_compare           10\ngenunix`avl_find                       10\nunix`default_lock_delay                11\nzfs`fletcher_4_native                  12\nunix`mutex_enter                       16\nunix`mutex_delay_default               54\nzfs`lzjb_compress                     151\n```\n\nWe can see that the most called function is by far lzjb_compress. Again, DTrace reveals that all the kernel stacks that lead to lzjb_compress pass through the function zio_write_bp_init, which I assume is the guilty function behind all these writes...\n\nDoes this all mean anything to you?\n\nEdit:  kernel stacks\n\n``` DTrace\n              zfs`zio_compress_data+0x8e\n              zfs`zio_write_bp_init+0x216\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              genunix`taskq_thread+0x248\n              unix`thread_start+0x8\n                1\n\n              zfs`zio_compress_data+0x8e\n              zfs`zio_write_bp_init+0x216\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              genunix`taskq_thread+0x248\n              unix`thread_start+0x8\n                1\n\n              zfs`zio_compress_data+0x8e\n              zfs`zio_write_bp_init+0x216\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n                2\n\n              zfs`zio_compress_data+0x8e\n              zfs`zio_write_bp_init+0x216\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              genunix`taskq_thread+0x248\n              unix`thread_start+0x8\n                5\n\n              zfs`zio_compress_data+0x8e\n              zfs`zio_write_bp_init+0x216\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              genunix`taskq_thread+0x248\n              unix`thread_start+0x8\n                6\n\n              zfs`zio_compress_data+0x8e\n              zfs`zio_write_bp_init+0x216\n              zfs`zio_execute+0x8d\n              zfs`zio_notify_parent+0xa6\n              zfs`zio_ready+0x18b\n              zfs`zio_execute+0x8d\n              genunix`taskq_thread+0x248\n              unix`thread_start+0x8\n               13\n\n              zfs`zio_compress_data+0x8e\n              zfs`zio_write_bp_init+0x216\n              zfs`zio_execute+0x8d\n              genunix`taskq_thread+0x248\n              unix`thread_start+0x8\n               31\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/1277385/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/1281342", "body": "Here is a list processes which called the write system call on my system, accompanied by the filenames and the total number of bytes written to the files (sampled during one minute):\n\n```\ndtrace -n 'syscall::*write:entry {@[execname, fds[arg0].fi_pathname] = sum (arg2);}'\ndtrace: description 'syscall::*write:entry ' matched 2 probes\n^C\n\n  dtrace                                              /dev/pts/1                                                        1\n  sshd                                                /devices/pseudo/clone@0:ptm                                       1\n  sshd                                                <unknown>                                                        52\n  rsfcli                                              <unknown>                                                       105\n  basename                                            <unknown>                                                       164\n  hostname                                            <unknown>                                                       304\n  syslogd                                             /devices/pseudo/sysmsg@0:sysmsg                                 320\n  awk                                                 <unknown>                                                       331\n  zfs                                                 /devices/pseudo/mm@0:null                                       370\n  java                                                /var/opt/nest/config/site/scheduledjobs/emailnotifications/Notification_ERROR.xml              433\n  java                                                /var/opt/nest/config/site/scheduledjobs/emailnotifications/Notification_WARNING.xml              437\n  java                                                /var/opt/nest/config/site/scheduledjobs/configurationreplications/SiteConfigReplication.xml              511\n  grep                                                <unknown>                                                       725\n  cron                                                /var/cron/log                                                   976\n  java                                                /var/opt/nest/config/site/scheduledjobs/snapshots/Snapshot2290924711711069275.xml             1014\n  java                                                /var/opt/nest/config/site/scheduledjobs/snapshots/Snapshot12337134254217831034.xml             1020\n  java                                                /var/opt/nest/config/site/scheduledjobs/snapshots/Snapshot17476938304947820872.xml             1020\n  java                                                /var/opt/nest/config/site/scheduledjobs/asyncreplications/AsyncReplication13100962750907134551.xml             1056\n  java                                                /var/opt/nest/config/site/scheduledjobs/asyncreplications/AsyncReplication13881922923541859328.xml             1056\n  ksh                                                 <unknown>                                                      1096\n  fcinfo                                              <unknown>                                                      1152\n  zpool                                               <unknown>                                                      1452\n  ksh                                                 /tmp/sf0l.2ol                                                  1650\n  ksh                                                 /tmp/sf0p.gnd                                                  1650\n  ksh                                                 /tmp/sf10.jmu                                                  1650\n  ksh                                                 /tmp/sf1g.3nv                                                  1650\n  ksh                                                 /tmp/sf1i.jvb                                                  1650\n  ksh                                                 /tmp/sf24.5eq                                                  1650\n  ksh                                                 /tmp/sf2f.jop                                                  1650\n  ksh                                                 /tmp/sf3b.beh                                                  1650\n  ksh                                                 /tmp/sf10.ujs                                                  3000\n  ksh                                                 /tmp/sf19.9c4                                                  3000\n  ksh                                                 /tmp/sf2a.o3i                                                  3000\n  ksh                                                 /tmp/sf2p.8d0                                                  3000\n  ksh                                                 /tmp/sf3k.j08                                                  3000\n  svcprop                                             <unknown>                                                      3140\n  format                                              <unknown>                                                      3644\n  sed                                                 <unknown>                                                      3704\n  fmd                                                 /var/fm/fmd/infolog_hival                                      4480\n  nscd                                                <unknown>                                                      5268\n  zfs                                                 <unknown>                                                      5778\n  init                                                /etc/svc/volatile/init-next.state                              9064\n  iostat                                              <unknown>                                                      9102\n  svccfg                                              <unknown>                                                      9801\n  fmtopo                                              <unknown>                                                    429332\n```\n\nIn contrast, for the same duration, here is the list of actual disk writes that were initiated, again accompanied by the filenames and number of bytes.\n\n```\ndtrace -n 'io:::start /args[0]->b_flags & B_WRITE/ {@[execname, args[2]->fi_pathname]=sum(args[0]->b_bcount);}'\ndtrace: description 'io:::start ' matched 6 probes\n^C\n\n  sched                                               /var/opt/nest/config/site/scheduledjobs/configurationreplications/SiteConfigReplication.xml             4096\n  sched                                               /var/opt/nest/config/site/scheduledjobs/emailnotifications/Notification_ERROR.xml             4096\n  sched                                               /var/opt/nest/config/site/scheduledjobs/emailnotifications/Notification_WARNING.xml             4096\n  sched                                               /var/opt/nest/config/site/scheduledjobs/asyncreplications/AsyncReplication13100962750907134551.xml             8192\n  sched                                               /var/opt/nest/config/site/scheduledjobs/asyncreplications/AsyncReplication13881922923541859328.xml             8192\n  sched                                               /var/opt/nest/config/site/scheduledjobs/snapshots/Snapshot12337134254217831034.xml             8192\n  sched                                               /var/opt/nest/config/site/scheduledjobs/snapshots/Snapshot17476938304947820872.xml             8192\n  sched                                               /var/opt/nest/config/site/scheduledjobs/snapshots/Snapshot2290924711711069275.xml             8192\n  zpool-rpool                                         <none>                                                     10463232\n```\n\nThe total number of bytes written to the disk by zpool-rpool alone is much higher than the total of bytes for which the write system call was used. Doesn't that mean that zpool-rpool is acting on its own?\n\nEdit: The two dtrace commands were run in parallel.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/1281342/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/1283961", "body": "Hmmm it's too bad there isn't a moderately easy way of knowing where all this I/O activity comes from. I tried disabling fmtopo (which is the biggest write-system-call writer) and still, zpool-rpool's io activity didn't seem to lower as significantly as it should have.\n\nAnyway, thanks for your input. I'll post if I find a solution to this on my side.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/1283961/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/7731553", "body": "hmm. why not to do it in that way: let O_DIRECT always return true? does it metter that ZFS copies everything in to the ARC cache? let fake a bit an OS. It shouldn't hurt so much.... oh, and that is just my freak idea\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/7731553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/1873708", "body": "Hi - Not sure what's going on here as I don't know much regarding the programming/debugging of zfs but I seems to experience that issue, that is with or without rsync. I never had to read much files on my system as I use zfs for backups storage, however I just had to restore things from the zfs pool and it crashed after a while.. rebooted.. crashed after a while..\n\nHere is the error I found in dmesg/syslog : http://pastebin.com/jMTCNEFy\n\nIf there is anything I can do to help, as far as testings, don't hesitate to let me know.\n\nThanks,\n\nedit: using git from 2011-08-22 on debian squeeze\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/1873708/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/2064709", "body": "Alphalead : I think your trick allowed my rsync session to last longer but after a while it crashed again unfortunately\n\nMessage from syslogd@stor01 at Sep 11 12:18:11 ...\n kernel:[1714241.981661] Oops: 0002 [#1] SMP\n\nMessage from syslogd@stor01 at Sep 11 12:18:11 ...\n kernel:[1714241.981673] last sysfs file: /sys/module/mbcache/initstate\n\nMessage from syslogd@stor01 at Sep 11 12:18:11 ...\n kernel:[1714241.982056] Stack:\n\nMessage from syslogd@stor01 at Sep 11 12:18:11 ...\n kernel:[1714241.982133] Call Trace:\n\nMessage from syslogd@stor01 at Sep 11 12:18:11 ...\n kernel:[1714241.982206] Code: 49 8b 14 04 48 c1 e2 03 e8 83 88 ff ff 85 c0 75 10 48 8d 54 24 70 48 89 de 44 89 ef e8 5b f3 ff ff 48 8b 54 24 50 be d0 00 00 00 <48> c7 02 00 00 00 00 48 8b 54 24 48 48 8b 7c 24 70 e8 7d f6 ff\n\nMessage from syslogd@stor01 at Sep 11 12:18:11 ...\n kernel:[1714241.982346] CR2: 0000000000000000\n\nedit: version used is latest commit (2708f716c0)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/2064709/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/2006354", "body": "I can confirm that his bug does **not exist** in zfs-fuse for linux. \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/2006354/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/2352510", "body": "Gunnar Beutner was able to come up with a patch for this. I tested it on my development device and so far it works exactly as intended. We're doing some more testing later this week; at this point I would consider this ready for official evaluation so that it can be committed and this bug closed. I will post back here if we encounter any problems while we are testing this patch.\n\nhttps://gunnar-beutner.de/files/0001-Fixed-invalid-resource-re-use-in-file_find.patch\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/2352510/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/2811567", "body": "zfs-fuse:\n\n<pre>\ndd if=/dev/zero of=/tank/xxx/test.io bs=1024k count=1000\n1000+0 records in\n1000+0 records out\n1048576000 bytes (1.0 GB) copied, 8.48609 s, 124 MB/s\n</pre>\n\nubuntu-zfs:\n\n<pre>\ndd if=/dev/zero of=/tank/xxx/test.io bs=1024k count=1000\n1000+0 records in\n1000+0 records out\n1048576000 bytes (1.0 GB) copied, 114.533 s, 9.2 MB/s\n</pre>\n\nOk, i try recreate pool under ubuntu-zfs\n\n<pre>\n# zpool offline tank sdc\n# zpool detach tank sdc\n# zpool create -f test sdc\n# zpool status test\n  pool: test\n state: ONLINE\n scan: none requested\nconfig:\n\n        NAME        STATE     READ WRITE CKSUM\n        test        ONLINE       0     0     0\n          sdc       ONLINE       0     0     0\n\nerrors: No known data errors\n# zfs create test/xxx\n# dd if=/dev/zero of=/test/xxx/test.io bs=1024k count=1000\n1000+0 records in\n1000+0 records out\n1048576000 bytes (1.0 GB) copied, 53.5897 s, 19.6 MB/s\n</pre>\n\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/2811567/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/3299069", "body": "behlendorf, probably, i had bad results because i was try to use 32 bit OS.\nFresh install ferdora 16 32 bit was the same, but zfs on fedora 16 (x64) shows performance near to raw device.\nThanks.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/3299069/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/11986018", "body": "Please fix this, a year later it is still not working. Rudd-O has an open pull request, can it be pulled into te main branche?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/11986018/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/3399837", "body": "Pretty much this is how I went about setting up everything.\n\nMy platform is Ubuntu 10.04.3 x86 (also used x64) running as a VMware VM on my laptop (as test).\n\nI downloaded the source and compiled as per the instructions on the ZFS on linux website.\n\nThen I compiled the Linux iSCSI target core backports from linux-iscsi.org. I also compiled the lio-utils and targetcli (the management tools).\n\nAfter I installed the deb packages of everything (iscsi target and ZFS) I created my zpool called (tank) and my zfs vol (fish). Because it was a test I just used the names from the website because it did not matter.\n\nThe command I use were the following;\n\nparted /dev/sdb\nmklabel gpt\nquit\n\nparted /dev/sdc\nmklabel gpt\nquit\n\nzpool create tank mirror /dev/sdb /dev/sdc\n\nzfs create tank/fish -V 18G\n\nAfter that was done I dropped into the targetcli tool and tried to add a block device to the /backstores/iblock section. The targetcli emulates a file system, kind of reminds me of /proc or /sys. When I execute the command \"create disk0 /dev/zd0\" it returns and error to me saying the chosen device is not a valid \"TYPE_DISK\". I am not sure though if \"TYPE_DISK\" is something internal to the target or if it is a Linux thing.\n\nThe only way I could use ZFS with the target was to format the ZFS vol with something like ext4 and then create an image file with dd then use the file_io feature of the target. But the is not only complicated but completely undermines the entire point of using ZFS.\n\nWhen I mentioned that the ZFS vols have no vendor information I was referring to what I see when I run \"parted -l\" and look at /dev/zd0. When compared to the VMware disks there is information about who made the disk or anything, not even faked information just to fill the space. I will add an output when I have a chance.\n\nIf you need anymore info let me know.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/3399837/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/3442037", "body": "I get the code from there git repo. git://risingtidesystems.com/\n\nThe only slightly annoying thing is you have to build several packages before you can build the targetcli tool.\n\nBut everything you need is there.\n\nYou need to build the tools in an similar order to this;\n1. lio-utils\n2. configshell\n3. rtslib\n4. targetcli\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/3442037/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/3707762", "body": "I have small update to announce based on my reported issue, I may have a source of the problem.\n\nI believe the error \"Not TYPE_DISK\" is a problem with the iSCSI target drivers and may have nothing to do with ZFS.\n\nThe reason for the error I hypothoize is because ZFS is not listing it ZVOLs in /dev/disk which is most likely where the iSCSI target is look for them and that would sort of explain the error, because it is say that the ZVOL is not a type of device found is /dev/disk. \n\nSo unless something changes with the iSCSI target drivers before the \"final\" release with kernel v3.4 then ZFS just may have sit out on that one.\n\nI will \"try\" to report the issue to the devs of the iSCSI target but I have not heard anything since before Christmas when I last attempted.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/3707762/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/5439790", "body": "Experiencing the same issues with 2.6.32-41 on 10.04 (AMD X2-555 proc in an ASUS M4A88T MB, 16GB ecc).  No apparent problems with 2.6.32-40.  Sorry for lack of trace info, may have time this weekend.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/5439790/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/5441206", "body": "Correction, 12GB.  Might as well mention this:\nJust did a quick check and BIOS version was latest but release date appeared inconsistent.  So updated BIOS anyway, disabled legacy USB, and booted 2x4GB with just channel A (matched pair).  Checked dmesg and errata message is still there.   There's a sleeping zfs mount -a process (configured automount) and any zpool/zfs commands in a shell hang.  FWIW.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/5441206/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/7986088", "body": "I've installed Fedora 17 to a test System with ZFS due to @Rudd-O  \n+1 to this\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/7986088/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/20962527", "body": "Allow me to shed some light on this.\n\nLet's consider an old-school nfs4 export using a native Linux filesystem, one share called 'pmr':\n\n``` /etc/fstab\n/dev/groups/pmr /storage/pmr      xfs    inode64,logdev=/dev/ssdcache/pmr,logbufs=8  1 2\n/storage/pmr    /exports/pmr      none   rw,bind         0 0\n```\n\n``` /etc/exports\n/exports     [nfs4 export root settings]\n/exports/pmr [per-share settings]\n```\n\nWhen the system is booting, the xfs filesystem will be mounted first, followed by a bind mount from /storage/pmr to /exports/pmr. The latter then is exported via /etc/exports using nfs4 and we're all happy.\n\nNow consider a zfs-based scenario.\n\nSince there are no zfs entries in fstab, it becomes:\n\n``` /etc/fstab\n/storage/pmr    /exports/pmr      none   rw,bind         0 0\n```\n\nWhen the system boots, a bind-type mount will be created from /storage/pmr to /exports/pmr which is effectively mounting the underlying filesystem (most likely / ) to the bind point and exporting that. The clients will see the contents of an empty directory as the exporter uses the / bind mount. On the server, the confused administrator will see the actual zfs and will scratch their head.\n\nI don't think this is a bug in zfs rather a race condition between the distribution's native localfs init script and zfs. Perhaps localfs should depend on zfs and not the other way around.\n\nAlternatively, the zfs service should parse some file that will tell it how the binds go and bind after mounting the zfs filesystem. Perhaps a file in /etc/zfs/ like 'binds' would work.\n\nPersonally (sysadmin cap on) /etc/zfs/binds would work for me (together with a bit of parsing in /etc/init.d/zfs) as it's sufficiently low-tech and doesn't require changes in the actual zfs stack.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/20962527/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/20965023", "body": "Proposed patch (only lsb script, others are most likely derivative):\n\n``` patch\n--- etc/init.d/zfs.lsb.in.orig  2013-07-15 12:47:20.055257882 +0100\n+++ etc/init.d/zfs.lsb.in       2013-07-15 12:49:44.732137370 +0100\n@@ -29,6 +29,7 @@\n ZFS=\"@sbindir@/zfs\"\n ZPOOL=\"@sbindir@/zpool\"\n ZPOOL_CACHE=\"@sysconfdir@/zfs/zpool.cache\"\n+ZFS_NFS4_BINDS=\"@sysconfdir@/zfs/binds\"\n\n # Source zfs configuration.\n [ -r '/etc/default/zfs' ] &&  . /etc/default/zfs\n@@ -78,6 +79,26 @@\n                log_end_msg $?\n        fi\n\n+        # Create (optional) binds to the NFS4 export tree\n+        if [ -e \"$ZFS_NFS4_BINDS\" ] ; then\n+                log_begin_msg \"Binding NFS4 mounts\"\n+                sed -e \"s/#.*//\" -e \"/^$/d\" $ZFS_NFS4_BINDS | while read LINE\n+                do\n+                        MODE=\"`echo $LINE | awk '{print $1}'`\"\n+                        SRC=\"`echo $LINE | awk '{print $2}'`\"\n+                        DEST=\"`echo $LINE | awk '{print $3}'`\"\n+                        case $MODE in\n+                                bind)   MOUNTPOINT=\"`zfs get mountpoint $SRC | grep \"$SRC\" | awk '{print $3}'`\"\n+                                        mount -o $MODE $MOUNTPOINT $DEST\n+                                        log_end_msg $?\n+                                        ;;\n+                                *)      echo \"Unknown bind mode ($MODE) in $ZFS_NFS4_BINDS. Aborting.\"\n+                                        exit 4\n+                                        ;;\n+                        esac\n+                done\n+        fi\n+\n        touch \"$LOCKFILE\"\n }\n\n@@ -85,6 +106,25 @@\n {\n        [ ! -f \"$LOCKFILE\" ] && return 3\n\n+       if [ -e \"$ZFS_NFS4_BINDS\" ] ; then\n+                log_begin_msg \"Detaching NFS4 binds\"\n+                sed -e \"s/#.*//\" -e \"/^$/d\" $ZFS_NFS4_BINDS | while read LINE\n+                do\n+                        MODE=\"`echo $LINE | awk '{print $1}'`\"\n+                        SRC=\"`echo $LINE | awk '{print $2}'`\"\n+                        DEST=\"`echo $LINE | awk '{print $3}'`\"\n+                        case $MODE in\n+                                bind)   MOUNTPOINT=\"`zfs get mountpoint $SRC | grep \"$SRC\" | awk '{print $3}'`\"\n+                                        umount $DEST\n+                                        log_end_msg $?\n+                                        ;;\n+                                *)      echo \"Unknown bind mode ($MODE) in $ZFS_NFS4_BINDS. Aborting.\"\n+                                        exit 4\n+                                        ;;\n+                        esac\n+                done\n+        fi\n+\n        log_begin_msg \"Unmounting ZFS filesystems\"\n        \"$ZFS\" umount -a\n        log_end_msg $?\n```\n\n$MODE may look redundant but perhaps could be kept for future expansion, maybe there could be other bind types.\n\nThe /etc/zfs/binds file would look like this:\n\n``` /etc/zfs/binds\n#    zpool[/dataset]        mountpoint\nbind storage/pmr            /exports/pmr\n```\n\nOf course the distribution source would only contain the first line. I believe this is consistent with other files in /etc/zfs.\n\nCheers,\ngrok\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/20965023/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45432317", "body": "@FransUrbo `/etc/rc.local` does not exist and is not called in all distributions. Even more, `systemd` based distributions (good luck finding one without it these days) won't have it by definition.\n\nAre you suggesting that instead of editing a config file (present, documented) you would rather ask everyone to roll their own code, manually create bind mounts? That doesn't sound like a sane systems management practice.\n\nWhen ZoL filesystem needs to be exported over NFS4, a bind mount must be created. No standard mechanism in GNU/Linux will allow for it if the filesystem is not present in `/etc/fstab`. Since it's ZFS that's 'special', I will argue that it is its own responsibility to provide the functionality required for other parts of the system to continue to function.\n\nIf you don't like my solution, that's fine, please provide a better one or show where exactly am I incorrect. Saying something is 'hackish' and then suggesting that sysadmins 'sort it out in rc.local' isn't constructive.\n\nRegards,\njz\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45432317/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45433327", "body": "@FransUrbo \n\nWhat use is a filesystem that cannot be exported over network?\n\nNFS4 exports are different from NFS3 exports. There is a certain, established standard of creating them in GNU/Linux, there exists a well documented process that is different from Solaris-isms still present in ZoL.\n\nI wasn't aware `zfs set sharenfs=on` is able to produce NFS4 mounts. Could you please quote options required to make that happen? How do you define the mount tree? This is different from NFS3.\n\nWhat bugs in other software are you referring to? Exporting NFS4 works perfectly fine in GNU/Linux. Since ZoL provides PV, VG and LV management as well as filesystem mount points in a way that is abstracted from the current device paradigm on Linux, certain steps need to be taken to make those two work together.\n\nWhile you are free to disagree, I still haven't seen a patch that solves the problem. GNU/Linux nfs-kernel-server (and this is ZFS on _Linux_) requires mount points bound into a central exports tree. Since binding is done early (and you can't make the `zfs` init script depend on `$localfs`) ZoL needs to catch up. \n\nNFS4 provides capabilities like idmapd (how would you propose to integrate `zfs set sharenfs` with starting `idmapd`, are there hooks for that? How do I call them?), caching, subtree checks, consistent filesystem IDs and performance improvements over NFS3.\n\nThe logical way to do it (and I have consulted this with a number of Linux Sysadmins before presenting it here) is for the init script to have a mechanism to create the required bound mounts to the exports tree. The section in the init script is self-contained, fails safe (no action if the config file isn't present) and does introduce required compatibility with the host operating system. In one file that is owned by the ZFS package.\n\nIf you continue to disagree, please produce a patch that solves the issue for NFS4 and ZoL or provide a way of exporting NFS4, including all the required export options like the following excerpt from a production environment:\n\n``` /etc/exports\n/exports     172.5.125.0/24(ro,async,wdelay,insecure,root_squash,no_subtree_check,fsid=0)\n/exports     172.5.124.0/25(ro,async,wdelay,insecure,root_squash,no_subtree_check,fsid=0)\n/exports/pmr 172.5.125.0/24(rw,async,wdelay,root_squash,no_subtree_check)\n/exports/pmr 172.5.124.0/25(rw,sync,wdelay,no_root_squash,no_subtree_check)\n```\n\nPlease understand, `rc.local` is the last resort, it isn't available on all distributions, some don't even have an equivalent script and requiring systems administrators to manually do those steps is error-prone. Perhaps one can do it on their home computer but hardly in an enterprise environment where consistency and sustainability is key.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45433327/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45434151", "body": "@FransUrbo \n\nThe issue #1029 you referred me to is highlighting the problem I've solved; no way to correctly set up NFS4 shares using Solaris-isms under Linux.\n\n> > Could you please quote options required to make that happen?\n> \n> I did. You need to slow down and read what's given to you.\n\nUnless you meant the four dots at the end of `zfs set sharenfs=on`, I must have missed it.\n\nI'm not going to continue this conversation with you as it's no longer productive.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45434151/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/73043264", "body": ":+1: \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/73043264/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/10101505", "body": "oh, how embarrassing.. adding autogen.sh to my weekly routine. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/10101505/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/13097318", "body": "Hey, so while apt-get update automatically chooses 3.6.0-23-virtual for the chroot , I should rather install 3.6.0-29-generic which is the same as the hosts?  Gotcha.\nJust worth noting that i've followed the HOW TO step-by-step and that a virtual kernel (different from the hosts) gets installed by default when installing ubuntu-minimal in a chroot environment.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/13097318/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/13114557", "body": "Thanks for the replies, No objections behlendorf. \nCheers\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/13114557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/17124603", "body": "http://zfsonlinux.org/faq.html#WhyShouldIUseA64BitSystem\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/17124603/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/39197997", "body": "Some testing on SLES11 SP3 (obs build instance):\n\nThe `path_lookup()` is also triggered on SLES' 3.0.101 kernel so it looks like a proper autoconf check is required.\n\n@Milan-Benes:\n\nSince `spl_kern_path_parent` macro can expand to either `path_lookup(path, LOOKUP_PARENT, nd)`, `kern_path_parent_fn(path, nd)` or `kern_path_parent(path, nd)`, a _quick and very, very dirty_ fix would be to manually patch and build if you're desperate for the functionality. \n\nNote that the `kern_` functions use 2 arguments and not 3 so (I'm going to hell for this!) the middle one needs to go.\n\nSo after applying https://github.com/zfsonlinux/zfs/pull/1655 to 0.6.2 you can try something like this:\n\n``` patch\n--- module/zfs/zfs_ctldir.c.orig        2014-04-01 12:48:37.756605773 +0100\n+++ module/zfs/zfs_ctldir.c     2014-04-01 12:50:58.674195921 +0100\n@@ -997,8 +997,8 @@\n                goto out_path_buff;\n        }\n\n-       error = path_lookup(path_buff, LOOKUP_FOLLOW | LOOKUP_DIRECTORY, &nd);\n-       if (!error)\n+       error = kern_path_parent(path_buff, &nd);\n+       if (!error)\n                path_put(&nd.path);\n\n out_path_buff:\n```\n\nIt builds but **be warned**, may eat your gerbil.\n\n**Edit:** \n\n```\nZFS: snapshot home/tank@auto_daily-2014-03-27-1600 auto mounted at /home/tank/.zfs/snapshot/auto_daily-2014-03-27-1600 unexpectedly unmounted\n```\n\nAnd a nice NULL pointer:\n\n```\nApr  1 13:32:24 hematus kernel: [   83.305579] ZFS: snapshot home/tank@auto_daily-2014-03-27-1600 auto mounted at /home/tank/.zfs/snapshot/auto_daily-2014-03-27-1600 unexpectedly unmounted\nApr  1 13:35:00 hematus kernel: [  239.301971] BUG: unable to handle kernel NULL pointer dereference at 0000000000000020\nApr  1 13:35:00 hematus kernel: [  239.301978] IP: [<ffffffffa065e3a6>] zfsctl_lookup_snapshot_path+0x1a6/0x2c0 [zfs]\nApr  1 13:35:00 hematus kernel: [  239.302022] PGD 0\nApr  1 13:35:00 hematus kernel: [  239.302024] Oops: 0000 [#1] SMP\nApr  1 13:35:00 hematus kernel: [  239.302027] CPU 10\nApr  1 13:35:00 hematus kernel: [  239.302028] Modules linked in: md5 nfsd autofs4 binfmt_misc edd nfs lockd fscache auth_rpcgss nfs_acl sunrpc mpt3sas mpt2sas scsi_transport_sas raid_class mptctl mptbase bonding mperf microcode ext3 jbd mbcache loop flashcache(FN) pciehp zfs(PFN) zcommon(PFN) znvpair(PFN) zavl(PFN) zunicode(PFN) spl(FN) ipv6 ipv6_lib zlib_deflate ixgbe joydev usbhid hid igb usb_storage dca ptp dcdbas(X) pcspkr shpchp pci_hotplug sr_mod mei ses iTCO_wdt cdrom enclosure iTCO_vendor_support button wmi acpi_power_meter rtc_cmos pps_core acpi_pad sg mdio xfs dm_mirror dm_region_hash dm_log linear ehci_hcd usbcore usb_common sd_mod crc_t10dif processor thermal_sys hwmon scsi_dh_rdac scsi_dh_hp_sw scsi_dh_emc scsi_dh_alua scsi_dh dm_snapshot dm_mod ahci libahci libata megaraid_sas scsi_mod\nApr  1 13:35:00 hematus kernel: [  239.302072] Supported: No, Proprietary and Unsupported modules are loaded\nApr  1 13:35:00 hematus kernel: [  239.302074]\nApr  1 13:35:00 hematus kernel: [  239.302076] Pid: 7433, comm: nfsd Tainted: PF          NX 3.0.101-0.15-default #1 Dell Inc. PowerEdge R720/0X3D66\nApr  1 13:35:00 hematus kernel: [  239.302080] RIP: 0010:[<ffffffffa065e3a6>]  [<ffffffffa065e3a6>] zfsctl_lookup_snapshot_path+0x1a6/0x2c0 [zfs]\nApr  1 13:35:00 hematus kernel: [  239.302099] RSP: 0018:ffff8817dbee99e0  EFLAGS: 00010246\nApr  1 13:35:00 hematus kernel: [  239.302100] RAX: 0000000000000000 RBX: ffff8817f116c000 RCX: 0000000000000020\nApr  1 13:35:00 hematus kernel: [  239.302102] RDX: ffff8817f116e4c8 RSI: 0000000000001000 RDI: ffff8817dbee9ab0\nApr  1 13:35:00 hematus kernel: [  239.302104] RBP: 0000000000000da2 R08: e848000000000000 R09: 1200000000000000\nApr  1 13:35:00 hematus kernel: [  239.302106] R10: 0000000000000000 R11: ffffffff8120f630 R12: ffff8817dbee9b60\nApr  1 13:35:00 hematus kernel: [  239.302108] R13: ffff8817f116e4c8 R14: ffff8817f116c000 R15: 0000000000000006\nApr  1 13:35:00 hematus kernel: [  239.302110] FS:  0000000000000000(0000) GS:ffff88187faa0000(0000) knlGS:0000000000000000\nApr  1 13:35:00 hematus kernel: [  239.302112] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b\nApr  1 13:35:00 hematus kernel: [  239.302114] CR2: 0000000000000020 CR3: 0000000001a09000 CR4: 00000000001407e0\nApr  1 13:35:00 hematus kernel: [  239.302116] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nApr  1 13:35:00 hematus kernel: [  239.302118] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400\nApr  1 13:35:00 hematus kernel: [  239.302120] Process nfsd (pid: 7433, threadinfo ffff8817dbee8000, task ffff8817dbee6380)\nApr  1 13:35:00 hematus kernel: [  239.302122] Stack:\nApr  1 13:35:00 hematus kernel: [  239.302123]  0000000000011800 ffff8817dbee9c44 ffff88187f429a00 0000000000000da2\nApr  1 13:35:00 hematus kernel: [  239.302129]  ffff8817dbee9bd8 0000000000000004 0000000000000004 00000000000000a8\nApr  1 13:35:00 hematus kernel: [  239.302133]  00000000000000a8 ffffffff81145a8e ffff8817f420d400 ffff8817f1656800\nApr  1 13:35:00 hematus kernel: [  239.302137] Call Trace:\nApr  1 13:35:00 hematus kernel: [  239.302221]  [<ffffffffa065e52b>] zfsctl_lookup_objset+0x6b/0x90 [zfs]\nApr  1 13:35:00 hematus kernel: [  239.302284]  [<ffffffffa0672241>] zfs_vget+0xf1/0x350 [zfs]\nApr  1 13:35:00 hematus kernel: [  239.302356]  [<ffffffffa068edf1>] zpl_fh_to_dentry+0x41/0x60 [zfs]\nApr  1 13:35:00 hematus kernel: [  239.302420]  [<ffffffff811d69df>] exportfs_decode_fh+0x6f/0x290\nApr  1 13:35:00 hematus kernel: [  239.302429]  [<ffffffffa086198d>] nfsd_set_fh_dentry+0x17d/0x380 [nfsd]\nApr  1 13:35:00 hematus kernel: [  239.302440]  [<ffffffffa0861d6b>] fh_verify+0x1db/0x2b0 [nfsd]\nApr  1 13:35:00 hematus kernel: [  239.302448]  [<ffffffffa0870b41>] nfsd4_proc_compound+0x341/0x520 [nfsd]\nApr  1 13:35:00 hematus kernel: [  239.302463]  [<ffffffffa085e381>] nfsd_dispatch+0xb1/0x250 [nfsd]\nApr  1 13:35:00 hematus kernel: [  239.302474]  [<ffffffffa07826a3>] svc_process_common+0x333/0x620 [sunrpc]\nApr  1 13:35:00 hematus kernel: [  239.302488]  [<ffffffffa0782ce1>] svc_process+0x101/0x160 [sunrpc]\nApr  1 13:35:00 hematus kernel: [  239.302500]  [<ffffffffa085eb3d>] nfsd+0xcd/0x150 [nfsd]\nApr  1 13:35:00 hematus kernel: [  239.302505]  [<ffffffff81082966>] kthread+0x96/0xa0\nApr  1 13:35:00 hematus kernel: [  239.302511]  [<ffffffff81469ee4>] kernel_thread_helper+0x4/0x10\nApr  1 13:35:00 hematus kernel: [  239.302514] Code: 00 00 00 00 00 48 8b 87 c8 34 00 00 4c 8d af c8 24 00 00 48 8d bc 24 d0 00 00 00 be 00 10 00 00 4c 89 ea 48 89 84 24 d0 00 00 00\nApr  1 13:35:00 hematus kernel: <48>[  239.302527]  8b 40 20 48 89 84 24 d8 00 00 00 e8 49 fd ff ff 85 c0 0f 84\nApr  1 13:35:00 hematus kernel: [  239.302533] RIP  [<ffffffffa065e3a6>] zfsctl_lookup_snapshot_path+0x1a6/0x2c0 [zfs]\nApr  1 13:35:00 hematus kernel: [  239.302551]  RSP <ffff8817dbee99e0>\nApr  1 13:35:00 hematus kernel: [  239.302552] CR2: 0000000000000020\nApr  1 13:35:00 hematus kernel: [  239.302554] ---[ end trace c16be50e3596fc64 ]---\n```\n\nSo yeah, doesn't work just yet.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/39197997/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/39320535", "body": "@andrey-ve \n\nI grepped `/usr/src/linux/include` on a current SLES11 SP3 (and do bear in mind SuSE patches their kernels heavily so the 3.0.101 has interfaces probably similar to 3.6 vanilla) shows no 'HAVE_MOUNT_*' strings.\n\nThe only thing in the region of that was:\n\n```\n$ grep -Ri MOUNT_NODEV *\nlinux/fs.h:extern struct dentry *mount_nodev(struct file_system_type *fs_type,\n```\n\nit's defined as:\n\n``` h\nextern struct dentry *mount_nodev(struct file_system_type *fs_type,\n        int flags, void *data,\n        int (*fill_super)(struct super_block *, void *, int));\n```\n\nThere's also:\n\n``` h\n#define MNT_NODEV       0x02\n```\n\nin `linux/mount.h`.\n\nJust in case, I've put the src.rpm for the stock SLES11 SP3 kernel source at https://anorien.csc.warwick.ac.uk/kernel-source-3.0.76-0.11.1.x86_64.rpm - it will produce the /usr/src/linux used for building on SLES. \n\nHope this helps anyway, don't hesitate to ask if you need more information.\n(edit: updated rpm url)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/39320535/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45447243", "body": "@andrey-ve \n\nThank you for the reply, I'll get the path construct into the kernel module and see how we get on with that.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45447243/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/44266771", "body": "Hi,\n\nI've had some luck integrating ZoL builds with OpenSUSE's Open Build Service (OBS).\n\nI have reasonably well packaged RPMs for SLES11SP3 and a few other, RH-based distributions. Since the events daemon was a recent addition, I'm not packaging it yet but this is on the todo list. The work is based on SuSE's packages.\n\nPerhaps you will find the specs useful, especially since they can build just fine against non-running kernel and inside a clean-room build environment without network access.\n\nPlease ignore the version meta (0.6.3), I'm aiming to have those ready for the release, it's current-ish git checkout.\n\nhttp://anorien.csc.warwick.ac.uk/mirrors/OBS/home:ccscab:ZFSonLinux63/\n\nOh, and they use the same spec to build RH-ish dracut and SUSE-ish kmp rpms.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/44266771/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/44355279", "body": "The kernel module build is based on SuSE's older ZoL build. This is mostly work of munix9 on [gmail].  It's rather standard SUSE KMP build (their mechanism for 3rd party kernel modules).\n\nThe little tweak in zfs was:\n\n``` bash\nsed -i zfs_config.h -e \"s|/\\* #undef HAVE_BDI_SETUP_AND_REGISTER \\*/|#define HAVE_BDI_SETUP_AND_REGISTER 1|\" \n```\n\nThe only change to recent gitpull of ZoL was updating META to 0.6.3.\n\nTo build SPL in a clean room environment (OBS), one needs to:\n\n``` bash\nfind Makefile.am include -name Makefile.am -exec sed -e \"s|/usr/src/spl-|/usr/lib/spl-|g\" -i {} \\;\n```\n\nFor the time being, until I properly understand zfs events, I'm removing it from the package:\n\n```\nrm -rf %{buildroot}/%{_libdir}/%{name}/zed.d\nrm -rf %{buildroot}/%{_libexecdir}/%{name}/zed.d\nrm -rf %{buildroot}/%{_sysconfdir}/%{name}/zed.d\n```\n\nApologies for that, it popped up shortly (hours) before we froze for deployment and I had no time to get around to it.\n\nI noticed something hardcoded in the current implementation if I remember correctly it was a lib64/lib problem in SuSE/RH. Will report back when I get to the bottom of this and can properly package/integrate it with SLES.\n\nIt's not much but I hope it helps you guys.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/44355279/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/26214723", "body": "I executed `zdb -dddd storage` and there was a huge output.\nBut it stuck at a object about DDT.\nI had dedup enabled for a few weeks but then i disabled it. Seems the Dedup-Table is far to big.\nI created a new filesystem and made a snapshot of my old one. Currently im copying the data from the snapshot to the new filesystem, Maybe i get rid of the dedup-tables this way.\nIm Using Send and Receive due to the fact that i cant mount my old filesystem\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/26214723/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/26219519", "body": "Since i completed the Send/Receive i cant import both filesystems without problems... Strange.\nBut thank you very much for your support :)\n\nzfsonlinux rules!\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/26219519/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/26360188", "body": "`zdb -dddd storage`\nhttp://pastebin.com/8hHgMac5\n\n`zdb -DDD storage`\nhttp://pastebin.com/sQR4DfCY\n\nI already deleted files of my old pool (forgot this issue here).\nHe stucks at the end... and i wont wait until he finish this, sorry.\nMy first (old) filesystem was the root one (not a good idea, now i know)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/26360188/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/46299838", "body": "@behlendorf \n0.6.3 packages (_including zed_) should be ready by close of business today and shipped out at \n\nhttps://anorien.csc.warwick.ac.uk/mirrors/OBS/zfsonlinux.org/stable/\n\nSLE is already done, I'm making sure all RedHats are happy.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/46299838/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/46410617", "body": "With exception of ScientificLinux 6/7 x86_64 (there seems to be an upstream problem) and CentOS 5 (zed.d is holding it back and I haven't found the problem yet), the following are published with the latest 0.6.3 checkout and zed.d packaged:\n\nCentOS {6,7}/{i586/x86_64}\nRHEL {6,7}/{i586/x86_64}\nSLE11 SP3/{i586/x86_64}\nFedora {19,20}/{i586/x86_64}\nopenSUSE {12.3,13.1}/{i586/x86_64}\nScientificLinux {6,7}/i586\n\nAs of today, {CentOS,RHEL} 5/{i586/x86_64} and ScientificLinux {6,7}/x86_64 repos carry an older checkout with zed.d unpackaged. Those will be replaced when I fix the zed.d packaging (for RH5) and our build system builds for SL 64bit again.\n\nWe use SLE11 internally and tested on that. Others received only minimal build-testing. @greg-hydrogen see how you get on with those.\n\nhttps://anorien.csc.warwick.ac.uk/mirrors/OBS/zfsonlinux.org/stable/\n\nAll repositories contain source rpms, the relevant ones are `zfs-0.6.3-*.src.rpm` and `spl-0.6.3-*.src.rpm` and carry all spec files. The kmod and dkms src.rpms are soft links to the above (OBS by default uses packagename.spec for building, softlinking packages allows for different specs to be selected while carrying source only once).\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/46410617/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/46550501", "body": "ScientificLinux builds have been fixed. Only CentOS/RHEL 5 remains.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/46550501/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/38567768", "body": "Perhaps something that could serve as a stop-gap before full integration with Linux quota toolchain can be achieved.\n`.zfs/quota` directory at tank/dataset level (where it is now), allowing access to `/proc`-style quota information. For example:\n\n(Consistent with the \"Everything is a file\" UNIX paradigm).\n\n```\n# cd /tank/.zfs/quota/user\n# ls -l\ntotal 4\n-rw-------  1 root    root  0 Sep 26  2007 root\n-r--------  1 hamster users 0 Sep 26  2007 hamster\n# cat root\nTYPE        NAME     USED  QUOTA\nPOSIX User  root     442K   none\n# cat hamster\nTYPE        NAME     USED  QUOTA                         \nPOSIX User hamster  2.15G    80G\n```\n\n(Very) optionally:\n\n```\n# echo '100G' > /tank/.zfs/quota/user/hamster\n# cat hamster\nTYPE        NAME     USED  QUOTA                         \nPOSIX User hamster  2.15G   100G\n```\n\nAs user _hamster_:\n\n```\n$ cat root\nPermission denied\n$ cat hamster\nTYPE        NAME     USED  QUOTA                         \nPOSIX User hamster  2.15G    80G\n```\n\nThis logic could be followed by having `/tank/.zfs/quota/group`, `/tank/.zfs/quota/dataset`.\n\nWhile this does _not_ integrate with Linux quota toolchain, it provides a nice interface consistent with snapshots.\n\nSince quota and quota-nfs requires fstab entries, it may pose a problem since ZoL lacks them.\n\nThere is already a precedent for other filesystems requiring different quota management tools (GPFS for example) so perhaps it's not critical to have tight quotatools integration from get-go.\n\nI think it may also be easier to try to feed patches to quota and quota-nfs maintainers if all the do is manipulate a stable, proc-like API in .zfs directory.\n\nThis is one idea. Not necessarily correct but a start. \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/38567768/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/38630480", "body": "> The other issue you mentioned is that quota requires fstab entires. Are you positive about this? \n> Might it just require /etc/mtab entries which do exist for ZFS?\n\nHmm, good point. Let's see:\n\n```\n# grep zfs /etc/mtab\nhome /home zfs rw,xattr 0 0\n```\n\nFine, maybe we can add the _usrquota_ mount option?\n\n```\n# mount -o remount,usrquota /home\nfilesystem 'home' cannot be mounted due to invalid option 'usrquota'.\nUse the '-s' option to ignore the bad mount option.\n```\n\nDarn.\n\n```\n# quota -f /home\nquota: Mountpoint (or device) /home not found or has no quota enabled.\nquota: Not all specified mountpoints are using quota.\n```\n\nHow about fstab:\n\n```\n# echo \"home                    /home           zfs     usrquota 0 0\" >> /etc/fstab\n# quota -f /home\nquota: Mountpoint (or device) /home not found or has no quota enabled.\nquota: Not all specified mountpoints are using quota.\n```\n\nYou're absolutely right, it's `/etc/mtab`.  But editing it and adding 'usrquota' returns the same error message as lack thereof. Obviously. ;)\n\nSo maybe, if  _userquota@..._ or _groupquota@..._ are set, then the mtab entry could contain usrquota,grpquota? It's already passing _rw,xattr_ depending on what's defined by `zfs set attribute=value tank`\n\n> That should be relatively straight forward if we can find a sane mapping for the \n> ZFS <-> Linux quota semantics.\n\n| Feature | ZFS | Linux |\n| --- | --- | --- |\n| data quota | Y* | Y** |\n| inode quota | N\u2020 | Y |\n| hard quotas | Y | Y |\n| soft quotas | N | Y |\n| grace period | N | Y |\n- in human readable units\n  *\\* in blocks\n  \u2020 kind of pointless anyway\n\nI don't think there's a need for inode quota on ZFS (dynamic allocation et all) but _if_ it can be done easily, it would improve Linux compatibility.  With my Sysadmin hat on, I can see a benefit in being able to limit users down to say 20 million files in case they forkbomb themselves. ;)\n\nAs for where to store the additional, non-ZFS-canonical information (soft, grace), I think it could go into xattr. \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/38630480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/51672409", "body": "Saw exactly the same problem. No /etc/hostid file exists after fresh zfs installation.\nWorkaround, after https://github.com/zfsonlinux/zfs/issues/703\n\n`dd if=/dev/urandom of=/etc/hostid bs=4 count=1`\n`zpool import -f STORE`\n\nEverything fine in subsequent reboot.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/51672409/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/52011885", "body": "But something has changed. Before you had\n`cannot import 'STORE': pool may be in use from other system`\nNow the error is\n`cannot import 'STORE': no such pool or dataset`\nSo it seems the hostid issue is fixed, but somehow you lost the 'STORE' pool along the way.\n\nDoes zdb still show any pool?\nIf not, have you tried to create a new pool?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/52011885/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/56584461", "body": "As a simple workaround you could create e.g. the following crontab entry for root:\n\n`@reboot    sleep 30; zpool import -a`\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/56584461/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/56594548", "body": "Second sleep should not be necessary:\n`@reboot sleep 30; /sbin/zpool import -a; /sbin/zfs share -a`\n\nHow to change dev names is explained here:\nhttp://zfsonlinux.org/faq.html#HowDoIChangeNamesOnAnExistingPool\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/56594548/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/80732497", "body": "Misslick, sorry.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/80732497/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/93184014", "body": "@kernelOfTruth , of course, I already set best (I think) settings for Mysql/innoDB:\n1.\n**recordsize=16k** //for dataset\n**recordsize=128k** //for binlog/relaylog\n**ashift=12** //for pool\n\n2.\nI didn't use zil, because my pool built on ssd (RAID1, 2 x SSD Intel S3500)\n\n3.\nI limit arc-cache (/etc/modprobe.d/zfs.conf):\n**options zfs zfs_arc_min=1 zfs_arc_max=67108865** //64Mb - minimal value, I can set, it hardcoded in sources:\nless -N ./module/zfs/arc.c \n\n```\n   4115          * Allow the tunables to override our calculations if they are\n   4116          * reasonable (ie. over 64MB)\n   4117          */\n   4118         if (zfs_arc_max > 64<<20 && zfs_arc_max < physmem * PAGESIZE)\n   4119                 arc_c_max = zfs_arc_max;\n   4120         if (zfs_arc_min > 0 && zfs_arc_min <= arc_c_max)\n   4121                 arc_c_min = zfs_arc_min;\n```\n\n4.\nI set 'primarycache' and 'secondarycache' properties to 'none' for all filesystems on host.\n<br>\nWhy in this setup i see lots of readings from arc-cache and I can't manage it using 'primarycache' property?..\nMaybe, I misunderstood, how arc works, or, maybe, it bug in zfs?...\nThanks in advance!\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/93184014/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/97305618", "body": "@behlendorf , may you clarify my case?\n- I set arc size to minimal (64M); in my case big arc cache don't need: server running MySQL and I want to avoid double buffering (innodb buffer pool already contains all neccesary data for reads)\n\n```\nzfs_arc_min=1 zfs_arc_max=67108865\n```\n- regardless of the value of property 'primarycache' (none, metedata, all), I see lots of reading from arc-cache (stats below - 'primarycache=none'): \n\n```\ntime  read  miss  miss%  dmis  dm%  pmis  pm%  mmis  mm%  arcsz     c\n10:32:23  1.3K  1.3K     96  1.3K   96     0    0   928   95    81M   64M  \n10:32:24   432   412     95   412   95     0    0   336   94    74M   64M  \n10:32:25   611   565     92   565   92     0    0   265   85    79M   64M  \n10:32:26   632   618     97   618   97     0    0   340   96    79M   64M  \n10:32:27   346   302     87   302   87     0    0   197   81    73M   64M  \n10:32:28   765   708     92   708   92     0    0   467   89    76M   64M  \n```\n\nI suppose, that if read ops always going through arc, in case of small arc size and lots of reading, read ops may be longer, than in case of direct read from fs, and [arc_adapt] consume lot of cpu. And in case of 'primarycache=none' I expected to see zero reads from arc...\n\n**Is it known behavior of zfs? May I reduce reading from arc this case?**\n**[arc_adapt] thread consume up to 80-100% cpu time: is it expected behaviour in this case?**\n**Is a bottleneck for zfs read perfomance - case, when [arc_adapt] consume 100% cpu?**\n\nThanks in advance!\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/97305618/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/97055427", "body": "I forgot to mention that the ZFS from Solaris had totally failed and I had reused the drives, I wanted to keep things similar, hence i created a new pool with the same name.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/97055427/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/141050131", "body": "It seems to me that the deadlock is between threads 1 and 3.\nThread 1 takes zvol mutex and then tries to lock the dsl_pool.\nThread 3 takes the same locks, just in reverse order.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/141050131/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/158259172", "body": "Thanks, looks like I should subscribe to the mailing list. Sorry!\n\nOn Thursday, November 19, 2015, Brian Behlendorf notifications@github.com\nwrote:\n\n> Right, you'll want to stick with a 4.3 or older kernel until support is\n> added for 4.4.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/zfsonlinux/zfs/issues/4026#issuecomment-158240322.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/158259172/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/161724713", "body": "Sorry, i wants to make pull request to my fork, not to main.\nI cancel it.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/161724713/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/214550129", "body": "I've added a check for grep and made the generator more in line with the dmsquash one here: https://github.com/zfsonlinux/zfs/pull/4562  My boot was failing because grep was missing.\n\nI have a couple comments on these changes:\n\nI don't think there's a race in the import services.  One will run based on whether the cache is there or not.\n\nI don't think 'Before=dracut-mount.service' should be added to the import services, as they will be used on systems that have systemd, but not dracut.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/214550129/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/219096152", "body": "You removed this change from your other pull request after I mentioned that the ConditionPathExists option should prevent the services from running at the same time.  Is it not working?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/219096152/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/264106223", "body": "In case Debian users come across this post just as I did: I had this same issue in Debian 8 using jessie-backports and package zfsutils-linux (0.6.5.8-1~bpo8+1). I have reported this bug via https://bugs.debian.org/cgi-bin/pkgreport.cgi?pkg=zfsutils-linux;dist=unstable which I hope is the right place. ~~I'm waiting for the report to be accepted and listed there.~~ https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=846460 I have no idea if the same issue has been reported to the Ubuntu package maintainers. Anyone that uses Ubuntu and can reproduce this issue should probably report the problem via https://launchpad.net/ubuntu/+source/zfs-linux/+bugs?orderby=status&start=0", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/264106223/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/266286115", "body": "There was once a freelancer who was having issues with his storage array. So he went and asked for help on the public bug tracker of the open source project he was using for his storage.\r\n\r\nAnd the first member of the open source community came, and said \"_do you need help? try running this command_\" to which he replied \"_no thanks, i have already run another one_\".\r\n\r\nThen another member came, and he also tried to help the freelancer, but he said \"_no thanks, the Developer will save me_\".\r\n\r\nAfter a while he lost his storage array, and maybe also the job he was contracted for, because even if he was profiled in the \"top 3% overall\" on stackexchange he couldn't get the help of the open source community because he was always rude and impolite.\r\n\r\nThen, one day, he finally met the Developer. And he said \"_Developer, why didn't you help me?_\" and the Developer said \"_i sent you the open source community, fool!_\"\r\n\r\nThe End.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/266286115/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/312738781", "body": "I also have this problem. I hope it gets fixed.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/312738781/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "wayneeseguin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/62596974", "body": "Fascinating...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/62596974/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/64419483", "body": "Ok, I'll try that the next time I encounter this, thanks!\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/64419483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "drudru": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/41956162", "body": "Thanks. Will do. What will the next version be?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/41956162/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "gebi": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/320781", "body": "why not just try to remove it and be silent if remove fail?\nlet the kernel check the usecount on the module.\n\nThere is no suitable action anyway to a non-removable kernel module.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/320781/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/14834639", "body": "thx :) @dajhorn !\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/14834639/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "blair": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/17409757", "body": "I think a short term workaround is to apply the following diff to `/usr/share/grub/grub-mkconfig_lib`, this will chop everything after the first @.\n\n```\n--- grub-mkconfig_lib.ORIG  2013-05-03 11:14:08.476233913 -0700\n+++ grub-mkconfig_lib   2013-05-03 11:14:48.412292299 -0700\n@@ -46,7 +46,7 @@\n\n make_system_path_relative_to_its_root ()\n {\n-  \"${grub_mkrelpath}\" \"$1\"\n+  \"${grub_mkrelpath}\" \"$1\" | sed -e \"s,@.*,@,\"\n }\n\n is_path_readable_by_grub ()\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/17409757/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/20004390", "body": "@atomjack Thanks!  I noticed that the instructions don't mention Precise any more and when I did installs on both Raring and Precise a few months back, there are some differences.  Do the updated instructions still work for Precise?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/20004390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/20006865", "body": "@atomjack If/when I do another fresh Precise install, I'll follow the instructions and update them as necessary for the differences between them.  I wouldn't take the time to do a VMWare install yourself.  My two cents.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/20006865/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "skorgu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/172615535", "body": "I threw this into /etc/cron.daily/zfs-update-dkms on CentOS 7 (7.2.1511). Obviously if you reboot immediately after updating kernels this won't help unless you remember to run it manually. \n\n``` bash\n#!/bin/bash\nset -eu\n# Set this to something high to get modules built for all of your kernels \n# or 1 to just build for the most recent.\nN_KERNELS=10\n\nSPL_VER=$(rpm -qa --qf \"%{VERSION}\\n\"  spl-dkms | sort -V | tail -n1)\nZFS_VER=$(rpm -qa --qf \"%{VERSION}\\n\"  zfs-dkms | sort -V | tail -n1)\nfor KERNEL_VER in $(rpm -qa --qf \"%{VERSION}-%{RELEASE}.%{ARCH}\\n\"  kernel | sort -V | tail -n${N_KERNELS})\ndo\n  dkms install -q spl/${SPL_VER} -k ${KERNEL_VER}\n  dkms install -q zfs/${ZFS_VER} -k ${KERNEL_VER}\ndone\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/172615535/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "aruiz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/345478828", "body": "With regards to the zfs-fuse problem, it looks like there's an opportunity here to move the userland tools upstream and have zfs-fuse and the kernel drivers being different packages. Have you considered reaching out to the zfs-fuse maintainer to talk about this?", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/345478828/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "rcorrear": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/15209789", "body": "You should probably make it obvious that you have to add users to Samba's database (sudo smbpasswd -a user). Otherwise it \"Just Works\", good job!.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/15209789/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/15304864", "body": "While I agree that telling users how to manage their samba servers isn't \"your\" thing, a little pointer would be nice (for example, I had to read the relevant sections of the zfs manual -expected- before I figured you used \"net\", and then I had to read the \"net\" manual which is quite long to understand how this all worked). You don't really even have to point users to a specific authentication backend (the smbpasswd was just an example to get someone started), just point out that authenticating against samba isn't automagically configured. The thing that wasn't immediately obvious to me was that sharesmb was actually getting much of its configuration from the system installed samba server.\nThis is however just a suggestion, something that could save another noob like me a couple hours.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/15304864/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ank": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/42701253", "body": "I can confirm similar behavior with ubuntu 12.04 and zfs /home. This is after a reboot.\n\n```\nzfs get all | grep readonly\ntank                   readonly              off                            default\ntank/home              readonly              on                             temporary\ntank/timemachine       readonly              off                            default\n```\n\n/etc/fstab\n\n```\n# <file system> <mount point>   <type>  <options>       <dump>  <pass>\nproc            /proc           proc    nodev,noexec,nosuid 0       0\n# / was on /dev/sda1 during installation\nUUID=c072c0e6-3a25-46ea-b1aa-2f30eb614ab8 /               ext4    errors=remount-ro 0       1\n# swap was on /dev/sda5 during installation\nUUID=553b9524-eae6-4510-8c9c-5f7f5077080f none            swap    sw              0       0\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/42701253/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ivan": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/6819687", "body": "Things should work with a non-hardened kernel, as I've discovered in https://github.com/zfsonlinux/zfs/issues/809\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/6819687/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/6819190", "body": "In the video, you might catch some irrelevant changes I've made to my initramfs:\n1) make genkernel put the xts module into the initramfs (https://bugs.gentoo.org/show_bug.cgi?id=425028)\n2) make genkernel/defaults/linuxrc startVolumes only after asking for LUKS password\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/6819190/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/6819334", "body": "Might be the same as https://github.com/zfsonlinux/zfs/issues/794\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/6819334/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/6819678", "body": "Looks like this is a hardened-sources+ZFS problem.  The system boots fine with a gentoo-sources-3.4.2-r1 kernel.\n\n(Both hardened-sources-3.4.2-hardened-r1 and gentoo-sources-3.4.2-r1 were built with default options, other than `CONFIG_SCSI_LPFC=n` to avoid breaking the build.)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/6819678/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/6925698", "body": "I applied that patch during both `emerge zfs` and `genkernel all -> module-rebuild`, and I see the exact same kernel oops during boot.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/6925698/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/7084211", "body": "sys-kernel/hardened-sources-3.2.23 + zfs-9999/spl-9999 boots fine\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/7084211/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/7084864", "body": "[deleted incorrect comment about sys-kernel/hardened-sources-3.4.2-r1 + zfs-9999/spl-9999 working]\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/7084864/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/7086261", "body": "sys-kernel/hardened-sources-3.4.2-r1 + zfs-9999/spl-9999 results in the same kernel oops\n\nsys-kernel/hardened-sources-3.4.4-r2 + zfs-9999/spl-9999 results in the same kernel oops\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/7086261/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/253192001", "body": "I never saw this with ZFS in the xenial kernel, or xenial's zfs-dkms with a mainline kernel.  But after building an Ubuntu linux-generic kernel with zfs af4db70f368d0e9c9ad90f0837a77e511a77b0a7 and spl [`0d267566`](https://github.com/zfsonlinux/spl/commit/0d267566650d89bde8bd5ec4665749810d5bafc7), I saw exactly what kicmuc sees.  Maybe this is just an incompatibility between the older libzfs2linux/zfsutils-linux (0.6.5.6-0ubuntu13) I have installed and the newer zfs module in the kernel?  If that were the case, it would still be better if `zfs-zed` didn't go into a loop and consume 100% CPU.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/253192001/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "mcandre": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/333387298", "body": "Update: Managed to fix zfs by running `sudo kldload /boot/kernel/zfs.ko` and retrying the zpool commands. Looks like the zfs module is not automatically loaded by the FreeBSD Vagrant image, but otherwise the zfs system is working fine.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/333387298/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "Flink": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/9090260", "body": "Hi, I\u2019ve just upgraded to Quantal and grub-probe won\u2019t recognize my zfs filesystem :/\nI don\u2019t get the same error than described here but I have this instead:\n\n```\ngrub-core/kern/fs.c:55: Detecting zfs...\ngrub-core/kern/emu/hostdisk.c:909: opening the device `/dev/sda' in open_device()\ngrub-core/fs/zfs/zfs.c:951: label ok 0\ngrub-core/kern/emu/hostdisk.c:886: reusing open device `/dev/sda'\ngrub-core/kern/emu/hostdisk.c:886: reusing open device `/dev/sda'\ngrub-core/kern/emu/hostdisk.c:886: reusing open device `/dev/sda'\ngrub-core/fs/zfs/zfs.c:791: check 2 passed\ngrub-core/fs/zfs/zfs.c:3072: incorrect nvlist header\ngrub-core/kern/emu/hostdisk.c:886: reusing open device `/dev/sda'\ngrub-core/fs/zfs/zfs.c:951: label ok 1\ngrub-core/kern/emu/hostdisk.c:886: reusing open device `/dev/sda'\ngrub-core/kern/emu/hostdisk.c:886: reusing open device `/dev/sda'\ngrub-core/fs/zfs/zfs.c:791: check 2 passed\ngrub-core/fs/zfs/zfs.c:3072: incorrect nvlist header\ngrub-core/kern/emu/hostdisk.c:886: reusing open device `/dev/sda'\ngrub-core/fs/zfs/zfs.c:951: label ok 2\ngrub-core/kern/emu/hostdisk.c:886: reusing open device `/dev/sda'\ngrub-core/kern/emu/hostdisk.c:886: reusing open device `/dev/sda'\ngrub-core/fs/zfs/zfs.c:791: check 2 passed\ngrub-core/fs/zfs/zfs.c:3072: incorrect nvlist header\ngrub-core/kern/emu/hostdisk.c:886: reusing open device `/dev/sda'\ngrub-core/fs/zfs/zfs.c:951: label ok 3\ngrub-core/kern/emu/hostdisk.c:886: reusing open device `/dev/sda'\ngrub-core/kern/emu/hostdisk.c:886: reusing open device `/dev/sda'\ngrub-core/fs/zfs/zfs.c:791: check 2 passed\ngrub-core/fs/zfs/zfs.c:3072: incorrect nvlist header\ngrub-core/kern/fs.c:77: zfs detection failed.\n```\n\nAny idea?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/9090260/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/9097152", "body": "@dajhorn Seems to work. I didn\u2019t reboot yet though ;) Thank you!\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/9097152/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "uzytkownik": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/318728952", "body": "I reproduced the problem when I tried to migrate to zfs and started copying images to zvol on Gentoo (kernel 4.9, zfs 0.7.0). Server has 16 GiB of memory and it chokes after 22GiB of data on 750 GiB partition. Increasing limits and periodically dropping cache seems to work but the latter is annoying.\r\n\r\nMost annoying is that system seems idle - no disk/cpu activity beyond background radiation level, swap empty etc.\r\n\r\nFWIW clearing pagecache seems to be the most important thing...", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/318728952/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/318769639", "body": "@behlendorf No, at least not during operation is running. I'm rebooting the system to check.\r\n\r\nI have an additional data point - I have zfs on lvm on mdraid. When I tried to pvmove an unrelated partition but it got stuck/hanged as well. Is it possible that Linux scheduler is in some weird state for whatever reason?", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/318769639/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/318953093", "body": "No. I run into the same problem after reboot.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/318953093/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "fdr": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/221351688", "body": "I've managed to run across something very similar. The workload was a parallel bulk load to Postgres on EC2, with Amazon Linux 2016.03. The ZFS version is 0.6.5.7-1.el6.  A postmaster process was using 100% system CPU right before the instance was auto-restarted by monitoring, and that means that it was unable to accept connections for a couple of minutes consecutively. Interesting other information is it is preceded by an OOM condition, though this system has a `sysctl.conf` that disables overcommit. Without further ado, the logs:\n\n```\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4803.913779] [ pid ]   uid  tgid total_vm      rss nr_ptes nr_pmds swapents oom_score_adj name\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4803.918620] [ 2310]     0  2310     2721       94      11       3        0         -1000 udevd\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4803.923526] [ 6796]     0  6796     2340      122       9       3        0             0 dhclient\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4803.928459] [ 6855]     0  6855    28022       68      25       3        0         -1000 auditd\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4803.933328] [ 6876]     0  6876    61847       72      23       3        0             0 rsyslogd\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4803.938381] [ 6890]     0  6890     3468       78      11       3        0             0 irqbalance\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4803.943471] [ 6899]     0  6899     1095       21       8       3        0             0 rngd\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4803.948297] [ 6917]    32  6917     8823       99      23       3        0             0 rpcbind\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4803.953395] [ 6938]    29  6938     9965      201      24       3        0             0 rpc.statd\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4803.958230] [ 6969]    81  6969     5448       58      16       3        0             0 dbus-daemon\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4803.963083] [ 7020]     0  7020     7835       62      20       3        0             0 zed\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4803.967636] [ 7055]    99  7055     3291       41      10       3        0             0 dnsmasq\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4803.972194] [ 7169]     0  7169    19460      205      41       3        0         -1000 sshd\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4803.976664] [ 7291]    38  7291     7322      143      18       3        0             0 ntpd\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4803.981176] [ 7377]     0  7377    22247      429      45       3        0             0 sendmail\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4803.989394] [ 7386]    51  7386    20112      367      41       3        0             0 sendmail\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4803.993958] [ 7398]     0  7398    29879      145      15       4        0             0 crond\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4803.998390] [ 7412]     0  7412     4267       39      13       3        0             0 atd\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.002896] [ 7492]     0  7492     2720       97      10       3        0         -1000 udevd\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.007442] [ 7499]     0  7499     2830      207      10       3        0         -1000 udevd\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.011840] [ 7670]     0  7670     1615       30       8       3        0             0 agetty\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.016591] [ 7672]     0  7672     1078       24       8       3        0             0 mingetty\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.021168] [ 7675]     0  7675     1078       24       8       3        0             0 mingetty\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.025730] [ 7678]     0  7678     1078       24       8       3        0             0 mingetty\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.030375] [ 7680]     0  7680     1078       24       7       3        0             0 mingetty\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.034942] [ 7682]     0  7682     1078       23       8       3        0             0 mingetty\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.039505] [ 7684]     0  7684     1078       24       8       3        0             0 mingetty\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.044220] [ 8636]     0  8636    29451      268      62       4        0             0 sshd\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.049072] [ 8638]   500  8638    29643      470      62       4        0             0 sshd\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.053854] [ 8639]   500  8639    28837       96      15       3        0             0 bash\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.058169] [ 9348]     0  9348    46016      155      47       3        0             0 sudo\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.062624] [ 9349]     0  9349    28870      144      14       3        0             0 bash\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.067074] [14296]    26 14296  4088198   101665     253       5        0             0 postmaster\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.071878] [14298]    26 14298    49334      301      47       3        0             0 postmaster\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.076560] [14300]    26 14300  4090188   825739    1680       8        0             0 postmaster\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.081445] [14301]    26 14301  4088243    31372     123       5        0             0 postmaster\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.086085] [14302]    26 14302  4088198     4420      60       4        0             0 postmaster\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.090669] [14303]    26 14303  4088339      643      59       5        0             0 postmaster\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.095259] [14304]    26 14304    49801      310      48       4        0             0 postmaster\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.100784] [14305]    26 14305    49836      341      48       4        0             0 postmaster\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.105494] [14306]    26 14306  4088229      337      51       4        0             0 postmaster\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.110183] [15432]    26 15432  4093319    42310    1103       8        0             0 postmaster\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.114796] [15433]    26 15433  4088581    22839     136       5        0             0 postmaster\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.119705] [15434]    26 15434  4093183    22827     171       5        0             0 postmaster\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.124520] [15435]    26 15435  4088577    22932     165       5        0             0 postmaster\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.129736] [15436]    26 15436  4096914   400685    1691       8        0             0 postmaster\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.135685] [15437]    26 15437  4094302   119586     564       8        0             0 postmaster\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.140870] [15438]    26 15438  4093223   171873     648       7        0             0 postmaster\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.145620] [15439]    26 15439  4102753   490134    1854       8        0             0 postmaster\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.150205] [22421]     0 22421    33775       95      24       3        0             0 zpool\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.154848] Out of memory: Kill process 14300 (postmaster) score 52 or sacrifice child\nMay 24 17:28:37 ip-172-31-55-208 kernel: [ 4804.158789] Killed process 14300 (postmaster) total-vm:16360752kB, anon-rss:9040kB, file-rss:3293916kB\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.420074] INFO: task kthreadd:2 blocked for more than 120 seconds.\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.423361]       Tainted: P           OE   4.4.10-22.54.amzn1.x86_64 #1\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.426716] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.430538] kthreadd        D ffff880f0e3eb6a0     0     2      0 0x00000000\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.434075]  ffff880f0e3eb6a0 ffff880f0dc30000 ffff880f0e379d40 ffff880f0e3ec000\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.437992]  ffff880f08b59220 ffff880f08b59370 ffff880f08b59248 0000000000000000\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.441925]  ffff880f0e3eb6b8 ffffffff814d99f5 ffff880f08b59368 ffff880f0e3eb718\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.445755] Call Trace:\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.446985]  [<ffffffff814d99f5>] schedule+0x35/0x80\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.449452]  [<ffffffffa02e274d>] cv_wait_common+0xed/0x120 [spl]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.452385]  [<ffffffff810ac670>] ? prepare_to_wait_event+0xf0/0xf0\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.455414]  [<ffffffffa02e2795>] __cv_wait+0x15/0x20 [spl]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.458113]  [<ffffffffa0b1065a>] txg_wait_open+0xba/0x100 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.461152]  [<ffffffffa0acf19f>] dmu_tx_wait+0x36f/0x380 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.464098]  [<ffffffffa0acf23e>] dmu_tx_assign+0x8e/0x4e0 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.467001]  [<ffffffffa0b492f3>] zfs_inactive+0xd3/0x230 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.469906]  [<ffffffffa0b5e603>] zpl_evict_inode+0x43/0x60 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.472905]  [<ffffffff811ebf6e>] evict+0xbe/0x1a0\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.475228]  [<ffffffff811ec086>] dispose_list+0x36/0x50\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.477962]  [<ffffffff811ed3cb>] prune_icache_sb+0x4b/0x60\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.480884]  [<ffffffff811d5d21>] super_cache_scan+0x141/0x190\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.483841]  [<ffffffff81170934>] shrink_slab.part.41+0x1e4/0x390\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.486853]  [<ffffffff811746a9>] shrink_zone+0x2a9/0x2c0\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.489548]  [<ffffffff81174a45>] do_try_to_free_pages+0x175/0x440\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.492512]  [<ffffffff81174dc5>] try_to_free_pages+0xb5/0x170\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.495293]  [<ffffffff81168a8a>] __alloc_pages_nodemask+0x53a/0xa60\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.498436]  [<ffffffff8100c345>] ? xen_clocksource_read+0x15/0x20\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.501416]  [<ffffffff8109fd3f>] ? update_curr+0xdf/0x170\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.504079]  [<ffffffff81169171>] alloc_kmem_pages_node+0x51/0xd0\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.506985]  [<ffffffff8106b63a>] copy_process+0x15a/0x1a60\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.509743]  [<ffffffff8109dfc1>] ? set_next_entity+0x1d1/0x710\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.512606]  [<ffffffff8109fb65>] ? pick_next_entity+0xa5/0x160\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.515409]  [<ffffffff81089fb0>] ? kthread_park+0x60/0x60\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.518100]  [<ffffffff8106d0b2>] _do_fork+0x82/0x300\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.520492]  [<ffffffff814d94bb>] ? __schedule+0x34b/0x850\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.523138]  [<ffffffff810b2c91>] ? __raw_callee_save___pv_queued_spin_unlock+0x11/0x20\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.526943]  [<ffffffff8106d359>] kernel_thread+0x29/0x30\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.529592]  [<ffffffff8108abf4>] kthreadd+0x2d4/0x320\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.532119]  [<ffffffff8108a920>] ? kthread_create_on_cpu+0x60/0x60\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.535294]  [<ffffffff814dd6cf>] ret_from_fork+0x3f/0x70\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.538085]  [<ffffffff8108a920>] ? kthread_create_on_cpu+0x60/0x60\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.541395] INFO: task kswapd0:689 blocked for more than 120 seconds.\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.544870]       Tainted: P           OE   4.4.10-22.54.amzn1.x86_64 #1\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.548558] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.552586] kswapd0         D ffff880f0a7e7998     0   689      2 0x00000000\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.556253]  ffff880f0a7e7998 ffff880f0dc31d40 ffff880f0dfdd7c0 ffff880f0a7e8000\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.560344]  ffff880f08b59220 ffff880f08b59370 ffff880f08b59248 0000000000000000\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.564361]  ffff880f0a7e79b0 ffffffff814d99f5 ffff880f08b59368 ffff880f0a7e7a10\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.568389] Call Trace:\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.569701]  [<ffffffff814d99f5>] schedule+0x35/0x80\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.572271]  [<ffffffffa02e274d>] cv_wait_common+0xed/0x120 [spl]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.575350]  [<ffffffff810ac670>] ? prepare_to_wait_event+0xf0/0xf0\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.578673]  [<ffffffffa02e2795>] __cv_wait+0x15/0x20 [spl]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.581697]  [<ffffffffa0b1065a>] txg_wait_open+0xba/0x100 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.584768]  [<ffffffffa0acf19f>] dmu_tx_wait+0x36f/0x380 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.587788]  [<ffffffffa0acf23e>] dmu_tx_assign+0x8e/0x4e0 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.590910]  [<ffffffffa0b492f3>] zfs_inactive+0xd3/0x230 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.593953]  [<ffffffffa0b5e603>] zpl_evict_inode+0x43/0x60 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.597091]  [<ffffffff811ebf6e>] evict+0xbe/0x1a0\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.599599]  [<ffffffff811ec086>] dispose_list+0x36/0x50\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.602253]  [<ffffffff811ed3cb>] prune_icache_sb+0x4b/0x60\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.604943]  [<ffffffff811d5d21>] super_cache_scan+0x141/0x190\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.607832]  [<ffffffff81170934>] shrink_slab.part.41+0x1e4/0x390\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.610886]  [<ffffffff811746a9>] shrink_zone+0x2a9/0x2c0\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.613666]  [<ffffffff81175614>] kswapd+0x4b4/0x960\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.616304]  [<ffffffff81175160>] ? mem_cgroup_shrink_node_zone+0x190/0x190\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.619710]  [<ffffffff8108a079>] kthread+0xc9/0xe0\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.622223]  [<ffffffff81089fb0>] ? kthread_park+0x60/0x60\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.624860]  [<ffffffff814dd6cf>] ret_from_fork+0x3f/0x70\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.627484]  [<ffffffff81089fb0>] ? kthread_park+0x60/0x60\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.630240] INFO: task txg_sync:5378 blocked for more than 120 seconds.\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.633420]       Tainted: P           OE   4.4.10-22.54.amzn1.x86_64 #1\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.636628] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.640425] txg_sync        D ffff880efb98faf8     0  5378      2 0x00000000\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.643951]  ffff880efb98faf8 ffff880f0c0b0000 ffff880efb9157c0 ffff880efb990000\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.647793]  0000000000000000 7fffffffffffffff ffff880e0d977dd0 0000000000000001\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.651874]  ffff880efb98fb10 ffffffff814d99f5 ffff880f52015340 ffff880efb98fbb0\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.655825] Call Trace:\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.657084]  [<ffffffff814d99f5>] schedule+0x35/0x80\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.659497]  [<ffffffff814dc2d1>] schedule_timeout+0x231/0x2b0\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.662333]  [<ffffffff810b2c91>] ? __raw_callee_save___pv_queued_spin_unlock+0x11/0x20\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.666161]  [<ffffffff8100c365>] ? xen_clocksource_get_cycles+0x15/0x20\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.669387]  [<ffffffff810d233a>] ? ktime_get+0x3a/0x90\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.671912]  [<ffffffff814d9106>] io_schedule_timeout+0xa6/0x110\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.674937]  [<ffffffffa02e270d>] cv_wait_common+0xad/0x120 [spl]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.677939]  [<ffffffff810ac670>] ? prepare_to_wait_event+0xf0/0xf0\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.681163]  [<ffffffffa02e27d8>] __cv_wait_io+0x18/0x20 [spl]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.684102]  [<ffffffffa0b5739e>] zio_wait+0x10e/0x1f0 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.686905]  [<ffffffffa0ae60e8>] dsl_pool_sync+0xa8/0x410 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.689957]  [<ffffffffa0afda38>] spa_sync+0x358/0xaf0 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.692865]  [<ffffffff810ac682>] ? autoremove_wake_function+0x12/0x40\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.696122]  [<ffffffff810b2c91>] ? __raw_callee_save___pv_queued_spin_unlock+0x11/0x20\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.700154]  [<ffffffffa0b111aa>] txg_sync_thread+0x3aa/0x600 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.703204]  [<ffffffffa0b10e00>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.706457]  [<ffffffffa02ddca1>] thread_generic_wrapper+0x71/0x80 [spl]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.709634]  [<ffffffffa02ddc30>] ? __thread_exit+0x20/0x20 [spl]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.712590]  [<ffffffff8108a079>] kthread+0xc9/0xe0\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.714970]  [<ffffffff81089fb0>] ? kthread_park+0x60/0x60\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.717639]  [<ffffffff814dd6cf>] ret_from_fork+0x3f/0x70\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.720272]  [<ffffffff81089fb0>] ? kthread_park+0x60/0x60\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.722947] INFO: task postmaster:14300 blocked for more than 120 seconds.\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.726261]       Tainted: P           OE   4.4.10-22.54.amzn1.x86_64 #1\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.729520] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.733300] postmaster      D ffff880e1d033c48     0 14300  14296 0x00100004\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.736888]  ffff880e1d033c48 ffff880f086c9d40 ffff8800eb1957c0 ffff880e1d034000\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.740769]  0000000000000000 7fffffffffffffff ffff88066b0c44f0 0000000000000001\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.744686]  ffff880e1d033c60 ffffffff814d99f5 ffff880f520d5340 ffff880e1d033cf8\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.748669] Call Trace:\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.750021]  [<ffffffff814d99f5>] schedule+0x35/0x80\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.752440]  [<ffffffff814dc2d1>] schedule_timeout+0x231/0x2b0\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.755263]  [<ffffffff8100c365>] ? xen_clocksource_get_cycles+0x15/0x20\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.758495]  [<ffffffff810d233a>] ? ktime_get+0x3a/0x90\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.761249]  [<ffffffff814d9106>] io_schedule_timeout+0xa6/0x110\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.764288]  [<ffffffffa02e270d>] cv_wait_common+0xad/0x120 [spl]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.767317]  [<ffffffff810ac670>] ? prepare_to_wait_event+0xf0/0xf0\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.770369]  [<ffffffffa02e27d8>] __cv_wait_io+0x18/0x20 [spl]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.773369]  [<ffffffffa0b5739e>] zio_wait+0x10e/0x1f0 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.776020]  [<ffffffffa0b51654>] zil_commit.part.11+0x434/0x7a0 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.779208]  [<ffffffff810b2c91>] ? __raw_callee_save___pv_queued_spin_unlock+0x11/0x20\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.783078]  [<ffffffffa0b519d7>] zil_commit+0x17/0x20 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.785826]  [<ffffffffa0b435ea>] zfs_fsync+0x7a/0xf0 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.788529]  [<ffffffffa0b5cc65>] zpl_fsync+0x65/0x90 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.791203]  [<ffffffff812032bd>] vfs_fsync_range+0x3d/0xb0\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.793917]  [<ffffffff8120338d>] do_fsync+0x3d/0x70\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.796317]  [<ffffffff81203600>] SyS_fsync+0x10/0x20\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.798794]  [<ffffffff814dd36e>] entry_SYSCALL_64_fastpath+0x12/0x71\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.801928] INFO: task postmaster:15432 blocked for more than 120 seconds.\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.805233]       Tainted: P           OE   4.4.10-22.54.amzn1.x86_64 #1\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.808484] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.812275] postmaster      D ffff880e307979f0     0 15432  14296 0x00000000\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.815839]  ffff880e307979f0 ffff880efb913a80 ffff880ef958ba80 ffff880e30798000\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.819722]  ffff880f08b59220 ffff880f08b59370 ffff880f08b59248 0000000000000000\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.823616]  ffff880e30797a08 ffffffff814d99f5 ffff880f08b59368 ffff880e30797a68\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.827565] Call Trace:\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.828808]  [<ffffffff814d99f5>] schedule+0x35/0x80\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.831120]  [<ffffffffa02e274d>] cv_wait_common+0xed/0x120 [spl]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.834337]  [<ffffffff810ac670>] ? prepare_to_wait_event+0xf0/0xf0\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.837568]  [<ffffffffa02e2795>] __cv_wait+0x15/0x20 [spl]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.840307]  [<ffffffffa0b1065a>] txg_wait_open+0xba/0x100 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.843249]  [<ffffffffa0acf19f>] dmu_tx_wait+0x36f/0x380 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.846186]  [<ffffffffa0acf23e>] dmu_tx_assign+0x8e/0x4e0 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.849142]  [<ffffffffa0b4495d>] zfs_write+0x3dd/0xbb0 [zfs]\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.851884]  [<ffffffff81167f8c>] ? get_page_from_freelist+0x3cc/0x990\nMay 24 17:29:28 ip-172-31-55-208 kernel: [ 4920.855049]  [<ffffffff8116867f>] ? __alloc_pages_nodemask+0x12f/0xa60\nMay 24 17:29:29 ip-172-31-55-208 kernel: [ 4920.858253]  [<ffffffff810b2c91>] ? __raw_callee_save___pv_queued_spin_unlock+0x11/0x20\nMay 24 17:29:29 ip-172-31-55-208 kernel: [ 4920.862117]  [<ffffffffa0b5c7ba>] zpl_write_common_iovec+0x7a/0xd0 [zfs]\nMay 24 17:29:29 ip-172-31-55-208 kernel: [ 4920.865447]  [<ffffffffa0b5c9b8>] zpl_write+0x78/0xa0 [zfs]\nMay 24 17:29:29 ip-172-31-55-208 kernel: [ 4920.868121]  [<ffffffff811d2378>] __vfs_write+0x28/0xe0\nMay 24 17:29:29 ip-172-31-55-208 kernel: [ 4920.870670]  [<ffffffff810b26a7>] ? percpu_down_read+0x17/0x50\nMay 24 17:29:29 ip-172-31-55-208 kernel: [ 4920.873436]  [<ffffffff811d2a22>] vfs_write+0xa2/0x1a0\nMay 24 17:29:29 ip-172-31-55-208 kernel: [ 4920.875889]  [<ffffffff8105ea40>] ? __do_page_fault+0x1a0/0x3f0\nMay 24 17:29:29 ip-172-31-55-208 kernel: [ 4920.878807]  [<ffffffff811d3726>] SyS_write+0x46/0xa0\nMay 24 17:29:29 ip-172-31-55-208 kernel: [ 4920.881248]  [<ffffffff811d25a7>] ? SyS_lseek+0x87/0xb0\nMay 24 17:29:29 ip-172-31-55-208 kernel: [ 4920.883766]  [<ffffffff814dd36e>] entry_SYSCALL_64_fastpath+0x12/0x71\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/221351688/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "jf": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/307575166", "body": "same for `0.6.5.9`, btw. And ditto - I'd like to have this.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/307575166/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "avsej": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/344264622", "body": "Fedora has been released recently. Any plans to deploy F27 repository of zfs and http://download.zfsonlinux.org/fedora/zfs-release.fc27.noarch.rpm?", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/344264622/reactions", "total_count": 10, "+1": 10, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "steev": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/24344758", "body": "Yes that fixes the warnings.  Compile completes without issue.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/24344758/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "grigio": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32130568", "body": "probably related to #599 \nI also have to use `-f` because sometimes I get \"pool may be in use from other system\"\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32130568/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32240298", "body": "i imagined, any hint where to add the delay in ubuntu/upstart ?\ni tried with rc.local but i have to force the zfs mount\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32240298/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32288344", "body": "@behlendorf I removed my hacks in `rc.local` and tried it but I alwais get `no pools available`\n\nzfs-mountall is installed, to see the volume after the boot I've to run:\n\n```\n$ sudo zpool import cpool\ncannot mount '/cpool': directory is not empty\n$ sudo zpool status\n  pool: cpool\n state: ONLINE\n  scan: scrub repaired 79K in 0h0m with 0 errors on Sun Jan 12 13:06:21 2014\nconfig:\n\n    NAME          STATE     READ WRITE CKSUM\n    cpool         ONLINE       0     0     0\n      sda3_crypt  ONLINE       0     0     0\n\nerrors: No known data errors\n```\n\nIt's weird, i get \"cannot mount..\" but then it is mounted correctly\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32288344/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32317482", "body": "Yes it mounted and the volumes and data are correcly shown inside the directory, I read the LUKS part but it isn't my situation. My root / is LUKS(ext4) and /cpool is LUKS(zfs). The LUKS(zfs) container is correctly unlocked at boot but then somethng goes wrong, sometimes `zpool status` says ONLINE but it isn't.\n\nMy hacky solution is a `/etc/rc.local` with:\n\n```\n# HACK: zfs crypto mount\nzpool export cpool\nsleep 1\nzpool import cpool -f\n\nexit 0\n```\n\nI've to export (because sometime it is uncorrectly imported) and use the `-f` @behlendorf do you think I risk to damage the data in this way?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32317482/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32324607", "body": "No, it's alwais dm-1\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32324607/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32325347", "body": "i think it just label generated by luks\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32325347/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/99356386", "body": "I confirm this bug with `zfs-dkms 0.6.4.1-1~vivid` and `linux 4.0.0-040000-generic`.\nI've zfs on a single disk on LUKS and I use [zfs-systemd](https://github.com/paulczar/zfs-systemd).\n\nI think it's something related to standby of the disks, because I didn't notice this problem with manual import/export\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/99356386/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "jwiegley": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/12724175", "body": "I'll wait until rc14 and try it then.  This isn't really a debugging server,\nso until then I'll just not access snapshots except via clone.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/12724175/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/13046310", "body": "Things look good with rc14, thanks!\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/13046310/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/20395585", "body": "With the latest release this is fixed.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/20395585/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/28564110", "body": "I'm getting regular lockups trying to rsync to a filesystem with the current nightly beta.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/28564110/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/28603188", "body": "Let me describe my scenario in a bit more detail:\n\nI've got a home-built file server here, with 16GB of ECC RAM and a 12 drives in a RAID-10 array.  The OS is Ubuntu 12.04.3, installed on a USB flash drive.\n\nI use git-annex heavily for maintaining my archives, and one thing that it does is create a LOT of files (it's the biggest inode consumer I've ever seen).  These all get populated within the `.git` directory.\n\nI have seen it happen, maybe 4-5 times in the past, that trying to run `rm -fr` on such a `.git` directory leads to a lockup in the kernel.  Previously, the answer was to reboot the system.  Last night, however, something new happened: it appears that the filesystem became borked somehow.  Any attempts to mount it with `zfs mount` would deadlock the kernel.\n\nI worked around this by destroying the filesystem (which had no snapshots, btw) and recreating it from a backup copy.\n\nThe reason `rsync` is mentioned is that I was rsync'ing a newer copy of this filesystem's content which did not have a `.git` directory, and so rsync was attempting to delete all the files there.  It got through a whole bunch of them before the rsync process just stopped and things were never right again.  The console kept reporting a \"hung task after 120 seconds\".  After reboot, `mount.zfs` would hang likewise every time (I tried 2-3 times).\n\nI don't have the tarpit filesystem anymore, but I wonder if this could be reproduce on your side by just artificially manufacturing maybe a million files in, say, 2000 directories at maximum depth 4, and then trying to delete them all at once.\n\nLastly, I have not modified any ZFS variables in the kernel; I don't have an L2ARC or a ZIL device.  Pre-lockup I don't know what the memory situation was.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/28603188/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/29258528", "body": "This has happened to me again.  While trying to rsync from my Mac Pro to my ZOL mount, it started giving me a bunch of errors like:\n\n```\nrsync: chgrp \"/tank/Data/Machines/OED.vmwarevm/Applications/Remote Access Phonebook \u2014 Oxford English Dictionary.app/Contents/Resources/zh-Hans.lproj/doMainMenu.nib\" failed: No such file or directory (2)\n```\n\nThen the filesystem completely locked up in the kernel, nothing can be done with it.  After a hard reset, any attempt to mount this filesystem results in a deadlock, necessitating a reset.\n\nIs there a way to \"clear\" this filesystem so that I can mount it again?  I don't think the rsync even made any changes yet, it was just trying to match permissions between files at this point.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/29258528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/29338364", "body": "Any suggestion on how to unbork a filesystem that will no longer mount after experiencing this problem?  I'd like to get some of the data back out of it.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/29338364/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/29361795", "body": "The pool seems to import fine, and if I `zfs set canmount=off` on the filesystem, I can run `zfs mount -a`.  But if I try to mount the problem system itself, zombie.  I'll run `zdb` when I get back home from my vacation.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/29361795/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/29822900", "body": "@dweeezil Here is the zdb dump of my delete queue on the affected filesystem:\n\n```\nDataset tank/Data/Machines [ZPL], ID 11099, cr_txg 27186, 233G, 8601 objects, rootbp DVA[0]=<5:c00002600:200> DVA[1]=<0:1780013000:1000> [L0 DMU objset] fletcher4 lzjb LE contiguous unique double size=800L/200P birth=3884808L/3884808P fill=8601 cksum=19c37601ae:764c9b9848f:1336917f1dfe8:24ca16a5f5b226\n\n    Object  lvl   iblk   dblk  dsize  lsize   %full  type\n         3    1    16K    512     8K    512  100.00  ZFS delete queue\n    dnode flags: USED_BYTES USERUSED_ACCOUNTED \n    dnode maxblkid: 0\n    microzap: 512 bytes, 0 entries\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/29822900/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31685679", "body": "@dweeezil I've tried both of the `echo` commands you mentioned, but I didn't see any traces.  Are they accumulated to a special file?\n\nAlso, this issue of filesystems becoming unmountable is far too easy to trigger.  I just had it happen again by running \"chown btsync .*\" in a nearly unpopulated filesystem.  Now I have two filesystems that I cannot access anymore.  Is there a way to \"reset\" the filesystem so that I can mount it and recover the data?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31685679/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31686478", "body": "@dweeezil I found the traces.  I'm seeing the exact same behavior as #1911.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31686478/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32327994", "body": "No objection.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32327994/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/28236248", "body": "I'd like to note that I'm seeing this same speed problem (multi-second pauses in the same types of calls to `ioctl`), but I have no L2ARC configured in the pool.  I have 16G of RAM, of which `free` is reporting half unused, and yet I still get the same pauses every time I run `zpool status`.  If you'd like me to debug further in this simpler scenario, let me know.  The pool is composed of 5 mirrored pairs, ashift=12, with two of the pairs being 3TB drives, and the other three pairs being 2TB drives.  Running `zpool status` often takes upward of 15 seconds to run.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/28236248/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31687359", "body": "I can also confirm that turning ZIL replay off allowed me to mount some filesystems that otherwise hung in the kernel exactly as @sanmai-NL described.  Is there any reason why this issue has been closed?  This is still happening with the latest version of ZOL.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31687359/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31687716", "body": "Here is all that it takes to reproduce this on my fileserver:\n\n```\nroot@titan:~# cd /tank/Data/Downloads/\nroot@titan:/tank/Data/Downloads# ls -ltra\ntotal 89\n-rw-r--r--  1 johnw dialout     0 Sep 21  2012 .localized\ndrwxrwxrwt  3 johnw dialout     3 Sep 26  2012 .TemporaryItems\ndrwxrwxrwt  3 johnw dialout     3 Apr 29  2013 .Trashes\n-rw-r--r--  1 johnw dialout 12292 Jun 15  2013 .DS_Store\n-rwxr-xr-x  1 johnw dialout   141 Nov  6 04:12 frob\ndrwx------  2 johnw dialout    15 Nov  7 20:14 .fseventsd\ndrwxr-xr-x  5 johnw dialout     8 Nov  8 03:33 .\ndrwxr-xr-x 15 johnw dialout    18 Dec 16 11:46 ..\ndr-xr-xr-x  1 root  root        0 Jan  6 15:11 .zfs\nroot@titan:/tank/Data/Downloads# chown btsync .\nchown: changing ownership of `.': No such file or directory\nroot@titan:/tank/Data/Downloads# ls -la\ntotal 89\ndrwxr-xr-x  5 btsync dialout     8 Nov  8 03:33 .\ndrwxr-xr-x 15 johnw  dialout    18 Dec 16 11:46 ..\n-rw-r--r--  1 johnw  dialout 12292 Jun 15  2013 .DS_Store\n-rwxr-xr-x  1 johnw  dialout   141 Nov  6 04:12 frob\ndrwx------  2 johnw  dialout    15 Nov  7 20:14 .fseventsd\n-rw-r--r--  1 johnw  dialout     0 Sep 21  2012 .localized\ndrwxrwxrwt  3 johnw  dialout     3 Sep 26  2012 .TemporaryItems\ndrwxrwxrwt  3 johnw  dialout     3 Apr 29  2013 .Trashes\ndr-xr-xr-x  1 root   root        0 Jan  6 15:11 .zfs\nroot@titan:/tank/Data/Downloads# chown btsync /tank/Data/Downloads\nls -la\n```\n\nThe server is now out to lunch, and must be force restarted, and the ZIL disabled in order to mount the filesystem.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31687716/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31703470", "body": "Ack, I should have kept that filesystem, I have since destroyed it in order to move forward with my use of the filesystem.  I'm using `xattr=on`, if that helps.  `ls -di` does show a 4.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31703470/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31810775", "body": "Just to note, my pool came from ZEVO as well.  Today I wiped the pool entirely and recreated all the filesystems, so I'll be back if the hanging behavior continues with the all-ZOL solution.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31810775/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31705126", "body": "And a bit later:\n\n```\nJan  6 19:17:07 titan kernel: [ 1080.103872] INFO: task dropbox:4302 blocked for more than 120 seconds.\nJan  6 19:17:07 titan kernel: [ 1080.103902] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nJan  6 19:17:07 titan kernel: [ 1080.103948] dropbox         D ffff880424624320     0  4302      1 0x00000000\nJan  6 19:17:07 titan kernel: [ 1080.103953]  ffff8803ef1bfb38 0000000000000086 ffff8803ef1bfbd8 ffff88043fc13ec0\nJan  6 19:17:07 titan kernel: [ 1080.103958]  ffff8803ef1bffd8 ffff8803ef1bffd8 ffff8803ef1bffd8 0000000000013ec0\nJan  6 19:17:07 titan kernel: [ 1080.103963]  ffffffff81c15440 ffff8803ef1c0000 ffff8803ef1bfb48 ffff88041d878af0\nJan  6 19:17:07 titan kernel: [ 1080.103968] Call Trace:\nJan  6 19:17:07 titan kernel: [ 1080.103974]  [<ffffffff816f4169>] schedule+0x29/0x70\nJan  6 19:17:07 titan kernel: [ 1080.103987]  [<ffffffffa009d74d>] cv_wait_common+0xfd/0x1b0 [spl]\nJan  6 19:17:07 titan kernel: [ 1080.103992]  [<ffffffff8107fde0>] ? add_wait_queue+0x60/0x60\nJan  6 19:17:07 titan kernel: [ 1080.104005]  [<ffffffffa009d855>] __cv_wait+0x15/0x20 [spl]\nJan  6 19:17:07 titan kernel: [ 1080.104053]  [<ffffffffa01e3a3b>] txg_wait_open+0x8b/0x110 [zfs]\nJan  6 19:17:07 titan kernel: [ 1080.104088]  [<ffffffffa01a794d>] dmu_tx_wait+0xed/0xf0 [zfs]\nJan  6 19:17:07 titan kernel: [ 1080.104136]  [<ffffffffa021c467>] zfs_write+0x3a7/0xc70 [zfs]\nJan  6 19:17:07 titan kernel: [ 1080.104141]  [<ffffffff81108515>] ? tracing_is_on+0x15/0x40\nJan  6 19:17:07 titan kernel: [ 1080.104147]  [<ffffffff8108f5f5>] ? check_preempt_curr+0x75/0xa0\nJan  6 19:17:07 titan kernel: [ 1080.104153]  [<ffffffff81092080>] ? wake_up_state+0x10/0x20\nJan  6 19:17:07 titan kernel: [ 1080.104158]  [<ffffffff810b8306>] ? wake_futex+0x76/0xa0\nJan  6 19:17:07 titan kernel: [ 1080.104162]  [<ffffffff810b9a46>] ? futex_wake_op+0x3b6/0x540\nJan  6 19:17:07 titan kernel: [ 1080.104209]  [<ffffffffa0231422>] zpl_write_common+0x52/0x80 [zfs]\nJan  6 19:17:07 titan kernel: [ 1080.104256]  [<ffffffffa02314b8>] zpl_write+0x68/0xa0 [zfs]\nJan  6 19:17:07 titan kernel: [ 1080.104261]  [<ffffffff8119bd23>] vfs_write+0xb3/0x180\nJan  6 19:17:07 titan kernel: [ 1080.104266]  [<ffffffff8119c062>] sys_write+0x52/0xa0\nJan  6 19:17:07 titan kernel: [ 1080.104271]  [<ffffffff816f95ce>] ? do_page_fault+0xe/0x10\nJan  6 19:17:07 titan kernel: [ 1080.104276]  [<ffffffff816fdf1d>] system_call_fastpath+0x1a/0x1f\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31705126/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31705140", "body": "(That last trace was for a process that was not in the affected filesystem, which may describe why it's different?)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31705140/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31707322", "body": "I cannot export the pool now either after this 3rd filesystem corruption.  The export attempt dies with:\n\n```\nJan  6 20:01:57 titan kernel: [  240.535617] INFO: task zpool:2602 blocked for more than 120 seconds.\nJan  6 20:01:57 titan kernel: [  240.535672] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nJan  6 20:01:57 titan kernel: [  240.535726] zpool           D ffff880424db5020     0  2602   2539 0x00000000\nJan  6 20:01:57 titan kernel: [  240.535732]  ffff88041e5d76e8 0000000000000086 0000000000000001 ffff88043fc53ec0\nJan  6 20:01:57 titan kernel: [  240.535738]  ffff88041e5d7fd8 ffff88041e5d7fd8 ffff88041e5d7fd8 0000000000013ec0\nJan  6 20:01:57 titan kernel: [  240.535743]  ffff880429541740 ffff8804255a8000 ffff88041e5d76f8 ffff88041e78ea60\nJan  6 20:01:57 titan kernel: [  240.535749] Call Trace:\nJan  6 20:01:57 titan kernel: [  240.535760]  [<ffffffff816f4169>] schedule+0x29/0x70\nJan  6 20:01:57 titan kernel: [  240.535780]  [<ffffffffa00e074d>] cv_wait_common+0xfd/0x1b0 [spl]\nJan  6 20:01:57 titan kernel: [  240.535787]  [<ffffffff8107fde0>] ? add_wait_queue+0x60/0x60\nJan  6 20:01:57 titan kernel: [  240.535800]  [<ffffffffa00e0855>] __cv_wait+0x15/0x20 [spl]\nJan  6 20:01:57 titan kernel: [  240.535855]  [<ffffffffa01deb73>] txg_wait_synced+0xb3/0x190 [zfs]\nJan  6 20:01:57 titan kernel: [  240.535902]  [<ffffffffa02323c0>] ? dsl_destroy_head_check_impl+0xf0/0xf0 [zfs]\nJan  6 20:01:57 titan kernel: [  240.535949]  [<ffffffffa0233920>] ? dsl_destroy_head_sync_impl+0x910/0x910 [zfs]\nJan  6 20:01:57 titan kernel: [  240.535991]  [<ffffffffa01bf5f3>] dsl_sync_task+0xf3/0x260 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536037]  [<ffffffffa02323c0>] ? dsl_destroy_head_check_impl+0xf0/0xf0 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536084]  [<ffffffffa0233920>] ? dsl_destroy_head_sync_impl+0x910/0x910 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536130]  [<ffffffffa0231d6d>] dsl_destroy_head+0xcd/0x150 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536176]  [<ffffffffa0231e62>] dsl_destroy_inconsistent+0x72/0x80 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536209]  [<ffffffffa01998ba>] dmu_objset_find_impl+0x10a/0x3f0 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536255]  [<ffffffffa0231df0>] ? dsl_destroy_head+0x150/0x150 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536299]  [<ffffffffa01c65d4>] ? rrw_exit+0x44/0x150 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536331]  [<ffffffffa019996e>] dmu_objset_find_impl+0x1be/0x3f0 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536377]  [<ffffffffa0231df0>] ? dsl_destroy_head+0x150/0x150 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536423]  [<ffffffffa0231df0>] ? dsl_destroy_head+0x150/0x150 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536455]  [<ffffffffa0199bf2>] dmu_objset_find+0x52/0x80 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536502]  [<ffffffffa01d306b>] spa_load+0x16fb/0x1a10 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536550]  [<ffffffffa01dfebe>] ? txg_list_create+0x2e/0x60 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536596]  [<ffffffffa01d2368>] spa_load+0x9f8/0x1a10 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536602]  [<ffffffff8107fde0>] ? add_wait_queue+0x60/0x60\nJan  6 20:01:57 titan kernel: [  240.536647]  [<ffffffffa01cbb38>] ? spa_activate+0x138/0x400 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536693]  [<ffffffffa01d33ce>] spa_load_best+0x4e/0x250 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536740]  [<ffffffffa01d371f>] spa_open_common+0x14f/0x370 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536786]  [<ffffffffa01d3953>] spa_open+0x13/0x20 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536835]  [<ffffffffa0207360>] pool_status_check+0x40/0xa0 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536883]  [<ffffffffa0207762>] zfsdev_ioctl+0x3a2/0x530 [zfs]\nJan  6 20:01:57 titan kernel: [  240.536889]  [<ffffffff811adafa>] do_vfs_ioctl+0x8a/0x340\nJan  6 20:01:57 titan kernel: [  240.536894]  [<ffffffff81165e28>] ? do_munmap+0x248/0x2f0\nJan  6 20:01:57 titan kernel: [  240.536898]  [<ffffffff811ade41>] sys_ioctl+0x91/0xb0\nJan  6 20:01:57 titan kernel: [  240.536904]  [<ffffffff816fdf1d>] system_call_fastpath+0x1a/0x1f\n\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31707322/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31707690", "body": "Scratch that, I'm locked out of the entire pool.  Every attempt to use it dies with:\n\n```\nJan  6 20:08:01 titan kernel: [   49.995114] SPLError: 2812:0:(spl-err.c:67:vcmn_err()) WARNING: can't open objset for tank/Archives\n```\n\nFollowed several minutes later by a trace like the one above.\n\nIs there any setting I can tune to at least have a chance of copying the data off?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31707690/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31709824", "body": "I was able to get the data in the end, copying off now.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31709824/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31772885", "body": "@dweeezil Unfortunately, this server is too critical for me to leave it in a broken state.  Since you didn't respond after a few hours, I had to just copy the data off, destroy the entire pool, and restart from scratch in the hopes that something about the old system was just rotten.\n\nThe pool in question was originally created over a year ago on a Mac Pro using ZEVO, and had hundreds of snapshots per filesystem.  Maybe ZOL didn't like something that ZEVO had done?  I'm going to try with a brand new pool, and brand new filesystems, all created by ZOL itself.  If things still hang after that, it will be time to bite the bullet and just move to FreeBSD or OI.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31772885/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31784172", "body": "There were zpool version 28, filesystem version 5.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31784172/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31810724", "body": "I don't know which compiler it was built with.  My system has 4.6.3 installed, but I installed the kernel modules from:\n\n```\nPackage: zfs-dkms\nSource: zfs-linux\nPriority: extra\nSection: kernel\nInstalled-Size: 10527\nMaintainer: Darik Horn <dajhorn@vanadac.com>\nArchitecture: amd64\nVersion: 0.6.2-2~precise~2.gbp8db412\nReplaces: lzfs, lzfs-dkms\nProvides: lustre-backend-fs, lzfs, lzfs-dkms\nDepends: dkms (>= 2.2.0.2)\nConflicts: lzfs, lzfs-dkms\nPre-Depends: spl-dkms (>= 0.6.2)\nFilename: pool/main/z/zfs-linux/zfs-dkms_0.6.2-2~precise~2.gbp8db412_amd64.deb\nSize: 2166440\nMD5sum: fa85f1adabe17e82c050b21675d682ef\nSHA1: a46ba1a1c062466996881566670de37ab522dad2\nSHA256: 007a2ceea89182ccf376ad21abd3c7bf7a71ebb88a181790020948cc71bcdda9\nDescription: Native ZFS filesystem kernel modules for Linux\n An advanced integrated volume manager and filesystem that is designed for\n performance and data integrity. Snapshots, clones, checksums, deduplication,\n compression, and RAID redundancy are built-in features.\n .\n Includes the SPA, DMU, ZVOL, and ZPL components of ZFS.\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31810724/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32131484", "body": "Sounds about right to me.  My ZOL-only pool has been doing just fine so far, so I'm glad I ditched the ZEVO heritage.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32131484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "gaika": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/326816136", "body": "same here, 100% reproducible after zfs send | zfs recv of an encrypted filesystem. zpool scrub is clean, all other operations work fine, only mount fails.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/326816136/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "elan": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/10037601", "body": "3.2.0-32-generic #51-Ubuntu SMP, zfs-0.6.0-rc11\n\nI just saw this today, seemed to make an AFP copy hang for a while, but no ill effects besides that observed:\n\n```\nNov  1 21:06:26 whopper kernel: [30451.753316] umount          D ffffffff81806200     0 21237  21236 0x00000000\nNov  1 21:06:26 whopper kernel: [30451.753322]  ffff880219145d28 0000000000000082 ffff880219145cd8 ffffffffa012d069\nNov  1 21:06:26 whopper kernel: [30451.753327]  ffff880219145fd8 ffff880219145fd8 ffff880219145fd8 00000000000137c0\nNov  1 21:06:26 whopper kernel: [30451.753332]  ffff8804155e9700 ffff880151510000 ffff880219145d38 ffff8804150c8500\nNov  1 21:06:26 whopper kernel: [30451.753337] Call Trace:\nNov  1 21:06:26 whopper kernel: [30451.753368]  [<ffffffffa012d069>] ? dbuf_rele_and_unlock+0x159/0x200 [zfs]\nNov  1 21:06:26 whopper kernel: [30451.753375]  [<ffffffff81658f2f>] schedule+0x3f/0x60\nNov  1 21:06:26 whopper kernel: [30451.753386]  [<ffffffffa006bead>] __taskq_wait_id+0x7d/0x150 [spl]\nNov  1 21:06:26 whopper kernel: [30451.753391]  [<ffffffff811b76b4>] ? fsnotify_clear_marks_by_inode+0x94/0xf0\nNov  1 21:06:26 whopper kernel: [30451.753397]  [<ffffffff8108ab80>] ? add_wait_queue+0x60/0x60\nNov  1 21:06:26 whopper kernel: [30451.753405]  [<ffffffffa006cff3>] __taskq_wait+0x53/0xf0 [spl]\nNov  1 21:06:26 whopper kernel: [30451.753436]  [<ffffffffa0166679>] ? rrw_enter+0x69/0x190 [zfs]\nNov  1 21:06:26 whopper kernel: [30451.753470]  [<ffffffffa01abbfc>] zfs_sb_teardown+0x5c/0x3a0 [zfs]\nNov  1 21:06:26 whopper kernel: [30451.753475]  [<ffffffff81193033>] ? dispose_list+0x23/0x50\nNov  1 21:06:26 whopper kernel: [30451.753505]  [<ffffffffa01abfb0>] zfs_umount+0x30/0xc0 [zfs]\nNov  1 21:06:26 whopper kernel: [30451.753535]  [<ffffffffa01c710e>] zpl_put_super+0xe/0x10 [zfs]\nNov  1 21:06:26 whopper kernel: [30451.753540]  [<ffffffff8117a0e2>] generic_shutdown_super+0x62/0xe0\nNov  1 21:06:26 whopper kernel: [30451.753544]  [<ffffffff8117a1f6>] kill_anon_super+0x16/0x30\nNov  1 21:06:26 whopper kernel: [30451.753572]  [<ffffffffa01c6f0e>] zpl_kill_sb+0x1e/0x30 [zfs]\nNov  1 21:06:26 whopper kernel: [30451.753576]  [<ffffffff8117a83c>] deactivate_locked_super+0x3c/0xa0\nNov  1 21:06:26 whopper kernel: [30451.753580]  [<ffffffff8117b0be>] deactivate_super+0x4e/0x70\nNov  1 21:06:26 whopper kernel: [30451.753584]  [<ffffffff8119773d>] mntput_no_expire+0x9d/0xf0\nNov  1 21:06:26 whopper kernel: [30451.753588]  [<ffffffff81198a6b>] sys_umount+0x5b/0xd0\nNov  1 21:06:26 whopper kernel: [30451.753592]  [<ffffffff81663442>] system_call_fastpath+0x16/0x1b\nNov  1 21:08:26 whopper kernel: [30571.636117] INFO: task umount:21237 blocked for more than 120 seconds.\nNov  1 21:08:26 whopper kernel: [30571.636490] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nNov  1 21:08:26 whopper kernel: [30571.657955] umount          D ffffffff81806200     0 21237  21236 0x00000000\nNov  1 21:08:26 whopper kernel: [30571.657965]  ffff880219145d28 0000000000000082 ffff880219145cd8 ffffffffa012d069\nNov  1 21:08:26 whopper kernel: [30571.657978]  ffff880219145fd8 ffff880219145fd8 ffff880219145fd8 00000000000137c0\nNov  1 21:08:26 whopper kernel: [30571.657990]  ffff8804155e9700 ffff880151510000 ffff880219145d38 ffff8804150c8500\nNov  1 21:08:26 whopper kernel: [30571.658003] Call Trace:\nNov  1 21:08:26 whopper kernel: [30571.658044]  [<ffffffffa012d069>] ? dbuf_rele_and_unlock+0x159/0x200 [zfs]\nNov  1 21:08:26 whopper kernel: [30571.658058]  [<ffffffff81658f2f>] schedule+0x3f/0x60\nNov  1 21:08:26 whopper kernel: [30571.658075]  [<ffffffffa006bead>] __taskq_wait_id+0x7d/0x150 [spl]\nNov  1 21:08:26 whopper kernel: [30571.658083]  [<ffffffff811b76b4>] ? fsnotify_clear_marks_by_inode+0x94/0xf0\nNov  1 21:08:26 whopper kernel: [30571.658091]  [<ffffffff8108ab80>] ? add_wait_queue+0x60/0x60\nNov  1 21:08:26 whopper kernel: [30571.658102]  [<ffffffffa006cff3>] __taskq_wait+0x53/0xf0 [spl]\nNov  1 21:08:26 whopper kernel: [30571.658137]  [<ffffffffa0166679>] ? rrw_enter+0x69/0x190 [zfs]\nNov  1 21:08:26 whopper kernel: [30571.658174]  [<ffffffffa01abbfc>] zfs_sb_teardown+0x5c/0x3a0 [zfs]\nNov  1 21:08:26 whopper kernel: [30571.658181]  [<ffffffff81193033>] ? dispose_list+0x23/0x50\nNov  1 21:08:26 whopper kernel: [30571.658214]  [<ffffffffa01abfb0>] zfs_umount+0x30/0xc0 [zfs]\nNov  1 21:08:26 whopper kernel: [30571.658247]  [<ffffffffa01c710e>] zpl_put_super+0xe/0x10 [zfs]\nNov  1 21:08:26 whopper kernel: [30571.658254]  [<ffffffff8117a0e2>] generic_shutdown_super+0x62/0xe0\nNov  1 21:08:26 whopper kernel: [30571.658261]  [<ffffffff8117a1f6>] kill_anon_super+0x16/0x30\nNov  1 21:08:26 whopper kernel: [30571.658292]  [<ffffffffa01c6f0e>] zpl_kill_sb+0x1e/0x30 [zfs]\nNov  1 21:08:26 whopper kernel: [30571.658299]  [<ffffffff8117a83c>] deactivate_locked_super+0x3c/0xa0\nNov  1 21:08:26 whopper kernel: [30571.658305]  [<ffffffff8117b0be>] deactivate_super+0x4e/0x70\nNov  1 21:08:26 whopper kernel: [30571.658312]  [<ffffffff8119773d>] mntput_no_expire+0x9d/0xf0\nNov  1 21:08:26 whopper kernel: [30571.658319]  [<ffffffff81198a6b>] sys_umount+0x5b/0xd0\nNov  1 21:08:26 whopper kernel: [30571.658325]  [<ffffffff81663442>] system_call_fastpath+0x16/0x1b\nNov  1 21:10:26 whopper kernel: [30691.539760] INFO: task umount:21237 blocked for more than 120 seconds.\nNov  1 21:10:26 whopper kernel: [30691.551243] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nNov  1 21:10:26 whopper kernel: [30691.573174] umount          D ffffffff81806200     0 21237  21236 0x00000000\nNov  1 21:10:26 whopper kernel: [30691.573180]  ffff880219145d28 0000000000000082 ffff880219145cd8 ffffffffa012d069\nNov  1 21:10:26 whopper kernel: [30691.573186]  ffff880219145fd8 ffff880219145fd8 ffff880219145fd8 00000000000137c0\nNov  1 21:10:26 whopper kernel: [30691.573190]  ffff8804155e9700 ffff880151510000 ffff880219145d38 ffff8804150c8500\nNov  1 21:10:26 whopper kernel: [30691.573195] Call Trace:\nNov  1 21:10:26 whopper kernel: [30691.573228]  [<ffffffffa012d069>] ? dbuf_rele_and_unlock+0x159/0x200 [zfs]\nNov  1 21:10:26 whopper kernel: [30691.573239]  [<ffffffff81658f2f>] schedule+0x3f/0x60\nNov  1 21:10:26 whopper kernel: [30691.573253]  [<ffffffffa006bead>] __taskq_wait_id+0x7d/0x150 [spl]\nNov  1 21:10:26 whopper kernel: [30691.573260]  [<ffffffff811b76b4>] ? fsnotify_clear_marks_by_inode+0x94/0xf0\nNov  1 21:10:26 whopper kernel: [30691.573269]  [<ffffffff8108ab80>] ? add_wait_queue+0x60/0x60\nNov  1 21:10:26 whopper kernel: [30691.573280]  [<ffffffffa006cff3>] __taskq_wait+0x53/0xf0 [spl]\nNov  1 21:10:26 whopper kernel: [30691.573314]  [<ffffffffa0166679>] ? rrw_enter+0x69/0x190 [zfs]\nNov  1 21:10:26 whopper kernel: [30691.573350]  [<ffffffffa01abbfc>] zfs_sb_teardown+0x5c/0x3a0 [zfs]\nNov  1 21:10:26 whopper kernel: [30691.573357]  [<ffffffff81193033>] ? dispose_list+0x23/0x50\nNov  1 21:10:26 whopper kernel: [30691.573390]  [<ffffffffa01abfb0>] zfs_umount+0x30/0xc0 [zfs]\nNov  1 21:10:26 whopper kernel: [30691.573424]  [<ffffffffa01c710e>] zpl_put_super+0xe/0x10 [zfs]\nNov  1 21:10:26 whopper kernel: [30691.573431]  [<ffffffff8117a0e2>] generic_shutdown_super+0x62/0xe0\nNov  1 21:10:26 whopper kernel: [30691.573438]  [<ffffffff8117a1f6>] kill_anon_super+0x16/0x30\nNov  1 21:10:26 whopper kernel: [30691.573469]  [<ffffffffa01c6f0e>] zpl_kill_sb+0x1e/0x30 [zfs]\nNov  1 21:10:26 whopper kernel: [30691.573476]  [<ffffffff8117a83c>] deactivate_locked_super+0x3c/0xa0\nNov  1 21:10:26 whopper kernel: [30691.573482]  [<ffffffff8117b0be>] deactivate_super+0x4e/0x70\nNov  1 21:10:26 whopper kernel: [30691.573489]  [<ffffffff8119773d>] mntput_no_expire+0x9d/0xf0\nNov  1 21:10:26 whopper kernel: [30691.573496]  [<ffffffff81198a6b>] sys_umount+0x5b/0xd0\nNov  1 21:10:26 whopper kernel: [30691.573503]  [<ffffffff81663442>] system_call_fastpath+0x16/0x1b\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/10037601/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "contentfree": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/40128951", "body": "So what was the fix/workaround/recovery here? Do we uninstall zfs-dkms (which uninstalls zfs) and then reinstall zfs?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/40128951/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "antifuchs": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/215142437", "body": "@kpeterson11 - I have run into this exact (`EINVAL` issue) myself on ubuntu 16.04. I believe this particular case might be related to #2718, but I haven't yet managed to fully verify it is.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/215142437/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "myers": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/4146696", "body": "In this [thread](http://groups.google.com/a/zfsonlinux.org/group/zfs-discuss/browse_thread/thread/649b5a56a370e607/4f282299c84650a1?lnk=raot#4f282299c84650a1) it was suggested to up the /proc/sys/vm/min_free_kbytes \n\necho 135168 > /proc/sys/vm/min_free_kbytes \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/4146696/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "three3q": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/173145042", "body": "can confirm problem after fedup 20->21 still exists. had to reinstall spl zfs and then manually yum reinstall for spl-dkms and zfs-dkms.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/173145042/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "maxigs": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/226923756", "body": "Thanks @tuxoko, indeed error message seems similar. But in my case it even needed a bit more memory.\n\nI tried to set spl_kmem_alloc_max to 12M now, hoping this would be enough.\nNo idea if it was supposed to change anything.\n\nI assume my zpool is still unrecoverable? \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/226923756/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/227015882", "body": "I'm closing it, i gave up on the old zpool, and the bug itself already is addressed.\n\nThanks guys :)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/227015882/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "stef": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45609476", "body": "http://www.musl-libc.org/ - not all distros ship glibc\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45609476/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45681577", "body": "I don't know if it's the easiest way, but alpine linux comes with musl and has ZoL in their testing repo, where it was disabled i believe around linux-3.13. I fiddled with that package and updated it to the latest git version. So what i can recommend is:\n1. install alpine linux in your fav vm from (takes 4min, the iso is only 80MB):\nwget http://nl.alpinelinux.org/alpine/edge/releases/x86_64/alpine-edge-3.0.0-x86_64.iso\n2. install git, and from git clone http://github.com/stef/aports\n3. go to aports/testing/spl-grsec and run abuild -r - afterwards apk add spl-grsec\n4. go to aports/testing/zfs-grsec and run abuild -r - afterwards apk add zfs-grsec\n\nit seems to be running for me. if you want i can provide you with a qemu image where this has been done. i guess you want to compile yourself, that's why i didn't offer the binary package, which is of course available if you want to try that.\n\nwhich of the currently possible routes you prefer?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45681577/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/46949153", "body": "if you take the package from alpine linux, then the arch is disabled, because zfs-grsec does not build. so you have to uncomment the arch line in APKBUILD yourself. in my repo this does not have to be done.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/46949153/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/51759777", "body": "working on it.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/51759777/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/52406820", "body": "i have simplified the patch and published a clean version in my aports repo. also i'm closing this pull request down and create a new one with the content of the aports repo.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/52406820/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/52408864", "body": "check out the follow-up pull request: https://github.com/zfsonlinux/zfs/pull/2604\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/52408864/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/55103543", "body": "On Sat, Aug 16, 2014 at 04:49:59PM -0700, Hinnerk van Bruinehsen wrote:\n\n> I'm not affiliated with ZoL but I ask myself why you didn't just force-pushed your changes to this PR to update it, but created a new PR instead?\n\ni did. yes. this repo is dedicated to this one purpose only, to create a patch\nthat makes musl work.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/55103543/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/52421253", "body": "will try to fix the results of this: http://buildbot.zfsonlinux.org/console?branch=refs/pull/2604/head\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/52421253/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/55512101", "body": "please consider the follow-up patch at: https://github.com/zfsonlinux/zfs/pull/2699\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/55512101/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/120970539", "body": "re: licensing of libtirpc\n\nryao commented on config/musl.m4 in 20e5610 on Aug 18, 2014\n\n> libtirpc is GPLv2 licensed. We cannot link to it in a manner that permits redistribution. We will need to port the Transport-Independent RPC library from Illumos.\n\nand later:\n\nryao commented on module/nvpair/nvpair.c in 20e5610 on Aug 18, 2014\n\n> ...\n> Unfortunately, we cannot do that due to the library license.\n\nit seems that libtirpc is in fact 3 clause BSD licensed, and the license is wrong in the alpine linux APKBUILD file.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/120970539/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/161450977", "body": "i'm not sure i have the time in the short term for this.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/161450977/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/54753955", "body": "sorry for being absent for a while. how should we fix up, merge and finalize this and my patch? thank you all so much for the valuable comments.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/54753955/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "unya": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/64227710", "body": "Except richacls are not NFSv4 ACLs, they are (bit insane) result of merging NFSv4 schema with POSIX ACLs, designed for ext4 and retaining all the worst parts of POSIX ACLs, IIRC.\n\nWhat we need is a proper interface for NFSv4 ACLs, so that filesystems that support them can have them set. Mind you, there's at least one more type of ACLs supported (partially at least) by linux - AFS ACLs. So the possibility of having multiple schemes supported is not insane, though I guess we might need a Solaris-like API to support it best...\n\nOf course, if richacls can be wrangled to stop all the parts that go outside NFSv4, and assuming userland doesn't screw you over (hello there, POSIX ACL mask bits!), and assuming that they actually implement all of NFSv4 spec... That's a lot of assumptions, to be honest.\n\nI would actually propose adding an IOCTL applicable to files on ZFS at this rate\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/64227710/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/24300923", "body": "A bit from me (Sabayon amd64, distro kernel v3.8, standard packages, zfs 0.6.1-r3)\n    $ zfs create -o quota=100M RPOOL/opt/foo\n    $ zfs set devices=yes RPOOL/opt/foo\n    $ dd if=/dev/urandom of=/opt/foo/filler bs=128k\n    $ mkfifo /opt/foo/fifo\n    mkfifo: cannot create fifo \u2018/opt/foo/fifo\u2019\n    $ strace mkfifo /opt/foo/fifo\n\n> execve(\"/usr/bin/mkfifo\", [\"mkfifo\", \"/opt/foo/fifo\"], [/\\* 93 vars */]) = 0\n> brk(0)                                  = 0x7f749b0b9000\n> mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f7499b2d000\n> access(\"/etc/ld.so.preload\", R_OK)      = -1 ENOENT (No such file or directory)\n> open(\"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) = 3\n> fstat(3, {st_mode=S_IFREG|0644, st_size=268788, ...}) = 0\n> mmap(NULL, 268788, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f7499aeb000\n> close(3)                                = 0\n> open(\"/lib64/libc.so.6\", O_RDONLY|O_CLOEXEC) = 3\n> read(3, \"\\177ELF\\2\\1\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\3\\0>\\0\\1\\0\\0\\0PM\\2\\0\\0\\0\\0\\0\"..., 832) = 832\n> fstat(3, {st_mode=S_IFREG|0755, st_size=1753240, ...}) = 0\n> mmap(NULL, 3866496, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f749955d000\n> mprotect(0x7f7499704000, 2093056, PROT_NONE) = 0\n> mmap(0x7f7499903000, 24576, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x1a6000) = 0x7f7499903000\n> mmap(0x7f7499909000, 16256, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0x7f7499909000\n> close(3)                                = 0\n> mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f7499aea000\n> mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f7499ae9000\n> mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f7499ae8000\n> arch_prctl(ARCH_SET_FS, 0x7f7499ae9700) = 0\n> mprotect(0x7f7499903000, 16384, PROT_READ) = 0\n> mprotect(0x7f7499d38000, 4096, PROT_READ) = 0\n> mprotect(0x7f7499b2e000, 4096, PROT_READ) = 0\n> munmap(0x7f7499aeb000, 268788)          = 0\n> brk(0)                                  = 0x7f749b0b9000\n> brk(0x7f749b0da000)                     = 0x7f749b0da000\n> open(\"/usr/lib64/locale/locale-archive\", O_RDONLY|O_CLOEXEC) = 3\n> fstat(3, {st_mode=S_IFREG|0644, st_size=4377408, ...}) = 0\n> mmap(NULL, 4377408, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f7499130000\n> close(3)                                = 0\n\nHere comes the fun!\n\n> mknod(\"/opt/foo/fifo\", S_IFIFO|0666)    = 122\n\n122 is utter bullshit according to spec!\n\n> open(\"/usr/share/locale/locale.alias\", O_RDONLY|O_CLOEXEC) = 3\n> fcntl(3, F_GETFD)                       = 0x1 (flags FD_CLOEXEC)\n> fstat(3, {st_mode=S_IFREG|0644, st_size=2502, ...}) = 0\n> mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f7499b2c000\n> read(3, \"# Locale name alias data base.\\n#\"..., 2560) = 2502\n> read(3, \"\", 2560)                       = 0\n> close(3)                                = 0\n> munmap(0x7f7499b2c000, 4096)            = 0\n> open(\"/usr/share/locale/en_US.UTF-8/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\n> open(\"/usr/share/locale/en_US.utf8/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\n> open(\"/usr/share/locale/en_US/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\n> open(\"/usr/share/locale/en.UTF-8/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\n> open(\"/usr/share/locale/en.utf8/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\n> open(\"/usr/share/locale/en/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\n> open(\"/usr/lib64/charset.alias\", O_RDONLY|O_NOFOLLOW) = -1 ENOENT (No such file or directory)\n> write(2, \"mkfifo: \", 8mkfifo: )                 = 8\n> write(2, \"cannot create fifo \\342\\200\\230/opt/foo/f\"..., 38cannot create fifo \u2018/opt/foo/fifo\u2019) = 38\n> write(2, \"\\n\", 1)                       = 1\n> close(1)                                = 0\n> close(2)                                = 0\n> exit_group(1)                           = ?\n> +++ exited with 1 +++\n\nThe \"no such file or directory\" output seems to me to be an issue in whatever the hell Ubuntu did to their coreutils \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/24300923/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45084856", "body": "I would like to add that systemd is not the only thing benefitting from non-forking model - many other supervisor packages handle such services better, and forking behaviour can be retained as an option.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45084856/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "hadees": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/156218804", "body": "Awesome! I've really been wanting to write a web interface for my box and this will make it way easier. :+1: \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/156218804/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/162764220", "body": "@yada is this live yet? How do I use it?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/162764220/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/163054503", "body": "Here is my `zfs get all`\n\n```\nNAME  PROPERTY              VALUE                  SOURCE\ntank  type                  filesystem             -\ntank  creation              Thu Mar  5  0:27 2015  -\ntank  used                  29.5T                  -\ntank  available             15.4T                  -\ntank  referenced            29.5T                  -\ntank  compressratio         1.00x                  -\ntank  mounted               yes                    -\ntank  quota                 none                   default\ntank  reservation           none                   default\ntank  recordsize            128K                   default\ntank  mountpoint            /mnt/raid              local\ntank  sharenfs              off                    local\ntank  checksum              on                     default\ntank  compression           off                    local\ntank  atime                 on                     default\ntank  devices               on                     default\ntank  exec                  on                     default\ntank  setuid                on                     default\ntank  readonly              off                    default\ntank  zoned                 off                    default\ntank  snapdir               hidden                 default\ntank  aclinherit            restricted             default\ntank  canmount              on                     default\ntank  xattr                 on                     default\ntank  copies                1                      default\ntank  version               5                      -\ntank  utf8only              off                    -\ntank  normalization         none                   -\ntank  casesensitivity       sensitive              -\ntank  vscan                 off                    default\ntank  nbmand                off                    default\ntank  sharesmb              off                    default\ntank  refquota              none                   default\ntank  refreservation        none                   default\ntank  primarycache          all                    default\ntank  secondarycache        all                    default\ntank  usedbysnapshots       0                      -\ntank  usedbydataset         29.5T                  -\ntank  usedbychildren        2.69G                  -\ntank  usedbyrefreservation  0                      -\ntank  logbias               latency                default\ntank  dedup                 off                    default\ntank  mlslabel              none                   default\ntank  sync                  standard               default\ntank  refcompressratio      1.00x                  -\ntank  written               29.5T                  -\ntank  logicalused           29.4T                  -\ntank  logicalreferenced     29.4T                  -\ntank  filesystem_limit      none                   default\ntank  snapshot_limit        none                   default\ntank  filesystem_count      none                   default\ntank  snapshot_count        none                   default\ntank  snapdev               hidden                 default\ntank  acltype               off                    default\ntank  context               none                   default\ntank  fscontext             none                   default\ntank  defcontext            none                   default\ntank  rootcontext           none                   default\ntank  relatime              on                     temporary\ntank  redundant_metadata    all                    default\ntank  overlay               off                    default\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/163054503/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dpavlin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/123294202", "body": "I'm creating filesystem with\n\n`zfs create -V 60G -b 4k -s klinz/block/blabla`\n\nHowever, I'm not getting warning in dmesg after that command, but instead after\n\n`mkfs.ext4 -m 0 -L blabla /dev/zvol/klinz/block/blabla`\n\nI also noticed that mkfs.ext4 does finish and create file-system, but label never appears in `/dev/disk/by-label/`. I'm not sure if this is zfs related or not, but it did work correctly with last zfs Debian package revision. Kernel used is `Linux klin 4.0.0-2-amd64 #1 SMP Debian 4.0.8-1 (2015-07-11) x86_64 GNU/Linux` from Debian sid. I don't think I updated it since it worked list time, and even if I did it was some version, but newer build.\n\nI have script which waits for label of new file-system to appear, so this is somewhat a show-stopper for me.\n\nWith sparse and default volblocksize of 8K i'm getting also warning for for smaller allocation: `Large kmem_alloc(65536, 0x1000)` and same backtrace.\n\nRemoving sparse option to zfs create doesn't change warnings.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/123294202/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/123401348", "body": "I tried similar test on `Linux mlin 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64 GNU/Linux` on Debian jessie (current stable) and I'm not seeing any warnings in dmesg nor problems with labels. So it seems that it's a kernel problem (perhaps) and not zfsonlinux one. Feel free to close this issue if you agree with this.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/123401348/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ehamberg": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45441793", "body": "I switched to zfs-fuse on that machine, so I don't know if it was fixed. Since no one else has reported anything in two years it's probably safe to close it (and let people re-open if they hit this in recent versions).\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45441793/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "eatnumber1": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/11963093", "body": "I'm still seeing this issue.\n\nIs this even supposed to be supported? The zfs(8) man page says no.\n\n```\nZFS Volumes as Swap\n    Do not swap to a file on a ZFS file system. A ZFS swap file configuration is not supported.\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/11963093/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/13265823", "body": "FYI, I believe there are some bugs relating to swapping to a zvol which has compression enabled. I was getting some fairly frequent deadlocks when in tight memory conditions until I disabled compression.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/13265823/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/12193669", "body": "I'm having difficulty easily reproducing the issue, despite the fact that I get it a lot. Here's some traces from the last time I got the issue (which is right now).\n\nMy system is currently resilvering.\n\n```\n<6>[200753.852099] SysRq : Show backtrace of all active CPUs\n<6>[200753.852173] sending NMI to all CPUs:\n<4>[200753.852221] NMI backtrace for cpu 0\n<4>[200753.852263] CPU 0 \n<4>[200753.852289] Modules linked in: vhost_net macvtap macvlan ebtable_nat ebtables pci_stub vboxpci(O) vboxnetadp(O) vboxnetflt(O) vboxdrv(O) tun xfrm_user ip6table_mangle ip6table_filter ip6_tables xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 xt_DSCP xt_comment xt_owner iptable_mangle ipt_REJECT xt_tcpudp binfmt_misc iptable_filter ip_tables x_tables deflate twofish_generic ctr twofish_x86_64_3way xts lrw gf128mul twofish_x86_64 twofish_common blowfish_generic camellia_generic serpent_generic blowfish_x86_64 blowfish_common cast5 des_generic cbc xcbc rmd160 sha512_generic sha256_generic sha1_ssse3 sha1_generic hmac crypto_null af_key xfrm_algo fuse rpcsec_gss_krb5 nfsd nfs nfs_acl auth_rpcgss fscache lockd sunrpc ext2 mbcache dm_crypt bridge stp llc tcp_lp i2c_dev eeprom msr cpuid jc42 w83795 w83627ehf hwmon_vid w83627hf_wdt nouveau coretemp kvm_intel snd_pcm_oss video ttm snd_mixer_oss drm_kms_helper snd_pcm kvm drm snd_page_alloc snd_timer crc32c_intel i2c_algo_bit snd ghash_clmulni_intel aesni_intel mxm_wmi wmi soundcore ioatdma i7core_edac i2c_i801 shpchp aes_x86_64 edac_core lpc_ich aes_generic i2c_core mfd_core acpi_cpufreq pci_hotplug cryptd evdev mperf dca processor microcode thermal_sys button pcspkr btrfs crc32c libcrc32c raid1 raid0 md_mod zfs(PO) zunicode(PO) zavl(PO) zcommon(PO) znvpair(PO) spl(O) zlib_deflate usbhid hid ohci_hcd dm_mirror dm_region_hash dm_log dm_mod sr_mod cdrom sg sd_mod crc_t10dif usb_storage uhci_hcd sata_mv firewire_ohci firewire_core crc_itu_t ehci_hcd usbcore e1000e usb_common ahci libahci libata scsi_mod [last unloaded: scsi_wait_scan]\n<4>[200753.856483] \n<4>[200753.856520] Pid: 7184, comm: tcpconsole Tainted: P           O 3.5.2-zeniba #3 Supermicro X8SAX/X8SAX\n<4>[200753.856650] RIP: 0010:[<ffffffff8102a94f>]  [<ffffffff8102a94f>] native_apic_mem_read+0xc/0xe\n<4>[200753.856741] RSP: 0018:ffff8802c1139d78  EFLAGS: 00000046\n<4>[200753.856787] RAX: 0000000000001400 RBX: 00000000000003e9 RCX: 0000000000000001\n<4>[200753.856849] RDX: 00000000000000fe RSI: 0000000000000002 RDI: 0000000000000300\n<4>[200753.856911] RBP: ffff8802c1139d78 R08: ffffffff81686f60 R09: 0000000000000000\n<4>[200753.856973] R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000002\n<4>[200753.857035] R13: 000000000000dc02 R14: ffffffff81686f60 R15: 0000000000000002\n<4>[200753.857097] FS:  00007fe30eb17700(0000) GS:ffff88033fc00000(0000) knlGS:0000000000000000\n<4>[200753.857161] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b\n<4>[200753.857208] CR2: 00007f2ed8411000 CR3: 00000002b9bac000 CR4: 00000000000007e0\n<4>[200753.857270] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n<4>[200753.857332] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400\n<4>[200753.857395] Process tcpconsole (pid: 7184, threadinfo ffff8802c1138000, task ffff88032da9b6b0)\n<4>[200753.857459] Stack:\n<4>[200753.857498]  ffff8802c1139d88 ffffffff81026b77 ffff8802c1139da8 ffffffff81026e9e\n<4>[200753.857646]  0000000000000002 0000000000000002 ffff8802c1139dc8 ffffffff81027927\n<4>[200753.857794]  0000000000000001 0000000000000086 ffff8802c1139e08 ffffffff810279a5\n<4>[200753.857941] Call Trace:\n<4>[200753.857986]  [<ffffffff81026b77>] apic_read+0x11/0x13\n<4>[200753.858033]  [<ffffffff81026e9e>] native_safe_apic_wait_icr_idle+0x16/0x40\n<4>[200753.858083]  [<ffffffff81027927>] __default_send_IPI_dest_field.constprop.0+0x1e/0x58\n<4>[200753.858148]  [<ffffffff810279a5>] default_send_IPI_mask_sequence_phys+0x44/0x78\n<4>[200753.858212]  [<ffffffff8102aa2c>] physflat_send_IPI_all+0x12/0x14\n<4>[200753.858260]  [<ffffffff81027ad0>] arch_trigger_all_cpu_backtrace+0x52/0x86\n<4>[200753.858311]  [<ffffffff81250126>] sysrq_handle_showallcpus+0x9/0xb\n<4>[200753.858359]  [<ffffffff81250446>] __handle_sysrq+0x9d/0x137\n<4>[200753.858406]  [<ffffffff812504e0>] ? __handle_sysrq+0x137/0x137\n<4>[200753.858454]  [<ffffffff81250504>] write_sysrq_trigger+0x24/0x34\n<4>[200753.858502]  [<ffffffff81158095>] proc_reg_write+0x88/0xa7\n<4>[200753.858550]  [<ffffffff8110ea80>] vfs_write+0x9b/0xfd\n<4>[200753.858596]  [<ffffffff8110ec85>] sys_write+0x3e/0x6b\n<4>[200753.858643]  [<ffffffff813846f9>] system_call_fastpath+0x16/0x1b\n<4>[200753.858689] Code: 41 5f 5d c3 55 48 89 e5 57 9d 66 66 90 66 90 5d c3 55 89 ff 48 89 e5 89 b7 00 b0 5f ff 5d c3 55 89 ff 48 89 e5 8b 87 00 b0 5f ff <5d> c3 55 31 c0 81 ff fe 00 00 00 48 89 e5 0f 9e c0 5d c3 48 8b \n<4>[200753.860321] NMI backtrace for cpu 1\n<4>[200753.860366] CPU 1 \n<4>[200753.860392] Modules linked in: vhost_net macvtap macvlan ebtable_nat ebtables pci_stub vboxpci(O) vboxnetadp(O) vboxnetflt(O) vboxdrv(O) tun xfrm_user ip6table_mangle ip6table_filter ip6_tables xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 xt_DSCP xt_comment xt_owner iptable_mangle ipt_REJECT xt_tcpudp binfmt_misc iptable_filter ip_tables x_tables deflate twofish_generic ctr twofish_x86_64_3way xts lrw gf128mul twofish_x86_64 twofish_common blowfish_generic camellia_generic serpent_generic blowfish_x86_64 blowfish_common cast5 des_generic cbc xcbc rmd160 sha512_generic sha256_generic sha1_ssse3 sha1_generic hmac crypto_null af_key xfrm_algo fuse rpcsec_gss_krb5 nfsd nfs nfs_acl auth_rpcgss fscache lockd sunrpc ext2 mbcache dm_crypt bridge stp llc tcp_lp i2c_dev eeprom msr cpuid jc42 w83795 w83627ehf hwmon_vid w83627hf_wdt nouveau coretemp kvm_intel snd_pcm_oss video ttm snd_mixer_oss drm_kms_helper snd_pcm kvm drm snd_page_alloc snd_timer crc32c_intel i2c_algo_bit snd ghash_clmulni_intel aesni_intel mxm_wmi wmi soundcore ioatdma i7core_edac i2c_i801 shpchp aes_x86_64 edac_core lpc_ich aes_generic i2c_core mfd_core acpi_cpufreq pci_hotplug cryptd evdev mperf dca processor microcode thermal_sys button pcspkr btrfs crc32c libcrc32c raid1 raid0 md_mod zfs(PO) zunicode(PO) zavl(PO) zcommon(PO) znvpair(PO) spl(O) zlib_deflate usbhid hid ohci_hcd dm_mirror dm_region_hash dm_log dm_mod sr_mod cdrom sg sd_mod crc_t10dif usb_storage uhci_hcd sata_mv firewire_ohci firewire_core crc_itu_t ehci_hcd usbcore e1000e usb_common ahci libahci libata scsi_mod [last unloaded: scsi_wait_scan]\n<4>[200753.864667] \n<4>[200753.864705] Pid: 0, comm: swapper/1 Tainted: P           O 3.5.2-zeniba #3 Supermicro X8SAX/X8SAX\n<4>[200753.864838] RIP: 0010:[<ffffffff812086ef>]  [<ffffffff812086ef>] intel_idle+0xab/0x109\n<4>[200753.864930] RSP: 0018:ffff8803314c5e48  EFLAGS: 00000046\n<4>[200753.864977] RAX: 0000000000000020 RBX: 0000000000000008 RCX: 0000000000000001\n<4>[200753.865042] RDX: 0000000000000000 RSI: 0000000014b7562e RDI: 12d8e17f2deff000\n<4>[200753.865106] RBP: ffff8803314c5ea8 R08: 00000000000000c8 R09: 0000000000000004\n<4>[200753.865170] R10: 0000000000000012 R11: 0000000000000202 R12: 0000000000000003\n<4>[200753.865234] R13: ffff88033fc39398 R14: 12d8e17f42a7462e R15: 0000000000000020\n<4>[200753.865299] FS:  0000000000000000(0000) GS:ffff88033fc20000(0000) knlGS:0000000000000000\n<4>[200753.865365] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b\n<4>[200753.865413] CR2: ffffffffff600400 CR3: 000000000160b000 CR4: 00000000000007e0\n<4>[200753.865477] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n<4>[200753.865541] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400\n<4>[200753.865606] Process swapper/1 (pid: 0, threadinfo ffff8803314c4000, task ffff88033149b6b0)\n<4>[200753.865671] Stack:\n<4>[200753.865711]  0000000000000000 00000000019aa46c ffff8803314c5eb8 00000001812977bb\n<4>[200753.865862]  0000000000000000 00000000019aa46c 0000000000000000 ffff88033fc39398\n<4>[200753.866014]  0000000000000000 0000000000000003 ffffffff8164ce80 0000000000000001\n<4>[200753.866165] Call Trace:\n<4>[200753.866211]  [<ffffffff812961ff>] cpuidle_enter+0x12/0x14\n<4>[200753.866261]  [<ffffffff812966c6>] cpuidle_idle_call+0xc9/0x192\n<4>[200753.866312]  [<ffffffff810154f4>] cpu_idle+0x9d/0xe7\n<4>[200753.866361]  [<ffffffff8136fb5c>] start_secondary+0x1dd/0x1e4\n<4>[200753.866409] Code: 48 8b 04 25 a0 c6 00 00 48 89 d1 48 2d c8 1f 00 00 0f 01 c8 0f ae f0 e8 47 ff ff ff 85 c0 75 0b b9 01 00 00 00 4c 89 f8 0f 01 c9 <e8> c2 dd e6 ff 48 89 c7 4c 29 f7 e8 70 97 e3 ff 48 89 45 a0 48 \n<4>[200753.867924] NMI backtrace for cpu 2\n<4>[200753.867969] CPU 2 \n<4>[200753.867996] Modules linked in: vhost_net macvtap macvlan ebtable_nat ebtables pci_stub vboxpci(O) vboxnetadp(O) vboxnetflt(O) vboxdrv(O) tun xfrm_user ip6table_mangle ip6table_filter ip6_tables xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 xt_DSCP xt_comment xt_owner iptable_mangle ipt_REJECT xt_tcpudp binfmt_misc iptable_filter ip_tables x_tables deflate twofish_generic ctr twofish_x86_64_3way xts lrw gf128mul twofish_x86_64 twofish_common blowfish_generic camellia_generic serpent_generic blowfish_x86_64 blowfish_common cast5 des_generic cbc xcbc rmd160 sha512_generic sha256_generic sha1_ssse3 sha1_generic hmac crypto_null af_key xfrm_algo fuse rpcsec_gss_krb5 nfsd nfs nfs_acl auth_rpcgss fscache lockd sunrpc ext2 mbcache dm_crypt bridge stp llc tcp_lp i2c_dev eeprom msr cpuid jc42 w83795 w83627ehf hwmon_vid w83627hf_wdt nouveau coretemp kvm_intel snd_pcm_oss video ttm snd_mixer_oss drm_kms_helper snd_pcm kvm drm snd_page_alloc snd_timer crc32c_intel i2c_algo_bit snd ghash_clmulni_intel aesni_intel mxm_wmi wmi soundcore ioatdma i7core_edac i2c_i801 shpchp aes_x86_64 edac_core lpc_ich aes_generic i2c_core mfd_core acpi_cpufreq pci_hotplug cryptd evdev mperf dca processor microcode thermal_sys button pcspkr btrfs crc32c libcrc32c raid1 raid0 md_mod zfs(PO) zunicode(PO) zavl(PO) zcommon(PO) znvpair(PO) spl(O) zlib_deflate usbhid hid ohci_hcd dm_mirror dm_region_hash dm_log dm_mod sr_mod cdrom sg sd_mod crc_t10dif usb_storage uhci_hcd sata_mv firewire_ohci firewire_core crc_itu_t ehci_hcd usbcore e1000e usb_common ahci libahci libata scsi_mod [last unloaded: scsi_wait_scan]\n<4>[200753.872274] \n<4>[200753.872312] Pid: 0, comm: swapper/2 Tainted: P           O 3.5.2-zeniba #3 Supermicro X8SAX/X8SAX\n<4>[200753.872445] RIP: 0010:[<ffffffff812086ef>]  [<ffffffff812086ef>] intel_idle+0xab/0x109\n<4>[200753.872536] RSP: 0018:ffff8803314c7e48  EFLAGS: 00000046\n<4>[200753.872583] RAX: 0000000000000020 RBX: 0000000000000008 RCX: 0000000000000001\n<4>[200753.872647] RDX: 0000000000000000 RSI: 000000000e5992ba RDI: 12d8e17f2deff000\n<4>[200753.872711] RBP: ffff8803314c7ea8 R08: 00000000000000c8 R09: 0000000000000005\n<4>[200753.876692] R10: ffff8803314c7eb8 R11: ffffffff812961ff R12: 0000000000000003\n<4>[200753.876756] R13: ffff88033fc59398 R14: 12d8e17f3c4982ba R15: 0000000000000020\n<4>[200753.876821] FS:  0000000000000000(0000) GS:ffff88033fc40000(0000) knlGS:0000000000000000\n<4>[200753.876887] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b\n<4>[200753.876936] CR2: ffffffffff600400 CR3: 000000000160b000 CR4: 00000000000007e0\n<4>[200753.877000] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n<4>[200753.877064] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400\n<4>[200753.877129] Process swapper/2 (pid: 0, threadinfo ffff8803314c6000, task ffff88033149be80)\n<4>[200753.877194] Stack:\n<4>[200753.877234]  0000000000000000 00000000088faa6c ffff8803314c7eb8 00000002812977bb\n<4>[200753.877386]  0000000000000000 00000000088faa6c 0000000000000000 ffff88033fc59398\n<4>[200753.877537]  0000000000000000 0000000000000003 ffffffff8164ce80 0000000000000002\n<4>[200753.877688] Call Trace:\n<4>[200753.877734]  [<ffffffff812961ff>] cpuidle_enter+0x12/0x14\n<4>[200753.877783]  [<ffffffff812966c6>] cpuidle_idle_call+0xc9/0x192\n<4>[200753.877833]  [<ffffffff810154f4>] cpu_idle+0x9d/0xe7\n<4>[200753.877882]  [<ffffffff8136fb5c>] start_secondary+0x1dd/0x1e4\n<4>[200753.877930] Code: 48 8b 04 25 a0 c6 00 00 48 89 d1 48 2d c8 1f 00 00 0f 01 c8 0f ae f0 e8 47 ff ff ff 85 c0 75 0b b9 01 00 00 00 4c 89 f8 0f 01 c9 <e8> c2 dd e6 ff 48 89 c7 4c 29 f7 e8 70 97 e3 ff 48 89 45 a0 48 \n<4>[200753.879445] NMI backtrace for cpu 3\n<4>[200753.879490] CPU 3 \n<4>[200753.879517] Modules linked in: vhost_net macvtap macvlan ebtable_nat ebtables pci_stub vboxpci(O) vboxnetadp(O) vboxnetflt(O) vboxdrv(O) tun xfrm_user ip6table_mangle ip6table_filter ip6_tables xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 xt_DSCP xt_comment xt_owner iptable_mangle ipt_REJECT xt_tcpudp binfmt_misc iptable_filter ip_tables x_tables deflate twofish_generic ctr twofish_x86_64_3way xts lrw gf128mul twofish_x86_64 twofish_common blowfish_generic camellia_generic serpent_generic blowfish_x86_64 blowfish_common cast5 des_generic cbc xcbc rmd160 sha512_generic sha256_generic sha1_ssse3 sha1_generic hmac crypto_null af_key xfrm_algo fuse rpcsec_gss_krb5 nfsd nfs nfs_acl auth_rpcgss fscache lockd sunrpc ext2 mbcache dm_crypt bridge stp llc tcp_lp i2c_dev eeprom msr cpuid jc42 w83795 w83627ehf hwmon_vid w83627hf_wdt nouveau coretemp kvm_intel snd_pcm_oss video ttm snd_mixer_oss drm_kms_helper snd_pcm kvm drm snd_page_alloc snd_timer crc32c_intel i2c_algo_bit snd ghash_clmulni_intel aesni_intel mxm_wmi wmi soundcore ioatdma i7core_edac i2c_i801 shpchp aes_x86_64 edac_core lpc_ich aes_generic i2c_core mfd_core acpi_cpufreq pci_hotplug cryptd evdev mperf dca processor microcode thermal_sys button pcspkr btrfs crc32c libcrc32c raid1 raid0 md_mod zfs(PO) zunicode(PO) zavl(PO) zcommon(PO) znvpair(PO) spl(O) zlib_deflate usbhid hid ohci_hcd dm_mirror dm_region_hash dm_log dm_mod sr_mod cdrom sg sd_mod crc_t10dif usb_storage uhci_hcd sata_mv firewire_ohci firewire_core crc_itu_t ehci_hcd usbcore e1000e usb_common ahci libahci libata scsi_mod [last unloaded: scsi_wait_scan]\n<4>[200753.883793] \n<4>[200753.883831] Pid: 0, comm: swapper/3 Tainted: P           O 3.5.2-zeniba #3 Supermicro X8SAX/X8SAX\n<4>[200753.883964] RIP: 0010:[<ffffffff812086ef>]  [<ffffffff812086ef>] intel_idle+0xab/0x109\n<4>[200753.884054] RSP: 0018:ffff8803315b9e48  EFLAGS: 00000046\n<4>[200753.884102] RAX: 0000000000000020 RBX: 0000000000000008 RCX: 0000000000000001\n<4>[200753.884166] RDX: 0000000000000000 RSI: 000000001497ccf1 RDI: 12d8e17f2deff000\n<4>[200753.884230] RBP: ffff8803315b9ea8 R08: 0000000000000898 R09: 000000000000000a\n<4>[200753.884294] R10: ffff8803315b9eb8 R11: ffffffff812961ff R12: 0000000000000003\n<4>[200753.884358] R13: ffff88033fc79398 R14: 12d8e17f4287bcf1 R15: 0000000000000020\n<4>[200753.884423] FS:  0000000000000000(0000) GS:ffff88033fc60000(0000) knlGS:0000000000000000\n<4>[200753.884489] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b\n<4>[200753.884537] CR2: ffffffffff600400 CR3: 000000000160b000 CR4: 00000000000007e0\n<4>[200753.884601] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n<4>[200753.884666] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400\n<4>[200753.884730] Process swapper/3 (pid: 0, threadinfo ffff8803315b8000, task ffff88033149c650)\n<4>[200753.884795] Stack:\n<4>[200753.884836]  0000000000000000 0000000002b433c1 ffff8803315b9eb8 00000003812977bb\n<4>[200753.884988]  0000000000000000 0000000002b433c1 0000000000000000 ffff88033fc79398\n<4>[200753.885139]  0000000000000000 0000000000000003 ffffffff8164ce80 0000000000000003\n<4>[200753.885290] Call Trace:\n<4>[200753.885335]  [<ffffffff812961ff>] cpuidle_enter+0x12/0x14\n<4>[200753.885384]  [<ffffffff812966c6>] cpuidle_idle_call+0xc9/0x192\n<4>[200753.885434]  [<ffffffff810154f4>] cpu_idle+0x9d/0xe7\n<4>[200753.885483]  [<ffffffff8136fb5c>] start_secondary+0x1dd/0x1e4\n<4>[200753.885531] Code: 48 8b 04 25 a0 c6 00 00 48 89 d1 48 2d c8 1f 00 00 0f 01 c8 0f ae f0 e8 47 ff ff ff 85 c0 75 0b b9 01 00 00 00 4c 89 f8 0f 01 c9 <e8> c2 dd e6 ff 48 89 c7 4c 29 f7 e8 70 97 e3 ff 48 89 45 a0 48 \n<4>[200753.887045] NMI backtrace for cpu 4\n<4>[200753.887090] CPU 4 \n<4>[200753.887116] Modules linked in: vhost_net macvtap macvlan ebtable_nat ebtables pci_stub vboxpci(O) vboxnetadp(O) vboxnetflt(O) vboxdrv(O) tun xfrm_user ip6table_mangle ip6table_filter ip6_tables xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 xt_DSCP xt_comment xt_owner iptable_mangle ipt_REJECT xt_tcpudp binfmt_misc iptable_filter ip_tables x_tables deflate twofish_generic ctr twofish_x86_64_3way xts lrw gf128mul twofish_x86_64 twofish_common blowfish_generic camellia_generic serpent_generic blowfish_x86_64 blowfish_common cast5 des_generic cbc xcbc rmd160 sha512_generic sha256_generic sha1_ssse3 sha1_generic hmac crypto_null af_key xfrm_algo fuse rpcsec_gss_krb5 nfsd nfs nfs_acl auth_rpcgss fscache lockd sunrpc ext2 mbcache dm_crypt bridge stp llc tcp_lp i2c_dev eeprom msr cpuid jc42 w83795 w83627ehf hwmon_vid w83627hf_wdt nouveau coretemp kvm_intel snd_pcm_oss video ttm snd_mixer_oss drm_kms_helper snd_pcm kvm drm snd_page_alloc snd_timer crc32c_intel i2c_algo_bit snd ghash_clmulni_intel aesni_intel mxm_wmi wmi soundcore ioatdma i7core_edac i2c_i801 shpchp aes_x86_64 edac_core lpc_ich aes_generic i2c_core mfd_core acpi_cpufreq pci_hotplug cryptd evdev mperf dca processor microcode thermal_sys button pcspkr btrfs crc32c libcrc32c raid1 raid0 md_mod zfs(PO) zunicode(PO) zavl(PO) zcommon(PO) znvpair(PO) spl(O) zlib_deflate usbhid hid ohci_hcd dm_mirror dm_region_hash dm_log dm_mod sr_mod cdrom sg sd_mod crc_t10dif usb_storage uhci_hcd sata_mv firewire_ohci firewire_core crc_itu_t ehci_hcd usbcore e1000e usb_common ahci libahci libata scsi_mod [last unloaded: scsi_wait_scan]\n<4>[200753.891390] \n<4>[200753.891427] Pid: 0, comm: swapper/4 Tainted: P           O 3.5.2-zeniba #3 Supermicro X8SAX/X8SAX\n<4>[200753.891560] RIP: 0010:[<ffffffff812086ef>]  [<ffffffff812086ef>] intel_idle+0xab/0x109\n<4>[200753.891650] RSP: 0000:ffff8803315bbe48  EFLAGS: 00000046\n<4>[200753.891697] RAX: 0000000000000020 RBX: 0000000000000008 RCX: 0000000000000001\n<4>[200753.891762] RDX: 0000000000000000 RSI: 000000001498cc56 RDI: 12d8e17f2deff000\n<4>[200753.891825] RBP: ffff8803315bbea8 R08: 00000000000000c8 R09: 0000000000000005\n<4>[200753.891890] R10: ffff8803315bbeb8 R11: ffffffff812961ff R12: 0000000000000003\n<4>[200753.891954] R13: ffff88033fc99398 R14: 12d8e17f4288bc56 R15: 0000000000000020\n<4>[200753.892019] FS:  0000000000000000(0000) GS:ffff88033fc80000(0000) knlGS:0000000000000000\n<4>[200753.892085] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b\n<4>[200753.892133] CR2: 00007f2ed8411000 CR3: 000000000160b000 CR4: 00000000000007e0\n<4>[200753.892197] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n<4>[200753.892261] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400\n<4>[200753.892326] Process swapper/4 (pid: 0, threadinfo ffff8803315ba000, task ffff88033149ce20)\n<4>[200753.892391] Stack:\n<4>[200753.892431]  0000000000000000 000000001904a4e9 ffff8803315bbeb8 00000004812977bb\n<4>[200753.892583]  0000000000000000 000000001904a4e9 0000000000000000 ffff88033fc99398\n<4>[200753.892734]  0000000000000000 0000000000000003 ffffffff8164ce80 0000000000000004\n<4>[200753.892885] Call Trace:\n<4>[200753.892930]  [<ffffffff812961ff>] cpuidle_enter+0x12/0x14\n<4>[200753.892979]  [<ffffffff812966c6>] cpuidle_idle_call+0xc9/0x192\n<4>[200753.893029]  [<ffffffff810154f4>] cpu_idle+0x9d/0xe7\n<4>[200753.893078]  [<ffffffff8136fb5c>] start_secondary+0x1dd/0x1e4\n<4>[200753.893125] Code: 48 8b 04 25 a0 c6 00 00 48 89 d1 48 2d c8 1f 00 00 0f 01 c8 0f ae f0 e8 47 ff ff ff 85 c0 75 0b b9 01 00 00 00 4c 89 f8 0f 01 c9 <e8> c2 dd e6 ff 48 89 c7 4c 29 f7 e8 70 97 e3 ff 48 89 45 a0 48 \n<4>[200753.894640] NMI backtrace for cpu 5\n<4>[200753.894684] CPU 5 \n<4>[200753.894709] Modules linked in: vhost_net macvtap macvlan ebtable_nat ebtables pci_stub vboxpci(O) vboxnetadp(O) vboxnetflt(O) vboxdrv(O) tun xfrm_user ip6table_mangle ip6table_filter ip6_tables xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 xt_DSCP xt_comment xt_owner iptable_mangle ipt_REJECT xt_tcpudp binfmt_misc iptable_filter ip_tables x_tables deflate twofish_generic ctr twofish_x86_64_3way xts lrw gf128mul twofish_x86_64 twofish_common blowfish_generic camellia_generic serpent_generic blowfish_x86_64 blowfish_common cast5 des_generic cbc xcbc rmd160 sha512_generic sha256_generic sha1_ssse3 sha1_generic hmac crypto_null af_key xfrm_algo fuse rpcsec_gss_krb5 nfsd nfs nfs_acl auth_rpcgss fscache lockd sunrpc ext2 mbcache dm_crypt bridge stp llc tcp_lp i2c_dev eeprom msr cpuid jc42 w83795 w83627ehf hwmon_vid w83627hf_wdt nouveau coretemp kvm_intel snd_pcm_oss video ttm snd_mixer_oss drm_kms_helper snd_pcm kvm drm snd_page_alloc snd_timer crc32c_intel i2c_algo_bit snd ghash_clmulni_intel aesni_intel mxm_wmi wmi soundcore ioatdma i7core_edac i2c_i801 shpchp aes_x86_64 edac_core lpc_ich aes_generic i2c_core mfd_core acpi_cpufreq pci_hotplug cryptd evdev mperf dca processor microcode thermal_sys button pcspkr btrfs crc32c libcrc32c raid1 raid0 md_mod zfs(PO) zunicode(PO) zavl(PO) zcommon(PO) znvpair(PO) spl(O) zlib_deflate usbhid hid ohci_hcd dm_mirror dm_region_hash dm_log dm_mod sr_mod cdrom sg sd_mod crc_t10dif usb_storage uhci_hcd sata_mv firewire_ohci firewire_core crc_itu_t ehci_hcd usbcore e1000e usb_common ahci libahci libata scsi_mod [last unloaded: scsi_wait_scan]\n<4>[200753.902798] \n<4>[200753.902834] Pid: 0, comm: swapper/5 Tainted: P           O 3.5.2-zeniba #3 Supermicro X8SAX/X8SAX\n<4>[200753.902964] RIP: 0010:[<ffffffff812086ef>]  [<ffffffff812086ef>] intel_idle+0xab/0x109\n<4>[200753.903051] RSP: 0000:ffff8803315bde48  EFLAGS: 00000046\n<4>[200753.903096] RAX: 0000000000000020 RBX: 0000000000000008 RCX: 0000000000000001\n<4>[200753.903158] RDX: 0000000000000000 RSI: 0000000015169203 RDI: 12d8e17f2deff000\n<4>[200753.903220] RBP: ffff8803315bdea8 R08: 00000000000000c8 R09: 0000000000000005\n<4>[200753.903282] R10: ffff8803315bdeb8 R11: ffffffff812961ff R12: 0000000000000003\n<4>[200753.903344] R13: ffff88033fcb9398 R14: 12d8e17f43068203 R15: 0000000000000020\n<4>[200753.903408] FS:  0000000000000000(0000) GS:ffff88033fca0000(0000) knlGS:0000000000000000\n<4>[200753.903471] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b\n<4>[200753.903518] CR2: 00007f2ed8411000 CR3: 000000000160b000 CR4: 00000000000007e0\n<4>[200753.903580] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n<4>[200753.903642] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400\n<4>[200753.903705] Process swapper/5 (pid: 0, threadinfo ffff8803315bc000, task ffff88033149d5f0)\n<4>[200753.903768] Stack:\n<4>[200753.903807]  000000000000002d 00000000186b7ce5 ffff8803315bdeb8 00000005812977bb\n<4>[200753.903955]  000000000000002d 00000000186b7ce5 000000000000002d ffff88033fcb9398\n<4>[200753.904103]  0000000000000000 0000000000000003 ffffffff8164ce80 0000000000000005\n<4>[200753.904253] Call Trace:\n<4>[200753.904296]  [<ffffffff812961ff>] cpuidle_enter+0x12/0x14\n<4>[200753.904343]  [<ffffffff812966c6>] cpuidle_idle_call+0xc9/0x192\n<4>[200753.904391]  [<ffffffff810154f4>] cpu_idle+0x9d/0xe7\n<4>[200753.904437]  [<ffffffff8136fb5c>] start_secondary+0x1dd/0x1e4\n<4>[200753.904483] Code: 48 8b 04 25 a0 c6 00 00 48 89 d1 48 2d c8 1f 00 00 0f 01 c8 0f ae f0 e8 47 ff ff ff 85 c0 75 0b b9 01 00 00 00 4c 89 f8 0f 01 c9 <e8> c2 dd e6 ff 48 89 c7 4c 29 f7 e8 70 97 e3 ff 48 89 45 a0 48 \n<4>[200753.905963] NMI backtrace for cpu 6\n<4>[200753.906006] CPU 6 \n<4>[200753.906031] Modules linked in: vhost_net macvtap macvlan ebtable_nat ebtables pci_stub vboxpci(O) vboxnetadp(O) vboxnetflt(O) vboxdrv(O) tun xfrm_user ip6table_mangle ip6table_filter ip6_tables xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 xt_DSCP xt_comment xt_owner iptable_mangle ipt_REJECT xt_tcpudp binfmt_misc iptable_filter ip_tables x_tables deflate twofish_generic ctr twofish_x86_64_3way xts lrw gf128mul twofish_x86_64 twofish_common blowfish_generic camellia_generic serpent_generic blowfish_x86_64 blowfish_common cast5 des_generic cbc xcbc rmd160 sha512_generic sha256_generic sha1_ssse3 sha1_generic hmac crypto_null af_key xfrm_algo fuse rpcsec_gss_krb5 nfsd nfs nfs_acl auth_rpcgss fscache lockd sunrpc ext2 mbcache dm_crypt bridge stp llc tcp_lp i2c_dev eeprom msr cpuid jc42 w83795 w83627ehf hwmon_vid w83627hf_wdt nouveau coretemp kvm_intel snd_pcm_oss video ttm snd_mixer_oss drm_kms_helper snd_pcm kvm drm snd_page_alloc snd_timer crc32c_intel i2c_algo_bit snd ghash_clmulni_intel aesni_intel mxm_wmi wmi soundcore ioatdma i7core_edac i2c_i801 shpchp aes_x86_64 edac_core lpc_ich aes_generic i2c_core mfd_core acpi_cpufreq pci_hotplug cryptd evdev mperf dca processor microcode thermal_sys button pcspkr btrfs crc32c libcrc32c raid1 raid0 md_mod zfs(PO) zunicode(PO) zavl(PO) zcommon(PO) znvpair(PO) spl(O) zlib_deflate usbhid hid ohci_hcd dm_mirror dm_region_hash dm_log dm_mod sr_mod cdrom sg sd_mod crc_t10dif usb_storage uhci_hcd sata_mv firewire_ohci firewire_core crc_itu_t ehci_hcd usbcore e1000e usb_common ahci libahci libata scsi_mod [last unloaded: scsi_wait_scan]\n<4>[200753.910202] \n<4>[200753.910239] Pid: 0, comm: swapper/6 Tainted: P           O 3.5.2-zeniba #3 Supermicro X8SAX/X8SAX\n<4>[200753.910368] RIP: 0010:[<ffffffff812086ef>]  [<ffffffff812086ef>] intel_idle+0xab/0x109\n<4>[200753.910455] RSP: 0018:ffff8803315bfe48  EFLAGS: 00000046\n<4>[200753.910500] RAX: 0000000000000020 RBX: 0000000000000008 RCX: 0000000000000001\n<4>[200753.910563] RDX: 0000000000000000 RSI: 0000000015182a88 RDI: 12d8e17f2deff000\n<4>[200753.910625] RBP: ffff8803315bfea8 R08: 00000000000000c8 R09: 0000000000000005\n<4>[200753.910687] R10: ffff8803315bfeb8 R11: ffffffff812961ff R12: 0000000000000003\n<4>[200753.910749] R13: ffff88033fcd9398 R14: 12d8e17f43081a88 R15: 0000000000000020\n<4>[200753.910811] FS:  0000000000000000(0000) GS:ffff88033fcc0000(0000) knlGS:0000000000000000\n<4>[200753.910875] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b\n<4>[200753.910921] CR2: 00000000006a4068 CR3: 000000000160b000 CR4: 00000000000007e0\n<4>[200753.910983] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n<4>[200753.911046] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400\n<4>[200753.911108] Process swapper/6 (pid: 0, threadinfo ffff8803315be000, task ffff88033149ddc0)\n<4>[200753.911171] Stack:\n<4>[200753.911210]  0000000000000095 0000000008eb73b9 ffff8803315bfeb8 00000006812977bb\n<4>[200753.911358]  0000000000000095 0000000008eb73b9 0000000000000095 ffff88033fcd9398\n<4>[200753.911506]  0000000000000000 0000000000000003 ffffffff8164ce80 0000000000000006\n<4>[200753.911653] Call Trace:\n<4>[200753.911696]  [<ffffffff812961ff>] cpuidle_enter+0x12/0x14\n<4>[200753.911743]  [<ffffffff812966c6>] cpuidle_idle_call+0xc9/0x192\n<4>[200753.911791]  [<ffffffff810154f4>] cpu_idle+0x9d/0xe7\n<4>[200753.911837]  [<ffffffff8136fb5c>] start_secondary+0x1dd/0x1e4\n<4>[200753.911883] Code: 48 8b 04 25 a0 c6 00 00 48 89 d1 48 2d c8 1f 00 00 0f 01 c8 0f ae f0 e8 47 ff ff ff 85 c0 75 0b b9 01 00 00 00 4c 89 f8 0f 01 c9 <e8> c2 dd e6 ff 48 89 c7 4c 29 f7 e8 70 97 e3 ff 48 89 45 a0 48 \n<4>[200753.913363] NMI backtrace for cpu 7\n<4>[200753.913407] CPU 7 \n<4>[200753.913432] Modules linked in: vhost_net macvtap macvlan ebtable_nat ebtables pci_stub vboxpci(O) vboxnetadp(O) vboxnetflt(O) vboxdrv(O) tun xfrm_user ip6table_mangle ip6table_filter ip6_tables xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 xt_DSCP xt_comment xt_owner iptable_mangle ipt_REJECT xt_tcpudp binfmt_misc iptable_filter ip_tables x_tables deflate twofish_generic ctr twofish_x86_64_3way xts lrw gf128mul twofish_x86_64 twofish_common blowfish_generic camellia_generic serpent_generic blowfish_x86_64 blowfish_common cast5 des_generic cbc xcbc rmd160 sha512_generic sha256_generic sha1_ssse3 sha1_generic hmac crypto_null af_key xfrm_algo fuse rpcsec_gss_krb5 nfsd nfs nfs_acl auth_rpcgss fscache lockd sunrpc ext2 mbcache dm_crypt bridge stp llc tcp_lp i2c_dev eeprom msr cpuid jc42 w83795 w83627ehf hwmon_vid w83627hf_wdt nouveau coretemp kvm_intel snd_pcm_oss video ttm snd_mixer_oss drm_kms_helper snd_pcm kvm drm snd_page_alloc snd_timer crc32c_intel i2c_algo_bit snd ghash_clmulni_intel aesni_intel mxm_wmi wmi soundcore ioatdma i7core_edac i2c_i801 shpchp aes_x86_64 edac_core lpc_ich aes_generic i2c_core mfd_core acpi_cpufreq pci_hotplug cryptd evdev mperf dca processor microcode thermal_sys button pcspkr btrfs crc32c libcrc32c raid1 raid0 md_mod zfs(PO) zunicode(PO) zavl(PO) zcommon(PO) znvpair(PO) spl(O) zlib_deflate usbhid hid ohci_hcd dm_mirror dm_region_hash dm_log dm_mod sr_mod cdrom sg sd_mod crc_t10dif usb_storage uhci_hcd sata_mv firewire_ohci firewire_core crc_itu_t ehci_hcd usbcore e1000e usb_common ahci libahci libata scsi_mod [last unloaded: scsi_wait_scan]\n<4>[200753.917603] \n<4>[200753.917639] Pid: 0, comm: swapper/7 Tainted: P           O 3.5.2-zeniba #3 Supermicro X8SAX/X8SAX\n<4>[200753.917769] RIP: 0010:[<ffffffff812086ef>]  [<ffffffff812086ef>] intel_idle+0xab/0x109\n<4>[200753.917856] RSP: 0018:ffff8803315e1e48  EFLAGS: 00000046\n<4>[200753.917902] RAX: 0000000000000020 RBX: 0000000000000008 RCX: 0000000000000001\n<4>[200753.917964] RDX: 0000000000000000 RSI: 000000001519af21 RDI: 12d8e17f2deff000\n<4>[200753.918026] RBP: ffff8803315e1ea8 R08: 00000000000000c8 R09: 0000000000000004\n<4>[200753.918088] R10: ffff8803315e1eb8 R11: ffffffff812961ff R12: 0000000000000003\n<4>[200753.918149] R13: ffff88033fcf9398 R14: 12d8e17f43099f21 R15: 0000000000000020\n<4>[200753.918212] FS:  0000000000000000(0000) GS:ffff88033fce0000(0000) knlGS:0000000000000000\n<4>[200753.918276] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b\n<4>[200753.918322] CR2: ffffffffff600400 CR3: 000000000160b000 CR4: 00000000000007e0\n<4>[200753.918384] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n<4>[200753.918446] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400\n<4>[200753.918509] Process swapper/7 (pid: 0, threadinfo ffff8803315e0000, task ffff88033149e590)\n<4>[200753.918572] Stack:\n<4>[200753.918611]  0000000000000000 0000000003de036e ffff8803315e1eb8 00000007812977bb\n<4>[200753.918759]  0000000000000000 0000000003de036e 0000000000000000 ffff88033fcf9398\n<4>[200753.918907]  0000000000000000 0000000000000003 ffffffff8164ce80 0000000000000007\n<4>[200753.919054] Call Trace:\n<4>[200753.919097]  [<ffffffff812961ff>] cpuidle_enter+0x12/0x14\n<4>[200753.919144]  [<ffffffff812966c6>] cpuidle_idle_call+0xc9/0x192\n<4>[200753.919192]  [<ffffffff810154f4>] cpu_idle+0x9d/0xe7\n<4>[200753.919239]  [<ffffffff8136fb5c>] start_secondary+0x1dd/0x1e4\n<4>[200753.919285] Code: 48 8b 04 25 a0 c6 00 00 48 89 d1 48 2d c8 1f 00 00 0f 01 c8 0f ae f0 e8 47 ff ff ff 85 c0 75 0b b9 01 00 00 00 4c 89 f8 0f 01 c9 <e8> c2 dd e6 ff 48 89 c7 4c 29 f7 e8 70 97 e3 ff 48 89 45 a0 48 \n```\n\n## \n\n```\n<4>[200822.751777]  [<ffffffffa03e12bf>] zfs_write+0x51e/0xa9e [zfs]\n<4>[200822.751825]  [<ffffffff8137e95a>] ? schedule+0x60/0x62\n<4>[200822.751872]  [<ffffffff8107df0c>] ? futex_wait_queue_me+0x82/0x99\n<4>[200822.751920]  [<ffffffff81062121>] ? ttwu_do_wakeup+0x27/0xd1\n<4>[200822.751969]  [<ffffffff8113d5c5>] ? fsnotify+0x259/0x2ae\n<4>[200822.752057]  [<ffffffffa03ef5fd>] zpl_write_common+0x4d/0x65 [zfs]\n<4>[200822.752118]  [<ffffffffa016b546>] ? crhold+0x19/0x1c [spl]\n<4>[200822.752205]  [<ffffffffa03ef66e>] zpl_write+0x59/0x7d [zfs]\n<4>[200822.752252]  [<ffffffff8110ea80>] vfs_write+0x9b/0xfd\n<4>[200822.752298]  [<ffffffff8110ec85>] sys_write+0x3e/0x6b\n<4>[200822.752345]  [<ffffffff813846f9>] system_call_fastpath+0x16/0x1b\n<6>[200822.752392] java            D ffff8802ad701368     0 16650      1 0x00000000\n<4>[200822.752483]  ffff880152dd78a8 0000000000000082 ffff8802ad700fa0 ffff880152dd7fd8\n<4>[200822.752631]  ffff880152dd7fd8 0000000000013600 ffff88033149e590 ffff8802ad700fa0\n<4>[200822.752779]  ffff880152dd78b8 ffff8803294cbcd8 ffff8803294cbca8 0000000000000002\n<4>[200822.752926] Call Trace:\n<4>[200822.752968]  [<ffffffff8137e95a>] schedule+0x60/0x62\n<4>[200822.753026]  [<ffffffffa016ae6a>] cv_wait_common+0xc9/0x15d [spl]\n<4>[200822.753075]  [<ffffffff810572e9>] ? abort_exclusive_wait+0x8a/0x8a\n<4>[200822.753134]  [<ffffffffa016af1c>] __cv_wait+0xe/0x12 [spl]\n<4>[200822.753221]  [<ffffffffa03b1d7e>] spa_config_enter+0x66/0xd9 [zfs]\n<4>[200822.753311]  [<ffffffffa03ec5d0>] zio_vdev_io_start+0x42/0x21e [zfs]\n<4>[200822.753401]  [<ffffffffa03eb593>] zio_nowait+0xd7/0xfe [zfs]\n<4>[200822.753470]  [<ffffffffa037246c>] arc_read_nolock+0x660/0x671 [zfs]\n<4>[200822.753542]  [<ffffffffa0372515>] arc_read+0x98/0xfe [zfs]\n<4>[200822.753611]  [<ffffffffa0376d38>] ? dmu_buf_rele+0x2c/0x2c [zfs]\n<4>[200822.753696]  [<ffffffffa039c5d1>] dsl_read+0x2c/0x2e [zfs]\n<4>[200822.753766]  [<ffffffffa0375d2c>] dbuf_read+0x392/0x4c7 [zfs]\n<4>[200822.753841]  [<ffffffffa037b9e8>] dmu_buf_hold_array_by_dnode+0x1f5/0x320 [zfs]\n<4>[200822.753932]  [<ffffffffa037bb6b>] dmu_buf_hold_array+0x58/0x79 [zfs]\n<4>[200822.754009]  [<ffffffffa037cdad>] dmu_read_uio+0x41/0xc1 [zfs]\n<4>[200822.754099]  [<ffffffffa03e04cd>] zfs_read+0x333/0x3dc [zfs]\n<4>[200822.754188]  [<ffffffffa03ef51b>] zpl_read_common+0x4d/0x65 [zfs]\n<4>[200822.754249]  [<ffffffffa016b546>] ? crhold+0x19/0x1c [spl]\n<4>[200822.754336]  [<ffffffffa03ef58c>] zpl_read+0x59/0x7d [zfs]\n<4>[200822.754383]  [<ffffffff8110eb7a>] vfs_read+0x98/0xfa\n<4>[200822.754429]  [<ffffffff8110ec1a>] sys_read+0x3e/0x6b\n<4>[200822.754475]  [<ffffffff813846f9>] system_call_fastpath+0x16/0x1b\n<6>[200822.754521] java            D ffff8802ad700b98     0 16651      1 0x00000000\n<4>[200822.754613]  ffff8801738ef7e8 0000000000000082 ffff8802ad7007d0 ffff8801738effd8\n<4>[200822.754761]  ffff8801738effd8 0000000000013600 ffff88033149e590 ffff8802ad7007d0\n<4>[200822.754909]  ffff8801738ef7f8 ffff8803294cbcd8 ffff8803294cbca8 0000000000000002\n<4>[200822.755057] Call Trace:\n<4>[200822.755099]  [<ffffffff8137e95a>] schedule+0x60/0x62\n<4>[200822.755157]  [<ffffffffa016ae6a>] cv_wait_common+0xc9/0x15d [spl]\n<4>[200822.755205]  [<ffffffff810572e9>] ? abort_exclusive_wait+0x8a/0x8a\n<4>[200822.755264]  [<ffffffffa016af1c>] __cv_wait+0xe/0x12 [spl]\n<4>[200822.755352]  [<ffffffffa03b1d7e>] spa_config_enter+0x66/0xd9 [zfs]\n<4>[200822.755441]  [<ffffffffa03ec5d0>] zio_vdev_io_start+0x42/0x21e [zfs]\n<4>[200822.755532]  [<ffffffffa03eb593>] zio_nowait+0xd7/0xfe [zfs]\n<4>[200822.755600]  [<ffffffffa037246c>] arc_read_nolock+0x660/0x671 [zfs]\n<4>[200822.755672]  [<ffffffffa0372515>] arc_read+0x98/0xfe [zfs]\n<4>[200822.755742]  [<ffffffffa0376d38>] ? dmu_buf_rele+0x2c/0x2c [zfs]\n<4>[200822.755826]  [<ffffffffa039c5d1>] dsl_read+0x2c/0x2e [zfs]\n<4>[200822.755896]  [<ffffffffa0375d2c>] dbuf_read+0x392/0x4c7 [zfs]\n<4>[200822.755966]  [<ffffffffa0376fa2>] __dbuf_hold_impl+0x196/0x3ab [zfs]\n<4>[200822.756040]  [<ffffffffa0377234>] dbuf_hold_impl+0x7d/0x9f [zfs]\n<4>[200822.756111]  [<ffffffffa0377ea3>] dbuf_hold+0x1b/0x29 [zfs]\n<4>[200822.756183]  [<ffffffffa037b987>] dmu_buf_hold_array_by_dnode+0x194/0x320 [zfs]\n<4>[200822.756277]  [<ffffffffa0387d9f>] ? dnode_hold_impl+0x34b/0x364 [zfs]\n<4>[200822.756354]  [<ffffffffa037bb6b>] dmu_buf_hold_array+0x58/0x79 [zfs]\n<4>[200822.756431]  [<ffffffffa037cdad>] dmu_read_uio+0x41/0xc1 [zfs]\n<4>[200822.756521]  [<ffffffffa03e04cd>] zfs_read+0x333/0x3dc [zfs]\n<4>[200822.756610]  [<ffffffffa03ef51b>] zpl_read_common+0x4d/0x65 [zfs]\n<4>[200822.756670]  [<ffffffffa016b546>] ? crhold+0x19/0x1c [spl]\n<4>[200822.756757]  [<ffffffffa03ef58c>] zpl_read+0x59/0x7d [zfs]\n<4>[200822.756804]  [<ffffffff8110eb7a>] vfs_read+0x98/0xfa\n<4>[200822.756850]  [<ffffffff8110ec1a>] sys_read+0x3e/0x6b\n<4>[200822.756896]  [<ffffffff813846f9>] system_call_fastpath+0x16/0x1b\n<6>[200822.756943] java            D ffff8802ad707128     0 16654      1 0x00000000\n<4>[200822.757035]  ffff88012f5d5438 0000000000000082 ffff8802ad706d60 ffff88012f5d5fd8\n<4>[200822.757182]  ffff88012f5d5fd8 0000000000013600 ffff88033149e590 ffff8802ad706d60\n<4>[200822.757330]  ffff88012f5d5448 ffff8803294cbcd8 ffff8803294cbca8 0000000000000002\n<4>[200822.757477] Call Trace:\n<4>[200822.757519]  [<ffffffff8137e95a>] schedule+0x60/0x62\n<4>[200822.757577]  [<ffffffffa016ae6a>] cv_wait_common+0xc9/0x15d [spl]\n<4>[200822.757626]  [<ffffffff810572e9>] ? abort_exclusive_wait+0x8a/0x8a\n<4>[200822.757685]  [<ffffffffa016af1c>] __cv_wait+0xe/0x12 [spl]\n<4>[200822.757772]  [<ffffffffa03b1d7e>] spa_config_enter+0x66/0xd9 [zfs]\n<4>[200822.757862]  [<ffffffffa03ec5d0>] zio_vdev_io_start+0x42/0x21e [zfs]\n<4>[200822.757953]  [<ffffffffa03eb593>] zio_nowait+0xd7/0xfe [zfs]\n<4>[200822.758021]  [<ffffffffa037246c>] arc_read_nolock+0x660/0x671 [zfs]\n<4>[200822.758093]  [<ffffffffa0372515>] arc_read+0x98/0xfe [zfs]\n<4>[200822.758162]  [<ffffffffa0376d38>] ? dmu_buf_rele+0x2c/0x2c [zfs]\n<4>[200822.758247]  [<ffffffffa039c5d1>] dsl_read+0x2c/0x2e [zfs]\n<4>[200822.758316]  [<ffffffffa0375d2c>] dbuf_read+0x392/0x4c7 [zfs]\n<4>[200822.758387]  [<ffffffffa0376fa2>] __dbuf_hold_impl+0x196/0x3ab [zfs]\n<4>[200822.758458]  [<ffffffffa0377234>] dbuf_hold_impl+0x7d/0x9f [zfs]\n<4>[200822.758530]  [<ffffffffa0377ea3>] dbuf_hold+0x1b/0x29 [zfs]\n<4>[200822.758602]  [<ffffffffa037b724>] dmu_buf_hold+0x6a/0xe9 [zfs]\n<4>[200822.758691]  [<ffffffffa03c42da>] zap_lockdir+0x45/0x444 [zfs]\n<4>[200822.758739]  [<ffffffff810ff7fa>] ? __kmalloc+0xd6/0xe6\n<4>[200822.758827]  [<ffffffffa03c5461>] zap_lookup_norm+0x3a/0x151 [zfs]\n<4>[200822.758916]  [<ffffffffa03c5c1a>] ? zfs_acl_next_ace+0xf8/0x11e [zfs]\n<4>[200822.759006]  [<ffffffffa03c55ea>] zap_lookup+0x2e/0x30 [zfs]\n<4>[200822.759095]  [<ffffffffa03ca71e>] zfs_dirent_lock+0x361/0x408 [zfs]\n<4>[200822.763103]  [<ffffffffa03ca98e>] zfs_dirlook+0x1c9/0x225 [zfs]\n<4>[200822.763193]  [<ffffffffa03c8073>] ? zfs_zaccess+0x1c9/0x247 [zfs]\n<4>[200822.763283]  [<ffffffffa03dec28>] zfs_lookup+0x253/0x29e [zfs]\n<4>[200822.763373]  [<ffffffffa03efb6a>] zpl_lookup+0x4b/0x81 [zfs]\n<4>[200822.763421]  [<ffffffff8111e17a>] ? spin_unlock+0x9/0xb\n<4>[200822.763467]  [<ffffffff81116ecc>] __lookup_hash+0x9d/0xdb\n<4>[200822.763515]  [<ffffffff8137a92a>] lookup_slow+0x42/0xa6\n<4>[200822.763561]  [<ffffffff81117c79>] walk_component+0x53/0x121\n<4>[200822.763608]  [<ffffffff81117d7d>] lookup_last+0x36/0x38\n<4>[200822.763654]  [<ffffffff811188c8>] path_lookupat+0x8b/0x2a7\n<4>[200822.763701]  [<ffffffff810e0ab7>] ? set_pte_at+0x9/0xd\n<4>[200822.763748]  [<ffffffff81118b07>] do_path_lookup+0x23/0x8e\n<4>[200822.763795]  [<ffffffff8111a5a6>] user_path_at_empty+0x52/0x90\n<4>[200822.763844]  [<ffffffff812aa800>] ? release_sock+0x109/0x116\n<4>[200822.763892]  [<ffffffff81327e5c>] ? inet6_ioctl+0x90/0x99\n<4>[200822.763939]  [<ffffffff812a58ce>] ? sock_do_ioctl+0x22/0x40\n<4>[200822.763986]  [<ffffffff8111a5f0>] user_path_at+0xc/0xe\n<4>[200822.764033]  [<ffffffff81111ff7>] vfs_fstatat+0x30/0x62\n<4>[200822.764079]  [<ffffffff8111bc9d>] ? vfs_ioctl+0x21/0x34\n<4>[200822.764126]  [<ffffffff8111c4f8>] ? do_vfs_ioctl+0x3b6/0x3f9\n<4>[200822.764173]  [<ffffffff8111205a>] vfs_stat+0x16/0x18\n<4>[200822.764219]  [<ffffffff81112311>] sys_newstat+0x15/0x30\n<4>[200822.764266]  [<ffffffff8111c59d>] ? sys_ioctl+0x62/0x75\n<4>[200822.764312]  [<ffffffff813846f9>] system_call_fastpath+0x16/0x1b\n<6>[200822.764359] java            D ffff8802ad702ad8     0 16657      1 0x00000000\n<4>[200822.764451]  ffff8801dace78a8 0000000000000082 ffff8802ad702710 ffff8801dace7fd8\n<4>[200822.764598]  ffff8801dace7fd8 0000000000013600 ffff88033149ce20 ffff8802ad702710\n<4>[200822.764746]  ffff8801dace78b8 ffff8803294cbcd8 ffff8803294cbca8 0000000000000002\n<4>[200822.764893] Call Trace:\n<4>[200822.764936]  [<ffffffff8137e95a>] schedule+0x60/0x62\n<4>[200822.764994]  [<ffffffffa016ae6a>] cv_wait_common+0xc9/0x15d [spl]\n<4>[200822.765042]  [<ffffffff810572e9>] ? abort_exclusive_wait+0x8a/0x8a\n<4>[200822.765101]  [<ffffffffa016af1c>] __cv_wait+0xe/0x12 [spl]\n<4>[200822.765188]  [<ffffffffa03b1d7e>] spa_config_enter+0x66/0xd9 [zfs]\n<4>[200822.765278]  [<ffffffffa03ec5d0>] zio_vdev_io_start+0x42/0x21e [zfs]\n<4>[200822.765369]  [<ffffffffa03eb593>] zio_nowait+0xd7/0xfe [zfs]\n<4>[200822.765438]  [<ffffffffa037246c>] arc_read_nolock+0x660/0x671 [zfs]\n<4>[200822.765509]  [<ffffffffa0372515>] arc_read+0x98/0xfe [zfs]\n<4>[200822.765579]  [<ffffffffa0376d38>] ? dmu_buf_rele+0x2c/0x2c [zfs]\n<4>[200822.765663]  [<ffffffffa039c5d1>] dsl_read+0x2c/0x2e [zfs]\n<4>[200822.765733]  [<ffffffffa0375d2c>] dbuf_read+0x392/0x4c7 [zfs]\n<4>[200822.765808]  [<ffffffffa037b9e8>] dmu_buf_hold_array_by_dnode+0x1f5/0x320 [zfs]\n<4>[200822.765899]  [<ffffffffa037bb6b>] dmu_buf_hold_array+0x58/0x79 [zfs]\n<4>[200822.765977]  [<ffffffffa037cdad>] dmu_read_uio+0x41/0xc1 [zfs]\n<4>[200822.766067]  [<ffffffffa03e04cd>] zfs_read+0x333/0x3dc [zfs]\n<4>[200822.766156]  [<ffffffffa03ef51b>] zpl_read_common+0x4d/0x65 [zfs]\n<4>[200822.766216]  [<ffffffffa016b546>] ? crhold+0x19/0x1c [spl]\n<4>[200822.766304]  [<ffffffffa03ef58c>] zpl_read+0x59/0x7d [zfs]\n<4>[200822.766351]  [<ffffffff8110eb7a>] vfs_read+0x98/0xfa\n<4>[200822.766397]  [<ffffffff8110ec1a>] sys_read+0x3e/0x6b\n<4>[200822.766443]  [<ffffffff813846f9>] system_call_fastpath+0x16/0x1b\n<6>[200822.766490] java            D ffff8802ad706188     0 16661      1 0x00000000\n<4>[200822.766582]  ffff8803251e57e8 0000000000000082 ffff8802ad705dc0 ffff8803251e5fd8\n<4>[200822.766729]  ffff8803251e5fd8 0000000000013600 ffff88033149ce20 ffff8802ad705dc0\n<4>[200822.766877]  ffff8803251e57f8 ffff8803294cbcd8 ffff8803294cbca8 0000000000000002\n<4>[200822.767024] Call Trace:\n<4>[200822.767066]  [<ffffffff8137e95a>] schedule+0x60/0x62\n<4>[200822.767125]  [<ffffffffa016ae6a>] cv_wait_common+0xc9/0x15d [spl]\n<4>[200822.767173]  [<ffffffff810572e9>] ? abort_exclusive_wait+0x8a/0x8a\n<4>[200822.767232]  [<ffffffffa016af1c>] __cv_wait+0xe/0x12 [spl]\n<4>[200822.767319]  [<ffffffffa03b1d7e>] spa_config_enter+0x66/0xd9 [zfs]\n<4>[200822.767409]  [<ffffffffa03ec5d0>] zio_vdev_io_start+0x42/0x21e [zfs]\n<4>[200822.767500]  [<ffffffffa03eb593>] zio_nowait+0xd7/0xfe [zfs]\n<4>[200822.767568]  [<ffffffffa037246c>] arc_read_nolock+0x660/0x671 [zfs]\n<4>[200822.767640]  [<ffffffffa0372515>] arc_read+0x98/0xfe [zfs]\n<4>[200822.767710]  [<ffffffffa0376d38>] ? dmu_buf_rele+0x2c/0x2c [zfs]\n<4>[200822.767794]  [<ffffffffa039c5d1>] dsl_read+0x2c/0x2e [zfs]\n<4>[200822.767864]  [<ffffffffa0375d2c>] dbuf_read+0x392/0x4c7 [zfs]\n<4>[200822.767935]  [<ffffffffa0376fa2>] __dbuf_hold_impl+0x196/0x3ab [zfs]\n<4>[200822.768008]  [<ffffffffa0377234>] dbuf_hold_impl+0x7d/0x9f [zfs]\n<4>[200822.768079]  [<ffffffffa0377ea3>] dbuf_hold+0x1b/0x29 [zfs]\n<4>[200822.768151]  [<ffffffffa037b987>] dmu_buf_hold_array_by_dnode+0x194/0x320 [zfs]\n<4>[200822.768245]  [<ffffffffa0387d9f>] ? dnode_hold_impl+0x34b/0x364 [zfs]\n<4>[200822.768322]  [<ffffffffa037bb6b>] dmu_buf_hold_array+0x58/0x79 [zfs]\n<4>[200822.768399]  [<ffffffffa037cdad>] dmu_read_uio+0x41/0xc1 [zfs]\n<4>[200822.768489]  [<ffffffffa03e04cd>] zfs_read+0x333/0x3dc [zfs]\n<4>[200822.768578]  [<ffffffffa03ef51b>] zpl_read_common+0x4d/0x65 [zfs]\n<4>[200822.768639]  [<ffffffffa016b546>] ? crhold+0x19/0x1c [spl]\n<4>[200822.768726]  [<ffffffffa03ef58c>] zpl_read+0x59/0x7d [zfs]\n<4>[200822.768773]  [<ffffffff8110eb7a>] vfs_read+0x98/0xfa\n<4>[200822.768819]  [<ffffffff8110ec1a>] sys_read+0x3e/0x6b\n<4>[200822.768865]  [<ffffffff813846f9>] system_call_fastpath+0x16/0x1b\n<6>[200822.768912] java            D ffff8802ad704248     0 16662      1 0x00000000\n<4>[200822.769003]  ffff88027567d8a8 0000000000000082 ffff8802ad703e80 ffff88027567dfd8\n<4>[200822.769152]  ffff88027567dfd8 0000000000013600 ffff88033149ce20 ffff8802ad703e80\n<4>[200822.769299]  ffff88027567d8b8 ffff8803294cbcd8 ffff8803294cbca8 0000000000000002\n<4>[200822.769447] Call Trace:\n<4>[200822.769489]  [<ffffffff8137e95a>] schedule+0x60/0x62\n<4>[200822.769547]  [<ffffffffa016ae6a>] cv_wait_common+0xc9/0x15d [spl]\n<4>[200822.769596]  [<ffffffff810572e9>] ? abort_exclusive_wait+0x8a/0x8a\n<4>[200822.769654]  [<ffffffffa016af1c>] __cv_wait+0xe/0x12 [spl]\n<4>[200822.769742]  [<ffffffffa03b1d7e>] spa_config_enter+0x66/0xd9 [zfs]\n<4>[200822.769832]  [<ffffffffa03ec5d0>] zio_vdev_io_start+0x42/0x21e [zfs]\n<4>[200822.769922]  [<ffffffffa03eb593>] zio_nowait+0xd7/0xfe [zfs]\n<4>[200822.769991]  [<ffffffffa037246c>] arc_read_nolock+0x660/0x671 [zfs]\n<4>[200822.770062]  [<ffffffffa0372515>] arc_read+0x98/0xfe [zfs]\n<4>[200822.770132]  [<ffffffffa0376d38>] ? dmu_buf_rele+0x2c/0x2c [zfs]\n<4>[200822.770217]  [<ffffffffa039c5d1>] dsl_read+0x2c/0x2e [zfs]\n<4>[200822.770287]  [<ffffffffa0375d2c>] dbuf_read+0x392/0x4c7 [zfs]\n<4>[200822.770362]  [<ffffffffa037b9e8>] dmu_buf_hold_array_by_dnode+0x1f5/0x320 [zfs]\n<4>[200822.770453]  [<ffffffffa037bb6b>] dmu_buf_hold_array+0x58/0x79 [zfs]\n<4>[200822.770530]  [<ffffffffa037cdad>] dmu_read_uio+0x41/0xc1 [zfs]\n<4>[200822.770620]  [<ffffffffa03e04cd>] zfs_read+0x333/0x3dc [zfs]\n<4>[200822.770710]  [<ffffffffa03ef51b>] zpl_read_common+0x4d/0x65 [zfs]\n<4>[200822.770770]  [<ffffffffa016b546>] ? crhold+0x19/0x1c [spl]\n<4>[200822.770858]  [<ffffffffa03ef58c>] zpl_read+0x59/0x7d [zfs]\n<4>[200822.770905]  [<ffffffff8110eb7a>] vfs_read+0x98/0xfa\n<4>[200822.770950]  [<ffffffff8110ec1a>] sys_read+0x3e/0x6b\n<4>[200822.770997]  [<ffffffff813846f9>] system_call_fastpath+0x16/0x1b\n<6>[200822.771043] java            D ffff8802ad7078f8     0 16663      1 0x00000000\n<4>[200822.771135]  ffff880119bdd8a8 0000000000000082 ffff8802ad707530 ffff880119bddfd8\n<4>[200822.771283]  ffff880119bddfd8 0000000000013600 ffff88033149ce20 ffff8802ad707530\n<4>[200822.771430]  ffff880119bdd8b8 ffff8803294cbcd8 ffff8803294cbca8 0000000000000002\n<4>[200822.771578] Call Trace:\n<4>[200822.771620]  [<ffffffff8137e95a>] schedule+0x60/0x62\n<4>[200822.771678]  [<ffffffffa016ae6a>] cv_wait_common+0xc9/0x15d [spl]\n<4>[200822.771726]  [<ffffffff810572e9>] ? abort_exclusive_wait+0x8a/0x8a\n<4>[200822.771785]  [<ffffffffa016af1c>] __cv_wait+0xe/0x12 [spl]\n<4>[200822.771873]  [<ffffffffa03b1d7e>] spa_config_enter+0x66/0xd9 [zfs]\n<4>[200822.771962]  [<ffffffffa03ec5d0>] zio_vdev_io_start+0x42/0x21e [zfs]\n<4>[200822.772053]  [<ffffffffa03eb593>] zio_nowait+0xd7/0xfe [zfs]\n<4>[200822.772121]  [<ffffffffa037246c>] arc_read_nolock+0x660/0x671 [zfs]\n<4>[200822.772193]  [<ffffffffa0372515>] arc_read+0x98/0xfe [zfs]\n<4>[200822.772263]  [<ffffffffa0376d38>] ? dmu_buf_rele+0x2c/0x2c [zfs]\n<4>[200822.772347]  [<ffffffffa039c5d1>] dsl_read+0x2c/0x2e [zfs]\n<4>[200822.772417]  [<ffffffffa0375d2c>] dbuf_read+0x392/0x4c7 [zfs]\n<4>[200822.772492]  [<ffffffffa037b9e8>] dmu_buf_hold_array_by_dnode+0x1f5/0x320 [zfs]\n<4>[200822.772583]  [<ffffffffa037bb6b>] dmu_buf_hold_array+0x58/0x79 [zfs]\n<4>[200822.772660]  [<ffffffffa037cdad>] dmu_read_uio+0x41/0xc1 [zfs]\n<4>[200822.772751]  [<ffffffffa03e04cd>] zfs_read+0x333/0x3dc [zfs]\n<4>[200822.772840]  [<ffffffffa03ef51b>] zpl_read_common+0x4d/0x65 [zfs]\n<4>[200822.772900]  [<ffffffffa016b546>] ? crhold+0x19/0x1c [spl]\n<4>[200822.772987]  [<ffffffffa03ef58c>] zpl_read+0x59/0x7d [zfs]\n<4>[200822.773034]  [<ffffffff8110eb7a>] vfs_read+0x98/0xfa\n<4>[200822.773080]  [<ffffffff8110ec1a>] sys_read+0x3e/0x6b\n<4>[200822.773126]  [<ffffffff813846f9>] system_call_fastpath+0x16/0x1b\n<6>[200822.773173] java            D ffff8802ad704a18     0 16666      1 0x00000000\n<4>[200822.773265]  ffff8801c96b18a8 0000000000000082 ffff8802ad704650 ffff8801c96b1fd8\n<4>[200822.773413]  ffff8801c96b1fd8 0000000000013600 ffff88033149ce20 ffff8802ad704650\n<4>[200822.773560]  ffff8801c96b18b8 ffff8803294cbcd8 ffff8803294cbca8 0000000000000002\n<4>[200822.773708] Call Trace:\n<4>[200822.773750]  [<ffffffff8137e95a>] schedule+0x60/0x62\n<4>[200822.773808]  [<ffffffffa016ae6a>] cv_wait_common+0xc9/0x15d [spl]\n<4>[200822.773857]  [<ffffffff810572e9>] ? abort_exclusive_wait+0x8a/0x8a\n<4>[200822.773916]  [<ffffffffa016af1c>] __cv_wait+0xe/0x12 [spl]\n<4>[200822.774002]  [<ffffffffa03b1d7e>] spa_config_enter+0x66/0xd9 [zfs]\n<4>[200822.774092]  [<ffffffffa03ec5d0>] zio_vdev_io_start+0x42/0x21e [zfs]\n<4>[200822.774183]  [<ffffffffa03eb593>] zio_nowait+0xd7/0xfe [zfs]\n<4>[200822.774251]  [<ffffffffa037246c>] arc_read_nolock+0x660/0x671 [zfs]\n<4>[200822.774323]  [<ffffffffa0372515>] arc_read+0x98/0xfe [zfs]\n<4>[200822.774392]  [<ffffffffa0376d38>] ? dmu_buf_rele+0x2c/0x2c [zfs]\n<4>[200822.774477]  [<ffffffffa039c5d1>] dsl_read+0x2c/0x2e [zfs]\n<4>[200822.774546]  [<ffffffffa0375d2c>] dbuf_read+0x392/0x4c7 [zfs]\n<4>[200822.774621]  [<ffffffffa037b9e8>] dmu_buf_hold_array_by_dnode+0x1f5/0x320 [zfs]\n<4>[200822.774713]  [<ffffffffa037bb6b>] dmu_buf_hold_array+0x58/0x79 [zfs]\n<4>[200822.774790]  [<ffffffffa037cdad>] dmu_read_uio+0x41/0xc1 [zfs]\n<4>[200822.774880]  [<ffffffffa03e04cd>] zfs_read+0x333/0x3dc [zfs]\n<4>[200822.774969]  [<ffffffffa03ef51b>] zpl_read_common+0x4d/0x65 [zfs]\n<4>[200822.775030]  [<ffffffffa016b546>] ? crhold+0x19/0x1c [spl]\n<4>[200822.775117]  [<ffffffffa03ef58c>] zpl_read+0x59/0x7d [zfs]\n<4>[200822.779082]  [<ffffffff8110eb7a>] vfs_read+0x98/0xfa\n<4>[200822.779128]  [<ffffffff8110ec1a>] sys_read+0x3e/0x6b\n<4>[200822.779174]  [<ffffffff813846f9>] system_call_fastpath+0x16/0x1b\n<6>[200822.779221] java            D ffff8802ad702308     0 16671      1 0x00000000\n<4>[200822.779312]  ffff8801c8d8b8a8 0000000000000082 ffff8802ad701f40 ffff8801c8d8bfd8\n<4>[200822.779460]  ffff8801c8d8bfd8 0000000000013600 ffff88033149e590 ffff8802ad701f40\n<4>[200822.779608]  ffff8801c8d8b8b8 ffff8803294cbcd8 ffff8803294cbca8 0000000000000002\n<4>[200822.779755] Call Trace:\n<4>[200822.779798]  [<ffffffff8137e95a>] schedule+0x60/0x62\n<4>[200822.779856]  [<ffffffffa016ae6a>] cv_wait_common+0xc9/0x15d [spl]\n<4>[200822.779904]  [<ffffffff810572e9>] ? abort_exclusive_wait+0x8a/0x8a\n<4>[200822.779963]  [<ffffffffa016af1c>] __cv_wait+0xe/0x12 [spl]\n<4>[200822.780051]  [<ffffffffa03b1d7e>] spa_config_enter+0x66/0xd9 [zfs]\n<4>[200822.780141]  [<ffffffffa03ec5d0>] zio_vdev_io_start+0x42/0x21e [zfs]\n<4>[200822.780231]  [<ffffffffa03eb593>] zio_nowait+0xd7/0xfe [zfs]\n<4>[200822.780300]  [<ffffffffa037246c>] arc_read_nolock+0x660/0x671 [zfs]\n<4>[200822.780371]  [<ffffffffa0372515>] arc_read+0x98/0xfe [zfs]\n<4>[200822.780441]  [<ffffffffa0376d38>] ? dmu_buf_rele+0x2c/0x2c [zfs]\n<4>[200822.780525]  [<ffffffffa039c5d1>] dsl_read+0x2c/0x2e [zfs]\n<4>[200822.780595]  [<ffffffffa0375d2c>] dbuf_read+0x392/0x4c7 [zfs]\n<4>[200822.780671]  [<ffffffffa037b9e8>] dmu_buf_hold_array_by_dnode+0x1f5/0x320 [zfs]\n<4>[200822.780762]  [<ffffffffa037bb6b>] dmu_buf_hold_array+0x58/0x79 [zfs]\n<4>[200822.780839]  [<ffffffffa037cdad>] dmu_read_uio+0x41/0xc1 [zfs]\n<4>[200822.780929]  [<ffffffffa03e04cd>] zfs_read+0x333/0x3dc [zfs]\n<4>[200822.781018]  [<ffffffffa03ef51b>] zpl_read_common+0x4d/0x65 [zfs]\n<4>[200822.781078]  [<ffffffffa016b546>] ? crhold+0x19/0x1c [spl]\n<4>[200822.781166]  [<ffffffffa03ef58c>] zpl_read+0x59/0x7d [zfs]\n<4>[200822.781213]  [<ffffffff8110eb7a>] vfs_read+0x98/0xfa\n<4>[200822.781259]  [<ffffffff8110ec1a>] sys_read+0x3e/0x6b\n<4>[200822.781305]  [<ffffffff813846f9>] system_call_fastpath+0x16/0x1b\n<6>[200822.781352] java            D ffff8802ad7051e8     0 16674      1 0x00000000\n<4>[200822.781443]  ffff8801286857e8 0000000000000082 ffff8802ad704e20 ffff880128685fd8\n<4>[200822.781591]  ffff880128685fd8 0000000000013600 ffff88033149ce20 ffff8802ad704e20\n<4>[200822.781738]  ffff8801286857f8 ffff8803294cbcd8 ffff8803294cbca8 0000000000000002\n<4>[200822.781886] Call Trace:\n<4>[200822.781928]  [<ffffffff8137e95a>] schedule+0x60/0x62\n<4>[200822.781986]  [<ffffffffa016ae6a>] cv_wait_common+0xc9/0x15d [spl]\n<4>[200822.782035]  [<ffffffff810572e9>] ? abort_exclusive_wait+0x8a/0x8a\n<4>[200822.782094]  [<ffffffffa016af1c>] __cv_wait+0xe/0x12 [spl]\n<4>[200822.782181]  [<ffffffffa03b1d7e>] spa_config_enter+0x66/0xd9 [zfs]\n<4>[200822.782271]  [<ffffffffa03ec5d0>] zio_vdev_io_start+0x42/0x21e [zfs]\n<4>[200822.782361]  [<ffffffffa03eb593>] zio_nowait+0xd7/0xfe [zfs]\n<4>[200822.782430]  [<ffffffffa037246c>] arc_read_nolock+0x660/0x671 [zfs]\n<4>[200822.782501]  [<ffffffffa0372515>] arc_read+0x98/0xfe [zfs]\n<4>[200822.782571]  [<ffffffffa0376d38>] ? dmu_buf_rele+0x2c/0x2c [zfs]\n<4>[200822.782655]  [<ffffffffa039c5d1>] dsl_read+0x2c/0x2e [zfs]\n<4>[200822.782725]  [<ffffffffa0375d2c>] dbuf_read+0x392/0x4c7 [zfs]\n<4>[200822.782796]  [<ffffffffa0376fa2>] __dbuf_hold_impl+0x196/0x3ab [zfs]\n<4>[200822.782869]  [<ffffffffa0377234>] dbuf_hold_impl+0x7d/0x9f [zfs]\n<4>[200822.782940]  [<ffffffffa0377ea3>] dbuf_hold+0x1b/0x29 [zfs]\n<4>[200822.783013]  [<ffffffffa037b987>] dmu_buf_hold_array_by_dnode+0x194/0x320 [zfs]\n<4>[200822.783107]  [<ffffffffa0387d9f>] ? dnode_hold_impl+0x34b/0x364 [zfs]\n<4>[200822.783183]  [<ffffffffa037bb6b>] dmu_buf_hold_array+0x58/0x79 [zfs]\n<4>[200822.783260]  [<ffffffffa037cdad>] dmu_read_uio+0x41/0xc1 [zfs]\n<4>[200822.783350]  [<ffffffffa03e04cd>] zfs_read+0x333/0x3dc [zfs]\n<4>[200822.783440]  [<ffffffffa03ef51b>] zpl_read_common+0x4d/0x65 [zfs]\n<4>[200822.783500]  [<ffffffffa016b546>] ? crhold+0x19/0x1c [spl]\n<4>[200822.783587]  [<ffffffffa03ef58c>] zpl_read+0x59/0x7d [zfs]\n<4>[200822.783635]  [<ffffffff8110eb7a>] vfs_read+0x98/0xfa\n<4>[200822.783680]  [<ffffffff8110ec1a>] sys_read+0x3e/0x6b\n<4>[200822.783726]  [<ffffffff813846f9>] system_call_fastpath+0x16/0x1b\n<6>[200822.783773] java            D ffff8802ad703a78     0 16677      1 0x00000000\n<4>[200822.783865]  ffff8801f5e85a38 0000000000000082 ffff8802ad7036b0 ffff8801f5e85fd8\n<4>[200822.784012]  ffff8801f5e85fd8 0000000000013600 ffff88033149be80 ffff8802ad7036b0\n<4>[200822.784160]  ffff8801f5e85a48 ffff88001e9d5610 ffff88001e9d55d8 0000000000000002\n<4>[200822.784308] Call Trace:\n<4>[200822.784350]  [<ffffffff8137e95a>] schedule+0x60/0x62\n<4>[200822.784408]  [<ffffffffa016ae6a>] cv_wait_common+0xc9/0x15d [spl]\n<4>[200822.784456]  [<ffffffff810572e9>] ? abort_exclusive_wait+0x8a/0x8a\n<4>[200822.784515]  [<ffffffffa016af1c>] __cv_wait+0xe/0x12 [spl]\n<4>[200822.784584]  [<ffffffffa0375e21>] dbuf_read+0x487/0x4c7 [zfs]\n<4>[200822.784654]  [<ffffffffa03752ea>] ? dbuf_find+0xb4/0xc6 [zfs]\n<4>[200822.784726]  [<ffffffffa037718b>] ? __dbuf_hold_impl+0x37f/0x3ab [zfs]\n<4>[200822.784800]  [<ffffffffa0376fa2>] __dbuf_hold_impl+0x196/0x3ab [zfs]\n<4>[200822.784874]  [<ffffffffa0377234>] dbuf_hold_impl+0x7d/0x9f [zfs]\n<4>[200822.784945]  [<ffffffffa0377ea3>] dbuf_hold+0x1b/0x29 [zfs]\n<4>[200822.785018]  [<ffffffffa037b987>] dmu_buf_hold_array_by_dnode+0x194/0x320 [zfs]\n<4>[200822.785112]  [<ffffffffa0387d9f>] ? dnode_hold_impl+0x34b/0x364 [zfs]\n<4>[200822.785188]  [<ffffffffa037bb6b>] dmu_buf_hold_array+0x58/0x79 [zfs]\n<4>[200822.785265]  [<ffffffffa037cdad>] dmu_read_uio+0x41/0xc1 [zfs]\n<4>[200822.785355]  [<ffffffffa03e04cd>] zfs_read+0x333/0x3dc [zfs]\n<4>[200822.785444]  [<ffffffffa03ef51b>] zpl_read_common+0x4d/0x65 [zfs]\n<4>[200822.785505]  [<ffffffffa016b546>] ? crhold+0x19/0x1c [spl]\n<4>[200822.785592]  [<ffffffffa03ef58c>] zpl_read+0x59/0x7d [zfs]\n<4>[200822.785639]  [<ffffffff8110eb7a>] vfs_read+0x98/0xfa\n<4>[200822.785685]  [<ffffffff8110ec1a>] sys_read+0x3e/0x6b\n<4>[200822.785731]  [<ffffffff813846f9>] system_call_fastpath+0x16/0x1b\n<6>[200822.785778] java            D ffff8802ad7003c8     0 16680      1 0x00000000\n<4>[200822.785870]  ffff88026a8338a8 0000000000000082 ffff8802ad700000 ffff88026a833fd8\n<4>[200822.786017]  ffff88026a833fd8 0000000000013600 ffff88033149e590 ffff8802ad700000\n<4>[200822.786165]  ffff88026a8338b8 ffff8803294cbcd8 ffff8803294cbca8 0000000000000002\n<4>[200822.786313] Call Trace:\n<4>[200822.786355]  [<ffffffff8137e95a>] schedule+0x60/0x62\n<4>[200822.786413]  [<ffffffffa016ae6a>] cv_wait_common+0xc9/0x15d [spl]\n<4>[200822.786461]  [<ffffffff810572e9>] ? abort_exclusive_wait+0x8a/0x8a\n<4>[200822.786520]  [<ffffffffa016af1c>] __cv_wait+0xe/0x12 [spl]\n<4>[200822.786608]  [<ffffffffa03b1d7e>] spa_config_enter+0x66/0xd9 [zfs]\n<4>[200822.786698]  [<ffffffffa03ec5d0>] zio_vdev_io_start+0x42/0x21e [zfs]\n<4>[200822.786789]  [<ffffffffa03eb593>] zio_nowait+0xd7/0xfe [zfs]\n<4>[200822.786857]  [<ffffffffa037246c>] arc_read_nolock+0x660/0x671 [zfs]\n<4>[200822.786929]  [<ffffffffa0372515>] arc_read+0x98/0xfe [zfs]\n<4>[200822.786999]  [<ffffffffa0376d38>] ? dmu_buf_rele+0x2c/0x2c [zfs]\n<4>[200822.787083]  [<ffffffffa039c5d1>] dsl_read+0x2c/0x2e [zfs]\n<4>[200822.787153]  [<ffffffffa0375d2c>] dbuf_read+0x392/0x4c7 [zfs]\n<4>[200822.787228]  [<ffffffffa037b9e8>] dmu_buf_hold_array_by_dnode+0x1f5/0x320 [zfs]\n<4>[200822.787319]  [<ffffffffa037bb6b>] dmu_buf_hold_array+0x58/0x79 [zfs]\n<4>[200822.787396]  [<ffffffffa037cdad>] dmu_read_uio+0x41/0xc1 [zfs]\n<4>[200822.787486]  [<ffffffffa03e04cd>] zfs_read+0x333/0x3dc [zfs]\n<4>[200822.787575]  [<ffffffffa03ef51b>] zpl_read_common+0x4d/0x65 [zfs]\n<4>[200822.787635]  [<ffffffffa016b546>] ? crhold+0x19/0x1c [spl]\n<4>[200822.787723]  [<ffffffffa03ef58c>] zpl_read+0x59/0x7d [zfs]\n<4>[200822.787770]  [<ffffffff8110eb7a>] vfs_read+0x98/0xfa\n<4>[200822.787815]  [<ffffffff8110ec1a>] sys_read+0x3e/0x6b\n<4>[200822.787862]  [<ffffffff813846f9>] system_call_fastpath+0x16/0x1b\n<6>[200822.787908] java            D ffff8802ad701b38     0 16681      1 0x00000000\n<4>[200822.788000]  ffff88016fb2d7e8 0000000000000082 ffff8802ad701770 ffff88016fb2dfd8\n<4>[200822.788148]  ffff88016fb2dfd8 0000000000013600 ffff88033149e590 ffff8802ad701770\n<4>[200822.788295]  ffff88016fb2d7f8 ffff8803294cbcd8 ffff8803294cbca8 0000000000000002\n<4>[200822.788443] Call Trace:\n<4>[200822.788485]  [<ffffffff8137e95a>] schedule+0x60/0x62\n<4>[200822.788543]  [<ffffffffa016ae6a>] cv_wait_common+0xc9/0x15d [spl]\n<4>[200822.788591]  [<ffffffff810572e9>] ? abort_exclusive_wait+0x8a/0x8a\n<4>[200822.788650]  [<ffffffffa016af1c>] __cv_wait+0xe/0x12 [spl]\n<4>[200822.788738]  [<ffffffffa03b1d7e>] spa_config_enter+0x66/0xd9 [zfs]\n<4>[200822.788827]  [<ffffffffa03ec5d0>] zio_vdev_io_start+0x42/0x21e [zfs]\n<4>[200822.788918]  [<ffffffffa03eb593>] zio_nowait+0xd7/0xfe [zfs]\n<4>[200822.788986]  [<ffffffffa037246c>] arc_read_nolock+0x660/0x671 [zfs]\n<4>[200822.789058]  [<ffffffffa0372515>] arc_read+0x98/0xfe [zfs]\n<4>[200822.789127]  [<ffffffffa0376d38>] ? dmu_buf_rele+0x2c/0x2c [zfs]\n<4>[200822.789212]  [<ffffffffa039c5d1>] dsl_read+0x2c/0x2e [zfs]\n<4>[200822.789282]  [<ffffffffa0375d2c>] dbuf_read+0x392/0x4c7 [zfs]\n<4>[200822.789352]  [<ffffffffa0376fa2>] __dbuf_hold_impl+0x196/0x3ab [zfs]\n<4>[200822.789426]  [<ffffffffa0377234>] dbuf_hold_impl+0x7d/0x9f [zfs]\n<4>[200822.789497]  [<ffffffffa0377ea3>] dbuf_hold+0x1b/0x29 [zfs]\n<4>[200822.789570]  [<ffffffffa037b987>] dmu_buf_hold_array_by_dnode+0x194/0x320 [zfs]\n<4>[200822.789664]  [<ffffffffa0387d9f>] ? dnode_hold_impl+0x34b/0x364 [zfs]\n<4>[200822.789740]  [<ffffffffa037bb6b>] dmu_buf_hold_array+0x58/0x79 [zfs]\n<4>[200822.789817]  [<ffffffffa037cdad>] dmu_read_uio+0x41/0xc1 [zfs]\n<4>[200822.789907]  [<ffffffffa03e04cd>] zfs_read+0x333/0x3dc [zfs]\n<4>[200822.789996]  [<ffffffffa03ef51b>] zpl_read_common+0x4d/0x65 [zfs]\n<4>[200822.790057]  [<ffffffffa016b546>] ? crhold+0x19/0x1c [spl]\n<4>[200822.790144]  [<ffffffffa03ef58c>] zpl_read+0x59/0x7d [zfs]\n<4>[200822.790191]  [<ffffffff8110eb7a>] vfs_read+0x98/0xfa\n<4>[200822.790237]  [<ffffffff8110ec1a>] sys_read+0x3e/0x6b\n<4>[200822.790283]  [<ffffffff813846f9>] system_call_fastpath+0x16/0x1b\n<6>[200822.790330] java            D ffff8802ca568b98     0 16683      1 0x00000000\n<4>[200822.790421]  ffff880158113bb8 0000000000000082 ffff8802ca5687d0 ffff880158113fd8\n<4>[200822.790569]  ffff880158113fd8 0000000000013600 ffff88033149ce20 ffff8802ca5687d0\n<4>[200822.790717]  ffff880158113bc8 ffff880309351b10 ffff880309351ad8 0000000000000002\n<4>[200822.790865] Call Trace:\n<4>[200822.790907]  [<ffffffff8137e95a>] schedule+0x60/0x62\n<4>[200822.790965]  [<ffffffffa016ae6a>] cv_wait_common+0xc9/0x15d [spl]\n<4>[200822.791013]  [<ffffffff810572e9>] ? abort_exclusive_wait+0x8a/0x8a\n<4>[200822.791072]  [<ffffffffa016af1c>] __cv_wait+0xe/0x12 [spl]\n<4>[200822.791144]  [<ffffffffa037bab7>] dmu_buf_hold_array_by_dnode+0x2c4/0x320 [zfs]\n<4>[200822.791236]  [<ffffffffa037bb6b>] dmu_buf_hold_array+0x58/0x79 [zfs]\n<4>[200822.795230]  [<ffffffffa037cdad>] dmu_read_uio+0x41/0xc1 [zfs]\n<4>[200822.795320]  [<ffffffffa03e04cd>] zfs_read+0x333/0x3dc [zfs]\n<4>[200822.795410]  [<ffffffffa03ef51b>] zpl_read_common+0x4d/0x65 [zfs]\n<4>[200822.795470]  [<ffffffffa016b546>] ? crhold+0x19/0x1c [spl]\n<4>[200822.795558]  [<ffffffffa03ef58c>] zpl_read+0x59/0x7d [zfs]\n<4>[200822.795605]  [<ffffffff8110eb7a>] vfs_read+0x98/0xfa\n<4>[200822.795651]  [<ffffffff8110ec1a>] sys_read+0x3e/0x6b\n<4>[200822.795697]  [<ffffffff813846f9>] system_call_fastpath+0x16/0x1b\n<6>[200822.795743] java            D ffff8802ca569b38     0 16685      1 0x00000000\n<4>[200822.795835]  ffff88012ac257e8 0000000000000082 ffff8802ca569770 ffff88012ac25fd8\n<4>[200822.795983]  ffff88012ac25fd8 0000000000013600 ffff88033149e590 ffff8802ca569770\n<4>[200822.796130]  ffff88012ac257f8 ffff8803294cbcd8 ffff8803294cbca8 0000000000000002\n<4>[200822.796278] Call Trace:\n<4>[200822.796320]  [<ffffffff8137e95a>] schedule+0x60/0x62\n<4>[200822.796378]  [<ffffffffa016ae6a>] cv_wait_common+0xc9/0x15d [spl]\n<4>[200822.796427]  [<ffffffff810572e9>] ? abort_exclusive_wait+0x8a/0x8a\n<4>[200822.796486]  [<ffffffffa016af1c>] __cv_wait+0xe/0x12 [spl]\n<4>[200822.796573]  [<ffffffffa03b1d7e>] spa_config_enter+0x66/0xd9 [zfs]\n<4>[200822.796663]  [<ffffffffa03ec5d0>] zio_vdev_io_start+0x42/0x21e [zfs]\n<4>[200822.796753]  [<ffffffffa03eb593>] zio_nowait+0xd7/0xfe [zfs]\n<4>[200822.796822]  [<ffffffffa037246c>] arc_read_nolock+0x660/0x671 [zfs]\n<4>[200822.796894]  [<ffffffffa0372515>] arc_read+0x98/0xfe [zfs]\n<4>[200822.796964]  [<ffffffffa0376d38>] ? dmu_buf_rele+0x2c/0x2c [zfs]\n<4>[200822.797048]  [<ffffffffa039c5d1>] dsl_read+0x2c/0x2e [zfs]\n<4>[200822.797117]  [<ffffffffa0375d2c>] dbuf_read+0x392/0x4c7 [zfs]\n<4>[200822.797188]  [<ffffffffa0376fa2>] __dbuf_hold_impl+0x196/0x3ab [zfs]\n<4>[200822.797262]  [<ffffffffa0377234>] dbuf_hold_impl+0x7d/0x9f [zfs]\n<4>[200822.797333]  [<ffffffffa0377ea3>] dbuf_hold+0x1b/0x29 [zfs]\n<4>[200822.797405]  [<ffffffffa037b987>] dmu_buf_hold_array_by_dnode+0x194/0x320 [zfs]\n<4>[200822.797499]  [<ffffffffa0387d9f>] ? dnode_hold_impl+0x34b/0x364 [zfs]\n<4>[200822.797576]  [<ffffffffa037bb6b>] dmu_buf_hold_array+0x58/0x79 [zfs]\n<4>[200\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/12193669/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45441049", "body": "Not sure. I don't run ZOL anymore, so I can't test. Closing.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45441049/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/14181712", "body": "For whatever reason, I woke up this morning and it was working. Resilver is progressing normally now. Closing.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/14181712/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ScOut3R": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/258657202", "body": "@tuxoko I have #4672 since I am running 0.6.5.8 (the package in Ubuntu 16.10, compiled on 16.04, with kernel 4.4.0-45-generic). As soon as I remove a snapshot the kernel starts logging the error shown in OP. As a workaround I tried unmounting the snapshot before removing, but that didn't help.\nWhat other information should I provide to help diagnose the issue?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/258657202/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "georgyo": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/150922987", "body": "I have also noticed this problem.\n\nI have 4 1TB SSDs, split into two mirrors. I can read at about 1GB/s and write around 600MB/s sustained.\n\nHowever doing a cp the speed is capped at 12MB/s. Painfully slow.\n\nWorking with a 40gig file, it is 20x faster to copy the file to another partition on the drives and copy it back. Doing it this way results in a copy at a speed of around 400MB/s  in both directions. \n\nThis also removes the theory of drive io being the bottle neck. It is 100% a problem of copy on the same zpool.\n\nEach drive is partitioned exactly the same\n\n```\nNumber  Start   End     Size    File system  Name  Flags\n 1      1049kB  2149MB  2147MB  fat32              boot, legacy_boot, esp\n 2      2149MB  4296MB  2147MB  ext4               bios_grub\n 3      4296MB  38.7GB  34.4GB\n 4      38.7GB  1024GB  986GB\n```\n\nAnd here is a copy of my zdb output.\n\n```\nrpool:\n    version: 5000\n    name: 'rpool'\n    state: 0\n    txg: 7382685\n    pool_guid: 9801629415702614327\n    errata: 0\n    hostid: 2831200052\n    hostname: 'bigtower'\n    vdev_children: 2\n    vdev_tree:\n        type: 'root'\n        id: 0\n        guid: 9801629415702614327\n        children[0]:\n            type: 'mirror'\n            id: 0\n            guid: 9234036754859107455\n            whole_disk: 0\n            metaslab_array: 37\n            metaslab_shift: 28\n            ashift: 9\n            asize: 985548980224\n            is_log: 0\n            create_txg: 4\n            children[0]:\n                type: 'disk'\n                id: 0\n                guid: 107605033391006887\n                path: '/dev/sdb4'\n                whole_disk: 0\n                DTL: 6135\n                create_txg: 4\n            children[1]:\n                type: 'disk'\n                id: 1\n                guid: 7474249608093431881\n                path: '/dev/sda4'\n                whole_disk: 0\n                DTL: 6133\n                create_txg: 4\n        children[1]:\n            type: 'mirror'\n            id: 1\n            guid: 1585376546109541994\n            whole_disk: 0\n            metaslab_array: 34\n            metaslab_shift: 28\n            ashift: 9\n            asize: 985548980224\n            is_log: 0\n            create_txg: 4\n            children[0]:\n                type: 'disk'\n                id: 0\n                guid: 4044387596452549632\n                path: '/dev/sdd4'\n                whole_disk: 0\n                DTL: 1614\n                create_txg: 4\n            children[1]:\n                type: 'disk'\n                id: 1\n                guid: 168694465845471017\n                path: '/dev/sdc4'\n                whole_disk: 0\n                DTL: 1616\n                create_txg: 4\n    features_for_read:\n        com.delphix:hole_birth\n        com.delphix:embedded_data\nztemp:\n    version: 5000\n    name: 'ztemp'\n    state: 0\n    txg: 4\n    pool_guid: 14071170611957067770\n    errata: 0\n    hostid: 2831200052\n    hostname: 'bigtower'\n    vdev_children: 4\n    vdev_tree:\n        type: 'root'\n        id: 0\n        guid: 14071170611957067770\n        create_txg: 4\n        children[0]:\n            type: 'disk'\n            id: 0\n            guid: 6092877110596562040\n            path: '/dev/sda3'\n            whole_disk: 0\n            metaslab_array: 39\n            metaslab_shift: 28\n            ashift: 9\n            asize: 34355019776\n            is_log: 0\n            create_txg: 4\n        children[1]:\n            type: 'disk'\n            id: 1\n            guid: 1539552750830704949\n            path: '/dev/sdb3'\n            whole_disk: 0\n            metaslab_array: 37\n            metaslab_shift: 28\n            ashift: 9\n            asize: 34355019776\n            is_log: 0\n            create_txg: 4\n        children[2]:\n            type: 'disk'\n            id: 2\n            guid: 8574376514556456649\n            path: '/dev/sdc3'\n            whole_disk: 0\n            metaslab_array: 36\n            metaslab_shift: 28\n            ashift: 9\n            asize: 34355019776\n            is_log: 0\n            create_txg: 4\n        children[3]:\n            type: 'disk'\n            id: 3\n            guid: 12062958083244608527\n            path: '/dev/sdd3'\n            whole_disk: 0\n            metaslab_array: 34\n            metaslab_shift: 28\n            ashift: 9\n            asize: 34355019776\n            is_log: 0\n            create_txg: 4\n    features_for_read:\n        com.delphix:hole_birth\n        com.delphix:embedded_data\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/150922987/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/150939141", "body": "@kernelOfTruth I did not specify an ashift when I created the zpools.\n\nThe SSDs in question are Samsung 850 pros. Which as far as I can tell have a sector size of 512. \nI could change it, although I can't find anything that would say that these drives have 4k or 8k sectors.\n\nI will bump up the ashift to 13 and report the results, but it will take some time to migrate all the data around.\n\n```\n# lsblk -o NAME,PHY-SeC,LOG-SEC,DISC-ALN,SIZE,TYPE,MODEL -S\nNAME PHY-SEC LOG-SEC DISC-ALN   SIZE TYPE MODEL\nsda      512     512        0 953.9G disk Samsung SSD 850 \nsdb      512     512        0 953.9G disk Samsung SSD 850 \nsdc      512     512        0 953.9G disk Samsung SSD 850 \nsdd      512     512        0 953.9G disk Samsung SSD 850 \n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/150939141/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "xnox": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/197567269", "body": "Requires-spl: refs/pull/537/head\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/197567269/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/197568980", "body": "let's try again.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/197568980/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/197590107", "body": "My style is wrong somehow? i thought i did do copy & pastes....\n\n```\nmake checkstyle\n in dir /var/lib/buildbot/slaves/zfs/Amazon_2015_09_x86_64__BUILD_/build/zfs (timeout 1200 secs)\n watching logfiles {}\n argv: ['make', 'checkstyle']\n using PTY: False\n./lib/libspl/include/sys/isa_defs.h: 161: extra space between function name and left paren\n./lib/libspl/include/sys/isa_defs.h: 164: #define followed by space instead of tab\n./lib/libspl/include/sys/isa_defs.h: 168: #define followed by space instead of tab\n./lib/libspl/include/sys/isa_defs.h: 172: #define followed by space instead of tab\nmake: *** [cstyle] Error 1\nprogram finished with exit code 2\nelapsedTime=5.551528\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/197590107/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/197595363", "body": "@ryao \n\n```\nscripts/cstyle.pl ./lib/libspl/include/sys/isa_defs.h\nUnescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/\\S{ <-- HERE / at scripts/cstyle.pl line 608.\nUnescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/{{ <-- HERE / at scripts/cstyle.pl line 608.\n```\n\ni think it's good, albeit \"old\" perl.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/197595363/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/197599466", "body": "@ryao even if nothing works, this is not a regression, but a compile fix. Cause clearly nobody even tried to build it yet. I was mostly interested in just spl to be honest. If this gets zfs module to build, it is a pleasant side-effect. Maybe somebody else will try to test it, et.al.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/197599466/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/197602363", "body": "I am having a suspecion that isa_defs.h header should also have (a) check that either/or `_SUNOS_VTOC_[8|16]` are defined (b) define `_SUNOS_VTOC_[8|16]` for s390/s390x. What should it be? _8 for s390, and _16 for s390x?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/197602363/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/197609461", "body": "i think _SUNOS_VTOC_16 is correct here for this repo, but shouldn't be defined in the spl repo.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/197609461/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/197795232", "body": "can someone explain the failures above to me? I somehow doubt I have caused them....\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/197795232/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/197939929", "body": "@behlendorf only one define is needed `__s390__` is defined on both s390 and s390x. the _16 vtoc is added too, so i should be all good then. Please merge.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/197939929/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/333488753", "body": "Horum, I see that s390x implementation for setjmp.S is missing =( I don't think I can quite write that from scratch / clean-room. Should I look into cherrypicking *BSD implementation of it?", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/333488753/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/336669351", "body": "@behlendorf could somebody add a blind carbon-copy implementation for s390x from *BSD? I would then compile, test, and fix it up. I can fix build/test/fix things up on s390x, but I'm not sure I know where to look for *BSD sources and integrate an s390x implementation.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/336669351/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/337872016", "body": "Building on s390x I have a few issues. Not sure if they come from this pull request, or fixed in master etc.\r\n\r\n```\r\n  CC       vdev_raidz_math.lo\r\n../../module/zfs/vdev_raidz_math.c: In function 'vdev_raidz_math_get_ops':\r\n../../module/zfs/vdev_raidz_math.c:135:24: error: array subscript is above array bounds [-Werror=array-bounds]\r\n   ops = raidz_supp_impl[impl];\r\n         ~~~~~~~~~~~~~~~^~~~~~\r\ncc1: all warnings being treated as errors\r\n```\r\n\r\n```\r\nMaking all in hkdf\r\n  CCLD     hkdf_test\r\nhkdf_test.o: In function `run_test':\r\n/home/ubuntu/zfs/tests/zfs-tests/tests/functional/hkdf/hkdf_test.c:194: undefined reference to `hkdf_sha512'\r\ncollect2: error: ld returned 1 exit status\r\nMakefile:639: recipe for target 'hkdf_test' failed\r\nmake[6]: *** [hkdf_test] Error 1\r\n```\r\n\r\n```\r\nMaking all in ztest\r\n  CCLD     ztest\r\nztest.o: In function `ztest_znode_init':\r\n/home/ubuntu/zfs/cmd/ztest/ztest.c:1242: undefined reference to `refcount_create'\r\nztest.o: In function `ztest_znode_get':\r\n/home/ubuntu/zfs/cmd/ztest/ztest.c:1291: undefined reference to `zfs_refcount_add'\r\n/home/ubuntu/zfs/cmd/ztest/ztest.c:1285: undefined reference to `zfs_refcount_add'\r\nztest.o: In function `ztest_znode_put':\r\n/home/ubuntu/zfs/cmd/ztest/ztest.c:1305: undefined reference to `refcount_remove'\r\n/home/ubuntu/zfs/cmd/ztest/ztest.c:1306: undefined reference to `refcount_is_zero'\r\nztest.o: In function `ztest_znode_fini':\r\n/home/ubuntu/zfs/cmd/ztest/ztest.c:1252: undefined reference to `refcount_is_zero'\r\n/home/ubuntu/zfs/cmd/ztest/ztest.c:1255: undefined reference to `refcount_destroy'\r\nztest.o: In function `ztest_replay_write':\r\n/home/ubuntu/zfs/cmd/ztest/ztest.c:1961: undefined reference to `dmu_assign_arcbuf_by_dbuf'\r\nztest.o: In function `ztest_dmu_read_write_zcopy':\r\n/home/ubuntu/zfs/cmd/ztest/ztest.c:4503: undefined reference to `dmu_assign_arcbuf_by_dbuf'\r\n/home/ubuntu/zfs/cmd/ztest/ztest.c:4503: undefined reference to `dmu_assign_arcbuf_by_dbuf'\r\n/home/ubuntu/zfs/cmd/ztest/ztest.c:4506: undefined reference to `dmu_assign_arcbuf_by_dbuf'\r\n/home/ubuntu/zfs/cmd/ztest/ztest.c:4508: undefined reference to `dmu_assign_arcbuf_by_dbuf'\r\nztest.o:/home/ubuntu/zfs/cmd/ztest/ztest.c:4503: more undefined references to `dmu_assign_arcbuf_by_dbuf' follow\r\ncollect2: error: ld returned 1 exit status\r\nMakefile:615: recipe for target 'ztest' failed\r\nmake[3]: *** [ztest] Error 1\r\n```\r\n\r\n```\r\nMaking all in module\r\n/home/ubuntu/zfs/module/zfs/vdev_raidz_math.c: In function 'vdev_raidz_math_get_ops':\r\n/home/ubuntu/zfs/module/zfs/vdev_raidz_math.c:135:24: error: array subscript is above array bounds [-Werror=array-bounds]\r\n   ops = raidz_supp_impl[impl];\r\n         ~~~~~~~~~~~~~~~^~~~~~\r\ncc1: all warnings being treated as errors\r\nscripts/Makefile.build:302: recipe for target '/home/ubuntu/zfs/module/zfs/vdev_raidz_math.o' failed\r\n```", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/337872016/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "GICodeWarrior": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/189931406", "body": "Can you clarify what the problem is?  You say your pool isn't mounted at boot, but then you show a dataset mount operation.  Are you having trouble importing your pool or mounting a dataset?  What error are you seeing?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/189931406/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/189930662", "body": "What versions of ZFS, SPL, and Grsec?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/189930662/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "durin42": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/7294325", "body": "I'm also seeing this on the stock Ubuntu kernel. CONFIG_PREEMPT_VOLUNTARY is set to y, and I do have a pool over 90% full. I'll try to move some data around and see if the problem stops happening if I can unload the pool a bit.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/7294325/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "nprncbl": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/327799303", "body": "Same problem here: I ran a keycloak docker container, stopped it, wanted to restart, and now docker tells me:\r\n\r\n`# docker start keycloak`\r\n`Error response from daemon: error creating zfs mount of z/docker/6f5c6faba0a81ec262381bdc65a978cda6426170ad384fe8721babe026ea2b49 to /var/lib/docker/zfs/graph/6f5c6faba0a81ec262381bdc65a978cda6426170ad384fe8721babe026ea2b49: device or resource busy                             \r\nError: failed to start containers: keycloak`\r\n\r\nAny idea how to solve this?", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/327799303/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "charles-dyfis-net": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/175721006", "body": "End-user tools using this (cgroup) data include `systemd-cgtop`. If a better alternative is available for cgroup-level IO accounting, I would be happy to submit an appropriate patch to systemd.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/175721006/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/176326187", "body": "@ryao, the numbers on the old kernel were realistic enough that we had them plumbed into our monitoring system and they looked reasonable enough to work with. (Lots of writes accounted to the database, smaller numbers to other processes, etc).\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/176326187/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "RFDAJE": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/264504395", "body": "I am having the same issue with 0.6.5.8 on kernel-3.4.x, too. \r\nbut 0.6.5.7 works fine.\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/264504395/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/269614173", "body": "http://pastebin.com/JVHQzTXB\r\nkernel config 3.4.106 with spl-0.6.5.7 works.but the same kernel with 0.6.5.8 does not work, the host has no zfs configured, just upon loading the zfs module, I will encounter the issue.\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/269614173/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/269751877", "body": "I can confirm the issue disappeared if I use the latest git version.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/269751877/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "linuxfood": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31794911", "body": "I've also got a pool with `xattr=on`, and I managed to capture the output of `zdb -ddddd Cargobay/TestFS 9` on a directory for which a chown is currently hung. System is a fresh Arch install with kernel 3.12.6-1-ARCH and the corresponding builds from http://demizerone.com. (version 0.6.2_3.12.6-1 built on 2013-12-26)\n\nEDIT: The pool came from ZEVO, but I have not yet attempted an upgrade.\nEDIT2: The TestFS filesystem does fail to mount at this point.\n\nIt's worth noting that often times commands will return 'File not found' even for things that are plainly there. I don't think I've seen an mount issues attributable to this. Here's the paste:\n\n```\nDataset Cargobay/TestFS [ZPL], ID 5120, cr_txg 2041990, 356K, 56 objects, rootbp DVA[0]=<0:16001bb000:1000> DVA[1]=<0:74000e5000:1000> [L0 DMU objset] fletcher4 lzjb LE contiguous unique double size=800L/200P birth=7887399L/7887399P fill=56 cksum=17531407fd:7b70c9644da:15e8c67a8bf87:2bef7d78cd97c3\n\nObject  lvl   iblk   dblk  dsize  lsize   %full  type\n     9    1    16K    512     8K    512  100.00  ZFS directory\n                                    136   bonus  System attributes\ndnode flags: USED_BYTES USERUSED_ACCOUNTED \ndnode maxblkid: 0\npath    /.Trashes\nuid     501\ngid     20\natime   Mon Dec 31 21:10:20 2012\nmtime   Sun Jan  5 17:46:18 2014\nctime   Sun Jan  5 17:46:18 2014\ncrtime  Mon Dec 31 21:10:20 2012\ngen 2041991\nmode    41777\nsize    3\nparent  4\nlinks   3\npflags  0\nmicrozap: 512 bytes, 1 entries\n\n    501 = 52 (type: Directory)\nIndirect blocks:\n           0 L0 0:3e890f4000:1000 200L/200P F=1 B=7872338/7872338\n\n    segment [0000000000000000, 0000000000000200) size   512\n```\n\nHere's the output of `/proc/spl/kstat/zfs/dmu_tx`\n\n```\n3 1 0x01 12 576 4545263761 4742202820528\nname                            type data\ndmu_tx_assigned                 4    71577\ndmu_tx_delay                    4    0\ndmu_tx_error                    4    0\ndmu_tx_suspended                4    0\ndmu_tx_group                    4    2\ndmu_tx_how                      4    0\ndmu_tx_memory_reserve           4    0\ndmu_tx_memory_reclaim           4    0\ndmu_tx_memory_inflight          4    0\ndmu_tx_dirty_throttle           4    0\ndmu_tx_write_limit              4    0\ndmu_tx_quota                    4    0\n```\n\nThis pool came from ZEVO (which I'm beginning to gather has some \"interesting\" quirks), but it's been far too long to recall how it was created.\n\nInterestingly the `TestFS` filesystem is actually empty (I forget why I created it ages ago), but I have another filesystem that also exhibits this problem but I was able to rsync off the contents at some point to a new pool created under ZoL. I might have enough disk available to shuffle all my pools around, but I'm not confident in that.\n\nI'm happy to leave this system in this state if you want me to collect anything else. \nInterestingly, the stacks in dmesg make it look like txg_quiesce is the culprit here.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31794911/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31807004", "body": "@dweeezil it was _probably_ built with GCC 4.8.X (likely .2) because that's what I've got on my Arch machine, but I'll drop a line to the maintainer and see. No upgrades were done of the archzfs package, this was a fresh setup, though the pool was created from the last release ZEVO.\n\nThe exact command was `chown -R <username>:<groupname> .Trash`\nI was able to run `chown <username>:<groupname> .` for the parent, though I doubt it's something about the recursive changes.\n\nI'm happy to hang onto the pool, and can probably make it available for manual inspection as well.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31807004/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31873390", "body": "@dweeezil I'll see what I can do about getting to this tonight.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/31873390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32008305", "body": "Sorry for the delay, I didn't manage to have access to a machine with Zevo on it last night, but I got it tonight, here's a link to the image: http://bcs.testing.linuxfood.net/static/tank.img.gz\n\nI've also confirmed that it behaves the same as my pool with actual data in it, so reproducing the issue should be trivial.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32008305/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32071267", "body": "@dweeezil very interesting. Is there any reason to apply this change if I can just do a \"noop chmod\"? If the work around is sufficient, then I'm happy to do that work around and then wait for the revised code. Regardless, as soon as I can validate that my data is safe, I'll be doing the great data shuffle and recreating the pools anyways so I can take advantage of stuff like lz4.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/32071267/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "cburroughs": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/9227953", "body": "As was brought up in the thread we currently using http://www.xmailserver.org/flcow.html  on ext4 for file/dir level COW.  This works, but we would much prefer if we were using ZFS to have the filesystem take care of COW goodness.  (For our narrow use case we can probably do everything we need to do with a filesystem per directory, but having our code just work with ``cp` would be nice to have.) \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/9227953/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/53441664", "body": "I looked but could not find one, is there an upstream illumos ticket/discussion on limiting L2ARC header size somewhere?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/53441664/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/67880371", "body": "I was trying to use the zed script on rhel6 and ran into two compatibilty:\n- `/lib/init/vars.sh` does not exist\n- `start-stop-daemon` does not exist and seems from my limited understanding to be a debian specific thing.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/67880371/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/63179733", "body": "@p4tino Thanks for picking this up!  Please let me know if there is anything I should do to help.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/63179733/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/64664564", "body": "FWIW I think the current accounting of the memory cgroup is hard to reason about ('whomever touched the page first?') and inflexible (you can't just limit RSS).   This is one situation where the arc/page-cache split is a significnat advantage.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/64664564/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/94120660", "body": "@sl6xx with your VM: Could you repro only with a 0.6.3->0.6.4 upgrade, or are legacy mountpoints broken even when created with 0.6.4?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/94120660/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/96008006", "body": "From a user perspetive it would be nice if the relase that removed `zfs_autoimport_disable` was the one _after_ the release that obviated the need for it.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/96008006/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/8895932", "body": "Does the kernel upstream ocnsider the GPL export only flags on the neccisary functions an oversite or intentional policy?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/8895932/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/8900674", "body": "I don't have any special way to ask besides the LKML and I'm not even sure how to intelligenlty phrase the question for the LKML.  I'm just a user who wants to trace all the things.\n\nPerhaps @ryao would know since I think he has interacted with the kernel devs before.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/8900674/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "novel": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/49907381", "body": "Good, thanks!\n\nI think usage info (HELP_GET) should be updated as well to include this new flag:\n\nhttps://github.com/FransUrbo/zfs/blob/4f200f286ca3b4d1d7adedd227dcd7a56ca2836e/cmd/zpool/zpool_main.c#L268\n\nBy the way, while here, I think having '-p' support for 'zpool list' would be also useful.\n\nThanks\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/49907381/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/50175059", "body": "> There's no END to some peoples needs and wants!! :)\n\nThat is very true :)\n\n@ilovezfs thanks for the suggestion, I'll try this one as well.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/50175059/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/50178204", "body": "@ilovezfs unfortunately, 'zpool list -o all -H' is not very convenient for scripts as well because of the missing '-p' for the list command. And I guess it's even simpler to parse an arbitrary number of spaces than human-readable values.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/50178204/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/50227730", "body": "Looks like it's caused by an old spl that I have as I noticed that 0.6.3 doesn't provide 'howmany' macros, but it's there in the git version.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/50227730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/50229144", "body": "Yeah, the issue was caused by an outdated spl, sorry for the noise.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/50229144/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "AndyA": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/28627714", "body": "I think I have the same problem - a particular snapshot that I can't destroy on Debian.\n\n```\n# dpkg -l | grep '\\<spl\\>'\nii  spl                                   0.6.2-2                            amd64        Solaris Porting Layer user-space utilities for Linux\nii  spl-dkms                              0.6.2-2                            all          Solaris Porting Layer kernel modules for Linux\n# dpkg -l | grep 'zfs'\nii  debian-zfs                            7~wheezy                           amd64        Native ZFS filesystem metapackage for Debian.\nii  dkms                                  2.2.0.3-1.2+zfs6                   all          Dynamic Kernel Module Support Framework\nii  libzfs1                               0.6.2-2                            amd64        Native ZFS filesystem library for Linux\nrc  zfs                                   0.6.1-2                            amd64        Commands to control the kernel modules and libraries\nii  zfs-dkms                              0.6.2-2                            all          Native ZFS filesystem kernel modules for Linux\nii  zfsonlinux                            2~wheezy                           all          archive.zfsonlinux.org trust package\nii  zfsutils                              0.6.2-2                            amd64        command-line tools to manage ZFS filesystems\n# uname -a\nLinux igloo 3.2.0-4-amd64 #1 SMP Debian 3.2.51-1 x86_64 GNU/Linux\n```\n\nI'm going to boot into SmartOS and attempt to delete the troublesome snapshot - but before I do are there any useful forensics I can gather that might help better characterise the issue?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/28627714/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/28649208", "body": "Correction - after a reboot I can now delete the snapshot. I guess I'm not seeing the same issue. Apologies for noise.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/28649208/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/94265566", "body": "Since it's doing some I/O I'll leave it doing that for a while. What might it be doing?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/94265566/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/94265819", "body": "Yup. I reckon that it's a bug if a version upgrade leaves it in an unusable state, no?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/94265819/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/94267382", "body": "Ah - thanks. I've subscribed to the mailing list and will post there too.\n\nThere are a lot of snapshots on that pool. However the zpool upgrade appeared to complete without issue and the system was up and usable for several hours after it.\n\nIt was only on a second reboot (~24 hours later) that import hung.\n\nAlso, as mentioned above, it now reports:\n\n```\nApr 19 11:45:32 igloo kernel: [  339.033628] WARNING: Pool 'data' has encountered an uncorrectable I/O failure and has been suspended.\n```\n\nwhich sounds more serious than upgrade taking a while to complete.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/94267382/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/94267731", "body": "Can I do a scrub while it's still trying to mount? It seems understandably reluctant:\n\n```\n# zpool scrub data\ncannot scrub data: pool I/O is currently suspended\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/94267731/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/95843455", "body": "I was just about to report back :)\n\nThe machine in question is clearly unwell; haven't done any further diagnosis but I put the drives into a different machine which has, after two scrubs, removed the unlinked damaged objects and is now reporting good health.\n\nThanks for following up. It's clearly not a bug; apologies for the noise and thanks for all the great work.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/95843455/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "sparksh": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/190246433", "body": "After upgrading from Fedora 22 to Fedora 23, zfs modules were not rebuilt. In fact, I can't recall that dkms updates or upgrades ever worked for me. I've always had to re-install the packages and/or delete files in /var/lib/dkms.\n\nI opened the associated rpms to look at the spec files. There is a section to clean up the /var/lib/dkms directories:\n\n```\n%preun\necho -e \"Uninstall of %{module} module (version %{version}) beginning:\"\ndkms remove -m %{module} -v %{version} --all --rpm_safe_upgrade\nexit 0\n```\n\nIf this actually happens, I don't see how all the old stuff we see remains in /var/lib/dkms.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/190246433/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/190310516", "body": "(Continuing from previous post)\n\nLater the same day a bunch of updates came in for F23 that included a new kernel.\nThis time, I tried to pay attention.\n\nCurrently:\n\n```\nkernel-4.3.5-300.fc23.x86_64\nzfs-dkms-0.6.5.4-1.fc23.noarch\n```\n\nPending update:\n\n```\nkernel-4.3.5-300.fc23.x86_64\n```\n\nDirectory /var/lib/dkms/zfs before update:\n\n```\n0.6.5.4/\nkernel-4.3.5-300.fc23.x86_64-x86_64 -> ...\n```\n\nNow \"dnf update\"...\n\nDirectory /var/lib/dkms/zfs before reboot:\n\n```\n0.6.5.4\nkernel-4.3.5-300.fc23.x86_64-x86_64 -> ...\nkernel-4.4.2-301.fc23.x86_64-x86_64 -> ...\n```\n\nDirectory /usr/src before reboot:\n\n```\nkmods/\ndebug/\nkernels/\nspl-0.6.5.4/\nzfs-0.6.5.4/\n```\n\nThe spl and zfs directories both contain dkms.conf files.\n\nIn previous situations, I found that dkms.conf was missing from older directories\nin this location. (There was a theory prevailing then that old versions remaining\nin /var/lib/dkms went after these sources and derailed when dkms.conf was missing.)\n\nNow reboot...\n\n_Everything works fine._\nI checked the dkms logs and the actual module dates to make sure it really happend.\n\nI only had one version of zfs in /var/lib/dkms because of past manual cleanups.\nThe next time zfs gets updated, I'll post the results. And then we have the case where\nboth the kernel and zfs get updated. That definitely didn't work for me a few days ago when\nI went from F22 to F23.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/190310516/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/249665461", "body": "Same symptoms on Fedora 24 with zfs-dkms. This method worked for me:\n\n1) Set the bootfs property in the pool:\n\n```\nzpool set bootfs=zfs/ROOT/fedora\n```\n\n2) Eliminate the \"root=ZFS=zfs/ROOT/fedora ro\" clause entirely in /etc/grub2.cfg\n\nHopefully these workarounds will be temporary...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/249665461/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "rryan": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/824187", "body": "Adding:\n\n```\n#include <linux/seq_file.h>\n```\n\nAt the top of zpl_super.c fixes the issue for me.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/824187/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/828323", "body": "btw, tank is a raid-z pool with 2x 2TB SATA harddrives.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/828323/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/844866", "body": "Thanks -- I'm pretty sure I'm not using a pre-emptible kernel, it's a stock Ubuntu Desktop 9.10 install. Also, no errors show in the syslog when this happens. \n\nI'll give the latest head versions a try, thanks!\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/844866/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/906370", "body": "Using 0.6.0 RC2 seems like a big improvement. I'm not seeing any of the strange hanging I saw with RC1. Closing for now. I'll re-open if I see it again. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/906370/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ari": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/84196187", "body": "@ryao are you still working on an update to the Gentoo packaging to avoid this (pretty serious) bug\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/84196187/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/96522057", "body": "@ryao Likewise, I was about to move from Gentoo to ZFS on a better supported Linux, but the 9999 has worked well. I think it does ZFS a disservice on Gentoo to have a fundamental part of it broken for such a long time. Perhaps all users should be advised that only 9999 is usable at this time for recent kernels?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/96522057/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/96525779", "body": "Well, actually I'm a veteran (15 year and more) FreeBSD user. I have exactly one Linux box in all my server farms and I picked gentoo since it looked a little bit like FreeBSD :-)   All this stuff \"just works\" on FreeBSD, partially because the ZFS implementation there is much older and more tested. It is also part of the core OS distribution so gets more eyes on it.\n\nI'm still struggling to get Gentoo booting from ZFS, but I hope to get there eventually do that I can create and destroy ZFS pools with version 9999.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/96525779/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/96851088", "body": "Thanks everyone for your help. I'd just like to confirm that version 9999 (which is hard masked) on Gentoo tracking the master branch, works perfectly for my use case. In particular I can now destroy pools.\n\nI advise all gentoo users finding this thread to try that ebuild. It would be nice if Gentoo tracked the stable tags rather than master, but this will do for now.\n\nThanks\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/96851088/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/84196432", "body": "My review of that other ticket seems to indicate that there is no update to Gentoo packages since last December. Thanks for the pointer though, I'll close this ticket.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/84196432/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/101656792", "body": "I get a similar issue on gentoo 3.17.7 kernel with zfs from git as of today.\n\n[   14.234119] Large kmem_alloc(953672, 0x1000), please file an issue at:\nhttps://github.com/zfsonlinux/zfs/issues/new\n[   14.234122] CPU: 3 PID: 1585 Comm: zpool Tainted: P           O   3.17.7-gentoo #1\n[   14.234130] Hardware name:                  /DH67BL, BIOS BLH6710H.86A.0128.2011.0823.1224 08/23/2011\n[   14.234131]  0000000000000000 ffff88011aa1bbd8 ffffffffbf3a4d73 0000000000000086\n[   14.234133]  00000000000e8d48 ffff88011aa1bc18 ffffffffc040d747 000000000000035b\n[   14.234135]  0000000000000000 ffff88011f283000 0000000000000001 000000000000000a\n[   14.234136] Call Trace:\n[   14.234141]  [<ffffffffbf3a4d73>] dump_stack+0x46/0x58\n[   14.234146]  [<ffffffffc040d747>] spl_kmem_zalloc+0xa3/0x176 [spl]\n[   14.234156]  [<ffffffffc0d25b0d>] vdev_metaslab_init+0x88/0x1b0 [zfs]\n[   14.234164]  [<ffffffffc0d27bb0>] ? vdev_load+0x81/0x9e [zfs]\n[   14.234170]  [<ffffffffc0d27b9a>] vdev_load+0x6b/0x9e [zfs]\n[   14.234174]  [<ffffffffc0d27b51>] vdev_load+0x22/0x9e [zfs]\n[   14.234184]  [<ffffffffc0d17d8e>] spa_vdev_remove+0x12b2/0x2185 [zfs]\n[   14.234192]  [<ffffffffc0d197b2>] spa_tryimport+0x96/0x3bd [zfs]\n[   14.234200]  [<ffffffffc0d40ff4>] zfs_secpolicy_smb_acl+0x1850/0x4202 [zfs]\n[   14.234206]  [<ffffffffc0d457a5>] pool_status_check+0x395/0x470 [zfs]\n[   14.234209]  [<ffffffffbf0e5afd>] ? mntput+0x28/0x2a\n[   14.234210]  [<ffffffffbf0cef35>] ? __fput+0x183/0x1a3\n[   14.234212]  [<ffffffffbf0dc2f5>] do_vfs_ioctl+0x402/0x44c\n[   14.234213]  [<ffffffffbf0cef83>] ? ____fput+0x9/0xb\n[   14.234215]  [<ffffffffbf049686>] ? task_work_run+0x7e/0x90\n[   14.234217]  [<ffffffffbf0dc378>] SyS_ioctl+0x39/0x61\n[   14.234219]  [<ffffffffbf3a9c52>] system_call_fastpath+0x16/0x1b\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/101656792/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/102012628", "body": "@behlendorf I've got pretty major problems with this pool. I created it originally with two real disks (3Tb) and two sparse files because I wanted to migrate the data off another two disks. Once the data was copied, I replaced the sparse files with the real disks. The first one went fine, but the second one hung the whole system once it reached 100% completion. After an hour of waiting, I forced a reboot. It hasn't been the same since.\n\nI guess I could do the whole thing again and make another pool, but it takes almost a day to copy all the data.\n\n```\n# zpool status\n  pool: store\n state: DEGRADED\n  scan: resilvered 4.27M in 0h0m with 0 errors on Wed May 13 23:02:14 2015\nconfig:\n\n    NAME              STATE     READ WRITE CKSUM\n    store             DEGRADED     0     0     0\n      raidz2-0        DEGRADED     0     0     0\n        sdb3          ONLINE       0     0     0\n        sde3          ONLINE       0     0     0\n        sda3          ONLINE       0     0     0\n        replacing-3   DEGRADED     0     0     0\n          /mnt/disk2  OFFLINE      0     0     0\n          sdc3        ONLINE       0     0     0\n```\n\n```\n# zdb -m store\n\nMetaslabs:\n    vdev          0\n    metaslabs 119209   offset                spacemap          free\n    metaslab      0   offset            0   spacemap     37   free     100K\n    metaslab      1   offset      4000000   spacemap     54   free    1.41M\n    metaslab      2   offset      8000000   spacemap     52   free    1.08M\n    metaslab      3   offset      c000000   spacemap     61   free    2.09M\n    metaslab      4   offset     10000000   spacemap     60   free    2.07M\n    metaslab      5   offset     14000000   spacemap     59   free    1.27M\n```\n\nfollowed by about 119,000 lines of more output.\n\nAny thoughts about how to recover from this? Trying to force the replacement of the last disk causes it to reliably crash. Scrub also crashes it.\n\nI'm guessing that ZFS on linux doesn't perform happily when under extreme memory pressures like in this case? I've only got 4Gb of RAM and for a while there I didn't have any swap configured. I'm going to try it again with 16Gb of swap.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/102012628/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/102014036", "body": "I've just discovered the pool is also 100% full. That's not good and probably contributing to the problems because zfs hasn't got anywhere to relocate blocks. So mine might be a pretty extreme case.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/102014036/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "drescherjm": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/277313064", "body": "> I urgently need it, my SSD is failing. \r\n\r\nI don't believe TRIM will help fix a failing SSD. ", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/277313064/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/132002071", "body": "That kernel panic is a duplicate. I saw that months ago and reported it.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/132002071/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/132004097", "body": "#3257\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/132004097/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/265773901", "body": "I thought this was fixed long ago.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/265773901/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/268538363", "body": "At work I ran a set of 10 SATA drives that had an unusually high URE rate like that for a few years in raidz3 but never had any bit rot.  Although I did weekly scrubs which each week there were repairs made.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/268538363/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/306980248", "body": "Yes", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/306980248/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/312438495", "body": "This is why you scrub.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/312438495/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/312438993", "body": "They do if the data read does not match.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/312438993/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/312439283", "body": "The issue is if you don't perform scrubs then on a read error the a sector will not be fixed. If you do perform scubs I believe it is less of an issue depending on your frequency of scrubs.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/312439283/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/312467272", "body": "I am pretty sure that was not talking about scrubs or resilvers. ", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/312467272/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/315376349", "body": "I believe in my testing with drives with high URE rates in raidz3 that scrubs did fix this problem. I did get checksum errors for the drives and the sectors appeared to be reallocated in the SMART data. The system is powered off since I retired it a few months ago. However next week I could possibly power it on an do some testing.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/315376349/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/315378474", "body": "BTW,  I had the system running like this for a few years with actual data on it (mostly downloads and isos for our software several hundred GB of data) and never lost any data. When I retired it I did a send and receive to a new zfs server and there was no issue with that.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/315378474/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/35644330", "body": "On Thu, Feb 20, 2014 at 11:23 AM, Turbo Fredriksson <\nnotifications@github.com> wrote:\n\n> Are you running with dedup? if so, this is expected behavior...\n\nI see the same type of thing when compression is enabled.\n\nJohn\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/35644330/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/35648635", "body": "> This is expected and correct behavior. The due utility shows you the\n> number of blocks required to save the file on disk. This may be smaller\n> then the size reported by stat I'd the file was able to be compressed or\n> dedupped.\n> \n> Agreed.\n\nI use du --apparent-size to get the uncompressed size\n\ndatastore4 jdrescher # du -sh --apparent-size ILA_SCP_0\n52G     ILA_SCP_0\ndatastore4 jdrescher # du -sh ILA_SCP_0\n1.6G    ILA_SCP_0\n\nYes there is a very high compression rate with these files. The data has a\nlot of image mask files that represent the result of segmentation of lung\nct. This data is highly compressible.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/35648635/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/142733809", "body": "@arlininger I believe this is one way to tell:\n\njmd0 ~ # smartctl --all /dev/sdc\nsmartctl 6.4 2015-06-04 r4109 [x86_64-linux-4.1.8-gentoo-jmd0.comcast.net-20150922](local build)\nCopyright (C) 2002-15, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF INFORMATION SECTION ===\nModel Family:     Hitachi Deskstar 5K4000\nDevice Model:     Hitachi HDS5C4040ALE630\nSerial Number:    PL1321LAGA2W7H\nLU WWN Device Id: 5 000cca 22ec49684\nFirmware Version: MPAOA3B0\nUser Capacity:    4,000,787,030,016 bytes [4.00 TB]\nSector Sizes:     512 bytes logical, 4096 bytes physical\nRotation Rate:    5700 rpm\nForm Factor:      3.5 inches\nDevice is:        In smartctl database [for details use: -P show]\nATA Version is:   ATA8-ACS T13/1699-D revision 4\nSATA Version is:  SATA 3.0, 6.0 Gb/s (current: 3.0 Gb/s)\nLocal Time is:    Wed Sep 23 17:24:13 2015 EDT\nSMART support is: Available - device has SMART capability.\nSMART support is: Enabled\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/142733809/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/41103190", "body": "I had that a few weeks back on Gentoo but it isn't happening now.\n\njmd0 ~ # cat /sys/module/{spl,zfs}/version\n0.6.2-33_g89aa970\n0.6.2-254_g0b75bdb\njmd0 ~ # zpool status\n  pool: zfs_2t0\n state: ONLINE\n  scan: scrub repaired 0 in 1h16m with 0 errors on Wed Mar 12 00:00:21 2014\nconfig:\n\n```\n    NAME         STATE     READ WRITE CKSUM\n    zfs_2t0      ONLINE       0     0     0\n      2t0-part2  ONLINE       0     0     0\n```\n\nerrors: No known data errors\n\n  pool: zfs_4t0\n state: ONLINE\n  scan: scrub repaired 0 in 5h42m with 0 errors on Sat Apr  5 05:25:34 2014\nconfig:\n\n```\n    NAME        STATE     READ WRITE CKSUM\n    zfs_4t0     ONLINE       0     0     0\n      4t0       ONLINE       0     0     0\n```\n\nerrors: No known data errors\n\n  pool: zfs_4t1\n state: ONLINE\n  scan: scrub repaired 0 in 5h19m with 0 errors on Fri Mar 28 15:59:04 2014\nconfig:\n\n```\n    NAME        STATE     READ WRITE CKSUM\n    zfs_4t1     ONLINE       0     0     0\n      sde1      ONLINE       0     0     0\n```\n\nerrors: No known data errors\n\n  pool: zfs_4t2\n state: ONLINE\n  scan: scrub repaired 0 in 7h22m with 0 errors on Sun Mar 30 20:43:22 2014\nconfig:\n\n```\n    NAME        STATE     READ WRITE CKSUM\n    zfs_4t2     ONLINE       0     0     0\n      sdd       ONLINE       0     0     0\n```\n\nerrors: No known data errors\n\njmd0 ~ # uname -a\nLinux jmd0.comcast.net 3.12.17-jmd0-zfs-9999-2 #3 SMP PREEMPT Tue Apr 15\n00:21:09 EDT 2014 x86_64 Intel(R) Core(TM)2 Quad CPU Q9550 @ 2.83GHz\nGenuineIntel GNU/Linux\n\nI am currently using vanilla-sources-3.12.17.\n\nJohn\n\nOn Tue, Apr 22, 2014 at 6:12 PM, AndCycle notifications@github.com wrote:\n\n> thanks,\n> I checked those threads, my spl ver. is 0.6.2-33_g89aa970,\n> so the patch about Simplify hostid logic doesn't works for me,\n> I am using Gentoo, current default is still OpenRC apparently it's not\n> systemd's problem,\n> maybe it's something else, any advise?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/zfsonlinux/zfs/issues/2273#issuecomment-41102546\n> .\n\n## \n\nJohn M. Drescher\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/41103190/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45889598", "body": "This is clearly not an issue and @FransUrbo  is a developer. \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45889598/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/46027926", "body": "> > Do not put swap on a zvol\n> \n> I thought that was fixed 'long' ago.. ?\n\nI have had swap on a zvol on 6 or so systems since February. No problems at\nall.\n\nJohn\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/46027926/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/346843982", "body": "zpool status and iostat implemented this months ago. With the -c parameter.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/346843982/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/92160651", "body": "I had that a couple weeks ago when browsing snapshots via samba.\n\ndatastore4 ~ # cat /sys/module/{spl,zfs}/version\n0.6.3-76_g6ab0866\n0.6.3-240_g40749aa\ndatastore4 ~ # uname -a\nLinux datastore4 3.19.2-gentoo-datastore4-zfs-20150319 #1 SMP Thu Mar\n19 17:12:53 EDT 2015 x86_64 Intel(R) Xeon(R) CPU E31230 @ 3.20GHz\nGenuineIntel GNU/Linux\n\nHere is the kernel panic:\n[18333.744680] Kernel panic - not syncing: avl_find() succeeded inside avl_add()\n[18333.744715] CPU: 1 PID: 3226 Comm: z_unmount/0 Tainted: P        W\nO   3.19.2-gentoo-datastore4-zfs-20150319 #1\n[18333.744740] Hardware name: To be filled by O.E.M. To be filled by\nO.E.M./P8B-X series, BIOS 2107 05/04/2012\n[18333.744765]  ffff8801f995f9c0 ffff8802232c3cb8 ffffffff8166a131\n0000000000000001\n[18333.744801]  ffffffffa02ceba8 ffff8802232c3d38 ffffffff81666104\n0000000000000002\n[18333.744950]  0000000000000008 ffff8802232c3d48 ffff8802232c3ce8\nffff8802232c3d38\n[18333.745082] Call Trace:\n[18333.745150]  [<ffffffff8166a131>] dump_stack+0x45/0x57\n[18333.745228]  [<ffffffff81666104>] panic+0xb6/0x1da\n[18333.745300]  [<ffffffffa02ce705>] avl_add+0x45/0x50 [zavl]\n[18333.745371]  [<ffffffff8166d611>] ? mutex_lock+0x11/0x32\n[18333.745465]  [<ffffffffa03ad3f4>] zfsctl_unmount_snapshot+0x184/0x1d0 [zfs]\n[18333.745558]  [<ffffffffa03ad608>] zfsctl_snapdir_remove+0x1c8/0x220 [zfs]\n[18333.745635]  [<ffffffffa0298088>] taskq_cancel_id+0x2c8/0x470 [spl]\n[18333.746729]  [<ffffffff81068ed0>] ? wake_up_process+0x40/0x40\n[18333.746799]  [<ffffffffa0297ee0>] ? taskq_cancel_id+0x120/0x470 [spl]\n[18333.746868]  [<ffffffff8105fda4>] kthread+0xc4/0xe0\n[18333.746935]  [<ffffffff8105fce0>] ? kthread_create_on_node+0x170/0x170\n[18333.747004]  [<ffffffff8166f508>] ret_from_fork+0x58/0x90\n[18333.747091]  [<ffffffff8105fce0>] ? kthread_create_on_node+0x170/0x170\n[18333.747194] Kernel Offset: 0x0 from 0xffffffff81000000 (relocation\nrange: 0xffffffff80000000-0xffffffff9fffffff)\n[18333.747595] ---[ end Kernel panic - not syncing: avl_find()\nsucceeded inside avl_add()\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/92160651/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/103597571", "body": "I am seeing this too. I have not upgraded my pools.\n\n[10642.880532] Large kmem_alloc(65536, 0x1000), please file an issue at:\nhttps://github.com/zfsonlinux/zfs/issues/new\n[10642.880537] CPU: 0 PID: 2766 Comm: zvol/12 Tainted: P        W  O    4.0.4-gentoo-jmd0.comcast.net-20150518 #1\n[10642.880539] Hardware name: System manufacturer System Product Name/P5Q-PRO, BIOS 1306    08/20/2008\n[10642.880540]  0000000000010000 ffff8800c30bbbf8 ffffffff815d6909 000000000000001a\n[10642.880544]  0000000000000000 ffff8800c30bbc38 ffffffffa041db7b ffff8800c30bbcd0\n[10642.880547]  ffff880100ed1848 00000050e8000000 0000000000000000 0000000004000000\n[10642.880550] Call Trace:\n[10642.880553]  [<ffffffff815d6909>] dump_stack+0x45/0x57\n[10642.880561]  [<ffffffffa041db7b>] spl_kmem_zalloc+0x11b/0x190 [spl]\n[10642.880575]  [<ffffffffa04c4e5f>] dmu_buf_rele_array+0xaf/0x520 [zfs]\n[10642.880590]  [<ffffffffa04c53a8>] dmu_buf_hold_array_by_bonus+0xd8/0x100 [zfs]\n[10642.880605]  [<ffffffffa04c6a02>] dmu_write_req+0x62/0x1d0 [zfs]\n[10642.880627]  [<ffffffffa0566c01>] zrl_is_locked+0x1111/0x18e0 [zfs]\n[10642.880631]  [<ffffffff815d81ab>] ? __schedule+0x32b/0x890\n[10642.880638]  [<ffffffffa0421088>] taskq_cancel_id+0x2c8/0x470 [spl]\n[10642.880641]  [<ffffffff81067f40>] ? wake_up_process+0x40/0x40\n[10642.880648]  [<ffffffffa0420ee0>] ? taskq_cancel_id+0x120/0x470 [spl]\n[10642.880651]  [<ffffffff8105ebb4>] kthread+0xc4/0xe0\n[10642.880654]  [<ffffffff8105eaf0>] ? kthread_create_on_node+0x170/0x170\n[10642.880657]  [<ffffffff815dba08>] ret_from_fork+0x58/0x90\n[10642.880660]  [<ffffffff8105eaf0>] ? kthread_create_on_node+0x170/0x170\n[11208.997631] EXT4-fs (zd0): mounted filesystem with ordered data mode. Opts: (null)\n[11423.283988] Large kmem_alloc(64992, 0x1000), please file an issue at:\nhttps://github.com/zfsonlinux/zfs/issues/new\n[11423.283996] CPU: 2 PID: 2760 Comm: zvol/6 Tainted: P        W  O    4.0.4-gentoo-jmd0.comcast.net-20150518 #1\n[11423.283998] Hardware name: System manufacturer System Product Name/P5Q-PRO, BIOS 1306    08/20/2008\n[11423.284016]  000000000000fde0 ffff8800c30afbf8 ffffffff815d6909 0000000000000001\n[11423.284020]  0000000000000000 ffff8800c30afc38 ffffffffa041db7b ffff8800c30afcd0\n[11423.284023]  ffff880048f81f38 0000003ddca89000 0000000000000000 0000000003f77000\n[11423.284027] Call Trace:\n[11423.284035]  [<ffffffff815d6909>] dump_stack+0x45/0x57\n[11423.284062]  [<ffffffffa041db7b>] spl_kmem_zalloc+0x11b/0x190 [spl]\n[11423.284088]  [<ffffffffa04c4e5f>] dmu_buf_rele_array+0xaf/0x520 [zfs]\n[11423.284104]  [<ffffffffa04c53a8>] dmu_buf_hold_array_by_bonus+0xd8/0x100 [zfs]\n[11423.284120]  [<ffffffffa04c6a02>] dmu_write_req+0x62/0x1d0 [zfs]\n[11423.284142]  [<ffffffffa0566c01>] zrl_is_locked+0x1111/0x18e0 [zfs]\n[11423.284161]  [<ffffffff815d81ab>] ? __schedule+0x32b/0x890\n[11423.284175]  [<ffffffffa0421088>] taskq_cancel_id+0x2c8/0x470 [spl]\n[11423.284181]  [<ffffffff81067f40>] ? wake_up_process+0x40/0x40\n[11423.284187]  [<ffffffffa0420ee0>] ? taskq_cancel_id+0x120/0x470 [spl]\n[11423.284192]  [<ffffffff8105ebb4>] kthread+0xc4/0xe0\n[11423.284195]  [<ffffffff8105eaf0>] ? kthread_create_on_node+0x170/0x170\n[11423.284199]  [<ffffffff815dba08>] ret_from_fork+0x58/0x90\n[11423.284202]  [<ffffffff8105eaf0>] ? kthread_create_on_node+0x170/0x170\n\njmd0 ~ # zpool status\n  pool: zfs_2t0\n state: ONLINE\nstatus: Some supported features are not enabled on the pool. The pool can\n        still be used, but some features are unavailable.\naction: Enable all features using 'zpool upgrade'. Once this is done,\n        the pool may no longer be accessible by software that does not support\n        the features. See zpool-features(5) for details.\n  scan: scrub repaired 0 in 4h26m with 0 errors on Thu Jun  5 14:23:53 2014\nconfig:\n\n```\n    NAME        STATE     READ WRITE CKSUM\n    zfs_2t0     ONLINE       0     0     0\n      sdg2      ONLINE       0     0     0\n```\n\nerrors: No known data errors\n\n  pool: zfs_4t0\n state: ONLINE\nstatus: Some supported features are not enabled on the pool. The pool can\n        still be used, but some features are unavailable.\naction: Enable all features using 'zpool upgrade'. Once this is done,\n        the pool may no longer be accessible by software that does not support\n        the features. See zpool-features(5) for details.\n  scan: scrub repaired 0 in 5h50m with 0 errors on Thu Jun  5 15:48:06 2014\nconfig:\n\n```\n    NAME        STATE     READ WRITE CKSUM\n    zfs_4t0     ONLINE       0     0     0\n      sdb       ONLINE       0     0     0\n```\n\nerrors: No known data errors\n\n  pool: zfs_home\n state: ONLINE\nstatus: Some supported features are not enabled on the pool. The pool can\n        still be used, but some features are unavailable.\naction: Enable all features using 'zpool upgrade'. Once this is done,\n        the pool may no longer be accessible by software that does not support\n        the features. See zpool-features(5) for details.\n  scan: none requested\nconfig:\n\n```\n    NAME        STATE     READ WRITE CKSUM\n    zfs_home    ONLINE       0     0     0\n      sda3      ONLINE       0     0     0\n```\n\nerrors: No known data errors\n\njmd0 ~ # cat /sys/module/{spl,zfs}/version\n0.6.4-4_g62e2eb2\n0.6.4-74_g141b638\n\njmd0 ~ # zpool upgrade\nThis system supports ZFS pool feature flags.\n\nAll pools are formatted using feature flags.\n\nSome supported features are not enabled on the following pools. Once a\nfeature is enabled the pool may become incompatible with software\nthat does not support the feature. See zpool-features(5) for details.\n\n## POOL  FEATURE\n\nzfs_2t0\n      filesystem_limits\n      large_blocks\nzfs_4t0\n      filesystem_limits\n      large_blocks\nzfs_home\n      filesystem_limits\n      large_blocks\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/103597571/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/116768971", "body": "I have not had this problem on gentoo. I have a few systems using the 9999 builds and gentoo-sources-4.0.6 or gentoo-sources-4.1.0. Here is one:\n\nvm_gentoo ~ # uname -a\nLinux vm_gentoo 4.1.0-gentoo-vm_gentoo-20150627 #4 SMP Sat Jun 27 00:07:00 EDT 2015 x86_64 Intel(R) Core(TM) i7 CPU 860 @ 2.80GHz GenuineIntel GNU/Linux\nvm_gentoo ~ # cat /sys/module/{zfs,spl}/version\n0.6.4-126_ga254ecf\n0.6.4-11_g3c82160\nvm_gentoo ~ # equery l zfs spl zfs-kmod\n- Searching for zfs ...\n  [IP-] [  ] sys-fs/zfs-9999:0\n- Searching for spl ...\n  [IP-] [  ] sys-kernel/spl-9999:0\n- Searching for zfs-kmod ...\n  [IP-] [  ] sys-fs/zfs-kmod-9999:0\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/116768971/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/122669908", "body": "In my opinion this is not the correct fix. avl_insert does not take a pointer to avl_index_t like avl_find. Instead avl_index_t is passed by value.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/122669908/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/138416328", "body": "Use the 9999 git ebuilds or wait for 0.6.5.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/138416328/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/176159767", "body": "Does this memory test also fail with only 2 of the 4 dimms installed? You may have to run it much longer. \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/176159767/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/176277739", "body": "Yes. I meant to test rowhammer with 2 DIMMs. At work and otherwise I have seen quite a few RAM problems over the years with all slots populated  especially when using DIMMs that are higher density than the system initially supported. \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/176277739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/176295926", "body": "I saw that but I do not agree with the conclusion. \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/176295926/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/177512550", "body": "On gentoo I am not seeing that with the zfs-9999 build so its most likely something in the new api.\n\n2016-01-30.01:54:51 zpool import -c /etc/zfs/zpool.cache -N -a\n2016-01-31.09:30:52 zfs create zfs_home/test\n2016-01-31.09:32:26 zfs destroy zfs_home/test\n\njmd0 ~ # equery l zfs\n- Searching for zfs ...\n  [IP-] [  ] sys-fs/zfs-9999:0\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/177512550/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/204178027", "body": "I am now getting an error building zfs-kmod-9999 in gentoo related to WANT_DEVNAME2DEVID:\n- Running elibtoolize in: zfs-kmod-9999/\n- Running elibtoolize in: zfs-kmod-9999/config/\n-   Applying portage/1.2.0 patch ...\n-   Applying sed/1.5.6 patch ...\n-   Applying as-needed/2.4.3 patch ...\n  >>> Source prepared.\n  >>> Configuring source in /var/tmp/portage/sys-fs/zfs-kmod-9999/work/zfs-kmod-9999 ...\n  >>> Working in BUILD_DIR: \"/var/tmp/portage/sys-fs/zfs-kmod-9999/work/zfs-kmod-9999\"\n- econf: updating zfs-kmod-9999/config/config.guess with /usr/share/gnuconfig/config.guess\n- econf: updating zfs-kmod-9999/config/config.sub with /usr/share/gnuconfig/config.sub\n  /var/tmp/portage/sys-fs/zfs-kmod-9999/work/zfs-kmod-9999/configure --prefix=/usr --build=x86_64-pc-linux-gnu --host=x86_64-pc-linux-gnu --mandir=/usr/share/man --infodir=/usr/share/info --datadir=/usr/share --sysconfdir=/etc --localstatedir=/var/lib --disable-dependency-tracking --disable-silent-rules --libdir=/usr/lib64 --docdir=/usr/share/doc/zfs-kmod-9999 --bindir=/bin --sbindir=/sbin --with-config=kernel --with-linux=/usr/src/linux --with-linux-obj=/usr/src/linux --with-spl=/usr/src/spl-0.6.5 --with-spl-obj=/usr/src/spl-0.6.5/4.5.0-gentoo-r1-20160331-vm_gentoo --disable-debug\n  checking for gawk... gawk\n  checking metadata... git describe\n  checking build system type... x86_64-pc-linux-gnu\n  checking host system type... x86_64-pc-linux-gnu\n  checking target system type... x86_64-pc-linux-gnu\n  checking whether to enable maintainer-specific portions of Makefiles... no\n  checking whether make supports nested variables... yes\n  checking for a BSD-compatible install... /usr/lib/portage/python2.7/ebuild-helpers/xattr/install -c\n  checking whether build environment is sane... yes\n  checking for a thread-safe mkdir -p... /bin/mkdir -p\n  checking whether make sets $(MAKE)... yes\n  checking for x86_64-pc-linux-gnu-gcc... x86_64-pc-linux-gnu-gcc\n  checking whether the C compiler works... yes\n  checking for C compiler default output file name... a.out\n  checking for suffix of executables...\n  checking whether we are cross compiling... no\n  checking for suffix of object files... o\n  checking whether we are using the GNU C compiler... yes\n  checking whether x86_64-pc-linux-gnu-gcc accepts -g... yes\n  checking for x86_64-pc-linux-gnu-gcc option to accept ISO C89... none needed\n  checking whether x86_64-pc-linux-gnu-gcc understands -c and -o together... yes\n  checking for style of include used by make... GNU\n  checking dependency style of x86_64-pc-linux-gnu-gcc... none\n  checking how to print strings... printf\n  checking for a sed that does not truncate output... /bin/sed\n  checking for grep that handles long lines and -e... /bin/grep\n  checking for egrep... /bin/grep -E\n  checking for fgrep... /bin/grep -F\n  checking for ld used by x86_64-pc-linux-gnu-gcc... /usr/x86_64-pc-linux-gnu/bin/ld\n  checking if the linker (/usr/x86_64-pc-linux-gnu/bin/ld) is GNU ld... yes\n  checking for BSD- or MS-compatible name lister (nm)... /usr/bin/x86_64-pc-linux-gnu-nm -B\n  checking the name lister (/usr/bin/x86_64-pc-linux-gnu-nm -B) interface... BSD nm\n  checking whether ln -s works... yes\n  checking the maximum length of command line arguments... 1572864\n  checking how to convert x86_64-pc-linux-gnu file names to x86_64-pc-linux-gnu format... func_convert_file_noop\n  checking how to convert x86_64-pc-linux-gnu file names to toolchain format... func_convert_file_noop\n  checking for /usr/x86_64-pc-linux-gnu/bin/ld option to reload object files... -r\n  ...skipping...\n  checking whether make_request_fn() returns blk_qc_t... yes\n  checking whether generic IO accounting symbols are avaliable... yes\n  checking whether asm/fpu/api.h exists... yes\n  checking whether debugging is enabled... no\n  checking whether dmu tx validation is enabled... no\n  checking that generated files are newer than configure... done\n  configure: error: conditional \"WANT_DEVNAME2DEVID\" was never defined.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/204178027/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/204203903", "body": "This is from an ebuild that builds the latest git. It will create a clean work directory each time. \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/204203903/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/204214109", "body": "Thanks. I'll hold off the update until this is fixed. I have a recent enough build anyways. \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/204214109/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/204485694", "body": "The latest commit fixed this issue for me. Thanks.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/204485694/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/206485488", "body": "If you install the zfs-9999, spl-9999, zfs-kmod-9999 you should get the current master without the new stable API patches that are in some of the other ebuilds.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/206485488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/206617824", "body": "9999 usually means live (what is in the source control - master, trunk ...) ebuild of a repository. In this case I believe you get what is in the github master branch at the time you emerge.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/206617824/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/207793645", "body": "I would say this is unusual. \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/207793645/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/209370714", "body": "PR = Pull Request.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/209370714/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/280922004", "body": "I just use the 9999 ebuilds. Even in production servers (one with 70TB+ of data in 2 zpools) at home and at work. Although I install first on less important systems.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/280922004/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/217167408", "body": "The zfs-9999, spl-9999, zfs-kmod-9999 ebuilds will have the current master without the stable API patches. \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/217167408/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/218283959", "body": "The zfs-discuss mailing list is for these type of requests.\n\nhttps://github.com/zfsonlinux/zfs/wiki/Mailing-Lists\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/218283959/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/220681564", "body": "I would like this for the same reason. I also use nagios to monitor my servers.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/220681564/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/240096362", "body": "I am having this issue on recent (last 2 weeks) gentoo git builds (zfs-9999 spl-9999 zfs-kmod-9999) using genkernel-next. ldd does not detect libgcc_s.so.1 as a dependency of zpool so it is not added to the initramfs. With that said I believe genkernel-next will have to be updated instead of anything in zol.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/240096362/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/240262805", "body": "I have the same issue in gentoo as well. Are you using genkernel or genkernel-next?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/240262805/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/243891472", "body": "I did not know you could modify / replace the kernel of the gentoo livedvd easily. In the past I have messed with SysRescCD with the srm updating then recreating the iso. I am slightly concerned about this because I am not sure any of the isos have the large_dnode enabled and I have that enabled in several of my servers. Some of which have / on zfs.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/243891472/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/250564795", "body": "I believe this is a question for the mailing list.\n\nhttp://list.zfsonlinux.org/mailman/listinfo/zfs-discuss\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/250564795/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/251477429", "body": "I deleted my comment because I see your problem is a different issue.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/251477429/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/251502367", "body": "I always use real_root. Here is what I have on one server here at work:\n\n`datastore2 zfs # grep ZFS /etc/default/grub\nGRUB_CMDLINE_LINUX_DEFAULT=\"dozfs=force domdadm vga=791 scandelay=5 real_root=ZFS=zfs/os/gentoo\"\n`\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/251502367/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/258948200", "body": "https://www.dan.me.uk/blog/2012/11/14/increase-capacity-of-freebsd-zfs-array-by-replacing-disks/\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/258948200/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/258984363", "body": "I think this should be mailing list question instead. Anyways the only time I have done this I used \"set autoexpand=on\" not sure if this has changed since then.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/258984363/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/269819455", "body": "There were old versions that were patched with the stable API. The newer ones I believe don't have this patch. \r\n\r\n\r\nI just checked and 0.6.5.40-rx releases are patched. 0.6.5.6 and above are not patched.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/269819455/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/272628629", "body": "I just tried that with gentoo-sources-4.9.3 and the 9999 ebuilds for zfs and it worked fine.\r\n\r\n```\r\nvm_gentoo ~ #  cat /sys/module/{zfs,spl}/version\r\n0.7.0-rc2_117_g2dbf1bf\r\n0.7.0-rc2_9_g120faef\r\nvm_gentoo ~ # uname -a\r\nLinux vm_gentoo 4.9.3-gentoo-20170114-0920-vm_gentoo #1 SMP Sat Jan 14 09:26:07 EST 2017 x86_64 Intel(R) Core(TM) i7 CPU 860 @ 2.80GHz GenuineIntel GNU/Linux\r\nvm_gentoo ~ # zpool list\r\nNAME       SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT\r\nzfs_data  28.8G  15.5G  13.2G         -    72%    53%  1.00x  ONLINE  -\r\n\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/272628629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/310746048", "body": "This works for me under gentoo with the recent zfs-9999 builds and gentoo-sources 4.9.3X kernels.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/310746048/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/329273788", "body": "Here is a thread about the Ryzen segfault bug:\r\nhttps://community.amd.com/thread/215773", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/329273788/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/357070218", "body": "> 3.6GHz running @ 1.1?? Quite a bit of underclocking there.\r\n\r\nThat is normal depending on the power saving options enabled.. The CPU clocks down when under low load.", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/357070218/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "Bluewind": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/121974703", "body": "dmesg is clean and as I said, my issues with dovecot are a couple months old and I haven't tested if they still happen since. I'd rather not break the index by trying if it's still broken with mmap.\n\nAs for `tail -f` I see the following in strace:\n\n```\n...\nread(3, \"\", 8192)                       = 0\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 32\nfstat(3, {st_mode=S_IFREG|0640, st_size=6834244, ...}) = 0\nread(3, \"Jul 16 15:45:03 mistral sshd[150\"..., 8192) = 412\nread(3, \"\", 8192)                       = 0\nwrite(1, \"Jul 16 15:45:03 mistral sshd[150\"..., 412) = 412\nfstat(3, {st_mode=S_IFREG|0640, st_size=6834244, ...}) = 0\nopen(\"/usr/share/locale/locale.alias\", O_RDONLY|O_CLOEXEC) = 5\nfstat(5, {st_mode=S_IFREG|0644, st_size=2492, ...}) = 0\nmmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fe696e8e000\nread(5, \"# Locale name alias data base.\\n#\"..., 2560) = 2492\nread(5, \"\", 2560)                       = 0\nclose(5)                                = 0\nmunmap(0x7fe696e8e000, 4096)            = 0\nopen(\"/usr/share/locale/en_US.UTF-8/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\nopen(\"/usr/share/locale/en_US.utf8/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\nopen(\"/usr/share/locale/en_US/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\nopen(\"/usr/share/locale/en.UTF-8/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\nopen(\"/usr/share/locale/en.utf8/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\nopen(\"/usr/share/locale/en/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\nwrite(2, \"tail: \", 6tail: )                   = 6\nwrite(2, \"/var/log/everything.log: file tr\"..., 39/var/log/everything.log: file truncated) = 39\nwrite(2, \"\\n\", 1\n)                       = 1\nlseek(3, 0, SEEK_SET)                   = 0\nread(3, \"Jul 12 00:00:21 calima syslog-ng\"..., 8192) = 8192\n...\n```\n\nAs you can see at some point a \"file truncated\" error is thrown and tail seeks to 0 so the whole file is printed. I guess I missed that \"file truncated\" error before due to the large amount of output afterwards. I'm not sure where that error occurs in the strace output though. Does that help or do you need more information?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/121974703/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/122045158", "body": "The file is not being truncated, certainly not by logrotate. It does indeed look like fstat is returning a size way too low. Whole output below (same run of strace as before). Actually the fstat calls with the bigger and then smaller size values are also in the excerpt I posted earlier.\n\n```\nexecve(\"/usr/bin/tail\", [\"tail\", \"-f\", \"/var/log/everything.log\"], [/* 61 vars */]) = 0\nbrk(0)                                  = 0x2005000\naccess(\"/etc/ld.so.preload\", R_OK)      = -1 ENOENT (No such file or directory)\nopen(\"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) = 3\nfstat(3, {st_mode=S_IFREG|0644, st_size=44293, ...}) = 0\nmmap(NULL, 44293, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7fe696e85000\nclose(3)                                = 0\nopen(\"/usr/lib/libc.so.6\", O_RDONLY|O_CLOEXEC) = 3\nread(3, \"\\177ELF\\2\\1\\1\\3\\0\\0\\0\\0\\0\\0\\0\\0\\3\\0>\\0\\1\\0\\0\\0\\260\\10\\2\\0\\0\\0\\0\\0\"..., 832) = 832\nfstat(3, {st_mode=S_IFREG|0755, st_size=1979984, ...}) = 0\nmmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fe696e84000\nmmap(NULL, 3807760, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7fe6968cd000\nmprotect(0x7fe696a66000, 2093056, PROT_NONE) = 0\nmmap(0x7fe696c65000, 24576, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x198000) = 0x7fe696c65000\nmmap(0x7fe696c6b000, 14864, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0x7fe696c6b000\nclose(3)                                = 0\nmmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fe696e83000\nmmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fe696e82000\narch_prctl(ARCH_SET_FS, 0x7fe696e83700) = 0\nmprotect(0x7fe696c65000, 16384, PROT_READ) = 0\nmprotect(0x60e000, 4096, PROT_READ)     = 0\nmprotect(0x7fe696e90000, 4096, PROT_READ) = 0\nmunmap(0x7fe696e85000, 44293)           = 0\nbrk(0)                                  = 0x2005000\nbrk(0x2026000)                          = 0x2026000\nopen(\"/usr/lib/locale/locale-archive\", O_RDONLY|O_CLOEXEC) = 3\nfstat(3, {st_mode=S_IFREG|0644, st_size=1607712, ...}) = 0\nmmap(NULL, 1607712, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7fe696cf9000\nclose(3)                                = 0\nopen(\"/var/log/everything.log\", O_RDONLY) = 3\nfstat(3, {st_mode=S_IFREG|0640, st_size=6831034, ...}) = 0\nlseek(3, 0, SEEK_CUR)                   = 0\nlseek(3, 0, SEEK_END)                   = 6831034\nlseek(3, 6823936, SEEK_SET)             = 6823936\nread(3, \"t.\\nJul 16 15:33:14 ostro systemd\"..., 7098) = 7098\nfstat(1, {st_mode=S_IFCHR|0666, st_rdev=makedev(1, 3), ...}) = 0\nioctl(1, TCGETS, 0x7ffc8a886600)        = -1 ENOTTY (Inappropriate ioctl for device)\nmmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fe696e8f000\nread(3, \"\", 0)                          = 0\nfstat(3, {st_mode=S_IFREG|0640, st_size=6831034, ...}) = 0\nfstatfs(3, {f_type=0x2fc12fc1, f_bsize=131072, f_blocks=9927371, f_bfree=9913477, f_bavail=9913477, f_files=2537915343, f_ffree=2537850200, f_fsid={-1490398645, 13629989}, f_namelen=255, f_frsize=131072}) = 0\nlstat(\"/var/log/everything.log\", {st_mode=S_IFREG|0640, st_size=6831034, ...}) = 0\ninotify_init()                          = 4\nwrite(1, \"Jul 16 15:40:21 mistral dhcpd[51\"..., 900) = 900\ninotify_add_watch(4, \"/var/log/everything.log\", IN_MODIFY) = 1\nstat(\"/var/log/everything.log\", {st_mode=S_IFREG|0640, st_size=6831034, ...}) = 0\nfstat(3, {st_mode=S_IFREG|0640, st_size=6831034, ...}) = 0\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 16\nfstat(3, {st_mode=S_IFREG|0640, st_size=6831223, ...}) = 0\nread(3, \"Jul 16 15:41:42 mistral dhcpd[51\"..., 8192) = 189\nread(3, \"\", 8192)                       = 0\nwrite(1, \"Jul 16 15:41:42 mistral dhcpd[51\"..., 189) = 189\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 16\nfstat(3, {st_mode=S_IFREG|0640, st_size=6831355, ...}) = 0\nread(3, \"Jul 16 13:44:49 192.168.4.46 odh\"..., 8192) = 132\nread(3, \"\", 8192)                       = 0\nwrite(1, \"Jul 16 13:44:49 192.168.4.46 odh\"..., 132) = 132\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 16\nfstat(3, {st_mode=S_IFREG|0640, st_size=6831619, ...}) = 0\nread(3, \"Jul 16 13:44:49 192.168.4.46 odh\"..., 8192) = 396\nread(3, \"\", 8192)                       = 0\nwrite(1, \"Jul 16 13:44:49 192.168.4.46 odh\"..., 396) = 396\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 16\nfstat(3, {st_mode=S_IFREG|0640, st_size=6831883, ...}) = 0\nread(3, \"Jul 16 13:44:49 192.168.4.46 odh\"..., 8192) = 132\nread(3, \"\", 8192)                       = 0\nwrite(1, \"Jul 16 13:44:49 192.168.4.46 odh\"..., 132) = 132\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 16\nfstat(3, {st_mode=S_IFREG|0640, st_size=6832015, ...}) = 0\nread(3, \"Jul 16 13:44:49 192.168.4.46 odh\"..., 8192) = 132\nread(3, \"\", 8192)                       = 0\nwrite(1, \"Jul 16 13:44:49 192.168.4.46 odh\"..., 132) = 132\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 16\nfstat(3, {st_mode=S_IFREG|0640, st_size=6832015, ...}) = 0\nread(3, \"\", 8192)                       = 0\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 16\nfstat(3, {st_mode=S_IFREG|0640, st_size=6832096, ...}) = 0\nread(3, \"Jul 16 15:44:52 mistral dhcpd[51\"..., 8192) = 81\nread(3, \"\", 8192)                       = 0\nwrite(1, \"Jul 16 15:44:52 mistral dhcpd[51\"..., 81) = 81\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 16\nfstat(3, {st_mode=S_IFREG|0640, st_size=6832199, ...}) = 0\nread(3, \"Jul 16 15:44:53 mistral dhcpd[51\"..., 8192) = 103\nread(3, \"\", 8192)                       = 0\nwrite(1, \"Jul 16 15:44:53 mistral dhcpd[51\"..., 103) = 103\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 16\nfstat(3, {st_mode=S_IFREG|0640, st_size=6832422, ...}) = 0\nread(3, \"Jul 16 15:44:53 mistral dhcpd[51\"..., 8192) = 223\nread(3, \"\", 8192)                       = 0\nwrite(1, \"Jul 16 15:44:53 mistral dhcpd[51\"..., 223) = 223\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 16\nfstat(3, {st_mode=S_IFREG|0640, st_size=6832494, ...}) = 0\nread(3, \"Jul 16 15:44:54 mistral arpwatch\"..., 8192) = 72\nread(3, \"\", 8192)                       = 0\nwrite(1, \"Jul 16 15:44:54 mistral arpwatch\"..., 72) = 72\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 16\nfstat(3, {st_mode=S_IFREG|0640, st_size=6832638, ...}) = 0\nread(3, \"Jul 16 15:44:55 mistral arpwatch\"..., 8192) = 144\nread(3, \"\", 8192)                       = 0\nwrite(1, \"Jul 16 15:44:55 mistral arpwatch\"..., 144) = 144\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 16\nfstat(3, {st_mode=S_IFREG|0640, st_size=6832782, ...}) = 0\nread(3, \"Jul 16 15:44:56 mistral arpwatch\"..., 8192) = 144\nread(3, \"\", 8192)                       = 0\nwrite(1, \"Jul 16 15:44:56 mistral arpwatch\"..., 144) = 144\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 16\nfstat(3, {st_mode=S_IFREG|0640, st_size=6832945, ...}) = 0\nread(3, \"Jul 16 15:44:58 mistral dhcpd[51\"..., 8192) = 163\nread(3, \"\", 8192)                       = 0\nwrite(1, \"Jul 16 15:45:01 pearl crond[2289\"..., 179) = 179\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 16\nfstat(3, {st_mode=S_IFREG|0640, st_size=6833214, ...}) = 0\nread(3, \"Jul 16 15:45:01 pearl CROND[2289\"..., 8192) = 90\nread(3, \"\", 8192)                       = 0\nwrite(1, \"Jul 16 15:45:01 pearl CROND[2289\"..., 90) = 90\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 16\nfstat(3, {st_mode=S_IFREG|0640, st_size=6833317, ...}) = 0\nread(3, \"Jul 16 15:45:01 calima crond[248\"..., 8192) = 218\nread(3, \"\", 8192)                       = 0\nwrite(1, \"Jul 16 15:45:01 calima crond[248\"..., 218) = 218\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 16\nfstat(3, {st_mode=S_IFREG|0640, st_size=6833432, ...}) = 0\nread(3, \"\", 8192)                       = 0\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 16\nfstat(3, {st_mode=S_IFREG|0640, st_size=6833687, ...}) = 0\nread(3, \"Jul 16 15:45:03 pearl sshd[22913\"..., 8192) = 255\nread(3, \"\", 8192)                       = 0\nwrite(1, \"Jul 16 15:45:03 pearl sshd[22913\"..., 255) = 255\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 16\nfstat(3, {st_mode=S_IFREG|0640, st_size=6833906, ...}) = 0\nread(3, \"Jul 16 15:45:03 pearl systemd-lo\"..., 8192) = 219\nread(3, \"\", 8192)                       = 0\nwrite(1, \"Jul 16 15:45:03 pearl systemd-lo\"..., 219) = 219\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 16\nfstat(3, {st_mode=S_IFREG|0640, st_size=6833906, ...}) = 0\nread(3, \"\", 8192)                       = 0\nread(4, \"\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\1\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\", 40) = 32\nfstat(3, {st_mode=S_IFREG|0640, st_size=6834244, ...}) = 0\nread(3, \"Jul 16 15:45:03 mistral sshd[150\"..., 8192) = 412\nread(3, \"\", 8192)                       = 0\nwrite(1, \"Jul 16 15:45:03 mistral sshd[150\"..., 412) = 412\nfstat(3, {st_mode=S_IFREG|0640, st_size=6834244, ...}) = 0\nopen(\"/usr/share/locale/locale.alias\", O_RDONLY|O_CLOEXEC) = 5\nfstat(5, {st_mode=S_IFREG|0644, st_size=2492, ...}) = 0\nmmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fe696e8e000\nread(5, \"# Locale name alias data base.\\n#\"..., 2560) = 2492\nread(5, \"\", 2560)                       = 0\nclose(5)                                = 0\nmunmap(0x7fe696e8e000, 4096)            = 0\nopen(\"/usr/share/locale/en_US.UTF-8/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\nopen(\"/usr/share/locale/en_US.utf8/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\nopen(\"/usr/share/locale/en_US/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\nopen(\"/usr/share/locale/en.UTF-8/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\nopen(\"/usr/share/locale/en.utf8/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\nopen(\"/usr/share/locale/en/LC_MESSAGES/coreutils.mo\", O_RDONLY) = -1 ENOENT (No such file or directory)\nwrite(2, \"tail: \", 6tail: )                   = 6\nwrite(2, \"/var/log/everything.log: file tr\"..., 39/var/log/everything.log: file truncated) = 39\nwrite(2, \"\\n\", 1\n)                       = 1\nlseek(3, 0, SEEK_SET)                   = 0\nread(3, \"Jul 12 00:00:21 calima syslog-ng\"..., 8192) = 8192\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/122045158/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/122650984", "body": "Thanks for the fix (didn't test it yet). I've enable mmap in dovecot again and I'll create a new bug if the issue is still there. It's been 2 months after all.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/122650984/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "jefferai": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/10836027", "body": "I don't get this segfault when I take out the \"listsnaps=on\" from the -o property.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/10836027/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/10836069", "body": "Okay, I now realize that I had totally wrong syntax for the comment. I'd argue that it still shouldn't segfault, though.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/10836069/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/11790739", "body": "OK -- I'll have to do the iostat another time as we're working around the slowness now and that has to take priority.\n\nThat said, this is in a VM. The backing store for all of the virtual disks is the same, and the other two (non-ZFS) disks in the same VM are also totally fine. So I don't think it's that.\n\nI've turned off atime, although I don't expect a drastic improvement, as my problem is also with writes, not just reads.\n\nI also tried setting arc_max to 1GB, even though there are 12GB on the machine (and it uses nowhere near that) since I've read that it can cause pressure on Linux. No luck though.\n\nThanks, and I'll get back about iostat.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/11790739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/12465828", "body": "I just wanted to update this -- I've been testing as I've had time. On stock Precise (no updates) and Wheezy I'm still seeing this issue. I'm trying a Quantal install now to see if I can replicate the problem.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/12465828/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/14481054", "body": "Just to update -- same issue on Quantal.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/14481054/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45650432", "body": "I unfortunately do not have access to the testbed where I was seeing this problem and have not replicated a similar testbed anywhere else. So sure, I'll close it, but I can't say whether or not it's fixed.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/45650432/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/64221788", "body": "That did _not_ fix my issue. In fact, using \"apt-get update; apt-get dist-upgrade\" followed by a reboot is how I ended up in this mess in the first place. I ended up with two kernels on my system (new one from the upgrade, and the previous one) and ZFS using the old modules with the new kernel.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/64221788/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/65863998", "body": "What worked for me (@neutreno) is purging all of the ZFS packages and all old kernels (so only one version of kernel package was on the system), then reinstalling ZoL. Some of those steps may be more extreme than necessary, but it did get my system back to a working state.\n\nI believe I may have also had to import my pool, but honestly don't remember if that was necessary or just something I tried along the way.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/65863998/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "theogfx": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/33963743", "body": "I also confirm the issue on Ubuntu Server 12.04 (x64). Putting `zfs share -a` in `/etc/rc.local` solved the problem (as @igmgit suggested).\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/33963743/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "tycho": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/54470961", "body": "Agree, would like to see this one prioritized. What work is remaining at this point? A rebase of the patch series?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/54470961/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/216620571", "body": "I just hit this as well. Would be ideal to get this working properly because deduplication could have a lot of wins when replicating an entire pool.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/216620571/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/85710926", "body": "Sounds good. I just updated my SPL/ZFS packages in response to this issue, we'll see if it crops up again.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/85710926/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/85711039", "body": "Oh, wait, that's not in the master repo yet?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/85711039/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/85711302", "body": "The EC2 kernel you point out is the one I maintain. I run it on my bare metal boxes too. :)\n\nNot much load. Just a Feed the Beast server, Apache, git daemon, SSHd... System's only using about half its 48GB of RAM at any one moment.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/85711302/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/85711624", "body": "The SPL/ZFS PKGBUILDs are in my ec2-packages repo: \n\nhttp://git.uplinklabs.net/snoonan/projects/archlinux/ec2/ec2-packages.git/tree/spl-git\nhttp://git.uplinklabs.net/snoonan/projects/archlinux/ec2/ec2-packages.git/tree/zfs-git\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/85711624/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/85715488", "body": "Yeah, already on it:\n\nhttp://git.uplinklabs.net/snoonan/projects/archlinux/ec2/ec2-packages.git/commit/?id=c80bc6566a6cd769606b8ac4047225bc341855d8\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/85715488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/85810005", "body": "@kernelOfTruth Yeah, that diff looks like the one I pulled in. When I added his repo as a remote and did a 'git remote update' it didn't fetch dweeezil@f04f972, so I sifted through his branches to find the right commit.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/85810005/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/88289710", "body": "So far no issues... Still running strong.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/88289710/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/89852606", "body": "Haven't updated since 12 days ago, I'll kick off some package builds.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/89852606/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/92169290", "body": "Been running Linux 3.19.3 + ZFS-on-Linux release 0.6.4 for a few days. No issues so far. I believe the issue was indeed fixed by #3225. Closing.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/92169290/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/105392782", "body": "I just ran into this. The solution I used was this:\n\nhttp://git.uplinklabs.net/snoonan/projects/archlinux/ec2/ec2-packages.git/plain/zfs-git/0001-zfs-import-.service-ExecStart-doesn-t-support.patch\n\nFeel free to pull that patch into the zfs repo if it looks alright.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/105392782/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/105667151", "body": "@behlendorf In this case, I believe ExecStart/ExecStartPre will be functionally the same. I'll pose the question on the systemd-devel mailing list.\n\nhttp://lists.freedesktop.org/archives/systemd-devel/2015-May/032268.html\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/105667151/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/105672078", "body": "There's a reply on the mailing list.\n\nhttp://lists.freedesktop.org/archives/systemd-devel/2015-May/032269.html\n\nCopying here for posterity.\n\n```\nFrom: Christian Seiler christian at iwakd.de \nDate: Tue May 26 14:26:11 PDT 2015\nSubject: [systemd-devel] Re: ExecStart vs ExecStartPre\n\nOn 05/26/2015 11:12 PM, Steven Noonan wrote:\n> Hi there,\n> \n> I'm wondering what the functional difference is between doing:\n> \n> ExecStartPre=/bin/foo\n> ExecStart=/bin/bar\n> \n> and\n> \n> ExecStart=/bin/foo\n> ExecStart=/bin/bar\n> \n> From my read of the systemd.service man page, they appear to have the\n> same behavior in the common use case.\n\nThree differences come directly to mind:\n\n - If you have unit of type that is NOT oneshot (simple, forking,\n   etc.), you can have only a single ExecStart= line, not multiple\n   ones. The main service process must be started in ExecStart=\n   and not ExecStartPre=.\n\n - Even in Type=oneshot units you must have at least one ExecStart=\n   line (but can in any case have an arbitrary amount of ExecStartPre=\n   lines, even zero).\n\n - If you set PermissionsStartOnly= or RootDirectoryStartOnly=, then\n   certain settings will be applied to ExecStart= but not to\n   ExecStartPre= (see manpage for details).\n\n(There are probably more.)\n\nGenerally speaking, I follow the following guidelines when writing\nunits:\n\n - Type=oneshot: I typically use only ExecStart= and only use\n   ExecStartPre= if I have to use *StartOnly=true (see above).\n\n - other types: ExecStart= to start the service proper, ExecStartPre=\n   for preparatory things (like generating a default config if none\n   is already present or something along those lines)\n\nBut it really depends on your use case and as always YMMV.\n\nHope that helps!\n\nChristian\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/105672078/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/105677459", "body": "Yep. Already signed off on your version in the pull request. :smile: \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/105677459/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/105667560", "body": "```\nSigned-off-by: Steven Noonan <steven@uplinklabs.net>\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/issues/comments/105667560/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "JakeWharton": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/147018", "body": "These two lines should have four spaces before them so they are rendered like this:\n\n```\n$ ./configure\n$ make pkg\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/147018/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "rdylina": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/194382", "body": "Would it not be better to treat this somewhat like a security issue by blacklisting all known devices that are definitely not available to be used as block devices? Somehow seems safer to me.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/194382/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "fajarnugraha": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/282697", "body": "Brian, cmd/zvol_id/Makefile.am is missing. Could you please upload it? Thanks.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/282697/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/282698", "body": "Sorry, I mean cmd/zvol_id/Makefile.in\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/282698/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/282796", "body": "Create another branch, then copy previous comments manually?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/282796/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/283041", "body": "Copying previous note from email:\n\n<pre>\nfrom    : behlendorf <noreply@github.com>\ndate    : Fri, Feb 25, 2011 at 3:36 AM\nsubject : Re: [GitHub] Use udev to create /dev/zvol/[dataset_name] links [behlendorf/zfs feaf4b2]\n</pre>\n\n- Removed Makefile-sample, with the full integration in the build system it isn't needed.\n- Added all autogen.sh products (Makefile.in, configure) using the following versions of the utils.  Using the same versions of the tools minimizes how much change there is in the autogen products and makes it easier to review.\n  \n  autoconf (GNU Autoconf) 2.63\n  automake (GNU automake) 1.11.1\n  ltmain.sh (GNU libtool) 2.2.6b\n- Added the CDDL header to zvol_id_main.c, including correctly attributing the source.\n- Minor stray whitespace cleanup.\n- Update kmem_free() in zvol_remove_minors() to match  kmem_zalloc()'s use of MAXNAMELEN.  If we fail to do the the memory account code will flag this is a memory leak.  It's critical to ensure you alloc/free both use the same size for the buffer.\n- Add <sys/stat.h> header in zvol_id_main.c, without it my build was failing on RHEL6.\n\nhttps://github.com/behlendorf/zfs/commit/feaf4b287322d6123336f139049686114f6c6ee8#commitcomment-282533\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/283041/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "baryluk": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/383541", "body": "$ mkfs.ext3 /dev/zd0\n$ resize2fs /dev/zd0\n\nI hope you mean\n$ mkfs.ext3 /dev/tank/zd0\n$ resize2fs /dev/tank/zd0\n\nHaving full volume path as well pool name under dev is crucial to prevent conflicts. I would even like to have it under /dev/zvols/tank/zd0, to not conflict with default devices. Consider doing zpool create sda /dev/sda, zfs create -V 10g sda/sda. Horrible.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/383541/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/383556", "body": "See https://github.com/behlendorf/zfs/issues/152#issuecomment-1162158 for some more discussion.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/383556/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/383557", "body": "Probably releated to https://github.com/behlendorf/zfs/commit/4c0d8e50b99b4f3b4a9b7bc67ac7fc4e406f5755\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/383557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/383767", "body": "Hmm. For me, more natural and more reliable will be to do this in opposite direction. Create device node in from start /dev/zvol/pool/dataset and then create symlink /dev/zd\\* (this actually can be skipped, as is in most practical cases useless beyond eventually symlinking to it from somewhere) and in /dev/disk/by-*/xxx pointing to /dev/zvol/pool/dataset. Is there any reasons or limitations of other tools (kernel, udev, sysfs?, creating DOS partitions on zvols? etc) that you want to put devices directly in the /dev/ directory? It is not necessary to put them there directly. \n\nPS. Hmm. I just checked open-iscsi, and do the same as you. First create /dev/sdX, and then symlink it into /dev/disk/by-path/ip-X.X.X.X:PP-iscsi-iqn-XYZ by udev. So you are right. IMHO it is remenescent of archaic structure of /dev/ directory. Not best possible and easy way, but looks to be standard in Linux. Neverthless /dev/zd\\* shouldn't be used anyway in such tools like fstab, mount, fdisk, fsck, mkfs.*, etc., as this names are unstable, for example doing zfs create -V 10g tank/zd1, will still create /dev/zd0 (at least if this was first volume create/mounted after reboot right?), and symlink in /dev/zvol/tank/zd1 -> ../../zd0. It is pretty confusing. This is the reason why I was somehow against it.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/383767/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "dajhorn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/461576", "body": "Kernel parameters are subject to decimal/octal/hexadecimal interpretation, so this example should be `spl_hostid=0x00bab10c`.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/461576/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/545339", "body": "zfsonlinux/zfs#370\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/545339/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/545340", "body": "zfsonlinux/zfs#370\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/545340/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/545341", "body": "zfsonlinux/zfs#370\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/545341/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/545343", "body": "zfsonlinux/zfs#370\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/545343/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/545347", "body": "zfsonlinux/zfs#370\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/545347/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/545348", "body": "zfsonlinux/zfs#370\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/545348/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/545350", "body": "zfsonlinux/zfs#370\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/545350/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/623579", "body": "This line breaks dracut on DEB systems with a dash system shell because its built-in printf does not implement the hex format.\n\nSuppose AA=12; BB=34; CC=56; DD=78. This results in a literal \"\\x04\\x03\\x02\\x01\" in the /etc/hostid file.\n\nThe fix is to call the external /usr/bin/printf or to change the whack-bang to bash.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/623579/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/624786", "body": "Yes, I will submit a pull request when I get a moment at my workstation. The change should be trivial; just copied from the commit that you referenced in the ticket.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/624786/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/633279", "body": "There are no bash consumers in a regular Ubuntu initrd, and I'm pretty sure that using bash in the boot stack is a Debian policy violation.\n\nUsing /usr/bin/printf is preferable because it is much smaller than /bin/bash.  (48kB vice 932kB for Oneiric.)\n\nThis seemed like an easy job, but the general solution needs bench testing.  Right now, I'm thinking that I should just patch it for Ubuntu in pkg-zfs to use whatever busybox provides.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/633279/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/4521738", "body": "@ryao, why was the library revision number decremented here?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/4521738/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/4522109", "body": "// I'm asking because this change caused my local builds to squawk on the apparent downgrade and I don't want to handle this case during a 0.6.2 to 0.6.3 production upgrade.  Rather, is this change only cosmetic to make it the same as upstream?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/4522109/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/414977", "body": "If this is a redundancy, then I will remove the override. I am trying to stay tight with Debian packaging policy, and (slowly) working towards making pkg-zfs lintian clean.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/414977/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/415330", "body": "The `extra-license-file` override currently satisfies the lintian warning about the `OPENSOLARIS.LICENSE` file, which cannot be changed like the `COPYING` file because it is in the Oracle copyright attributions.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/415330/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "kylef": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/511969", "body": "I didn't think about that.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/511969/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "hobbes1069": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/655859", "body": "Should --with-udevdir default to EPREFIX/lib/udev?\n\nI'm a Fedora packager so EPREFIX may make sense for other distros, but on Fedora the guideline is the packages install to /usr/{bin,lib{,64},share}... etc. But the udev rules do not go into /usr/lib/udev but /lib/udev as you mention.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/655859/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/656261", "body": "It was easy enough to override manually, just thought I'd mention it.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/656261/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "evansus": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2201510", "body": "This is looping up to 3000 times rather than up to 3000 milliseconds\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2201510/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "imp": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2201666", "body": "But each iteration adds a millisecond delay, so that it effectively converts iterations into time delay. Doesn't it ?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2201666/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "richardelling": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2260628", "body": "For most devices, including modern HDDs, a write buffer masks the impact of writes while simultaneously hiding the effects of I/O task scheduling in the device. In other words, write latency for HDDs is not directly correlated to the number of writes queued or the LBAs. It is not safe to assume that the claimed performance improvements are generically applicable.\n\nThat said, the negative impact of the changes is negligible and the future benefits of FASTWRITE logic can be applied in other ways. I like the changes proposed.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2260628/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "lundman": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2322697", "body": "This fixes my problems as well, thanks. Testing will resume.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2322697/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/5337747", "body": "Should this not keep the value of \"error\" like the other lines do?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/5337747/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6983232", "body": "Amusingly, this was causing some issues for us over in OSX. With our slice allocator, often it would allocate from size 16, truncate the string, then spa_strfree() would free it as shorter than 16, and be placed on the size 8 slice. The the memory would be in two slices, and be given out to two allocs simultaneously. Highly amusing. Strangely IllumOS uses kmem_alloc with `len` to allocate the string, but call spa_strfree (on the shorter string) even though they have `len` still. We had to go with proper \nhttps://github.com/openzfsonosx/zfs/compare/b0421400029e...e2eafee60f47\nbut also add debug to our SPL slice allocator to throw a fit when alloc and free sizes do not agree, so we can find these situations much more quickly.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6983232/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "nightwalk": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2424432", "body": "ZFS already resilvers when it needs to, so I don't think there'd be many people who'd want the fsck script running 'zpool scrub'.\n\nFirstly, doing so would violate the principle of least surprise. \n\nSecondly, scrubs are disk-intensive, which is not the sort of thing that should be happening out of the blue and outside of scheduled scrub windows.\n\nThe cron jobs which kick off periodic scrubs should be good enough for most use cases, and the user can kick off a manual scrub if they really need to besides.\n\nThere is a misspelling in the script's comments (accomidate vs. accommodate) but I think it should be safe to call this 'good' and move on to something more important.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2424432/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "shenyan1": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2498997", "body": "@dechamps\uff0cI think the vdev 's align is always 512 bytes.  The data may not be aligned in the zvol or zfs. As I see,when data pass the dmu layer,dmu will make the data aligned. So it is always aligned in zio's pipeline. The code path isn't heavily used. Am I right?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/2498997/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "casualfish": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/3524622", "body": "@ryao In fact I did exactly what you said here on the first try, referring to https://github.com/zfsonlinux/spl/pull/254 :)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/3524622/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "pyavdr": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6064823", "body": "@behlendorf \n\nBrian, i got a compile warning-error from this commit. In arc.c the use of the new functions seems to be undeclared, so there is a undeclared usage error from gcc. I didnt find anything wrong, so i did a \"git revert\" to go on. This was on kernel 3.4, opensuse 12.2.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6064823/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6064972", "body": "Ok, missed that. Thank you.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6064972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "CMCDragonkai": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6910375", "body": "@behlendorf  So can one set atime=off and relatime=on. Does both need to be on to take effect?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/6910375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "ilovezfs": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/7685208", "body": "Should this commit permit not using innodb_use_native_aio=0 in /etc/mysql/my.cnf? I tried removing it, but mysql would not work unless I put it back.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/7685208/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31380799", "body": "s/This because/This is because/\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31380799/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "rightfold": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/8040208", "body": "This is not RAII.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/8040208/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "haraldh": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/9681070", "body": "Why aren't you writing to \"$initdir/etc/hostid\" directly but use mktemp?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/9681070/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "FransUrbo": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10248098", "body": "I think this one is a duplicate. This information is now located in both <code>zfs send</code> AND <code>zfs receive</code>.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10248098/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10970751", "body": "Ah :). I didn't even know/remember that file! But I'm not going to push anything until you say you're done with the review :)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10970751/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972317", "body": "Yes. The <code>zfs-common</code> takes this into account. But that file isn't installed automatically.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972317/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972376", "body": "It just looked a little better (more 'compact') this way, but I've changed this. Didn't turn out to bad.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972376/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972489", "body": "I think the idea is/was that the systemd scripts should use this file as well for some of it's information. Don't know if that's actually the case, but maybe they should?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972489/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972558", "body": "Misstake probably..\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972558/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972634", "body": "Can't remember, but there was some issue we (you and I I think) talked about a year or so ago\u2026 But looking at it again, it doesn't make much sense. I'll remove it.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972634/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972721", "body": "Oh, now I remember! It was when I had problem with my pool, and I wanted to boot the system, but NOT have it import the pool or load the module. I wanted manual control over it to not 'stress' it unnecessarily.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972721/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972830", "body": "I think the option is valid. There IS a use-case for not 'enabling' ZFS at boot. And checking that all the needed components is there and working, seems to be a valid job to do...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972830/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972842", "body": "Ok, good.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972842/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972890", "body": "Yeah, udevd (which is what I'm used to, don't know about any other) is sometimes a little \u2026 \"slow\". But I'm adding a comment about this. Don't know if one sec is 'enough' in all circumstances, but I haven't had any complaints so...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972890/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972934", "body": "I think that part comes directly from 'someone else' (some other script or functionality and it looked good and safe to have there - there HAVE been reports of \"some\" distribution not having <code>/etc/mtab</code> as a symlink).\n\nSince it's only run _if need be_, I think it should be safe as well as supporting dists that isn't \u2026 \"modern\" :)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972934/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972998", "body": "Remember, the whole point of this PR is to have _ONE_ script for everyone/everything so we don't have to write special script for special cases/distribution and end up with 'a bunch' to support. Eventually, yes, maybe this will grow out of control, but we can cross that bridge if/when it happens :). If it's not \"in the way\", I'd like to keep it. Unless we find out this will NEVER happen...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10972998/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973027", "body": "Agreed. No idea where that came from Possibly from Gentoo. @ryao, is <code>mountinfo</code> something you recognize?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973027/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973909", "body": "That sounds reasonable. I'll see what I can do about that.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973909/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973923", "body": "And then we're back to having multiple scripts to maintain\u2026 Which is the whole purpose of this.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973923/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973929", "body": "Yes.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973929/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973972", "body": "No, because they solve two slightly different problems. That first part is for mounting zvols (with any FS) and the second is for ZFS filesystems/datasets. I guess one could/should combine them, but regexp have never been my strong suite..\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10973972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974038", "body": "But that would make this whole endeavor completely pointless\u2026. Having ONE set of scripts, with different purpose. Not \"a whole bunch\" which is only slightly different. As one for each distribution. And not everyone have gone all-in on systemd! There's still a few bastions for freedom out there! :)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974038/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974056", "body": "How do you mean?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974056/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974076", "body": "Simpler to write (and understand perhaps), yes. But not to maintain. The five (!!) scripts that's there now started out as \"slightly different\" but are now quite (!) different. So whoever is adding support for THEIR distribution, isn't keeping 'the others' up to date on their changes. SOMEONE is lagging behind (considering the functionality of MY scripts, I'd say all five of them do :). \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974076/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974104", "body": "No, I checked (which is the reason I added the sleep). <code>udevadm</code> returns immediately and does not wait for success. And there's no way to tell it to wait for response from the daemon\u2026 There is an issue somewhere in the tracker about this.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974104/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974201", "body": "But using <code>wait_udev()</code> is a good idea.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974201/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974348", "body": "The script isn't really about simplicity. It's about portability.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974348/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974520", "body": "Ah, ok.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974520/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974671", "body": "> Portability is good. But what exactly are we trying to be portable with?\n> \n> There are A LOT of distributions out there\u2026\n\nIf you're not interested in portability and having a unified script, then the discussion is really moot, isn't there? This PR is exactly that. Sure, I could start splitting them up into lsb and redhat versions, but then we would just be back to EXACTLY what we already have - scripts that no one is interested in maintaining and that will start diverging at almost the same moment it's accepted...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974671/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974755", "body": "Perhaps. But if that is what you want, then there is little to no point in this PR. It adds only one or two additional features and those can easily be ported to those two.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974755/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10975180", "body": "> refresh this with markups we've discussed and keep it as just the one version.\n> \n> Done. But I split up zfs-mount into two: zfs-import and zfs-mount. I still need to verify if that works in practice, but we have something to go forward with with this.\n\nI also renamed the etc/init.d/zfs-{import,mount,share}.in files (removed the .in suffix). It wasn't needed - all path config is in the zfs-common.in script and is the only one that actually needs to be sed'd.\n\nI also took another look at the zfs-zed script. It's VERY Debian GNU/Linux centric at the moment. I've missed to fix this, so we need to run that through a couple of times to.\n\n> If it's possible that's may preference as well, I'm just a bit concerned it won't be.\n> \n> Looking at the scripts and the function, there should be very little need to tweak the current code. It's quite generic. But additional functions for special corner cases for certain (as yet unnamed) distributions might be needed [eventually].\n\nThe only place I recon we're going to need to tweak is the 'logging stuff'. I know it's crude, but in the end I think it's ok - it's \"out of sight\" in /etc/zfs/common.init (or whatever we name it).\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10975180/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10981262", "body": "Is thirty a good number? Sounds a little long\u2026 ?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10981262/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10981331", "body": "ACtually, oups. That's the <code>Should-Start</code> and <code>Should-Stop</code> in <code>zfs-share</code> (which I have to rename to <code>zfs-share.in</code> again *blush*).\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10981331/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10982263", "body": "I've added some code to <code>config/zfs-build.m4</code> to find if it should be <code>/etc/default</code> or <code>/etc/sysconfig</code> and install the defaults (renamed to <code>zfs</code>) accordingly.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10982263/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10992658", "body": "LOL, yeah :). I put 15 sec there now. It will return as soon as it's done, so 15 seconds is the MOST we're prepared to wait...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10992658/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11052025", "body": "Fair enough. My first instinct is just not to hardcode stuff, but ok...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11052025/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11060591", "body": "It's just a warning, so I think (until we've had extensive testing on 32bit) leaving it is warranted.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11060591/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11060853", "body": "That's just horrible! I much prefer a single \"Exporting ZFS pool \u2026\" with progress entry for each pool being exported.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11060853/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11060866", "body": "This was intended for people NOT running a root pool and wanted to do \"something\" (upgrade or whatever) and wanted to make sure that everything was unshared, unmounted, exported and unloaded\u2026. It wasn't JUST intended to be run at system shutdown/reboot...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11060866/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11061001", "body": "Actually, I've extended <code>zfs_installed()</code> to check if the command is executable:\n\n```\nzfs_installed()\n{\n        local RET\n\n        if [ ! -x $ZPOOL ]; then\n            return 1\n        else\n            # Test if it works (will catch missing/broken libs etc)                                                                               \n            \"$ZPOOL\" -? > /dev/null 2>&1\n            return $?\n        fi\n\n        if [ ! -x $ZFS ]; then\n            return 2\n        else\n            # Test if it works (will catch missing/broken libs etc)                                                                               \n            \"$ZFS\" -? > /dev/null 2>&1\n            return $?\n        fi\n\n        return 0\n}\n```\n\nThat way we can catch problems in the installation. And if we don't log this clearly, user won't know why it didn't import anything\u2026\n\nAnd I'm only doing this ONCE now - in <code>zfs-import</code> (where it matters and which the other depend on the success of).\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11061001/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11061125", "body": "I don't know. But as you said, by splitting the import and the mount into two, where the import is/can be run VERY early, this shouldn't be needed. Mind if I keep the code, but comment out the whole block (so it doesn't get lost in the rebase) and we can test it out (and I'll try to dig up the issues - there where one or two at least - and I'll see if I can convince the issuer to test these new scripts - with this commented out)?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11061125/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11061564", "body": "Wheezy doesn't have the <code>-r</code> option. And that isn't even \"ancient\" (just old :).\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11061564/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11065177", "body": "Starting to remember a little about this. I don't think it matters that there's now a zfs-import script. If we have legacy filesystem and/or volumes:\n\n```\nrpool/test/test9                /mnt/test9      zfs     defaults,noauto 0 0\n/dev/zvol/test/test/zvol0       /mnt/test0      ext4    defaults,noauto 0 0\n/dev/zvol/test/test/zvol1       /mnt/test1      ext4    defaults,noauto 0 0\n```\n\nThe mount procedure to mount filesystems mentioned in fstab is run to early for even zfs-import to be 'available'.\n\nSo we need the <code>noauto</code> and no fsck value for this to work. I've reworked part of the PR to actually work (it was a while since I checked this).\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11065177/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11071422", "body": "No, I think it's ugly for both. I could always just output a dot for each pool imported/exported. And no 'using [cache|/dev/...]' etc.\n\nThat information isn't really _required_. I think all of that could, potentially be important (at one point or another), but maybe only if interactive (as in, not started through init or whatever - calling zfs-{import,mount} manually from the shell). But then, that doesn't help those that have their system on ZFS/ZoL..\n\nI honestly don't know. I just know I don't like one line per pool importted/exported. That is really ugly.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11071422/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11071463", "body": "I've rewritten this a little in the new push. Now this is only run if we're not exporting a root pool. And all modules, in the right order, is called.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11071463/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11071745", "body": "This is the startup scripts on a basic Wheezy system:\n\n```\n/etc/rcS.d/S01hostname.sh\n/etc/rcS.d/S01mountkernfs.sh\n/etc/rcS.d/S02udev\n/etc/rcS.d/S03keyboard-setup\n/etc/rcS.d/S04mountdevsubfs.sh\n/etc/rcS.d/S05hwclock.sh\n/etc/rcS.d/S06checkroot.sh\n/etc/rcS.d/S07checkroot-bootclean.sh\n/etc/rcS.d/S07kmod\n/etc/rcS.d/S07mtab.sh\n/etc/rcS.d/S08checkfs.sh\n/etc/rcS.d/S09mountall.sh                          <- this is where fstab mounts are mounted\n/etc/rcS.d/S10mountall-bootclean.sh\n/etc/rcS.d/S11procps\n/etc/rcS.d/S11udev-mtab\n/etc/rcS.d/S11urandom\n/etc/rcS.d/S11zfs-zed\n/etc/rcS.d/S12networking\n/etc/rcS.d/S12zfs-import                           <- and here we import the pool(s)\n/etc/rcS.d/S13rpcbind\n/etc/rcS.d/S14nfs-common\n/etc/rcS.d/S15mountnfs.sh\n/etc/rcS.d/S16mountnfs-bootclean.sh\n/etc/rcS.d/S17kbd\n/etc/rcS.d/S18console-setup\n/etc/rcS.d/S19bootmisc.sh\n/etc/rcS.d/S19plymouth-log\n/etc/rcS.d/S19x11-common\n```\n\nThis is just the <code>S</code> runlevel. After this, the multiuser runlevel scripts are run:\n\n```\n/etc/rc2.d/S01motd\n/etc/rc2.d/S01zfs-mount                            <- this is where we mount legacy filesystems and volumes (now).\n/etc/rc2.d/S13rpcbind\n/etc/rc2.d/S14nfs-common\n/etc/rc2.d/S16rsyslog\n/etc/rc2.d/S16virtualbox-guest-utils\n/etc/rc2.d/S16zfs-share\n/etc/rc2.d/S17acpid\n/etc/rc2.d/S17atd\n/etc/rc2.d/S17cron\n/etc/rc2.d/S17exim4\n/etc/rc2.d/S17ssh\n/etc/rc2.d/S19bootlogs\n/etc/rc2.d/S20plymouth\n/etc/rc2.d/S20rc.local\n/etc/rc2.d/S20rmnologin\n```\n\nIt depends on where <code>zfs-import</code> and <code>zfs-mount</code> is run on CentOS6. The purpose of the new script was so that we can run it as early as possible. I'm not even running as early as I can, I just noticed.\n\nIt should be possible to run it between <code>S07checkroot-bootclean.sh</code> (which cleans up <code>/var/lock</code> which we need 'intact'/'unchanged' after <code>zfs-import</code> and <code>S08checkfs.sh</code> (which checks all filesystems in <code>/etc/fstab</code> - because I doesn't now, I need the fsck column to be <code>0</code>).\n\nSo if I can get it to run at the right level, maybe this ISN'T necessary after all. I'll do some tests (it's difficult to hit the target, because Wheezy isn't actually using those links - it uses <code>startpar</code> to run things in parallel and getting the dependencies right have been a pain).\n\nHow does it look like on CentOS? Do you have any command line examples to show how to create them correctly (in the right 'place'/'level')?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11071745/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11071760", "body": "Ok, I can live with that compromise.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11071760/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11072293", "body": "Yes. Tweaking the LSB header made it work as intended (without that extra code). I see that you're reviewing again, so I'll push that part later.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11072293/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11072732", "body": "But is that in the single user runlevel or the multiuser? Because I haven't tweaked the <code>chkconfig</code> header at all.\n\nFor <code>zfs-import</code> it just say <code>chkconfig:    2345 01 99</code>, which should be start at level 1 in runlevel 2 to five. Which isn't single user. Does CentOS even HAVE that (it should, but\u2026)?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11072732/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11073138", "body": "Have you \"unchkconfig\" and \"chkconfig\" the scripts after each pull test? I've been constantly trying to tweak it and I now think I got it right...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11073138/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11223881", "body": "I thought that's what you meant. I'm not particularly fond of that way myself, but either seems to be valid. I'll change it back to without leading parenthesis in all the scripts.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11223881/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11280851", "body": "It used to be, but I removed it because for some reason it didn't work. The pid file was removed.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11280851/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/355007", "body": "I have no idea :). I've used smb.c as base, and it's not implemented there either...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/355007/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/355011", "body": "The smb.c I used as reference did stuff here, but my version seems to work in all cases anyway. I might not have tested each and every use-case...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/355011/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/625176", "body": "I tripplechecked that, and it was one tab, no spaces. So it must be Github... I just can't understand why it does that. The indentation above is spaces though. Emacs shell script mode sets four spaces for first indentation level and one tab for the second one (and 1 tab + 4 spaces for the third).\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/625176/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637660", "body": "According to the specs of how to create a IQN, it's supposed to reflect the domain you're running on AND the date the target was created. Not a generic one (such as 'com.sun'), nor is it a reflection on 'today'.\n\nBut you're (partly) right about the date. In the sence that it needs to be static. I'm working on a patch to instead set this (the whole IQN) in 'shareiscsi', much like the 'sharenfs' property does it. This because once every month, my VBox can no longer find it's targets (because the've been \"renamed\" - i.e. reshared). So that is definitively a bug that will have to be adressed.\n\nEither that, or I need to figure out a way to 'store' the first IQN created, and reuse it every time I do a share on the ZVOL...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637660/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637664", "body": "According to specs, this should be the domain name in reverse. But just for  the sake of argument, could you double check what /proc/sys/kernel/domainname say? Mine didn't work either, before I set this. It's documented in the code/manpage.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637664/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637700", "body": "Could you just check and set this? With sysctl or similar (echo might work to) and try the code again. I'll see what I can do about adding a check later.\n\nSolaris isn't dealing with this at al. It have apparently hardcoded this to 'com.sun'...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/637700/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/639191", "body": "Could we take a step back here, and figure out what the actuall problem is?\n\nWe all seem to agree that 'uniqueness' is paramount (?).\n\nBut what you seem to suggest is to use a static IQN (com.sun - or better - org.zfsonlinux) with a 'random number of digits'. I fail to see why this is better than my solution.\n\nImagine hundreds, maybe thousands of targets, all with a large number of digits. There's no possibility to figure out or distinguish between them. Can't remember if it's possible to 'backtrace' the iqn to the ZVOL, but even if so, it will take some administration overhead.\n\nAs an administrator, what would you prefer? A nice readable list where the share name is in (almost) clear text, or a list of random digits?\n\nI.e.:\n\n  iqn.2001-04.org.zfsonlinux:01:13104831232857787788:43214\n\nor\n\n  iqn.2012-01.com.bayour:share.test1\n\nNeither of these are 'forbidden', the're both part of/allowed by the standard. Granted, I have interpreted the \"date that the naming authority took ownership of the domain\" to mean 'date when the target was created/shared\". But this seem to be a valid interpretation, at least if one looks through the big, bad 'Net :). Besides, the IQN is supposed to be the domain you're running on, not a generic one (com.sun or org.zfsonlinux). Or at least that's how i interpret \"reversed domain name of the authority\", although your interpretation could stand as well I guess...\n\nAnd about the 'domain names can change ownership'. If that happens, a lot more changes have to be made on the system/site/network besides the/a iSCSI target name.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/639191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/643807", "body": "Any idea on how I can figure out the type (FS or VOL) here? That code there obviously didn't work, because impl_share = NULL when I need(ed) it.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/643807/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/643899", "body": "The simplest, just to test and to get a proof-of-concept to work with, was just to add one target (with LUN=0) for each target. And each target is one ZVOL. I was also unsure on why/where to use LUNs at the time, but your (patdk-wk?) comment seems resonable. But considering that we can create a ZVOL with a colon, we could use that as the LUN.\n\nI.e.:\n\n  zfs create -V 15G share/tests/iscsi2:0\n  zfs create -V 15G share/tests/iscsi2:1\n  [etc]\n\nand then use that as the LUN.\n\nOR, I set the LUN to the same value as the TID instead of hardcoding it to '0' (the LUN that is). Might be simpler...\n\nSo techically, it's possible to do both (10/3 or 30/1). Actuall solution can be done in a number of ways. Even at the same time!\n\nAnd you're right about loosing your targets if your restart ietd. A reshare would fix that, but... Do we really, really want/need to modify the ietd.conf?! Since I'm running a custom script after the target have been 'share'd, one could always document this behaviour and recomend adding that code to the script... ?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/643899/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/644347", "body": "Ok, so with some minor modifications, this is what it looks like now:\n\ndebianzfs:~# cat /proc/net/iet/volume \ntid:1 name:iqn.2012-04.com.bayour:share.tests.iscsi1\n        lun:1 state:0 iotype:fileio iomode:wt blocks:31457280 blocksize:512 path:/dev/zvol/share/tests/iscsi1\n        lun:2 state:0 iotype:fileio iomode:wt blocks:31457280 blocksize:512 path:/dev/zvol/share/tests/iscsi2\ndebianzfs:~# iscsiadm -m discovery -t st -p 192.168.69.13:3260\n192.168.69.13:3260,1 iqn.2012-04.com.bayour:share.tests.iscsi1\n\nI'm not sure if this is expected behaviour though...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/644347/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/644473", "body": "Thinking more closly on this, it should probably be something:\n\nzfs create share/tests/hostA\nzfs create share/tests/hostA/device_1a\nzfs create share/tests/hostA/device_1b\nzfs set shareiscsi=on share/tests/hostA       (this will be the target name - the 'container' for the lun's)\nzfs set shareiscsi=on share/tests/hostA/device_1a\nzfs set shareiscsi=on share/tests/hostA/device_1b\n\nThis _should_ give something like:\n\ndebianzfs:~# cat /proc/net/iet/volume\ntid:1 name:iqn.2012-04.com.bayour:share.tests.hostA\n        lun:1 state:0 iotype:fileio iomode:wt blocks:31457280 blocksize:512 path:/dev/zvol/share/tests/hostA/device_1a\n        lun:2 state:0 iotype:fileio iomode:wt blocks:31457280 blocksize:512 path:/dev/zvol/share/tests/hostA/device_1b\ndebianzfs:~# iscsiadm -m discovery -t st -p 192.168.69.13:3260\n192.168.69.13:3260,1 iqn.2012-04.com.bayour:share.tests.hostA\n\nHow does this look? But that will require quite a lot of work. Currently you can't set 'shareiscsi' on a FS (which 'share/tests/hostA' would become).\n\nPlease tell me again, in much simpler words, why having one TID per target is bad... All i saw was 'usually'. But looking through the 'Net, I see only 'my' version, not one LUN per target... What would you like to accomplish?\n\nTalking this over at work, we're not doing the 'LUN thing' either. BUT, we did agree that if you have _a lot_ of machines, with _a lot_ of devices to each, it would be very ... 'pretty' to do it that way. But how do Solaris do it?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/644473/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/649102", "body": "Newline... ?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/649102/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/649123", "body": "It's to create packages which are 'tagged' with the current running kernel, so that you can have the same version built, but for different kernels and they don't overwrite eachother.\n\nI told brian to ignore this patch, but take what he want/need.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/649123/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/649128", "body": "I had to add that in rc8 to be able to compile but I don't know why everyone else seem to be able to compile without this...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/649128/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30403776", "body": "> Sharing by default is a change in the default behavior.\n> \n> Yes, because it didn't make any sense to have it set to 'no'. If any share\\* property is set, then the user/admin obviously want them used...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30403776/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30407835", "body": "> when backing up datasets to another server. I want to keep those share properties on the dataset (for easy recovery), but I don't want the backup server to be sharing those shares.\n\nFair enough, that's a valid point.\n\n> Whether or not sharing should be on by default is it's own discussion.\n\nHowever, this is not. \"By default\" means just that. Any local opinion you have besides that IS your decision. But \"by default\" (as in \"useful/correct for the large majority\"), it should be enabled.\n\nNow, I get (and agree with you) that for YOU (and probably quite a number of users) automatic sharing should be off. But so should probably mounting\ufffd I need to think of the large majority, and that is those that do NOT send their data to another system and therefor needs/wants automatic mounting and sharing.\n\nNow, since this is a local configuration file which is NOT automatically updated when/if it already exists, you should be safe from upgrades (once you changed it ONCE).\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30407835/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30412895", "body": "> They should tell the sysadmin (via NEWS.Debian for example) that this behavior has changed.\n> \n> Well, if we talk about the debs I provide, that was changed a very, very long time ago. I can't even find when it was changed (because I deleted the original file I used for this file and replaced it with the one from the PR). But if I had to guess, sometimes in the 0.6.2 series.\n\nAnd again, if a user have the previous default in their config, this still won't be changed. The config file is NOT updated (unless specifically told to).\n\nSo this new default is ONLY for new installs (where I still consider that doing automatic sharing is valid - for the majority of users, but granted, not for you and other users). But for the majority, it is by default [to do the automatic sharing].\n\nAnd I do not consider this a change that I need to tell users about. It is in their own interest to look over any config file for the service and change it appropriately for ANY new package they install and use.\n\n> I never stated that my situation should be applied to all users.\n> \n> You said:\n> Sharing by default is a change in the default behavior. If you're going to do this, please make sure it's included in the news of any packages that are built.\n> \n> \u2026 and I have tried to explain that I really haven't _changed_ anything (for existing users).\n> \n> It makes it more difficult to have a productive conversation with you.\n> \n> If you can't handle the fact that others have different opinion on this than you and you try to _force_ your opinion, maybe you shouldn't comment? You have iterated the exact same arguments the whole time, without supplying me with others when I have refuted yours.\n\nI do not agree with you, I've said that from the start.\n\nBut since this isn't really relevant for this PR, but for the debs I provide, this discussion should be moved to the pkg-zfs repository tracker.\n\nFor this PR, which is \"new\" (for everyone else), I consider that doing the automatic sharing (and mounting) is a reasonable default for the majority (and being \"new\", there's no need to document this \"change\" - because there IS no change!).\n\nFor those that disagree [that this should be done on their system], it is a config file and SHOULD be modified for local specifications.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30412895/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30552492", "body": "I kind'a agree on this, but at the very most, we wait one second 'to much'. Since <code>sleep</code> on a task switching OS is quite 'friendly', I think this might be ok.\n\nAnd I never liked busy loops. It's to much \u2026 \"Windows\" over it all :). It basically locks the system/process to do only that and forking <code>echo</code> many, many times per second (?) is a very sloppy way to do it in my opinion.\n\nPlaying with fork bombs in the mid nineties taught me that it can be VERY \u2026 sloppy. Granted, with \"todays\" CPU power, \"who cares\". But @behlendorf have mentioned low-memory systems and I agree. Sooner or later, someone will be bitten by that [the fork bomb].\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30552492/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30556451", "body": "> It won't lock the system and it won't run very long.\n> \n> I did a very simple test:\n\n```\nwhile /bin/true; do echo \"x\"; done\n```\n\nran it for about two seconds. 622 executions\u2026 Imagine on a slow system with very little memory. I would not be surprised if it took four, five seconds.\n\nNot only is it [the os] very busy with loading the module and doing everything needed there, but ALSO running echo fifteen hundred times!\n\nSleep was intended for this exact purpose. And as I said, doing a busy loop will only gain you less than a seconds speed gain. I honestly do not think it's worth it.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30556451/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30694089", "body": "@ryao Assuming that it will ONLY be run at the boot procedure is wrong, It should be able to run at any time (with or without busybox etc). And if you checked my test case, it doesn't even care if the module is loaded or anything. It was just a [simple] fork bomb.\n\nAlso, although the scheduler is the same, the resources is not. Running <code>echo</code> fifteen hundred times WILL use unnecessary resources. I also think that \"The Gentoo\" developers (including you and everyone else that think using busy-loops is a good idea in shell scripts etc) is dead wrong. I think one [of many] reasons why Linus decided to do his own version of minix, was that it didn't have a proper scheduler. And keeping that occupied with running <code>echo</code> (or whatever) is just wrong! NASA couldn't use msdos for example for their satellites (etc) because it didn't have sleep (yes, really - a missing sleep! - it simply drained to much energy). So on system not powered by the wall socket, busy-loops is evil! Not that I can see that anyone would use ZoL/ZFS on such a system, but still :)\n\nThat said, I'm all for #3431, but could/should we assume that these script should ONLY be used with ZoL that have that feature? We probably could/should, but should we [assume that]?\n\nBesides, #3431 is using sleep!! Micro sleep, granted, but that's just sleep that can wait a shorter amount of time [than whole seconds], so that's not really a busy-loop anyway.\n\nBut if you think waiting a whole second each time (which I think is silly to argue about really), I can wait 0.1 second etch time if that makes you feel better about it\u2026\n\n```\n# time /bin/sleep 0.1\n\nreal    0m0.102s\nuser    0m0.000s\nsys     0m0.000s\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30694089/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30694650", "body": "@Alexqw From what I read about that last comment, everything and everyone did exactly as intended.\n\n<code>dpkg</code> saw that a file tagged as a config file was modified and notified you, the administrator, about this. YOU did what you where asked to do and checked the differences and rejected that [and/or more] part(s). As you should, because you didn't need or wanted that.\n\nThis is EXACTLY the reason why it's tagged as a config file - for your to decide if the [new] defaults is valid or not. _I_ consider this new default the best for the most amount of people. Some people will disagree, as they should. But since it's a config file, it is their _responsibility_ to modify it according to their needs and requirements and being a config file, it is protected from any changes I, the packager, do and provide for you.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30694650/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30700193", "body": "> @FranzUrbo I agree, with one exception: the package maintainer did not call special attention to a significant change in behavior\n\nIt doesn't have to. It's a config file\ufffd And there WAS no change to your system, dpkg stopped that from happening, as it was supposed to.\n\n> I humbly request that in the future, if such a change is made, that it be included in the NEWS.Debian file\n\nIf it's in a config file, no. If it's somewhere I'm _forcing_ this on the user/admin, I'll try.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30700193/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31169562", "body": "Ok, thanx. Is this ok? It looks like <code>before</code> can be stacked, but just to be sure.\n\n```\nindex aaf33fb..914d97c 100755\n--- a/etc/init.d/zfs-zed.in\n+++ b/etc/init.d/zfs-zed.in\n@@ -54,7 +54,7 @@ do_depend()\n        fi\n\n        # bootmisc will log to /var which may be a different zfs than root.\n-       before bootmisc logger\n+       before bootmisc logger zfs-import\n        use mtab\n        keyword -lxc -openvz -prefix -vserver\n }\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31169562/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31169707", "body": "Probably. But I have a PR for this, and even if Gentoo won't be using it (it would be nice if it could, at least a library file) so it would be nice if this is still here. It just seems \u2026 unnecessary to have a separate config file for that. I could add a note that \"This is not applicable on Gentoo\" as an interim?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31169707/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31170000", "body": "Or you could add that right away (the sleeps) in your existing script now, and we'll work something out in the initrd PR for Gentoo as well later\u2026 ?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31170000/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31170164", "body": "Thanx. Fixing.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31170164/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31170224", "body": "Ah, ok. Thanx.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31170224/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31170439", "body": "Ok, thanx.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31170439/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31171107", "body": "I've added a <code>Only applicable for Debian GNU/Linux {dkms,initramfs}.</code> instead. I don't know if any other dkms package supports this at the moment.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31171107/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31173816", "body": "I'll change it to <code>boot</code>. It makes more sense anyway.\n\nThe manual for <code>rc-update</code> mentioned <code>sysinit</code>, but not the binary it uses (much like Debian GNU/Linux <code>update-rc.d</code>, <code>rc-update</code> seems to be a wrapper script for a binary and the man page for that didn't mention <code>sysinit</code> and manually using it didn't work).\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31173816/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31380879", "body": "I think that sounds strange\u2026 Would \"That's because\" be better?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/31380879/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "kpande": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974962", "body": "why's there so much unrecognized or unused code in this init script? did you copy and paste things without considering what they were?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/10974962/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30551368", "body": "I am a fan of busy loop over `sleep` for that reason - sometimes sleep may not wait long enough or it will wait too long.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30551368/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "dun": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270625", "body": "These scripts _were_...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270625/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270627", "body": "_They_ have...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270627/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270630", "body": "..._an_ install...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270630/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270631", "body": "...to think 'portably'.\n\nor, ...to keep portability in mind.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270631/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270634", "body": "...(_probably_)...\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270634/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270635", "body": "..._are_ called differently\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270635/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270636", "body": "...way _too_ easy\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270636/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270637", "body": "Why is `zfs_daemon_start()`, `zfs_daemon_stop()`, `zfs_daemon_status()`, and `zfs_daemon_reload()` in `etc/init.d/common.init.in`?  Seems like they should be in the zed init script since they're not \"common\" to anything else.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270637/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270641", "body": "I think you also need `--pidfile \"$PIDFILE\"` in the second call to `start-stop-daemon`.\n\nThat's the idiom I see used in `/etc/init.d/skeleton:do_start()` on Debian 7.\n\nThe `start-stop-daemon(8)` manpage there states:\n\n> Note: unless --pidfile is specified, start-stop-daemon behaves  similar\n> to  killall(1).   start-stop-daemon will scan the process table looking\n> for any processes which match the process name,  uid,  and/or  gid  (if\n> specified). Any matching process will prevent --start from starting the\n> daemon.\n\nBut I haven't tested this.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270641/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270644", "body": "`\"$PIDFILE\"`, not `\"$ZED_PIDFILE\"`.\n\n`\"$DAEMON_BIN\"`, not `\"$ZED\"`.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270644/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270646", "body": "You should add `rm -f \"$PIDFILE\"` here before returning. The `$PIDFILE` won't be cleaned up upon receipt of a `SIGKILL`.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270646/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270648", "body": "`killproc -p \"$PIDFILE\" \"$DAEMON_NAME\"`\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270648/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270650", "body": "As above, `rm -f \"$PIDFILE\"` before returning.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270650/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270651", "body": "`status -p \"$PIDFILE\" \"$DAEMON_BIN\"`\n\n`\"$PIDFILE\"` should probably be passed in as an arg.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270651/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270653", "body": "`killproc -p \"$PIDFILE\" \"$DAEMON_NAME\" -HUP`\n\nWhy are you redirecting this cmd's stdout to `/dev/null` here?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270653/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270656", "body": "Is the assumption that `/sbin/udevtrigger` and `/sbin/udevsettle` will _always_ be installed together?\n\nOr should this be something like:\n\n```\nelse\n        [ -x /sbin/udevtrigger ] && /sbin/udevtrigger\n        [ -x /sbin/udevsettle ] && /sbin/udevsettle\nfi\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270656/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270657", "body": "In other parts of this file, you're double-quoting `$ZPOOL`, and here you're not. These should be `\"$ZPOOL\"` to be consistent.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270657/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270659", "body": "Why not use:\n\n```\nif [ -z \"$available_pools\" -a \"$USE_DISK_BY_ID\" = 'yes' ]\n```\n\nIf `USE_DISK_BY_ID` is set to `yes` then it must also be a non-zero length string.\n\nAlso, using `==` within a `[` (a.k.a. `test`) command is a bashism. The `=` operator should be used to test for string equality.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270659/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270660", "body": "`\"$ZPOOL\"`\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270660/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270661", "body": "Can pool names contain whitespace? If so, the above filtering won't work as expected.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270661/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270663", "body": "Assumes pool names can't contain whitespace.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270663/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270666", "body": "Bashism. Should be a single `=`. This is done throughout. I'm not going to comment on the others since you can find them with a global search for `==`.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270666/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270670", "body": "..._preferred_ way.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270670/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270673", "body": "..._let's_ be paranoid.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270673/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270674", "body": "The leading `(` should be dropped.\n\nShould this also accept `0`?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270674/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270676", "body": "The leading `(` should be dropped.\n\nShould this also accept `0`?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270676/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270677", "body": "The leading `(` should be dropped.\n\nShould this also accept `0`?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270677/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270678", "body": "The leading `(` should be dropped.\n\nShould this also accept `0`?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270678/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270680", "body": "..._need_ to be started _extremely_ early,\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270680/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270681", "body": "..._too_ late.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270683", "body": "OK, this handles the case of removing the pidfile if needed (which I mentioned in earlier comments above). But separating part of this function into a \"common\" `zfs_daemon_stop` (which is only called from here, yet exists in a separate file) makes this needlessly complex and error-prone.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270689", "body": "...somewhat _misleading_.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270689/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270691", "body": "mounted _verbosely_?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270691/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270693", "body": "...would make _sense_.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270693/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270694", "body": "on _available_ options.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270694/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270774", "body": "Sorry for all the notification spam.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270774/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/16926629", "body": "The semicolon for the \"cut -d\" arg needs to be quoted or escaped.  My personal preference here would be \"cut -d\\; -f2\", but either is fine.  Just be consistent with io-email.sh:53.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/16926629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/27068845", "body": "I think you meant `(labels != NULL)` here for use as a guard when assigning the count to `labels` in the subsequent line.  The memory already malloc()d for `label` must be non-NULL here.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/27068845/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/27069405", "body": "If the `labels` parameter was instead named something like `num_labels`, its use would be more clear and the confusion on line 927 would have been less likely.  Also, its use should be documented in the function comment above.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/27069405/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/27069549", "body": "Perhaps `ne_labels` should instead be something like `ne_num_labels` to indicate its purpose.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/27069549/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/27071078", "body": "The 3 consecutive `if-continue` blocks here at the end of the loop seem a bit awkward since they follow two `if-break` blocks.  Would it be more clear as an `if / else if / else if` without the `continue` since it's at the end of the loop block?  It could also be done as an `if (a || b || c)` since the if-blocks are all the same, but that might be less readable here.  Yeah, I guess I'm nitpicking.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/27071078/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30655881", "body": "Shouldn't a `libzfs_init()` failure here be fatal, especially since you've removed the `ASSERT`?\n\nNow if `libzfs_init()` fails, you return early from `import_pool()` . That function returns a void, so you can't return the error to the caller (`zhack_spa_open()`) which obliviously rumbles on.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30655881/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30656217", "body": "The name `libzfs_error_init` makes me think you're initializing some `libzfs_error` subsystem.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30656217/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30661597", "body": "`timeout` here is always 0.  I think you mean to swap `MIN` and `MAX`:\n\n```\ntimeout = MAX(MIN(timeout, (10 * 60)), 0); /* 0 <= N <= 600 */\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30661597/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30664025", "body": "What do you think of structuring the loop like this?\n\n``` C\nstart = gethrtime();\ndo {\n        fd = open(ZFS_DEV, O_RDWR);\n        if (fd >= 0) {\n                (void) close(fd);\n                return (0);\n        } else if (errno != ENOENT) {\n                return (errno);\n        } else if (NSEC2MSEC(gethrtime() - start) < busy_timeout) {\n                sched_yield();\n        } else {\n                usleep(10 * MILLISEC);\n        }\n} while (NSEC2MSEC(gethrtime() - start) < (timeout * MILLISEC));\n\nreturn (ENOENT);\n```\n\nThe `do-while` ensures the `open()` will be attempted at least once even if `timeout` is 0.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30664025/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "dswartz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270717", "body": "good lord.  are you being paid by the message???\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11270717/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "tom71-zz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11280269", "body": "The error has changed, after applying the patch. When editing the file on host A and making it shorter (deleting lines). I get \n\nls: cannot access filexx: File exists \n\non host B. When making the file bigger (adding lines) all is fine.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/comments/11280269/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "maxximino": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/600169", "body": "But in this context secpolicy_vnode_setids_setgids should not check if the current user can create a setgid of the requested gid? (checking vap->va_gid, which should be different for example when we are creating a file inside a setguid directory? See zpl_vap_init in zpl_inode.c)\nThe current user is always a groupmember of the current gid, so secpolicy_vnode_setids_setgids is always going to return 0.\nI've tried to exploit this,unsuccessfully. I _think_ that Linux VFS shields against this problem, as of now.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/600169/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/600172", "body": "According to comments in struct cred ( http://lxr.linux.no/#linux+v3.2/include/linux/cred.h#L131 )\nshouldn't we change every crgetuid with crgetfsuid, as we are always checking for permissions related to filesystem? (same applies for gid).\nI mean in the entire ZoL codebase (possibile exception: delegations) not only in this patch.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/600172/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/600519", "body": "I've solved the problem with commit maxximino/zfs@b2f001299194f7622f80e6377188386b7261e54e . Did you miss it (it was referenced in the text of previous pull request) or you've solved differently on purpose?\nIf you prefer to \"approximate\" with a oneliner, vap->va_gid looks more reasonable to me.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/600519/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/600533", "body": "If you want, I'll take care of this. May I suggest using \"effective UID\" instead of \"real UID\" for delegations?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/600533/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/623345", "body": "maxximino/zfs@f3ddf21d5caeaa2d4d79c888a2569eef011a9906\nmaxximino/spl@6e9e8460527328aa11695f7cddaf13cbbd3f2af3\nI still have to test them deeply, however those patches look reasonably safe. (On successful end of tests I will make another pull request.)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/623345/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25574084", "body": "I'll try to save a bit of the precious behlendorf's time by answering this question:\nLook at lines 34,75 and 742/743 of this file.\nMost of this file (between lines 75 and 742) gets compiled only if <code>__KERNEL__</code> is not defined, so: in userspace.\nThis is useful for ztest and userspace utilities.\nIn userspace we don't need FSTRANS mutex-es, so it's simply redefined to a normal mutex.\nInstead, in kernelspace, the definition that will be used by the compiler will be the one at https://github.com/behlendorf/spl/commit/b0d285706ca2ca5ae29791e2cf06a6a80274ae7d#diff-120f4508a55b150b43c2781b1ccbe485R36 , which gets included by sys/t_lock.h at line 38 of this file.\nIs this clear enough?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25574084/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "rosscameron": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/935689", "body": "zvol_id fails on hardened GCC profiles removing the \"@CFLAGS@\" variable allows for compiling zvol_id without stack protection.\n\nCode still needs to be cleaned up with Valgrind but I haven't had time to do this yet.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/935689/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "mrobbetts": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/1517158", "body": "Surely that's a typo, and meant to be 'not uncommon'. Which sounds fine to me - but perhaps it's more common to say this in British English.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/1517158/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/1517337", "body": "Controversial! I would suggest that something which is not uncommon is _not_ necessarily common, so I would choose to replace the phrase with something along the lines of '...because drives are sometimes known to misreport...'.\n\nIn any case, I'd consider the existing wording a [litote](http://en.wikipedia.org/wiki/Litotes) and thus correct usage :)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/1517337/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "b333z": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4275065", "body": "I this locking correct/required here?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/4275065/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "likewhoa": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/9025488", "body": "Not everyone is using /etc/zfs/zpool.cache so this should not be forced on the service.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/9025488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "setharnold": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/14439647", "body": "What happens if fork() fails?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/14439647/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/14439666", "body": "The final string will have \"triggerto\" in it because no space was left after \"trigger\" or before \"to\" -- probably the \"to\" can be removed.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/14439666/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "goulvenriou": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20586519", "body": "we 've now implemented whatever we need before and after zfs_for_each() for clean invocation of callback with null argument \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20586519/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587104", "body": "break : done !\n\nfor the p option : you can use -Jp and you have your output in byte \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587104/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587113", "body": "we 've implemented json  printer in the caller \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587113/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587120", "body": "done !\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587120/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587126", "body": "fixed\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587126/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587137", "body": "json_list_callback() implemented\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587137/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587143", "body": "fixed\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587143/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587148", "body": "fixed\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587148/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587151", "body": "fixed\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587151/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587156", "body": "style : fixed !\nzfs_prop_to_name() called !\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587156/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587160", "body": "done \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587160/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587165", "body": "done \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587165/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587168", "body": "done \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587168/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587172", "body": "done \n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/20587172/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "yarikoptic": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25550686", "body": "Dear @behlendorf , please educate me a bit.... how does defining preprocessor variable MUTEX_FSTRANS to the same value as MUTEX_DEFAULT changes anything is the functioning?  It will still compile to the same value as far as I see it, or MUTEX_DEFAULT is redefined somewhere (not visible here) to be a different value?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25550686/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25631184", "body": "Dear @maxximino and @behlendorf   THANK YOU for spelling it out for me -- very much appreciated!  I will try to get to ZFS patching later today/tomorrow (I guess I should just take this patch alone for clarity) and report back in a week or earlier if stall occurs again ;-)\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/25631184/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "kernelOfTruth": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/26507078", "body": "same DEBUG build error, originating from #2129 \n\n```\nerror: variably modified 'pad' at file scope\n```\n\nnot sure in what (best) way to fix it - so leaving it to author\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/26507078/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/26506102", "body": "Debug build fails with:\n\n```\nerror: variably modified 'pad' at file scope\n```\n\n```\n`In file included from ../../include/sys/dmu.h:49:0,\n                 from ../../include/sys/spa.h:564,\n                 from ../../lib/libzpool/util.c:31:\n../../include/sys/abd.h:59:8: error: variably modified 'pad' at file scope\n  CC     zfs_deleg.lo\nmake[5]: *** [util.lo] Error 1\nmake[5]: *** Waiting for unfinished jobs....\nIn file included from ../../include/sys/dmu.h:49:0,\n                 from ../../include/sys/spa.h:564,\n                 from ../../lib/libzpool/kernel.c:33:\n../../include/sys/abd.h:59:8: error: variably modified 'pad' at file scope\nmake[5]: *** [kernel.lo] Error 1\nIn file included from ../../include/sys/dmu.h:49:0,\n                 from ../../include/sys/dsl_deleg.h:29,\n                 from ../../module/zcommon/zfs_deleg.c:40:\n../../include/sys/abd.h:59:8: error: variably modified 'pad' at file scope\nmake[5]: *** [zfs_deleg.lo] Error 1\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/26506102/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/26845393", "body": "@tuxoko \nSince the buildbots are upset about every itty-bitty thing and your patchset, due to the \"pad\" change are tested more on #3189 , #3190 I'd wanted to share the results with you:\n\nThe following error seems to appear occasionally and/or not at all times:\n\nhttp://buildbot.zfsonlinux.org/builders/ubuntu-12.10-amd64-builder/builds/2883/steps/shell_4\nhttp://buildbot.zfsonlinux.org/builders/ubuntu-12.10-amd64-builder/builds/2883/steps/shell_4/logs/stdio\n\n```\n../../module/zfs/abd.c: In function 'abd_get_offset':\n../../module/zfs/abd.c:975:43: error: 'PAGE_SHIFT' undeclared (first use in this function)\n../../module/zfs/abd.c:975:43: note: each undeclared identifier is reported only once for each function it appears in\n../../module/zfs/abd.c: In function 'abd_alloc_scatter':\n../../module/zfs/abd.c:1027:38: error: 'PAGE_SHIFT' undeclared (first use in this function)\nmake[5]: *** [abd.lo] Error 1\n```\n\nnot sure how to tackle this - since:\n\nPAGE_SHIFT is defined at\ninclude/asm-generic/page.h\nand subsequently for each arch on an option-basis differently, but default seems to be 12\narch/*/include/asm/page.h\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/26845393/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/26845558", "body": "same like line 972, buildbot complains about PAGE_SHIFT being undeclared\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/26845558/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/26845766", "body": "@tuxoko thanks ! that makes the build-bots happy,\n\nI'm just wanting to make sure that potential issues that might crop up later or on some archs are tackled early-on - the build-bots appear to be a worthwhile help in this\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/26845766/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "trisk": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28466830", "body": "This is the only option name with an underscore.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28466830/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28466975", "body": "Could be simplified as:\n\n```\nzprop_source_t source = ZPROP_SRC_LOCAL;\n...\nif (nvlist_exists(opts, \"received\")) {\n    source = ZPROP_SRC_RECEIVED;\n```\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28466975/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28467093", "body": "Is this clear_received_props subject to a a race with another set_props?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28467093/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28467438", "body": "Any reason for the explicit umem_alloc() here? AFAIK this is the only call in libzfs that doesn't use a specific cache.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28467438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28467798", "body": "General cstyle nit:\nWhen using a non-boolean expression in a condition it's preferred to explicitly compare to 0.\n`if ((error = ...) != 0)`\n`if (nvlist_lookup_nvlist(...) != 0)`\netc.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28467798/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28726390", "body": "Gotcha. I was wondering about the use of umem_alloc with no umem_nofail_callback. zfs_alloc seems like it would be suitable here.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/28726390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "aqw": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30392586", "body": "Sharing by default is a change in the default behavior. If you're going to do this, please make sure it's included in the news of any packages that are built.\n\n---Alex\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30392586/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30404545", "body": "Not necessarily. For example, when backing up datasets to another server. I\nwant to keep those share properties on the dataset (for easy recovery), but\nI don't want the backup server to be sharing those shares.\n\nWhether or not sharing should be on by default is its own discussion. But\nmy main point is that this is a change in default behavior that has a big\nimpact (sharing data over the network). That's kinda a big deal in my book\nand not a change that should be made silently.\n\n---Alex\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30404545/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30410795", "body": "@FransUrbo Calm down. My main concern is a silent change in behavior that involves sharing data over the network. I'm advocating that this not be done silently, like the current situation in the zfs dailies that you're providing. They should tell the sysadmin (via NEWS.Debian for example) that this behavior has changed. That's just common courtesy.\n\nAnd as for \"Whether or not sharing should be on by default is it's own discussion.\", I meant exactly that. I don't care strongly on that topic, but it is truly it's own discussion. And the Debian and RHEL camps take very different views on this. IIRC, RHEL tends to not start sharing services/shares by default while Debian tends to.\n\nIn the future, please stop using straw man arguments. I never stated that my situation should be applied to all users. It makes it more difficult to have a productive conversation with you.\n\n---Alex\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30410795/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30419833", "body": "> Well, if we talk about the debs I provide, that was changed a very, very long time ago.\n\nOk. Maybe this is where we're misunderstanding each other. Because I just did an upgrade today on a system with wheezy-daily and I was prompted to resolve conflicts in /etc/default/zfs. I did the diffs, and saw these changes. I then noticed the lock file name was changed to the new lock file\nname used in this patch set. So I assumed that you were including this PR in your builds (as you have done in the past). If I am in error, please let me know.\n\n> If you can't handle the fact that others have different opinion on this than you and you try to _force_ your opinion, maybe you shouldn't comment?\n\nFrans. Seriously. I am not trying to force my opinion on anyone. And I am sorry that you continue to see it that way.\n\n> You have iterated the exact same arguments the whole time, without supplying me with others when I have refuted yours. I do not agree with you, I've said that from the start. But since this isn't really relevant for this PR, but for the debs I provide, this discussion should be moved to the pkg-zfs repository tracker.\n\nOk. Then please respond to the issue I have already opened there (zfsonlinux/pkg-zfs#153), asking for clarification about the wheezy-daily builds.\n\n---Alex\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30419833/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30698117", "body": "> @Alexqw https://github.com/Alexqw From what I read about that last\n> comment, everything and everyone did exactly as intended.\n> \n> @FransUrbo I agree, with one exception: the package maintainer did not call\n> special attention to a significant change in behavior (and sharing data on\n> the network is significant in my book).\n\nI humbly request that in the future, if such a change is made, that it be\nincluded in the NEWS.Debian file, so that way the sysadmin has two places\nto be notified about such a change.\n\n---Alex\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/30698117/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "taneli76": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38345996", "body": "The newline is superfluous.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38345996/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38348932", "body": "I think two concurrent processes may both be calling this and not take the branch but continue with the mounting, at least the following occasionally fails not here but the below `call_usermodehelper` will return 8192 on my system:\n`cd /tank/.zfs/snapshot ; for x in * ; do for i in $(seq 1 10) ; do ls ${x}\u00a0& done ; done` (assuming they are unmounted to begin with).\n\nI am not sure if it has any negative consequences (beside logspam) though.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38348932/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "iamjamestl": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38363096", "body": "See the end of https://github.com/zfsonlinux/zfs/issues/3513#issuecomment-114600082 and the beginning of https://github.com/zfsonlinux/zfs/issues/3513#issuecomment-114604681.  The apparent issue is that there is a dependency loop with `zfs-mount` depending on `localmount` which depends on `procfs` which depends on `localmount`.  I read the `procfs` service script and saw that it only really took care of mounting things like `binfmt_misc` and `usbfs`, neither of which are required for `zfs-mount` so I removed the dependency and that solved the dependency loop.  OpenRC itself seems to take care of mounting `/proc` (and thus making `/proc/mounts` available) before starting any other services.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38363096/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38363323", "body": "That is true, as long as `zfs-mount` runs in the boot runlevel and `zfs-zed` runs in the default runlevel.  I don't know of a way to enforce that, though, so it seems better to me to make the intention that `zfs-zed` depends on `zfs-mount` more explicitly clear.  I'm happy to defer to your expertise in this case.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38363323/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38495324", "body": "That's sort of the whole point of this change.  zfs-mount mounts `/var` when `/var` is a separate dataset.  ZED writes `/var/lib/run/zed.state` so `/var` needs to be mounted first.\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38495324/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38495725", "body": "Thinking about this some more, I realize it makes more sense in the general case to have `zfs-zed` depend on `localmount` instead, which in turn depends on `zfs-mount` when `/` is ZFS, and supports the case when `/` is not ZFS and `/var` is a separate ext4 filesystem, as another example.  Do you agree?\n", "reactions": {"url": "https://api.github.com/repos/zfsonlinux/zfs/pulls/comments/38495725/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}}}}