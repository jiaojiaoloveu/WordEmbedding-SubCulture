{"_default": {"1": {"Trass3r": {"issues": [{"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/issues/2", "title": "remove boost dependency", "body": "would be nice not having to pull in that monster\n", "reactions": {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/issues/2/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/comments/18863992", "body": "putting the enable_if into the template part would give a clean return type\n`template <typename InputIterator, typename OutputIterator, typename = std::enable_if_t<....>>`\n", "reactions": {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/comments/18863992/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "fhoefling": {"issues": [], "commits": [{"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/1849afdc613e7184575bdea995e99769ed519e1a", "message": "update AUTHORS file"}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/5a396c33b53eb506df878cc2098d40b3e1d99181", "message": "release cuda_wrapper under the terms of the 3-clause BSD license\n\nAll contributors have agreed to this change of the license, see\nhttps://github.com/halmd-org/cuda-wrapper/issues/1"}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/209c5711de3446a7413f416c1aad117c3bb03f7f", "message": "re-implementation of cuda::allocator\n\nThe re-implementation has become necessary to avoid issues with the\nupcoming change of license. The template for how to write a custom STL\nallocator is textbook knowledge."}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/8f4ed65f9c471925e89a375e7a581cc66e442dc3", "message": "Revert \"Merge branch 'testing'\"\n\nThe branch name 'testing' was misleading. The merge was premature and\nbreaks the use of cuda_wrapper in HALMD, at least on devices of compute\ncapability less than 2.0 (pre-Fermi). (Yet, commit facd758 suggests that\nthere is a work-around for such devices.)\n\nNevertheless, mapped page-locked memory seems to be an interesting\nperformance-relevant feature, which deserves further investigations.\n\nThis reverts commit fa47d2c30694fee7612a80c352c8f706a303d129, reversing\nchanges made to c9263c89b32ee61e5a388ff93eeb7cc239c76ea1.\n\nTo include the reverted commits downstream, revert this commit before\nmerging:\nhttps://www.kernel.org/pub/software/scm/git/docs/howto/revert-a-faulty-merge.txt"}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/fa47d2c30694fee7612a80c352c8f706a303d129", "message": "Merge branch 'testing'\n\n* testing:\n  host::vector test: purge mapped_host_memory_pure\n  host::vector: fix allocation flags\n  test mapping of host memory by direct CUDA calls\n  add test for device-mapping of page-locked host memory"}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/320bf3dd1d98ba6c29cee9ccf33318ca3a2a7892", "message": "disable Clang \u2264 3.3 warning: unneeded internal declaration\n\nThis is a follow-up to commit:6914b42."}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/4b59e2907e791dff375491328071f8a9e26d94e6", "message": "host::vector test: purge mapped_host_memory_pure\n\nThe test case was useful to point out a problem in the allocation of\nhost::vector and is not needed anymore."}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/262c96918597cf4c7934bef558cc4dc6876caa29", "message": "host::vector: fix allocation flags\n\ncudaHostMalloc() needs to be called with the flag cudaHostAllocMapped,\notherwise calling cudaHostGetDevicePointer() fails.\n\nWe should check whether this flag impacts the performance and whether there are\nuse cases of host::vector where the default mode is preferred."}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/facd7586eefe99060f1da3c246c1bf2509b225e0", "message": "test mapping of host memory by direct CUDA calls\n\nThe device mapping of host memory works also with the Tesla C1060 if we make\nuse of direct calls to the CUDA library. This points at a problem in\ncuda_wrapper."}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/83a362bbfa5f6982a5e28c78b59ade0e758045f6", "message": "add test for device-mapping of page-locked host memory"}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/088a11cd1182bdf0d96dc38c2a984f001f3c74be", "message": "texture: replace private reference by pointer\n\nA default initialisation of a reference is not possible without\npotentially dangling objects. (Clang \u2265 3.3 and GCC \u2265 4.8 complain about\nthis.) The use of pointers is semantically equivalent and safer."}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/6914b423e1011e3f470e33f5605ad88628649819", "message": "cuda_wrapper: disable warning for CUDA 5.5 headers\n\nClang \u2265 3.3 emits a warning for CUDA 5.5 headers:\n\n    /usr/local/cuda-5.5/include/cuda_runtime.h:225:33: warning: function\n    'cudaMallocHost' is not needed and will not be emitted\n    [-Wunneeded-internal-declaration]"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/issues/comments/228983047", "body": "For completeness:\n\nI agree that my contributions to the cuda-wrapper project until this point of writing are distributed under the terms of the 3-clause BSD license.\n\nFelix\n", "reactions": {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/issues/comments/228983047/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/issues/comments/244126006", "body": "We're using the wrapper mostly in HALMD, and there Boost is needed anyway. Sure, it would be nice to get rid of it. At the same time, it would be nice to switch to C++11, which would simplify some things. But I don't have time to care about such stuff.\n\nOf course, your are welcome to contribute improvements to the package.\n", "reactions": {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/issues/comments/244126006/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "commit_comments": [], "review_comments": []}, "ekpyron": {"issues": [], "commits": [{"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/e727c8684215823fe3d0f5e7bfbf267000d2d724", "message": "Merge branch 'master' into staging"}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/00189b8bcbcc4fdc293f594ee198a27efd73e102", "message": "device: add max_threads_per_multi_processor() member\n\nAdds a new member function that can be used to query the\nmaximum number of resident threads per multiprocessor for\ncuda runtime >= 4.0"}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/b31c4c9ef957913f38fb050b0b8f122b93d467cd", "message": "Fix warnings for GCC 7 about minor and major macro definitions.\n\nFixes the following warning when using GCC 7:\n\n Warning: In the GNU C Library, \"minor\" is defined\n by <sys/sysmacros.h>. For historical compatibility, it is\n currently defined by <sys/types.h> as well, but we plan to\n remove this soon. To use \"minor\", include <sys/sysmacros.h>\n directly. If you did not intend to use a system-defined macro\n \"minor\", you should undefine it after including <sys/types.h>.\n         unsigned int minor() const\n\nThis commit undefines the macros major and minor in device.hpp\nif they are defined."}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/cf78b3c04f4fa16656c85380b9a7572ffb920814", "message": "function: optional variable shared memory function in cuda::function\n\nThe constructor of cuda::function now optionally takes a unary\nfunction as second argument that is used to calculate the shared\nmemory requirement of the kernel for a given block size.\n\nThis function is used to calculate the maximal block size for\noptimal device occupancy."}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/aeae5d59bb8341878ebbba46c8807dcfc4567eed", "message": "function: query max block size and min grid size for optimal occupancy\n\nUpon construction of a cuda::function wrapper the maximal block size\nand minimal grid size for optimal occupancy are queried and stored.\n\nThis is only supported for cuda >= 6.5; for older cuda versions both\nvalues default to 0."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "the-nic": {"issues": [], "commits": [{"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/c9263c89b32ee61e5a388ff93eeb7cc239c76ea1", "message": "make random_access_iterator c++03 compatible"}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/e43f4cf6514ed1e8d277578d552788028bca08a7", "message": "compatibility with \u2265 Boost 1.57.0\n\nSince Boost 1.57.0, iterator operators are conditionally defined based\non the iterator category. This is tested by boost::is_convertible of\niterator_category of the random_access_iterator defined in\ncuda_wrapper/detail/random_access_iterator.hpp.\nAs cuda::device_random_access_iterator_tag does not derive from\nany iterator tag, the device iterator is not detected as random\naccess and therefore all random access operators (e.g. operator-) are\nhidden.\n\nFix this by adding a manual specialization of boost::is_convertible\nfrom cuda::device_random_access_iterator_tag to\nstd::random_access_iterator_tag.\n\nThis causes a inconsistency between std::is_convertible and the\nboost::is_convertible, given that they now return different values.\n\nAlternatively, one would have to write an own iterator_adaptor."}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/a43a2c9309b1f0e757d9bd6a59f5d9ceb6afade6", "message": "Fix FTBS for nvcc\n\nIntroduced in fa89a2a087b64 and was unnoticed during testing"}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/fa89a2a087b648bfceab0ef9515ec194bf2cc713", "message": "texture.hpp: CUDA 6.0 compatibility\n\nOtherwise CUDA \u2265 6.0 will fail with \"invalid channel descriptor\"\n\nCloses #394938"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/issues/comments/223633224", "body": "I agree that my contributions to the cuda-wrapper project until this point of writing are distributed under the terms of the 3-clause BSD license.\n\nNicolas\n", "reactions": {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/issues/comments/223633224/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "commit_comments": [], "review_comments": []}, "petercolberg": {"issues": [], "commits": [{"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/e7fc47e73b2a0732dd2a8043f5c40f1a2fd74e71", "message": "Remove __DEVICE_EMULATION__ guard\n\nCUDA device emulation was removed in CUDA 3.1"}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/1f03ebebbf2bcd0a30c1ceae543bd5bed549f191", "message": "Support events with flags in CUDA \u2265 2.2"}, {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/commits/b3a66044f358a6da14f4354f55962fd5d7edd69f", "message": "Add bogus cuda::texture constructor to avoid GCC 4.4 compiler warning\n\n    [ 51%] Building CXX object src/ljgpu/CMakeFiles/ljgpu_gpu_neighbour.dir/mdlib.cpp.o\n    In file included from /home/peter/thesis/mdsim/ljgpu/include/cuda_wrapper.hpp:60,\n\t\t     from /home/peter/thesis/mdsim/ljgpu/src/ljgpu/mdsim/sample.hpp:26,\n\t\t     from /home/peter/thesis/mdsim/ljgpu/src/ljgpu/mdsim/traits.hpp:30,\n\t\t     from /home/peter/thesis/mdsim/ljgpu/src/ljgpu/mdsim/exception.hpp:25,\n\t\t     from /home/peter/thesis/mdsim/ljgpu/src/ljgpu/mdsim/base.hpp:29,\n\t\t     from /home/peter/thesis/mdsim/ljgpu/src/ljgpu/mdsim/hardsphere.hpp:31,\n\t\t     from /home/peter/thesis/mdsim/ljgpu/src/ljgpu/mdlib.cpp:30:\n    /home/peter/thesis/mdsim/ljgpu/include/cuda_wrapper/texture.hpp: In instantiation of \u2018cuda::texture<float4>\u2019:\n    /home/peter/thesis/mdsim/ljgpu/src/ljgpu/mdsim/ljfluid_gpu_nbr.hpp:433:   instantiated from \u2018void ljgpu::ljfluid<ljgpu::ljfluid_impl_gpu_neighbour, dimension>::threads(unsigned int) [with int dimension = 3]\u2019\n    /home/peter/thesis/mdsim/ljgpu/src/ljgpu/mdsim/mdsim.hpp:148:   instantiated from \u2018void ljgpu::mdsim<mdsim_backend>::threads(const boost::true_type&) [with mdsim_backend = ljgpu::ljfluid<ljgpu::ljfluid_impl_gpu_neighbour, 3>]\u2019\n    /home/peter/thesis/mdsim/ljgpu/src/ljgpu/mdsim/mdsim.hpp:284:   instantiated from \u2018ljgpu::mdsim<mdsim_backend>::mdsim(const ljgpu::options&) [with mdsim_backend = ljgpu::ljfluid<ljgpu::ljfluid_impl_gpu_neighbour, 3>]\u2019\n    /home/peter/thesis/mdsim/ljgpu/src/ljgpu/mdlib.cpp:111:   instantiated from \u2018typename boost::enable_if<typename mdsim_backend::impl_type::impl_gpu, int>::type _mdsim(const ljgpu::options&) [with mdsim_backend = ljgpu::ljfluid<ljgpu::ljfluid_impl_gpu_neighbour, 3>]\u2019\n    /home/peter/thesis/mdsim/ljgpu/src/ljgpu/mdlib.cpp:186:   instantiated from here\n    /home/peter/thesis/mdsim/ljgpu/include/cuda_wrapper/texture.hpp:64: warning: non-static reference \u2018const textureReference& cuda::texture<float4>::tex\u2019 in class without a constructor\n    /home/peter/thesis/mdsim/ljgpu/include/cuda_wrapper/texture.hpp: In instantiation of \u2018cuda::texture<float2>\u2019:\n    /home/peter/thesis/mdsim/ljgpu/src/ljgpu/mdsim/ljfluid_gpu_nbr.hpp:434:   instantiated from \u2018void ljgpu::ljfluid<ljgpu::ljfluid_impl_gpu_neighbour, dimension>::threads(unsigned int) [with int dimension = 2]\u2019\n    /home/peter/thesis/mdsim/ljgpu/src/ljgpu/mdsim/mdsim.hpp:148:   instantiated from \u2018void ljgpu::mdsim<mdsim_backend>::threads(const boost::true_type&) [with mdsim_backend = ljgpu::ljfluid<ljgpu::ljfluid_impl_gpu_neighbour, 2>]\u2019\n    /home/peter/thesis/mdsim/ljgpu/src/ljgpu/mdsim/mdsim.hpp:284:   instantiated from \u2018ljgpu::mdsim<mdsim_backend>::mdsim(const ljgpu::options&) [with mdsim_backend = ljgpu::ljfluid<ljgpu::ljfluid_impl_gpu_neighbour, 2>]\u2019\n    /home/peter/thesis/mdsim/ljgpu/src/ljgpu/mdlib.cpp:111:   instantiated from \u2018typename boost::enable_if<typename mdsim_backend::impl_type::impl_gpu, int>::type _mdsim(const ljgpu::options&) [with mdsim_backend = ljgpu::ljfluid<ljgpu::ljfluid_impl_gpu_neighbour, 2>]\u2019\n    /home/peter/thesis/mdsim/ljgpu/src/ljgpu/mdlib.cpp:189:   instantiated from here\n    /home/peter/thesis/mdsim/ljgpu/include/cuda_wrapper/texture.hpp:64: warning: non-static reference \u2018const textureReference& cuda::texture<float2>::tex\u2019 in class without a constructor"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/issues/comments/223703701", "body": "I agree that my contributions to the cuda-wrapper project until this point of writing are distributed under the terms of the 3-clause BSD license.\n\nPeter\n", "reactions": {"url": "https://api.github.com/repos/halmd-org/cuda-wrapper/issues/comments/223703701/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}}}}