{"_default": {"1": {"iAnanich": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3086", "title": "Extend `load_object` to work with attributes ", "body": "I propose to add some special character to the `utils.misc.load_object` function's `path` argument, for example `:` to mark attribute(s) to be loaded too.\r\n\r\n#### Background\r\nIssue created as part of this [issue in `scrapy-splash` plugin](https://github.com/scrapy-plugins/scrapy-splash/issues/163) and inspired by [this PR](https://github.com/scrapy-plugins/scrapy-splash/pull/157). \r\n\r\nIn the `scrapy-splash` plugin it is `SPLASH_SLOT_POLICY` setting that takes values from `scrapy_splash.middleware.SlotPolicy` attributes (by default - `PER_DOMAIN`). So the user needs to import something (!) in his/her `settings.py` to define `SPLASH_SLOT_POLICY` value. And even using `load_object` inside plugin won't help because it can't access attributes of the class.\r\n\r\n## Specification\r\nFirst of all, this change will keep backward compatibility because we still use `.` (dots) for everything that we can import.\r\n\r\nThen it is some simple options:\r\n* allow only one attribute to load. In this case, `scrapy_splash.middleware.SlotPolicy:PER_DOMAIN` will be a solution.\r\n* allow multiple attributes to load. In this case path like `package.module.klass:constant:attr:var` is a bit overkill and looks bad.\r\n* allow multiple attributes to load, but use only one `:` to split \"importable\" objects from \"accessible\" (by `.` dot) objects. For example: `package.module.klass:constant.attr.var` - looks more pythonic.\r\n\r\nAny ideas on how to be?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3086/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hduyyg": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3083", "title": "Why use UserAgentMiddleware when DefaultHeadersMiddleware exists?", "body": "DefaultHeadersMiddleware can do all UserAgentMiddleware can do? So what's the meaning of UserAgentMiddleware?\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3083/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shanmuga-cv": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3077", "title": "scrapy selector fails when large lines are present response", "body": "Originally encoutered when scraping [Amazon restaurant](https://www.amazon.com/restaurants/zzzuszimbos0015gammaloc1name-new-york/d/B01HH7CS44?ref_=amzrst_pnr_cp_b_B01HH7CS44_438).  \r\nThis page contains multiple script tag with lines greater then 64,000 character in one line. \r\nThe selector (xpath and css) does not search beyond these lines. \r\n\r\nDue to this the following xpath `'//h1[contains(@class, \"hw-dp-restaurant-name\")]/text()'` to extract name of the restaurant returns empty even though there is a matching tag is present.\r\n\r\n\r\nPFA the response text at [original_response.html.txt.gz](https://github.com/scrapy/scrapy/files/1631425/original_response.html.txt.gz)\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3077/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "crisfan": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3075", "title": "Telnet Console ", "body": "hi, @kmike,I use telnetlib to pause scrapy engine, there is some error! although it pause the scrapy engine.\r\nthis is my poor code:\r\n``` javascript\r\nimport telnetlib\r\nhost = '127.0.0.1'\r\ntn = telnetlib.Telnet(host, 6023)\r\ntn.write(\"engine.pause()\\n\") \r\n``` \r\nerror:\r\n``` javascript\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\"'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\"'\r\n2018-01-13 16:19:26 [twisted] ERROR: Unhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x1f'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x1f'\r\n2018-01-13 16:19:26 [twisted] ERROR: Unhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x03'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x03'\r\n2018-01-13 16:19:26 [twisted] ERROR: Unhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x01'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x01'\r\n``` \r\n\r\ncould you fix my problem, thansk ~~~\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3075/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Caleb-Wade": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3074", "title": " Cxfreeze+python3.4+scrapy1.4 failed to bundle to executables(AttributeError: module object has no attribute '_fix_up_module')", "body": "After I failed to bundle .py document into .exe document by pyinstaller, I tried cxfreeze. Similar error happened. Someting went wrong when importing scrapy module, and AttributeError: module object has no attribute '_fix_up_module' appeared in the command window. Would someone tell me what happened. Can I make it? Thks!", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3074/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3073", "title": "Pyinstaller+Python3.4+Scrapy1.4 Failed to bundled exe (ImportError:No module named \"XXXX\")", "body": "I have tried to bundle my spider .py into .exe, but weeks pasted, I didn't make it. It pointed that some modules were missing. Even I used --hidden-import to put the missing modules into my bundled exe, it still didn't work. It's amazing that I can see the modules in the HTML document under build file. Pls help me ,Thks!", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lifei1245": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3069", "title": "about the signal retry_complete", "body": "I didn't find the singnal in the singnal list,how can I use it", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3069/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "3xp10it": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3068", "title": "scrapy always Starting new HTTP connection after crawl finished ", "body": "I reopen [#3066][1] here,there are more details [here][2].\r\n\r\n[1]: https://github.com/scrapy/scrapy/issues/3066\r\n[2]: https://stackoverflow.com/questions/48182204/scrapy-always-starting-new-http-connection-after-crawl", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3068/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "theduman": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3067", "title": "scrapy.Request doesnt wait for loop", "body": "I fetch url and other parameters from database and use them in for loop and pass to scrapy.request() function but when i run the code i succesfully pass first element only. Scrapy can't get other elements of list. Here is my code\r\n\r\n```py\r\nclass QuotesSpider(scrapy.Spider):\r\n    name = \"quotes\"\r\n    custom_settings = {\r\n        'CONCURRENT_REQUESTS': 1,\r\n    }\r\n    def start_requests(self):\r\n        sourceArr = []\r\n        connection = db.connect()\r\n        try:\r\n            with connection.cursor() as cursor:\r\n                # Read a single record\r\n                sql = \"SELECT `code`, `url`, `target`,`next` FROM `source`\"\r\n                cursor.execute(sql)\r\n                result = cursor.fetchall()\r\n        finally:\r\n            connection.close()\r\n        print(result)\r\n        for i in result:\r\n            source = Source(i['code'], i['url'], i['target'], i['next'])\r\n            sourceArr.append(source)\r\n        print(sourceArr)\r\n        #for url in urls:\r\n           #yield scrapy.Request(url=url, callback=self.parse)\r\n        for s in sourceArr:\r\n            print(s.target)\r\n            yield scrapy.Request(url=s.url, meta={'target': s.target, 'next': s.next})\r\n            print(\"slept\")\r\n\r\n\r\n    def parse(self, response):\r\n        titlearr = []\r\n        count = 0\r\n        print(response.meta)\r\n        for title in response.css(response.meta['target']):\r\n            count += 1\r\n            titlearr.append(title.css('p a::text').extract_first())\r\n            #yield {'title': title.css('p a::text').extract_first()}\r\n        print(\"total count \" + str(count))\r\n        print(titlearr)\r\n        for next_page in response.css(response.meta['next']):\r\n            yield response.follow(next_page, self.parse)\r\n```\r\n\r\nWhen i print response.meta i got target and next values for only first item. List contains more than 1 item. How can i make scrapy.Request() function wait for the next element to run?\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3067/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "benjolitz": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3065", "title": "`Connection to the other side was lost` for older French site", "body": "Hi,\r\n\r\nI'm attempting to run scrapy on `https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences`.\r\n\r\nDespite upgrading all my brew packages, scrapy installation via `pip install -U scrapy`, the website disconnects uncleanly. Specifying `-s DOWNLOADER_CLIENT_TLS_METHOD=TLSv1.0` makes no difference to the following run. Neither does a user-agent clear things up with `-s USER_AGENT=\"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36\"`\r\n\r\nI am uncertain as how to debug this further.\r\n\r\n`scrapy shell https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences`\r\n\r\n```\r\n2018-01-09 18:03:33 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\r\n2018-01-09 18:03:33 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.3.1, w3lib 1.18.0, Twisted 17.9.0, Python 3.6.2 (default, Jul 17 2017, 16:44:45) - [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017), cryptography 2.1.4, Platform Darwin-16.7.0-x86_64-i386-64bit\r\n2018-01-09 18:03:33 [scrapy.crawler] INFO: Overridden settings: {'DOWNLOADER_CLIENT_TLS_METHOD': 'TLSv1.0', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0}\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage']\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2018-01-09 18:03:33 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\r\n2018-01-09 18:03:33 [scrapy.core.engine] INFO: Spider opened\r\n2018-01-09 18:03:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n2018-01-09 18:03:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n2018-01-09 18:03:35 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\nTraceback (most recent call last):\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/bin/scrapy\", line 11, in <module>\r\n    sys.exit(execute())\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/cmdline.py\", line 150, in execute\r\n    _run_print_help(parser, _run_command, cmd, args, opts)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/cmdline.py\", line 90, in _run_print_help\r\n    func(*a, **kw)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/cmdline.py\", line 157, in _run_command\r\n    cmd.run(args, opts)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/commands/shell.py\", line 73, in run\r\n    shell.start(url=url, redirect=not opts.no_redirect)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/shell.py\", line 48, in start\r\n    self.fetch(url, spider, redirect=redirect)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/shell.py\", line 115, in fetch\r\n    reactor, self._schedule, request, spider)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\r\n    result.raiseException()\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/twisted/python/failure.py\", line 385, in raiseException\r\n    raise self.value.with_traceback(self.tb)\r\ntwisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n```\r\n\r\n\r\n`scrapy version -v`:\r\n```\r\nScrapy       : 1.5.0\r\nlxml         : 4.1.1.0\r\nlibxml2      : 2.9.7\r\ncssselect    : 1.0.3\r\nparsel       : 1.3.1\r\nw3lib        : 1.18.0\r\nTwisted      : 17.9.0\r\nPython       : 3.6.2 (default, Jul 17 2017, 16:44:45) - [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)]\r\npyOpenSSL    : 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017)\r\ncryptography : 2.1.4\r\nPlatform     : Darwin-16.7.0-x86_64-i386-64bit\r\n```\r\n\r\n`curl -sLv https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences 2>&1 | head -20`:\r\n```\r\n*   Trying 160.92.134.3...\r\n* TCP_NODELAY set\r\n* Connected to agences.creditfoncier.fr (160.92.134.3) port 443 (#0)\r\n* TLS 1.0 connection using TLS_RSA_WITH_3DES_EDE_CBC_SHA\r\n* Server certificate: agences.creditfoncier.fr\r\n* Server certificate: RapidSSL RSA CA 2018\r\n* Server certificate: DigiCert Global Root CA\r\n> GET /credit-immobilier/toutes-nos-agences HTTP/1.1\r\n> Host: agences.creditfoncier.fr\r\n> User-Agent: curl/7.54.0\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 200 OK\r\n< Date: Wed, 10 Jan 2018 02:06:54 GMT\r\n< Server: Microsoft-IIS/6.0\r\n< X-Powered-By: ASP.NET\r\n< Content-Length: 35884\r\n< Content-Type: text/html\r\n< Set-Cookie: ASPSESSIONIDQQBACDTS=JHOJNNCCJNIFABOPLNCEEAFH; path=/\r\n< Cache-control: private\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3065/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "reaCodes": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3064", "title": "I ran into a problem when I used ipython in a scrapy shell", "body": "I used this command, `scrapy shell 'www.baidu.com'`, and then used `response.body`. \r\n\r\nWhen I input `resp` and click `Tab`\r\n\r\n![image](https://user-images.githubusercontent.com/13817254/34660511-e159fe1a-f47d-11e7-8499-b6589ef7de3f.png)\r\n\r\nAs you can see, there was a display error\r\n\r\n![image](https://user-images.githubusercontent.com/13817254/34660548-16f0b794-f47e-11e7-996b-9862f788a771.png)\r\n\r\nUse the command completion function is encountered a display error", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3064/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kmagonski": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3060", "title": "HTTPERROR_ALLOWED_CODES can't parse response with 301", "body": "In scrapy.downloadermiddlewares.redirect.RedirectMiddleware#process_response \r\ndosn't get anything from settings like HTTPERROR_ALLOWED_CODES only in HttpErrorMiddleware we have \r\nhandle_httpstatus_list.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "NewUserHa": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3057", "title": "configure_logging unable to handle GBK", "body": "I added \r\n`import logging\r\nfrom scrapy.utils.log import configure_logging\r\n\r\nconfigure_logging(install_root_handler=False)\r\nlogging.basicConfig(\r\n    filename='log.txt',\r\n    format='%(levelname)s: %(message)s',\r\n    level=logging.INFO\r\n)`\r\nfrom scrapy documents blow my class define.\r\n\r\nthen:\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\logging\\__init__.py\", line 982, in emit\r\n    stream.write(msg)\r\nUnicodeEncodeError: 'gbk' codec can't encode character '\\ufe0f' in position 190: illegal multibyte sequence\r\nCall stack:\r\n....\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\scrapy\\core\\scraper.py\", line 237, in _itemproc_finished\r\n    logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\r\nMessage: 'Scraped from %(src)s\\r\\n%(item)s'\r\nArguments: {'src': <200 http://...>, 'item': {'date': '12-02', 'floor': '...\\n    \u7535\\ufe0f', 'pics': ['...', ...]}}\r\n\r\nI also have 'LOG_LEVEL': 'INFO' in the spider.\r\n\r\nI googled and have no idea how to fix this.\r\nany help, please?\r\nThanks!", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3057/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3056", "title": "[suggest] add a option to pipeline for some sites requiring reference in heads", "body": "it's usual case and it's ugly to override get_media_requests method of pipelines.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3056/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3055", "title": "[bug] pillow will always recode images in imagepipieline", "body": "https://github.com/scrapy/scrapy/blob/aa83e159c97b441167d0510064204681bbc93f21/scrapy/pipelines/images.py#L151\r\n\r\nthis line will always recode images silently and damage the image quality.\r\n\r\nplease add an option to avoid this.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "elacuesta": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3054", "title": "Request serialization should fail for non-picklable objects", "body": "The Pickle-based disk queues silently serialize requests that shouldn't be serialized in Python<=3.5. I found this problem when dumping a request with an `ItemLoader` object in its `meta` dict. Python 3.6 fails in [this line](https://github.com/scrapy/scrapy/blob/1.4/scrapy/squeues.py#L27) with `TypeError: can't pickle HtmlElement objects`, because the loader contains a `Selector`, which in turns contains an `HtmlElement` object.\r\n\r\nI tested this using the https://github.com/scrapinghub/scrapinghub-stack-scrapy repository, and found that `pickle.loads(pickle.dumps(selector))` doesn't fail, but generates a broken object.\r\n\r\n#### Python 2.7, Scrapy 1.3.3 (https://github.com/scrapinghub/scrapinghub-stack-scrapy/tree/branch-1.3)\r\n```\r\nroot@04bfc6cf84cd:/# scrapy version -v\r\nScrapy    : 1.3.3\r\nlxml      : 3.7.2.0\r\nlibxml2   : 2.9.3\r\ncssselect : 1.0.1\r\nparsel    : 1.1.0\r\nw3lib     : 1.17.0\r\nTwisted   : 16.6.0\r\nPython    : 2.7.14 (default, Dec 12 2017, 16:55:09) - [GCC 4.9.2]\r\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1t  3 May 2016)\r\nPlatform  : Linux-4.9.44-linuxkit-aufs-x86_64-with-debian-8.10\r\nroot@04bfc6cf84cd:/# scrapy shell \"http://example.org\"\r\n2017-12-29 16:49:27 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\r\n(...)\r\n>>> from six.moves import cPickle as pickle\r\n>>> s2 = pickle.loads(pickle.dumps(response.selector, protocol=2))\r\n>>> response.selector.css('a')\r\n[<Selector xpath=u'descendant-or-self::a' data=u'<a href=\"http://www.iana.org/domains/exa'>]\r\n>>> s2.css('a')\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/site-packages/parsel/selector.py\", line 227, in css\r\n    return self.xpath(self._css2xpath(query))\r\n  File \"/usr/local/lib/python2.7/site-packages/parsel/selector.py\", line 203, in xpath\r\n    **kwargs)\r\n  File \"src/lxml/lxml.etree.pyx\", line 1584, in lxml.etree._Element.xpath (src/lxml/lxml.etree.c:59349)\r\n  File \"src/lxml/xpath.pxi\", line 257, in lxml.etree.XPathElementEvaluator.__init__ (src/lxml/lxml.etree.c:170478)\r\n  File \"src/lxml/apihelpers.pxi\", line 19, in lxml.etree._assertValidNode (src/lxml/lxml.etree.c:16482)\r\nAssertionError: invalid Element proxy at 140144569743064\r\n```\r\n\r\n\r\n#### Python 3.5, Scrapy 1.3.3 (https://github.com/scrapinghub/scrapinghub-stack-scrapy/tree/branch-1.3-py3)\r\n```\r\nroot@1945e2154919:/# scrapy version -v\r\nScrapy    : 1.3.3\r\nlxml      : 3.7.2.0\r\nlibxml2   : 2.9.3\r\ncssselect : 1.0.1\r\nparsel    : 1.1.0\r\nw3lib     : 1.17.0\r\nTwisted   : 16.6.0\r\nPython    : 3.5.4 (default, Dec 12 2017, 16:43:39) - [GCC 4.9.2]\r\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1t  3 May 2016)\r\nPlatform  : Linux-4.9.44-linuxkit-aufs-x86_64-with-debian-8.10\r\nroot@1945e2154919:/# scrapy shell \"http://example.org\"\r\n2017-12-29 16:52:37 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\r\n(...)\r\n>>> from six.moves import cPickle as pickle\r\n>>> s2 = pickle.loads(pickle.dumps(response.selector, protocol=2))\r\n>>> response.selector.css('a')\r\n[<Selector xpath='descendant-or-self::a' data='<a href=\"http://www.iana.org/domains/exa'>]\r\n>>> s2.css('a')\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/site-packages/parsel/selector.py\", line 227, in css\r\n    return self.xpath(self._css2xpath(query))\r\n  File \"/usr/local/lib/python3.5/site-packages/parsel/selector.py\", line 203, in xpath\r\n    **kwargs)\r\n  File \"src/lxml/lxml.etree.pyx\", line 1584, in lxml.etree._Element.xpath (src/lxml/lxml.etree.c:59349)\r\n  File \"src/lxml/xpath.pxi\", line 257, in lxml.etree.XPathElementEvaluator.__init__ (src/lxml/lxml.etree.c:170478)\r\n  File \"src/lxml/apihelpers.pxi\", line 19, in lxml.etree._assertValidNode (src/lxml/lxml.etree.c:16482)\r\nAssertionError: invalid Element proxy at 139862544625976\r\n```\r\n\r\n\r\n#### Python 3.6, Scrapy 1.3.3 (https://github.com/scrapinghub/scrapinghub-stack-scrapy/tree/branch-1.3-py3)\r\n```\r\nroot@43e690443ca7:/# scrapy version -v\r\nScrapy    : 1.3.3\r\nlxml      : 3.7.2.0\r\nlibxml2   : 2.9.3\r\ncssselect : 1.0.1\r\nparsel    : 1.1.0\r\nw3lib     : 1.17.0\r\nTwisted   : 16.6.0\r\nPython    : 3.6.4 (default, Dec 21 2017, 01:35:12) - [GCC 4.9.2]\r\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1t  3 May 2016)\r\nPlatform  : Linux-4.9.44-linuxkit-aufs-x86_64-with-debian-8.10\r\nroot@43e690443ca7:/# scrapy shell \"http://example.org\"\r\n2017-12-29 16:54:49 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\r\n(...)\r\n>>> from six.moves import cPickle as pickle\r\n>>> s2 = pickle.loads(pickle.dumps(response.selector, protocol=2))\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\nTypeError: can't pickle HtmlElement objects\r\n```", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3054/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/793b2376f83caa01d3e883f7642147de8e174c01", "message": "Populate spider variable when using shell.inspect_response"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/25535dba9ca7e6f6f3c2279dd77240a21b1cc672", "message": "Feed exports: edit note, fix typos"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/63b8caf5debf84e8da7f299782d15d0a41bf8a14", "message": "Feed exports: rewrite indentation test without .strip()"}], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3082", "title": "Do not serialize unpickable objects (py3)", "body": "~This addresses #3054 partially, as it catches the exception raised by `pickle`.\r\nHowever, since this exception is only raised in python >= 3.6, for earlier versions it's still not able to realize it shouldn't serialize some requests.~\r\n\r\nFixes #3054.\r\n\r\nAdded a monkey patch to prevent the pickling of `lxml.html.HtmlElement` objects.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2956", "title": "Add from_crawler support to dupefilters", "body": "Fixes #2940\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2784", "title": "CaselessDict, Headers: Preserve capitalization", "body": "Fixes #2711\r\n\r\nI see two ways of implementing this feature:\r\n- Store a `lower key: actual key` mapping within the `CaselessDict` class, and provide options to store the actual keys as they are provided (capitalize by default for backwards compatibility). This increases space complexity, and it's how this PR does it.\r\n- Normalize and compare any provided key with the stored keys on all operations that require it (lookup or deletion, for instance). This would increase time complexity.\r\n\r\nAn example showing the changes of this PR:\r\n```\r\n>>> from scrapy.http import Request, Headers\r\n>>> # capitalize keys (current behaviour)\r\n>>> req1 = Request('http://example.org', headers={'will-be-capitalized': 'value'})\r\n>>> req1.headers\r\n{'Will-Be-Capitalized': ['value']}\r\n>>> # preserve keys\r\n>>> h = Headers({'will-be-preserved': 'value'}, capitalize=False)\r\n>>> req2 = Request('http://example.org', headers=h)\r\n>>> req2.headers\r\n{'will-be-preserved': ['value']}\r\n```\r\n\r\nLooking forward to reading your comments!", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2409", "title": "Docs: add FAQ entry about splitting items", "body": "See #2240", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2400", "title": "CookiesMiddleware: keep cookies from 'Cookie' request header", "body": "I would like to start a discussion to solve #1992.\r\n\r\nThis PR modifies the `CookiesMiddleware._get_request_cookies` method to prevent the middleware from discarding cookies already set in the headers (either by the `DefaultHeadersMiddleware` or by using the `Request`'s `headers` constructor parameter).", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2061", "title": "process_spider_exception on generators", "body": "This PR is a starting point to solve #220.\nIt could probably use some more test cases, mostly to figure out what exactly is the desired behaviour when processing the exceptions.\nI can't take much credit for this: if it breaks, blame @dangra \ud83d\ude1b \n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stummjr": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3046", "title": "Item loader missing values from base item", "body": "ItemLoaders behave oddly when they get a pre-populated item as an argument and `get_output_value()` gets called for one of the pre-populated fields before calling `load_item()`.\r\n\r\nCheck this out:\r\n\r\n```python\r\n>>> from scrapy.loader import ItemLoader\r\n>>> item = {'url': 'http://example.com', 'summary': 'foo bar'}\r\n>>> loader = ItemLoader(item)\r\n>>> loader.load_item()\r\n{'summary': 'foo bar', 'url': 'http://example.com'}\r\n\r\n# so far, so good... what about now?\r\n>>> item = {'url': 'http://example.com', 'summary': 'foo bar'}\r\n>>> loader = ItemLoader(item)\r\n>>> loader.get_output_value('url')\r\n[]\r\n>>> loader.load_item()\r\n{'summary': 'foo bar', 'url': []}\r\n```\r\n\r\nThere are **2** unexpected behaviors in this snippet (at least from my point of view):\r\n\r\n**1)** `loader.get_output_value()` doesn't return the pre-populated values, even though they end up in the final item.\r\n\r\nIt seems to be like this on purpose, though. The `get_output_value()` method only queries the `_local_values` defaultdict ([here](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L125)).\r\n\r\n\r\n**2)** once we call `loader.get_output_value('url')`, that field is not included in the `load_item()` result anymore.\r\n\r\nThis one doesn't look right, IMHO.\r\n\r\nIt happens because when we call `loader.get_output_value('url')` for the first time, such value is not available on `_local_values`, and so a new entry in the `_local_values` defaultdict will be created with an empty list on it ([here](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L121)). Then, when `loader.load_item()` gets called, [these lines](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L116-L117) overwrite the current value from the internal item because the value returned by `get_output_value()` is `[]` and not `None`.\r\n\r\nAny thoughts on this?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3046/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/bb0bd691d9dd42725af6291a15e357f0146de17f", "message": "Improve error message when callback is not callable"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/4b6f68b9ee2830534c25b824103b13d502005fe1", "message": "make reqser tests create Request with proper callback/errback"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/0f6f486769d7761054274509de08fcf455d492de", "message": "fix parse command issue with callback as a string"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/60727dedf605fad2ed4be844cb2ec44e305257f0", "message": "verify if Request callback is callable"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/80b160d0d739bc3667adf65817a43111f14b9599", "message": "include references to scrapy subreddit in the docs"}], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3047", "title": "Avoid missing base item fields in item loaders", "body": "This is an attempt to fix the behavior described in #3046.\r\n\r\nInstead of just checking if the value inside the loader is not None in order to decide if a field from the initial item should be overwritten or not, `load_item()` should also make sure that the value returned by `get_output_value()` is not an empty list.\r\n\r\nThat is because `self._local_values` , which stores the new values included via `add_*` or `replace_*` methods, is a[`defaultdict(list)`](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L37). Then, when we call `get_output_value()` for a field only available in the initial item, an empty list will be set for that field in `self._local_values` (because of [this](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L125)).\r\n\r\n\r\nThis way, we make sure we don't miss fields from the initial item, in case `get_output_value()` gets called for one of the pre-populated fields before `load_item()`, as described on #3046.", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "exotfboy": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3036", "title": "File downloaded by pipeline are blank", "body": "I have deployed a spider in my remote server, and I am using `FilePipeline` to download images for `item`, however I found that the downloaded image have size of  zero, which is not expected.\r\n\r\nThen I test the project in my local machine, it worked. So I think maybe my remote server has been banned, however when I tried `wget ..` I can download the image normally, and I also tried `scrapy sheel image_src` it still work.\r\n\r\nNow I have no idea what's going on , I just want to intercept the response of the request re-sent by the pipeline, is it possible?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3036/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/1992", "title": "DEFAULT_REQUEST_HEADERS not work as expected", "body": "I am new in scrapy, and I meet some problems which I can not get answer from google, so I post it here:\n\n1 Cookie not work even set in DEFAULT_REQUEST_HEADERS:\n\n```\nDEFAULT_REQUEST_HEADERS = {\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'accept-encoding': 'gzip, deflate, sdch',\n    'cache-control': 'no-cache',\n    'cookie': 'xx=yy',\n    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.94 Safari/537.36'\n}\n```\n\n```\nclass MySpider(scrapy.Spider):\n    def make_requests_from_url(self, url):\n        return scrapy.http.Request(url, headers=DEFAULT_REQUEST_HEADERS)\n```\n\nI know the `make_requests_from_url` will only called once for the start_urls, and in my opinion, the first request will send the cookie I set in the `DEFAULT_REQUEST_HEADERS`, however it does not.\n\n2 Share settings between spiders.\n\nI have multiple spiders in the project which share most of the settings like `RandomAgentMiddleware` `RandomProxyMiddleware` `UserAgent` `DEFAULT_REQUEST_HEADERS` and etc, however they are configured inside the settings.py for each spider.\n\nIs it possible to share these settings?\n\n---\n\nThe \n`COOKIES_ENABLED` is set to true.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/1992/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ReLLL": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3034", "title": "Line-ends (unnecessary blank lines) problem in CSV export on Windows ", "body": "CSV export on Windows create unnecessary blank lines after each line.\r\n\r\nYou can fix the problem just by adding \r\nnewline='' \r\nas parameter to io.TextIOWrapper in the __init__ method of the CsvItemExporter class in scrapy.exporters\r\n\r\nDetails are over here:\r\nhttps://stackoverflow.com/questions/39477662/scrapy-csv-file-has-uniform-empty-rows/43394566#43394566", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3034/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3039", "title": "Fix for #3034, CSV export unnecessary blank lines problem on Windows", "body": "Fixed the issue I've mentioned there, this is the pull request to merge, (added one line), hope all is fine. \r\nhttps://github.com/scrapy/scrapy/issues/3034\r\n\r\nCloses #3034 ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cp2587": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3029", "title": "Invalid DNS pattern", "body": "Hello,\r\n\r\nWe are using https proxies to crawl some website and sometimes i get the following stack trace:\r\n\r\n```\r\nError during info_callback\r\nTraceback (most recent call last):\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/protocols/tls.py\", line 315, in dataReceived\r\n    self._checkHandshakeStatus()\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/protocols/tls.py\", line 235, in _checkHandshakeStatus\r\n    self._tlsConnection.do_handshake()\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 1442, in do_handshake\r\n    result = _lib.SSL_do_handshake(self._ssl)\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 933, in wrapper\r\n    callback(Connection._reverse_mapping[ssl], where, return_code)\r\n--- <exception caught here> ---\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/internet/_sslverify.py\", line 1102, in infoCallback\r\n    return wrapped(connection, where, ret)\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/scrapy/core/downloader/tls.py\", line 67, in _identityVerifyingInfoCallback\r\n    verifyHostname(connection, self._hostnameASCII)\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/service_identity/pyopenssl.py\", line 44, in verify_hostname\r\n    cert_patterns=extract_ids(connection.get_peer_certificate()),\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/service_identity/pyopenssl.py\", line 73, in extract_ids\r\n    ids.append(DNSPattern(n.getComponent().asOctets()))\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/service_identity/_common.py\", line 156, in __init__\r\n    \"Invalid DNS pattern {0!r}.\".format(pattern)\r\nservice_identity.exceptions.CertificateError: Invalid DNS pattern '194.167.13.105'.\r\n```\r\n\r\nI think this issue is somewhat similar to https://github.com/scrapy/scrapy/issues/2092 and this 'invalid DNS pattern' error should be caught similarly as the 'Invalid DNS-ID'. What do you think ?\r\n\r\nIn the meantime, how can i catch it myself and silent it ?\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3029/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2499", "title": "exceptions.AttributeError: '_SIGCHLDWaker' object has no attribute 'doWrite'", "body": "Hello,\r\n\r\nThank your for your fantastic project. We are facing a really hard to solve bug while running scapy inside celery task. Sometimes we get this error:\r\n```\r\nUnhandled Error\r\nTraceback (most recent call last):\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/python/log.py\", line 101, in callWithLogger\r\n    return callWithContext({\"system\": lp}, func, *args, **kw)\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/python/log.py\", line 84, in callWithContext\r\n    return context.call({ILogContext: newCtx}, func, *args, **kw)\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/python/context.py\", line 118, in callWithContext\r\n    return self.currentContext().callWithContext(ctx, func, *args, **kw)\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/python/context.py\", line 81, in callWithContext\r\n    return func(*args,**kw)\r\n--- <exception caught here> ---\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/internet/posixbase.py\", line 602, in _doReadOrWrite\r\n    why = selectable.doWrite()\r\nexceptions.AttributeError: '_SIGCHLDWaker' object has no attribute 'doWrite'\r\n```\r\nI think it's related to a threading problem (I am not sure twisted is very thread safe) but maybe you already have an explanation ? Anyway, it seems bloated to run scrapy inside celery tasks because we need to recreate the crawler for each task, do you have any good advice on how to built a system to distribute crawls ? We are open to use scrapinghub but we have some very specific pipelines & download middlewares which prevent us to do so :(", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2499/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cisko3000": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3024", "title": "startproject command does not work for latest pyOpenSSL version", "body": "I did an install of scrapy on virtualenv, upgraded all packages and command \"startproject\" did not work.\r\nI would get his error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/me/sites/cot/env/bin/scrapy\", line 11, in <module>\r\n    sys.exit(execute())\r\n  File \"/home/me/sites/cot/env/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 121, in execute\r\n    cmds = _get_commands_dict(settings, inproject)\r\n  File \"/home/me/sites/cot/env/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 45, in _get_commands_dict\r\n    cmds = _get_commands_from_module('scrapy.commands', inproject)\r\n  File \"/home/me/sites/cot/env/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 28, in _get_commands_from_module\r\n    for cmd in _iter_command_classes(module):\r\n  File \"/home/me/sites/cot/env/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 19, in _iter_command_classes\r\n    for module in walk_modules(module_name):\r\n  File \"/home/me/sites/cot/env/local/lib/python2.7/site-packages/scrapy/utils/misc.py\", line 71, in walk_modules\r\n    submod = import_module(fullpath)\r\n  File \"/usr/lib/python2.7/importlib/__init__.py\", line 37, in import_module\r\n    __import__(name)\r\n  File \"/home/me/sites/cot/env/local/lib/python2.7/site-packages/scrapy/commands/version.py\", line 6, in <module>\r\n    import OpenSSL\r\n  File \"/home/me/sites/cot/env/local/lib/python2.7/site-packages/OpenSSL/__init__.py\", line 8, in <module>\r\n    from OpenSSL import crypto, SSL\r\n  File \"/home/me/sites/cot/env/local/lib/python2.7/site-packages/OpenSSL/crypto.py\", line 12, in <module>\r\n    from cryptography import x509\r\n  File \"/home/me/sites/cot/env/local/lib/python2.7/site-packages/cryptography/x509/__init__.py\", line 7, in <module>\r\n    from cryptography.x509 import certificate_transparency\r\nImportError: cannot import name certificate_transparency\r\n\r\n```\r\nSo I went to an environment where I was able to do startproject (over a year ago), and noticed it has pyOpenSSL version 16.0.0. and then installed this version and then I was able to startproject using latest environment and latest scrapy.\r\n\r\nWhat is up with this? Is this something I can fix here in this scrapy repo?\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3024/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "AndrewMishchenko": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3022", "title": "When i'm using proxy to scrape - sometimes i got timeouterror", "body": "I'm trying to scrape one https site. Also i build spider for it and the problem is next:\r\n1. It is work perfect without proxy. It scrape 100% pages and very fast.\r\n2. When i'm trying to scrape through proxy(crawlera and etc) through time it raise timeout(at all works not stable) and i don't know what to do with this.\r\n\r\nHeders i'm using:\r\n'Host': '.....,\r\n'Connection': 'keep-alive',\r\n'Pragma': 'no-cache',\r\n 'Cache-Control': 'no-cache',\r\n 'User-Agent': 'Mozilla/4.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/5.0)',\r\n 'DNT': '1',\r\n 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\r\n 'Accept-Encoding': 'gzip, deflate, br',\r\n 'Accept-Language': 'en-US,en;q=0.8',\r\n 'Upgrade-Insecure-Requests': '1',\r\n 'X-Requested-With': 'XMLHttpRequest',\r\n 'Referer': '....',\r\nAlso i was trying to add 'DOWNLOADER_CLIENT_TLS_METHOD': 'TLSv1.2' - it doesn't help!\r\nand write something like:\r\nclass SClientContextFactory(ScrapyClientContextFactory):\r\n     def __init__(self, method=SSL.TLSv1_2_METHOD):\r\n        self.method = method\r\n\r\nI'm 100 percent sure that the problem is not in the proxy\r\nCan someone help me?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3022/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Vineeth-Mohan": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3017", "title": "Entire HTML is not checked for finding base tag", "body": "In the HTML we are using the base tag is set. It also happens that this HTML has huge amount of comment and white space , and base tag is not coming in first 4096 characters.\r\n\r\nIn the code here - https://github.com/scrapy/scrapy/blob/b8870ee8a10360aaa74298324d97c823b88ec5c6/scrapy/utils/response.py#L27\r\n\r\n```\r\ndef get_base_url(response):\r\n    \"\"\"Return the base url of the given response, joined with the response url\"\"\"\r\n    if response not in _baseurl_cache:\r\n        text = response.text[0:4096]\r\n        _baseurl_cache[response] = html.get_base_url(text, response.url,\r\n            response.encoding)\r\n    return _baseurl_cache[response]\r\n```\r\n\r\nWe could find that , in the code above , we are NOT checking for the base tag beyond first 4096 characters. This has failed our crawl.\r\nI believe there could not be any hard coding in any code and  I request to make this configurable atleast. \r\n\r\nWe are stuck with this , please advice what needs to be done.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "woxcab": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3001", "title": "Derived spider with overwritten custom_settings ignores custom_settings of the base spider", "body": "If the base spider has `custom_settings` field and the inherited spider from the base spider also has `custom_settings` field then all settings from base spider are ignored by derived spider.\r\n\r\nFor example:\r\n\r\n```\r\nimport scrapy \r\n\r\nclass BaseSpider(scrapy.Spider):\r\n    custom_settings = {'ITEM_PIPELINES': {\r\n        'test_proj.pipelines.FirstPipeline': 300,\r\n        'test_proj.pipelines.SecondPipeline': 301,\r\n    }}\r\n\r\nclass DerivedSpider(BaseSpider):\r\n    name = 'derived'\r\n    custom_settings = {'CONCURRENT_REQUESTS': 2}\r\n```\r\n\r\nIn this example `DerivedSpider` will have only `{'CONCURRENT_REQUESTS': 2}` among overwritten settings whereas item pipelines from the base spider will not be presented in the `DerivedSpider` settings.\r\n\r\nSo I suggest to merge `custom_settings` of the all base spider classes of a spider when spider settings are initialized. What do you think about it? ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3001/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/a84652e775fda1135fe959465b27cf6ed2c25e1d", "message": "Init tests are split by initializer' input"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/fbb411a805724fec50b786f369be79dc221c798e", "message": "Allowed passing objects of Mapping class or its subclass to the CaselessDict initializer"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kmike": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2998", "title": "dont_merge_cookies docs are incomplete", "body": "dont_merge_cookies [docs](https://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request) are incomplete: they say that \r\n\r\n> When some site returns cookies (in a response) those are stored in the cookies for that domain and will be sent again in future requests. That\u2019s the typical behaviour of any regular web browser. However, if, for some reason, you want to avoid merging with existing cookies you can instruct Scrapy to do so by setting the dont_merge_cookies key to True in the Request.meta.\r\n\r\nBut this flag not only prevents merging of cookies, but also prevents sending of them: https://github.com/scrapy/scrapy/blob/b8870ee8a10360aaa74298324d97c823b88ec5c6/scrapy/downloadermiddlewares/cookies.py#L27\r\n\r\nMaybe a separate issue, but it'd be nice to have separate flags for sending and merging of cookies.\r\n\r\n//cc @whalebot-helmsman", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2998/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2951", "title": "SSLv3 method is no longer available", "body": "DOWNLOADER_CLIENT_TLS_METHOD='SSLv3' no longer works with recent Twisted, PyOpenSSL, OpenSSL, etc.:\r\n\r\n```\r\nscrapy shell 'https://example.com' -s DOWNLOADER_CLIENT_TLS_METHOD=SSLv3\r\n2017-10-03 13:17:49 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot)\r\n2017-10-03 13:17:49 [scrapy.utils.log] INFO: Versions: lxml 4.0.0.0, libxml2 2.9.5, cssselect 1.0.1, parsel 1.2.0, w3lib 1.18.0, Twisted 17.9.0, Python 3.6.1 (default, Apr  4 2017, 09:40:21) - [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.38)], pyOpenSSL 17.3.0 (OpenSSL 1.1.0f  25 May 2017), cryptography 2.0.3, Platform Darwin-16.7.0-x86_64-i386-64bit\r\n2017-10-03 13:17:49 [scrapy.crawler] INFO: Overridden settings: {'DOWNLOADER_CLIENT_TLS_METHOD': 'SSLv3', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'EDITOR': 'nano', 'LOGSTATS_INTERVAL': 0}\r\n2017-10-03 13:17:49 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage']\r\n2017-10-03 13:17:49 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2017-10-03 13:17:49 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2017-10-03 13:17:49 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2017-10-03 13:17:49 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\r\n2017-10-03 13:17:49 [scrapy.core.engine] INFO: Spider opened\r\nTraceback (most recent call last):\r\n  File \"/Users/kmike/envs/deepdeep/lib/python3.6/site-packages/OpenSSL/SSL.py\", line 618, in __init__\r\n    method_func = self._methods[method]\r\nKeyError: 2\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/kmike/envs/deepdeep/bin/scrapy\", line 11, in <module>\r\n    load_entry_point('Scrapy', 'console_scripts', 'scrapy')()\r\n  File \"/Users/kmike/svn/scrapy/scrapy/cmdline.py\", line 150, in execute\r\n    _run_print_help(parser, _run_command, cmd, args, opts)\r\n  File \"/Users/kmike/svn/scrapy/scrapy/cmdline.py\", line 90, in _run_print_help\r\n    func(*a, **kw)\r\n  File \"/Users/kmike/svn/scrapy/scrapy/cmdline.py\", line 157, in _run_command\r\n    cmd.run(args, opts)\r\n  File \"/Users/kmike/svn/scrapy/scrapy/commands/shell.py\", line 73, in run\r\n    shell.start(url=url, redirect=not opts.no_redirect)\r\n  File \"/Users/kmike/svn/scrapy/scrapy/shell.py\", line 48, in start\r\n    self.fetch(url, spider, redirect=redirect)\r\n  File \"/Users/kmike/svn/scrapy/scrapy/shell.py\", line 115, in fetch\r\n    reactor, self._schedule, request, spider)\r\n  File \"/Users/kmike/envs/deepdeep/lib/python3.6/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\r\n    result.raiseException()\r\n  File \"/Users/kmike/envs/deepdeep/lib/python3.6/site-packages/twisted/python/failure.py\", line 385, in raiseException\r\n    raise self.value.with_traceback(self.tb)\r\nValueError: No such protocol\r\n```", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2951/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2702", "title": "error at telnet shutdown in Python 3", "body": "Scrapy version:\r\n\r\n```\r\nScrapy    : 1.3.3\r\nlxml      : 3.7.3.0\r\nlibxml2   : 2.9.3\r\ncssselect : 1.0.1\r\nparsel    : 1.1.0\r\nw3lib     : 1.17.0\r\nTwisted   : 17.1.0\r\nPython    : 3.5.2 (default, Nov 17 2016, 17:05:23) - [GCC 5.4.0 20160609]\r\npyOpenSSL : 0.15.1 (OpenSSL 1.0.2g  1 Mar 2016)\r\nPlatform  : Linux-4.4.0-59-generic-x86_64-with-Ubuntu-16.04-xenial\r\n```\r\n\r\nI've got this exception on shutdown:\r\n\r\n```\r\n2017-04-05 18:18:10 [scrapy.core.engine] INFO: Spider closed (finished)\r\n2017-04-05 18:18:10 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method TelnetConsole.stop_listening of <scrapy.extensions.telnet.TelnetConsole object at 0x7fbb993bfeb8>>\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/.local/lib/python3.5/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\r\n    result = f(*args, **kw)\r\n  File \"/home/ubuntu/.local/lib/python3.5/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\r\n    return receiver(*arguments, **named)\r\n  File \"/home/ubuntu/.local/lib/python3.5/site-packages/scrapy/extensions/telnet.py\", line 63, in stop_listening\r\n    self.port.stopListening()\r\nAttributeError: 'TelnetConsole' object has no attribute 'port'\r\n```\r\n\r\nNot sure if it is a real problem, but it'd be nice to shut down without error messages. No idea how to reproduce it, I've only seen it once.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2702/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2653", "title": "http11 connections are left opened", "body": "I haven't checked it, but there is a https://github.com/scrapy/scrapy/pull/999#discussion_r105122341 by @adiroiban  which suggests HTTP11DownloadHandler.close implementation is not complete.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2653/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2583", "title": "add response.submit or Selector(List?).submit shortcut", "body": "As discussed in https://github.com/scrapy/scrapy/issues/1940, what do you think about adding response.submit shortcut for FormRequest.from_response? Proposed API:\r\n\r\n```py\r\ndef submit(self, \r\n    # Selector or SelectorList, e.g. response.xpath('//form')[1] \r\n    # or response.css('#myform')\r\n    sel=None, \r\n\r\n    # similar to FormRequest.from_response formcss, a shortcut \r\n    # for passing response.css in sel argument\r\n    css=None, \r\n\r\n    # similar to FormRequest.from_response formxpath, a shortcut \r\n    # for passing response.xpath in sel argument\r\n    xpath=None,  \r\n\r\n    formdata=None,  # same as FormRequest.from_response formdata\r\n    clickdata=None, # same as FormRequest.from_response clickdata\r\n    dont_click=False, # same as FormRequest.from_response dont_click\r\n    **kwargs  # same as FormRequest.from_response keyword arguments\r\n)\r\n```\r\n\r\nDifferences from FormRequest API:\r\n\r\n1. Selector/SelectorList support;\r\n2. formcss / formxpath are renamed to just css/xpath;\r\n3. formid/formname/formnumber arguments are dropped. id is trivial with css. I'm on fence about dropping formnumber.\r\n4. Only one of `sel`, `css` and `xpath` can be passed at the same time, there are no fallbacks. \r\n5. There shouldn't be gotchas like https://github.com/scrapy/scrapy/issues/1163. If no forms are matched, exception is raised.\r\n\r\n\r\nI think it makes sense to wrap FormRequest instead of starting from scratch; this way issues fixed for FormRequest (like https://github.com/scrapy/scrapy/issues/667 or https://github.com/scrapy/scrapy/issues/508) will be fixed for `response.submit` as well, and vice versa.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2583/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2582", "title": "response.follow_all or SelectorList.follow_all shortcut", "body": "What do you think about adding response.follow_all shortcut, which returns a list of requests? This is inspired by this note in docs:\r\n\r\n> `response.follow(response.css('li.next a'))` is not valid because response.css returns a list-like object with selectors for all results, not a single selector. A for loop like in the example above, or response.follow(response.css('li.next a')[0]) is fine.\r\n\r\nSo instead of \r\n\r\n```py\r\nfor href in response.css('li.next a::attr(href)'):\r\n    yield response.follow(href, callback=self.parse)\r\n```\r\n\r\nusers would be able to write (in Python 3)\r\n\r\n```py\r\nyield from response.follow_all(response.css('li.next a::attr(href)'), self.parse)\r\n```\r\n\r\nWe can also add 'css' and 'xpath' support to it, as keyword arguments; it would shorten the code to this:\r\n\r\n```py\r\nyield from response.follow_all(css='li.next a::attr(href)', callback=self.parse)\r\n```\r\n\r\n(this is a follow-up to https://github.com/scrapy/scrapy/issues/1940 and https://github.com/scrapy/scrapy/issues/2540)", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2582/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2568", "title": "finish relocations", "body": "What do you think about removing shims for moved/renamed modules from https://github.com/scrapy/scrapy/issues/1063? Scrapy 1.0 release was 20 months ago, and upgrade path for users is clear.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2568/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2545", "title": "enable test_proxy_connect in Python 3", "body": "mitmproxy is ported to Python 3 (in fact, it is now Python3-only), so it'd be nice to enable test_proxy_connect.py in Python 3.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2545/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2365", "title": "provide a way to work with scrapy http cache without making requests", "body": "Currently it is hard to extract information from scrapy cache: cache storages want 'spider' and 'request' objects, one can't just list all files in cache and get Response instances from them. I think it can be a good to refactor FilesystemCacheStorage:\n1. allow reading individual cache entries by their path;\n2. allow to disable expiration logic.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2365/reactions", "total_count": 3, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 1}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2222", "title": "cache storages or HttpCacheMiddleware should handle retrieve_response errors", "body": "I think it makes sense to handle broken cache records as cache misses, with a proper log message. I had this problem with FileSystemStorage when gzip is enabled, and spider is killed. In this case it is possible to get a broken gzip file. It means spider can miss a request on a re-crawl.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2222/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2221", "title": "HTTP cache should expose storage time", "body": "FilesystemCacheStorage, DbmCacheStorage and LeveldbCacheStorage store a timestamp to meta, but unlike all other parameters this information is not available to a spider when response is retrieved from cache. \n\nI think Scrapy should expose this timestamp as a meta key for cached responses - it is useful not only for expiration. For example, it is required to parse relative dates properly - what \"1 hour ago\" means depends on a time page was downloaded.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2221/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2186", "title": "move more functions from scrapy.utils.url to w3lib", "body": "Hey,\n\nWhat do you think about moving `add_http_if_no_scheme` and `escape_ajax` functions to w3lib? \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2186/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2174", "title": "gzip-compressed item exports", "body": "I think compressing exported data can be useful in a lot of cases. It'd be good to have a built-in way for compressed exports in Scrapy. I have this implementation, but probably it makes more sense to handle .jl.gz / csv.gz / ... extensions just like .jl / .csv / ... instead of creating a storage, I'm not sure:\n\n``` py\n# -*- coding: utf-8 -*-\nimport os\nimport gzip\n\nfrom zope.interface import Interface, implementer\nfrom w3lib.url import file_uri_to_path\nfrom scrapy.extensions.feedexport import IFeedStorage\n\n\n@implementer(IFeedStorage)\nclass GzipFileFeedStorage(object):\n    \"\"\"\n    Storage which exports data to a gzipped file.\n    To use it, add\n\n    ::\n\n        FEED_STORAGES = {\n            'gzip': 'myproject.exports.GzipFileFeedStorage',\n        }\n\n    to settings.py and then run scrapy crawl like this::\n\n        scrapy crawl foo -o gzip:/path/to/items.jl\n\n    The command above will create ``/path/to/items.jl.gz`` file\n    (.gz extension is added automatically).\n\n    Other export formats are also supported, but it is recommended to use .jl.\n    If a spider is killed then gz archive may be partially broken.\n    In this case it user should read the broken archive line-by-line and stop\n    on gzip decoding errors, discarding the tail. It works OK with .jl exports.\n    \"\"\"\n    COMPRESS_LEVEL = 4\n\n    def __init__(self, uri):\n        self.path = file_uri_to_path(uri) + \".gz\"\n\n    def open(self, spider):\n        dirname = os.path.dirname(self.path)\n        if dirname and not os.path.exists(dirname):\n            os.makedirs(dirname, exist_ok=True)\n        return gzip.open(self.path, 'ab', compresslevel=self.COMPRESS_LEVEL)\n\n    def store(self, file):\n        file.close()\n```\n\nWhat do you think?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2174/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2173", "title": "Dump stats to log periodically, not only at the end of the crawl", "body": "It is useful to check Scrapy stats as spider runs, but there is no a built-in way to do that. What do you think about adding `DUMP_STATS_INTERVAL` option and outputting current stats to logs each `DUMP_STATS_INTERVAL` seconds?\r\n\r\nAnother related proposal (sorry for putting them all into this ticket) is to add more logging to Downloader and log periodically a number of pages Downloader is currently trying to fetch (`MONITOR_DOWNLOADS_INTERVAL` option). Checking that helps to understand what is crawler doing - is it busy downloading data or not.\r\n\r\nImplementation draft:\r\n\r\n``` python\r\nimport logging\r\nimport pprint\r\n\r\nfrom twisted.internet.task import LoopingCall\r\nfrom scrapy import signals\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\nclass _LoopingExtension:\r\n    def setup_looping_task(self, task, crawler, interval):\r\n        self._interval = interval\r\n        self._task = LoopingCall(task)\r\n        crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)\r\n        crawler.signals.connect(self.spider_closed, signal=signals.spider_closed)\r\n\r\n    def spider_opened(self):\r\n        self._task.start(self._interval, now=False)\r\n\r\n    def spider_closed(self):\r\n        if self._task.running:\r\n            self._task.stop()\r\n\r\n\r\nclass MonitorDownloadsExtension(_LoopingExtension):\r\n    \"\"\"\r\n    Enable this extension to periodically log a number of active downloads.\r\n    \"\"\"\r\n    def __init__(self, crawler, interval):\r\n        self.crawler = crawler\r\n        self.setup_looping_task(self.monitor, crawler, interval)\r\n\r\n    @classmethod\r\n    def from_crawler(cls, crawler):\r\n        # fixme: 0 should mean NotConfigured\r\n        interval = crawler.settings.getfloat(\"MONITOR_DOWNLOADS_INTERVAL\", 10.0)\r\n        return cls(crawler, interval)\r\n\r\n    def monitor(self):\r\n        active_downloads = len(self.crawler.engine.downloader.active)\r\n        logger.info(\"Active downloads: {}\".format(active_downloads))\r\n\r\n\r\nclass DumpStatsExtension(_LoopingExtension):\r\n    \"\"\"\r\n    Enable this extension to log Scrapy stats periodically, not only\r\n    at the end of the crawl.\r\n    \"\"\"\r\n    def __init__(self, crawler, interval):\r\n        self.stats = crawler.stats\r\n        self.setup_looping_task(self.print_stats, crawler, interval)\r\n\r\n    def print_stats(self):\r\n        stats = self.stats.get_stats()\r\n        logger.info(\"Scrapy stats:\\n\" + pprint.pformat(stats))\r\n\r\n    @classmethod\r\n    def from_crawler(cls, crawler):\r\n        interval = crawler.settings.getfloat(\"DUMP_STATS_INTERVAL\", 60.0)\r\n        # fixme: 0 should mean NotConfigured\r\n        return cls(crawler, interval)\r\n```\r\n\r\nTo get a feel on how it works copy-paste the code above to a project (e.g. to myproject/extensions.py) file, then add them to EXTENSIONS in settings.py:\r\n\r\n``` python\r\nEXTENSIONS = {\r\n    'myproject.extensions.MonitorDownloadsExtension': 100,\r\n    'myproject.extensions.DumpStatsExtension': 101,\r\n}\r\n\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2173/reactions", "total_count": 6, "+1": 6, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/1940", "title": "Better API for creating requests from responses", "body": "Sometime Request needs information about the response to be sent correctly. There are at least 2 use cases:\n1. URL encoding depends on response encoding (see https://github.com/scrapy/scrapy/pull/1923);\n2. relative URLs should be resolved based on response.url or its base url (see https://github.com/scrapy/scrapy/issues/548).\n\nI think the current API is not good enough. The most obvious code is not correct:\n\n``` py\nfor url in response.xpath(\"//a/@href\").extract():\n    yield scrapy.Request(url, self.parse)\n```\n\nTo do that correctly user has to write the following:\n\n``` py\nfor url in response.xpath(\"//a/@href\").extract():\n    yield scrapy.Request(response.urljoin(url), self.parse, encoding=response.encoding)\n```\n\nOr this:\n\n``` py\nfor link in LinkExtractor().extract_links(response):\n    yield scrapy.Request(link.url, self.parse, encoding=response.encoding)\n```\n\nLinkeExtractor solution has gotchas, e.g. canonicalize_url is called by default and fragments are removed. It means that e.g. Ajax crawlable URLs are not handled (no `escaped_fragment` even if a website supports it); it also makes it harder to use Scrapy with scrapy-splash which can handle fragments.\n\nThis all is too easy to get wrong; I think just documenting these gotchas is not good enough for a framework - it should make the easiest way to write something the correct way. IMHO in the API shouldn't require user to instantiate weird objects or pass response encoding:\n\n``` py\nfor url in response.xpath(\"//a/@href\").extract():\n    something.send_request(url, self.parse)\n```\n\nThis can be implemented if we provide a method on Response to send new requests.\n\nA related use case is `async def` functions or methods (https://github.com/scrapy/scrapy/issues/1144#issuecomment-141843616): it is not possible to yield Requests in `async def` functions, so adding a request should be either a method of `self` or a method of `response` if we want to support `async def` callbacks.\n\n`FormRequest.from_response(response, ...)` can also be written as something like `response.submit(...)`.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/1940/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/7c9e32213db2ce757c784e7c29e30caa57dc3d48", "message": "Merge pull request #3059 from jesuslosada/fix-typo\n\nFix typo in comment"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/786144e0c7b25a02151b6d3135451da90e912719", "message": "Merge pull request #3058 from jesuslosada/fix-link\n\nFix link in news.rst"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/aa83e159c97b441167d0510064204681bbc93f21", "message": "Bump version: 1.4.0 \u2192 1.5.0"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/d07fe11981a07e493faf7454db79b98c02a53118", "message": "set release date"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/c107059ef82a4b7b491b23b740b71353f03ab891", "message": "DOC fix rst syntax"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/d4e5671d07a8dcf18b665ed3ce4136dccae222fb", "message": "make release docs more readable, add highlights"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/45b0e1a0e4c51a773b39be14334a999cc5f0fe56", "message": "DOC draft 1.5 release notes"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/930f6ed8002e27c9d51f5d5abbb28c36460eb1bb", "message": "Merge pull request #3050 from lopuhin/pypy3\n\nAdd PyPy3 support"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/632f1cc07305d6967c9e8ce3bf150337e2bf3ecd", "message": "Merge pull request #3049 from scrapy/trove-classifiers\n\n[MRG+1] setup.py: mention that we support PyPy. See GH-2213."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9f9edeadfc9d8d3422d4ca15b2b13d5b502ca70e", "message": "Merge pull request #3048 from lopuhin/pypy-install-docs\n\n[MRG+1] Mention PyPy support, add PyPy to install docs"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/1058169f0e3a8646dbd20f9b4c0b599ed9f6d08e", "message": "setup.py: mention that we support PyPy. See GH-2213."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/bdc12f39949731e69aaa32c36490e1b6e56ca98b", "message": "Merge pull request #3045 from hugovk/rm-3.3\n\n[MRG+1] Drop support for EOL Python 3.3"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9aa9dd8d45a2ce0c8e6ae0732e610f020735df7e", "message": "DOC mention an easier way to track pull requests locally.\nThanks @eliasdorneles!"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f716843a66829350063e55f8df768eb538c6b05c", "message": "DOC update \"Contributing\" docs:\n\n* suggest Stack Overflow for Scrapy usage questions;\n* encourage users to submit test-only pull requests with reproducable examples;\n* encourage users to pick up stalled pull requests;\n* we don't use AUTHORS file as a main acknowledgement source;\n* suggest using Sphinx autodocs extension"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/bd16951a055d022c7867a02c5e532ced0d005501", "message": "Merge pull request #3013 from KosayJabre/patch-1\n\nSeparated import statements"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/b7a88fadbc4b6183c68fd8e67e0e9e00514277c1", "message": "Merge pull request #2991 from lopuhin/pypy-build-2\n\nFix PyPy build"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/108f8c4fd20a47bb94e010e9c6296f7ed9fdb2bd", "message": "Merge pull request #2982 from codeaditya/https-links\n\nFix broken links and use https links wherever possible"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/79df51aae1b091d39e90bf18da9ccda2f56fd10e", "message": "Merge pull request #2978 from codeaditya/https-links\n\nUse https for external links wherever possible in docs"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/98fb03e8bde18d35e8fe5bb150dfa4f0ad611fd2", "message": "Merge pull request #2958 from codeaditya/update-links\n\nLink \"Debugging in Python\" article to its new location"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/496fc60b251b12db89afecf3edc8289edf18d7d4", "message": "Merge pull request #2963 from djunzu/mention_request_meta_depth_on_depthmiddleware_doc\n\nAdd note about request.meta['depth'] in DepthMiddleware"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/73c40eb32428a9c299ccdbbe9c2edd8fd63be877", "message": "Merge pull request #2964 from weldon0405/patch-1\n\nUpdate tutorial.rst startproject files"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/124d577da146dcd9d514111f772baa74e22e940e", "message": "Merge pull request #2976 from weldon0405/patch-2\n\nupdated file structure to include middlewares.py"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/92fa74fb4362bb8e6ee8efa1a754fb3c3360d7fc", "message": "Merge pull request #2922 from stav/doc.response-body\n\n[Doc] Update Response.body type"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/3431870b53657500a1f7436a3df95579ef46d6aa", "message": "Merge pull request #2924 from superyyrrzz/patch-1\n\nminor fix typo"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/2e4ddc691271126fea4dd8e0e0c456e73dfa831f", "message": "Merge pull request #2947 from lagenar/fix-tests-typo\n\nFix typos in tests"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/346310cd108c08b6669d1d17ef1ba583c3c3a5f1", "message": "Merge pull request #2915 from redapple/versions-with-cryptography\n\nPrint cryptography package version"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/5d17f38aee5d4b43e01524315e82c55b7812530e", "message": "Merge pull request #2935 from lopuhin/pypy2.7-build\n\nUpdate pypy version regexp to get last release"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/6213fa5175c69a5d354b79a1f14555f5a4aa6fea", "message": "Merge pull request #2909 from cclauss/patch-2\n\nur'string' not needed in Py 2, syntax error in Py3"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/65ac0b06dfcef8c0466595931de3e8d37c6174ae", "message": "Merge pull request #2894 from redapple/log-custom-overriden-settings\n\nMove logging of overriden settings to Crawler init"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/1ba77f0837643130374bf69e1c2efac5831edad1", "message": "Merge pull request #2869 from cclauss/patch-3\n\n# noqa to close #2836"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/885289f497f31d721a3414867fc6fda2f959790f", "message": "Merge pull request #2854 from jenya/2853-sitemap-follow-alternate-fix\n\n[MRG+1] Follow alternate link for all types of sitemaps #2853"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/984c0c19ee378231e9d02a9177201d05de58fab1", "message": "Merge pull request #2884 from iamminji/patch-1\n\nfix typo"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9d34b2fe166f34400b15b21e95e37cb5a62aff6d", "message": "Merge pull request #2857 from redapple/scrapy-components-logs-startup\n\nLog versions information at startup"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/17bbd71433d3cc3de78b1baf39c36751c1e6fef5", "message": "Merge pull request #2812 from elacuesta/inspect_response_populate_spider\n\nPopulate spider variable when using shell.inspect_response"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/17fe6d2553bb61d7ef1f6611d5d6bae2eddc8c04", "message": "Merge pull request #2828 from cconrad/patch-1\n\nSpelling mistake"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/15abc0b9e3d8d1ba960ce644f2e2222c7ce4c542", "message": "Merge pull request #2816 from redapple/dns-cache-disabled\n\nFix DNS resolver when DNSCACHE_ENABLED=False"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/dd5ab6c5fe54b3d64cdbf89fcfa2b333cd90f22f", "message": "Merge pull request #2793 from lopuhin/pypy2\n\nFix PyPy test failures"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/5aebdac45d0563a17b7cd0d0da0078585d950605", "message": "Merge pull request #2781 from crasker/patch-1\n\nuse suggest method instead of DEPRECATED one"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/4e84424f2642baa01b2cd9e4eabafc4b68ad17d7", "message": "Merge pull request #2777 from chuanjin/patch-1\n\nUpdate extensions.rst"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/5e1f7a9eadbb5e6bd1ef1f200e7422b77e3377c9", "message": "DOC change \"releases\" section content"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/083880888b3d4916c3713aae251f42ae4c77e7f7", "message": "DOC fixed rst syntax in DOWNLOAD_FAIL_ON_DATALOSS docs"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/76e5b0f65c256373411c8fa6e005a85c48cad6ee", "message": "DOC 1.4 deprecations and backwards incompatible changes, add recent commits to news."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/edcde7a2cf1b7abbc0b27d51d83f40fe6fe8f5fa", "message": "DOC tweak release notes: promote response.follow, mention logging/stats changes"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9ce03d096d7ed569071a1951d9e327d7294e1a83", "message": "codecov config: disable project check, tweak PR comments"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/532400f993b0518438a6c2c22592876130b6e743", "message": "Merge pull request #2643 from harshasrinivas/set-retry-times-per-request\n\n[MRG+1] Add feature to set RETRY_TIMES per request (#2642)"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/1acacab93b6530baef273ccd5fe97b9ff871e0b3", "message": "Merge pull request #2741 from eliat123/2576_cleanup_MEMUSAGE_REPORT\n\n[FIX #2576] cleanup: removed unused MEMUSAGE_REPORT"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f0122e2cd94c3dbebe836670190fa6853221da66", "message": "Merge pull request #2729 from yandongxu/master\n\nFix doc: open file with \"wb\" mode will get an error in python 3"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/7fc11c13486ad47aaa007ec330e10075f2237064", "message": "Merge pull request #2720 from redapple/update-test-server-cert\n\nChange \"localhost\" test server certificate"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9731077a54978c375f386b6c156f9e74fe68cd03", "message": "Merge pull request #2683 from harshasrinivas/docs-SelectorList\n\n[MRG+1] Remove __nonzero__ from SelectorList docs"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9e04e77542292a38f6ab655119a39d57d030fc44", "message": "Merge pull request #2705 from redapple/docs-selectors-sections\n\nDOC Rearrange selector sections"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/8753b458fbef80a6962ae2dfa5082969ba6b7923", "message": "Merge pull request #2690 from redapple/faq-py3-windows\n\nFAQ Rewrite note on Python 3 support on Windows"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/34f4434b174404d49f63829aaa9dba500b500272", "message": "Merge pull request #2682 from harshasrinivas/compile-docs-open-webbrowser\n\n[MRG+1] Update Makefile to open webbrowser in MacOS (#2661)"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/ec55799d5e08e8c28f2af1b08491a8c9d692fd56", "message": "Merge pull request #2616 from redapple/mediapipeline-redirect-fix-continued\n\n[MRG] Allow redirections in media files downloads"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/4e4395f16f1b621d5f969b12229a7681acedad7d", "message": "Merge pull request #2644 from delftswa2017/fsfilestore_fixme_fix\n\n[MRG+1] Fixed the FIXME in FSFilesStore"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9c69e90056ab46cf5ba918790e44d3df9f2f92ad", "message": "Merge pull request #2632 from redapple/spider-loader-warn-or-fail\n\n[MRG] Add SPIDER_LOADER_WARN_ONLY to toggle between spiderloader failure or warning"}], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1331", "title": "Make sure RetryMiddleware logs ASCII-only exception strings", "body": "A fix for #1330. \nIt is better to understand why can exception messages contain weird characters before making this change.\n", "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1045", "title": "[WIP,DOC] Processors Overhaul", "body": "Hi,\n\nI think that scrapy.contrib.loader.processor should be separated from scrapy.contrib.loader. It should have its own docs and motivation. IMHO Scrapy [Item loaders](http://doc.scrapy.org/en/latest/topics/loaders.html) are hard to graps because they are documented in a top-bottom order: first docs show item loaders, then built-in processors are shown, and then an example of custom processors are given.\n\nI think that the following order is better:\n- show a complex non-composable data extraction example;\n- show how to make it better by using functions;\n- show how to combine functions declaratively using Compose and MapCompose;\n- show other scrapy built-in processors;\n- only then show ItemLoaders as a way to simplify populating of items.\n\nI've started to write a SEP to discuss the existing modules and their alternatives. The end result could be either a better docs for existing modules, or some changes to them, I'm not sure. SEP is very incomplete.\n\nThe SEP starts very opinionated; I'm going to remove \"I like / don't like\" parts after we discuss them.\n\nPlease let me know what do you think. \n\n//cc @nramirezuy @dangra @redapple @nyov @pablohoffman @Digenis and anyone interested.\n", "author_association": "OWNER"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/4948548", "body": "whoops, I clicked github's Edit button and thought it will become a PR\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4948548/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104443", "body": "For me it is 2014-01-18 :)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104443/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/7239242", "body": "shortcuts are working but classes themselves are not :)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/7239242/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/7239244", "body": "but at least they are importable now\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/7239244/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/7417716", "body": "remove a debug line?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/7417716/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/7430062", "body": "it is\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/7430062/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/10998639", "body": "what problems did it cause?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/10998639/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/19223839", "body": "Actually it repeats from step 1: start_requests are not sent to scheduler all in once, see https://github.com/scrapy/scrapy/issues/456\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/19223839/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5897966", "body": "this comparison is done once, at import time\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5897966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6374283", "body": "This will clear the existing setup.py file. I think it is wrong to clear it, because it is possible that setup.py is not auto-generated.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6374283/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6374692", "body": "Yes, setup.py is generated, but it is not overwritten if already present, because user can (and in many cases must) customize it.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6374692/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6376433", "body": "setup.py is not tied to scrapy/scrapyd; it is just a standard mean of creating setuptools distributions / eggs. Users could customize it to include data files or change the metadata.\n\nPlease correct me if I'm wrong, but the template is hardcoded in scrapy/scrapyd code. It is not a good idea to change the installed Scrapy version for multiple reasons: template can be shared with other scrapy projects, scrapy updates will overwrite the changes, these changes won't be in VCS, etc.\n\nMultiple deploy targets feature looks reasonable, but I haven't dug in to check how it iteracts with other scrapy/scrapyd parts - sorry, I don't have suggestions now.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6376433/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6392095", "body": "Some use cases for custom setup.py:\n- Add data files to the project. For example, this could be some supporting data files that spiders require, or pickled scikit-learn models for data extraction or classification, or even read-only sqlite database. They go under \"package_data\" setup.py key.\n- Custom scripts that should be available on scrapyd server; they go under \"scripts\" setup key. \n- Excluding some files from distribution; they can be excluded e.g. using 'exclude' argument to find_packages.\n\nThis is not an evidence, but all scrapy projects I touched last month had setup.py customized for one or more reasons from the above, and there could be reasons for using custom setup.py other than those 3. I see that this feature (custom setup.py files) is not widely used everywhere, but at least in some environments developers rely on it, so we can't just remove it without providing a (better) alternative.\n\nRe template changes: you're right that templates are not changed in this PR, but I was answering your question \"Couldn't they just edit the template in that case?\" -  templates are in scrapy so changing them requires changing installed scrapy.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6392095/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6952209", "body": "It is possible to instantiate Selector with `text` instead of `response`, so 'must' looks too strong.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6952209/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6952703", "body": "Kudos for getting it right! :) These methods return ascii-only bytestrings even if query and data is unicode, and this is right thing to do.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6952703/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7063412", "body": "I believe this import was intended: scrapy.utils.url was moved to w3lib.url and `import *` ensures this move doesn't break old code.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7063412/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7063418", "body": "what is it for?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7063418/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7063452", "body": "I'm not sure if this import was intended or not; maybe somebody is actially using `scrapy.utils.datatypes.defaultdict`.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7063452/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7080602", "body": "This will print a tuple in Python 2.x unless `from __future__ import print_function` import is in effect. There are similar issues later in this PR. Could you please review that? \n\nThere are 2 possible fixes: either use print with a single argument, i.e. `print(\"    %s\" % func.__doc__)`, or add `from __future__ import print_function` to the top of the file. Single-argument prints are better because to understand what are they doing there is no need to check if print_function future import is active. Future import has an another advantage: it raises an exception if one tries to use print with Python 3.x incompatile syntax.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7080602/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7080612", "body": "Hmm, I'm not sure if it is a good idea to fix bundled libraries. There is 2/3 compatible pydispatch library available - does anybody know why is pydispatch bundled? @pablohoffman @dangra ?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7080612/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7132681", "body": "this prints \"()\" instead of blank line in Python 2.x; one option is to use `print(\"\")`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7132681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7132708", "body": "this also prints \"()\" in 2.x\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7132708/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7132909", "body": "you're right!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7132909/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7133337", "body": "@pablohoffman also suggested that in private conversation :) I'm not a big fan of this idea, because \n\na) there are false positives, and\nb) 2to3 is of limited utility, and its purpose is to convert code to 3.x, not to add 3.x support.\n\nAbout false positives: for example, [here](https://github.com/scrapy/scrapy/pull/435/files#diff-b5bcf8c671c40b8e1227a4aa806f6839R94) touching map was unnecessary, and there could be methods named 'next' that are not related to iteration.\n\nSo I'd prefer working testing suite for detecting such issues; we can make 'overall' cleanup first (without caring much if some PR will break it) and then start porting modules one by one by making their tests green.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7133337/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7133438", "body": "I prefer relying only on tests not because I think extra sanity checks are bad, but because I think this work is unnecessary. If you think it is easy to add some checks then +1 for including them. They can be added to https://github.com/scrapy/scrapy/blob/master/bin/runtests.sh script.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7133438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7133657", "body": "@alexcepoi test requirements are here: https://github.com/scrapy/scrapy/tree/master/.travis - they are used automatically by Travis and tox.\n\nScrapy supports distutils, so install_requirements in setup.py isn't always working; `requirements.txt` + pip could be used instead.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7133657/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7395144", "body": "Usually when string must be a bytestring in 2.x and unicode string in 3.x it is easier to write `__version__ = str(pkgutil.get_data(__package__, 'VERSION').strip())`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7395144/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7395355", "body": "ah, i see\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7395355/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7800389", "body": "I think it is better to also document `item` parameter here\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7800389/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7800478", "body": "This makes an example a bit less clear: now it is not obvious if there could be 2 `add_xpath` or `add_css` calls with the same name or not.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7800478/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7800522", "body": "I think it is better to illustrate that with two `add_css` or two `add_xpath` calls, or maybe 3 calls (e.g. 2xCSS + 1xXPath).\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7800522/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7811615", "body": "By the way, what's the reason exception is not raised in this case?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7811615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7811846", "body": "this makes sense\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7811846/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8406789", "body": "Hey @dangra!\n\nWe'll have to review all occurrences of str/unicode to add python3 support; this one would be likely replaced with isinstance(six.string_types). So this is not a problem. Checking for Request instead of checking for str/unicode would make porting a bit easier, but that's fine not to worry about that now.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8406789/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8498123", "body": "No advantage. I blindly copied this code from MetaRefreshMiddleware (to make it consistent), but in MetaRefreshMiddleware it serves a purpose (MetaRefreshMiddleware is a subclass of BaseRedirectMiddleware and overrides this attribute), and here it is pointless. I'll fix it. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8498123/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8408758", "body": "We can use `self.assertIn` because we dropped 2.6 support. I believe the advantage is a nicer failure message.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8408758/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8408980", "body": "This test doesn't check that passing None unsets a header (main motivation for #473) because X-Unset header is not set.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8408980/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8542970", "body": "sets\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8542970/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8542971", "body": "sets\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8542971/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574697", "body": "I only changed formatting here (line was too long), sorry for noise - other changes have nothing to do with autothrottle extension.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574697/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574702", "body": "so we should fix autothrottle docs also?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574702/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574725", "body": "I used vague \"website\" intentionally - it could mean either \"domain\" or \"ip\"; this is clarified later.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574725/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574762", "body": "+1. I don't like how `CONCURRENT_REQUESTS_PER_IP` affects download delay. Maybe simply `CONCURRENT_REQUESTS`? We don't have `DOWNLOAD_DELAY_PER_SLOT`.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574762/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574811", "body": "- `CONCURRENT_REQUESTS`: user can think it is a global limit (per-spider?)\n- `CONCURRENT_REQUESTS_PER_SLOT`: what is slot? \n- old names: it is clear what are they for - they are for limiting concurrency per domain or per IP.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574811/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574981", "body": "Learning how exactly downloader enqueues requests is a more advanced topic than learning how not to hit remote websites too hard. From this point of view `CONCURRENT_REQUESTS_PER_SLOT` + `SLOT_ASSIGNMENT_POLICY` are worse than `CONCURRENT_REQUESTS_PER_IP` + `CONCURRENT_REQUESTS_PER_DOMAIN`, even if they match internals better - this is what bugs me.\n\nIndeed, there is already `CONCURRENT_REQUESTS` :)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574981/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8572237", "body": "Good idea about computing magic number; I'll add it, and some tests as well. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8572237/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8572242", "body": ".. if you're ok with this approach\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8572242/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8588839", "body": "See previous line. I've done this in a more verbose way to preserve a reference to empty dict. Not sure if it is meaningful though (i.e. can updates propogate to class dict after creation?).\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8588839/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8892480", "body": "I think this method does not belong to BaseDupeFilter, because dupe filters may not use request fingerprints, and it is Request Fingerprint duplicates filter (`RFPDupeFilter`) who needs request fingerprint. Let's just remove this method from here. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8892480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8892538", "body": "I think this docstring is meaningless - it doesn't tell what is method doing, and it is obvious that you can override a method. I'd just remove the docstring; this would also make code more consistent with other code in this module.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8892538/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8892795", "body": "I don't have strong preference. If you like it, then let's keep it (with a formatting fixed - it's better to remove extra lines). \n\nI think the main extension point is still `request_seen` method, so if we add a comment here it's better to add comment to `BaseDupeFilter.request_seen` method as well.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8892795/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896059", "body": "There is a conflict, but only in class definition scope (not in scope of methods themselves). \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896059/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896122", "body": "If class (not instance) is already created there are `request_fingerprint` and `self.request_fingerprint`, and they are different.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896528", "body": "Hmm, right. Technicaly this paragraph was correct. But I'm reading \"crawling process\" as \"OS process where crawling happens\". The remark was important in past, because when many spiders are executed in a single process they can affect each other, and checking which spider causes a leak was complicated; later chapters explained how to find which spider caused a leak. \n\nBut I don't see a point of keeping this paragraph now - of course, if a process uses a lot of memory it can affect other processes executed on a same machine, but it is trivial, and it has nothing to do with big projects.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8897185", "body": "Old docs followed this canvas:\n1. There could be many spiders in big projects, some of them can be leaking.\n2. We provide useful tools to debug leaks, \"which quite often consist in an answer to the question: which spider is leaking?\"\n\nI think (1) was justified by (2). But since (2) was changed to just \"we provide useful tools\" (1) is no longer justified here even if it is technically correct. \n\nIt reads a bit funny if we leave it :) Chapter is called \"Common causes of memory leaks\", and we list these causes in this chapter:\n\na) putting objects somewhere to Request;\nb) bad developers (\"different people\");\nc) improper resources handling in extensions, pipelines, middlewares.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8897185/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8897790", "body": "You're right that it is important to deallocate allocated resources. But it can't cause memory leaks now, so this sentence is not helpful in debugging existing memory leaks, is it? I could add it back to make docs more future-proof.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8897790/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8968518", "body": "crawl\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8968518/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8968560", "body": "RFPDupeFilter was changed, we don't have DupeFilter class\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8968560/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9009462", "body": "I think it is better to use assertTrue/assertFalse here: \n`self.assertFalse(any(hasattr(e._root, 'getparent') for e in div_class))`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9009462/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9010071", "body": "does it work without `;`?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9010071/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9010183", "body": "I think this warning should be reformulated, because if user followed the previous command `apt-get update` will do nothing.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9010183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9010461", "body": "One idea:\n\n```\n .. note:: Please note that these packages are updated frequently, so we're\n    using ``apt-get update`` to update package list first.\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9010461/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9011255", "body": "I like all your options (including \"remove this note\").\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9011255/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9224972", "body": "Can we use a more specific exception here? At least `Exception` - using bare `except:` is rarely a good idea because it catches KeyboardInterrupt and SystemExit.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9224972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9224991", "body": "I haven't looked at this in details, but maybe move stats enabling to from_crawler method? \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9224991/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9225051", "body": "But it seems we can avoid this by enabling stats only when spider is created with a crawler.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9225051/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "andywubit": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2994", "title": "scrapy FormRequest  ignore Content-Type other values", "body": "The following code\r\n```\r\nsuper(FormRequest, self).__init__(*args, **kwargs)\r\nif formdata:\r\n    items = formdata.items() if isinstance(formdata, dict) else formdata\r\n    querystr = _urlencode(items, self.encoding)\r\n    if self.method == 'POST':\r\n        self.headers.setdefault('Content-Type', 'application/x-www-form-urlencoded')\r\n        self._set_body(querystr)\r\n    else:\r\n        self._set_url(self.url + ('&' if '?' in self.url else '?') + querystr)\r\n```\r\nAlthough the FormRequest base class Request sets the request header based on the value of the headers band, such as setting Content-Type = \"application / json; charset = utf-8\", the code \r\n```\r\n self.headers.setdefault('Content-Type', 'application/x-www-form-urlencoded')\r\n```\r\nsets the Content- Type value back to \"application / x -www-form-urlencoded \". He did not consider headers own Content-Type situation.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2994/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "netcaf": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2977", "title": "Response data is not released after processing?", "body": "Hello,\r\n\r\ncode sample:\r\n```\r\ndef start_requests(self):\r\n    for url in url_list:\r\n            yield scrapy.Request(url=url, headers = xxx, callback=self.parse, errback=None,dont_filter=False)\r\n\r\ndef parse(self, response):\r\n    ...\r\n    response.xpath('//*[@id=\"feed-main-list\"]/li')\r\n    ...\r\n```\r\nIf we call selector related function like xpath(...), the response data will not be released after parsing of each url.\r\n\r\nIn scrapy/http/response/text.py, \r\n```\r\nclass TextResponse(Response):\r\n...\r\n    def selector(self):\r\n        from scrapy.selector import Selector\r\n        if self._cached_selector is None:\r\n            self._cached_selector = Selector(self)\r\n        return self._cached_selector\r\n\r\n    def xpath(self, query, **kwargs):\r\n        return self.selector.xpath(query, **kwargs)\r\n```\r\nIt has the cyclic reference like response -> Selector -> response.\r\nIn my rough test, if i don't use self._cached_selector to keep Selector object, the memory usage will reduce from 6x% -> 4x%. \r\n\r\nSo, am i right? or how to understand it?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2977/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2969", "title": "DOWNLOAD_DELAY and ctrl-c", "body": "Hello,\r\nIf we set DOWNLOAD_DELAY for a list of urls,\r\nIt may wait too long to terminate the job for the first cancel operation.\r\nCan we improve it?\r\nFor example, wake up the sleeping spider(DOWNLOAD_DELAY) and let it respond to cancel operation.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2969/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "D-Kalck": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2973", "title": "bindaddress doesn't bind IP when getting robots.txt", "body": "I have several failover IPs that are well configured (they work with wget or curl), and I would like to bind them when I use Scrapy, so I use the bindaddress key to achieve this, but the public IP is still the main IP.\r\n\r\n    # -*- coding: utf-8 -*-\r\n    import scrapy\r\n    import random\r\n    #https://ipinfo.io/ip\r\n    \r\n    class SpiderTest(scrapy.Spider):\r\n        name = \"failover\"\r\n    \r\n        def start_requests(self):\r\n            url = 'https://ipinfo.io/ip'\r\n            bind_addresses = ['BBB.BBB.BBB.BBX', 'BBB.BBB.BBB.BBY', 'BBB.BBB.BBB.BBZ']\r\n            bind_address = random.choice(bind_addresses)\r\n            print('Bind :      ' + bind_address)\r\n            request = scrapy.Request(url=url,\r\n                                 callback=self.test,\r\n                                 meta={\"bindaddress\": (bind_address, 0)})\r\n            yield request\r\n    \r\n        def test(self, response):\r\n            html = response.body\r\n            print('Public IP : ' + html.decode('ascii'))\r\n\r\nI tried also :\r\n\r\n    request.meta['bindaddress'] = (bind_address, 0)\r\nor\r\n\r\n    request.meta['bindaddress'] = bind_address\r\n\r\nAn excerpt from ip addr :\r\n\r\n    2: ens3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\r\n        link/ether xx:xx:xx:xx:xx:xx brd ff:ff:ff:ff:ff:ff\r\n        inet AAA.AAA.AAA.AAA/32 brd AAA.AAA.AAA.AAA scope global ens3\r\n           valid_lft forever preferred_lft forever\r\n        inet BBB.BBB.BBB.BBX/32 brd BBB.BBB.BBB.BBX scope global ens3:0\r\n           valid_lft forever preferred_lft forever\r\n        inet BBB.BBB.BBB.BBY/32 brd BBB.BBB.BBB.BBY scope global ens3:1\r\n           valid_lft forever preferred_lft forever\r\n        inet BBB.BBB.BBB.BBZ/32 brd BBB.BBB.BBB.BBZ scope global ens3:2\r\n           valid_lft forever preferred_lft forever\r\n\r\nMy environment :\r\nDebian 9, Python 3.5.3, Scrapy 1.4.0 and Twisted 17.9.0 (tested with Twisted 17.5.0)", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2973/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tuangeek": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2970", "title": "Override \"Connection\" in headers will result in two \"Connection\" in headers.", "body": "If I try to override the \"Connection\" in the headers with either custome_settings\r\n```\r\ncustom_settings = {\r\n        'DEFAULT_REQUEST_HEADERS': {\r\n            'Connection' : 'Keep-Alive'\r\n        }\r\n}\r\n```\r\nor within the request headers\r\n```\r\nheaders = {}\r\n headers['Connection'] : 'Keep-Alive'\r\n headers['User-Agent'] : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\r\n\r\nfor url in urls:\r\n     yield scrapy.Request(url, headers=headers, callback = self.parse)\r\n```\r\n\r\nIt will result with a request with two \"Connection\"s in the header.\r\n\r\n```\r\nGET http://example.com HTTP/1.1\r\nConnection: close\r\nConnection: Keep-Alive\r\nUser-Agent: Scrapy/1.4.0 (+http://scrapy.org)\r\nAccept-Encoding: gzip,deflate\r\n```", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2970/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Anton24322224": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2967", "title": "<a> element has no href attribute", "body": "So, why does scrapy raise ValueError when you gave `<a>` tag without href to response.follow(), why not just ignore those?\r\nAs far as my understanding goes a tags without href is still completely valid, so this behaviour looks strange to me.\r\n\r\n\r\n> [The href attribute on a and area elements is not required; when those elements do not have href attributes they do not create hyperlinks.](https://www.w3.org/TR/html/links.html#element-attrdef-a-href)", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2967/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pattywgm": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2966", "title": "ERROR: Unable to send mail: To=['wgm@163.com'] Cc=[] Subject=\"Scrapy Error Alarm\" Attachs=0- object of type 'module' has no len()", "body": "I caught this error when I wanted to send email with scrapy.mail.MailSender.\r\n[twisted] CRITICAL: Unhandled Error\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/python/log.py\", line 103, in callWithLogger\r\n    return callWithContext({\"system\": lp}, func, *args, **kw)\r\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/python/log.py\", line 86, in callWithContext\r\n    return context.call({ILogContext: newCtx}, func, *args, **kw)\r\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/python/context.py\", line 122, in callWithContext\r\n    return self.currentContext().callWithContext(ctx, func, *args, **kw)\r\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/python/context.py\", line 85, in callWithContext\r\n    return func(*args,**kw)\r\n--- <exception caught here> ---\r\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/posixbase.py\", line 597, in _doReadOrWrite\r\n    why = selectable.doRead()\r\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/tcp.py\", line 208, in doRead\r\n    return self._dataReceived(data)\r\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/tcp.py\", line 214, in _dataReceived\r\n    rval = self.protocol.dataReceived(data)\r\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/protocols/tls.py\", line 330, in dataReceived\r\n    self._flushReceiveBIO()\r\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/protocols/tls.py\", line 295, in _flushReceiveBIO\r\n    ProtocolWrapper.dataReceived(self, bytes)\r\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/protocols/policies.py\", line 120, in dataReceived\r\n    self.wrappedProtocol.dataReceived(data)\r\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/protocols/basic.py\", line 571, in dataReceived\r\n    why = self.lineReceived(line)\r\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/mail/smtp.py\", line 995, in lineReceived\r\n    why = self._okresponse(self.code, b'\\n'.join(self.resp))\r\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/mail/smtp.py\", line 1044, in smtpState_to\r\n    return self.smtpState_toOrData(0, b'')\r\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/mail/smtp.py\", line 1062, in smtpState_toOrData\r\n    self.sendLine(b'RCPT TO:' + quoteaddr(self.lastAddress))\r\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/mail/smtp.py\", line 179, in quoteaddr\r\n    res = email.utils.parseaddr(addr)\r\n  File \"/usr/lib/python2.7/email/utils.py\", line 214, in parseaddr\r\n    addrs = _AddressList(addr).addresslist\r\n  File \"/usr/lib/python2.7/email/_parseaddr.py\", line 457, in __init__\r\n    self.addresslist = self.getaddrlist()\r\n  File \"/usr/lib/python2.7/email/_parseaddr.py\", line 217, in getaddrlist\r\n    while self.pos < len(self.field):\r\nexceptions.TypeError: object of type 'module' has no len()\r\n\r\n## The environment is scrapy 1.4.0 , twisted 17.5.0.\r\n## But it was succeed in my own pc with scrapy 1.4.0 , twisted 13.1.0.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "grammy-jiang": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2960", "title": "Class Request without the method __eq__ can't be compared directly in the unittest", "body": "I met a interesting failure when I did a unittest about the method `process_spider_exception` of the `spider middleware`:\r\n\r\nIn my project, this method returns a iterable (list) of request objects, which will be sent to the method `process_spider_output`. Now I want to assert this iterable (list) of request objects with `assertEqual` or `assertSequence`, but the test is always failed. Then I debug my code, also with the test case, then I realized that: Class `Request` has no method `__eq__`, so python will compare the `id` instead, and in my middleware the request is modified with `replace` method, definitely not the previous one, the test will never return `True`.\r\n\r\nMy solution is to compare `__dict__` of each pair requests in a loop, as mentioned in the following code:\r\n\r\n**spider middleware**\r\n\r\n```python\r\nclass BlockInspector(object):\r\n    def __init__(self, crawler):\r\n        self.crawler = crawler\r\n        self.settings = crawler.settings\r\n        self.signal_manager = crawler.signals\r\n        self.stats = crawler.stats\r\n\r\n        self.inspect_block = load_object(\r\n            self.settings.get('BLOCK_INSPECTOR_SPIDERMIDDLEWARE'))\r\n        if self.settings.get('RECYCLE_BLOCK_REQUEST_SPIDERMIDDLEWARE'):\r\n            self.recycle_block_request = load_object(\r\n                self.settings.get('RECYCLE_BLOCK_REQUEST_SPIDERMIDDLEWARE'))\r\n        else:\r\n            self.recycle_block_request = lambda x: x\r\n        self.block_signals = list(map(\r\n            lambda x: load_object(x),\r\n            self.settings.get('BLOCK_SIGNALS_SPIDERMIDDLEWARE')))\r\n\r\n    @classmethod\r\n    def from_crawler(cls, crawler):\r\n        obj = cls(crawler)\r\n        return obj\r\n\r\n    def process_spider_input(self, response, spider):\r\n        if self.inspect_block(response):\r\n            self.stats.inc_value('block_inspector/block', spider=spider)\r\n            raise BlockException(response)\r\n\r\n    def process_spider_exception(self, response, exception, spider):\r\n        if isinstance(exception, BlockException):\r\n            _ = list(map(\r\n                lambda x: self.signal_manager.send_catch_log(\r\n                    x, spider=spider, response=response, exception=exception),\r\n                self.block_signals))\r\n\r\n            r = response.request.replace(dont_filter=True)\r\n\r\n            return [self.recycle_block_request(r)]\r\n```\r\n\r\n**test case**\r\n\r\n```python\r\nclass TestBlockInspector(TestCase):\r\n    def setUp(self):\r\n        crawler = get_crawler(Spider, settings_dict={\r\n            'BLOCK_INSPECTOR_SPIDERMIDDLEWARE': 'tests.test_spidermiddleware_block_inspector._block_inspector',\r\n            # TODO: add signals and test cases\r\n            'BLOCK_SIGNALS_SPIDERMIDDLEWARE': [],\r\n            'RECYCLE_BLOCK_REQUEST_SPIDERMIDDLEWARE': 'tests.test_spidermiddleware_block_inspector._recycle_block_request'\r\n        })\r\n\r\n        self.spider = Spider.from_crawler(crawler, name='foo')\r\n        self.mw = BlockInspector(crawler)\r\n        self.req = Request('http://scrapytest.org')\r\n        self.res_succeed, self.res_block = _responses(self.req)\r\n\r\n    def test_process_spider_input(self):\r\n        self.assertEqual(\r\n            None,\r\n            self.mw.process_spider_input(self.res_succeed, self.spider))\r\n        self.assertRaises(\r\n            BlockException,\r\n            self.mw.process_spider_input, self.res_block, self.spider)\r\n\r\n    def test_process_spider_exception_default(self):\r\n        crawler = get_crawler(Spider, settings_dict={\r\n            'BLOCK_INSPECTOR_SPIDERMIDDLEWARE': 'tests.test_spidermiddleware_block_inspector._block_inspector',\r\n            # TODO: add signals and test cases\r\n            'BLOCK_SIGNALS_SPIDERMIDDLEWARE': []})\r\n\r\n        spider = Spider.from_crawler(crawler, name='foo')\r\n        mw = BlockInspector(crawler)\r\n\r\n        for req, req_after_exception in zip(\r\n                [self.req.replace(dont_filter=True)],\r\n                mw.process_spider_exception(\r\n                    self.res_block, BlockException(self.res_block),\r\n                    spider)):\r\n            self.assertEqual(req.__dict__, req_after_exception.__dict__)\r\n\r\n    def test_process_spider_exception(self):\r\n        # this will get test failure:\r\n        # self.assertEqual(\r\n        #     [self.req.replace(dont_filter=True)],\r\n        #     self.mw.process_spider_exception(\r\n        #         self.res_block, BlockException(self.res_block), self.spider))\r\n        # self.assertSequenceEqual(\r\n        #     [self.req.replace(dont_filter=True)],\r\n        #     self.mw.process_spider_exception(\r\n        #         self.res_block, BlockException(self.res_block), self.spider))\r\n\r\n        # this will work:\r\n        for req, req_after_exception in zip(\r\n                [self.req.replace(dont_filter=True)],\r\n                self.mw.process_spider_exception(\r\n                    self.res_block, BlockException(self.res_block),\r\n                    self.spider)):\r\n            self.assertEqual(req.__dict__, req_after_exception.__dict__)\r\n```\r\n\r\nI have gone through the test cases in scrapy source code, and there is no one just like my situation.\r\n\r\nIs it a good idea to add the method `__eq__` to class `request`?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2960/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2937", "title": "IPython version incompatible problem in tests of python 2.7", "body": "The version of IPython in tests/requirements.txt is not fixed to less then 6.0, so when testing is running under python 2.7, there will be an error on the requirement installing stage.\r\n\r\nPlease refer to [Installation \u2014 IPython 6.2.0 documentation](https://ipython.readthedocs.io/en/stable/install/index.html):\r\n\r\n> This documentation covers IPython versions 6.0 and higher. Beginning with version 6.0, IPython stopped supporting compatibility with Python versions lower than 3.3 including all versions of Python 2.7.\r\nIf you are looking for an IPython version compatible with Python 2.7, please use the IPython 5.x LTS release and refer to its documentation (LTS is the long term support release).\r\n\r\nSo it should be like this in tests/requirements.txt:\r\n`ipython<6.0`", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Dainius-P": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2944", "title": "sslv3 alert handshake failure", "body": "I have been getting sslv3 errors in scrapy after updating it.\r\n`2017-09-11 17:33:20 [scrapy.core.engine] INFO: Spider opened\r\n2017-09-11 17:33:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2017-09-11 17:33:20 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6025\r\n2017-09-11 17:33:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://api.citypantry.com/packages/search?page=1&sortBy=relevance> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'sslv3 alert handshake failure')]>]`\r\n\r\nVersions:\r\n`Scrapy    : 1.4.0\r\nlxml      : 3.8.0.0\r\nlibxml2   : 2.9.3\r\ncssselect : 1.0.1\r\nparsel    : 1.2.0\r\nw3lib     : 1.18.0\r\nTwisted   : 17.5.0\r\nPython    : 2.7.12+ (default, Sep 17 2016, 12:08:02) - [GCC 6.2.0 20160914]\r\npyOpenSSL : 17.2.0 (OpenSSL 1.1.0f  25 May 2017)\r\nPlatform  : Linux-4.8.0-59-generic-x86_64-with-Ubuntu-16.10-yakkety`\r\n\r\nThe same thing happens while using python 3.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2944/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Chratho": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2940", "title": "Give `BaseDupeFilter` access to spider-object", "body": "I am in a situation where a single item gets defined over a sequence of multiple pages, passing values between the particular callbacks using the `meta`-dict. I believe this is a common approach among scrapy-users.\r\n\r\nHowever, it feels like this approach is difficult to get right. With the default implementation of `RFPDupefilter`, my callback-chain is teared apart quite easy, as fingerprints don't take the meta-dict into account. The corresponding requests are thrown away, the information in the meta-dict which made this request unique is lost.\r\n\r\nI have currently implemented by own meta-aware DupeFilter, but I am still facing the problem that it lacks access to the specific spider in use - and only the Spider really knows the meta-attributes that make a request unique. I could now take it a step further and implement my own scheduler, but I'm afraid that all these custom extensions make my code very brittle wrt future versions of scrapy.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2940/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "infostash": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2933", "title": "Overwrite settings in image pipeline?", "body": "Hello,\r\nfrom the documentation it is not clear to me how to change IMAGES_STORE path inside a pipeline.\r\nI am scraping two different images which shall be stored in different paths.\r\nSo my idea was to write for each image an extra pipeline and overwrite the images_store path.\r\nIs that even possible? Or is there another method to change the path depending on which image i am downloading? ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2933/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "talhashraf": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2919", "title": "FormRequest.formdata with GET method duplicates same key in query string", "body": "```\r\n>>> from scrapy import FormRequest\r\n>>> FormRequest('http://example.com/?id=9', method='GET', formdata={'id': '8'}).url\r\n'http://example.com/?id=9&id=8'\r\n```\r\nNotice how `id` is being duplicated in query string. It must replace the value if same key is found. I believe this can be handled in [form.py#L36](https://github.com/scrapy/scrapy/blob/master/scrapy/http/request/form.py#L36).", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2919/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "unk2k": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2916", "title": "SSL website. `twisted.internet.error.ConnectionLost`", "body": "Hi everybody!\r\nI catch this error on both OS. This HTTPS site can't be downloaded via scrapy (twisted). I looked on this issue board and I don't found solution.\r\n\r\nBoth: Debian 9 / Mac OS\r\n```\r\n$ scrapy shell \"https://wwwnet1.state.nj.us/\"\r\n2017-09-07 16:23:02 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot)\r\n2017-09-07 16:23:02 [scrapy.utils.log] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter'}\r\n2017-09-07 16:23:02 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.corestats.CoreStats']\r\n2017-09-07 16:23:02 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2017-09-07 16:23:02 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2017-09-07 16:23:03 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2017-09-07 16:23:03 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\r\n2017-09-07 16:23:03 [scrapy.core.engine] INFO: Spider opened\r\n2017-09-07 16:23:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://wwwnet1.state.nj.us/> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n2017-09-07 16:23:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://wwwnet1.state.nj.us/> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n2017-09-07 16:23:04 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://wwwnet1.state.nj.us/> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\nTraceback (most recent call last):\r\n  File \"scrapy\", line 11, in <module>\r\n    sys.exit(execute())\r\n  File \"/lib/python3.5/site-packages/scrapy/cmdline.py\", line 149, in execute\r\n    _run_print_help(parser, _run_command, cmd, args, opts)\r\n  File \"/lib/python3.5/site-packages/scrapy/cmdline.py\", line 89, in _run_print_help\r\n    func(*a, **kw)\r\n  File \"/lib/python3.5/site-packages/scrapy/cmdline.py\", line 156, in _run_command\r\n    cmd.run(args, opts)\r\n  File \"/lib/python3.5/site-packages/scrapy/commands/shell.py\", line 73, in run\r\n    shell.start(url=url, redirect=not opts.no_redirect)\r\n  File \"/lib/python3.5/site-packages/scrapy/shell.py\", line 48, in start\r\n    self.fetch(url, spider, redirect=redirect)\r\n  File \"/lib/python3.5/site-packages/scrapy/shell.py\", line 115, in fetch\r\n    reactor, self._schedule, request, spider)\r\n  File \"/lib/python3.5/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\r\n    result.raiseException()\r\n  File \"/lib/python3.5/site-packages/twisted/python/failure.py\", line 385, in raiseException\r\n    raise self.value.with_traceback(self.tb)\r\ntwisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n```\r\n\r\nMac OSx:\r\n```\r\n$ scrapy version -v\r\nScrapy    : 1.4.0\r\nlxml      : 3.8.0.0\r\nlibxml2   : 2.9.4\r\ncssselect : 1.0.1\r\nparsel    : 1.2.0\r\nw3lib     : 1.18.0\r\nTwisted   : 17.9.0rc1\r\nPython    : 3.5.1 (default, Jan 22 2016, 08:54:32) - [GCC 4.2.1 Compatible Apple LLVM 7.0.2 (clang-700.1.81)]\r\npyOpenSSL : 17.2.0 (OpenSSL 1.1.0f  25 May 2017)\r\nPlatform  : Darwin-16.7.0-x86_64-i386-64bit\r\n```\r\n\r\nDebian 9:\r\n```\r\n$ scrapy version -v\r\nScrapy    : 1.4.0\r\nlxml      : 3.8.0.0\r\nlibxml2   : 2.9.3\r\ncssselect : 1.0.1\r\nparsel    : 1.2.0\r\nw3lib     : 1.18.0\r\nTwisted   : 17.9.0rc1\r\nPython    : 3.4.2 (default, Oct  8 2014, 10:45:20) - [GCC 4.9.1]\r\npyOpenSSL : 17.2.0 (OpenSSL 1.1.0f  25 May 2017)\r\nPlatform  : Linux-3.16.0-4-amd64-x86_64-with-debian-8.7\r\n```\r\n\r\n\r\nMac OSx:\r\n```\r\n$ openssl s_client -connect wwwnet1.state.nj.us:443 -servername wwwnet1.state.nj.us\r\nCONNECTED(00000003)\r\n140736760988680:error:140790E5:SSL routines:ssl23_write:ssl handshake failure:s23_lib.c:177:\r\n---\r\nno peer certificate available\r\n---\r\nNo client certificate CA names sent\r\n---\r\nSSL handshake has read 0 bytes and written 336 bytes\r\n---\r\nNew, (NONE), Cipher is (NONE)\r\nSecure Renegotiation IS NOT supported\r\nCompression: NONE\r\nExpansion: NONE\r\nNo ALPN negotiated\r\nSSL-Session:\r\n    Protocol  : TLSv1.2\r\n    Cipher    : 0000\r\n    Session-ID: \r\n    Session-ID-ctx: \r\n    Master-Key: \r\n    Key-Arg   : None\r\n    PSK identity: None\r\n    PSK identity hint: None\r\n    SRP username: None\r\n    Start Time: 1504790705\r\n    Timeout   : 300 (sec)\r\n    Verify return code: 0 (ok)\r\n---\r\n```\r\n\r\nDebian 9:\r\n```\r\nCONNECTED(00000003)\r\n---\r\nCertificate chain\r\n 0 s:/C=US/ST=New Jersey/L=Trenton/O=New Jersey State Government/OU=E-Gov Services - wwwnet1.state.nj.us/CN=wwwnet1.state.nj.us\r\n   i:/C=US/O=Symantec Corporation/OU=Symantec Trust Network/CN=Symantec Class 3 Secure Server SHA256 SSL CA\r\n---\r\nServer certificate\r\n-----BEGIN CERTIFICATE-----\r\n<cut out>\r\n-----END CERTIFICATE-----\r\n<cut out>\r\n---\r\nNo client certificate CA names sent\r\n---\r\nSSL handshake has read 1724 bytes and written 635 bytes\r\n---\r\nNew, TLSv1/SSLv3, Cipher is DES-CBC3-SHA\r\nServer public key is 2048 bit\r\nSecure Renegotiation IS NOT supported\r\nCompression: NONE\r\nExpansion: NONE\r\nSSL-Session:\r\n    Protocol  : TLSv1\r\n    Cipher    : DES-CBC3-SHA\r\n    Session-ID: 930F00007F5944DC3C6010F96E95E7FA63656EF5EA35508B055078CEC249DC38\r\n    Session-ID-ctx:\r\n    Master-Key: 27B02D427F006A57B121CCEFEAA7F33B870DE262848BB6F851242F48F051ABB77BA4ED06706766EE8EE55F6643C9FF55\r\n    Key-Arg   : None\r\n    PSK identity: None\r\n    PSK identity hint: None\r\n    SRP username: None\r\n    Start Time: 1504790821\r\n    Timeout   : 300 (sec)\r\n    Verify return code: 21 (unable to verify the first certificate)\r\n---\r\n```\r\n\r\nThanks you for your time.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ollieglass": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2912", "title": "Download delay and autothrottle do not work when cache is enabled", "body": "Enabling the cache by setting `HTTPCACHE_ENABLED = True` causes any download delay or autothrottle settings to be ignored.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2912/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sseyboth": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2905", "title": "An error occurred while connecting: [Failure instance: Traceback: <class 'ValueError'>: filedescriptor out of range in select()", "body": "I'm trying crawl ~200k sites, only the home pages. In the beginning the crawl works fine but the logs quickly fill up with the following errors:\r\n\r\n2017-08-29 11:18:55,131 - scrapy.core.scraper - ERROR - Error downloading <GET http://axo-suit.eu>\r\nTraceback (most recent call last):\r\n  File \"venv/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1384, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"venv/lib/python3.6/site-packages/twisted/python/failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"venv/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectError: An error occurred while connecting: [Failure instance: Traceback: <class 'ValueError'>: filedescriptor out of range in select()\r\nvenv/lib/python3.6/site-packages/twisted/internet/base.py:1243:run\r\nvenv/lib/python3.6/site-packages/twisted/internet/base.py:1255:mainLoop\r\nvenv/lib/python3.6/site-packages/twisted/internet/selectreactor.py:106:doSelect\r\nvenv/lib/python3.6/site-packages/twisted/internet/selectreactor.py:88:_preenDescriptors\r\n--- <exception caught here> ---\r\nvenv/lib/python3.6/site-packages/twisted/internet/selectreactor.py:85:_preenDescriptors\r\n].\r\n\r\nlsof shows that the process indeed has >1024 open network connections, which I believe is the limit for select().\r\n\r\nI set CONCURRENT_REQUESTS = 100 and REACTOR_THREADPOOL_MAXSIZE = 20 based on https://doc.scrapy.org/en/latest/topics/broad-crawls.html.\r\n\r\nNot sure how the crawl ends up with so many open connections. Maybe it's leaking filedescriptors somewhere?\r\n\r\nI'm using:\r\nPython 3.6.2\r\nScrapy 1.4.0\r\nTwisted 17.5.0\r\nmacOS Sierra 10.12.6\r\n\r\nHappy to provide more info as needed.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2905/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2727", "title": "Bring back docstring comments in CrawlSpider", "body": "Can we please bring back the docstring comments in CrawlSpider that were removed in https://github.com/scrapy/scrapy/commit/e2290a5359ee80c75d8bcf8de8b6894e61fad83a? The code has zero comments now. Just spent two hours pulling my hair out trying to pass extra params down the crawler. Until I found the removed docstrings in this commit that where hugely helpful :) Thanks!", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2727/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lolobosse": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2900", "title": "_get_sitemap_body() from SitemapSpider is not compatible with all sitemaps", "body": "Hi there,\r\n\r\nHave a look at the following code:\r\n\r\n```python\r\nclass [PlaceHolder]nanzeigen(SitemapSpider):\r\n    name = '[PlaceHolder]nanzeigen.de '\r\n\r\n    sitemap_urls=['https://www.[PlaceHolder]nanzeigen.de/sitemap.xml']\r\n\r\n    sitemap_rules = [\r\n        ('[PlaceHolder]nanzeigen.de', 'parse_page'),\r\n    ]\r\n```\r\n\r\n**REPLACE [PlaceHolder] BY 'stelle'**\r\n\r\nIt will return `Ignoring invalid sitemap` for all 2nd level sitemaps because this server returns that the content is html (whereas it is XML indeed): `Content-Type: ['text/html; charset=utf-8']` \r\n\r\nTo me, it sounds reasonable to check if the response can be parsed in XML and if not then return that the body is not a sitemap.\r\n\r\nSomething like this:\r\n\r\n```python\r\ndef _get_sitemap_body(self, response):\r\n        \"\"\"Return the sitemap body contained in the given response,\r\n        or None if the response is not a sitemap.\r\n        \"\"\"\r\n        if isinstance(response, XmlResponse):\r\n            return response.body\r\n        elif is_gzipped(response):\r\n            return gunzip(response.body)\r\n        elif response.url.endswith('.xml'):\r\n            return response.body\r\n        elif response.url.endswith('.xml.gz'):\r\n            return gunzip(response.body)\r\n        else:\r\n            try:\r\n              #Parse the sitemap in XML\r\n              #Return response.body\r\n            except:\r\n               #return None\r\n```\r\n\r\nIs that something which could be considered as doable?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2900/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "djunzu": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2893", "title": "Feature suggest: add downloaded/uptodate status to information about downloaded media", "body": "Currently File/Image pipelines populate `files`/`images` fields with dicts containing information about the downloaded files (the downloaded path, the original scraped url, and the file checksum). It would be useful to have downloaded/uptodate status in this dict ([motivation](https://stackoverflow.com/questions/45823196/scrapy-how-to-get-status-of-file-download)).\r\n\r\nIt goes along with other features requests such as having width/height of images also in the dict output.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2893/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2602", "title": "Misleading error message about SPIDER_MODULES", "body": "This is a side effect from #2433.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/djunzu/.pyenv/versions/2.7.13/lib/python2.7/site-packages/scrapy/spiderloader.py\", line 31, in _load_all_spiders\r\n    for module in walk_modules(name):\r\n  File \"/home/djunzu/.pyenv/versions/2.7.13/lib/python2.7/site-packages/scrapy/utils/misc.py\", line 63, in walk_modules\r\n    mod = import_module(path)\r\n  File \"/home/djunzu/.pyenv/versions/2.7.13/lib/python2.7/importlib/__init__.py\", line 37, in import_module\r\n    __import__(name)\r\n  File \"/home/djunzu/crawler/crawler/spiders/__init__.py\", line 4, in <module>\r\n    from crawler.loaders import PersonLoader\r\n  File \"/home/djunzu/crawler/crawler/loaders.py\", line 4, in <module>\r\n    from processors.phone import PhoneOutputProcessor, extract_phone\r\nImportError: cannot import name extract_phone\r\nCould not load spiders from module 'crawler.spiders.development'. Check SPIDER_MODULES setting\r\n  warnings.warn(msg, RuntimeWarning)\r\n```\r\n\r\n\"Check SPIDER_MODULES setting\" is misleading; the error has nothing to do with `SPIDER_MODULES`.\r\n\r\nI would suggest to remove this sentence.\r\n\r\n(And I guess the last line should not be there, but it shows up in log.)", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2602/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2303", "title": "RedirectMiddleware does not respect spider's crawling rules", "body": "As described in #15 (and #1042), some links to offsite domains may be crawled via redirects. For example:\n\n``` python\n# in spider:\nallowed_domains = ['xxx.com']\n```\n\n``` bash\n# in log, offsite domain crawled:\nFiltered offsite request to 'www.yyy.com': <GET http://www.yyy.com/foo>\nRedirecting (302) to <GET http://www.yyy.com/> from <GET http://www.xxx.com/bar/>\nCrawled (200) <GET http://www.yyy.com/> (referer: http://www.xxx.com/bar/>)\n```\n\nBut the problem also affects `rules` from `CrawlSpider`:\n1. `deny` is not respected.\n2. (worse) a response may be parsed by the wrong function.\n\nFor example:\n\n``` python\n# in spider:\nrules = (Rule(LinkExtractor(deny='forbidden', allow='drinks'), callback='parse_drinks', follow=True),\n         Rule(LinkExtractor(deny='forbidden', allow='food'), callback='parse_food', follow=True)\n        )\n```\n\n``` bash\n# in log:\n\n# deny regexp crawled:\nRedirecting (302) to <GET http://www.xxx.com/forbidden> from <GET http://www.xxx.com/bar/>\nCrawled (200) <GET http://www.xxx.com/forbidden> (referer: http://www.xxx.com/bar/>)\n\n# http://www.xxx.com/drink/bar should be parsed by parse_drinks but it will be parsed by parse_food:\nRedirecting (302) to <GET http://www.xxx.com/drink/bar> from <GET http://www.xxx.com/food/bar>\nCrawled (200) <GET http://www.xxx.com/drink/bar> (referer: http://www.xxx.com/food/bar>)\n```\n\nA somewhat related discussion is going on #2241.\n\nPS:\n\nThere are other similar issues. #1744 is almost identical. But they are all closed without a clearly solution.\n\nAs I see a developer will never expect the current behavior. It seems a bug to me. If it is not a bug, or if it is a \"wont fix\", or if there is any motivation to don't change the behavior, at least a note should be placed on docs so developers are not taken by surprise!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2303/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/8288f78a39939dbffea467bf110c64e005847117", "message": "Add note about request.meta['depth'] in DepthMiddleware"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/dcb279bd6cc85cf1743b548e44b050edba6a2ed8", "message": "Add m4v extension to IGNORED_EXTENSIONS in LinkExtractor.\n\n\tmodified:   scrapy/linkextractors/__init__.py"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "apnewberry": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2885", "title": "Asynchronously query database for next URL to request", "body": "Thanks for all your great work on the Scrapy project -- I'm a huge fan!\r\n\r\nI have a remote database containing a job queue of URLs I want to scrape. In order to distribute the work efficiently, I want the Scrapy process to asynchronously query the database for the next URL to scrape. \r\n\r\nThe approach I found most natural was to yield Deferred database queries from `start_requests`, as in the code below. I'm using alchimia, which combines sqlalchemy with Twisted. However, I get an exception indicating that `start_requests` can't yield a Deferred.\r\n\r\n```python\r\n\"\"\"Asynchronously get next url to request.\"\"\"\r\n\r\nimport os\r\nimport scrapy\r\nimport alchimia\r\nfrom sqlalchemy import create_engine\r\nfrom twisted.internet import reactor\r\nfrom twisted.internet.defer import Deferred\r\n\r\n\r\ndb_url = os.environ['PGURL']\r\nengine = create_engine(\r\n    db_url, reactor=reactor, strategy=alchimia.TWISTED_STRATEGY\r\n)\r\n\r\n\r\nclass JobQueueSpider(scrapy.Spider):\r\n\r\n    def start_requests(self):\r\n\r\n        while True:\r\n\r\n            d = Deferred()\r\n            d.addCallbacks(engine.execute, 'pop_next_url()')\r\n            d.addCallbacks(alchimia.engine.TwistedResultProxy.scalar)\r\n            d.addCallback(scrapy.Request)\r\n            yield d\r\n\r\n```\r\n\r\nAn alternative approach is\r\n- yield fake requests to localhost from `start_requests`\r\n- use a downloader middleware to intercept each fake request and replace it with a Deferred query to the database for the next URL\r\n- add a callback to the database query to make a scrapy request from it\r\n\r\nthis approach works, but it seems quite roundabout.\r\n\r\nAm I missing something? Is there a clean way to asynchronously, one-at-a-time query a database for the next URL to request? If not, might it be changed so `start_requests` can yield Deferreds?\r\n\r\nThanks again!", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2885/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lhkthomas": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2881", "title": "Error when install scrapy in window by using pip install scrapy", "body": "Errors exists about the Twisted when installing scrapy in window by using pip install scrapy. Any one know how to fix it. \r\n>      copying src\\twisted\\internet\\test\\_yieldfromtests.py.3only -> build\\lib.win32-2.7\\twisted\\internet\\test\r\n>         creating build\\lib.win32-2.7\\twisted\\internet\\test\\fake_CAs\r\n>         copying src\\twisted\\internet\\test\\fake_CAs\\chain.pem -> build\\lib.win32-2.7\\twisted\\internet\\test\\fake_CAs\r\n>         copying src\\twisted\\internet\\test\\fake_CAs\\not-a-certificate -> build\\lib.win32-2.7\\twisted\\internet\\test\\fake_CAs\r\n>         copying src\\twisted\\internet\\test\\fake_CAs\\thing1.pem -> build\\lib.win32-2.7\\twisted\\internet\\test\\fake_CAs\r\n>         copying src\\twisted\\internet\\test\\fake_CAs\\thing2-duplicate.pem -> build\\lib.win32-2.7\\twisted\\internet\\test\\fake_CAs\r\n>         copying src\\twisted\\internet\\test\\fake_CAs\\thing2.pem -> build\\lib.win32-2.7\\twisted\\internet\\test\\fake_CAs\r\n>         copying src\\twisted\\mail\\test\\rfc822.message -> build\\lib.win32-2.7\\twisted\\mail\\test\r\n>         copying src\\twisted\\python\\test\\_deprecatetests.py.3only -> build\\lib.win32-2.7\\twisted\\python\\test\r\n>         copying src\\twisted\\words\\im\\instancemessenger.glade -> build\\lib.win32-2.7\\twisted\\words\\im\r\n>         copying src\\twisted\\words\\xish\\xpathparser.g -> build\\lib.win32-2.7\\twisted\\words\\xish\r\n>         running build_ext\r\n>         building 'twisted.test.raiser' extension\r\n>         error: INCLUDE environment variable is empty\r\n>     \r\n>         ----------------------------------------\r\n>     Command \"c:\\python27\\python.exe -u -c \"import setuptools, tokenize;__file__='c:\\\\users\\\\thomas~1\\\\appdata\\\\local\\\\temp\\\\pip-build-3oirm6\\\\Twisted\\\\setup.py';\r\n>     f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\"\r\n>      install --record c:\\users\\thomas~1\\appdata\\local\\temp\\pip-xvbgd2-record\\install-record.txt --single-version-externally-managed --compile\" \r\n>     failed with error code 1 in c:\\users\\thomas~1\\appdata\\local\\temp\\pip-build-3oirm6\\Twisted\\\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2881/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Kookabura": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2879", "title": "Is it possible to get time to first byte (TTFB)?", "body": "Hello,\r\nIs it possible to get time to first byte (TTFB) from request\\response? I can get download_latency but this is a time including downloading. I've looked through middlewares but couldn't understand how to get TTFB.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2879/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "guodeliang": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2855", "title": "The order problem of start_request and scheduler queue", "body": "```\r\n        while not self._needs_backout(spider):\r\n            **if not self._next_request_from_scheduler(spider):**\r\n                break\r\n\r\n        if slot.start_requests and not self._needs_backout(spider):\r\n            try:\r\n                **request = next(slot.start_requests)**\r\n            except StopIteration:\r\n                slot.start_requests = None\r\n            except Exception:\r\n                slot.start_requests = None\r\n                logger.error('Error while obtaining start requests',\r\n                             exc_info=True, extra={'spider': spider})\r\n            else:\r\n                self.crawl(request, spider)\r\n```\r\nWhen I started running more than two scrapy, start_requests confused me. \r\n I thought it run start_requests function first\uff0c and check the source code\u3002  I found out I was wrong\u3002\r\nMy judgment in start_requests was a complete failure,\r\n\r\nTo avoid two scrapy application running two times seed links, my start_requests code is as follows\uff1a\r\n\r\nBut It's nothing,   because of  the  order of the  funcitons\r\n```\r\n    def start_requests(self):\r\n        request = self.crawler.engine.slot.scheduler.next_request()\r\n        if request:\r\n            yield request\r\n            return\r\n        else:\r\n            self.logger.info(\"\u8c03\u5ea6\u961f\u5217\u4e3a\u7a7a\uff0c\u7eed\u6dfb\u52a0\u91c7\u96c6\u79cd\u5b50\")\r\n\r\n        for media_id in self.media_list:\r\n            toutiao_as_value = self.get_as_cp()\r\n            url = media_home.format(media_id=media_id, as_param=toutiao_as_value)\r\n            request = Request(url, dont_filter=True,\r\n                              headers=self.headers,\r\n                              callback=self.parse_media_homepage)\r\n\r\n            self.crawler.engine.slot.scheduler.enqueue_request(request)\r\n\r\n        yield self.crawler.engine.slot.scheduler.next_request()\r\n    pass\r\n\r\n```", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2855/reactions", "total_count": 2, "+1": 0, "-1": 1, "laugh": 0, "hooray": 0, "confused": 1, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "MaGuiSen": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2850", "title": "IOError: cannot identify image file <cStringIO.StringI object at 0x0000000005796E00>", "body": "i got this problem when i use the ImagesPipeline to download some image\uff08but some image can be download\uff09...such as this image url:\r\n\r\nhttp://mmbiz.qpic.cn/mmbiz_png/NmpHEAE5bXl1WYvWBOQBhDeB7Dar8JhIr4tHxouT9AbhZJO7wF1lv2bCUU1UX0YPtU70OUF7RRfnMQpubRJYpA/0?wx_fmt=png\r\n\r\nthis image can`t be download.\r\n\r\ni think it is because the image type is JFIF\uff0cand the ImagesPipeline is not support .\r\ni dont know how to Solve this problem, please help me\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2850/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lwm": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2846", "title": "Show warning if SPIDER_MODULES is not set", "body": "Following from https://github.com/scrapinghub/shub/issues/293#issuecomment-317388127.\r\n\r\nI'm trying to find the right place to show a warning for new users where they have not set the `SPIDER_MODULES` settings and cannot figure out how to run their spider (or get it registered on Scrapy Cloud).\r\n\r\nFor context, I wrote my spider from scratch without the `startproject` command.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2846/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "redapple": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2842", "title": "XMLFeedSpider iternodes iterator does not work on XML document with namespace", "body": "(Opening the issue so that we track it, although it is already known.)\r\n\r\n**Sample input document:**\r\n```\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\r\n<url><loc>http://www.argos.ie/static/Product/partNumber/2353030.htm</loc></url>\r\n<url><loc>http://www.argos.ie/static/Product/partNumber/2717339.htm</loc></url>\r\n(...)\r\n```\r\n\r\n**Symptom:**\r\nWith Scrapy 1.4.0 (and earlier for sure)\r\nUsing `XMLFeedSpider` and the default `iternodes` iterator, nodes using `itertag='loc'` cannot be found, \r\n\r\n```\r\n    (...) site-packages/scrapy/utils/iterators.py\", line 31, in xmliter\r\n        yield Selector(text=nodetext, type='xml').xpath('//' + nodename)[0]\r\n    exceptions.IndexError: list index out of range\r\n```\r\n\r\nand registering namespaces and using `itertag='prefix:loc'` does not work either.\r\n\r\nRecently seen (again) [on StackOverflow](https://stackoverflow.com/questions/45195861/how-to-extract-urls-from-an-xml-using-scrapy-xmlfeedspider).\r\nWas already discussed [on scrapy-users](https://groups.google.com/d/topic/scrapy-users/VN6409UHexQ/discussion).\r\n\r\nThere's a WIP PR #861. Last comments were about moving to iterparse-based implementation", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2842/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2825", "title": "Add debug print of config file path (if found and used)", "body": "Motivation: https://stackoverflow.com/questions/45076871/scrapy-is-trying-to-find-a-module-that-doesnt-exist\r\n\r\nWe could (DEBUG) log which `scrapy.cfg` is being used to configure settings.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2825/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2546", "title": "DOC Add (short) explanation of return vs. yield in callbacks", "body": "Follow up to https://github.com/scrapy/scrapy/pull/2533\r\n\r\nQuite a few Scrapy users are also Python-newbies (and that's great!), and using `yield` in callbacks (and generators in general) can be something odd at first.\r\n\r\nIt would be great to explain different callback patterns:\r\n- for single object \"returns\"\r\n- for multi-item or multi-request \"returns\"\r\n- for mixing items and requests\r\n\r\n(and why/when `return` with a list is not necessarily a bad thing)", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2546/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2283", "title": "Add note that CrawlerProcess.crawl() needs a crawler or spider class, not a spider instance", "body": "Even if [it's in the docs](http://doc.scrapy.org/en/latest/topics/api.html#scrapy.crawler.CrawlerProcess.crawl), \n\n> crawler_or_spidercls (Crawler instance, Spider subclass or string) \u2013 already created crawler, or a spider class or spider\u2019s name inside the project to create it\n\nusers still miss sometimes that `.crawl()` expects a Spider class (or a crawler) but not a Spider instance.\n\ne.g. http://stackoverflow.com/q/39639568\n\n> I overlooked the fact that the class itself has to be passed as an argument, and not an instance of that class.\n> This is quite subtle on the eye but makes all the difference\n\nThis ticket is to make it a bit more prominent.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2283/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2263", "title": "Add FAQ entry on how to deal with a very long \"allowed_domains\"", "body": "See https://github.com/scrapy/scrapy/issues/1908#issuecomment-206847756\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2263/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2253", "title": "Document how to customize path for written files with media pipelines", "body": "Motivation: https://github.com/scrapy/scrapy/issues/923\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2253/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2240", "title": "Add FAQ entry on splitting items in spider middleware", "body": "This is related to https://github.com/scrapy/scrapy/issues/1915 and the consensus that hacking on item pipeline to split items is not the way to go with current architecture,\nand that spider middleware are a good solution for this use-case.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2240/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2231", "title": "Make logging configuration (more) customizable", "body": "Would it make sense to have [`DEFAULT_LOGGING`](https://github.com/scrapy/scrapy/blob/ebef6d7c6dd8922210db8a4a44f48fe27ee0cd16/scrapy/utils/log.py#L45) be read from settings before going through [`dictConfig`](https://docs.python.org/2/library/logging.config.html#logging.config.dictConfig)?\n\nAlso, reading some of the issues with logging (e.g. https://github.com/scrapy/scrapy/issues/1977), it could even be useful to make `configure_logging` fully customizable too (for the brave), or at least having a way to add handlers.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2231/reactions", "total_count": 6, "+1": 6, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2127", "title": "Improve traceback when \"proxy\" cannot be parsed correctly (e.g. missing scheme)", "body": "See http://stackoverflow.com/q/38378710 for motivation.\n\nWhen using a `proxy` value without scheme, e.g. 'localhost:8080', scrapy breaks with an obscure exception on `to_bytes()`. Even if it's a wrong parameter value, I believe we can improve the message so that users can fix it swiftly.\n\n```\nIn [1]: import scrapy\n\nIn [2]: r = scrapy.Request('http://www.example.com', meta={'proxy': 'localhost:8080'})\n\nIn [3]: fetch(r)\n2016-07-15 17:17:21 [scrapy] INFO: Spider opened\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-3-0f7ba0a442fd> in <module>()\n----> 1 fetch(r)\n\n/home/paul/.virtualenvs/scrapy11/lib/python3.5/site-packages/scrapy/shell.py in fetch(self, request_or_url, spider)\n    110         try:\n    111             response, spider = threads.blockingCallFromThread(\n--> 112                 reactor, self._schedule, request, spider)\n    113         except IgnoreRequest:\n    114             pass\n\n/home/paul/.virtualenvs/scrapy11/lib/python3.5/site-packages/twisted/internet/threads.py in blockingCallFromThread(reactor, f, *a, **kw)\n    120     result = queue.get()\n    121     if isinstance(result, failure.Failure):\n--> 122         result.raiseException()\n    123     return result\n    124 \n\n/home/paul/.virtualenvs/scrapy11/lib/python3.5/site-packages/twisted/python/failure.py in raiseException(self)\n    366     if _PY3:\n    367         def raiseException(self):\n--> 368             raise self.value.with_traceback(self.tb)\n    369     else:\n    370         exec(\"\"\"def raiseException(self):\n\nTypeError: to_bytes must receive a unicode, str or bytes object, got NoneType\n\nIn [4]: \n\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2127/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2111", "title": "Add option to Log actual TLS connection options/Cipher Suite on 1st connection to a domain", "body": "See https://github.com/scrapy/scrapy/issues/1435#issuecomment-182393180 by @nyov for motivation\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2111/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/461f9daff5747728e26cd60e9dfe531092f58132", "message": "Update release notes for upcoming 1.4.1 version"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f729d74886be2290fcbbcaa21d366b770ff21008", "message": "Use a helper for to_bytes() and None input"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9cd348d94af782008f1a561a0b36da9231878833", "message": "Handle None values for smtp user and password"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/0d8a33fddccd7083c0f72fe75f89e9e7fd52cd82", "message": "Update docs"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/80bb4fcf9710598138d7604190636d723d6392df", "message": "Convert SMTP credentials to bytes if needed"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/12c7628fcbcfb3927f595e1fd6806ea6aefdf6fd", "message": "Encode message using supplied charset"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/e733f51d4b04f209bfec32d1bd7559a258f45d0c", "message": "Fix test"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/abaf466bb311f6416a58763ad7974825d88f4855", "message": "Print cryptography package version"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/7a35a1ad4ad38b2f413528b0184d8709dea2a495", "message": "Remove trailing bracket from components versions log"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/1968a8ec02913273a99a3137cb419e2649d69a5f", "message": "Move logging of overriden settings to Crawler init"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/a68a8f8fdca7c15dea2f68380b289db052d16a06", "message": "Merge remote-tracking branch 'origin/1.4'"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/0cb3085f8453265a8f37de684e62eda8b5398c75", "message": "Add test for alternate links"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/aaaa4da7a4e1fe75396028189de88fb9a6604200", "message": "Add template for a downloader middleware"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/bf7ef3e4c3ea70a60f6e5f9e5c8a6d842e2b72b9", "message": "Move methods to a new scrapy.utils.versions"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/219c8aa0b622260b9814379be41132367ef33e39", "message": "Log versions information at startup"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/15a5c533fa6f448b7e5cd72ef099725c2295ef6f", "message": "Add tests for HTTP 307 permanent redirects"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/1fdc10684fc427e5446350cada465ec934330a3f", "message": "HTTP Cache: treat 308 as 301"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/5dc9a88c347db3497b03949938184ca339f4e9cb", "message": "Handle HTTP 308 Permanent Redirect"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/1a18587d41cc7dabc0a829fc0b69016b054d64f8", "message": "Jessi toxenv: Add cryptography as per https://packages.debian.org/jessie/python-cryptography"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f0ded6b7759c9c2ec4b8946cdfc4abab97bb14d9", "message": "Do not cache DNS responses when cache size is 0"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/1f08d9a64884a05a541fe9649e3349cce5aa47be", "message": "Add test for DNS cache disabling"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/39ad0d0bddbc3d15b4186598bc69386e7321e17b", "message": "Fix setting name reference"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/e7061f7a4193ea4bd7587c6cac5c9a52f847bb58", "message": "Reformat a bit"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/fad6b70d92825b7530e39cc66020273f7f6b836b", "message": "Use https:// for readthedocs links"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/6b092c66809ec11245d03063961ca64e488580e1", "message": "Handle Twisted versions before 15.0"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/e162c1ff40bf74802c1a9f7b93fdbb56a5ff4a13", "message": "Pass proxy URI to ProxyAgent as bytes"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/c8dc158697453b610f168f97ce94945892163674", "message": "Use HTTP pool and proper endpoint key for ProxyAgent"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/d847e65aaef68009b1ba4c12e45f34a7783dcc5b", "message": "Merge pull request #2762 from stummjr/add-subreddit-to-docs\n\nInclude references to Scrapy subreddit in the docs"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/5f69ec98f70e1e1e5f65fb36eb1cfb23d0be5b45", "message": "Bump version: 1.3.2 \u2192 1.4.0"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/fc2846d637ca1ebc31e574284d9bc2537969bbe0", "message": "Set release date for v1.4.0"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/79b0d8605ca073cfdcb52cfb240b815b90af403b", "message": "Merge pull request #2630 from scrapy/release-notes-1.4.0\n\nRelease notes for 1.4.0"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/a3d3cd4cb7d256a0b876019279ec88c47d9957ff", "message": "Update with latest merges"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/432668acf7b8236229fca3b7726139acdc0c05f2", "message": "Mention implementation of #667"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/896c30a8eb1ea5d03898b27f238243ea629a3e42", "message": "Reference items pretty-printing issue number"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/55d10823603e8033a4c4c441b8e78da217a951bf", "message": "Reference recent fixes and commits"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/7d72394794bea63fdaa08d1264a50b0b83998424", "message": "Reword mention of new response.follow() shortcut"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/c6464cc4f5a152c2d23a79457b7e01ee90b18069", "message": "Add verbose introduction to new features"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/e139d990fca1e1e2144992bb2e05665d17c3f989", "message": "Fix sphinx-build warning on deprecated latex_paper_size"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/cba55cd190c573a03b8458cda7ba2a20da5651e6", "message": "Rephrase other sections"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/8729a91f7a136decb66b00ada858a67c41796e11", "message": "Rephrase \"New features\" section"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/851adcedf2220a783cdfb2d6b5872580e18b8d88", "message": "List merged pull requests since 1.3.3"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/0c237c6cac8f220ac2faf40a02ef035941fa5810", "message": "Merge pull request #2750 from scrapy/codecov\n\ntweak coverage reports"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/8aa2e4f9976d31bbe3a0014b4f1f96ace1b87043", "message": "Merge pull request #1829 from nyov/nyov/editor\n\n[MRG+1] Remove dependency on os.environ from default settings"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/143810410f3023a1f154271276b10b883b39d846", "message": "Merge pull request #2740 from luzfcb/luzfcb-run-scrapy-from-module\n\n[MRG+1] Add support for executing scrapy using -m option of python"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/dfe6d3d59aa3de7a96c1883d0f3f576ba5994aa9", "message": "Merge pull request #2456 from elacuesta/feed_export_beautify\n\n[MRG+1] Feed exports: beautify JSON and XML"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/3a0a86ed31df1d22fea3b5b05e853f212adc40c8", "message": "Clarify FEED_EXPORT_INDENT section"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/6d14e392f1096d94669c00456855b64164dba6bd", "message": "Remove old test certificate+key"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/e6ab8bc9a5460c5af29a330598f7afa5a6efb7e2", "message": "Change \"localhost\" test server certificate"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/c3d0f9b6c10b68e436054ce9421d2ddadfc47087", "message": "Add test for non-duplicated `Content-Length: 0` for bodyless POST"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/a63d9f502f50fbd948154fc65c8a94ffd4722d11", "message": "Restore comments on why POST needs `Content-Length: 0`"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/b1a0a6e25810353f46d314cea1fd34bd37109b0c", "message": "Make mockserver runnable outside of tox\n\nAdd POST support for Echo resource"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/97fc68fa1699b1c782c6e6d888e21d995bdf071d", "message": "Refactor conditions on body producer"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/aa59cf424115d50900be58f0533050d148abf3e4", "message": "Merge pull request #2668 from harshasrinivas/docs-sphinx-rtd-theme\n\n[MRG+1] Add sphinx_rtd_theme to docs setup readme"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/1e12187e44eb58bf784cb90290565e1dcb09fc37", "message": "Merge pull request #2714 from tbcardoso/docs_disable_dupefilter\n\n[MRG+1] Mention how to disable request filtering in documentation of DUPEFILTER_CLASS setting"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/3dee913d44e28887f73adc66917e347aba070683", "message": "Merge pull request #2713 from JulienPalard/master\n\n[PEDANTIC] FIX trailing whitespaces in LICENSE."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/2d7f11ecd5402b5a9f125216194b9cdb49e36d74", "message": "Merge pull request #2710 from redapple/travis-pypy-portable\n\nTravis CI: use portable pypy for Linux"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f3f7a4186150119377891069a3155cea8cabb142", "message": "Travis CI: use portable pypy for Linux"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/422b38f65ccbbe4479b6249c89bc57bd2d22d092", "message": "DOC Rearrange selector sections"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/252819151e07981eabaf2b6c0d88eb34bb5abd7b", "message": "Merge pull request #2695 from LMKight/command-list-fix\n\n[MRG+1] command-list-fix"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/163618c9b792ded60e451e929341bb502d9cc19a", "message": "FAQ Rewrite note on Python 3 support on Windows"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/99e3c0d653e23d1af3e4236b88a747317e1c8a8a", "message": "Set bodyproducer with empty content for POST"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/776129a9513e2b6ab6f7e8cda1dd3de66cbbff44", "message": "Merge pull request #2649 from pawelmhm/logformatter-2647\n\n[MRG+2] [logformatter] 'flags' format spec backward compatibility"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/1c0da1749d5250bbd82c2eb76fe26e49be13fb74", "message": "Merge pull request #2646 from woxcab/master\n\n[MRG+1] Allowed passing objects of Mapping class or its subclass to the CaselessDict initializer"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/871134ee22653f7f074fbfb8f3c393ba3555a6d4", "message": "Refactor to also test FilesPipeline"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/708f1b009b9b23971d73bfc7bc09163969ab6e00", "message": "Add integration tests for MEDIA_ALLOW_REDIRECTS"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/7dcc86e61adf37fdfb77b00375040fe25e7ad832", "message": "Add file listing resource + redirecting resource to MockServer"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/cfb56400b29316a43208053a8ef6d48eb0eb499e", "message": "Merge pull request #2641 from redapple/release-notes-1.3.3-master\n\nUpdate release notes and versionadded for SPIDER_LOADER_WARN_ONLY"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/a7f5207e9f2837040a3a08c3a8f4e76265b68bb2", "message": "Update version added for SPIDER_LOADER_WARN_ONLY"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/b2c505d8ec71fd1226782c65edd06d901c3ef211", "message": "Set release date in changelog for v1.3.3"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/d8865b33045c239e18704d7ab5012e334b128a32", "message": "Update changelog for upcoming 1.3.3"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9628a739723983f2d3c4079d228dcaa79f558e73", "message": "Update settings docs for new SPIDER_LOADER_WARN_ONLY"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f2ac24eb7b353d1140729ba7d8c81ad9635d5899", "message": "Do not only warn on wrong spider modules for \"scrapy list\""}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/36160f1b0aef5d80e495721559a437a37fcd252c", "message": "Merge pull request #2477 from scrapy/recommend-anaconda-for-win\n\n[MRG+2] docs: installation instructions, mention conda in the beginning (closes #2475)"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/c0cbaccb7bffc21785ff6f1e3aac2e338774eb96", "message": "Merge pull request #2581 from lopuhin/respect-custom-log-level\n\n[MRG+1] Respect custom log level: fixes GH-1612"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/db13ab5ec21dca07310873067fc0a21f71a2567e", "message": "Merge pull request #2611 from delftswa2017/httpcachemiddleware_log\n\n[MRG+1] [logging] HttpCacheMiddleware: log cache directory at instantiation"}], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2906", "title": "[WIP] Downloader timings in request.meta", "body": "While working on a requests log plugin for Scrapy, I felt the need to get more fine-grained timing information on the various steps a `Request` goes through from reaching the `Downloader` to getting the associated response body.\r\nOne can work with `request.meta['download_delay']` and timing inserted from a downloader middleware but I found this additional information interesting for example for HAR output.\r\n\r\nNote: This does not address DNS lookup time nor TCP/TLS connection time.\r\n\r\nI would even set this to ON by default myself.\r\n\r\nMissing:\r\n\r\n- [ ] Documentation\r\n- [ ] How does this play with proxies?\r\n- [x] Handle download exceptions (DNS errors, dowload timeouts, connection errors etc.)", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2897", "title": "[WIP][PoC] Allow per-slot concurrency and delay settings", "body": "Motivation: https://stackoverflow.com/questions/45861515/how-to-set-up-different-download-delay-from-different-domains-scrapy-python\r\n\r\nI've also seen use cases of the same spider contacting 2 different endpoints from the same main domain, e.g. www.example.com and api.example.com, but which have different throttling requirements.\r\n\r\nThis quick hack changes `Downloader` to check a new (optional) DOWNLOADER_SLOTS setting to create a new downloader Slot, \r\nso as to configure DOWNLOAD_DELAY and CONCURRENT_REQUEST_PER_DOMAIN\r\nper-hostname instead of globally.\r\n\r\nObviously, this needs a bit more work to cover various cases:\r\n- per-IP concurrency, \r\n- wildcard settings keys perhaps e.g. `*.example.com`\r\n- check how this plays with AutoThrottle.\r\n- check how this plays with `request.meta['download_slot']`, for example to support non-hostname keys\r\n...\r\n\r\nWhat do others think?\r\n\r\nI played with this with this spider:\r\n```\r\n# -*- coding: utf-8 -*-\r\nimport scrapy\r\n\r\n\r\nclass ExampleSpider(scrapy.Spider):\r\n    name = 'example'\r\n    allowed_domains = ['example.com',\r\n                       'httpbin.org']\r\n    start_urls = ['http://example.com/']\r\n\r\n    def parse(self, response):\r\n        for u in (['http://example.com/?q=%d' % i for i in range(11,21)] +\r\n                  ['http://httpbin.org/get?q=%d' % i for i in range(21,51)]):\r\n            yield scrapy.Request(u, callback=self.sink)\r\n\r\n    def sink(self, response):\r\n        pass\r\n```\r\n\r\nand these settings:\r\n```\r\nCONCURRENT_REQUESTS = 32\r\nDOWNLOAD_DELAY = 2\r\nDOWNLOADER_SLOTS = {\r\n    'httpbin.org': {\r\n        'DOWNLOAD_DELAY': 0.5,\r\n        'CONCURRENT_REQUESTS_PER_DOMAIN': 2,\r\n    }\r\n}\r\n```", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2841", "title": "[WIP] Support V4 signature with region name in S3 operations (botocore only)", "body": "Fixes #2448\r\n\r\nStill work in progress.\r\n\r\nFor the core S3 download handler, this patch makes use of Signature version 4 in all non-anonymous cases.\r\n[SigV4 is supported by all regions](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region). And is also [the signature one should use](https://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html#signature-versions).\r\n\r\n> Signature Versions\r\n> \r\n> AWS supports two signature versions: Signature Version 4 and Signature Version 2. You should use Signature Version 4. All AWS services support Signature Version 4, except Amazon SimpleDB which requires Signature Version 2. For AWS services that support both versions, we recommend that you use Signature Version 4.\r\n> \r\n> All AWS regions support Signature Version 4.\r\n\r\n**Note: this does NOT add SigV4 support for non-botocore users.**\r\n\r\nMissing:\r\n\r\n- [x] Support SigV4 in media pipelines\r\n- [ ] Fix AWS signature tests", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2835", "title": "Add option to expand date and time in logs as UTC", "body": "Fixes #2802\r\n\r\nThis adds a `LOG_DATETIME_UTC` setting, defaulting to `False`, to expand datetimes as UTC if set to `True`.\r\nThis new setting would be most useful with `LOG_DATEFORMAT` with a `Z` or `+0000` suffix.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2801", "title": "[WIP] Add XPath tutorial to docs", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2746", "title": "Add __str__ method on stats collector class", "body": "I was playing with formatting messages with Python templates including crawler stats and thought it might be useful to have a standard formatting for `StatsCollector' instances.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2398", "title": "[WIP] Move HttpCompression middleware closer to downloader side", "body": "Fixes https://github.com/scrapy/scrapy/issues/1895\r\n\r\nHttpCompression code needed to be made a bit more tolerant to failures for non-200 responses, there were tests for that, e.g.\r\n\r\n```\r\n===================================================================================== FAILURES =====================================================================================\r\n___________________________________________________________ DefaultsTest.test_3xx_and_invalid_gzipped_body_must_redirect ___________________________________________________________\r\n\r\nself = <tests.test_downloadermiddleware.DefaultsTest testMethod=test_3xx_and_invalid_gzipped_body_must_redirect>\r\n\r\n    def test_3xx_and_invalid_gzipped_body_must_redirect(self):\r\n        \"\"\"Regression test for a failure when redirecting a compressed\r\n            request.\r\n    \r\n            This happens when httpcompression middleware is executed before redirect\r\n            middleware and attempts to decompress a non-compressed body.\r\n            In particular when some website returns a 30x response with header\r\n            'Content-Encoding: gzip' giving as result the error below:\r\n    \r\n                exceptions.IOError: Not a gzipped file\r\n    \r\n            \"\"\"\r\n        req = Request('http://example.com')\r\n        body = b'<p>You are being redirected</p>'\r\n        resp = Response(req.url, status=302, body=body, headers={\r\n            'Content-Length': str(len(body)),\r\n            'Content-Type': 'text/html',\r\n            'Content-Encoding': 'gzip',\r\n            'Location': 'http://example.com/login',\r\n        })\r\n>       ret = self._download(request=req, response=resp)\r\n\r\n/home/paul/src/scrapy/tests/test_downloadermiddleware.py:79: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n/home/paul/src/scrapy/tests/test_downloadermiddleware.py:46: in _download\r\n    ret.raiseException()\r\n/home/paul/src/scrapy/.tox/py35/lib/python3.5/site-packages/twisted/python/failure.py:368: in raiseException\r\n    raise self.value.with_traceback(self.tb)\r\n/home/paul/src/scrapy/.tox/py35/lib/python3.5/site-packages/twisted/internet/defer.py:1260: in _inlineCallbacks\r\n    result = g.send(result)\r\n/home/paul/src/scrapy/scrapy/core/downloader/middleware.py:53: in process_response\r\n    spider=spider)\r\n/home/paul/src/scrapy/scrapy/downloadermiddlewares/httpcompression.py:30: in process_response\r\n    decoded_body = self._decode(response.body, encoding.lower())\r\n/home/paul/src/scrapy/scrapy/downloadermiddlewares/httpcompression.py:46: in _decode\r\n    body = gunzip(body)\r\n/home/paul/src/scrapy/scrapy/utils/gz.py:37: in gunzip\r\n    chunk = read1(f, 8196)\r\n/home/paul/src/scrapy/scrapy/utils/gz.py:24: in read1\r\n    return gzf.read1(size)\r\n/usr/lib/python3.5/gzip.py:287: in read1\r\n    return self._buffer.read1(size)\r\n/usr/lib/python3.5/_compression.py:68: in readinto\r\n    data = self.read(len(byte_view))\r\n/usr/lib/python3.5/gzip.py:461: in read\r\n    if not self._read_gzip_header():\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <gzip._GzipReader object at 0x7faadddf44a8>\r\n\r\n    def _read_gzip_header(self):\r\n        magic = self._fp.read(2)\r\n        if magic == b'':\r\n            return False\r\n    \r\n        if magic != b'\\037\\213':\r\n>           raise OSError('Not a gzipped file (%r)' % magic)\r\nE           OSError: Not a gzipped file (b'<p')\r\n\r\n/usr/lib/python3.5/gzip.py:409: OSError\r\n```\r\n\r\nIf HTTP 200 decompression failures are silenced, you have another failing test:\r\n\r\n```\r\n===================================================================================== FAILURES =====================================================================================\r\n_____________________________________________________________ DefaultsTest.test_200_and_invalid_gzipped_body_must_fail _____________________________________________________________\r\n\r\nself = <tests.test_downloadermiddleware.DefaultsTest testMethod=test_200_and_invalid_gzipped_body_must_fail>\r\n\r\n    def test_200_and_invalid_gzipped_body_must_fail(self):\r\n        req = Request('http://example.com')\r\n        body = b'<p>You are being redirected</p>'\r\n        resp = Response(req.url, status=200, body=body, headers={\r\n            'Content-Length': str(len(body)),\r\n            'Content-Type': 'text/html',\r\n            'Content-Encoding': 'gzip',\r\n            'Location': 'http://example.com/login',\r\n        })\r\n>       self.assertRaises(IOError, self._download, request=req, response=resp)\r\n\r\n/home/paul/src/scrapy/tests/test_downloadermiddleware.py:94: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n/home/paul/src/scrapy/.tox/py27/local/lib/python2.7/site-packages/twisted/trial/_synctest.py:414: in assertRaises\r\n    return context._handle(lambda: f(*args, **kwargs))\r\n/home/paul/src/scrapy/.tox/py27/local/lib/python2.7/site-packages/twisted/trial/_synctest.py:308: in _handle\r\n    self._returnValue = obj()\r\n/home/paul/src/scrapy/.tox/py27/local/lib/python2.7/site-packages/twisted/trial/_synctest.py:324: in __exit__\r\n    self._expectedName, self._returnValue)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <tests.test_downloadermiddleware.DefaultsTest testMethod=test_200_and_invalid_gzipped_body_must_fail>, msg = 'IOError not raised (<200 http://example.com> returned)'\r\n\r\n    def fail(self, msg=None):\r\n        \"\"\"\r\n            Absolutely fail the test.  Do not pass go, do not collect $200.\r\n    \r\n            @param msg: the message that will be displayed as the reason for the\r\n            failure\r\n            \"\"\"\r\n>       raise self.failureException(msg)\r\nE       FailTest: IOError not raised (<200 http://example.com> returned)\r\n\r\n/home/paul/src/scrapy/.tox/py27/local/lib/python2.7/site-packages/twisted/trial/_synctest.py:368: FailTest\r\n```", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2359", "title": "[WIP] Add X-Scrapy-Cache-Timestamp header in cached responses", "body": "Fixes #2221\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1949", "title": "Add encoding to Link object + re-enable HTML entities links tests", "body": "Add `encoding` parameter and attribute to `Link` object.\nThat's the only way I found to properly account for encoding information when building requests from link extractors.\n\nSupersedes #1880 \nShould fix #1403\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1118", "title": "[WIP] Upgrading optparse code (deprecated since Py2.7) to argparse", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/4617865", "body": "@pablohoffman , what would you think if the order was changed to `gzip,deflate,x-gzip`?\nI've found at least 1 web server that doesn't compress responses when `x-gzip` is first\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4617865/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4650042", "body": "yes @dangra , `gzip,x-gzip,deflate` also works for this server\n(as does `gzip,deflate,x-gzip`)\n\nFrom wireshark:\n\n```\nGET / HTTP/1.1\nHost: ...\nAccept-Language: en\nAccept-Encoding: gzip,x-gzip,deflate\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nUser-Agent: Scrapy/0.21.0 (+http://scrapy.org)\n```\n\n```\nHTTP/1.1 200 OK\nCache-Control: no-cache\nPragma: no-cache\nContent-Type: text/html; charset=utf-8\nContent-Encoding: gzip\nExpires: -1\nVary: Accept-Encoding\nServer: Microsoft-IIS/7.5\nSet-Cookie: ASP.NET_SessionId=xyps42n5jtpprtrormg45q45; path=/; HttpOnly\nX-AspNet-Version: 2.0.50727\nX-Powered-By: ASP.NET\nDate: Wed, 20 Nov 2013 09:47:54 GMT\nContent-Length: 11786\n\n.............`.I.%&/m.{.J.J..t...`.$..@.........iG#).*..eVe]f.@......{....{....;.N'...?\\fd.l..J...!....?~|.?\"~..7N....j.^..t...#|.....<-...4}.~.(}...2[^|.Q..(..Y.|..w^?-.lR.3j..G..w.^..........(.....M......&@.....A@[.L..w~.w.:;...#|A....E.$P..]=.{...j|uo\\..ww.>|x...D.....;...lF?...7N~.>h....^..U....y..M.~./?n.l9K.[-..gY3/.e...,.y..>/......y..x|W.0#,.6#&hW../Z...}4..m.l.A.uv..........c?&.-.E..Go.....5^......7...n..w...^........F..|Y,..)..y=Ji.\nQ._..W...w%.j..+. ..,o.u.j.\n.^L9.E$.....~..R.Y..k..d._1.K.xV..eV\n.j..M.5.`-.+.h..2o...:.......j4/.a.(..2%..u.V.y..n...8.a\\.D...jo...IY..5.'.._dK.'.J...&........F3...{.~`...3}.<.....u[y... @O....V$\"....rc...!]S...h...g.`..8[..7.9i:..u^~...u.N.mZ.f..P].}T,h.w.m.g..?k..g..tL..@..m>..\n.pN.l..UuQ...h...i......(.?.r./?y.-.G.;;.3\".......X......x......{......\n...\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4650042/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/6959716", "body": "This doesn't pass Travis lint tool at http://lint.travis-ci.org/\n\n`\"$TOXENV == py27 && $TRAVIS_TAG =~ ^[0-9]\\\\.[0-9]*[02468]\\\\.\"` does, if that would work. Not sure\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/6959716/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/6972205", "body": ":)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/6972205/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5920824", "body": "could be called stat_file\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5920824/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5920841", "body": "imagefile -> filepointer or something\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5920841/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5920858", "body": "same comments as for FSFilesStore\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5920858/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5920871", "body": "wouldn't `open(absolute_path, 'wb')` be safer?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5920871/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5922474", "body": "Small type: Overridable\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5922474/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5922616", "body": "Not really sure if my comment should be part of this change, but there was a question the other day on StackOverflow about changing the filename (for ImagesPipeline) based not only on URL, but also based on some info from the originating item. One of my ideas was to additionally pass the request or response to `image_key()`. It's helpful in case you add `meta` to `Request` in `get_media_requests()`\nMy answer to http://stackoverflow.com/questions/18081997/scrapy-customize-image-pipeline-with-renaming-defualt-image-name/18083143 suggests something else though\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5922616/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5922738", "body": "For people having overridden `image_key` to change the name of the output file, I would suggest keeping `image_key()` but as an alias to `file_key()` to avoid breaking things for them\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5922738/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5922799", "body": "Sure.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5922799/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5923265", "body": "I think you need to keep the call to `image_key` for people having overriden it\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5923265/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5923380", "body": "here also, I would keep `self.pipeline.image_key`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5923380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5926847", "body": "I see.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5926847/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7792157", "body": "well spotted. corrected.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7792157/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7800293", "body": "ah man... typo error, should be \"parameter\"...\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7800293/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7811719", "body": "Why ignore `response` if `selector` is given and not raise an exception? I don't really know.\n\nBut as it's legacy behavior, it's probably safer to keep it that way, although I doubt users pass both `selector` and `response`, but who knows...\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7811719/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7898849", "body": "I'll re-read through `lxml` code but I remember it's done already in `XPathElementEvaluator.__call__` (used in `_Element.xpath()`, via `xpath.xmlXPathEvalExpression`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7898849/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8857620", "body": "indeed ;)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8857620/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9225001", "body": "Running the tests without it raises an `AssertionError`\n\n```\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/unittest/case.py\", line 323, in run\n    self.setUp()\n  File \"/home/paul/dev/scrapy-dev/scrapy/tests/test_spidermiddleware_offsite.py\", line 13, in setUp\n    self.mw.spider_opened(self.spider)\n  File \"/home/paul/dev/scrapy-dev/scrapy/contrib/spidermiddleware/offsite.py\", line 67, in spider_opened\n    spider.crawler.stats\n  File \"/home/paul/dev/scrapy-dev/scrapy/spider.py\", line 41, in crawler\n    assert hasattr(self, '_crawler'), \"Spider not bounded to any crawler\"\nexceptions.AssertionError: Spider not bounded to any crawler\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9225001/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9225107", "body": "good point\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9225107/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9229449", "body": "stats on by default make this test fail: \n\n```\n===============================================================================\n[FAIL]\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/unittest/case.py\", line 332, in run\n    testMethod()\n  File \"/home/paul/dev/scrapy-dev/scrapy/tests/test_spidermiddleware_offsite.py\", line 33, in test_process_spider_output\n    out = list(self.mw.process_spider_output(res, reqs, self.spider))\n  File \"/home/paul/dev/scrapy-dev/scrapy/contrib/spidermiddleware/offsite.py\", line 38, in process_spider_output\n    spider.crawler.stats.inc_value('offsite/domains',\n  File \"/home/paul/dev/scrapy-dev/scrapy/spider.py\", line 41, in crawler\n    assert hasattr(self, '_crawler'), \"Spider not bounded to any crawler\"\nexceptions.AssertionError: Spider not bounded to any crawler\n\nscrapy.tests.test_spidermiddleware_offsite.TestOffsiteMiddleware.test_process_spider_output\n-------------------------------------------------------------------------------\n\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9229449/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9311043", "body": "@alexanderlukanin13 , I've been looking at the recent Travis build failures\nhttps://travis-ci.org/scrapy/scrapy/jobs/17914775\n\n(working on these fixes: https://github.com/scrapy/scrapy/pull/570)\n\nThe last one that's bugging me is this repetition link from `sgml_linkextractor.html`\n\nAccording to https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/linkextractors/sgml.py#L50\n\n```\nlinks = unique_list(links, key=lambda link: link.url) if self.unique else links\n```\n\nshouldn't the extractor pick up the non-\"repetition\" link?\n\na local test on my machine passes with `Link(url='http://example.com/sample3.html', text=u'sample 3 repetition')` so I don't know anymore :)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9311043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9311921", "body": "Ah yeah, this line shuffles the extracted links\n\n```\n        urlstext = set([(clean_url(url).encode(response_encoding), clean_text(text))\n                        for url, _, text in links_text])\n```\n\n@dangra , @pablohoffman , @nramirezuy , @kmike ,\nwhat's the expected behaviour of `RegexLinkExtractor` regarding duplicate URLs (that may have different link text)? take first one in document? random?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9311921/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9313012", "body": "FYI, fixing it as recommended by @dangra (change from `set` to `scrapy.utils.python.unique` in order to grab the first link)\nhttps://github.com/redapple/scrapy/commit/73ae6b850b11e8a6525138698fbe0c179fa983ff\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9313012/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "tonyyherb": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2834", "title": "Root log not logging to file after updating from 1.2 to 1.4", "body": "I have \r\n\r\n    d = {\r\n            'LOG_LEVEL': logging.DEBUG,\r\n            'LOG_FORMAT': '%(levelname)s %(asctime)s %(name)s: %(message)s',\r\n            'LOG_FILE': 'scrapyroot'\r\n    }\r\n    configure_logging(d)\r\n\r\nI am using CrawlerRunner(get_project_settings()) to run spiders. The setting above used to log the scrapy root logs to file 'scrapyroot'. \r\nAfter updating to 1.4, scrapy root logs are printed to console; the file  'scrapyroot' is created but empty. \r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2834/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wangtua1": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2833", "title": "telnet console abnormal", "body": "when I typed \u2018\u2019telnet localhost 6023\u2018\u2019,it replies like this \"escape chararctor is ']'\", and the program itself stops crawling data from the web.I can't figure out what is the matter.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2833/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2917", "title": "support giantfiles download ,fix CVE-2017-14158", "body": "from the issue https://github.com/scrapy/scrapy/issues/482.\r\nWe can see if we download a file with filespipeline, the whole file is stored into the memory.\r\nIf the file is a giant file or an extremely giant file,(over 1G or more), the crawling thread will be crashed.\r\nWe can use this POC to prove it.\r\n[POC.zip](https://github.com/scrapy/scrapy/files/1289514/POC.zip)\r\n\r\nThis has been asigned CVE-2017-14158.\r\nbut in this new testcase using GiantFilesPipeline in the commit,we can download it correctly.\r\n\r\n[testdownload.zip](https://github.com/scrapy/scrapy/files/1289512/testdownload.zip)\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lopuhin": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2824", "title": "DOC python 3 compatible module for memory profiling", "body": "Guppy (mentioned here https://doc.scrapy.org/en/latest/topics/leaks.html) is python 2 only AFAIK, so it would be nice to provide a python 3 compatible alternative, e.g. pympler.readthedocs.io", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2824/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2608", "title": "scrapy runspider in case of several spiders in a file", "body": "[Docs](https://doc.scrapy.org/en/latest/topics/commands.html?highlight=runspider#std:command-runspider) say: \"run a spider self-contained in a Python file, without having to create a project.\"\r\n\r\nIn case there are multiple spiders in a file, one of them will be selected and run, but spider selection is not deterministic.\r\n\r\nAlthough it's technically not backwards compatible, maybe the best course of action would be to raise an error in this case and do not run any spiders?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2608/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/1948", "title": "Incorrect traceback in downloader middleware with @inlineCallbacks", "body": "Here is a silly example (put it into `bad_traceback.py` and run with `python3.4 bad_traceback.py`):\n\n``` python\nimport scrapy\nfrom twisted.internet.defer import inlineCallbacks\nfrom scrapy.crawler import CrawlerProcess\n\nclass BuggyMiddleware(object):\n    @inlineCallbacks\n    def process_request(self, request, spider):\n        undefined  # this is the bug for which I expect to see the traceback\n        yield scrapy.Request('http://yandex.ru')\n\nclass Spider(scrapy.Spider):\n    name = 'spider'\n    start_urls = ['http://google.com']\n    def parse(self, response):\n        print(response)\n\nprocess = CrawlerProcess({\n    'DOWNLOADER_MIDDLEWARES': {'bad_traceback.BuggyMiddleware': 584}\n    })\nprocess.crawl(Spider)\nprocess.start()\n```\n\nThe output is duplicate for some reason (it's not the case when running it in a normal scrapy project), the traceback I get is:\n\n```\n2016-04-21 16:02:05 [scrapy] ERROR: Error downloading <GET http://google.com>\nTraceback (most recent call last):\n  File \"/Users/kostia/shub/memex/undercrawler/venv/lib/python3.4/site-packages/twisted/internet/defer.py\", line 1126, in _inlineCallbacks\n    result = result.throwExceptionIntoGenerator(g)\n  File \"/Users/kostia/shub/memex/undercrawler/venv/lib/python3.4/site-packages/twisted/python/failure.py\", line 389, in throwExceptionIntoGenerator\n    return g.throw(self.type, self.value, self.tb)\n  File \"/Users/kostia/shub/memex/undercrawler/venv/lib/python3.4/site-packages/scrapy/core/downloader/middleware.py\", line 37, in process_request\n    response = yield method(request=request, spider=spider)\nNameError: name 'undefined' is not defined\n```\n\nHere I expected to see `BuggyMiddleware.process_request`. I used CPython 3.4.1 with a recent scrapy (288f8c09f8bac4c92f49332c8a5f5a21a182cc15), using Twisted 15.5.0 and 16.1.1 (output is the same in both cases).\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/1948/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/bb1f31189128cb2272c1302350387075fbbb730a", "message": "Add PyPy3 support to faq and install doc"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/041308afe7c40de7088f75b0e0c312ecd5de428a", "message": "Fix get_func_args test for pypy3\n\nThese built-in functions are exposed as methods in PyPy3.\nFor scrapy this does not matter as:\n1) they do not work for CPython at all\n2) get_func_args is checked for presense of an argument in scrapy,\n   extra \"self\" does not matter.\nBut it still makes sense to leave these tests so that we know we\nshouldn't use get_func_args for built-in functions/methods."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f71df6f9addca10b562bb22890b5ea1c37efde5c", "message": "Run tests for PyPy3"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/ea41114cf0ab2782650792ad204cf43fc148c749", "message": "Mention PyPy support, add PyPy to install docs"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/a21b80041916f65e1ba1b28acd9ea6b545e17d3d", "message": "Merge pull request #3011 from Jane222/master\n\n[MRG+1] Issues a warning when user puts a URL into allowed_domains (#2250)"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/97dc5271918193fa69f378e67bc88568a9c10a02", "message": "Merge pull request #3020 from Jesse-Bakker/add-fromcrawler-middleware-docs\n\n[MRG+1] Added from_crawler to middleware docs"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/fd7334518f22a6c5419e07ffe08e3d082dc49cbf", "message": "Merge pull request #2983 from codeaditya/https-links\n\n[MRG+1] Use https link in default user agent"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/abb6d0a1c1dd4d1ab6f734f2b366b9d851a52a48", "message": "Use portable pypy directly\n\nThey are provided by https://github.com/squeaky-pl/portable-pypy"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/84111969c4250b486b15729b42f88300fc983511", "message": "Update pypy version regexp to get last release\n\nPyPy changed naming conention since 5.8 release, not it's called\npypy2.7-x.x.x"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/b0a9236357dd74381b5a014e3de15a3a52de4f7d", "message": "Use environment markers for custom PyPy requirements"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/5ba8e5adc0e1aa7d9430b63a0094c6a71c51564a", "message": "Remove duplicate PyPy toxenv from Travis config\n\nThanks for the catch @redapple"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/271b3a485cbdce1b0a866d9fb9938d46d7ddb497", "message": "Require pypy build to pass"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/ea08b952801b9af9bc45ae9918fcf5ab9bcb5aaf", "message": "Remove Jython gc branch: it's not supported"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/a8df0900713a1a56cffca8774bc137a6e781877e", "message": "Fix httpcache leveldb tests: gc.collect after del\n\nLevelDB does not have \"official\" close method, so we have\nto rely on garbage collection to close it."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/b4eb60e5270498a64200332587a065ec9978a333", "message": "Install PyPyDispatcher for PyPy tests\n\nUsing https://github.com/lopuhin/pydispatcher, pypy branch.\nThis is executed as a separate step to avoid changing\ndefault requirements.txt and setup.py. If just added to \"deps\"\nin tox, this install command will be executed as one command\nand PyPyDispatcher will not override PyDispatcher."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/19ca986aa1340b08658caaac0ce677aa22be9814", "message": "Move garbage_collect to scrapy.utils.python"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/7c67047e77914dfe0d6666e4dc535c684aa77090", "message": "Fix get_func_args tests under PyPy\n\nOn CPython get_func_args does not work correctly for built-in\nmethods."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/5abb70c8d71365adbf3a035b1f6d07f5fbbbf446", "message": "Fix test_weakkeycache on PyPy: run gc.collect()\n\nOne gc.collect() seems to be enough, but it's more reliable\nto run it several times (at most 100), until all objects are collected."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/c3d17659b33432ba8cd2e5b2da57105c1cc0da22", "message": "Fix queue serialization test on PyPy\n\nIt is not affected by Twisted bug #7989 and is more permissive\nwith pickling (especially with protocol=2)."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/6014856df5717496b57aaac6c8a2e64b125ac32b", "message": "Fix test_output_processor_error undere PyPy\n\nFor float(u'$10') PyPy includes \"u'\" in the error message,\nand it's more fair to check error message on input we are really\npassing."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/5a08cf3b9606bf77ef02a37dca1e2bc76f74558f", "message": "Fix test_start_requests_errors for PyPy\n\nTwisted prints errors in DebugInfo.__del__, but PyPy does not run\ngc.collect() on exit:\nhttp://doc.pypy.org/en/latest/cpython_differences.html?highlight=gc.collect#differences-related-to-garbage-collection-strategies"}], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2823", "title": "[WIP] An option to limit the cache size for the media pipeline", "body": "Fixes #939  (the memory issue with media pipeline)\r\n\r\nOnly docs so far - if this looks good, I'll proceed with implementation. An option to limit the cache size to some fixed number complicated implementation a little bit, but seems it's worth it in case the crawler is downloading all images, so the same image can be referenced from multiple pages.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "madspark": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2821", "title": "Catching exceptions on start urls of CrawlSpider", "body": "Here is the problem that I am having:\r\nI want to crawl some URLs with CrawlSpider and I expect some of them to give DNS lookup errors.\r\nI want to catch these errors and handle them gracefully (log them, etc). \r\n\r\nFor a generic spider, this can be done as shown in the documentation:\r\nhttps://doc.scrapy.org/en/latest/topics/request-response.html?highlight=failure.check#using-errbacks-to-catch-exceptions-in-request-processing\r\n\r\nFor sublinks of my initial pages, it can be done as suggested on stackoverflow:\r\nhttps://stackoverflow.com/questions/35866873/scrapy-get-website-with-error-dns-lookup-failed\r\n\r\nBut for the initial URL visited (i.e. before rules are used for extraction), I can't figure out a way to gracefully catch errors. Best I can do is write a downloader middleware, which can at least log them.\r\n\r\nThe following seemed promising, but does not work - I overwrote the start_requests function to include callback and errback in the initial request. This catches the errors, but it also makes the CrawlSpider not attempt to parse subpages for valid requests. I am actually not sure if this is a bug or if I am missing something.\r\n\r\nCan anyone point out if there is an obvious approach that I am missing to catching all request download errors (including DNS lookup errors) using CrawlSpider?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2821/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ghost": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2815", "title": "Invalid Hostname when \"_\" in domain", "body": "`2017-07-04 16:25:43 [scrapy.core.scraper] ERROR: Error downloading <GET http://wodnik_forum.orq.pl/>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\bukowa\\vritualenv2\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1384, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\bukowa\\vritualenv2\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\bukowa\\vritualenv2\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\nValueError: invalid hostname: www.bg_2014.team360.pl`\r\n\r\nValueError: invalid hostname: www.bg_2014.team360.pl\r\nValueError: invalid hostname: www.wolfs_rain.wxv.pl\r\nValueError: invalid hostname: wodnik_forum.orq.pl\r\nValueError: invalid hostname: kolo_pzw_miejskie_konskie.wedkuje.pl\r\nValueError: invalid hostname: bieganie_swiebo.e-fora.pl\r\nValueError: invalid hostname: rzymskie_koloseum.lovetotravel.pl\r\nValueError: invalid hostname: zbuntowana_anielica.story.gala.pl\r\nValueError: invalid hostname: forum_origami.bo.pl\r\nValueError: invalid hostname: dfk_forum.gbbsoft.pl\r\nValueError: invalid hostname: wodnik_forum.orq.pl\r\n\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2815/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3016", "title": "Added Scrapy logo", "body": "I replaced the \"Scrapy\" text with the logo shown on the Scrapy website", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135769120", "body": "Hey guys I'm new to web crawling.Sorry for disturbing you guys.\nI came to know that scrapy is the fastest web framework module for web crawling.\nI thought that you guys would be doing that in multithreading but I heard that you guys never use any threads.How could this be possible?can anyone give a short explanation to me...Please\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135769120/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312946128", "body": "@gmeans code gives warning:\r\n\r\n /usr/local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py:51: builtins.UserWarning:\r\n         'broadd.context.CustomContextFactory' does not accept `method` argument (type OpenSSL.SSL method, e.g. OpenSSL.SSL.SSLv23_METHOD). Please upgrade your context factory class to handle it or ignore it.\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312946128/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312947454", "body": "@redapple I had errors:\r\n`<twisted.python.failure.Failure OpenSSL.SSL.Error: ('SSL routines', 'SSL3_READ_BYTES', 'sslv3 alert handshake failure'), ('SSL routines', 'SSL3_WRITE_BYTES', 'ssl handshake failure')>` \r\nwith all packages up-to-date", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312947454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312956745", "body": "Scrapy    : 1.4.0\r\nlxml      : 3.8.0.0\r\nlibxml2   : 2.9.4\r\ncssselect : 1.0.1\r\nparsel    : 1.2.0\r\nw3lib     : 1.17.0\r\nTwisted   : 17.5.0\r\nPython    : 3.6.1 (v3.6.1:69c0db5, Mar 21 2017, 17:54:52) [MSC v.1900 32 bit (Intel)]\r\npyOpenSSL : 17.1.0 (OpenSSL 1.1.0f  25 May 2017)\r\nPlatform  : Windows-10-10.0.15063-SP0\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312956745/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312957674", "body": "@redapple Thank you", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312957674/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135768674", "body": "Hey guys I'm new to web crawling.Sorry for disturbing you guys.\nI came to know that scrapy is the fastest web framework module for web \ncrawling.\nI thought that you guys would be doing that in multithreading but I \nheard that you guys never use threading.How could this be possible?can \nanyone give a short explanation to me...Please\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135768674/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312883160", "body": "Thank you, missconfiguration.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312883160/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312909683", "body": "https://github.com/kjd/idna/issues/50#issuecomment-312908539", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312909683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "tsgoud69": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2810", "title": "Issue with Scrapy Install", "body": "Hi,\r\nI am new to python and trying to install scrapy in my virtual env. But its fails with following error. Can any one susggest what is the issue?\r\n\r\npython --version\r\nPython 2.7.13\r\n\r\n virtualenv --version\r\n15.1.0\r\n\r\n pip install scrapy\r\nCollecting scrapy\r\n  Using cached Scrapy-1.4.0-py2.py3-none-any.whl\r\nCollecting service-identity (from scrapy)\r\n  Using cached service_identity-17.0.0-py2.py3-none-any.whl\r\nCollecting parsel>=1.1 (from scrapy)\r\n  Downloading parsel-1.2.0-py2.py3-none-any.whl\r\nRequirement already satisfied: six>=1.5.2 in /home/swaraj/E2/VenvP27E1/lib/python2.7/site-packages (from scrapy)\r\nCollecting w3lib>=1.17.0 (from scrapy)\r\n  Downloading w3lib-1.17.0-py2.py3-none-any.whl\r\nCollecting lxml (from scrapy)\r\n  Downloading lxml-3.8.0-cp27-cp27m-manylinux1_x86_64.whl (6.8MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.8MB 172kB/s \r\nCollecting Twisted>=13.1.0 (from scrapy)\r\n  Could not find a version that satisfies the requirement Twisted>=13.1.0 (from scrapy) (from versions: )\r\nNo matching distribution found for Twisted>=13.1.0 (from scrapy)\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2810/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ShayChris": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2808", "title": "Mac os Python pip3 install Scrapy or Twisted gives\u201cCommand \u201dpython setup.py egg_info\u201c failed with error code 1 in /private/tmp/pip-build-51x6vr4n/Twisted/\u201d", "body": "sudo pip3 install Scrapy -i https://pypi.douban.com/simple \r\nCommand \"python setup.py egg_info\" failed with error code 1 in /private/tmp/pip-build-s3504tu6/Twisted/\r\n\r\nso I use sudo pip3 install Scrapy -i https://pypi.douban.com/simple but failed again. \r\nPlease give me some advices, Thank you so much", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2808/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "echoocking": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2806", "title": "error filedescriptor out of range in select()", "body": "2017-06-29 22:21:36 [crawler.download_middlewares.RetryMiddleware] DEBUG: Gave up retrying <GET https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv986&productId=1540024&score=0&sortType=6&page=88&pageSize=10&isShadowSku=0> (failed 3 times): An error occurred while connecting: [Failure instance: Traceback (failure with no frames): <type 'exceptions.ValueError'>: filedescriptor out of range in select()\r\n\r\ni use python 2.7 and scrapy 1.3.3\r\nif you want ,i can upload my code\r\nif you are interest in,and want more information ,please email:echooc@outlook.com \ud83d\udc4d ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2806/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rjbks": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2803", "title": "Feature suggestion: Preserve Header Order", "body": "I know it has been brought up some time ago (issue #223 in 2013 I believe). And at that time it was seen as not necessary with the respondent asking for some example sites which relied on header order. While I cannot say for sure that some of the sites I've had issues with are giving me problems because of header order, there have been a few papers (I can only recall this one for now: http://www.letmetrackyou.org/paper.pdf) which mention header construction/order as a way to fingerprint browsers, or in this case determine whether the UA string has been spoofed. I have tried switching CaselessDict to inherit from OrderedDict switching all references in the class from dict to OrderedDict and the Headers class seems to (in this very limited example) construct the headers appropriately.\r\n\r\nWould there be any drawbacks in making this change?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2803/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2658", "title": "large response causes scrapy to hang", "body": "I have tried stackoverflow but haven't received any answers for this as of yet.\r\n\r\n[scrapy hangs on large response](http://stackoverflow.com/questions/42822010/scrapy-hanging-on-api-call-which-returns-large-json-object)\r\n\r\nSummary:\r\n\r\nUsing scrapy to make thousands of API calls, all of them work fine except for 1 which returns a response of around 2GB (takes roughly 4 mins to complete using requests library). I have increased or disabled the related settings which may limit download size or timeout (DOWNLOAD_MAXSIZE DOWNLOAD_TIMEOUT). The logs show a 200 response then the warning for download size (which I left enabled just to indicate some form of progress) I then get 3 consecutive INFO logs from the stat tracker (1 minute apart), then it just hangs. I have left it running for up to 30 mins then I had to force quit (control-c does not work). Based on that, it would seem to hang after roughly 3 minutes after initiating that API call without any more logging whether I leave the download_timeout enable (for either 6.5 or 7 minutes when several attempts with requests package yields results in about 4 minutes) or disabled. Any ideas what could be causing this?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2658/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dalbani": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2802", "title": "Support ISO 8601 timestamps in logging", "body": "I haven't been able to configure logging via `LOG_DATEFORMAT` to output ISO 8601 timestamps.\r\nGiven that the logging formatter is passed local `time`s apparently (??), I suppose that I need to use the `%z` qualifier.\r\nBut `%z` always returns `+0000` in my configuration (Ubuntu 16.04, Linux 2.7) \u2014 whereas in my case `%Z` correctly produces `CEST`.\r\nMy understanding is that Python's `logging.Formatter()` doesn't support timezone offset: https://stackoverflow.com/questions/27858539/python-logging-module-emits-wrong-timezone-information##answer-27865750\r\nAm I missing something obvious to be able to produce ISO 8601 timestamps in my Scrapy logs?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2802/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Parth-Vader": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2792", "title": "[idea] Adding an option to profile the spider using vmprof", "body": "[vmprof](https://github.com/vmprof/vmprof-python) is a great tool for profiling the spider and the results can be uploaded to it's server (using `--web` option). \r\n\r\nUsers would be able to submit performance issues using the results obtained, and we could check if some parts are taking unexpectedly long time. \r\n\r\nExamples : [Benchmark Spider Result](https://vmprof.com/#/af9672fb-5bb0-4aaf-9214-b1bc496287ea)\r\n\r\nThis could be implemented like the `--profile` option.\r\n\r\ncc @lopuhin ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2792/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2629", "title": "scrapy bench needs to be more complex", "body": "The current benchmarking is done on a very simple page, where the bot just follows all the links. This does not actually give a correct scraping speed for a standard web page. \r\n\r\nIt could contain several images as well, along with some restrictions so that the benchmarking suite could give a realistic approximation of the scraping speed.\r\n\r\nAny other ideas?\r\nping @dangra ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2669", "title": "Use Reppy instead of robotsparser", "body": "Made the changes using https://github.com/scrapy/scrapy/pull/949 for https://github.com/scrapy/scrapy/issues/754.\r\n", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Samarpitr": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2782", "title": "Do not fire close_spider when using ImagesPipeline ", "body": "I am downloading the images by using ImagesPipeline and want to fire a script on the closing of the spider but scrapy close_spider not performing the its functionality. ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2782/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xhujerr": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2761", "title": "Images min dimensions cause unhandled exception", "body": "When I enable in settings.py \r\nIMAGES_MIN_HEIGHT = 350\r\nIMAGES_MIN_WIDTH = 350\r\n\r\nI start getting unhandled exceptions:\r\n> Unhandled error in Deferred:\r\n> Failure: scrapy.pipelines.images.ImageException: Image too small (62x59 < 350x350)\r\n\r\nThe exception looks like this:\r\n```\r\n2017-05-28 19:06:43 [scrapy.pipelines.files] WARNING: File (error): Error processing file from <GET http://example.com/images/0.jpg> referred in <http://www.example.com/>: Image too small (600x200 < 350x350)\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/dist-packages/scrapy/pipelines/files.py\", line 356, in media_downloaded\r\n    checksum = self.file_downloaded(response, request, info)\r\n  File \"/usr/lib/python2.7/dist-packages/scrapy/pipelines/images.py\", line 98, in file_downloaded\r\n    return self.image_downloaded(response, request, info)\r\n  File \"/usr/lib/python2.7/dist-packages/scrapy/pipelines/images.py\", line 102, in image_downloaded\r\n    for path, image, buf in self.get_images(response, request, info):\r\n  File \"/usr/lib/python2.7/dist-packages/scrapy/pipelines/images.py\", line 120, in get_images\r\n    (width, height, self.min_width, self.min_height))\r\nImageException: Image too small (600x200 < 350x350)\r\n```\r\n\r\nWhen I comment `except` statements:\r\nhttps://github.com/scrapy/scrapy/blob/1.3/scrapy/pipelines/files.py#L364\r\nhttps://github.com/scrapy/scrapy/blob/1.3/scrapy/pipelines/files.py#L372\r\nthe exceptions aren't unhandled anymore.\r\n\r\nIt seems to me like a bug. Isn't is? Or am I doing something wrong?\r\n\r\nIt may be something similar to:\r\nhttps://github.com/scrapy/scrapy/issues/1884\r\nbut in my case scrapy really crashes\r\n\r\npython-w3lib:  1.17.0-1~exp1\r\npython-scrapy: 1.3.3-1~exp1\r\nboth from a debian package\r\nPython 2.7.10+", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2761/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Manslow": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2749", "title": "Feature Request: Item class field access and item creation using this", "body": "Items provide a simple way to structure data. However, as items are currently implemented, you can only populate an item by referencing a field using a string that matches the field's name e.g `item['field_name'] = value`. This becomes a problem when you want to change the name of a field after creation. It also requires that you remember the name of the fields defined for an item.\r\n\r\nI think it would be nice to be able to populate items using syntax like the following:\r\n\r\n```\r\nitem = Item()\r\nitem[Item.field] = value\r\n```\r\n\r\nIn this way, you can use code completion in an IDE to be sure that you are referencing a field the Item class has defined and can leverage refactoring to change the names of fields across your entire project wherever Item.field is referenced.\r\n\r\nPerhaps there is a good reason why this option was discarded during development but it isn't obvious to me why.\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2749/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "starrify": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2744", "title": "[idea] HttpCacheMiddleware could be further enhanced", "body": "The current design of `HttpCacheMiddleware`:\r\n- Checks whether a request hits the cache in `process_request`\r\n- Stores a response to the cache storage in `process_response`\r\n\r\nThat makes it possible to download a same resource multiple times within a same job, given that a following request to the same resource comes before the first one returns.\r\n\r\nIt's possible to enhance this middleware by storing the state and returning a deferred when there is a pending request of the same resource.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2744/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2323", "title": "`LxmlLinkExtractor` fails handling unicode netlocs in Python2", "body": "**Affected version:**\ndc1f9ad\n\n**Affected Python version:**\nPython 2 only\n\n**Steps to reproduce:**\n\n``` Python\n>>> import scrapy.http\n>>> response = scrapy.http.TextResponse(url='http://foo.com', body=u'<a href=\"http://foo\\u263a\">', encoding='utf8')\n>>> response.css('a::attr(href)').extract()\n[u'http://foo\\u263a']\n>>> import scrapy.linkextractors\n>>> extractor = scrapy.linkextractors.LinkExtractor()\n>>> extractor.extract_links(response)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/tmp/virtualenv/src/scrapy/scrapy/linkextractors/lxmlhtml.py\", line 111, in extract_links\n    all_links.extend(self._process_links(links))\n  File \"/tmp/virtualenv/src/scrapy/scrapy/linkextractors/__init__.py\", line 104, in _process_links\n    link.url = canonicalize_url(urlparse(link.url))\n  File \"/tmp/virtualenv/lib/python2.7/site-packages/w3lib/url.py\", line 354, in canonicalize_url\n    parse_url(url), encoding=encoding)\n  File \"/tmp/virtualenv/lib/python2.7/site-packages/w3lib/url.py\", line 298, in _safe_ParseResult\n    netloc = parts.netloc.encode('idna')\n  File \"/tmp/virtualenv/lib/python2.7/encodings/idna.py\", line 164, in encode\n    result.append(ToASCII(label))\n  File \"/tmp/virtualenv/lib/python2.7/encodings/idna.py\", line 76, in ToASCII\n    label = nameprep(label)\n  File \"/tmp/virtualenv/lib/python2.7/encodings/idna.py\", line 21, in nameprep\n    newlabel.append(stringprep.map_table_b2(c))\n  File \"/usr/lib64/python2.7/stringprep.py\", line 197, in map_table_b2\n    b = unicodedata.normalize(\"NFKC\", al)\nTypeError: normalize() argument 2 must be unicode, not str\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2323/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/11a1f970b7b7b68b3d968df4b29c4269ab220ac6", "message": "Added: HTTP status code 522/524 to retry."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f712513ed709e2e6d71a19b4e259934fd6f86955", "message": "Added doc for `scrapy.exceptions.DontCloseSpider`. Also fixes inaccurate doc for `scrapy.signals.spider_idle`."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/b33e0d5a54a90588e02396e7a2162d4ea12ae7dd", "message": "Added: Now supporting <link> tags in Response.follow"}], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2988", "title": "Added: Customizing request fingerprint calculation via request meta", "body": "Partially implements #900", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2984", "title": "Added: Making the list of exceptions to retry configurable via settings", "body": "Implements #2701", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1582", "title": "added: Doc for `scrapy.http.TextResponse.urljoin`", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wRAR": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2735", "title": "Functions don't work as item processors in Python 3", "body": "The documentation says \"processors are just callable objects, which are called with the data to be parsed, and return a parsed value. So you can use any function as input or output processor\" and this works in Python 2:\r\n```python\r\ndef proc(i):\r\n    return i\r\n\r\nclass FooItemLoader(ItemLoader):\r\n    foo_in = proc\r\n```\r\nBut `FooItemLoader.foo_in` is a bound method in Python 3 so it receives a `self` argument and not just the field value.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2735/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zhongdai": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2733", "title": "Import Request in the Template file", "body": "I have been creating many spiders recently, and I noticed I had to add below line every time\r\n\r\n`from scrapy.http import Request`\r\n\r\nI think for most cases we need the Request to crawl to other pages, only for very simple spider we don't need that. Is it possible to include that line to the template?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2733/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alex": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2726", "title": "Expose certificate information on HTTPS responses", "body": "For scraper I'm working on, I'd like to have access to the certificate supplied by the server for HTTPS requests. As far as I can tell, this is not currently exposed on either requests or responses.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2726/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "csalazar": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2719", "title": "Misleading information in scrapy", "body": "Hi guys, I want to notify three misleading issues in scrapy information. \r\n\r\nFirst one, when a site sets two cookies, `response.headers` shows the key `Set-Cookie` with the data of the last key as a string. \r\n\r\n```python\r\n{\r\n    'Set-Cookie':` 'laravel_session=...; expires=Thu, 27-Apr-2017 14:18:16 GMT; Max-Age=7200; path=/; httponly',\r\n    [...]\r\n}\r\n```\r\n\r\nHowever, I can use `response.headers.getlist('Set-Cookie')` to get both two cookies, but I think the information returned by `response.headers` is misleading since it shows the value as a string and not as a list. It's rational to think that due to dictionary nature when there are two keys `Set-Cookie` the last will override the former one, then people will guess it's a problem of Scrapy handling cookies.\r\n\r\nThe second issue is that `custom_settings` in a spider won't be shown in the list of `Overriden settings` in scrapy log. I've created a vanilla project and with this spider as example:\r\n\r\n```python\r\nimport scrapy\r\n\r\n\r\nclass ExampleSpider(scrapy.Spider):\r\n    name = 'example'\r\n    allowed_domains = ['example.com']\r\n    start_urls = ['http://example.com/']\r\n    custom_settings = {\r\n        'COOKIES_ENABLED': False\r\n    }\r\n\r\n    def parse(self, response):\r\n        pass\r\n```\r\n\r\nWhen I run the spider, the output is:\r\n\r\n```\r\n2017-04-27 09:33:42 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: set)\r\n2017-04-27 09:33:42 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'set', 'NEWSPIDER_MODULE': 'set.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['set.spiders']}\r\n```\r\n\r\n`COOKIES_ENABLED` doesn't appears on the list but it's actually `False` in the logic, then it's just an information issue. I was in a project where `COOKIES_ENABLED=False` was set in `settings.py`, then when I set `COOKIES_ENABLED=True` in a spider it continued to show `COOKIES_ENABLED=False` in the spider log, making a confusion if `custom_settings` were already taken in consideration or not.\r\n\r\nThe last one is that I'm using master branch, with version `1.3.3` already released and `scrapy version` command shows that I'm using `1.3.2`.\r\n\r\nI could make a pull request, just let me know if there's some technical reason of that behavior in any of the three issues.\r\n\r\n--\r\n\r\n`scrapy version -v` output: \r\n\r\n```\r\n$ scrapy version -v\r\nScrapy    : 1.3.2\r\nlxml      : 3.7.3.0\r\nlibxml2   : 2.9.3\r\ncssselect : 1.0.1\r\nparsel    : 1.1.0\r\nw3lib     : 1.17.0\r\nTwisted   : 17.1.0\r\nPython    : 3.6.1 |Continuum Analytics, Inc.| (default, Mar 22 2017, 19:54:23) - [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\r\npyOpenSSL : 17.0.0 (OpenSSL 1.0.2k  26 Jan 2017)\r\nPlatform  : Linux-4.4.0-75-generic-x86_64-with-debian-stretch-sid\r\n```", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2719/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "povilasb": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2717", "title": "TLS handshake failure", "body": "I have this simple spider:\r\n```\r\nimport scrapy\r\n\r\n\r\nclass FailingSpider(scrapy.Spider):\r\n    name = 'Failing Spider'\r\n    start_urls = ['https://www.skelbiu.lt/']\r\n\r\n    def parse(self, response: scrapy.http.Response) -> None:\r\n        pass\r\n```\r\nOn debian 9 it fails with:\r\n```\r\n2017-04-25 19:01:39 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.skelbiu.lt/>\r\nTraceback (most recent call last):\r\n  File \"/home/povilas/projects/skelbiu-scraper/pyenv/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"/home/povilas/projects/skelbiu-scraper/pyenv/lib/python3.6/site-packages/twisted/python/failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"/home/povilas/projects/skelbiu-scraper/pyenv/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'sslv3 alert handshake failure')]>]\r\n```\r\nOn debian 8 it works well.\r\nAnd \"https://www.skelbiu.lt\" is the only target I can reproduce the problem.\r\n\r\nSome more context:\r\n```\r\n$ pyenv/bin/pip freeze\r\nasn1crypto==0.22.0\r\nattrs==16.3.0\r\nAutomat==0.5.0\r\ncffi==1.10.0\r\nconstantly==15.1.0\r\ncryptography==1.8.1\r\ncssselect==1.0.1\r\nfuncsigs==0.4\r\nidna==2.5\r\nincremental==16.10.1\r\nlxml==3.7.3\r\nmock==1.3.0\r\npackaging==16.8\r\nparsel==1.1.0\r\npbr==3.0.0\r\npy==1.4.33\r\npyasn1==0.2.3\r\npyasn1-modules==0.0.8\r\npycparser==2.17\r\nPyDispatcher==2.0.5\r\nPyHamcrest==1.8.5\r\npyOpenSSL==17.0.0\r\npyparsing==2.2.0\r\npytest==2.7.2\r\nqueuelib==1.4.2\r\nScrapy==1.3.3\r\nservice-identity==16.0.0\r\nsix==1.10.0\r\nTwisted==17.1.0\r\nw3lib==1.17.0\r\nzope.interface==4.4.0\r\n\r\n$ dpkg --get-selections | grep libssl\r\nlibssl-dev:amd64                                install\r\nlibssl-doc                                      install\r\nlibssl1.0.2:amd64                               install\r\nlibssl1.1:amd64                                 install\r\nlibssl1.1:i386                                  install\r\n\r\n\r\n$ apt-cache show libssl1.1\r\nPackage: libssl1.1\r\nSource: openssl\r\nVersion: 1.1.0e-1\r\n```\r\n\r\nAny ideas what I should look for? :)", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2717/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kspiridonova": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2711", "title": "Scrapy capitalizes headers for request", "body": "I'm setting the headers following way\r\n```python\r\nheaders = {\r\n    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\r\n    'cache-control': 'no-cache',\r\n    ...\r\n}\r\n```\r\nAnd calling request like that:\r\n```\r\nyield scrapy.Request(url='https:/myurl.com/', callback=self.parse, headers=headers, cookies=cookies, meta={'proxy': 'http://localhost:8888'})\r\n```\r\nAnd it makes that scrapy capitalizes all these headers and it looks like that (I'm using Charles proxy for debugging):\r\n```\r\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\r\nCache-Control: no-cache\r\n```\r\nAnd this is not working correctly for my case.\r\n\r\nIf I'm using curl and set headers lowercase\r\n```\r\naccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\r\ncache-control: no-cache\r\n```\r\neverything works like a charm.\r\n\r\nIs there any way how I can disable this capitalizing behavior in Scrapy?\r\nThanks for any help!", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2711/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tad3j": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2701", "title": "Make Exceptions Handled by Retry Middleware Configurable", "body": "I was having troubles finding which exceptions are handled by Retry Middleware, so I asked on freenode scrapy irc channel (should obviously check the source code). Paul from scrapinhub helped me there and pointed me to the source code. \r\n\r\nBeside that he also asked me to open an issue here.\r\nHe gave a great suggestion to make exceptions handled by RetryMiddleware configurable, so if we don't like defaults we can configure our own.\r\n\r\nBeside that he said that the handled exceptions in question are not documented well; I'm just mentioning it here, didn't want to open another issue (I guess by adding them to settings will also require some documentation).\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2701/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "swoertz": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2680", "title": "name collision of spider with existing module", "body": "The problem arises when I generate a spider with the name of an already existing module e.g., csv, urllib,...\r\nMy guess is that the import mechanism imports the existing module, which most likely does not contain a spider, instead of the generated spider. Hence the no spider found message.\r\n\r\n```\r\n$ scrapy version\r\nScrapy 1.3.3\r\n$ scrapy genspider csv csv.com\r\nCreated spider 'csv' using template 'basic' \r\n$ scrapy runspider csv.py \r\n2017-03-23 16:55:14 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\r\n2017-03-23 16:55:14 [scrapy.utils.log] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True}\r\nUsage\r\n=====\r\n  scrapy runspider [options] <spider_file>\r\n\r\nrunspider: error: No spider found in file: csv.py\r\n```", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2680/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "debosmit": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2645", "title": "Order of calling close_spider in pipelines", "body": "`process_item` is called based on the order of the pipeline classes mentioned in [`ITEM_PIPELINES`](https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-ITEM_PIPELINES) setting. But, `close_spider` follows the exact opposite order with the `close_spider` of the last pipeline getting called first [[link](https://github.com/scrapy/scrapy/blob/129421c7e31b89b9b0f9c5f7d8ae59e47df36091/scrapy/middleware.py#L64)].\r\n\r\nI can't think of a good reason for this. Should I submit a PR?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2645/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2171", "title": "Checking if defer gets an error callback while in start_request and killing process", "body": "This is to account for the case that an exception is being raised in the start_requests method. In `scrapy/crawler.py:Crawler/crawl` the values yielded are left unused. `scrapy/commands/crawl.py:Command.run` does nothing with the deferred that is yielded. This fix checks if an error is raised and calls `sys.exit(1)` if it is. The stack-trace leading to the error is also logged.\n\nThis has been tracked in issue [1231](https://github.com/scrapy/scrapy/issues/1231) and [1241](https://github.com/scrapy/scrapy/issues/1241).\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jorenham": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2633", "title": "Base classes for the item pipeline and middleware", "body": "In the [pipeline docs](https://doc.scrapy.org/en/master/topics/item-pipeline.html) it says:\r\n\r\n> Each item pipeline component is a Python class that must implement the following method\r\n\r\nI believe that it would be easier for the user and for documentation purposes to have an abstract class e.g. `ItemPipeline` available that raises a `NotImplementedError` if they forget to implement the required methods. This could also be extended to the optional methods so the user can see directly from the code which methods are supported.\r\n\r\nThis could also be applied to the downloader middleware, spider middleware and extensions.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2633/reactions", "total_count": 5, "+1": 5, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/3cd9185aa12430708b17eb8c02a6bdc3709ed5f9", "message": "Fixed the FIXME; more specific exception catching"}], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2666", "title": "Base class for extensions", "body": "As part of #2633 ", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2664", "title": "Base class for spider middleware", "body": "As part of #2633", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2657", "title": "Base class for downloader middleware", "body": "As part of #2633", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2651", "title": "Base class for item pipelines", "body": "Concerns #2633 ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "harshasrinivas": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2624", "title": "scrapy shell - bug in processing escaped URLs", "body": "For eg: URL - `https://altoona.craigslist.org/search/sss?query=cars&sort=rel&searchNearby=1` when pasted on the command line (shell-specific) creates a few escaping characters - backslashes\r\n\r\nThe following command gives a 404:\r\n```sh\r\n$ scrapy shell 'https://altoona.craigslist.org/search/sss\\?query\\=cars\\&sort\\=rel\\&searchNearby\\=1'\r\n```\r\n\r\nI saw a similar discussion in [#1232](https://github.com/scrapy/scrapy/issues/1232) - where `urllib.parse.unquote()` was used to fix it.\r\n\r\nIn this case even `unquote` doesn't seem to work.\r\n```python\r\n>>> unquote('https://altoona.craigslist.org/search/sss\\?query\\=cars\\&sort\\=rel\\&searchNearby\\=1')\r\n'https://altoona.craigslist.org/search/sss\\\\?query\\\\=cars\\\\&sort\\\\=rel\\\\&searchNearby\\\\=1'\r\n```\r\n\r\nEven though [URL-character escaping is an issue specific to the shell](http://unix.stackexchange.com/questions/293472/paste-url-into-terminalurxvt-zsh-failed-some-characters-get-escaped), wouldn't it be awesome if Scrapy processes such URLs automatically?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2624/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/2ff6b0572318742f1b16ee7f9f0c3b836b020633", "message": "Remove __nonzero__ from SelectorList docs"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/0298bcbe79ec9c010282eb59d79848f17bada7ed", "message": "Update Makefile to open webbrowser in MacOS (#2661)"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/38e6857c957ef023533128f054688152be223c87", "message": "Improvise the clarity of test cases"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/83aa0c5e1a971c07d0beab89fde91ce1184f098c", "message": "Clarify docs readme"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/4ec07ae7640df6fdb13a24a4265232fd11e53755", "message": "Create docs/requirements.txt"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/a57e49d55b7d703c8e5153811d6f5de3eacd2272", "message": "Add sphinx_rtd_theme to docs setup readme"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/10741aca720293a12dedda4d1872cf0604b49f0b", "message": "Update docs - improve clarity"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/0d9ebd6e1ed58654ea1996d5a236b4d0240590df", "message": "Update tests for max_retry_times"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/49c5afc5ff6c810c78678ea1c86beb19ca3487ad", "message": "Fix bug involving OR condition"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9d97d788c06c2e8fbbdfbcfbee65321ac2dfb517", "message": "Update docs for meta key"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/e321ac9931007360a38dbd6a5933794af415274b", "message": "Update unittests for max_retry_times"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/966bd49c421fdf40a8b21b49c76cd54ded06fe50", "message": "Update unittest for meta['max_retry_times']"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/694c6d3d7460ab80ace979c4563af8f310614d37", "message": "Simplify retry_times assignment statement"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/0d57b5cd43343a335fcf2e923b29e76d09dd0b51", "message": "Prevent max_retry_times override"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/810658bcc5b1897d57c0882a0f7ab4a0d264a778", "message": "Add feature to set RETRY_TIMES per request (#2642)"}], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2688", "title": "Update requirements for twisted[tls]", "body": "Related to #2473 ", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2685", "title": "runspider - warn in case of multiple spiders", "body": "Fixes #2608 ", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2631", "title": "Add note for processing escaped URLs in scrapy shell (#2624)", "body": "Added `Note` as discussed in #2624 ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pawelmhm": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2607", "title": "add params kwarg to scrapy.Request()", "body": "python-requests have useful params kwargs, you can do\r\n\r\n```python\r\nurl = 'http://aaa'\r\nparams = {\r\n    'expand': 'variations,informationBlocks,customisations',\r\n}\r\n\r\nresponse = requests.request(\r\n    method='GET',\r\n    url=url,\r\n    headers=headers,\r\n    params=params,\r\n)\r\n```\r\nand it will resut in `http://aaa?expand=variations,informationBlocks,customizations`\r\n\r\nwhat do you think about adding params kwarg to scrapy,Request()? It would simplify work, there would be no need to urlencode querystring if it's a dict and concatenate strings for url. ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2607/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2552", "title": "scrapy.Request no init error on invalid url", "body": "I stumbled on some weird issue, spider got some invalid url, but instead of crashing loudly when trying to create scrapy.Request() with invalid url it just silently ignored this error. Sample to reproduce\r\n\r\n```python\r\nfrom scrapy.spiders import Spider\r\nfrom scrapy import Request\r\n\r\n\r\nclass DmozSpider(Spider):\r\n    name = \"dmoz\"\r\n    allowed_domains = [\"dmoz.org\"]\r\n    start_urls = [\r\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\r\n    ]\r\n\r\n    def parse(self, response):\r\n        invalid_url = \"/container.productlist.productslist.productthumbnail.articledetaillink.layerlink:open-layer/0/CLASSIC/-1/WEB$007cARBO$007c13263065/null$007cDisplay$0020Product$002f111499$002fAil$0020blanc$007c?t:ac=13263065\"\r\n        yield Request(invalid_url)\r\n```\r\n\r\nthis generates following output:\r\n\r\n```\r\n2017-02-09 12:21:04 [scrapy.core.engine] INFO: Spider opened\r\n2017-02-09 12:21:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2017-02-09 12:21:04 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024\r\n2017-02-09 12:21:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\r\n2017-02-09 12:21:04 [scrapy.core.engine] INFO: Closing spider (finished)\r\n```\r\n\r\nthere is no information about trying to generate this Request with invalid_url, no stacktrace, no error info from middleware. Why?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2552/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2447", "title": "memory leaks in image pipeline", "body": "It seems to me that image pipeline is leaking memory in a very significant ways. I have spider that downloads lists of images. There were always problems with memory when downloading images, but now my list of images to download got larger and I thought about opening issue here.\r\n\r\nBasically after opening some images memory usage goes up and stays up (it's not reset to previous value). It might be some issue with PIL or it might be something we're doing in pipeline that is causing this. In any case this looks worrying and I think we should reflect on steps to take to limit this problem.\r\n\r\nFollowing code reproduces the problem (I know it's long but this is really shortest I could get), it relies on presence of images.txt file that contains list of urls to images.\r\n\r\n```python\r\nimport resource\r\nimport shutil\r\nimport sys\r\nimport tempfile\r\n\r\nimport scrapy\r\nfrom scrapy.pipelines.images import ImagesPipeline\r\nfrom scrapy.utils.test import get_crawler\r\nfrom twisted.internet import reactor\r\nfrom twisted.python import log\r\n\r\n\r\ndef log_memory(result):\r\n    mem = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\r\n    log.msg(\"{} bytes\".format(mem))\r\n\r\n\r\nclass SomeSpider(scrapy.Spider):\r\n    name = 'foo'\r\n\r\n\r\ndef download_image(url, pipe, spider):\r\n    # Download image with pipeline.\r\n    item = {\r\n        'image_urls': [url]\r\n    }\r\n    dfd = pipe.process_item(item, spider)\r\n    dfd.addBoth(log_memory)\r\n    return dfd\r\n\r\n\r\nlog.startLogging(sys.stdout)\r\n# Directory must be removed, otherwise pipeline will not attempt to download.\r\nsome_dir = tempfile.mkdtemp()\r\ncrawler = get_crawler(settings_dict={'IMAGES_STORE': some_dir,\r\n                                     \"IMAGE_EXPIRES\": 0})\r\n\r\nspider = SomeSpider()\r\nspider.crawler = crawler\r\ncrawler.crawl(spider)\r\npipeline = ImagesPipeline.from_crawler(crawler)\r\npipeline.open_spider(spider)\r\n\r\n\r\ndef clean_up():\r\n    print(\"removing {}\".format(some_dir))\r\n    log_memory(None)\r\n    shutil.rmtree(some_dir)\r\n\r\n\r\nwith open('images.txt') as image_list:\r\n    image_urls = image_list.read().split()\r\n\r\nfor url in image_urls[:20]:\r\n    dfd = download_image(url, pipeline, spider)\r\n\r\nreactor.addSystemEventTrigger('before', 'shutdown', clean_up)\r\nreactor.run()\r\n```\r\n\r\nSample output on attached image file: [images.txt](https://github.com/scrapy/scrapy/files/648899/images.txt)\r\n\r\n```\r\n2016-12-13 13:13:00+0100 [-] Log opened.\r\n2016-12-13 13:13:00+0100 [-] TelnetConsole starting on 6023\r\n2016-12-13 13:13:00+0100 [-] 44516 bytes\r\n2016-12-13 13:13:00+0100 [-] 44752 bytes\r\n2016-12-13 13:13:00+0100 [-] 49492 bytes\r\n2016-12-13 13:13:01+0100 [-] 49680 bytes\r\n2016-12-13 13:13:01+0100 [-] 49680 bytes\r\n2016-12-13 13:13:01+0100 [-] 49680 bytes\r\n2016-12-13 13:13:01+0100 [-] 52312 bytes\r\n2016-12-13 13:13:01+0100 [-] 52312 bytes\r\n2016-12-13 13:13:01+0100 [-] 52316 bytes\r\n2016-12-13 13:13:01+0100 [-] 52316 bytes\r\n2016-12-13 13:13:01+0100 [-] 52316 bytes\r\n2016-12-13 13:13:01+0100 [-] 52532 bytes\r\n2016-12-13 13:13:01+0100 [-] 52532 bytes\r\n2016-12-13 13:13:01+0100 [-] 52700 bytes\r\n2016-12-13 13:13:01+0100 [-] 52700 bytes\r\n2016-12-13 13:13:01+0100 [-] 52700 bytes\r\n2016-12-13 13:13:01+0100 [-] 52700 bytes\r\n2016-12-13 13:13:01+0100 [-] 52700 bytes\r\n2016-12-13 13:13:02+0100 [-] 52700 bytes\r\n2016-12-13 13:13:03+0100 [-] (TCP Port 6023 Closed)\r\n2016-12-13 13:13:03+0100 [-] 52700 bytes\r\n^C2016-12-13 13:13:13+0100 [-] Received SIGINT, shutting down.\r\n2016-12-13 13:13:13+0100 [-] removing /tmp/tmpD1dRmc\r\n2016-12-13 13:13:13+0100 [-] 52700 bytes\r\n2016-12-13 13:13:13+0100 [-] Main loop terminated.\r\n```\r\n\r\nNotice how memory goes up and stays up. from 44516 to 52700. Notice delay between final request and SIGINT ( 10 seconds). After this delay memory usage still stays at 52700.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2447/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2444", "title": "response.json()?", "body": "python-requests have response.json() that decodes json body and returns appropriate Python objects. Does it make sense to have something like this in Scrapy? ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2444/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2241", "title": "make offsite middleware downloader middleware instead of spider middleware", "body": "Currently [offsite middleware](http://doc.scrapy.org/en/1.1/topics/spider-middleware.html?highlight=offsite#module-scrapy.spidermiddlewares.offsite) is spider middleware. It works only on spider output, only processes requests generated in spider callback. But requests in Scrapy can be scheduled outside spider callback. This can happen in following cases:\n- requests scheduled on `start_requests`\n- requests scheduled with `crawler.engine.crawl()`, `crawler.engine.download()`, `crawler.engine.schedule`\n\nSecond case (downloading with crawler.engine.crawl()) is very common in my experience. Many spiders get lists of urls from some external source and schedule requests on `spider_idle` signal. Offsite middleware wont work properly for them, allowed_domains attribute will not have effect.\n\nI think we should make offsite middleware downloader middleware and make it work for all requests or at least discuss and clarify why we want to keep it as spider middleware and document that it does not work on all possible requests.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2241/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2007", "title": "Scrapy websockets downloader", "body": "Seems like now Scrapy does not provide any way to crawl website that use websockets. Can we support something like this? There are websocket implementations based on Twisted: http://autobahn.ws/python/ so probably it could be possible to add some downloader for websockets? I know that HTTP is main protocol used by majority of spiders, but we also support FTP or s3 so maybe we could support websockets too?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2007/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/1918", "title": "scrapy contracts issues", "body": "I tried using Scrapy contracts today and found following problems, I'm creating issue for future reference in case others find it helpful (perhaps it could also serve as guide for future improvements). \n\nIt's very difficult (or even impossible) to customize test case to suit your needs. In my project some class of spiders always expect response with some meta keys. This seems like common use case, but there is no easy way to pass meta to spider contract. I found out the only way is to create custom contract that updates args for request, but this custom contract has to be added to contract docstring as well which is not documented (I thought adding custom contract to settings is enough, but no you also have to add @custom_contract_name to docstring). \n\nThere is no easy way to customize request being made from contract. I need to test callback that is response to POST with some formdata, headers, cookies and some meta keys. Passing all those values to Request init should do the trick, but there is no simple api to do that. \n\nThere is no timeouts for some tests, I noticed that one of the spider was hanging for long time, it would be better to close it down rather than wait for it to end. \n\nThere is no way to pass command line arguments to spider test. It would be very useful but currently the way contracts are designed you cannot do this in any way (you can create custom contract that will pass arguments to Request(), but there's no contract for spider init). \n\nI think the problems are mostly result of docstring test format. Ideally you should be able to specify meta in dosctring, e.g.\n\n```\n@url http://example.com\n@meta {\"a\": \"b\"}\n@return Item\n```\n\nbut with current implementation args for meta will be parsed as strings and they are split on whitespace, so to parse this you'd have to actually write some JSON and this JSON must be without whitespace. Same for specyfing request init args. Ideally I would do something like this\n\n```\n@request {\"method\": \"POST\", \"headers\": {\"foo\":\"bar\"}}\n```\n\nbut this is not possible with current implementation.\n\nHow about switching to yaml? This will allow us to have test description like this\n\n``` yaml\n\nrequest: \n    url: http://example.com\n    method: POST,\n    meta : \n         variant_request: True\n         item: \n               name: foobar\n    headers:\n          header-one: header-value\n          header-two: another-value\nspider_init_args:\n    zipcode: 14001\nreturns:\n     item: \n         name: \"bar\"\n```\n\nthis should allow detailed specification of test case. Yaml would be processed into python dict, and from this dict we could create spider test cases that will control spider init, request init and add proper tests on output. Above would translate to following description: \"create POST Request with following meta and following headers; initialize spider with following argument; return item with name bar\". \n\nAside from switching to yaml we could move contracts out of spider docstrings. Maybe this yaml file with test specs could be stored outside spider code, e.g. you could have folder spider_tests full of yaml files describing each test case. In case your tests grow large you can easily manage that, you have it in yaml so there is nice syntax highlighting and everything is more readable. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/1918/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/4345eaf1b640bfecb2340a0e27649b1ab1079da6", "message": "[logformatter] backward compat comments"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/0f2a5cdb8edb3d4d6ce542f13b5d7e5858905bae", "message": "[logformatter] 'flags' format spec backward compatibility\n\npass 'flags' kwarg to logger so that it is compatible with old\nformat of CRAWLEDMSG."}], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2434", "title": "[logging] log in colors", "body": "This adds logging in colors. I still need to add tests for this but wanted to share for early feedback, let me know what you think. I used colorama (for different colors depending on log message) and pygments (for syntax highlighting of item fields). Sample output looks like this \r\n![color_log](https://cloud.githubusercontent.com/assets/2700942/21010948/1b001562-bd4f-11e6-83cf-fe8969df35c6.png)\r\n![colors_error](https://cloud.githubusercontent.com/assets/2700942/21010960/3421be10-bd4f-11e6-9f7b-23e65ec85509.png)\r\n\r\nLet me know what you think, I'll add tests and docs bit later. \r\n", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "immerrr": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2600", "title": "Add a RequestSet class for grouping requests", "body": "This came up in #2582.\r\n\r\nI think scrapy lacks a means of grouping requests and sharing some state between them that would become irrelevant once the last request in the group has been dealt with and should be cleaned up. Tracking groups manually with request callbacks & errorbacks is error-prone and hard to do right. A RequestSet class that allows the user to do something like\r\n\r\n```\r\ndef parse(self, response):\r\n    ....\r\n    yield RequestSet(Request(...) for ...)\r\n```\r\n\r\nand forget about shared state management would be nice.\r\n\r\nI think of RequestSet as something that:\r\n\r\n- has a `DeferredList`-like API (`asyncio.gather` has the drawback of being a function rather than an object)\r\n- ~~has its own `callback` to be run when the request set is dealt with, probably an `errback` too, for symmetry~~ has a Deferred that would fire when the RequestSet is being cleaned up\r\n- has its own `'meta'` dictionary, much like the one shared between Request & Response objects\r\n- knows that it contains Requests as deferreds\r\n- since it knows that it contains Requests, it can piggyback on the first received value, which should be a response, and do `response.meta['request_set'] = self` so that the callbacks can access the shared data\r\n- (maybe) it should silently copy fields from `request_set.meta` to `response.meta` if they are unset in `request.meta`, or maybe even make `request.meta` a ChainDict with fallback to `request_set.meta` so that middlewares don't have to trace through nested metadata dictionaries to check the fields they are interested in\r\n- it should wrap requests coming from its respective response callbacks unless specifically asked not to do that, e.g. with `request.meta['request_set'] = None`\r\n- (maybe) it should be possible to return other RequestSet from response callbacks \r\n- (maybe) returned RequestSets should be made nestable, i.e. to keep the parent RequestSet alive during their lifetime if not explicitly asked not to with `request_set.meta['request_set'] = None` (if nesting is considered, the `request_set` metadata key seems redundant and we might consider `parent_set` instead.\r\n- not sure if it's worth it to make them nestable, i.e. if a certain response callback produces a different RequestSet, should it be owned by the parent request set?\r\n\r\nOne more thing to consider is cross-referencing RequestSets, i.e. when two requests that should belong to one RequestSet are produced by different callbacks and thus have different scopes. Maybe a simple `WeakValueDictionary` would suffice to lookup the sets and ensure the references are cleaned up as necessary. But then you'd have the usual get-or-create operation, that might be worth creating an etalon implementation for.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2600/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2986", "title": "WIP: CookiesMiddleware: add \"reset_cookies\" meta to clear the jar", "body": "This PR adds a `reset_cookies` meta to clean the active cookiejar.\r\n\r\nWhen I try working with sessions I often find myself in a situation when I'd like to restart the session \"from scratch,\" including cookies. It's doable by just setting `cookiejar` meta to an arbitrary value, but then we'd be accumulating the cookiejars over time.\r\n\r\nI'm not entirely sure about the meta name, as I'd probably like it namespaced, e.g. `cookies_reset`, but there's a precedent already with `dont_merge_cookies`, so I chose to follow that pattern.\r\n\r\n@kmike am I missing something here?", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2985", "title": "utils.curl: add parse_curl_cmd func", "body": "Given that the major browsers are able to export the requests in cURL format, it's logical that we have a utility in scrapy to make it a request.\r\n\r\nThis PR works towards adding a function that creates kwargs that could be passed to a `Request` constructor.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2950", "title": "loader add_* funcs: pass **kw to self.selector.xpath", "body": "I have found out that kwargs that one specifies for `loader.add_xpath` are not getting to the actual `selector.xpath` invocation, which looks like a bug.\r\n\r\nI wonder if there's code out there that would be broken by this fix...", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2775", "title": "When item export fails, mention the item itself in error msg", "body": "I have recently run into an error when an item that looked like `{\"foo\": {None: \"bar\"}}` broke the exporter. I was able to deduce that there was a `None` key in the dictionary from the error message, but I was clueless as to where and how did it happen:\r\n```\r\nTraceback:\r\n  ...\r\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/exporters.py\", line 289, in export_item\r\n    result = dict(self._get_serialized_fields(item))\r\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/exporters.py\", line 75, in _get_serialized_fields\r\n    value = self.serialize_field(field, field_name, item[field_name])\r\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/exporters.py\", line 269, in serialize_field\r\n    return serializer(value)\r\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/exporters.py\", line 275, in _serialize_value\r\n    return dict(self._serialize_dict(value))\r\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/exporters.py\", line 285, in _serialize_dict\r\n    key = to_bytes(key) if self.binary else key\r\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/utils/python.py\", line 117, in to_bytes\r\n    'object, got %s' % type(text).__name__)\r\nTypeError: to_bytes must receive a unicode, str or bytes object, got NoneType\r\n```\r\n\r\nI have tweaked the exporter classes so that the error message now includes the culprit item itself and it  looks like this:\r\n```\r\nIn [9]: try: exporter.export_item({'foo': {None: 'bar'}})\r\n   ...: except Exception: traceback.print_exc()\r\n   ...: \r\nTraceback (most recent call last):\r\n  File \"<ipython-input-9-6392cb271d68>\", line 1, in <module>\r\n    try: exporter.export_item({'foo': {None: 'bar'}})\r\n  File \"scrapy/exporters.py\", line 50, in export_item\r\n    six.reraise(type(new_err), new_err, exc_info[2])\r\n  File \"scrapy/exporters.py\", line 45, in export_item\r\n    return self._export_item_impl(item)\r\n  File \"scrapy/exporters.py\", line 345, in _export_item_impl\r\n    result = dict(self._get_serialized_fields(item))\r\n  File \"scrapy/exporters.py\", line 85, in _get_serialized_fields\r\n    value = self.serialize_field(field, field_name, item[field_name])\r\n  File \"scrapy/exporters.py\", line 325, in serialize_field\r\n    return serializer(value)\r\n  File \"scrapy/exporters.py\", line 331, in _serialize_value\r\n    return dict(self._serialize_dict(value))\r\n  File \"scrapy/exporters.py\", line 341, in _serialize_dict\r\n    key = to_bytes(key) if self.binary else key\r\n  File \"scrapy/utils/python.py\", line 117, in to_bytes\r\n    'object, got %s' % type(text).__name__)\r\nValueError: Cannot serialize item: TypeError: to_bytes must receive a unicode, str or bytes object, got NoneType\r\nitem={'foo': {None: 'bar'}}\r\n```\r\n\r\nThe PR slightly tweaks the implementation details for `ItemExporters.export_item`: \r\n- ideally, you'd now override `_export_item_impl` function instead of `export_item` itself, but classes inherited from the old version should work as before\r\n- if there's code that catches exceptions from `export_item`, it might break now that it only throws `ValueError` instances\r\n - \"Item Exporters\" page in the doc should be updated, I'll do it if the PR is OK otherwise", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "liveresume": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2594", "title": "Jupyter run error ReactorNotRestartable", "body": "I am able to run Scrapy in a Jupyter notebook.  The first time it works fine.  \r\nHowever any subsequent attempts will fail with errors below.\r\n\r\nTo get it working again I must restart the python kernel.  It looks like a problem of starting a reactor when one is already up and running.\r\n\r\n```python\r\nfrom scrapy.crawler import CrawlerProcess\r\n \r\nprocess = CrawlerProcess({\r\n    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\r\n})\r\nprocess.crawl(BlogPostSpider)\r\nprocess.start() # the script will block here until the crawling is finished\r\n\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nReactorNotRestartable                     Traceback (most recent call last)\r\n<ipython-input-8-6d30d0f44f41> in <module>()\r\n     13 })\r\n     14 process.crawl(spider)\r\n---> 15 process.start() # the script will block here until the crawling is finished\r\n     16 \r\n     17 \r\n\r\n/opt/conda/lib/python3.5/site-packages/scrapy/crawler.py in start(self, stop_after_crawl)\r\n    278         tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\r\n    279         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\r\n--> 280         reactor.run(installSignalHandlers=False)  # blocking call\r\n    281 \r\n    282     def _get_dns_resolver(self):\r\n\r\n/opt/conda/lib/python3.5/site-packages/twisted/internet/base.py in run(self, installSignalHandlers)\r\n   1240 \r\n   1241     def run(self, installSignalHandlers=True):\r\n-> 1242         self.startRunning(installSignalHandlers=installSignalHandlers)\r\n   1243         self.mainLoop()\r\n   1244 \r\n\r\n/opt/conda/lib/python3.5/site-packages/twisted/internet/base.py in startRunning(self, installSignalHandlers)\r\n   1220         \"\"\"\r\n   1221         self._installSignalHandlers = installSignalHandlers\r\n-> 1222         ReactorBase.startRunning(self)\r\n   1223 \r\n   1224 \r\n\r\n/opt/conda/lib/python3.5/site-packages/twisted/internet/base.py in startRunning(self)\r\n    728             raise error.ReactorAlreadyRunning()\r\n    729         if self._startedBefore:\r\n--> 730             raise error.ReactorNotRestartable()\r\n    731         self._started = True\r\n    732         self._stopped = False\r\n\r\nReactorNotRestartable: \r\n\r\n```", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2594/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rmax": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2592", "title": "Improving headers values decoding (utf-8 vs latin1)", "body": "This raises from https://github.com/scrapy/scrapy/issues/2589 where a server returns a non-UTF-8 header value. According to [this RFC](https://tools.ietf.org/html/rfc7230#section-3.2.4):\r\n\r\n>    Historically, HTTP has allowed field content with text in the\r\n   ISO-8859-1 charset [ISO-8859-1], supporting other charsets only\r\n   through use of [RFC2047] encoding.  In practice, most HTTP header\r\n   field values use only a subset of the US-ASCII charset [USASCII].\r\n   Newly defined header fields SHOULD limit their field values to\r\n   US-ASCII octets.  A recipient SHOULD treat other octets in field\r\n   content (obs-text) as opaque data.\r\n\r\nSo the `latin1` encoding may be preferred over `utf-8` which is the Scrapy's default.  At the other hand, `requests` in some places uses `utf-8` with a fallback to `latin1`: https://github.com/kennethreitz/requests/blob/d6f4818c0b40bc6c00433c013b7daaea83b2cd51/requests/models.py#L908\r\n\r\nFollowing @kmike's [suggestion](https://github.com/scrapy/scrapy/pull/2588#issuecomment-282112022), it seems that decoding headers with `utf-8` having `iso-8859-1` as fallback is the best option.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2592/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/2b34c6edffcf14d81b5f11369648f4661e067ccc", "message": "Abort connection earlier and avoid to buffer data\n\nA symptom of this issue was having the log message \"Received (X) bytes\nlarger than download max size (Y)\" several times printed, with increased\nX values."}], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2637", "title": "Set default encoding to latin1. Fixes GH-2589", "body": "For background see GH-2592.\r\n\r\nRelated GH-2593.", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3100800", "body": "#62 is a duplicated report of this issue but includes a pull request with a fix.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3100800/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3011083", "body": "See issue #58\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3011083/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3418355", "body": "As a workaround you can use `process_value` argument:\n\n`SgmlLinkExtractor(..., process_value=lambda v: v.strip())`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3418355/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764580", "body": "I have reverted the redirect middleware order and added tests for gzipped meta redirection. Although I'm not sure about leaving the body compressed if it fails, I think there could be more errors (ie. connection drop) that could make fail the decompression and leaving the body as is could produce misbehavior in other components.\n\nAdding more tests for the middlewares integration could help us to identify the best solution.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764580/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764823", "body": "without any fix fails test_gzipped_redirect_30x\nwith the redirect reorder fails test_gzipped_meta_redirect\nwith dangra's suggestion passes all tests (See https://gist.github.com/1718659)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764823/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12075882", "body": "Nice!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12075882/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3657116", "body": "The purpose is to handle the case when a webpage does a redirection (metarefresh or 30x). Like the example below:\n\n```\n$> curl http://www.kijkjerijk.nl/\n> GET / HTTP/1.1\n> User-Agent: Mozilla/5.1; MSIE; YB/9.5.1 MEGAUPLOAD 1.0;\n> Host: www.kijkjerijk.nl\n> Accept: */*\n> Referer: \n> \n< HTTP/1.1 200 OK\n< Content-Length: 57\n< Content-Type: text/html\n< Content-Location: http://www.kijkjerijk.nl/Index.html\n< Last-Modified: Wed, 04 Nov 2009 08:35:42 GMT\n< Accept-Ranges: bytes\n< ETag: \"fa659acb295dca1:2f6\"\n< Server: Microsoft-IIS/6.0\n< X-Powered-By: ASP.NET\n< X-Powered-By: PleskWin\n< Date: Wed, 25 Jan 2012 19:25:30 GMT\n< \n* Connection #0 to host www.kijkjerijk.nl left intact\n* Closing connection #0\n<meta http-equiv=\"refresh\" content=\"0;URL=about:blank\" />\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3657116/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/13002275", "body": "Test added. Thanks for the pointers!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/13002275/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/13702753", "body": "@llonchj I've added tests and a tiny change to preserve same behavior as dict(item).\n\nCould you pull my branch? https://github.com/darkrho/scrapy/tree/item_serialization\n\nApparently I can't make a pull request to your branch :/\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/13702753/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/14690398", "body": "> I was thinking about something like javascript setTimeout.\n\nYou can use twisted's `callLater` method.\n\n``` python\n    def parse_item(self, response):\n        # do stuff ...\n        reactor.callLater(delay, self.crawler.engine.schedule, request=req, spider=self)\n```\n\nAlthough you will have to deal with the `spider_idle` signal to make sure the engine doesn't close your spider when you have scheduled a delayed request through `reactor.callLater`.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/14690398/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/125955032", "body": "Is there any plan to replace `mitmproxy` requirement for tests? \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/125955032/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/220796524", "body": "@AverHLV Twisted can run on python 3 but not on windows because `_win32stdio` has not been ported yet. See:\n- https://twistedmatrix.com/trac/wiki/Plan/Python3#Details\n- https://twistedmatrix.com/trac/ticket/8018\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/220796524/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/19063015", "body": "What about just adding a link to pypi? i.e. https://pypi.python.org/pypi?%3Aaction=search&term=scrapy&submit=search\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/19063015/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/19171202", "body": "@amferraz Sorry, I meant: in the section of \"Projects not maintained by Scrapy devs\" add a link to the pypi's search results for scrapy instead of listing the 3rd party projects. Although I think could useful to have a list of featured related projects.\n\nMy suggestion address the fact that maintaining an accurate and up to date list of related projects can be time-consuming.\n\nA wiki page might work well, though.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/19171202/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/281756036", "body": "Here is a live example at this time:\r\n\r\n```\r\n$ curl -v \"http://www.jindai.com.tw/\"\r\n> GET / HTTP/1.1\r\n> Host: www.jindai.com.tw\r\n> User-Agent: Mozilla/5.1 (MSIE; YB/9.5.1 MEGAUPLOAD 1.0)\r\n> Accept: */*\r\n> Referer:\r\n>\r\n< HTTP/1.1 200\r\n< Status: 200\r\n< Connection: close\r\n```", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/281756036/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283062977", "body": "It seems that this case is common with, for example, custom nginx modules which only set the response status and no reason.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283062977/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283064374", "body": "Twisted has a patch ready to fix this issue: https://twistedmatrix.com/trac/ticket/7673#comment:5 PR https://github.com/twisted/twisted/pull/723 \ud83c\udf89 ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283064374/reactions", "total_count": 2, "+1": 1, "-1": 0, "laugh": 0, "hooray": 1, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/25855710", "body": "> I would like to submit an actual example but am a little uncertain about which pages to crawl sufficiently long and in parallel to reproduce the issue while not having any legal issues with publishing the code.\n\nPerhaps a scrapy project is not necessary. I wrote a script to attempt to reproduce this issue: https://github.com/darkrho/scrapy-issue-415\n\nBut I wasn't able to get any error at all. It seems there is something else besides concurrent `item.save()` actions.\n\nHow was like your item data?\n\nHow was like your database schema?\n\nHow many spiders did you had running concurrently?\n\nHow many items were you scraping?\n\nWhat django/postgres versions were you running?\n\nDo you have the exception traceback?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/25855710/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32889466", "body": "+1 @daniel for solving a long standing mystery :)\n\nI've never needed to use that parameter but I recognize there are some\nsites that would require the button value.\n\nOn Tue, Jan 21, 2014 at 8:34 AM, Daniel Gra\u00f1a notifications@github.comwrote:\n\n> It selects what submitteable button it is hit and send along form values.\n> \n> The testcaseshttps://github.com/scrapy/scrapy/blob/master/scrapy/tests/test_http_request.py#L312-L402kind of describe it:\n> \n> ```\n> def test_from_response_submit_not_first_clickable(self):\n>     response = _buildresponse(\n>         \"\"\"<form action=\"get.php\" method=\"GET\">            <input type=\"submit\" name=\"clickable1\" value=\"clicked1\">            <input type=\"hidden\" name=\"one\" value=\"1\">            <input type=\"hidden\" name=\"two\" value=\"3\">            <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">            </form>\"\"\")\n>     req = self.request_class.from_response(response, formdata={'two': '2'}, \\\n>                                           clickdata={'name': 'clickable2'})\n>     fs = _qs(req)\n>     self.assertEqual(fs['clickable2'], ['clicked2'])\n>     self.assertFalse('clickable1' in fs, fs)\n>     self.assertEqual(fs['one'], ['1'])\n>     self.assertEqual(fs['two'], ['2'])\n> ```\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/issues/427#issuecomment-32870147\n> .\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32889466/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32912700", "body": "Replying by email is so confusing :(\n\nOn Tue, Jan 21, 2014 at 10:45 AM, Pablo Hoffman notifications@github.comwrote:\n\n> @dangra https://github.com/dangra @darkrho https://github.com/darkrhothat shows choosing a popular common name as github username is a bad idea\n> :)\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/issues/427#issuecomment-32890688\n> .\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32912700/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/27098557", "body": "I think the default is already the stable release, as going to http://doc.scrapy.org redirects to 0.18 doc.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/27098557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/27178912", "body": "That's a very general question. You'd better ask the scrapy-users mailing list with more details as this issue tracker is intended for users reporting bugs in the code itself and developers discussing improvements and new features.\n\nHaving said that, the general process is to figure out what requests generates the website (this using google/firefox dev tools) and then try to replicate those requests in scrapy (with the required parameters and post data). It's hard to tell \"the best way\" as every website is different and so your requirements.\n\nHere is the scrapy-users group: https://groups.google.com/forum/#!forum/scrapy-users\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/27178912/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/27454299", "body": "@pablohoffman It failed due the removed file requirements-lucid.txt. I did merge latest changes from master and now everything is OK.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/27454299/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/27183336", "body": "Oh, it seems travis installs cssselect when running the lucid environment even though the requirements-lucid does not include cssselect: https://travis-ci.org/scrapy/scrapy/jobs/13131752#L340\n\nIs that right?\n\nEdit: That's because the requirements are in the `setup.py` too. I see that travis' requirements serve the purpose to restrict to an specific version. In that case, isn't redundant having the same requirements in `.travis/requirements-latest.txt` as in `setup.py`? (not all of them, just the ones that are in both.)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/27183336/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/27313334", "body": "@dangra please take the lead, I'm bit busy this days.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/27313334/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/27933577", "body": "Closing this pull request as includes unrelated commits.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/27933577/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/27999397", "body": "Sure.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/27999397/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32553674", "body": "Sorry, I totally forgot about this. Thank you for your like.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32553674/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32617170", "body": "@dangra I did a rebase in an attempt to update the PR but lost the PR :(\n\nOn Fri, Jan 17, 2014 at 11:46 AM, Daniel Gra\u00f1a notifications@github.comwrote:\n\n> @darkrho https://github.com/darkrho why did you close the PR?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/pull/455#issuecomment-32616586\n> .\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32617170/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29141691", "body": "Very good work! I see a lot of potential in this feature. For example, most spiders perform the extraction using the same set of xpath expressions each time, by precompiling those rules there might a be a performance boost in long running spiders.\n\nMaybe it could be useful to have an item loader with precompiled rules, but I'm not sure if it's worth the extra code. The alternative, which is fair enough, is to keep instances of the XPath/CSS classes and using them in the add_xpath/add_css calls.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29141691/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29159126", "body": "Additionally, I think any class/function that have as parameter a xpath expression should accept a XPath instance as well. For example:\n\n```\nSgmlLinkExtractor(restrict_xpaths=CSS(\"a.pagination\"))\n```\n\nAnd in the case of initialization parameters, those classes should precompile the xpath expressions (in case of strings).\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29159126/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29162189", "body": "> Alas, SgmlLinkExtractor is not based on lxml, right? It'd have to be refactored.\n\nThat's right. The class itself is not built on top of lxml to extract the links, there is another class that is, but is the canonical link extractor for scrapy and when passed the `restrict_xpaths` argument it uses the `Selector` class:\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/contrib/linkextractors/sgml.py#L119\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29162189/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29171036", "body": "I did some unscientific timing and didn't show any significant differences in the execution time. I used very simple xpath expressions, though.\n\nAlthough I didn't look into what implies to precompile a xpath expression deep in the code, I think there will be a difference with overly complicated expressions, like the ones used when the html page does not have nice css identifiers and classes.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29171036/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32474295", "body": "Regardless the performance consideration, I think one useful outcome of this is the use of the XPath and CSS objects instead of raw xpath strings where only xpath is supported.\n\nNow, I think another useful feature would be adding operators to the XPath/CSS objects. I have seen plenty of spiders that have to write the xpath expression for a container and then add the xpath expression for a specific element many times (i.e. `\"//div[@class=\"foo\"]/table[contains(@foo,\"bar\")]\" + \"/h1/a/@href\"`). With custom operators we could write something like\n\n```\n>>> container = CSS(\".foo > bar\")\n>>> relative_element = XPath('descendant::a[contains(@data,\"foo\")]')\n>>> sel.xpath(container + relative_element)\n\n>>> sel.xpath(CSS(\"a.paginator\") | XPath(\"//div[a/span and @data-paginator]\"))\n```\n\nWhere the + operator will concatenate the xpath expressions, the | operator could perform \"rule1 or rule2\" matching. I'm unsure about the other operators and maybe this is way overcomplicated :).\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32474295/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29896973", "body": "I use `CONCURRENT_ITEMS = 1`  in this cases. I haven't verified how much improve the memory usage, though.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29896973/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/30936115", "body": "One issue is due this line https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/pipeline/files.py#L196 when `file_key` call raises an exception here https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/pipeline/files.py#L196\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/30936115/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/31329617", "body": "I found that LevelDB is very reliable and efficient to use as a cache backend. I haven't found any corruption issue when stopping the crawler (thing that I do a lot while developing spiders). But a sqlite-based backend could be a better default choice as it comes in the standard library.\n\nIIRC, the file-based backend was the default cache backend a time ago.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/31329617/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/31427928", "body": "Here is the docs for your scrapy version:\nhttp://doc.scrapy.org/en/0.20/intro/tutorial.html\n\nThere is a link at the bottom of the sidebar to switch between versions.\n\nOn Wed, Jan 1, 2014 at 2:48 PM, coffeeandcelluloid <notifications@github.com\n\n> wrote:\n> \n> Here's the tutorial http://doc.scrapy.org/en/latest/intro/tutorial.html\n> Version Scrapy 0.20.2\n> \n> I first did a pip install. Then I read somewhere that it's better to pull\n> the Git clone, so I did that, but I'm new to all this so I'm not sure if I\n> properly overwrote the old install or am calling the latest version.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/issues/511#issuecomment-31427743\n> .\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/31427928/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32029988", "body": "@saitx Can your share the code you were trying?\n\nPerhaps the mistake comes from the fact the doc mentions first the `XPathItemLoader` and the method `add_xpath`, and then shows a `ProductLoader` class inherit from `ItemLoader`. Thus calling `add_xpath` in an instance of `ProductLoader` will fail.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32029988/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32553900", "body": "Oh, so that's how people ended up following latest docs instead of stable? Kudos to Scorpil. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32553900/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32617946", "body": " I have conflicted feelings about this. It seems this only affects hardcore\nspider developers who iterate fast (thus stopping the crawls uncleanly).\n\nBut so far this haven't been an issue that affects a wide user base,\notherwise this would popup in scrapy-users years ago. Therefore the current\ndefault is good enough, and hardcore developers can always change the cache\nbackend anytime.\n\nMy vote is to keep the current default and work in a solution to this issue\nfor the next release.\n\nOn Fri, Jan 17, 2014 at 11:52 AM, Pablo Hoffman notifications@github.comwrote:\n\n> I feel less qualified to make a decision about this, since I haven't used\n> httpcache myself in more than a year, so whatever you guys decide is fine\n> for me.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/pull/541#issuecomment-32617090\n> .\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32617946/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32606576", "body": "The spider middlewares are meant to process the spider output/input. By using the `engine` directly you are returning nothing from the spider and thus bypassing the spider middlewares.\n\nI suppose you have an spider middleware, can you make it a extension or downloader middleware?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32606576/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32622263", "body": "Nice TL;DR. I think it should be somewhere in the docs because there are a\nfew examples in the wild using `.schedule`:\nhttps://www.google.com/search?q=%22crawler.engine.schedule%22\n\nOn Fri, Jan 17, 2014 at 12:35 PM, Daniel Gra\u00f1a notifications@github.comwrote:\n\n> TL;DR:\n> - Use engine.crawl() if you expect your requests to be injected into\n>   the spider<->downloader pipeline.\n> - Use engine.download() if you want a response back, only\n>   downloadmiddlewares are applied.\n> - Don't use .schedule(), it's legacy and needs a warning.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/issues/542#issuecomment-32621164\n> .\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32622263/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33527987", "body": "Perhaps this Twisted issue is related: http://twistedmatrix.com/trac/ticket/5192\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33527987/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32760561", "body": "Why not overwrite by default and provide an option to append?\n\nOn Mon, Jan 20, 2014 at 9:16 AM, Daniel Gra\u00f1a notifications@github.comwrote:\n\n> I've been bitten by this too, I prefer the single option to the modifier.\n> \n> +1 for -O as short option name and --overwrite-output (or\n> --truncate-output or --new-output) as long option name\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/issues/547#issuecomment-32758317\n> .\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32760561/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33448894", "body": "@nyov, this doesn't affect the general behavior of an item, and you can always use `item[\"field\"] = None`.\n\n> Example: Defuncts a SQL Database pipeline which uses None for NULL.\n\nThis has nothing to do with the pipelines. Moreover, it doesn't add any new behavior because `None` values already ignored by the item loader. For example, using Scrapy 0.20:\n\n``` python\nIn [1]: from scrapy.contrib.loader import ItemLoader\n\nIn [2]: il = ItemLoader(item={})\n\nIn [3]: il.add_value('foo', None)\n\nIn [4]: il.load_item()\nOut[4]: {}\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33448894/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33174617", "body": "Updated shell output in the docs.\n\nActually I went through the shell session examples and fixed the output with the latest output. Maybe a bit beyond the scope of this PR but also I improved the shell inspect example with a reproducible example code.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33174617/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33483214", "body": "@pablohoffman OK, I will add the description to the documentation.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33483214/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/50252604", "body": "@nramirezuy I totally forgot about this PR. I'll do it in the next days, although first I'll test the dockerfile against the latest docker version.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/50252604/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/149059762", "body": "Finally I updated this PR. Is it still good to be included? If so, I can move the description to a proper documentation section.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/149059762/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/259304244", "body": "Given docker is widespread and docker lib images are very good, there is no need to include a custom image. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/259304244/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33887692", "body": "> The extractor could even be renamed to SelectorLinkExtractor but users may expect that it takes a Selector parameter somehow...\n\nJust like the `Selector` class, I think this class can be named `LinkExtractor` and be importable from `scrapy.contrib.linkextractors`. This class is going to deprecate the sgml lx, right?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33887692/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33255194", "body": "Agreed.\n\nThe offending code is this block: https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/linkextractors/sgml.py#L118\n\nSpecifically the statement: `u''.join(f for x in self.restrict_xpaths for f in sel.xpath(x).extract()).encode(response.encoding)`.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33255194/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33288687", "body": "+1 to proposed patch. I think it's simple enough to backport it to 0.22, right?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33288687/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/94345158", "body": "#612 is good enough.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/94345158/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33638467", "body": "I've made a pull request (#569) which proposes a solution to this issue.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33638467/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33691156", "body": "I agree with with @shirk3y, instead of having an ambiguous method will be much better to have a new method. Another name could be `.first()`.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33691156/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33731202", "body": "I agree that `extract_first` is a better name than just `first`.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/33731202/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/34217994", "body": "@redapple, +1 to deprecate `.extract()` and add `.find()` and `.findall()` (or something along that names). The `.re()` can be a paremeter: `.find(re=\"\\d+\")`, just like the loader which allows the optional re parameter.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/34217994/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/34253890", "body": "The use of `loader.get_xpath` is very compelling because many times I want\nto extract a value, apply some cleaning  and use it for an url or\nsomething. The `get_xpath` provides a clean API for this use case.\n\nNevertheless, I can't wrap my ahead around the idea of having to use an\n_item loader_ when I don't want to load an item.\n\nThis might be solved by making `selector.xpath` and `selector.css` behave\nlike `loader.get_xpath`. But this implies changing the API and don't\naddress the fact that `selector.xpath` and `selector.css` return a\n`SelectorList`.\n\nOn Wed, Feb 5, 2014 at 4:16 PM, Nicol\u00e1s Alejandro Ram\u00edrez Quiros <\nnotifications@github.com> wrote:\n\n> You don't need it, but can use it.\n> \n> > > > response = HtmlResponse('http://scrapinghub.com', body='<html><body><form><input type=\"text\" name=\"username\" id=\"username\"><input type=\"password\" name=\"passwd\" id=\"passwd\"></form></body></html>'>>> loader = ItemLoader(response=response)>>> loader.get_xpath('//input[@type=\"text\"]/@_[name()=\"name\" or name()=\"id\"]', TakeFirst())u'username'>>> loader.get_xpath('//input[@type=\"password\"]/@_[name()=\"name\" or name()=\"id\"]', TakeFirst())u'passwd'\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/issues/568#issuecomment-34233291\n> .\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/34253890/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/36885431", "body": "A few comments about the general use of LevelDB as a cache backend:\n\nThe only thing that I find annoying when using LevelDB as a cache backend is the inability to start multiple processes (e.g. start the scrapy shell while a crawl is ongoing).\n\nAnother thought is to use the request keys in such way that they are retrieved in a sequential-order in a normal crawl to take full advantage of the backend performance. Perhaps this can achieved by using the timestamp as a key prefix.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/36885431/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/46458509", "body": "> @darkrho aside of the performance improvements, do you see any reason why we can't include a default implementation as a Scrapy contrib?\n\nLooks good to me.\n\nAs side note, recently I stumble upon a fork of LevelDB that improves the write concurrency: http://hyperdex.org/performance/leveldb/\n\nOne of the python bindings: https://github.com/rep/py-hyperleveldb\n\nI haven't dig into this implementation to see if it solves our issue, though.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/46458509/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/37361828", "body": "The function `sys.getsizeof` returns the size the object itself and not its references as I would like to, for example, getting the biggest `Request`'s instance. am I missing something?\n\n``` python\n    In [6]: s = 'foo' * 1024\n\n    In [7]: sys.getsizeof(s)\n    Out[7]: 3109\n\n    In [8]: sys.getsizeof({'foo': s})\n    Out[8]: 280\n\n    In [9]: from scrapy.http import Request\n\n    In [11]: req = Request('about:blank', meta={'foo': s})\n\n    In [12]: sys.getsizeof(req)\n    Out[12]: 64\n\n    In [13]: req = Request('about:blank', body=s, meta={'foo': s})\n\n    In [14]: sys.getsizeof(req)\n    Out[14]: 64\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/37361828/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/50913767", "body": "It works wonders. I added a few functions[1] and added to settings:\n\n```\n import xpathfuncs; xpathfuncs.setup()\n```\n\n[1] https://gist.github.com/darkrho/3338969c0f229ecf087d\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/50913767/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/49345586", "body": "Don't use \"scrapy\" as the filename for your script. In general, if you name\nyour script with the same name as an already existing module which you are\nimporting, the script is importing itself.\n\nOn Thu, Jul 17, 2014 at 12:53 PM, ILYG notifications@github.com wrote:\n\n> C:\\Python27\\Lib\\site-packages and C:\\Python27 already added in PATH .\n> still got this\n> [image: qq20140718001120]\n> https://cloud.githubusercontent.com/assets/752033/3616408/d2cd6ebc-0dd2-11e4-9834-dc4f46e6a534.jpg\n> \n> i have no idea .... any one can help ?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/scrapy/scrapy/issues/807.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/49345586/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/50421683", "body": "@nramirezuy indeed, the implementation is kinda hacky but that's the idea. It needs a try/finally around the yield to avoid your bug.\n\nI would like to hear more thoughts on this. @redapple @dangra \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/50421683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/50915329", "body": "A real example:\n\n``` python\n    def parse_stuff(self, response):\n        label_value = './/td[icontains(normalize-space(),\"{}\")]/following-sibling::td'.format\n        il = StuffLoader(response=response)\n        with il.push_css('.top_table'):\n            il.add_css('name', 'h2.fn')\n            il.add_css('image_url', 'img.photo::attr(src)', filter_unknown)\n            with il.push_css('.summary'):\n                il.add_css('city', '.adr', re=r'(.+),?')\n                il.add_css('state', '.adr', re=r', (.+)')\n                il.add_css('zipcode', '.postal-code')\n                il.add_xpath('member_since', label_value(\"member since\"))\n            il.add_css('lat', '.latitude .value-title::attr(title)')\n            il.add_css('lng', '.longitude .value-title::attr(title)')\n            il.add_css('hourly_rate', '.price', re=r'\\D(\\d+)\\D')\n            il.add_css('average_rating', '.current-rating::attr(style)', re=r'width:(\\d)0px')\n        with il.push_css('.profile'):\n            il.add_xpath('gender', label_value(\"gender\"))\n            il.add_xpath('subjects', label_value(\"subjects\") + '/a/text()')\n            il.add_xpath('education', label_value(\"education\"))\n            il.add_xpath('experience', label_value(\"experience\"))\n            il.add_xpath('hobbies', label_value(\"hobbies\"))\n        with il.push_css('.location'):\n            il.add_xpath('days_times', label_value('days/times'))\n            il.add_xpath('can_meet', label_value('can meet'))\n        il.add_value('url', response.url)\n        return il.load_item()\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/50915329/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/51404351", "body": "Here is another example from a page without useful css ids/classes:\n\n``` python\n        p_container = 'p[strong[icontains(normalize-space(),\"{}\")]]'.format\n        il = TutorLoader(response=response)\n        il.add_css('name', 'h1')\n        with il.push_xpath('//div[has-class(\"main-box\")]/descendant::table[1]'):\n            with il.push_css('td[width=\"200\"]'):\n                il.add_xpath('image_url', 'img/@src')\n                il.add_xpath('average_rating', 'p[strong[icontains(.,\"rating:\")]]',\n                             re=r'\\d+\\.\\d+')\n                with il.push_xpath(p_container(\"location\")):\n                    il.add_xpath('city', 'text()', re=r'[^,]+')\n                    il.add_xpath('state', 'text()', re=r',\\s*(.+)')\n                with il.push_xpath('following-sibling::td[1]'):\n                    # Looks like about is first two p.\n                    il.add_xpath('about', 'p[1]')\n                    il.add_xpath('about', 'p[2]')\n                    with il.push_xpath(p_container(\"subjects available\")):\n                        il.add_css('subjects', 'a')\n                    with il.push_xpath(p_container(\"additional subjects\")):\n                        il.add_xpath('subjects', 'text()', re=r':?\\s*(.+)')\n                    with il.push_xpath(p_container(\"highest degree\")):\n                        il.add_xpath('highest_degree', 'text()', re=r':?\\s*(.+)')\n                    with il.push_xpath(p_container(\"college\")):\n                        il.add_xpath('college', 'text()', re=r':?\\s*(.+)')\n                    il.add_xpath('years_tutoring', 'lower-case(normalize-space())',\n                                 re=r'as a tutor: (\\d+)')\n        with il.push_xpath('//script[icontains(.,\"glatlng\")]'):\n            il.add_xpath('lat', 'text()', re=r'LatLng\\(([^,]+),')\n            il.add_xpath('lng', 'text()', re=r'LatLng\\([^,]+,\\s*([^\\)]+)')\n            il.add_xpath('address', 'text()', re=r'createMarker\\(.+?(\\<br\\>.+?)[\\'\"],')\n        il.add_value('zipcode', il.get_output_value('address'), re=r'\\D(\\d{5})$')\n        il.add_value('url', response.url)\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/51404351/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/51779966", "body": "@barraponto in your specific case, you can use the `selector` argument in the item loader and don't need the push methods. The push methods' usefulness shines when you have many fields in a common container. Here is an example that mixes xml and html selectors into the same item loader:\n\n``` python\n        # response is a xml file with <marker name=\"foo\" .... />\n        for marker in response.xpath('//marker'):\n            cl = CenterLoader(response=response, selector=marker)\n            # this fields are extracted directly from the xml tag attributes\n            cl.add_xpath('name', '@name')\n            cl.add_xpath('lat', '@lat')\n            cl.add_xpath('lng', '@lng')\n            # the attribute htmlList contains html text which have additional fields we want to extract\n            html_list = Selector(text=marker.xpath('@htmlList').extract()[0])\n            with cl.push_selector(html_list):  # override the xml selector with the html selector\n                cl.add_css('address', '.address', _join)\n                cl.add_css('city', '.citystatezip', re=r'([^,]+)')\n                cl.add_css('state', '.citystatezip', re=r', (\\w{2})')\n                cl.add_css('zipcode', '.citystatezip', re=r'\\s+(\\d+)$')\n                cl.add_xpath('url', '//a[icontains(normalize-space(),\"view center\")]/@href')\n                yield cl.load_item()\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/51779966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/51781843", "body": "@redapple I'm not sure how nested items conflicts with the push methods. Here is an example where I (ab)use a subitem loader to load a table of values `[{\"col1\": \"value1\", \"col2\": \"vallue2\", ...}]`. Although it doesn't have too much to do with the push method except to combine css and xpath selectors to retrieve the table (which could be done by a single xpath selector).\n\n``` python\n        enroll = IdentityLoader(response=response)\n        with enroll.push_css('#schedule'):\n            table = enroll.selector.xpath('table[1]')\n            if table:\n                # First extract table headers to build the row items.\n                gl = GeneralLoader(response=response, selector=table)\n                gl.add_css('list', 'thead td')\n                headers = gl.load_item()['list']\n                # Extract each row as a subitem.\n                for tr in table.css('tbody tr'):\n                    gl = GeneralLoader(response=response, selector=tr)\n                    gl.add_css('list', 'td')\n                    values = gl.load_item()['list']\n                    data = dict(zip(headers, values))\n                    enroll.add_value('schedule_and_tuition', data or None)\n```\n\nAs pointed out by @nramirezuy, this implementation of the push methods causes an inconsistency in the selector value in the context loader, where input processors get the overrided value and the output processors might get the original selector (depending on where is called `load_item`).\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/51781843/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/149064863", "body": "@kmike oh, I was not aware of that. Thanks! I'm closing this PR.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/149064863/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/50940561", "body": "@dangra I've been fooled by httpbin.org as well. See the actual headers \">\" sent by curl vs the response:\n\n```\n$ curl -v -X POST http://httpbin.org/post                                  \n* Couldn't find host httpbin.org in the .netrc file; using defaults\n* Hostname was NOT found in DNS cache\n*   Trying 54.235.71.34...\n* Connected to httpbin.org (54.235.71.34) port 80 (#0)\n> POST /post HTTP/1.1\n> User-Agent: Mozilla/5.1; MSIE; YB/9.5.1 MEGAUPLOAD 1.0;\n> Host: httpbin.org\n> Accept: */*\n> Referer: \n> \n< HTTP/1.1 200 OK\n< Access-Control-Allow-Credentials: true\n< Access-Control-Allow-Origin: *\n< Content-Type: application/json\n< Date: Fri, 01 Aug 2014 22:09:33 GMT\n* Server gunicorn/18.0 is not blacklisted\n< Server: gunicorn/18.0\n< Content-Length: 420\n< Connection: keep-alive\n< \n{\n  \"args\": {}, \n  \"data\": \"\", \n  \"files\": {}, \n  \"form\": {}, \n  \"headers\": {\n    \"Accept\": \"*/*\", \n    \"Connection\": \"close\", \n    \"Content-Length\": \"0\", \n    \"Host\": \"httpbin.org\", \n    \"Referer\": \"\", \n    \"User-Agent\": \"Mozilla/5.1; MSIE; YB/9.5.1 MEGAUPLOAD 1.0;\", \n    \"X-Request-Id\": \"8af31eb3-af1b-43a2-aa25-f09116745255\"\n  }, \n  \"json\": null, \n  \"origin\": \"181.177.185.192\", \n  \"url\": \"http://httpbin.org/post\"\n* Connection #0 to host httpbin.org left intact\n}%                                                        \n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/50940561/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/171684883", "body": "The fix only works when using the `HTTP10` downloader handler.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/171684883/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/89055238", "body": "@Dineshs91 @nyov It works for me when copying the file to either `/usr/share/zsh/vendor-functions/_scrapy_zsh_completion` or `/usr/share/zsh/vendor-completions/_scrapy`. Notice that in the latter case the filename must be `_scrapy`.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/89055238/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/75605316", "body": "As an additional note, Twisted [handles this issue](https://github.com/twisted/twisted/blob/trunk/twisted/internet/_sslverify.py#L38) for `twisted.web.client.Agent` by using this class [BrowserLikePolicyForHTTPS](https://github.com/twisted/twisted/blob/trunk/twisted/web/client.py#L846).\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/75605316/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/76215506", "body": "@AlanChien Did you set up the change correctly?\n\nBecause it works for me:\n\n```\n$ scrapy shell https://app.gomlab.com/jpn/gom/GOMPLAYERJPSETUP.EXE\n2015-02-26 12:46:09-0400 [scrapy] INFO: Scrapy 0.25.1 started (bot: scrapybot)\n2015-02-26 12:46:09-0400 [scrapy] INFO: Optional features available: ssl, http11, boto, django\n2015-02-26 12:46:09-0400 [scrapy] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0}\n2015-02-26 12:46:09-0400 [scrapy] INFO: Enabled extensions: TelnetConsole, CloseSpider, CoreStats, SpiderState\n2015-02-26 12:46:10-0400 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2015-02-26 12:46:10-0400 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2015-02-26 12:46:10-0400 [scrapy] INFO: Enabled item pipelines: \n2015-02-26 12:46:10-0400 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2015-02-26 12:46:10-0400 [default] INFO: Spider opened\n2015-02-26 12:47:53-0400 [default] DEBUG: Crawled (200) <GET https://app.gomlab.com/jpn/gom/GOMPLAYERJPSETUP.EXE> (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    <scrapy.crawler.Crawler object at 0x7f47577f14d0>\n[s]   item       {}\n[s]   request    <GET https://app.gomlab.com/jpn/gom/GOMPLAYERJPSETUP.EXE>\n[s]   response   <200 https://app.gomlab.com/jpn/gom/GOMPLAYERJPSETUP.EXE>\n[s]   settings   <scrapy.settings.Settings object at 0x7f47577ebe50>\n[s]   spider     <DefaultSpider 'default' at 0x7f475678dc50>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n\nIn [1]: len(response.body)\nOut[1]: 22235496\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/76215506/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/114860898", "body": "@HassanQamar07 the site works for me. Can you paste the full output you are getting? By the way, I have upgraded with `pip install --upgrade --user scrapy`.\n\n```\n $ scrapy shell                                   \n2015-06-24 08:57:35 [scrapy] INFO: Scrapy 1.0.0 started (bot: scrapybot)\n2015-06-24 08:57:35 [scrapy] INFO: Optional features available: ssl, http11, boto\n...\n\nIn [1]: fetch('https://dms.psc.sc.gov/web/dockets')\n2015-06-24 08:57:46 [scrapy] INFO: Spider opened\n2015-06-24 08:57:47 [scrapy] DEBUG: Crawled (200) <GET https://dms.psc.sc.gov/web/dockets> (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    <scrapy.crawler.Crawler object at 0x7ff569721b50>\n[s]   item       {}\n[s]   request    <GET https://dms.psc.sc.gov/web/dockets>\n[s]   response   <200 https://dms.psc.sc.gov/web/dockets>\n[s]   settings   <scrapy.settings.Settings object at 0x7ff569721ad0>\n[s]   spider     <DefaultSpider 'default' at 0x7ff565f48e10>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n\nIn [2]: response.css('title').extract()\nOut[2]: [u'<title>Find Dockets</title>']\n\nIn [3]: fetch('https://www.techinasia.com/')\n2015-06-24 08:58:10 [scrapy] DEBUG: Crawled (200) <GET https://www.techinasia.com/> (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    <scrapy.crawler.Crawler object at 0x7ff569721b50>\n[s]   item       {}\n[s]   request    <GET https://www.techinasia.com/>\n[s]   response   <200 https://www.techinasia.com/>\n[s]   settings   <scrapy.settings.Settings object at 0x7ff569721ad0>\n[s]   spider     <DefaultSpider 'default' at 0x7ff565f48e10>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n\nIn [4]: response.css('title').extract()\nOut[4]: [u'<title>Tech in Asia - Asia Tech News for the World</title>']\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/114860898/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/102389675", "body": "@aliowka How big are your items? Did you tried using the LevelDB backend?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/102389675/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/92938996", "body": ">  it's hacky, debugging it is kind of hard\n\nI totally agree with that. :)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/92938996/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/124794894", "body": "I'd suggest to include `conda` (either through anaconda or miniconda distributions) as the recommended method to install scrapy instead of messing with system-wide packages in OS X. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/124794894/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/125032484", "body": "For testing purposes, I've published scrapy in my conda channel, it can be installed by using:\n\n```\n$ conda install -c https://conda.anaconda.org/rolando/ scrapy\n```\n\nThere are packages for the platforms: `linux-32`, `linux-64` and `osx-64`.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/125032484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/139427417", "body": "@sebiwi try using anaconda/miniconda for a hassle-free installation and package management: http://conda.pydata.org/miniconda.html\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/139427417/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/143080004", "body": "@dangra sounds good to me. Please go ahead and create the repository.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/143080004/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/148561630", "body": "I realized I create this PR against 1.0 branch. I will close it and create it against master.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/148561630/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/152613842", "body": "Btw, we are using `build_component_list` --which is very handy-- to build another kind of pipeline for our  project. So #1149 breaks our code and given we are using Scrapy Cloud, we are a bit worried about how to deal with that backwards incompatible so that all our jobs don't stop working overnight.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/152613842/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/148756348", "body": "That's a valid concern. I have created this issue to address it: https://github.com/scrapinghub/scrapinghub-conda-recipes/issues/2\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/148756348/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/148975931", "body": "@kmike this PR restores `service_identity` as a dependency https://github.com/scrapinghub/scrapinghub-conda-recipes/pull/4 \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/148975931/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/159959627", "body": "@kmike yes, it's good to merge.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/159959627/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/151952238", "body": "@darshanime Could you share the output of `conda info`? Did you try updating conda and python? If not, try running: `conda update conda` and `conda update python`. Then you could try creating a new conda env.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/151952238/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/152037493", "body": "@darshanime ~~The `OpenSSL` lines in your traceback are a bit suspicious.~~ ~~Did you used `pip` to install scrapy and its dependencies?~~ Installing packages that require compilation in `conda` via `pip` can cause problems sometimes.\n\nTry: `conda create -n scrapy-env -c scrapinghub scrapy`\n\nIt will create a new conda environment `scrapy-env` (change the name if already exists) and install `scrapy` from the `scrapinghub` channel.\n\n**Update:** If you require to run the developing version of `scrapy`, then you can run `pip develop .` inside the `scrapy` repository after installing most of its dependencies via `conda`, in particular `openssl` and related packags. (Running `python setup.py install` and `python setup.py develop` should work as well)\n\n```\n(scrapy-env)$ which scrapy\n/opt/conda/envs/scrapy-env/bin/scrapy\n(scrapy-env)$ scrapy version\nScrapy 1.1.0dev1\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/152037493/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/154537739", "body": "@jdemaeyer yes, our code works as expected on this PR. Thanks.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/154537739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/284196257", "body": "Fix https://github.com/scrapy/scrapy/pull/2622", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/284196257/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/268619248", "body": "Keep in mind that Sentry (and surely other tools as well) rely on proper lazy formatting to be able to aggregate/deduplicate/group log messages. ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/268619248/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/170687501", "body": "@palego I get this failure:\n\n```\nscrapy git:(pr/1657) \u2717 nosetests tests/test_commands.py\n...................F..\n======================================================================\nFAIL: test_startproject_template_override\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/rolando/miniconda/envs/orchardmile-spiders/lib/python2.7/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/Users/rolando/miniconda/envs/orchardmile-spiders/lib/python2.7/site-packages/twisted/internet/utils.py\", line 201, in runWithWarningsSuppressed\n    reraise(exc_info[1], exc_info[2])\n  File \"/Users/rolando/miniconda/envs/orchardmile-spiders/lib/python2.7/site-packages/twisted/internet/utils.py\", line 197, in runWithWarningsSuppressed\n    result = f(*a, **kw)\n  File \"/Users/rolando/Projects/gh/scrapy/tests/test_commands.py\", line 94, in test_startproject_template_override\n    (self.project_name, join(self.tmpl, 'project')), out)\n  File \"/Users/rolando/miniconda/envs/orchardmile-spiders/lib/python2.7/site-packages/twisted/trial/_synctest.py\", line 497, in assertIn\n    % (containee, container))\nFailTest: \"New Scrapy project 'testproject', using template directory '/var/folders/55/nbg15c6j4k3cg06tjfhqypd40000gn/T/tmpl2NVJx/templates/project', created in:\" not in \"New Scrapy project 'testproject' created in:\\n    /private/var/folders/55/nbg15c6j4k3cg06tjfhqypd40000gn/T/tmpl2NVJx/testproject\\n\\nYou can start your first spider with:\\n    cd testproject\\n    scrapy genspider example example.com\\n\"\n\n----------------------------------------------------------------------\nRan 22 tests in 28.886s\n\nFAILED (failures=1)\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/170687501/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/172008174", "body": "The test fails because `/var` is a link to `/private/var`. So the output contains the path with the unexpected `/private` prefix.\n\nThis failure is unrelated to the `mknod` issue.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/172008174/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/175137214", "body": "@redapple yes! This issue is fixed.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/175137214/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/219813787", "body": "I have a suggestion: focus on the response/requests part and leave for later the selector stuff.\n\nEven though the selector parts are a nice to have, I feel like it just just adds an overhead trying to interact with the scrapy's parser capabilities from external spiders.\n\nA few missing parts: \n- What's the result from a request being filtered out by the dupefilter? This is not an error.\n- What's the result from a request that returns 404 repsonse? These non-200 responses don't reach the spider under normal circumstances (i.e. they need to set the flag to handle all statuses). But an external spider may want to know what happened to its requests.\n\nIt may be good to have a way to run this protocol against real scrapy spiders (i.e.: running scrapy spiders as external spiders). This could give us a good sense of what can be supported and what not.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/219813787/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/268822440", "body": "I know that scrapy + python3 works in windows if you install it via [conda](http://conda.pydata.org/docs/intro.html): `conda install scrapy -c conda-forge`.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/268822440/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/268818211", "body": "See https://github.com/twisted/twisted/pull/633", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/268818211/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/268820758", "body": "It works with latest twisted dev version!\r\n\r\n```\r\nConnected to localhost.\r\n>>>\r\n>>> est()\r\nExecution engine status\r\n\r\ntime()-engine.start_time                        : 2.949514150619507\r\nengine.has_capacity()                           : False\r\nlen(engine.downloader.active)                   : 0\r\nengine.scraper.is_idle()                        : False\r\nengine.spider.name                              : extract\r\nengine.spider_is_idle(engine.spider)            : True\r\nengine.slot.closing                             : False\r\nlen(engine.slot.inprogress)                     : 0\r\nlen(engine.slot.scheduler.dqs or [])            : 0\r\nlen(engine.slot.scheduler.mqs)                  : 0\r\nlen(engine.scraper.slot.queue)                  : 0\r\nlen(engine.scraper.slot.active)                 : 0\r\nengine.scraper.slot.active_size                 : 0\r\nengine.scraper.slot.itemproc_size               : 0\r\nengine.scraper.slot.needs_backout()             : False\r\n\r\n\r\n>>> import sys; print(sys.version)\r\n3.5.2 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:52:12)\r\n[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\r\n>>>\r\n```\r\n\r\n```\r\n$ scrapy version -v\r\nScrapy    : 1.2.0\r\nlxml      : 3.6.4.0\r\nlibxml2   : 2.9.2\r\nTwisted   : 16.6.0dev0\r\nPython    : 3.5.2 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:52:12) - [GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\r\npyOpenSSL : 16.2.0 (OpenSSL 1.0.2j  26 Sep 2016)\r\nPlatform  : Darwin-15.6.0-x86_64-i386-64bit\r\n```", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/268820758/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/268817999", "body": "I had the same issue. I'm able to telnet but crash with `est()`.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/268817999/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/268822088", "body": "See https://github.com/scrapy/scrapy/issues/2155#issuecomment-268820758", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/268822088/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/259490927", "body": "@kmike IMO, it leaves the build dirty as only templates directory have `pyc` files. But, particularly this was breaking the conda-forge build on windows: https://github.com/conda-forge/staged-recipes/pull/1646#issuecomment-249061645\n\nThe conda build scripts cleaned up pyc files, but not in the templates directory. So when attempting to recreate pyc files it failed because templates already had pyc files around.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/259490927/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/264266557", "body": "@kmike The Manifest.in is only for `sdist` command: https://docs.python.org/2/distutils/sourcedist.html#the-manifest-in-template\r\n\r\nThe `sdist` output already does not include any `pyc` file except those two listed above, which usually don't cause a problem because the installers  (or binary packaging tools) compile all `pyc` files for the given platform.\r\n\r\nThis PR only aims to have a clean source distribution without any `pyc` file (those two files listed above).\r\n\r\nPD: The source distribution package does not include pyc files but default, but given templates are data directories those two `pyc` get included.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/264266557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283193922", "body": "This is related to https://github.com/scrapy/scrapy/issues/2586\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283193922/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283416696", "body": "@kmike yes, this is the case of the broken chunked response.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283416696/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/278238616", "body": "Additionally, there are 2-3x memory requirements for each image due to the format conversion and thumbnailing. Also, if I'm not wrong, the images pipeline bypass the concurrent requests limit causing to have a log of in-flight image requests.\r\n\r\nI haven't seen memory issues with the images downloader when setting ``CONCURRENT_ITEMS = 1``.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/278238616/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/268845758", "body": "Given `DNS_TIMEOUT` was not used at all, perhaps a straightforward fix is to make it a tuple and compatible with what twisted expects:\r\n\r\n> timeout: Default number of seconds after which to reissue the query. When the last timeout expires, the query is considered failed. (type: Sequence of int )", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/268845758/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/279729340", "body": "@pembeci I would recommend to use [(mini)conda](https://doc.scrapy.org/en/latest/intro/install.html#anaconda) to have the latest releases without having to upgrade system libraries in old systems.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/279729340/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283643006", "body": "@noprom The site does not complete the response when you use the default user agent (or the one you are using).\r\n\r\n```\r\n$ scrapy shell 'http://jbk.39.net/bw_t1/' --set USER_AGENT=Mozilla --loglevel INFO\r\n2017-03-02 09:38:49 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: scrapybot)\r\n2017-03-02 09:38:49 [scrapy.utils.log] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'USER_AGENT': 'Mozilla', 'LOG_LEVEL': 'INFO'}\r\n2017-03-02 09:38:49 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.corestats.CoreStats']\r\n2017-03-02 09:38:49 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2017-03-02 09:38:49 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2017-03-02 09:38:49 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2017-03-02 09:38:49 [scrapy.core.engine] INFO: Spider opened\r\n2017-03-02 09:38:50 [traitlets] WARNING: Config option `pager` not recognized by `InteractiveShellEmbed`.\r\n[s] Available Scrapy objects:\r\n[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\r\n[s]   crawler    <scrapy.crawler.Crawler object at 0x109100d68>\r\n[s]   item       {}\r\n[s]   request    <GET http://jbk.39.net/bw_t1/>\r\n[s]   response   <200 http://jbk.39.net/bw_t1/>\r\n[s]   settings   <scrapy.settings.Settings object at 0x109100eb8>\r\n[s]   spider     <DefaultSpider 'default' at 0x10bf23dd8>\r\n[s] Useful shortcuts:\r\n[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\r\n[s]   fetch(req)                  Fetch a scrapy.Request and update local objects\r\n[s]   shelp()           Shell help (print this help)\r\n[s]   view(response)    View response in a browser\r\nIn [1]: response.body[:100]\r\nb'\\r\\n<!doctype html>\\r\\n<html>\\r\\n<head>\\r\\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=g'\r\n```", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283643006/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282076681", "body": "Worth noting that browsers as well as other http clients (`curl`, `requests`, etc) handle data loss gracefully.\r\n\r\nHere is the issue that address this bug but for all statuses https://github.com/scrapy/scrapy/issues/2586", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282076681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283417653", "body": "@kmike yes, #2590 address this case in particular.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283417653/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/274818659", "body": "This aims to fix https://github.com/scrapy-plugins/scrapy-djangoitem/issues/18", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/274818659/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/276050144", "body": "OK!", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/276050144/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/279902729", "body": "See http://stackoverflow.com/a/42239833/140510\r\n\r\nTL;DR: It seems your python version was compiled without bzip2 support.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/279902729/reactions", "total_count": 4, "+1": 4, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282192288", "body": "@Keleir try using `conda`, see https://conda.io/docs/install/quick.html and https://doc.scrapy.org/en/latest/intro/install.html#anaconda", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282192288/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/281891695", "body": "This address #2586 ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/281891695/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282178033", "body": "The PR #2590 gives a better solution to this problem.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282178033/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282180116", "body": "Closing in favor of https://github.com/scrapy/scrapy/pull/2590", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282180116/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/281893575", "body": "The PR #2585 fixes this issue by adding a new setting `DOWNLOADER_ALLOW_DATA_LOSS` and customizing the transfer decoders to allow transfer data loss, that is, not raising `_DataLoss` nor `PotentialDataLoss` exceptions.\r\n\r\n```\r\n$ scrapy shell \"http://www.ebk-gruppe.com/\"  --set RETRY_ENABLED=0 --loglevel WARN --set DOWNLOADER_ALLOW_DATA_LOSS=1 -c \"len(response.body)\"\r\n2017-02-23 00:32:50 [scrapy.core.downloader.handlers.http11] WARNING: Data loss, not all data received\r\n8493\r\n```\r\nHowever, the fix requires to apply two patches to twisted ( https://github.com/twisted/twisted/pull/711 and https://github.com/twisted/twisted/pull/712  ) to allow to a clean solution.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/281893575/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282076205", "body": "@redapple nice approach without requiring twisted patches. I made a PR #2590 based on your suggestion and it works well, although the tests were a bit tricky.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282076205/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283376161", "body": "Fixed by #2590 ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283376161/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/281905458", "body": "This address #2589:\r\n\r\n```\r\nIn [2]: response.headers.to_unicode_dict(errors='ignore')\r\n\r\n{'accept-ranges': 'bytes',\r\n 'cache-control': 'max-age=604800',\r\n 'content-type': 'text/html',\r\n 'date': 'Thu, 23 Feb 2017 06:19:22 GMT',\r\n 'etag': '\"3acb6d9a9acf1:0\"',\r\n 'last-modified': 'Mon, 06 Jan 2014 06:38:03 GMT',\r\n 'public-key-pins': 'pin-sha256=SPKI_digest#1\"; pin-sha256=\"SPKI_digest#2\"; max-age=31536000',\r\n 'server': 'Microsoft-IIS/7.5',\r\n 'strict-transport-security': 'max-age=31536000; includeSubDomains',\r\n 'vary': 'Accept-Encoding',\r\n 'x-powered-by': 'ASP.NET',\r\n 'x-powered-by-plesk': 'PleskWin'}\r\n```", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/281905458/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282101448", "body": "@redapple I found out there is a [RFC](https://tools.ietf.org/html/rfc7230#section-3.2.4) that says:\r\n\r\n>    Historically, HTTP has allowed field content with text in the\r\n   ISO-8859-1 charset [ISO-8859-1], supporting other charsets only\r\n   through use of [RFC2047] encoding.  In practice, most HTTP header\r\n   field values use only a subset of the US-ASCII charset [USASCII].\r\n   Newly defined header fields SHOULD limit their field values to\r\n   US-ASCII octets.  A recipient SHOULD treat other octets in field\r\n   content (obs-text) as opaque data.\r\n\r\nSo, by default headers should be decoded with `latin1` charset. But that's a backwards incompatible change (only in cases where headers are non-ascii / non-latin1 compatible).\r\n\r\nThe example of the issue decodes fine with `latin1`:\r\n```\r\nIn [2]: response.headers.to_unicode_dict('latin1')\r\n\r\n{'accept-ranges': 'bytes',\r\n 'cache-control': 'max-age=604800',\r\n 'content-type': 'text/html',\r\n 'date': 'Thu, 23 Feb 2017 19:45:35 GMT',\r\n 'etag': '\"801764d9a9acf1:0\"',\r\n 'last-modified': 'Mon, 06 Jan 2014 06:38:03 GMT',\r\n 'public-key-pins': 'pin-sha256=\\x94SPKI_digest#1\"; pin-sha256=\"SPKI_digest#2\"; max-age=31536000',\r\n 'server': 'Microsoft-IIS/7.5',\r\n 'strict-transport-security': 'max-age=31536000; includeSubDomains',\r\n 'vary': 'Accept-Encoding,Accept-Encoding',\r\n 'x-powered-by': 'ASP.NET',\r\n 'x-powered-by-plesk': 'PleskWin'}\r\n```\r\n\r\nI've updated the PR to include a `encoding` parameter.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282101448/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282116697", "body": "@kmike I created this issue https://github.com/scrapy/scrapy/issues/2592 to follow the default encoding discussion.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282116697/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282141208", "body": "See PR https://github.com/scrapy/scrapy/pull/2593 for an alternative solution.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282141208/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282179916", "body": "Closing in favor of https://github.com/scrapy/scrapy/pull/2593\r\n\r\nI'm not sure if it's worth to add `errors` support to #2593 as, IMO, that implementation would cover 99% of the cases. Also it's not clear how would `errors` interact with the encodings fallback because it will disable the fallback. ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282179916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282002371", "body": "@redapple yes, as far I can tell. Here is the raw body: https://gist.github.com/rolando/45de1e7985cf78c753e362b6f1ece281", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282002371/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282141042", "body": "This PR https://github.com/scrapy/scrapy/pull/2593 provides an alternative solution using UTF-8 by default and falling back to ISO-8859-1.\r\n\r\nIt works nicely by default in the failure example:\r\n```\r\nIn [1]: response.headers.to_unicode_dict()\r\n\r\n{'accept-ranges': 'bytes',\r\n 'cache-control': 'max-age=604800',\r\n 'content-type': 'text/html',\r\n 'date': 'Thu, 23 Feb 2017 22:21:35 GMT',\r\n 'etag': '\"3acb6d9a9acf1:0\"',\r\n 'last-modified': 'Mon, 06 Jan 2014 06:38:03 GMT',\r\n 'public-key-pins': 'pin-sha256=\\x94SPKI_digest#1\"; pin-sha256=\"SPKI_digest#2\"; max-age=31536000',\r\n 'server': 'Microsoft-IIS/7.5',\r\n 'strict-transport-security': 'max-age=31536000; includeSubDomains',\r\n 'vary': 'Accept-Encoding',\r\n 'x-powered-by': 'ASP.NET',\r\n 'x-powered-by-plesk': 'PleskWin'}\r\n```", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282141042/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282191812", "body": "Superseded by https://github.com/scrapy/scrapy/issues/2592", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282191812/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282075687", "body": "This fixes #2586 ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282075687/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283102406", "body": "@paul forgot to mention that I added a warning to hint the use of the setting, which is shown before the error:\r\n\r\n```\r\nscrapy shell \"http://www.ebk-gruppe.com/\"  --set RETRY_ENABLED=0  --loglevel WARN\r\n2017-02-28 14:08:29 [scrapy.core.downloader.handlers.http11] WARNING: Found data loss in http://www.ebk-gruppe.com/. If you want to process broken responses set the setting DOWNLOAD_ALLOW_DATALOSS = True\r\nTraceback (most recent call last):\r\n  File \"/Users/rolando/miniconda3/envs/dev/bin/scrapy\", line 11, in <module>\r\n    load_entry_point('Scrapy', 'console_scripts', 'scrapy')()\r\n  File \"/Users/rolando/Projects/sh/scrapy/scrapy/cmdline.py\", line 142, in execute\r\n    _run_print_help(parser, _run_command, cmd, args, opts)\r\n  File \"/Users/rolando/Projects/sh/scrapy/scrapy/cmdline.py\", line 88, in _run_print_help\r\n    func(*a, **kw)\r\n  File \"/Users/rolando/Projects/sh/scrapy/scrapy/cmdline.py\", line 149, in _run_command\r\n    cmd.run(args, opts)\r\n  File \"/Users/rolando/Projects/sh/scrapy/scrapy/commands/shell.py\", line 73, in run\r\n    shell.start(url=url, redirect=not opts.no_redirect)\r\n  File \"/Users/rolando/Projects/sh/scrapy/scrapy/shell.py\", line 48, in start\r\n    self.fetch(url, spider, redirect=redirect)\r\n  File \"/Users/rolando/Projects/sh/scrapy/scrapy/shell.py\", line 115, in fetch\r\n    reactor, self._schedule, request, spider)\r\n  File \"/Users/rolando/miniconda3/envs/dev/lib/python3.5/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\r\n    result.raiseException()\r\n  File \"/Users/rolando/miniconda3/envs/dev/lib/python3.5/site-packages/twisted/python/failure.py\", line 372, in raiseException\r\n    raise self.value.with_traceback(self.tb)\r\ntwisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>, <twisted.python.failure.Failure twisted.web.http._DataLoss: >]\r\n```", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283102406/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283137577", "body": "> I still don't quite like 'data loss' term here because it is ambiguous; I think we should find a better option name.\r\n\r\nA new candidate name: ``DOWNLOAD_FAIL_ON_DATALOSS``. This describes the current behavior.\r\n\r\n> It'd be nice to explain in docs how this feature interacts with retries.\r\n\r\nPlease check current setting documentation.\r\n\r\n> A nitpick about warn message\r\n\r\nAdded a mention that no more messages will be shown.\r\n\r\n> decide on 'partial' vs 'data-loss' flag; I'm fine with both.\r\n\r\nI arbitrarily changed it to ``dataloss`` (as most flags are single-word).", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283137577/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283188896", "body": "Updated the setting name to ``DOWNLOAD_FAIL_ON_DATALOSS`` (default: ``True``).", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283188896/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283192799", "body": ":+1:\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283192799/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282141415", "body": "See PR https://github.com/scrapy/scrapy/pull/2593 as a proposed implementation. The nice thing about this one is that, when dealing with non-UTF8 strings, it doesn't require user intervention as with #2588.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282141415/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282140635", "body": "This address https://github.com/scrapy/scrapy/issues/2592 and fixes https://github.com/scrapy/scrapy/issues/2589", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282140635/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283670584", "body": "ping @redapple @kmike ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/283670584/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/285423310", "body": "@kmike found this related changes in webkit's changelog:\r\n\r\n> Ver 2.7.3\r\n>  - Use latin1 instead of UTF-8 for HTTP header values.\r\n> ...\r\n>\r\n> Ver 1.11.1\r\n> - Fix a crash in network backend with non-UTF8 HTTP header names.\r\n\r\nI'm in favor of using `latin1` as default, but that may be a backwards incompatible change.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/285423310/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/285424014", "body": "@kmike the fallback idea came from requests: https://github.com/scrapy/scrapy/issues/2592#issue-209880064", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/285424014/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/285435919", "body": "Closing in favor of https://github.com/scrapy/scrapy/pull/2637", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/285435919/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282191622", "body": "IIRC, twisted reactor is only meant to be ran once. It cannot be restarted.\r\n\r\nHowever, there is a handy package [crochet](https://github.com/itamarst/crochet) which allows to use twisted in a blocking fashion. I have played with it a while ago and published [scrapydo](https://github.com/rolando/scrapydo) but still very alpha and only covers very basic use cases. Below is an example:\r\n\r\n```\r\nimport logging\r\nimport scrapydo\r\nfrom scrapy import Request\r\nfrom scrapy.linkextractors import LinkExtractor\r\n\r\nlogging.basicConfig(level=logging.DEBUG)\r\n\r\nscrapydo.setup()\r\n\r\nscrapydo.default_settings.update({\r\n    'LOG_LEVEL': 'DEBUG',\r\n    'CLOSESPIDER_PAGECOUNT': 10,\r\n})\r\n\r\nlx = LinkExtractor()\r\n\r\ndef parse_page(response):\r\n    print(\"Got page: %s\" % response.url)\r\n    yield {'title': response.css('title::text').extract_first()}\r\n    for link in lx.extract_links(response):\r\n        yield Request(link.url, callback=parse_page)\r\n\r\nscrapydo.crawl(\"https://www.scrapy.org\", callback=parse_page)\r\n```", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/282191622/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/287293755", "body": "@redapple @kmike This should be ready for review.\r\n\r\nWhen the fix is not applied, the test fails as follows:\r\n\r\n```\r\ntrial tests.test_downloader_handlers.Http11TestCase.test_download_with_maxsize_very_large_file\r\ntests.test_downloader_handlers\r\n  Http11TestCase\r\n    test_download_with_maxsize_very_large_file ...                      [ERROR]\r\n\r\n===============================================================================\r\n[ERROR]\r\nTraceback (most recent call last):\r\n  File \"/Users/rolando/Projects/gh/twisted/src/twisted/internet/defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"/Users/rolando/Projects/gh/twisted/src/twisted/python/failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"/Users/rolando/Projects/sh/scrapy/tests/test_downloader_handlers.py\", line 399, in test_download_with_maxsize_very_large_file\r\n    yield d\r\n  File \"/Users/rolando/Projects/gh/twisted/src/twisted/internet/defer.py\", line 653, in _runCallbacks\r\n    current.result = callback(current.result, *args, **kw)\r\n  File \"/Users/rolando/Projects/sh/scrapy/tests/test_downloader_handlers.py\", line 388, in check\r\n    logger.error.assert_called_once_with(mock.ANY, mock.ANY)\r\n  File \"/Users/rolando/miniconda3/envs/dev/lib/python3.5/unittest/mock.py\", line 802, in assert_called_once_with\r\n    raise AssertionError(msg)\r\nbuiltins.AssertionError: Expected 'error' to be called once. Called 63 times.\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/287293755/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/297768547", "body": "PR rebased.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/297768547/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/284929445", "body": "IIRC, if you open double quotes `\"` before pasting the content then it doesn't auto-escape the string.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/284929445/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/301193299", "body": "This potentially is a backward incompatible change. ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/301193299/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/295761197", "body": "Doc is missing the reference to `DontCloseSpider`, which is catched only by the `spider_idle` sender.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/295761197/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/320082220", "body": "Hi @redapple, this seems a limitation in `pyflakes` as it doesn't not recognize special scoped variables (`__module__`, `__class__`, etc). See https://github.com/PyCQA/pyflakes/issues/259", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/320082220/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3073115", "body": "Fixed by using `json.loads`: https://github.com/darkrho/scrapy/commit/d765aa754913cc1e7550c9d49c5eb8b8859d40d7#L0L135\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3073115/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "Keleir": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2580", "title": "ImportError: No module named 'twisted.persisted'", "body": "I haved install scrapy on `Python 3.5.2`, but when I exeute `scrapy -v` on command-line, it occurs:\r\n```\r\n>> scrapy -v\r\nTraceback (most recent call last):\r\n  File \"/usr/bin/scrapy\", line 7, in <module>\r\n    from scrapy.cmdline import execute\r\n  File \"/usr/local/python3.5.2/lib/python3.5/site-packages/scrapy/__init__.py\", line 27, in <module>\r\n    from . import _monkeypatches\r\n  File \"/usr/local/python3.5.2/lib/python3.5/site-packages/scrapy/_monkeypatches.py\", line 20, in <module>\r\n    import twisted.persisted.styles  # NOQA\r\nImportError: No module named 'twisted.persisted'\r\n```\r\nSo I enter ipython3 to ensure whether it's installed porperly:\r\n```\r\nIn [2]: import twisted\r\nIn [3]: twisted.version\r\nOut[3]: Version('twisted', 15, 2, 1)\r\n```\r\nBut when I `import scrapy`, the same as command-line:\r\n```\r\nIn [4]: import scrapy\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-4-51c73a18167b> in <module>()\r\n----> 1 import scrapy\r\n\r\n/usr/local/python3.5.2/lib/python3.5/site-packages/scrapy/__init__.py in <module>()\r\n     25 \r\n     26 # Apply monkey patches to fix issues in external libraries\r\n---> 27 from . import _monkeypatches\r\n     28 del _monkeypatches\r\n     29 \r\n\r\n/usr/local/python3.5.2/lib/python3.5/site-packages/scrapy/_monkeypatches.py in <module>()\r\n     18 # Undo what Twisted's perspective broker adds to pickle register\r\n     19 # to prevent bugs like Twisted#7989 while serializing requests\r\n---> 20 import twisted.persisted.styles  # NOQA\r\n     21 # Remove only entries with twisted serializers for non-twisted types.\r\n     22 for k, v in frozenset(copyreg.dispatch_table.items()):\r\n\r\nImportError: No module named 'twisted.persisted'\r\n```\r\n\r\nI'm so confused, I just want to run a spaider, So help me ! :sos: ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2580/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 1}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "philonor": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2578", "title": "Raising CloseSpider from DownloaderMiddleware doesn't work", "body": "**Behavior**:\r\nRaising a [CloseSpider](https://doc.scrapy.org/en/latest/topics/exceptions.html#closespider) exception from a DownloaderMiddleware's `process_response` doesn't close the spider.\r\nInstead, the scraper only outputs `CloseSpider` to `stdout`.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/5816039/23133739/74a53a34-f792-11e6-8104-0dc641d0ce39.png)\r\n\r\n**Expected behavior**:\r\nSpider should shut down.\r\n\r\n**Workarounds**:\r\n- Use [`crawler.stop`](https://github.com/scrapy/scrapy/blob/129421c7e31b89b9b0f9c5f7d8ae59e47df36091/scrapy/crawler.py#L197):\r\n```\r\nself.crawler.stop()\r\nreturn None\r\n```\r\nwhich requires to make `self.crawler` available in the `DownloaderMiddleware` via:\r\n```\r\n    def __init__(self, crawler):\r\n        self.crawler = crawler\r\n\r\n    @classmethod\r\n    def from_crawler(cls, crawler):\r\n        return cls(crawler)\r\n```\r\n- ~~Use [`crawler._signal_shutdown()`](http://stackoverflow.com/questions/4448724/force-my-scrapy-spider-to-stop-crawling/7969243#7969243)~~ (doesn't work for me)\r\n- ~~Return [`None` from `process_response`](http://stackoverflow.com/questions/27001586/scrapy-not-responding-to-closespider-exception?answertab=active#tab-top)~~ (doesn't work for me)", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2578/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "andrewbaxter": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2516", "title": "get_output_value behavior differs from load_item with empty data", "body": "If an output processor requires a value and a field has no data, doing `get_output_value` will raise an exception whereas `load_item` doesn't.  I expected `get_output_value` to return `None` in this case.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2516/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "whwq2012": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2513", "title": "twisted error in log", "body": "> 2017-01-19 14:41:21 [twisted] CRITICAL: Unhandled Error\r\n> Traceback (most recent call last):\r\n> Failure: twisted.internet.error.ConnectionDone: Connection was closed cleanly.\r\n\r\nThis is error, I check my code, and I think the error that from scrapy\r\n\r\nscrapy version:1.30\r\nos:centos7.0\r\npython version: 2.7.11", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2513/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "manisoftwartist": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2504", "title": "Unable to retreive http return code from ImagesPipeline (or MediaPipeline) in scrapy", "body": "I have a working spider scraping image URLs and placing them in image_urls field of a scrapy.Item. I have a custom pipeline that inherits from ImagesPipeline. When a specific URL returns a non-200 http response code (like say a 401 error). For instance, in the log files, I find\r\n```\r\nWARNING:scrapy.pipelines.files:File (code: 404): Error downloading file from <GET http://a.espncdn.com/combiner/i%3Fimg%3D/i/headshots/tennis/players/full/425.png> referred in <None>\r\nWARNING:scrapy.pipelines.files:File (code: 307): Error downloading file from <GET http://www.fansshare.com/photos/rogerfederer/federer-roger-federer-406468306.jpg> referred in <None>\r\n```\r\n\r\n```\r\ndef item_completed(self, results, item, info):\r\n\r\nimage_paths = []\r\nfor download_status, x in results:\r\n    if download_status:\r\n        image_paths.append(x['path'])\r\n        item['images'] = image_paths  # update item image path\r\n        item['result_download_status'] = 1\r\n    else:\r\n        item['result_download_status'] = 0\r\n        #x.printDetailedTraceback()\r\n        logging.info(repr(x)) # x is a twisted failure object\r\n\r\nreturn item\r\n```\r\n\r\nHowever, I am unable to capture the error codes 404,307 etc in my custom image pipeline in the item_completed() function:\r\n\r\n\r\ndef item_completed(self, results, item, info):\r\n\r\n    image_paths = []\r\n    for download_status, x in results:\r\n        if download_status:\r\n            image_paths.append(x['path'])\r\n            item['images'] = image_paths  # update item image path\r\n            item['result_download_status'] = 1\r\n        else:\r\n            item['result_download_status'] = 0\r\n            #x.printDetailedTraceback()\r\n            logging.info(repr(x)) # x is a twisted failure object\r\n            \r\n    return item\r\n\r\nDigging through the scrapy source code, inside the media_downloaded() function in files.py, I found that for non-200 response codes, a warning is logged (which explains the above WARNING lines) and then a FileException is raised. \r\n\r\n```\r\n    if response.status != 200:\r\n            logger.warning(\r\n                'File (code: %(status)s): Error downloading file from '\r\n                '%(request)s referred in <%(referer)s>',\r\n                {'status': response.status,\r\n                 'request': request, 'referer': referer},\r\n                extra={'spider': info.spider}\r\n            )\r\n    \r\n            raise FileException('download-error')\r\n```\r\n\r\nHow do I also access this response code so I can handle it in my custom image pipeline in item_completed() function?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2504/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "medse": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2498", "title": "ItemLoader: zeros as field values", "body": "I'm scraping a eshop, there's a price xpath for an item's price, which is empty if the item is out of stock.\r\nI use ItemLoader, and add_xpath():\r\n\r\n`item.add_xpath('price', './/span[@class=\"price rub\"]/text()')`\r\n\r\nI want to set the price to 0.0 if the item is missing, for this I check if the xpath is empty in the price_in declaration within the ItemLoader-based class:\r\n\r\n`price_in=Compose(TakeFirst(), lambda _: float(_) if _ else 0)`\r\nBut the value of zero isn't stored in the _values dict because of the following code in the scrapy/loaders/__init__.py:_add_value():\r\n\r\n```\r\n    def _add_value(self, field_name, value):\r\n        value = arg_to_iter(value)\r\n        processed_value = self._process_input_value(field_name, value)\r\n        if processed_value:\r\n            self._values[field_name] += arg_to_iter(processed_value)\r\n```\r\n\r\nI don't know why the logic is like this, but it won't store neither zeros nor empty strings. Is it illegal? I use scrapy for about a week, so don't know the usage practices at all, but this seems strange to me.\r\nMaybe change the condition to \u201cprocessed_value is not None\u201d?\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2498/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "IAlwaysBeCoding": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2488", "title": "Feature Request: Adding \"add_jmes\" and \"replace_jmes\" method to ItemLoader", "body": "So, currently the `ItemLoader` class has 6 methods for loading values:\r\n`add_xpath()`\r\n`replace_xpath()`\r\n`add_css()`\r\n`replace_css()`\r\n`add_value()`\r\n`replace_value()`\r\n\r\nCould we add another 2 more methods for loading data through JmesPath selectors. Currently, I have to use the `SelectJmes` processor to do this. Eventually, it looks really ugly and ends up taking so much line real estate. \r\nI did a hack where I extended the ItemLoader to include those 2 extra methods that are desperately needed.\r\n\r\nWhen there is json data to parse instead of html, JmesPath selectors are the best way to go for parsing, so there should be support for JmesPath selectors in the ItemLoader class as well.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2407", "title": "SelectJmes fails to parse the right way due to arg_to_iter() turning the output into a list.", "body": "Here is an example of what I'm talking about:\r\n\r\n```python\r\nimport json\r\nfrom scrapy import Item\r\nfrom scrapy.loader import ItemLoader\r\nfrom scrapy.loader.processors import SelectJmes\r\nstring = r'''{\r\n  \"type\": [\r\n    \"http://schema.org/Physician\"\r\n  ], \r\n  \"properties\": {\r\n    \"url\": [\r\n      \"http://www.vitals.com/doctors/Dr_Patrick_B_Wright.html\"\r\n    ], \r\n    \"aggregateRating\": [\r\n      {\r\n        \"type\": [\r\n          \"http://schema.org/AggregateRating\"\r\n        ], \r\n        \"properties\": {\r\n          \"worstRating\": [\r\n            \"0\"\r\n          ], \r\n          \"reviewCount\": [\r\n            \"1\"\r\n          ], \r\n          \"bestRating\": [\r\n            \"5\"\r\n          ], \r\n          \"ratingValue\": [\r\n            \"4.3\"\r\n          ], \r\n          \"ratingCount\": [\r\n            \"6\"\r\n          ]\r\n        }\r\n      }\r\n    ], \r\n    \"medicalspecialty\": [\r\n      {\r\n        \"type\": [\r\n          \"http://schema.org/MedicalSpecialty\"\r\n        ], \r\n        \"properties\": {\r\n          \"name\": [\r\n            \"Pediatric Surgery\"\r\n          ]\r\n        }\r\n      }\r\n    ], \r\n    \"name\": [\r\n      \"Dr. Patrick Wright MD\"\r\n    ], \r\n    \"address\": [\r\n      {\r\n        \"type\": [\r\n          \"http://schema.org/PostalAddress\"\r\n        ], \r\n        \"properties\": {\r\n          \"addressLocality\": [\r\n            \"Jackson\"\r\n          ], \r\n          \"addressRegion\": [\r\n            \"MS\"\r\n          ], \r\n          \"streetAddress\": [\r\n            \"2500 N State St\"\r\n          ], \r\n          \"postalCode\": [\r\n            \"39216\"\r\n          ], \r\n          \"telephone\": [\r\n            \"(601) 984-2150\"\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}'''\r\n\r\ndata = json.loads(string)\r\n\r\n#This will work fine\r\nproc = SelectJmes('properties.address[0].properties.postalCode[0]')\r\n\r\n\r\n#However, if its used in an input processor it will fail and will need to add `[0]` before the query in order to work.\r\n\r\nclass SomeItem(Item):\r\n     postal_code = Field(\r\n          input_processor = SelectJmes('properties.address[0].properties.postalCode[0]')\r\n     )\r\n\r\nloader = ItemLoader(SomeItem())\r\nloader.add_value('postal_code', data)\r\nitem = loader.load_item()\r\n\r\n```\r\n\r\nThe only way to make it work will be to always add `[0]` to the JmesQuery when done inside an input or output field processor.\r\n#This will work\r\n[0].properties.address[0].properties.postalCode[0]\r\n\r\nShould we document this odd behaviour due to arg_to_iter doing this?\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2407/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/6af323d7c85eeee40d90a20133504df26a593304", "message": "Fix spelling mistake on scrapy parse command docs\n\nFixed spelling mistake from \"will be pass\" to \"will be passed\""}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/846fd83512bb45335195b060d76b46060b4b6e3d", "message": "removed commented out code, wrapped line to pep-8 and removed backlashes"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "astrung": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2484", "title": "Scrapy: how to rename project", "body": "I have created a new project with this command : `scrapy startproject first_scrapy`\r\n\r\nBut now i want to change this project name to \"web_crawler\" . After i tried to change project name , i can not start `scrapy` command . I got this error : \r\n\r\n      File \"c:\\python27\\lib\\runpy.py\", line 174, in _run_module_as_main\r\n      \"__main__\", fname, loader, pkg_name)\r\n      File \"c:\\python27\\lib\\runpy.py\", line 72, in _run_code\r\n        exec code in run_globals\r\n      File \"C:\\Python27\\Scripts\\scrapy.exe\\__main__.py\", line 9, in <module>\r\n      File \"c:\\python27\\lib\\site-packages\\scrapy\\cmdline.py\", line 108, in execute\r\n        settings = get_project_settings()\r\n      File \"c:\\python27\\lib\\site-packages\\scrapy\\utils\\project.py\", line 68, in get_project_settings\r\n        settings.setmodule(settings_module_path, priority='project')\r\n      File \"c:\\python27\\lib\\site-packages\\scrapy\\settings\\__init__.py\", line 282, in setmodule\r\n        module = import_module(module)\r\n      File \"c:\\python27\\lib\\importlib\\__init__.py\", line 37, in import_module\r\n        __import__(name)\r\n      ImportError: No module named first_scrapy.settings\r\n\r\nSo how can i rename old project ?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "eliasdorneles": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2481", "title": "Clean bad HTML", "body": "There are some cases of bad HTML that makes Scrapy (well, lxml really) to choke on the response content, and I was thinking it would make sense to add a CleanBadHtml middleware that could be optionally disabled.\r\n\r\nI've just stumbled on an example case from a real website, where the response had something like this (real phone number edited) in its content:\r\n\r\n    text = u'<a href=\"tel:111\\x00111\\x001111\">111-111-1111</a>'\r\n\r\nThe `\\x00` is interpreted as end of input by lxml, so the selector ends up stopping right there:\r\n\r\n```\r\n>>> parsel.Selector(text)\r\n<Selector xpath=None data=u'<html><body><a href=\"tel:111\"></a></body'>\r\n>>> parsel.Selector(text).extract()\r\nu'<html><body><a href=\"tel:111\"></a></body></html>'\r\n```\r\n\r\nI'm not sure what's the best place to fix this, but I think we gotta do something about it either in Scrapy or in Parsel, because these HTML pages are accepted by the browsers, who ignore the null characters.\r\n\r\nFor this specific case, simply removing the `\\x00` characters found in the body before passing to the selector would avoid the issue.\r\n\r\nWhat do you think?\r\nWhere do you think it would be the best place to do this?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2481/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2351", "title": "[MRG+1] Add artwork license, show both licenses in the docs", "body": "This introduces a license for the artwork (adapted from http://flask.pocoo.org/docs/0.11/license/#flask-artwork-license) and adds a documentation section for both the software and artwork licenses.\n\nLooks good?\n", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/15674495", "body": "cool! :)\nwouldn't we want to clean up the temp file?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/15674495/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/19232595", "body": "Ookay, haha, lemme bring back `step 1`.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/19232595/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/19232719", "body": "PR: https://github.com/scrapy/scrapy/pull/2296\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/19232719/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "mohmad-null": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2476", "title": "Consistent DOWNLOAD_WARNSIZE message", "body": "My logs have both of these messages:\r\n\r\nWARNING: Expected response size (135576123) larger than download warn size (33554432).\r\n\r\nWARNING: Received more bytes than download warn size (33554432) in request <GET https://example.com>.\r\n\r\nI'd suggest that both should include the url, or neither should. Ideally both as currently it's not readily possible to tell from the logs what URL triggered the former.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2476/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2474", "title": "Optimised dequeing", "body": "A feature request.\r\nI've got my settings set up so that Scrapy is being nice and polite to any given site. For example:\r\n\r\n```\r\n# Very high\r\nCONCURRENT_REQUESTS = 128\r\n\r\nCONCURRENT_REQUESTS_PER_IP = 3\r\nDOWNLOAD_DELAY = 2\r\n\r\nAUTOTHROTTLE_ENABLED = True\r\nAUTOTHROTTLE_TARGET_CONCURRENCY = 3\r\nAUTOTHROTTLE_START_DELAY = 1\r\nAUTOTHROTTLE_MAX_DELAY = 15\r\n\r\n#because I want to try and focus on breadth - I'm mostly making few a requests to many many domains\r\n#With a few domains getting many requests\r\nDEPTH_PRIORITY = 1\r\nSCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleFifoDiskQueue'\r\nSCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.FifoMemoryQueue'\r\n```\r\n\r\nAs you can see, for any given IP, scrapy is going to be slow, but in theory scrapy can handle many different IP's at once - that's why CONCURRENT_REQUESTS is very high. The problem is, it doesn't seem to.\r\n\r\n\r\nConsider the following hypothetical queue (assume all domains are on different IPs):\r\n\r\n> example.com/1\r\n> example.com/2\r\n> example.com/3\r\n> example.com/4\r\n> example.com/5\r\n> example.com/6\r\n> example.com/7\r\n> example.com/8\r\n> google.example/1\r\n> google.example/2\r\n> google.example/3\r\n> google.example/4\r\n> google.example/5\r\n> google.example/6\r\n> google.example/7\r\n> blue.example\r\n> yellow.example\r\n> red.example\r\n> green.example\r\n> purple.example\r\n> pink.example\r\n> blank.example\r\n\r\nAssuming scrapy is parsing the above top-down (and ignoring whether we're doing FIFO or reverse), what seems to happen is that scrapy will scrape all of the example.com pages first, then the google.example ones, in both instances hitting the per-ip/autothrottle limits and waiting for them to clear down.\r\n\r\nHowever, the optimal way to do the above list is to note all the different domains, and then try and do them all concurrently (within the limit of CONCURRENT_REQUESTS). This doesn't appear to happen at present.\r\n\r\n(At least, I'm guessing that's the reason I'm seeing very slow crawl rates, a low number of tcp/ip threads, as well as negligible CPU usage (<20% of a single core), and almost non-existent network usage (about 1MB per min - 130k bits/s!!!))", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2474/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2472", "title": "Scrapy loses trailing ?", "body": "Scrapy 1.2.1\r\nIf I populate start_urls with:\r\nhttp://localhost/myfile.json?\r\n\r\nMy actual request (as well as the contents of response.url) is to:\r\nhttp://localhost/myfile.json\r\n\r\nNote the trailing **?** has been removed.\r\n\r\nI don't know if there are any servers that will give a different response depending on whether the ? is there or not, however I'm recording the URL's as keys, so the fact that my return URL is different from my sending-url is making life difficult.\r\n\r\nI can work around it, but I suspect this isn't meant to be happening, hence the report.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2472/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2443", "title": "robots.txt requests don't use errback", "body": "I'm triggering a timeout error to test my script.\r\n\r\nEven though I have an errback, with basically the same code as here - https://doc.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-errbacks - the automated request that's made by scrapy to robots.txt for a host isn't using the errback function.\r\n\r\nAll other requests are though.\r\n\r\nScrapy 1.2.1 ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2443/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2442", "title": "Timeout raises exception", "body": "This issue has several components, all related to TimeoutErrors.\r\nScrapy 1.2.1\r\n\r\na) If a TimeoutError is raised, by default it will print the entire exception to the logger. This isn't in keeping with other exceptions (DNSLookupError, TCPTimedOutError, ConnectError) which are all transparent.\r\n\r\n~~b) The TimeoutError trace without a errback will halt the processing of the spider. This isn't in keeping with other exceptions (DNSLookupError, TCPTimedOutError, ConnectError) which are all transparent.~~", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2442/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2392", "title": "Load settings dynamically on a per-spider basis", "body": "Feature request.\r\n\r\nIt would be good if scrapy had an easily accessible means of reading settings on a per-spider basis, and then making them accessible to the spider. From my many attempts to do this so far, in theory all of the components for this appear to be in place. Populating settings is already done:\r\nhttps://doc.scrapy.org/en/latest/topics/settings.html#populating-the-settings - but then the problem is accessing them.\r\nIdeally in a fashion that's compatible with scrapyd (so no calling `process.crawl(spider, my_settings)`).\r\n\r\n**Ideally**: A project could have a generic project wide settings.py file with both the standard settings and any custom ones added by the developer. Then, using a command-line argument to indicate the settings file to use, the `__init__` method of the spider overrides specific settings, (much as `custom_settings` does), and these settings are then accessible throughout the spider via `self.settings` in the usual way.\r\n\r\n**Current Problems**\r\n*custom_settings*\r\nUnfortunately `custom_settings` doesn't seem to be usable for this because it cannot be declared in `__init__`, but needs to be declared earlier.\r\n\r\n*settings.py*\r\nCurrently, even if a user is willing to just use a different settings.py file entirely for each spider (thereby duplicating most of it), that's not readily possible either.\r\n\r\n\tos.environ['SCRAPY_SETTINGS_MODULE'] = 'myproject.settings'\r\n\tthese_settings = get_project_settings()\r\n\r\nThe above only gets the settings into the variable `these_settings`, they're not used by the spider or accessible via `self.settings`.\r\n\r\n**Desire for feature**\r\nBased on StackOverFlow, this is something a lot of people want. The fact there are so many answers that are all so different shows there isn't a particular good way of doing it.\r\n\r\nhttp://stackoverflow.com/questions/9814827/creating-a-generic-scrapy-spider\r\nhttp://stackoverflow.com/questions/12996910/how-to-setup-and-launch-a-scrapy-spider-programmatically-urls-and-settings\r\nhttp://stackoverflow.com/questions/35662146/dynamic-spider-generation-with-scrapy-subclass-init-error\r\nhttp://stackoverflow.com/questions/40510526/how-to-load-different-settings-for-different-scrapy-spiders\r\nhttp://stackoverflow.com/questions/2396529/using-one-scrapy-spider-for-several-websites\r\n\r\nBeing able to readily get allowed_domains and start_urls within it would also be good.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2392/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2378", "title": "get_project_settings of doesn't reflect per-spider settings", "body": "It's possible I'm using this wrong. But I'm doing the following:\r\n\r\n```\r\nclass MySpider (Spider)\r\n  def __init__(self):\r\n\t\tcustom_settings = {\r\n\t\t\t'AUTOTHROTTLE_ENABLED': False\r\n\t\t}\r\n\r\n  a = scrapy.utils.project.get_project_settings()\r\n  print(a.copy_to_dict())\r\n```\r\nMy **settings.py** has `AUTOTHROTTLE_ENABLED: True`.\r\n\r\nBased on my reading of https://doc.scrapy.org/en/latest/topics/settings.html#populating-the-settings - the custom_settings value should override the value of settings.py. But the result of the above is:\r\n\r\n> ....\r\n> AUTOTHROTTLE_ENABLED: True\r\n> ...\r\n\r\nIt's using the value from settings.py - am I misunderstanding this or is it a bug? I was hoping to load and alter settings dynamically during the __init__.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2378/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2376", "title": "disallow_domains", "body": "Feature request\r\nCurrently there's \"allowed_domains\" to create a whitelist of domains to scrape.\r\n\r\nIt would be good if there was a \"disallowed_domains\" or \"blocked_domains\" as well. I appreciate I could could probably do this in middleware, but I figure it's something quite a few people would want.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2376/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2350", "title": "Pipelines documentation limited", "body": "I've spent quite a while going through the documentation, and while I like the concept off pipelines, no-where can I find documentation which shows how to fully implement them end-to-end.\n\nThe Pipelines page (https://doc.scrapy.org/en/latest/topics/item-pipeline.html) only shows code for the pipeline itself, not how to use it / plug it in to the main project.\n\nThe example project quotesbot is no better. While it does contain a pipelines.py, the class within it is never used. Indeed, filling this file with junk that should raise numerous syntax Exceptions doesn't do anything either. As examples go, it is extremely limited.\nitems.py is similarly ignored.\n\nI would suggest the following:\n- Include documenation for how to glue all of the components of a complete spider together\n- Don't include files that are unused in the example quotesbot\n- Create another example project that does use items/pipelines etc.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2350/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2349", "title": "Invalid commands in example", "body": "The quotesbot:\nhttps://github.com/scrapy/quotesbot\n\nSays to run:\n`scrapy crawl`\nand\n`scrapy list`\n\nHowever, neither of these are a valid command for me. I only have:\n\n> Available commands:\n>   bench         Run quick benchmark test\n>   commands\n>   fetch         Fetch a URL using the Scrapy downloader\n>   genspider     Generate new spider using pre-defined templates\n>   runspider     Run a self-contained spider (without creating a project)\n>   settings      Get settings values\n>   shell         Interactive scraping console\n>   startproject  Create new project\n>   version       Print Scrapy version\n>   view          Open URL in browser, as seen by Scrapy\n> \n>   [ more ]      More commands available when run from project directory\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2349/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tituskex": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2473", "title": "Issue with running scrapy spider from script.", "body": "Hi, I'm trying to run scrapy from a script like this:\r\n\r\n    import scrapy\r\n    from scrapy.crawler import CrawlerProcess\r\n\r\n    class MySpider(scrapy.Spider):\r\n        name = \"basic\"\r\n        allowed_domains = [\"web\"]\r\n        start_urls = ['http://www.example.com']\r\n\r\n        def parse(self, response):\r\n            l = ItemLoader(item=PropertiesItem(), response = response)\r\n            l.add_xpath('title', '//h1[1]/text()')\r\n\r\n            return l.load_item()\r\n    process = CrawlerProcess({\r\n        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\r\n    })\r\n\r\n    process.crawl(MySpider)\r\n    process.start()\r\n\r\nHowever, when I run this script I get the following error:\r\n\r\n    File \"/Library/Python/2.7/site-packages/Twisted-16.7.0rc1-py2.7-macosx-10.11-\r\n    intel.egg/twisted/internet/_sslverify.py\", line 38, in <module>\r\n    TLSVersion.TLSv1_1: SSL.OP_NO_TLSv1_1,\r\n    AttributeError: 'module' object has no attribute 'OP_NO_TLSv1_1'\r\n\r\nDoes anyone know how to fix this? Thanks in advance.\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2473/reactions", "total_count": 21, "+1": 21, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "haowg": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2468", "title": "Can not access HTTPS web site with proxy", "body": "when access a http url with a proxy, it worked\r\nbut a https url it can't work\r\nis this a bug?\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2468/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mkaya93": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2452", "title": "Image Background converting to green.", "body": "Hi, \r\n\r\nProblem is I'm downloading images with crawler but they are transparency. So image is looking like this.\r\n\r\n![a2062c7f64b9a136c16f1a3d8491b70902986fc4](https://cloud.githubusercontent.com/assets/6665723/21229545/97a44c8c-c2ea-11e6-9f96-5e60106bd340.jpg)\r\n\r\nBut it should look like this.\r\n ##\r\n![wd-bvbz0120jch-12tb-my-cloud-ex2-ultra-gigabit-ethernet-kisisel-bulut-depolama](https://cloud.githubusercontent.com/assets/6665723/21229505/7bb3b8d2-c2ea-11e6-94fc-921adb2f21ee.png)\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2452/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "guohengkai": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2449", "title": "Retry downloading and delay in media pipeline", "body": "How to implement retrying and delay in media pipeline? Seem that the DownloaderMiddleware does not work for pipeline. Thanks.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2449/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vshlapakov": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2448", "title": "S3FilesStore: add option for AWS Signature v4 support", "body": "Newer AWS locations will only authenticate clients that use AWS Signature Version 4.\r\nFor example, when using `Images` addon with a custom S3 location you can encounter with:\r\n```\r\nboto.exception.S3ResponseError: S3ResponseError: 400 Bad Request\r\nThe authorization mechanism you have provided is not supported. Please use AWS4-HMAC-SHA256.\r\n```\r\nThe problem is pretty well known: https://github.com/boto/boto/issues/2741.\r\n\r\nBoto library [supports](https://github.com/boto/boto/blob/2.39.0/boto/auth.py#L1025) v4 if you pass a host explicitly and set `S3_USE_SIGV4` envvar. It's a bit [different](https://github.com/boto/botocore/issues/424#issuecomment-68394401) for `botocore`, but should work fine when providing a region short name.\r\n\r\nMy proposal is to implement it in a similar way to this [approach](https://github.com/danilop/yas3fs/issues/101): `S3FilesStore` could handle new `AWS_HOST` and `AWS_S3_USE_SIGV4` settings, use it to set a corresponding envvar and pass the host to `boto.S3Connection`(or extract the region from host and pass it to botocore client).", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2448/reactions", "total_count": 4, "+1": 4, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jacob1237": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2436", "title": "Access Response object from the Scrapy Pipeline", "body": "Hello,\r\n\r\nI'd like to implement a pipeline which compares a checksum of the current item with some old values in cache and drops the items that are not modified.\r\nLooks like this is a standard use-case for change detection systems (e.g. price monitoring).\r\n\r\nRight now I put the cache (old checksums) into the Request.meta field, but there is no way to access it inside the pipeline.\r\n\r\nAs a workaround I did the same trick with the Spider Middleware (process_spider_output), but now I have another problem: when I want to skip processing of some item and signalize other components about that with Exception (like `DropItem`), to catch it with the `spider_error` signal, I can't suppress the default exception handler.\r\n\r\nSo every time I get some tracebacks in the log (I don't want them because I have my own spider exception handler).\r\n\r\nThis is because of that code in `scrapy/core/scraper.py`:\r\n```python\r\ndef handle_spider_error(self, _failure, request, response, spider):\r\n    exc = _failure.value\r\n    if isinstance(exc, CloseSpider):\r\n        self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\r\n        return\r\n    referer = request.headers.get('Referer')\r\n    logger.error(\r\n        \"Spider error processing %(request)s (referer: %(referer)s)\",\r\n        {'request': request, 'referer': referer},\r\n        exc_info=failure_to_exc_info(_failure),\r\n        extra={'spider': spider}\r\n    )\r\n    self.signals.send_catch_log(\r\n        signal=signals.spider_error,\r\n        failure=_failure, response=response,\r\n        spider=spider\r\n    )\r\n    ...other code here...\r\n```\r\nIt seems there is no way to suppress Spider Exceptions before the signal execution (even if I have a handler).\r\n\r\nI know that I can implement a custom crawler signal (or use the `item_dropped`) and execute it from my middleware (as a workaround), but won't it be easier to access the Response object from the Pipeline handlers?\r\n\r\nI can try to make a patch which will add the additional parameter to the pipeline handler (without breaking compatibility with an old code), but first I'd like to discuss it with the community.\r\n\r\nIs it conceptually wrong to access the response object from the pipeline?\r\n\r\n**P.S.** Another question outside the topic - what if we want to suppress spider exceptions that differs from the `CloseSpider` exception?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2436/reactions", "total_count": 1, "+1": 0, "-1": 1, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sibiryakov": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2435", "title": "Exposing downloader stats to custom scheduler", "body": "In order to get maximum fetching performance, the queue have to be carefully metered. In order to do this the custom scheduler needs to know:\r\n- the type of the key in downloader (ip or hostname),\r\n- count of requests to specific hostname/ip in the queue,\r\n- delay/concurrency parameters of hostname/ip,\r\n- list of all hostname/ips in the queue.\r\n\r\nCurrent Scheduler API is designed for storage and resume-from-disk purposes, so I think it's time to re-think it taking into account efficiency of fetching. The most common problem with inefficient fetching is a queue filled with a single domain and polite crawling requirement.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2435/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "briehanlombaard": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2424", "title": "SSL handshake failure", "body": "Hi,\r\n\r\nI'm getting a handshake error for the sites listed below:\r\n\r\n```\r\n2016-12-03 00:02:19 [scrapy] ERROR: Error downloading <GET https://apnews.com>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]\r\n2016-12-03 00:03:25 [scrapy] ERROR: Error downloading <GET https://techcrunch.com>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]\r\n2016-12-03 00:03:53 [scrapy] ERROR: Error downloading <GET https://medium.com>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]\r\n2016-12-03 00:05:08 [scrapy] ERROR: Error downloading <GET https://theintercept.com>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]\r\n2016-12-03 00:06:32 [scrapy] ERROR: Error downloading <GET https://www.opendemocracy.net/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]\r\n2016-12-03 00:07:55 [scrapy] ERROR: Error downloading <GET https://www.rt.com>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]\r\n2016-12-03 00:19:53 [scrapy] ERROR: Error downloading <GET https://www.thestar.com>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]\r\n2016-12-03 00:58:42 [scrapy] ERROR: Error downloading <GET https://www.cnet.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]\r\n```\r\n\r\nWhat's strange is that it works if I try each one of those sites individually using `scrapy shell` so I might be doing something wrong.\r\n\r\nHere's some information about my environment:\r\n\r\n```\r\n$ scrapy version -v\r\nScrapy    : 1.2.1\r\nlxml      : 3.6.4.0\r\nlibxml2   : 2.9.4\r\nTwisted   : 16.6.0\r\nPython    : 2.7.12 (default, Jul  1 2016, 15:12:24) - [GCC 5.4.0 20160609]\r\npyOpenSSL : 16.2.0 (OpenSSL 1.0.2g-fips  1 Mar 2016)\r\nPlatform  : Linux-3.13.0-52-generic-x86_64-with-Ubuntu-16.04-xenial\r\n```\r\n\r\nAny ideas where I can look to troubleshoot the problem?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2424/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lksy0217": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2406", "title": "I don't know how to install this", "body": "```\r\n    running build_ext\r\n    building 'lxml.etree' extension\r\n    error: Unable to find vcvarsall.bat\r\n\r\n    ----------------------------------------\r\nCommand \"c:\\users\\jongho\\appdata\\local\\programs\\python\\python35\\python.exe -u -c \"import setuptools, tokenize;__file__='C:\\\\Users\\\\JongHo\\\\AppData\\\\Local\\\\Temp\\\\pip-build-yh509eor\\\\lxml\\\\setup.py';f=g\r\netattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record C:\\Users\\JongHo\\AppData\\Local\\Temp\\pip-629uwerr-record\\in\r\nstall-record.txt --single-version-externally-managed --compile\" failed with error code 1 in C:\\Users\\JongHo\\AppData\\Local\\Temp\\pip-build-yh509eor\\lxml\\\r\n```", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2406/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vionemc": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2399", "title": "Scrapy resume last batch after crash", "body": "Hi,\r\n\r\nI know there is a feature to resume Scrapy job, but it turns out, if I force close the spider by clicking Ctrl+C twice, it fails to resume. To be honest that makes the resume feature not that useful.\r\n\r\nWhat I want to ask is, can Scrapy survive a crash and resume the last batch? \r\n\r\nThanks,\r\nAmi", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2399/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yssoe": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2384", "title": "As soon as I start using import logging my default screen output logging when running spiders is gone.", "body": "Hi, \r\n\r\nI start to write a lot of relevant logging to my spiders using the 'import logging' and the guidelines described in the documentation. \r\n\r\nWhat I notice is that when I run a spider by hand 'scrapy crawl spidername' I don't see any of my previous default output anymore. That's a big problem for me. While writing and debugging spiders it's very nice to see what's going on with scrapy. The only way for me to have it back is by removing my logging code in the spiders. \r\n\r\nIs there a setting that I'm missing that I can use my specific code log and still see the verbose output while running a spider? \r\n\r\nCheers. ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2384/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chrisfromthelc": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2370", "title": "Permissions issue when installing attrs during pip install scrapy", "body": "OS: Rasbian Jessie 4.4\r\n\r\nWhen using a virtualenv (Python 2.7), there's an error that seems to be permissions-related during the `pip install scrapy` installation.\r\n\r\n```\r\nCollecting attrs\r\n  Using cached attrs-16.2.0-py2.py3-none-any.whl\r\nInstalling collected packages: attrs\r\nException:\r\nTraceback (most recent call last):\r\n  File \"/home/pi/TEST/local/lib/python2.7/site-packages/pip/basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"/home/pi/TEST/local/lib/python2.7/site-packages/pip/commands/install.py\", line 317, in run\r\n    prefix=options.prefix_path,\r\n  File \"/home/pi/TEST/local/lib/python2.7/site-packages/pip/req/req_set.py\", line 742, in install\r\n    **kwargs\r\n  File \"/home/pi/TEST/local/lib/python2.7/site-packages/pip/req/req_install.py\", line 831, in install\r\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\r\n  File \"/home/pi/TEST/local/lib/python2.7/site-packages/pip/req/req_install.py\", line 1032, in move_wheel_files\r\n    isolated=self.isolated,\r\n  File \"/home/pi/TEST/local/lib/python2.7/site-packages/pip/wheel.py\", line 346, in move_wheel_files\r\n    clobber(source, lib_dir, True)\r\n  File \"/home/pi/TEST/local/lib/python2.7/site-packages/pip/wheel.py\", line 317, in clobber\r\n    ensure_dir(destdir)\r\n  File \"/home/pi/TEST/local/lib/python2.7/site-packages/pip/utils/__init__.py\", line 83, in ensure_dir\r\n    os.makedirs(path)\r\n  File \"/home/pi/TEST/lib/python2.7/os.py\", line 157, in makedirs\r\n    mkdir(name, mode)\r\nOSError: [Errno 13] Permission denied: '/home/pi/TEST/lib/python2.7/site-packages/attrs-16.2.0.dist-info'\r\n```\r\n\r\nUsing `chmod`  on /lib/python2.7/site-packages/ to open up permissions allows installation of `attrs`.\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2370/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chekunkov": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2358", "title": "Items Pipeline output is documented to be dict or Item subclass, but can be anything", "body": "From [process_item documentation](https://doc.scrapy.org/en/latest/topics/item-pipeline.html#process_item):\n\n> This method is called for every item pipeline component. process_item() must either: return a dict with data, return an Item (or any descendant class) object, return a Twisted Deferred or raise DropItem exception. Dropped items are no longer processed by further pipeline components.\n\nThere was [a question in the scrapy-users group](https://groups.google.com/d/msg/scrapy-users/NNqSw0hBmu8/tdcezurtCAAJ) and it appeared that user wrote his pipeline in such a way that it swallowed the item - so his pipeline component returned `None` - which broke the following pipeline component in a way that wasn't intuitive enough for the user. Shouldn't Scrapy check pipeline output and stop item processing instead of passing `None` to the next pipeline component?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2358/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1256", "title": "Add CRAWLER setting to be able to set custom crawler via settings file", "body": "In my case it's needed for ScrapyRT - we are using custom Crawler class there.\n\nI started working on this PR before I noticed #1147, so now I have doubts if it will be useful. Theoretically now I can override `.crawl` method and pass custom crawler object there. So I decided to leave it to your judgment. I'll add test case if this PR wouldn't be rejected.\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1000", "title": "Argument to enable/disable start_requests from Crawler", "body": "For some projects we need a way to disable `start_requests` for spider based on some condition and it should work for any possible/existing spider. Now it's only possible with overriding `Crawler.crawl` method. This PR aims to provide a simple way to disable `start_requests` for all spider started from given crawler.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/11349248", "body": "is that correct? 1.1.0 right after 1.0.0 release candidate?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/11349248/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/11349273", "body": "or does this commit mean that starting from this point features would go to 1.1 release?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/11349273/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13308065", "body": "@dangra this is backwards-incompatible change to some extent. I know it wasn't a part of public API but we have > 20 spiders that were using `scrapy.xlib.pydispatch.dispatcher` to connect to some signals in `spider.__init__` and there's not way to use `crawler` for that as `crawler` object is not set in `__init__`. Looking for a way to remove that from repo, but maybe it should be possible to use `scrapy.xlib.pydispatch` with deprecation warning?\n\nNow spiders are failing with\n\n```\n  File \".../spiders/blahblahblah.py\", line 8, in <module>\n    from scrapy.xlib.pydispatch import dispatcher\nImportError: No module named pydispatch\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13308065/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13309700", "body": "I've fixed this by doing something like this for every influenced spider\n\n``` python\n@classmethod\ndef from_crawler(cls, crawler, *args, **kwargs):\n    spider = super(BaseSpider, cls).from_crawler(crawler, *args, **kwargs)\n    crawler.signals.connect(spider.spider_opened, signals.spider_opened)\n    crawler.signals.connect(spider.spider_closed, signals.spider_closed)\n    return spider\n```\n\nWDYT, is it valid way or there should be a better way of doing this?\n\nUPD: in this approach I really dislike amount of verbose boilerplate code that's required to get things done, not talking about how easy it appeared to forget to return spider from this subclassed method :(\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13309700/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8892529", "body": "You're right, I'll remove it.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8892529/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8892731", "body": "I wrote this docstring by anology with OffsiteMiddleware.get_host_regex docstring so it is obvious to developer what method he should override in first place:\n\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/contrib/spidermiddleware/offsite.py#L42\n\nIf you still think this comment is useless - please tell me, I'll remove it too.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8892731/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896151", "body": "If it's critical - could be changed to get_request_fingerprint or something like that. But I don't know if I could update it now.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896151/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896237", "body": "@kmike is right about the scopes.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896237/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8898602", "body": "I'm not sure I can write a proper documentation (I think my language skill is not high enough). What I was thinking of is adding a sentense to http://doc.scrapy.org/en/latest/topics/settings.html#dupefilter-class:\n\n> In order to change the way duplicates are checked you could subclass `RFPDupeFilter` and override it `request_fingerprint` method.\n\nPlease tell me if I should somehow commit this (or make another PR for documentation, or maybe? update??? current PR), or someone else will edit documentation more properly.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8898602/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ultragis": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2357", "title": "engine_started called multi times in one process, if multi spiders exists.", "body": "My Extension code is:\n\n```\nclass SpiderStatsExtension(object):\n    @classmethod\n    def from_crawler(cls, crawler):\n        if not hasattr(cls, '__instance'):\n            setattr(cls, '__instance', cls())\n        ext = getattr(cls, '__instance')\n        crawler.signals.connect(ext.engine_started, signal=signals.engine_started)\n        crawler.signals.connect(ext.engine_stopped, signal=signals.engine_stopped)\n        crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)\n        crawler.signals.connect(ext.spider_error, signal=signals.spider_error)\n        return ext\n```\n\nMy process start code is:\n\n```\nprocess = CrawlerProcess(get_project_settings())\nspiders = process.spider_loader.list()\nfor spider in spiders:\n    process.crawl(process.spider_loader.load(spider))\nprocess.start()\n```\n\nI found the from_crawler called multi times if my project have multi spiders.\nIs it expectant design? \nI need once call for on process to record stats info, how can i do for it?\nthanks.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2357/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dingld": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2356", "title": "twisted.internet.error with bindaddress with scrapy.Request", "body": "## Python3.5 with latest Scrapy\n\nI was trying to set the bindaddress with Request.meta , whatever (host, port) I  chose, the program always complained about \n\n```\nTraceback (most recent call last):\n  File \"/Users/ld/.virtualenvs/scrapy/lib/python3.5/site-packages/twisted/internet/defer.py\", line 1126, in _inlineCallbacks\n    result = result.throwExceptionIntoGenerator(g)\n  File \"/Users/ld/.virtualenvs/scrapy/lib/python3.5/site-packages/twisted/python/failure.py\", line 389, in throwExceptionIntoGenerator\n    return g.throw(self.type, self.value, self.tb)\n  File \"/Users/ld/.virtualenvs/scrapy/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\n    defer.returnValue((yield download_func(request=request,spider=spider)))\ntwisted.internet.error.ConnectBindError: Couldn't bind: 48: Address already in use.\n```\n\nIs there any approach to that?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2356/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shyandsy": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2352", "title": "how to let the log only log ERROR level information", "body": "I tried lots of way to use the ERROR level, but still all information was wrote into log file\n\npython 3.5 + scrapy 1.2\n\n``` python\ndmoz_spider.py\n\nconfigure_logging(install_root_handler=False)\nlogging.basicConfig(filename='log.txt', format='%(levelname)s: %(message)s', level=logging.ERROR)\n```\n\n``` python\nsettings.py\n\n...................\nLOG_LEVEL = \"ERROR\"\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2352/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bhagatsajan0073": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2324", "title": "sslv3 alert handshake failure", "body": "Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]\n<img width=\"809\" alt=\"sslv3\" src=\"https://cloud.githubusercontent.com/assets/10372487/19321896/75496a66-90d4-11e6-8c1c-9018bcaaecb4.PNG\">\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2324/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dolohow": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2309", "title": "Allow specifying infinite expire time for files", "body": "Documentation does not mentioned how to set up the infinite expire time of `IMAGES_EXPIRES` or `FILES_EXPIRES`.\n\nI look into the source code of the function `media_to_download` and I found this:\n\n``` python\n        self.expires = settings.getint(\n            resolve('FILES_EXPIRES'), self.EXPIRES\n        )\n```\n\nand that\n\n``` python\n            if age_days > self.expires:\n                return  # returning None force download\n```\n\nI concluded that there is no way to specify infinite time.  If I am wrong, please correct me.  I can also create PR for that.\n\nThank you.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2309/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2305", "title": "input_processor called only once", "body": "Hi guys,\n\nI am using `input_processor` on custom `ItemLoader` along one inside `Field` declaration that belongs to `Item` which is being used by `ItemLoader`.  The problem is that only `ItemLoader` fires up the `input_processor`.  Is it a desired behavior?\n\nRelated code:\n\n``` python\n\nclass ProductsItem(scrapy.Item):\n    # input_loader is not called\n    currency = scrapy.Field(input_processor=MapCompose(utils.get_unified_currency_name))\n\nclass CustomProductLoader(ItemLoader):\n    default_output_processor = TakeFirst()\n    # input_loader works fine here\n    currency_in = MapCompose(lambda x: x[0])\n\nclass MySpider(scrapy.Spider):\n    # ....\n    def parse(self, response):\n        for product in response.css('bla bla'):\n            loader = CustomProductLoader(ProductsItem(), product, response=response)\n            loader.add_css('currency', 'bla bla')\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2305/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ilovenwd": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2297", "title": "Order of HttpCacheMiddleware vs. RetryMiddleware", "body": "RetryMiddleware should be _AFTER_ HttpCacheMiddleware (thus retry.process_response called before cache)\n\nbecause a response that should be retried should NOT be cached.\n\n```\nDOWNLOADER_MIDDLEWARES_BASE = {\n    # Engine side\n    'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 400,\n    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 500,\n    'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware': 560,\n    'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600,\n    'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': 700,\n    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.downloadermiddlewares.stats.DownloaderStats': 850,\n    'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': 900,\n    # Downloader side\n}\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2297/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2295", "title": "Add item_failed signal", "body": "Sometimes we need to do something when item pipeline failed for some reason(code bug for example), like recording the corresponding request for later retry.\n\nCurrently scrapy just print a log for a failed item: \nhttps://github.com/scrapy/scrapy/blob/master/scrapy/core/scraper.py#L232\n\nrelated issue: #2147 \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2295/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pykler": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2289", "title": "Priority value, Higher or Lower?", "body": "There is a contradiction in the documentation.\n\nhttp://doc.scrapy.org/en/latest/topics/settings.html?highlight=priority#depth-priority\n\nvs\n\nhttp://doc.scrapy.org/en/latest/topics/request-response.html?highlight=priority#scrapy.http.Request\n\nThe first says a positive number will mean lower priority. The latter says a higher number means a higher priority. So what is it?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2289/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gabsn": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2228", "title": "Duplicate log", "body": "### Dubplicate Log\n\nThere is an annoying bug when using python multiprocessing lib whith scrapy :\n\n```\nclass CompaniesCrawler(object):\n\n    def __init__(self):\n        self.crawler = CrawlerProcess(SETTINGS)\n\n    def _crawl(self, spider):\n        self.crawler.crawl(spider)\n        self.crawler.start()\n        self.crawler.stop()\n\n    def crawl(self, spider):\n        p = Process(\n            target=self._crawl,\n            args=[spider]\n        )\n        p.start()\n        p.join()\n```\n\n**Issue :** Each time we instantiate [CrawlerProcess class](https://github.com/scrapy/scrapy/blob/ebef6d7c6dd8922210db8a4a44f48fe27ee0cd16/scrapy/crawler.py), [configure_logging function](https://github.com/scrapy/scrapy/blob/ebef6d7c6dd8922210db8a4a44f48fe27ee0cd16/scrapy/utils/log.py) is called and a new handler is created : \n\n```\nhandler = _get_handler(settings)\nlogging.root.addHandler(handler)\n```\n\nSo we eventually find ourselves with a lot of duplicate log handlers and the console becomes unreadable...\n\n<img width=\"683\" alt=\"scrapy-duplicate-log\" src=\"https://cloud.githubusercontent.com/assets/18901085/18355989/5348da7c-75eb-11e6-9ac1-363749ad7334.png\">\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2228/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shahidkarimi": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2203", "title": "This issue is very strange centos 6.8", "body": "When I check version using scapy -V or run any command, I get the following error, Please help\n\n```\nTraceback (most recent call last):\n  File \"/usr/local/bin/scrapy\", line 3, in <module>\n    from scrapy.cmdline import execute\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 7, in <module>\n    from scrapy.crawler import CrawlerProcess\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/crawler.py\", line 5, in <module>\n    from scrapy.core.engine import ExecutionEngine\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/core/engine.py\", line 14, in <module>\n    from scrapy.core.downloader import Downloader\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/core/downloader/__init__.py\", line 16, in <module>\n    from .middleware import DownloaderMiddlewareManager\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/core/downloader/middleware.py\", line 7, in <module>\n    from scrapy.http import Request, Response\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/http/__init__.py\", line 11, in <module>\n    from scrapy.http.request.form import FormRequest\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/http/request/form.py\", line 9, in <module>\n    import lxml.html\n  File \"/usr/local/lib/python2.7/site-packages/lxml-3.4.4-py2.7-linux-x86_64.egg/lxml/html/__init__.py\", line 42, in <module>\n    from lxml import etree\n  File \"lxml.etree.pyx\", line 161, in init lxml.etree (src/lxml/lxml.etree.c:198847)\nTypeError: encode() argument 1 must be string without null bytes, not unicode\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2203/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Canidy": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2199", "title": "there is frequent logging error while i scrap web pages", "body": "Logged from file retry.py, line 68\nTraceback (most recent call last):\n  File \"c:\\python27\\lib\\logging__init__.py\", line 884, in emit\n    stream.write(fs % msg.encode(\"UTF-8\"))\nUnicodeDecodeError: 'utf8' codec can't decode byte 0xd3 in position 234: invalid continuation byte\nLogged from file retry.py, line 68\nTraceback (most recent call last):\n  File \"c:\\python27\\lib\\logging__init__.py\", line 884, in emit\n    stream.write(fs % msg.encode(\"UTF-8\"))\nUnicodeDecodeError: 'utf8' codec can't decode byte 0xd3 in position 234: invalid continuation byte\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2199/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Digenis": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2194", "title": "Move scrapy.utils.deprecate to separate package", "body": "Among the pypi packages with a purpose similar to\n`scrapy.utils.deprecate.create_deprecated_class`,\nscrapy's implementation stands out\nbeing the only one that warns when subclassing.\n\nI think it deserves its own package\nwhere further development can continue\nwithout inflating scrapy's codebase.\n\n@dangra, you have [a name in pypi](https://pypi.python.org/pypi/deprecated/) reserved.\nDo you have a plan to create a package?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2194/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2515", "title": "use os.stat in FilesystemCacheStorage", "body": "From the python doc: https://docs.python.org/3/library/os.path.html#os.path.exists\r\n> Return True if path refers to an existing path or an open file descriptor. Returns False for broken symbolic links. On some platforms, this function may return False if permission is not granted to execute os.stat() on the requested file, even if the path physically exists.\r\n\r\nI recently restored a backup of my cache, while omitting to preserve ownership.\r\nWhen I tried to crawl using `HTTPCACHE_IGNORE_MISSING`\r\nall the requests seemed to be missing from the cache.\r\nThe FilesystemCacheStorage uses os.path.exists,\r\nwhich internally calls os.stat,\r\nwhich, as the doc says, may return False when lacking permissions.\r\n\r\nFrom my python:\r\n```python\r\ndef exists(path):\r\n    \"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\r\n    try:\r\n        os.stat(path)\r\n    except os.error:\r\n        return False\r\n    return True\r\n```\r\n\r\nI suggest we replace os.path.exists with os.stat\r\nwhile catching only ENOENT.\r\nThis way, such mistakes will not pass silently\r\nwhen using `HTTPCACHE_IGNORE_MISSING`.\r\nWhen not using this setting, `store_response()` already fails the same way.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2505", "title": "FTP file listing", "body": "I'm trying to revive #941.\r\nI made the changes easier for review by rebasing it on master (dropped the merges),\r\nmoving some hunks from the last commit to its parent to make the diffs cleaner\r\nand creating a separate commit from the hunk that adds the doc. \r\n\r\n\r\nCc: @bernardotorres (the author) and @kmike (the reviewer)\r\nFixes both #1478 and #941", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2450", "title": "Replicate stdout and stderr to logfile", "body": "Context: https://github.com/scrapy/scrapy/issues/1964#issuecomment-216814406\r\n\r\nWhen specifying `LOG_STDOUT = True`\r\nthe stdout shouldn't be redirected to the log\r\nbut replicated. Same for stderr.\r\n\r\nI remember another issue\r\nwith exceptions in places such as spiders' `__init__` methods\r\nthat were missing their tracebacks in the log,\r\nbeing logged only on stdout/stderr.\r\nI recently discovered that it's a problem on twisted-15.5.0\r\nbut not 16.2.0.\r\n\r\nThe second commit can be ignored for now.\r\nI was just testing something if the StreamLogger can be deprecated.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1799", "title": "Customize linkextractor's _collect_string_content()", "body": "I want to customize the linkextractor's _collect_string_content().\nMy use case is that the anchor (<a>) may enclose lots of elements\n(a bad web development practice)\nand _collect_string_content() collects some gibberish from them.\n\nI have more expectations from the achor's title attribute\nthan its text content.\n\nI first thought of moving _collect_string_content() to a method\nbut I couldn't avoid having to subclass 2 classes\noverriding one private and one magic method.\nThe most straightforward way I see is making it configurable.\n##### TLDR\n\n``` html\n<a title=\"I prefer this for link.text\">\n  <style>instead of this </style>\n  <script>or this</script>\n</a>\n```\n\nSo I want to be able to tell the extractor\nto use some other xpath for textualization.\n(see tests for example)\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1626", "title": "[MRG+1] Doc improve log formating examples", "body": "When the formatting is done by the logger (`logger.info('%s', val)`)\r\nand a formatting exception occurs, there are several problems:\r\n- The exception & stacktrace is printed to stderr, not the log (#2450)\r\n- The debugger is not invoked by twisted (#1551)\r\n- Extensions don't see errors; Errors are not even counted in the stats.\r\n- It's impossible to see the full stacktrace\r\n  with frames from the caller of the logger\r\n\r\nI demonstrated some of this in #1551.\r\nThis is why I prefer early formatting when logging.\r\n\r\nCode updated like this is backward incompatible\r\nbecause exceptions will occur early, in the caller of the log\r\nstopping execution of any code that failed formatting.\r\n(This is irrelevant to deprecation policy,\r\n I only update documentation examples)\r\n\r\nEdit: I also updated another formatting example that could be simple.\r\nEdit2: Build fails because of the twisted update\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "BjBlazkowicz": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2183", "title": "S3FilesStore - Wait for upload", "body": "Hello,\n\nIs it somehow possible to wait for an s3 upload to finish before continuing in the pipeline-chain?\n\nMaybe I am far off in my design, but the issue is that my post-processor(another application) does some further processing on the files that Scrapy uploads to S3. But sometimes the uploads haven't finished before the item is processed which results in 404s in my s3 client.\n\nI tried to subclass the S3FilesStore in order to make the upload a blocking call by skipping the defereds, but this resulted in memory problems in my spiders...\n\nThe main problem is that **item_completed** in the **FilesPipeline** is called before the uploads has finished. Is there some good way for extending the FilesPipeline in order to achieve what I want? I dont really want to implement polling or something.\n\nCheers\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "candlejack297": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2182", "title": "Generalize Request/Response", "body": "It would be nice if scrapy had a slightly more general abstraction over request/responses. \n\nMy use case involves a Rest API that has a python sdk. Instead of using scrapy to make the direct http requests/responses, I'd like to make the request using the sdk, but then follow the normal scrapy process (middleware, parse, item pipeline, etc). I can and have made this happen with a few custom classes, but it still feels like there should be an easier way to hook things into the pipeline (maybe there is, and I'm just missing it). I realize it's not exactly crawling to do something like this, but we have some \"normal\" spiders as well, and it's nice to have a single entry point for all our scraping.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2182/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "spenoir": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2164", "title": "openssl el capitan", "body": "Latest el capitan update breaks openssl which means Scrapy fails to install like this:\n\n```\nbuild/temp.macosx-10.11-intel-2.7/_openssl.c:429:10: fatal error: 'openssl/opensslv.h' file not found\n\n#include <openssl/opensslv.h>\n\n         ^\n\n1 error generated.\n\nerror: command 'cc' failed with exit status 1\n```\n\nNot a Scrapy issue perhaps but worth mentioning. Also problems with homebrew: https://stackoverflow.com/questions/38670295/brew-refusing-to-link-openssl which is perhaps related. Why did they have to mess with openssl? \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2164/reactions", "total_count": 4, "+1": 4, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rustjason": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2149", "title": "logging level won't working", "body": "Using the code in the doc:\n\n```\n logging.basicConfig(\n     filename='/home/jason/scrapyd/logs/scrapy.log',\n     format='%(levelname)s: %(message)s',\n     level=logging.ERROR\n )\n```\n\nBut in the file `scrapy.log`, I can still see `INFO` `DEBUG` etc\n\n```\nERROR: this is an error\nINFO: please\nERROR: this is an error\nDEBUG: Scraped from <200 http://www.xxxx.com>\n```\n\nI did not specify any log level in my settings.py, Which part could be wrong?\nThanks\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2149/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Gwill": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2147", "title": "How to handle exception in pipeline with signal?", "body": "Just like using Extention Class connectted spider_error signal. \n\nHow to handle exception in pipeline with signal?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2147/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "darshanime": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2144", "title": "PEP8 not enforced strictly", "body": "PEP8 is [recommended in the docs](http://doc.scrapy.org/en/latest/contributing.html#coding-style) but is not strictly enforced. \n\nSome violations:\n1. Multiple imports in [one line](https://github.com/scrapy/scrapy/blob/master/scrapy/commands/edit.py#L1)\n2. Two blank lines [not maintained](https://github.com/scrapy/scrapy/blob/master/scrapy/commands/edit.py#L5) between class definitions.\n3. [Unused imports](https://github.com/scrapy/scrapy/blob/master/scrapy/commands/check.py#L3) \n4. Lines longer than 80 characters everywhere\n\nI believe we should strictly enforce PEP8 and also edit our CI configurations to test for these violations. \nI would love to submit a patch for this if the idea sounds good to everyone. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2144/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2875", "title": "add BASE_DIR to settings", "body": "add a `BASE_DIR` setting to `settings.py`. This is useful when you want to read csvs (in spiders for eg)\r\n```python\r\n        filepath = os.path.join(\r\n            settings.BASE_DIR,\r\n            '/path/to/file.csv'\r\n        )\r\n\r\n        with open(filepath, 'r', errors='ignore') as csvfile:\r\n            # code\r\n```\r\n\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1806", "title": "[WIP] Handle 201 responses in pipelines.files ", "body": "Added a check for `location` field in `201` responses. The `location` field is to have the link for the resource created due to the request.\nFixes #1615 \n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ArturGaspar": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2141", "title": "Redirected request uses same download slot when domain changes", "body": "Redirected requests keep the `request.meta['download_slot']` value set by the downloader in the original request, making subsequent requests use the same download slot even if the domain changes between redirects.\n\nThis causes redirected requests to be treated as if they were all belonging to the original domain for per-domain concurrency.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2141/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2333", "title": "[MRG+1] DNS cache expiration.", "body": "Expires entries in the DNS cache. Fixes #905.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/982", "title": "Crawlspider rule enhancements", "body": "This adds a new feature to CrawlSpider Rules.\n\n\"to\" and \"in_\" values can be set in rules. Rules with an \"in_\" value will only be applied to responses that resulted from a rule with a \"to\" value equal to the \"in_\" value.\n\nThis can be used to only apply certain rules to the results of other specific rules. For example, in documentation I have also updated to describe the new feature, the item rule is only applied in results of the category rule.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rezhajulio": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2133", "title": "Scrapy ignoring AWS S3 KEY settings", "body": "I am running single spider with custom setting per this [documentation](http://doc.scrapy.org/en/1.1/topics/settings.html#settings-per-spider). However it keep looking everywhere else. Here is part of the log:\n\n```\n2016-07-19 20:30:21 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n2016-07-19 20:30:21 [scrapy] INFO: Overridden settings: {}\n2016-07-19 20:30:21 [botocore.credentials] DEBUG: Skipping environment variable credential check because profile name was explicitly set.\n2016-07-19 20:30:21 [botocore.credentials] DEBUG: Looking for credentials via: env\n2016-07-19 20:30:21 [botocore.credentials] DEBUG: Looking for credentials via: assume-role\n2016-07-19 20:30:21 [botocore.credentials] DEBUG: Looking for credentials via: shared-credentials-file\n2016-07-19 20:30:21 [botocore.credentials] DEBUG: Looking for credentials via: config-file\n2016-07-19 20:30:21 [botocore.credentials] DEBUG: Looking for credentials via: ec2-credentials-file\n2016-07-19 20:30:21 [botocore.credentials] DEBUG: Looking for credentials via: boto-config\n2016-07-19 20:30:21 [botocore.credentials] DEBUG: Looking for credentials via: container-role\n2016-07-19 20:30:21 [botocore.credentials] DEBUG: Looking for credentials via: iam-role\n2016-07-19 20:30:21 [botocore.vendored.requests.packages.urllib3.connectionpool] INFO: Starting new HTTP connection (1): 169.254.169.254\n2016-07-19 20:30:22 [botocore.utils] DEBUG: Caught exception while trying to retrieve credentials: HTTPConnectionPool(host='169.254.169.254', port=80): Max retries exceeded with url: /latest/meta-data/iam/security-credentials/ (Caused by ConnectTimeoutError(<botocore.awsrequest.AWSHTTPConnection object at 0x7f09baf77a90>, 'Connection to 169.254.169.254 timed out. (connect timeout=1)'))\nTraceback (most recent call last):\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/utils.py\", line 159, in _get_request\n    response = requests.get(url, timeout=timeout)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/api.py\", line 69, in get\n    return request('get', url, params=params, **kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/api.py\", line 50, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/sessions.py\", line 465, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/sessions.py\", line 573, in send\n    r = adapter.send(request, **kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/adapters.py\", line 419, in send\n    raise ConnectTimeout(e, request=request)\nConnectTimeout: HTTPConnectionPool(host='169.254.169.254', port=80): Max retries exceeded with url: /latest/meta-data/iam/security-credentials/ (Caused by ConnectTimeoutError(<botocore.awsrequest.AWSHTTPConnection object at 0x7f09baf77a90>, 'Connection to 169.254.169.254 timed out. (connect timeout=1)'))\n2016-07-19 20:30:22 [botocore.utils] DEBUG: Max number of attempts exceeded (1) when attempting to retrieve data from metadata service.\n2016-07-19 20:30:22 [botocore.loaders] DEBUG: Loading JSON file: /home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/data/endpoints.json\n2016-07-19 20:30:22 [botocore.loaders] DEBUG: Loading JSON file: /home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/data/s3/2006-03-01/service-2.json\n2016-07-19 20:30:22 [botocore.loaders] DEBUG: Loading JSON file: /home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/data/_retry.json\n2016-07-19 20:30:22 [botocore.client] DEBUG: Registering retry handlers for service: s3\n2016-07-19 20:30:22 [botocore.hooks] DEBUG: Event creating-client-class.s3: calling handler <function add_generate_presigned_post at 0x7f09baf92320>\n2016-07-19 20:30:22 [botocore.hooks] DEBUG: Event creating-client-class.s3: calling handler <function add_generate_presigned_url at 0x7f09baf8fa28>\n2016-07-19 20:30:22 [botocore.client] DEBUG: The s3 config key is not a dictionary type, ignoring its value of: None\n2016-07-19 20:30:22 [botocore.endpoint] DEBUG: Setting s3 timeout as (60, 60)\n2016-07-19 20:30:22 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.feedexport.FeedExporter',\n 'scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2016-07-19 20:30:22 [py.warnings] WARNING: /home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead\n  ScrapyDeprecationWarning)\n\n2016-07-19 20:30:22 [py.warnings] WARNING: /home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/scraper-0.0.1-py2.7.egg/scraper/middlewares.py:2: ScrapyDeprecationWarning: Module `scrapy.contrib.downloadermiddleware.useragent` is deprecated, use `scrapy.downloadermiddlewares.useragent` instead\n  from scrapy.contrib.downloadermiddleware.useragent import UserAgentMiddleware\n\n2016-07-19 20:30:22 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scraper.middlewares.RotateUserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-07-19 20:30:22 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-07-19 20:30:22 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-07-19 20:30:22 [scrapy] INFO: Spider opened\n2016-07-19 20:30:22 [botocore.credentials] DEBUG: Skipping environment variable credential check because profile name was explicitly set.\n2016-07-19 20:30:22 [botocore.credentials] DEBUG: Looking for credentials via: env\n2016-07-19 20:30:22 [botocore.credentials] DEBUG: Looking for credentials via: assume-role\n2016-07-19 20:30:22 [botocore.credentials] DEBUG: Looking for credentials via: shared-credentials-file\n2016-07-19 20:30:22 [botocore.credentials] DEBUG: Looking for credentials via: config-file\n2016-07-19 20:30:22 [botocore.credentials] DEBUG: Looking for credentials via: ec2-credentials-file\n2016-07-19 20:30:22 [botocore.credentials] DEBUG: Looking for credentials via: boto-config\n2016-07-19 20:30:22 [botocore.credentials] DEBUG: Looking for credentials via: container-role\n2016-07-19 20:30:22 [botocore.credentials] DEBUG: Looking for credentials via: iam-role\n2016-07-19 20:30:22 [botocore.vendored.requests.packages.urllib3.connectionpool] INFO: Starting new HTTP connection (1): 169.254.169.254\n2016-07-19 20:30:23 [botocore.utils] DEBUG: Caught exception while trying to retrieve credentials: HTTPConnectionPool(host='169.254.169.254', port=80): Max retries exceeded with url: /latest/meta-data/iam/security-credentials/ (Caused by ConnectTimeoutError(<botocore.awsrequest.AWSHTTPConnection object at 0x7f09ba27f050>, 'Connection to 169.254.169.254 timed out. (connect timeout=1)'))\nTraceback (most recent call last):\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/utils.py\", line 159, in _get_request\n    response = requests.get(url, timeout=timeout)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/api.py\", line 69, in get\n    return request('get', url, params=params, **kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/api.py\", line 50, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/sessions.py\", line 465, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/sessions.py\", line 573, in send\n    r = adapter.send(request, **kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/adapters.py\", line 419, in send\n    raise ConnectTimeout(e, request=request)\nConnectTimeout: HTTPConnectionPool(host='169.254.169.254', port=80): Max retries exceeded with url: /latest/meta-data/iam/security-credentials/ (Caused by ConnectTimeoutError(<botocore.awsrequest.AWSHTTPConnection object at 0x7f09ba27f050>, 'Connection to 169.254.169.254 timed out. (connect timeout=1)'))\n2016-07-19 20:30:23 [botocore.utils] DEBUG: Max number of attempts exceeded (1) when attempting to retrieve data from metadata service.\n2016-07-19 20:30:23 [botocore.loaders] DEBUG: Loading JSON file: /home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/data/endpoints.json\n2016-07-19 20:30:23 [botocore.loaders] DEBUG: Loading JSON file: /home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/data/s3/2006-03-01/service-2.json\n2016-07-19 20:30:23 [botocore.loaders] DEBUG: Loading JSON file: /home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/data/_retry.json\n2016-07-19 20:30:23 [botocore.client] DEBUG: Registering retry handlers for service: s3\n2016-07-19 20:30:23 [botocore.hooks] DEBUG: Event creating-client-class.s3: calling handler <function add_generate_presigned_post at 0x7f09baf92320>\n2016-07-19 20:30:23 [botocore.hooks] DEBUG: Event creating-client-class.s3: calling handler <function add_generate_presigned_url at 0x7f09baf8fa28>\n2016-07-19 20:30:23 [botocore.client] DEBUG: The s3 config key is not a dictionary type, ignoring its value of: None\n2016-07-19 20:30:23 [botocore.endpoint] DEBUG: Setting s3 timeout as (60, 60)\n\n\n==== log is too long\n\n\nTraceback (most recent call last):\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/twisted/python/threadpool.py\", line 246, in inContext\n    result = inContext.theWork()\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/twisted/python/threadpool.py\", line 262, in <lambda>\n    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/twisted/python/context.py\", line 118, in callWithContext\n    return self.currentContext().callWithContext(ctx, func, *args, **kw)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/twisted/python/context.py\", line 81, in callWithContext\n    return func(*args,**kw)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/scrapy/extensions/feedexport.py\", line 118, in _store_in_thread\n    Bucket=self.bucketname, Key=self.keyname, Body=file)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/client.py\", line 278, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/client.py\", line 561, in _make_api_call\n    operation_model, request_dict)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/endpoint.py\", line 117, in make_request\n    return self._send_request(request_dict, operation_model)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/endpoint.py\", line 142, in _send_request\n    request = self.create_request(request_dict, operation_model)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/endpoint.py\", line 126, in create_request\n    operation_name=operation_model.name)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/hooks.py\", line 227, in emit\n    return self._emit(event_name, kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/hooks.py\", line 210, in _emit\n    response = handler(**kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/signers.py\", line 90, in handler\n    return self.sign(operation_name, request)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/signers.py\", line 147, in sign\n    auth.add_auth(request)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/auth.py\", line 665, in add_auth\n    raise NoCredentialsError\nNoCredentialsError: Unable to locate credentials\n2016-07-19 20:30:43 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 55517,\n 'downloader/request_count': 72,\n 'downloader/request_method_count/GET': 72,\n 'downloader/response_bytes': 2529033,\n 'downloader/response_count': 72,\n 'downloader/response_status_count/200': 72,\n 'dupefilter/filtered': 20,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 7, 19, 13, 30, 43, 867304),\n 'item_scraped_count': 1196,\n 'log_count/DEBUG': 1322,\n 'log_count/ERROR': 1,\n 'log_count/INFO': 9,\n 'log_count/WARNING': 2,\n 'request_depth_max': 20,\n 'response_received_count': 72,\n 'scheduler/dequeued': 72,\n 'scheduler/dequeued/memory': 72,\n 'scheduler/enqueued': 72,\n 'scheduler/enqueued/memory': 72,\n 'start_time': datetime.datetime(2016, 7, 19, 13, 30, 23, 900947)}\n2016-07-19 20:30:43 [scrapy] INFO: Spider closed (finished)\n\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2133/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "FurryFur": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2124", "title": "Cookies not set when dont_merge_cookies is True", "body": "This example is [straight from the documentation](http://doc.scrapy.org/en/latest/topics/request-response.html#request-objects), and does not work because if dont_merge_cookies is set on the request, then the cookie middleware skips all cookie processing and no cookies are ever set.\n\n``` python\nRequest(url=\"http://www.example.com\",\n        cookies={'currency': 'USD', 'country': 'UY'},\n        meta={'dont_merge_cookies': True})\n```\n\nDoes not work, however, this does:\n\n``` python\nRequest(url=\"http://www.example.com\",\n        cookies={'currency': 'USD', 'country': 'UY'})\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2124/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sergei-sh": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2110", "title": "Files download fails with scrapy media_pipeline", "body": "When trying to download a number of large files with one instance of MediaPipeline, it stops downloading at some moment.\n\nhttp://stackoverflow.com/questions/38270780/large-files-download-fails-with-scrapy-media-pipeline\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2110/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2104", "title": "download_timeout with media_pipeline doesn't work", "body": "``` py\nrequest = scrapy.Request(\n                    url=video_url,\n                    method=\"GET\",\n                    headers={\n                        \"Accept\" : \"*/*\",\n                        \"User-Agent\" : \"Mozilla\",\n                    },\n                    meta={\"download_timeout\":600, \"item\":item, \"video_url\":video_url},\n                    dont_filter=True,\n                )\n                yield request`\n```\n\nThis is in MediaPipeline, subclassed, get_media_requests() method.\nThe timeout from meta has no effect. The timeout from settings.py is actually used.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2104/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/1977", "title": "scrapy logging with python logging module", "body": "I need to configure several custom loggers of the following type:\n-writing both to a file and console\n-writing to console only\n-writing to file only\nI use logging.fileConfig() which works fine without scrapy. I set rootLogger to write to /dev/null, so that I can setup every child logger individually.\n\nI've tried the following options:\n-Running a spider with CrawlerProcess()\n-Running a spider with CrawlerRunner() with:\n  a) configure_logging(install_root_handler=True)  \n  b) configure_logging(install_root_handler=False)\n  c) configure_logging(install_root_handler=False) and manually calling logging.getLogger(\"scrapy\").addHandler(...)\n\nI neither case it worked meaning all loggers write to intended location ONLY, AND I see all scrapy messages.\nThe last case c) was the closest to be correct, by I could see only a PART of scrapy messages (not depending on logging level).\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/1977/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rampage644": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2089", "title": "`FeedExporter` extension should throw when misconfigured", "body": "`FeedExporter` extension should either throw `NotConfigured` when spider lacks some properties it should have according to `FEED_URI`. Right now in that case `open_spider` initialize fails causing all subsequent signal handlers to fail as well.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2089/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nyov": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2087", "title": "Date/Time handling in scrapy", "body": "Scrapy uses UTC \"timestamps\" (datetime objects) internally. This is good.\n\n**1st**\n\nI would like to propose using [`RFC3339`](https://tools.ietf.org/html/rfc3339) (subset of [`ISO 8601`](https://en.wikipedia.org/wiki/ISO_8601), or `ISO 8601` (where [time intervals](https://en.wikipedia.org/wiki/ISO_8601#Time_intervals) might be represented) time formats just as universally, where timestamps are represented as string-serialized form.\nThis would allow for easier conversion back to usable date objects (e.g. with `iso8601.parse_date()`, `dateutil.parser.parse()` or others).\n\nNotably in `ScrapyJSONEncoder`, a simple (but backwards-incompatible) change would be:\n\n``` patch\ndiff --git a/scrapy/utils/serialize.py b/scrapy/utils/serialize.py\nindex 8320be0..d11d167 100644\n--- a/scrapy/utils/serialize.py\n+++ b/scrapy/utils/serialize.py\n@@ -11,11 +11,11 @@ from scrapy.item import BaseItem\n class ScrapyJSONEncoder(json.JSONEncoder):\n\n     DATE_FORMAT = \"%Y-%m-%d\"\n-    TIME_FORMAT = \"%H:%M:%S\"\n+    TIME_FORMAT = \"%H:%M:%SZ\"\n\n     def default(self, o):\n         if isinstance(o, datetime.datetime):\n-            return o.strftime(\"%s %s\" % (self.DATE_FORMAT, self.TIME_FORMAT))\n+            return o.strftime(\"%sT%s\" % (self.DATE_FORMAT, self.TIME_FORMAT))\n         elif isinstance(o, datetime.date):\n             return o.strftime(self.DATE_FORMAT)\n         elif isinstance(o, datetime.time):\n```\n\nThis would invalidate the use of the `DATE_FORMAT` and `TIME_FORMAT` variables, but with these formats being international standard the need for a customizable format is questionable, so a better change should drop these entirely and use `datetime.datetime.isoformat()`.\n\n**2nd**\n\nI'd like to discuss pro/contra of using \"offset-aware\"/timezone-aware `datetime` objects internally.\nMy belief is that UNIX timestamps/UTC datetimes as internal representations are enough, and localized date-conversions should be kept to frontend code. No change necessary.\n\nHowever, `datetime` objects seem to have the same memory footprint with or without timezone info (if that data is correct):\n\n``` py\n>>> import sys, time, datetime, pytz\n>>> utime = time.time()\n>>> dtime = datetime.datetime.utcfromtimestamp(utime)\n>>> ztime = datetime.datetime.fromtimestamp(utime, pytz.UTC)\n>>> sys.getsizeof(utime)\n24\n>>> sys.getsizeof(dtime)\n48\n>>> sys.getsizeof(ztime)\n48\n>>> utime\n1467642195.921702\n>>> print (\"%.20f\" % utime)\n1467642195.92170190811157226562\n>>> dtime.isoformat()\n'2016-07-04T14:23:15.921702'\n>>> ztime.isoformat()\n'2016-07-04T14:23:15.921702+00:00\n>>> dtime == ztime\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: can't compare offset-naive and offset-aware datetimes\n```\n\nIn the name of interoperability, adding UTC \"timezone\" info might be positive.\n(See PR #1056)\n\nAdding a `pytz` dependency, when there is no real need for timezone support, feels ugly.\nSince Python 3.2, UTC tzinfo was added to `datetime`, and here is a possible backport for older Python versions.\n\n``` patch\ndiff --git a/scrapy/utils/python.py b/scrapy/utils/python.py\nindex 42fbbda..8731d1b 100644\n--- a/scrapy/utils/python.py\n+++ b/scrapy/utils/python.py\n@@ -344,3 +344,34 @@ def without_none_values(iterable):\n         return {k: v for k, v in six.iteritems(iterable) if v is not None}\n     except AttributeError:\n         return type(iterable)((v for v in iterable if v is not None))\n+\n+# Backported `datetime.timezone.utc` (Python 3.2+)\n+# for \"timezone-aware\" datetimes (convoluted way to get unix timestamps)\n+# This implements a simple UTC \"timezone\" for datetime objects,\n+# without requiring the whole `pytz` shebang.\n+try:\n+    from datetime import timezone\n+    utc = timezone.utc\n+except ImportError:\n+    from datetime import timedelta, tzinfo\n+\n+    class timezone(tzinfo): pass\n+    class UTC(timezone):\n+        _zero = timedelta(0)\n+        def utcoffset(self, dt):\n+            return self._zero\n+        def tzname(self, dt):\n+            return \"UTC\"\n+        def dst(self, dt):\n+            return self._zero\n+        def __repr__(self):\n+            return 'datetime.timezone.utc'\n+        def __str__(self):\n+            return 'UTC+00:00'\n+    utc = UTC()\n+\n+    # optional monkey-patching datetime\n+    import datetime\n+    datetime.timezone = timezone\n+    datetime.timezone.utc = utc\n+    del datetime\n```\n\nCurrent `datetime.datetime.utcnow()` would change to `datetime.datetime.now(datetime.timezone.utc)` (Py3.2+), or something like this:\n\n``` py\nfrom datetime import datetime\ntry:\n    from datetime import timezone\nexcept ImportError:\n    from scrapy.utils.python import timezone\n\nprint (datetime.now(timezone.utc))\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2087/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2195", "title": "Allow updating dictionary-settings", "body": "This is another proposal for updating settings, without string prefixes. (See #1591 for the other)\n\nIt fails the mock tests\n`BaseSettingsTest.test_setdict_alias` and\n`BaseSettingsTest.test_setmodule_alias`\nand probably has other issues at this time.\n\nThe idea is that merging two dictionaries (Settings namespace + an 'UPDATE' dict, that shadows the Settings namespace) is more convenient and efficient than string-parsing each setting key for a potential \"update:\" prefix.\nIt should also be more versatile (allows `setmodule` with an update flag).\n\nThe implementation can perhaps still be improved.\nHaving an update flag on an update method is also confusing. But both are different \"updates\".\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1451", "title": "Allow Request/Response conversion to dict", "body": "(from #1450 )\nJust the basic dict interface here, so that this works:\n\n``` py\nr = dict(response)\nr.update({'request': dict(response.request)})\nprint(r['request']['headers']['User-Agent'])\n```\n\nIs this worth considering?\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1443", "title": "Add a shortcut method `idled()` for `spider_idle` signal.", "body": "Another take on #740\n\n@dangra,\nI require some clarification here: Is a `Crawler`<>`Spider` relationship always 1:1 nowadays? Or does a Crawler object ever handle multiple Spiders, in parallel?\n\n`crawler.signals.connect(self.idle, signals.spider_idle)` inside the `Spider` class connects the Spider/s to the SignalManager for the Crawler instance.\nSo we shouldn't get any signals from any other Crawler instance, correct?\nNow in the past, there has been code checking in a spider instance like this:`def idle(self): if spider != self: return`.\n\nIf I am correct, a Crawler will always only have one Spider, so `spider` will always only be `self` here now, right? So this can be a staticmethod.\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1349", "title": "(idea) Convert Spiders to Spider behaviours/logic (mixins)", "body": "The idea here would be to propagate a coding practice of defining\ncustom spiders from a collection of re-useable behaviours.\n\nIMO this could help shift to a coding mindset of building/releasing more logic parts of a Spider as re-useable Mixins.\n\nSome of the current existing spiders could already be combined, such as the `CrawlSpider` could be fed by the `SitemapSpider`, the `InitSpider` could be combined with pretty much any other.\n\nThat could look like\n`MySpider(InitMixin, SitemapMixin, CrawlMixin, Spider)` or\n`MySpider(RedisURLFeederLogic, PageLoginLogic, CrawlLogic, Spider)`...\n\nBut the approach needs consideration on how to fix the current methods,\nso that mixins don't mess with each other (`start_requests` overriding each other and such).\n\nI'd be interested in discussion whether this change would be good or bad,\nand ideas or help on how to do it.\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1117", "title": "make unittests IPv6-dualstack proof", "body": "Fixed 'localhost' to an IPv4 loopback address that is unlikely to be in use; making sure it can't resolve to [::1], as we have no one listening there.\n\nThis also ensures DNS-unrelated tests are not failing because of a flaky resolver.\n\nUnsetting all environment-set proxies at beginning of tests, as urllib doesn't handle $no_proxy correctly, the unittest webserver is always local, and we usually don't want to intercept our test runs either.\n## \n\nThis finally makes tests run all green for me :)\nAnd it preempts the case when Scrapy eventually gets an IPv6-capable resolver.\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1104", "title": "IPv6ThreadedResolver based on socket.getaddrinfo", "body": "For #1031\n\nI do not currently have the chance to test this in a v6-only environment.\nFeedback appreciated.\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/732", "title": "change Scraper API to call internal `_parse` method (fix #781)", "body": "As a fix to the CrawlSpider-can't-have-parse useability issue (#712 / now #781) ,\nI propose having another internal `_parse` method called by the `Scraper` instead of `parse`, and using that in spiders who want internal \"pre-processing\", then exposing `parse` as the public, documented, no-baggage method to implement/override from a spider.\n\n`Spider` simply calls the public `parse` method from `_parse`, while `CrawlSpider` and cohorts can do their thing in `_parse` and  give the cold shoulder to users trying to break them by re-defining `parse` without calling `super()`.\n\nSince all spiders presently should subclass from `Spider`, this also doesnt't break any existing spider classes who keep using `parse` exclusively.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pawneetdev": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2086", "title": "Scrapy logs [partial] when proxy is used", "body": "Scrapy logs [partial] when using proxy to scrape and does not return any data. I am using proxymesh.\n\n>  middleware.py\n\n```\nimport base64\n\nclass ProxyMiddleware(object):\n    process_request(self, request, spider):\n    request.meta['proxy'] = \"http://......\"\n\n    proxy_user_pass = \"username:password\"\n\n    encoded_user_pass = base64.encodestring(proxy_user_pass)\n    request.headers['Proxy-Authorization'] = 'Basic ' + encoded_user_pass\n```\n\n> settings.py\n\n```\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 1,\n    'mcaScrapy.middlewares.ProxyMiddleware': 100,\n}\n```\n\n> Output\n\n```\n2016-05-02 11:30:06 [scrapy] DEBUG: Crawled (200) <GET http://example.com/....> (referer: http://example.com/....) ['partial']\n2016-05-02 11:30:06 [scrapy] DEBUG: Crawled (200) <POST http://example.com/....> (referer: http://example.com/....) ['partial']\n2016-05-02 11:30:06 [scrapy] DEBUG: Crawled (200) <POST http://example.com/....> (referer: http://example.com/....) ['partial']\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2086/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wearpants": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2083", "title": "S3FilesStore should not disable SSL", "body": "Per [comment in source](https://github.com/scrapy/scrapy/blame/master/scrapy/pipelines/files.py#L121), the S3 storage backend for media pipelines silently strips SSL from the connection to S3, ostensibly because of this [python core bug](http://bugs.python.org/issue5103), which oddly enough was fixed 3 years before the code in scrapy was written.\n\nThis issue has been resolved in all modern versions of Python (2.7+/3.0+) for quite some time. Can this workaround be removed?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2083/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alicenara": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2074", "title": "Add tar.gz to ignored extensions in linkextractor ", "body": "I don't know if this goes here e_e but I've had problems when trying to parse a tar.gz as an html (now I check the extension) and I want to propose to include this type of file as an ignored one in scrapy/scrapy/linkextractors/**init**.py\n(btw I have modified this file and I've added \"tar.gz\" and \"gz\" because I had the feeling \"tar.gz\" didn't work) \n\nThanks :D\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2074/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mobcdi": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2071", "title": "In Scrapy 1.1.0 --output as xml appending to existing file when spider re-executed resulting in invalid xml file with duplicate declaration", "body": "## Issue\n\nScrapy is smart enough not to create duplicate file store copies when spider is re-run but appends output to xml file if `--output myxmlfile.xml` specified in command creating invalid xml file with 2nd declaration statement for each spider run after the xml file is created\n## Possible Solution:\n\nIf xml file exists overwrite the contents not append to existing file or add option to allow user specify overwrite or append. (While the logfile is appended when a spider is re-run its structure isn't important to valid interpretation by 3rd party tools as is the case with xml files.\n## To reproduce:\n\nRun a spider via the command prompt and include option `--output myxmlfile.xml` and log the output to a file `--logfile mylogfile` when spider completes validate the xml using http://codebeautify.org/xmlvalidate and notice the xml is valid.\nIf you have set-up your spider to use the files pipeline and store a copy of every page indexed you can count  the number of files created in your FILES_STORE and note for later.\n\nRe-run the spider command and notice the xml file increases in size, re-validate the xml and you should receive a similar validation message. \n\n> Error on->  Line :2 Column :155535 (or how ever many characters your line is before it encounters the 2nd declaration statement)\n> Message :XML declaration allowed only at the start of the document\n> Error on->  Line :3 Column :1\n> Message :Extra content at the end of the document \n\nCheck the FILES_STORE and note that you don't have any increase in number of files stored on disk. Check the log file for references `File (uptodate):` and note the crawler is smart enough to know the page was already created on disk so didn't create a duplicate but when exporting the items to xml it appends the duplicates to the existing file.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2071/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2024", "title": "Increase number of full spider examples in documents", "body": "I believe for new users it would be useful if you provided more full examples of spiders in the documentation e.g. working crawler. The documentation is detailed but with the various files that need to be edited or created to create a working spider it can be difficult for new users to get up to speed. \n\nThese reference spiders could also be used to guide users in solving their problem e.g use the basic crawler as per document .... and add a rule to ....\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2024/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "fpghost": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2066", "title": "Enable Azure Storages for files/images pipeline", "body": "Allow the user to use Azure Storage for files or images persistence when using the files or images pipelines, in an analogous way to AWS S3 Storage. \n\nI issued a PR that achieves this: #2064.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2066/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2064", "title": "Add Azure Storage to files/images pipeline options", "body": "These changes allow the user to use Azure Storage for files or images persistence of media when using the files or images pipelines, in an analogous way to AWS S3 Storage. The user simply needs to define the following settings:\n\n```\nIMAGES_STORE = \"azure://<container_name>/\"\nAZURE_ACCOUNT_NAME = <azure_account_name>\nAZURE_SECRET_ACCESS_KEY = <azure_secret_key>\n```\n\nand instead of `boto` install `azure-storage`: `pip install azure-storage`, and all should work out of the box.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "DrJackilD": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2046", "title": "Allow to create spider with custom templates", "body": "I don't know, am I wrong, but is it possible now to create custom templates and create spiders with `scrapy genspider -t <custom_template> <name> <domain>`? As I see in source code, user can set custom template folder in TEMPLATE_FOLDER variable, but is there any documentation about template language, which using in template? \n\nActually, I've planned to work on PR with possibilities to create custom templates, but probably it's already implemented. May be just add documentation about this feature?\n\nUPD. Ah, I see, that `_genspider` using `render_templatefile`, which just using `string.Template().substitute()` to render template and save .py file\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2046/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ibowen": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2028", "title": "Will enabling ajaxcrawl miss some web contents?", "body": "Hello Scrapy, when I enabled ajaxcrawl in setting.py, some contents on non-ajax pages were lost. What could it be the problem? Is it possible to switch the ajaxcrawl in spider? \n## My code:\n\nhttps://github.com/ibowen/crowdfunding/blob/master/seedinvest_crawl/seedinvest_crawl/spiders/seedinvest.py\n## Scrapy View of enabling ajaxcrawl:\n\n![enabling](https://cloud.githubusercontent.com/assets/12148218/15803205/10698864-2a87-11e6-904e-271eebd71ab8.jpg)\n## Scrapy View of unenabling ajaxcrawl:\n\n![unenabling](https://cloud.githubusercontent.com/assets/12148218/15803206/106aabc2-2a87-11e6-80df-04d1fbb9b21f.jpg)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2028/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "liusangel": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2018", "title": "The setting ITEM_PIPELINE can't be overridden from terminal", "body": "In my settings.py I have:\n\n```\nITEM_PIPELINES = {\n    'turing.pipelines.InitFieldsNotInitializedPipeline': 299,\n    'turing.pipelines.SetNoneIfFieldEmptyPipeline': 300,\n    'turing.pipelines.CheckCategoryPipeline': 301,\n    'turing.pipelines.CheckContactPipeline': 302,\n}\n```\n\nAnd it works great. But sometime I want run the spider without ANY pipelines.\nWhen I run\n\n```\nscrapy crawl -s FEED_URI=stdout: -s FEED_FORMAT=json -s ITEM_PIPELINES=[] -a test_extract_url=http://example.com/ example_spider\n```\n\nI get this error:\n    return d.iteritems(**kw)\nexceptions.AttributeError: 'str' object has no attribute 'iteritems'\n\nHow can I run the spider without the pipelines?\n\nSo far I tried:\n- scrapy crawl -s FEED_URI=stdout: -s FEED_FORMAT=json -s ITEM_PIPELINES=[] -a test_extract_url=http://example.com/ example_spider\n- scrapy crawl -s FEED_URI=stdout: -s FEED_FORMAT=json -s ITEM_PIPELINES={} -a test_extract_url=http://example.com/ example_spider\n- scrapy crawl -s FEED_URI=stdout: -s FEED_FORMAT=json -s \"ITEM_PIPELINES=[]\" -a test_extract_url=http://example.com/ example_spider\n- scrapy crawl -s FEED_URI=stdout: -s FEED_FORMAT=json -s \"ITEM_PIPELINES={}\" -a test_extract_url=http://example.com/ example_spider\n- scrapy crawl -s FEED_URI=stdout: -s FEED_FORMAT=json -s ITEM_PIPELINES=['turing.pipelines.InitFieldsNotInitializedPipeline': 299,] -a test_extract_url=http://example.com/ example_spider\n- scrapy crawl -s FEED_URI=stdout: -s FEED_FORMAT=json -s ITEM_PIPELINES={'turing.pipelines.InitFieldsNotInitializedPipeline': 299,} -a test_extract_url=http://example.com/ example_spider\n- Others combinations\n- Look in the docs http://doc.scrapy.org/en/latest/topics/settings.html\n\nHopefully you can help me. Thanks.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2018/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gdomod": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/2016", "title": "Request from Pipeline", "body": "Hi there,\n\ni want to send the item data to another webservice. how can i use scrapy.Request to send the item to other website\n\n```\nreq = scrapy.Request(\n                    url, method='POST',\n                    body=json.dumps(dict(item)),\n                    headers={'content-type': 'application/json'},\n                    priority=1000,\n                    callback=self.callback)\n```\n\nthe request is correct but scrapy.Request didnt fire the request\nwith yield req, or return req\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/2016/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "RussBaz": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/1998", "title": "Cannot import '_win32stdio' (but pywin32 is already installed)", "body": "I am using Python 3.5.1 (64 bit), Windows 10, VS 2015 Update 2. lxml (3.6.0) and pywin32 (220.1) are installed. Scrapy (1.1.0) was installed successfully. Then, when I run an example from 'http://doc.scrapy.org/en/latest/intro/overview.html' in my virtual environment, I get the following exception:\n\n```\n(env) D:\\Projects\\tscrapy> scrapy runspider stackoverflow_spider.py -o top-stackoverflow-questions.json\n2016-05-19 17:36:00 [scrapy] INFO: Scrapy 1.1.0 started (bot: scrapybot)\n2016-05-19 17:36:00 [scrapy] INFO: Overridden settings: {'FEED_URI': 'top-stackoverflow-questions.json', 'FEED_FORMAT': 'json'}\n2016-05-19 17:36:00 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.feedexport.FeedExporter']\nUnhandled error in Deferred:\n2016-05-19 17:36:00 [twisted] CRITICAL: Unhandled error in Deferred:\n\n\nTraceback (most recent call last):\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\commands\\runspider.py\", line 87, in run\n    self.crawler_process.crawl(spidercls, **opts.spargs)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\crawler.py\", line 163, in crawl\n    return self._crawl(crawler, *args, **kwargs)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\crawler.py\", line 167, in _crawl\n    d = crawler.crawl(*args, **kwargs)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1274, in unwindGenerator\n    return _inlineCallbacks(None, gen, Deferred())\n--- <exception caught here> ---\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1128, in _inlineCallbacks\n    result = g.send(result)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\crawler.py\", line 72, in crawl\n    self.engine = self._create_engine()\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\crawler.py\", line 97, in _create_engine\n    return ExecutionEngine(self, lambda _: self.stop())\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\core\\engine.py\", line 68, in __init__\n    self.downloader = downloader_cls(crawler)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\core\\downloader\\__init__.py\", line 88, in __init__\n    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\middleware.py\", line 58, in from_crawler\n    return cls.from_settings(crawler.settings, crawler)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\middleware.py\", line 34, in from_settings\n    mwcls = load_object(clspath)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\utils\\misc.py\", line 44, in load_object\n    mod = import_module(module)\n  File \"C:\\Program Files\\Python 3.5\\lib\\importlib\\__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\n\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\n\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\n\n  File \"<frozen importlib._bootstrap>\", line 673, in _load_unlocked\n\n  File \"<frozen importlib._bootstrap_external>\", line 662, in exec_module\n\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\n\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\downloadermiddlewares\\retry.py\", line 23, in <module>\n    from scrapy.xlib.tx import ResponseFailed\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\xlib\\tx\\__init__.py\", line 3, in <module>\n    from twisted.web import client\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\twisted\\web\\client.py\", line 41, in <module>\n    from twisted.internet.endpoints import TCP4ClientEndpoint, SSL4ClientEndpoint\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\twisted\\internet\\endpoints.py\", line 34, in <module>\n    from twisted.internet.stdio import StandardIO, PipeAddress\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\twisted\\internet\\stdio.py\", line 30, in <module>\n    from twisted.internet import _win32stdio\nbuiltins.ImportError: cannot import name '_win32stdio'\n2016-05-19 17:36:00 [twisted] CRITICAL:\nTraceback (most recent call last):\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1128, in _inlineCallbacks\n    result = g.send(result)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\crawler.py\", line 72, in crawl\n    self.engine = self._create_engine()\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\crawler.py\", line 97, in _create_engine\n    return ExecutionEngine(self, lambda _: self.stop())\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\core\\engine.py\", line 68, in __init__\n    self.downloader = downloader_cls(crawler)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\core\\downloader\\__init__.py\", line 88, in __init__\n    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\middleware.py\", line 58, in from_crawler\n    return cls.from_settings(crawler.settings, crawler)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\middleware.py\", line 34, in from_settings\n    mwcls = load_object(clspath)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\utils\\misc.py\", line 44, in load_object\n    mod = import_module(module)\n  File \"C:\\Program Files\\Python 3.5\\lib\\importlib\\__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 673, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 662, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\downloadermiddlewares\\retry.py\", line 23, in <module>\n    from scrapy.xlib.tx import ResponseFailed\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\xlib\\tx\\__init__.py\", line 3, in <module>\n    from twisted.web import client\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\twisted\\web\\client.py\", line 41, in <module>\n    from twisted.internet.endpoints import TCP4ClientEndpoint, SSL4ClientEndpoint\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\twisted\\internet\\endpoints.py\", line 34, in <module>\n    from twisted.internet.stdio import StandardIO, PipeAddress\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\twisted\\internet\\stdio.py\", line 30, in <module>\n    from twisted.internet import _win32stdio\nImportError: cannot import name '_win32stdio'\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/1998/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mborho": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/1988", "title": "\"Content-Encoding\" header gets stripped from response headers", "body": "See https://github.com/scrapy/scrapy/blob/master/scrapy/downloadermiddlewares/httpcompression.py#L36\n\nIMHO the \"Content-Encoding\" header should get preserved, since the spider probably wants to see all the original response headers.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/1988/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "BruceDone": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/1981", "title": "How to catch the log message from the extensions ?", "body": "Hi all:\n   since the scrapy log can be outputed as file or output to the shell screen , i want to use the extentions to collect all the log message content(not just the count) .I refer the doc and write this code .\n\n```\nimport logging\nfrom scrapy import signals\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.extensions.logstats import LogStats\nfrom scrapy.extensions.spiderstate import SpiderState\nfrom scrapy.extensions.corestats import CoreStats\nfrom scrapy.extensions.closespider import CloseSpider\nimport scrapy.utils.log\nimport scrapyd.runner\nfrom scrapy import log as log_level\nfrom scrapy.utils import signal as common_signal\n\nfrom scrapy.log import level_names\nfrom twisted.python import log as txlog\n\n\nfrom twisted.python import log as twisted_log\n\nlogger = logging.getLogger(__name__)\n\nclass SpiderOpenCloseLogging(object):\n\n    def __init__(self, item_count,stats):\n        self.item_count = item_count\n        self.items_scraped = 0\n        self.stats = stats\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # first check if the extension should be enabled and raise\n        # NotConfigured otherwise\n        if not crawler.settings.getbool('MYEXT_ENABLED'):\n            raise NotConfigured\n\n        # get the number of items from settings\n        item_count = crawler.settings.getint('MYEXT_ITEMCOUNT', 1000)\n\n        # instantiate the extension object\n        ext = cls(item_count,crawler.stats)\n\n        # connect the extension object to signals\n        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)\n        crawler.signals.connect(ext.spider_error, signal=signals.spider_error)\n        #crawler.signals.connect(ext.spider_error,signal=common_signal.send_catch_log)\n\n        # return the extension object\n        return ext\n\n    def spider_opened(self, spider):\n        logger.info(\"opened spider %s\", spider.name)\n        logger.info('the start time is %s ' % self.stats)\n\n    def spider_closed(self, spider):\n        logger.info(\"closed spider %s\", spider.name)\n        logger.info('the total count of of info is %s ' % self.stats)\n\n    def item_scraped(self, item, spider):\n        self.items_scraped += 1\n        if self.items_scraped % self.item_count == 0:\n            logger.info(\"scraped %d items\", self.items_scraped)\n\n    def spider_error(self,failure, response, spider):\n        logger.info('the error comes from the %s',spider.name)\n        logger.info('the message is %s',failure.value.message)\n```\n\nafter i try this ,it just collected the log count ,not the content .all the signals i try can not collect the log message content.\n\nthere may be two ways to do \n1.edit the source code \n2.just define a log function by myself and write to database.\nbut these two ways are not best practices\n\nCan you give me some suggestions ,really appreciate for your help:)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/1981/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mikkogozalo": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/1967", "title": "Issue with using bindaddress", "body": "It seems that bindaddress isn't working. Snippet of the code where I test it is here:\nhttps://gist.github.com/mikkogozalo/e44975444307a37078d43f8bc88e2489\n\nDebug output is here:\nhttps://gist.github.com/mikkogozalo/739d17964e2c133c80191bae22ef502e\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/1967/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ksimple": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/1964", "title": "twisted error stack trace doesn't output when the spider is running in scrapyd", "body": "The outputted log is something like the following text and no stack trace which makes diagnostic super hard.\n2016-05-02 20:25:02 [twisted] CRITICAL: Unhandled error in Deferred:\n2016-05-02 20:25:02 [twisted] CRITICAL:\n\nI created a from_crawler method on spider and it runs successfully on local machine but failed on scrapyd. Finally I notice that the signature of from_crawler on scrapyd should be from_crawler(cls, crawler, **kwargs) since there is a _job parameter. This is the first hard thing to discover since the reference doesn't mention it.\n\nBut the diagnostic of this one is toooooooooooo hard since there is no stack trace in twisted log. I have to write my own log observer and use addObeserve to log it by myself and find the first issue.\n\nSo please check why the stack trace doesn't output to log. I downloaded the code but can't figure out by just reading the code, thanks.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/1964/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "fanpei91": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/1955", "title": "ImportError: cannot import name walk_modules", "body": "When I execute `scrapy` command, here is the error message:\n\n```\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/bin/scrapy\", line 7, in <module>\n    from scrapy.cmdline import execute\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/__init__.py\", line 48, in <module>\n    from scrapy.spiders import Spider\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/spiders/__init__.py\", line 115, in <module>\n    from scrapy.spiders.crawl import CrawlSpider, Rule\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/spiders/crawl.py\", line 11, in <module>\n    from scrapy.utils.spider import iterate_spider_output\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/utils/spider.py\", line 7, in <module>\n    from scrapy.utils.misc import  arg_to_iter\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/utils/misc.py\", line 4, in <module>\n    from importlib import import_module\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/importlib/__init__.py\", line 5, in <module>\n    import scrapy.spiderloader\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/spiderloader.py\", line 7, in <module>\n    from scrapy.utils.misc import walk_modules\nImportError: cannot import name walk_modules\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/1955/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "AffableZonkey": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/1946", "title": "Doc Issue - Installing on Fedora 23 (possibly other rhel based as well)", "body": "Hello - this is a paste of how I got scrapy to successfully install on Fedora 23. It may be of use to other RHEL based distros as well. It's a bit tricky now that Python3 is the default.\n\n(sorry about the cut and paste - it's late)\nPre-Reqs;\n\npython-devel, python-rpm-macros, libffi-devel, redhat-rpm-config, openssl-devel, libxml2-devel, libxml-devel, python-lxml, python-libxml2, python-cffi, glib2-devel gnet2-devel libxslt-devel\n\nand then finally\n\npython2 -m pip install --user --upgrade pip \n\n(in a virtualenv)\n\nI might've missed something, but hope this helps.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/1946/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1952", "title": "added fedora23 info", "body": "So I raised a docs issue the other day for this, but in testing, just dnf install python-scrapy worked ok, however, without that package - in a virtualenv, eg - you would need to install some pre-reqs so that pip install worked. Hopefully this reads ok. It's my first experience with sphinx.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dangra": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/b170e9fb96dc763b9b78916e1557021e5e004d59", "message": "Merge pull request #2609 from otobrglez/extending-s3-files-store\n\nS3FilesStore support for other S3 providers (botocore options)"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/2dee191374e7fb7f2ed35ab9eea07ba7d1ee8b89", "message": "Merge branch 'master' into extending-s3-files-store"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9b4d6a40a6d56acbd9e068e15e6717dc06aee79b", "message": "Merge pull request #3053 from scrapy/release-notes-1.5\n\nRelease notes for the upcoming 1.5.0 version"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/57d04aa9601bc237da4b08777327df241483b389", "message": "Merge pull request #2767 from redapple/http-proxy-endpoint-key\n\n[MRG+1] Use HTTP pool and proper endpoint key for ProxyAgent"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/86c322c3a819405020cd884f128246ae55b6eaeb", "message": "Merge pull request #3038 from scrapy/update-contributing-docs\n\nDOC update \"Contributing\" docs"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/3cf0332ec315ea2b1e710507ab8a60340f56a27a", "message": "Merge pull request #2957 from ScrapingLab/add_meta_json_to_parse_command\n\n[MRG+1] Scrapy Command: add --meta/-m to the \"parse\" command to pass additional meta data into the request"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/b8870ee8a10360aaa74298324d97c823b88ec5c6", "message": "Merge pull request #2989 from colinmorris/ItemExporterDocsExample\n\n[MRG+1] Revise/modernize item exporter example in docs"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/1d9c8f5295f67160dbb8a2acdd1223746d608d23", "message": "Merge pull request #2921 from revolter/hotfix/disable-logging\n\n[MRG+1] Add option to disable automatic log handler install"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/dc4b36a89ea647e9940db44bfe31c6a212d3cb01", "message": "Merge pull request #2952 from NoExitTV/Unhelpful-log-message-from-core.downloader.handlers.http11\n\n[MRG+1] Changed unhelpful log message from core.downloader.handlers.http11"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/4f8b45ec83497691541d5d62dc30ee28d333b7c3", "message": "Merge pull request #2929 from djunzu/add_m4v_to_ignored_extensions\n\n[MRG+1] Add m4v extension to IGNORED_EXTENSIONS in LinkExtractor."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9be8f02de8b52b67c298e2f4cbb2a1f53fc9e93e", "message": "Merge pull request #2942 from rodrigc/twisted-17.9.0\n\n[MRG+1] Bump Twisted requirement on Python 3 to 17.9.0 to catch many Python 3 fixes."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/5fac2d7b90da8f06597df8536bbadd6cadef5d7e", "message": "Merge pull request #2923 from rhoboro/fixes-685\n\n[MRG+2] Fixes #685 FilesPipeline support for Google Cloud Storage."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/d5f3543d77421e4a451529b2846f6f5922872f7e", "message": "Merge pull request #2865 from kirankoduru/2831-explicit-msg-for-scrapy-parse-callback\n\n[MRG+1] Explicit message for scrapy parse callback"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/19382c567549298ba62ed36c483dbc05096da1b2", "message": "Merge pull request #2755 from redapple/downloader-mdw-template\n\n[MRG+1] Add template for a downloader middleware"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/5d9bac789d6fee94236be3f2f33bb1383b288463", "message": "Merge pull request #2849 from cclauss/patch-2\n\n[MRG+1] xrange() --> range() for Python 3"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/bbc56e6960ff4b9ea824bf8c85a9e29b1960ba3e", "message": "Merge pull request #2852 from starrify/new-http-codes-to-retry-2851\n\nAdded: HTTP status code 522/524 to retry."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/881a5e3a57fa6e72435d7c5964a2a8b41db64118", "message": "Merge pull request #2847 from redapple/redirect-308\n\nHandle HTTP 308 Permanent Redirect"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/b6d036ede9b783dcdaa912371a2dff17333d1b08", "message": "Merge pull request #2837 from dguo/patch-2\n\nFix a typo in the Items documentation"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/6de106e7a8062f30e605941385392e385c36968b", "message": "Merge pull request #2763 from scrapy/dataloss-typo\n\nDOC fixed rst syntax in DOWNLOAD_FAIL_ON_DATALOSS docs"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/12971a7b61e61ee48c029d11ccc2d9ae4c0dbd54", "message": "Merge pull request #2764 from scrapy/readme-cleanup\n\nDOC change \"releases\" section content"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/c01c1ef2ae5ae923d3a0aa6abdda3065238a3a4a", "message": "Merge pull request #2655 from Granitosaurus/ptpython_support\n\n[MRG+1] add support for embeded ptpython shell"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/2371a2a0dfbdc535bbe88ae68e986d35063653bf", "message": "Merge pull request #2789 from starrify/add-response-follow-tag-link\n\n[MRG+1] Added: Now supporting <link> tags in Response.follow"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/d1e948cef8597a879eb6690af50cbcbef94c3417", "message": "Merge pull request #2791 from starrify/doc-DontCloseSpider\n\n[MRG+1] Added doc for `scrapy.exceptions.DontCloseSpider`. Also fixes inaccurate doc for `scrapy.signals.spider_idle`."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/8e0b640d2b009a02f41d3e0d443472a5843af621", "message": "Merge pull request #2826 from dguo/patch-1\n\n[MRG+1] Tweak the CSVFeedSpider documentation"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/45b81693b46cd4a5abd95e2f4a106ca2126540f4", "message": "Merge pull request #2769 from stummjr/issue-2766\n\n[MRG+1] Add verification to check if Request callback is callable"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/50b2567d0df6f084078397e0cf45a22f6dae4db7", "message": "Merge pull request #2848 from redapple/tox-jessie-ssl\n\n[WIP] Jessie toxenv: Add cryptography as per https://packages.debian.org/jessie/python-cryptography"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/25f609e2a3c27ca7d7d98dbfddb2c049735935bb", "message": "Merge pull request #2675 from simongartz/png-p-to-jpg-conversion-fix\n\n[MRG+1] Fixes conversion of transparent PNG with palette images to jpg #2452"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/b9d3b447a53691e636c30fa73cebfaac8a6f68ec", "message": "Merge pull request #2670 from qhuang872/master\n\n[MRG+1] Update spiders.rst"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/73668ce4076b87d2d2493f2c9b445c643da9055a", "message": "Merge pull request #2721 from HarrisonGregg/feature-drop-from-response-field\n\n[MRG+1] Allow dropping field in from_response formdata"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/362d6f2d20f411940f4adc715d0c9b6463c52dc7", "message": "Merge pull request #2622 from rolando-contrib/download-maxsize-abort\n\n[MRG+1] Abort connection earlier and avoid to buffer data when max size limit is reached"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/2d66c58e01ddc0d76aeb1cac640e54c323ce0ad9", "message": "Merge pull request #2678 from redapple/double-content-length-0-post\n\nSet bodyproducer with empty content for POST"}], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1455", "title": "[WIP] POC for asyncio+aiohttp", "body": "*\\* DO NOT MERGE - Strongly toxic and cool drug here **\n\nIntegration of a http downloader handler based on [aiohttp](https://github.com/KeepSafe/aiohttp) and asyncio on python3.\n\nbehind the scenes It hooks asyncio loop as a twisted reactor (provided by [txtulip](https://github.com/itamarst/txtulip)).\n\nTests are not supposed to pass, this is just a working proof of concept!\n\nClone [testspiders's scrapy project](https://github.com/scrapinghub/testspiders) and run `scrapy crawl justfollow`, see how smoothly it hits websites :)\n\n```\n$ scrapy crawl justfollow\n2015-08-24 17:10:10 [scrapy] INFO: Scrapy 1.1.0dev1 started (bot: testspiders)\n2015-08-24 17:10:10 [scrapy] INFO: Optional features available: ssl, http11\n2015-08-24 17:10:10 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 3600, 'RETRY_ENABLED': False, 'NEWSPIDER_MODULE': 'testspiders.spiders', 'SPIDER_MODULES': ['testspiders.spiders'], 'CLOSESPIDER_PAGECOUNT': 1000, 'COOKIES_ENABLED': False, 'BOT_NAME': 'testspiders'}\n2015-08-24 17:10:11 [scrapy] INFO: Enabled extensions: SpiderState, CoreStats, CloseSpider, LogStats\n2015-08-24 17:10:11 [scrapy] INFO: Enabled downloader middlewares: RandomUserAgent, ErrorMonkeyMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2015-08-24 17:10:11 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2015-08-24 17:10:11 [scrapy] INFO: Enabled item pipelines: \n2015-08-24 17:10:11 [scrapy] INFO: Spider opened\n2015-08-24 17:10:11 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2015-08-24 17:10:11 [scrapy] DEBUG: Crawled (200) <GET http://scrapinghub.com> (referer: None)\n2015-08-24 17:10:11 [scrapy] DEBUG: Filtered duplicate request: <GET http://scrapinghub.com/platform/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\n2015-08-24 17:10:12 [scrapy] DEBUG: Crawled (200) <GET http://scrapinghub.com/pricing/> (referer: b'http://scrapinghub.com')\n2015-08-24 17:10:12 [scrapy] DEBUG: Crawled (200) <GET http://scrapinghub.com> (referer: b'http://scrapinghub.com')\n2015-08-24 17:10:12 [scrapy] DEBUG: Crawled (200) <GET http://scrapinghub.com/platform/> (referer: b'http://scrapinghub.com')\n2015-08-24 17:10:12 [scrapy] DEBUG: Redirecting (301) to <GET http://www.parsely.com/> from <GET http://parsely.com/>\n2015-08-24 17:10:12 [scrapy] DEBUG: Redirecting (301) to <GET https://eventup.com/> from <GET http://eventup.com/>\n2015-08-24 17:10:12 [scrapy] DEBUG: Crawled (200) <GET http://www.swoop.com/> (referer: b'http://scrapinghub.com')\n2015-08-24 17:10:12 [scrapy] DEBUG: Crawled (200) <GET http://appmonsta.com/> (referer: b'http://scrapinghub.com')\n2015-08-24 17:10:12 [scrapy] DEBUG: Redirecting (301) to <GET https://p2pi.org/> from <GET http://p2pi.org/>\n2015-08-24 17:10:12 [scrapy] DEBUG: Redirecting (301) to <GET https://www.linkedin.com/in/jacobperkins/> from <GET http://www.linkedin.com/in/jacobperkins/>\n2015-08-24 17:10:12 [scrapy] DEBUG: Redirecting (301) to <GET http://fabkids.com/wb/> from <GET http://wittlebee.com/>\n2015-08-24 17:10:12 [scrapy] DEBUG: Crawled (200) <GET http://scrapinghub.com/abuse-report/> (referer: b'http://scrapinghub.com')\n2015-08-24 17:10:12 [scrapy] DEBUG: Crawled (200) <GET http://scrapinghub.com/tos/> (referer: b'http://scrapinghub.com')\n2015-08-24 17:10:12 [scrapy] DEBUG: Crawled (200) <GET http://science-inc.com/> (referer: b'http://scrapinghub.com')\n^C2015-08-24 17:10:12 [scrapy] INFO: Received SIG_SETMASK, shutting down gracefully. Send again to force \n2015-08-24 17:10:12 [scrapy] INFO: Closing spider (shutdown)\n2015-08-24 17:10:12 [scrapy] DEBUG: Redirecting (301) to <GET https://mydeco.com/> from <GET http://mydeco.com/>\n2015-08-24 17:10:12 [scrapy] DEBUG: Redirecting (302) to <GET https://dash.scrapinghub.com/account/login/?next=/> from <GET https://dash.scrapinghub.com/>\n2015-08-24 17:10:12 [scrapy] DEBUG: Crawled (200) <GET http://www.parsely.com/> (referer: b'http://scrapinghub.com')\n2015-08-24 17:10:13 [scrapy] DEBUG: Crawled (200) <GET http://appmonsta.com/dashboard/api-documentation/> (referer: b'http://appmonsta.com/')\n2015-08-24 17:10:13 [scrapy] DEBUG: Redirecting (301) to <GET https://www.olark.com/?welcome> from <GET http://www.olark.com?welcome>\n2015-08-24 17:10:13 [scrapy] DEBUG: Crawled (200) <GET https://eventup.com/> (referer: b'http://scrapinghub.com')\n2015-08-24 17:10:13 [scrapy] DEBUG: Crawled (200) <GET http://status.scrapinghub.com/> (referer: b'http://scrapinghub.com')\n2015-08-24 17:10:13 [scrapy] DEBUG: Crawled (200) <GET http://ar.linkedin.com/in/juancatalano> (referer: b'http://scrapinghub.com')\n2015-08-24 17:10:13 [scrapy] DEBUG: Redirecting (302) to <GET http://www.fabkids.com/wb/> from <GET http://fabkids.com/wb/>\n2015-08-24 17:10:13 [scrapy] DEBUG: Crawled (200) <GET https://www.linkedin.com/pub/terese-herbig/1/137/370> (referer: b'http://scrapinghub.com')\n2015-08-24 17:10:13 [scrapy] DEBUG: Crawled (200) <GET http://scrapinghub.com/screenshots/large/forum.png> (referer: b'http://scrapinghub.com/platform/')\n2015-08-24 17:10:13 [scrapy] DEBUG: Crawled (200) <GET https://p2pi.org/> (referer: b'http://scrapinghub.com')\n2015-08-24 17:10:13 [scrapy] DEBUG: Crawled (200) <GET https://www.olark.com/site/2109-478-10-2394/contact> (referer: b'http://appmonsta.com/')\n2015-08-24 17:10:14 [scrapy] DEBUG: Crawled (200) <GET https://www.linkedin.com/in/jacobperkins/> (referer: b'http://scrapinghub.com')\n2015-08-24 17:10:15 [aiohttp.client] WARNING: Can not load response cookies: Illegal key value: wpgb_visit_last_php-http://scrapinghub_com\n2015-08-24 17:10:15 [scrapy] DEBUG: Crawled (200) <GET http://streamhacker.com> (referer: b'http://scrapinghub.com')\n2015-08-24 17:10:20 [scrapy] DEBUG: Crawled (200) <GET http://scrapinghub.com/screenshots/large/kb.png> (referer: b'http://scrapinghub.com/platform/')\n2015-08-24 17:10:21 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 9925,\n 'downloader/request_count': 30,\n 'downloader/request_method_count/GET': 30,\n 'downloader/response_bytes': 1157700,\n 'downloader/response_count': 30,\n 'downloader/response_status_count/200': 21,\n 'downloader/response_status_count/301': 7,\n 'downloader/response_status_count/302': 2,\n 'dupefilter/filtered': 667,\n 'finish_reason': 'shutdown',\n 'finish_time': datetime.datetime(2015, 8, 24, 20, 10, 21, 100933),\n 'log_count/DEBUG': 31,\n 'log_count/INFO': 8,\n 'log_count/WARNING': 1,\n 'request_depth_max': 3,\n 'response_received_count': 21,\n 'scheduler/dequeued': 30,\n 'scheduler/dequeued/memory': 30,\n 'scheduler/enqueued': 839,\n 'scheduler/enqueued/memory': 839,\n 'start_time': datetime.datetime(2015, 8, 24, 20, 10, 11, 321827)}\n2015-08-24 17:10:21 [scrapy] INFO: Spider closed (shutdown)\n```\n", "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1440", "title": "Use priority queues for Downloader slot queues", "body": "**WIP** \n\ncloses #1371 \n", "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1329", "title": "Use psutil if available to measure RSS on memusage extension", "body": "memusage extension fires immediately when the scrapy process is `exec()`'ed from another process whose max resident memory is higher than `MEMUSAGE_LIMIT_MB`.\n\nThe limitation comes from the use of stdlib `resource` package that can measure _max RSS_ and not _current RSS_. The overcome this limiation this PR proposes to use [psutil](https://github.com/giampaolo/psutil)\n\nFor a proof on how max RSS propagates across exec() calls see:\n\n_memtest_\n\n``` python\n#!/usr/bin/env python\nimport os\nimport sys\nimport resource\nimport psutil\nfrom argparse import ArgumentParser\n\ndef _mem():\n    rss, vms = psutil.Process().memory_info()\n    maxrss = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n    return {'maxrss': maxrss, 'rss': rss / 1024, 'vms': vms / 1024}\n\ndef _allocate_mb(mb):\n    return open('/dev/urandom', 'rb').read(mb * 1024**2)\n\ndef main():\n    ap = ArgumentParser()\n    ap.add_argument('-a', '--allocate', type=int, default=0)\n    ap.add_argument('subcmd', nargs=\"*\")\n    args = ap.parse_args()\n\n    if args.allocate:\n        _ = _allocate_mb(args.allocate)\n\n    print(\"{} {}\".format(_mem(), sys.argv))\n\n    if args.subcmd:\n        os.execvp(args.subcmd[0], args.subcmd)\n\n\nif __name__ == '__main__':\n    sys.exit(main())\n```\n\nRunning it:\n\n```\n$ ./memtest -- ./memtest -a 10 -- ./memtest \n{'maxrss': 11792, 'vms': 71084, 'rss': 11792} ['./memtest', '--', './memtest', '-a', '10', '--', './memtest']\n{'maxrss': 22232, 'vms': 81328, 'rss': 22232} ['./memtest', '-a', '10', '--', './memtest']\n{'maxrss': 22236, 'vms': 71084, 'rss': 11992} ['./memtest']\n```\n\nSee how RSS goes from 11792 to 22232 and then back to 11992, but maxrss jumps from 11792 to 22232 and is inherited by third memtest.\n\nThis is particulary problematic when running spiders inside docker containers because docker daemon can take up to GBs of RAM and its max_rss is propagated to init process of the docker container.\n\nThe following Dockerfile was used to build an image to test memtest script with docker:\n\n``` Dockerfile\nFROM ubuntu:12.04\nRUN apt-get update -q && apt-get install -qy python python-dev\nADD https://raw.github.com/pypa/pip/master/contrib/get-pip.py /get-pip.py\nRUN python /get-pip.py && pip install -U wheel==0.24.0\nRUN pip install psutil\nCOPY memtest /bin/memtest\nENTRYPOINT [\"/bin/memtest\"]\n```\n\nthen built and ran:\n\n``` bash\n$ docker build -t memtest .\n...\n\n$ docker run -it --rm memtest -- memtest -a 10 -- memtest\n{'maxrss': 8021068, 'vms': 40528, 'rss': 11504} ['/bin/memtest', '--', 'memtest', '-a', '10', '--', 'memtest']\n{'maxrss': 8021068, 'vms': 50772, 'rss': 21672} ['/bin/memtest', '-a', '10', 'memtest']\n{'maxrss': 8021068, 'vms': 40516, 'rss': 11416} ['/bin/memtest']\n\n$ ps v -p 26812\n  PID TTY      STAT   TIME  MAJFL   TRS   DRS   RSS %MEM COMMAND\n26812 ?        Ssl  649:53      0     0 26969720 8032092 12.1 /usr/bin/docker -d --iptables=false\n```\n\nSee how memtest maxrss reports ~8GB which coincides with docker daemon RSS reported by @ps@ unix command.\n", "author_association": "OWNER"}], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/736579", "body": "Hi Rob, thanks for taking time to improve get_meta_refresh.\n\nWhat usecase does this patch try to cover? Could be good to add them as unittests. Pasting some real urls doesn't covered by current implementation is useful.\n\nthanks.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/736579/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12832808", "body": "is there still interest in this feature and someone wants to take the lead?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12832808/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12832864", "body": "@kalessin: any news on adding tests so we can merge this?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12832864/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12832913", "body": "no example use case, no code attached, no one has asked for this in an year and no interest in implementing it.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12832913/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12835733", "body": "@pablohoffman take a look to above fix\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12835733/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12837059", "body": "`canonicalize_url` is not used by core functionality and this issue miss the proposed patch\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12837059/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12837303", "body": "we lost the patch for this feature, but it is still very easy to add the main problem is that we lack a proper way to test it.\n\nWe will address this issue once system tests are ready.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12837303/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/22539008", "body": "this is supported in new downloader handler by adding a \"bindaddress\" key to request meta.\n\ni.e.: `Request(url, meta={'bindaddress': '10.0.0.2'})`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/22539008/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12837129", "body": "~~wohoo! this was added (and tested) as part of recent changes to support RFC2616 cache policy.~~\n\nsomehow it is broken again.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12837129/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/40414840", "body": "broken, there is a `_dont_cache` meta keyword but it is not exactly the same. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/40414840/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12837505", "body": "fixed 10 months ago by https://github.com/scrapy/scrapy/commit/a0a1a5026bc710d8478d3da2b1bdf7afb41b8e67\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12837505/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5073130", "body": "support for chunked transfers is in stable branch since 0.14 using `scrapy.contrib.downloadermiddleware.chunked` (enabled by default). \n\nthere are some concerns about chunked encoding implementation being addressed in #109\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5073130/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12837610", "body": "this was a synthetic test case that never showed up in real life\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12837610/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12838183", "body": "just tried this, it logs a warning now and it is caused by `restrict_xpaths` argument, looks same bug than #199 \n\n```\n$ scrapy shell 'http://www.last.fm/music/AC%252FDC/+images'\n2013-01-29 12:51:24-0200 [scrapy] INFO: Scrapy 0.17.0 started (bot: scrapybot)\n...\n>>> from scrapy.contrib.linkextractors import sgml\n>>> sgml.SgmlLinkExtractor(restrict_xpaths=('//a[@class=\"nextlink\"]')).extract_links(response)\n/home/daniel/src/scrapy/scrapy/link.py:16: UserWarning: Do not instantiate Link objects with unicode urls. Assuming utf-8 encoding (which could be wrong)\n  warnings.warn(\"Do not instantiate Link objects with unicode urls. \" \\\n[Link(url='http://www.last.fm/music/AC/DC/+images?page=2', text=u'Next', fragment='', nofollow=False)]\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12838183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12862288", "body": "this is actually a different bug, happens that `canonicalize_url` is unquoting the url path converting `%2f` into `/` and that is a wrong url for last.fm.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12862288/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12888039", "body": "this is the same problem described at https://github.com/kennethreitz/requests/pull/273\n\nand wikipedia explain it more clearly what characters are reserved and must be quoted in a path http://en.wikipedia.org/wiki/Percent-encoding\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12888039/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12889138", "body": "pass tests\n\n```\nscrapy.tests.test_contrib_linkextractors\n  LinkExtractorTestCase\n    test_base_url ...                                                      [OK]\n    test_basic ...                                                         [OK]\n    test_extraction_encoding ...                                           [OK]\n    test_link_nofollow ...                                                 [OK]\n    test_link_text_wrong_encoding ...                                      [OK]\n    test_matches ...                                                       [OK]\n  SgmlLinkExtractorTestCase\n    test_base_url_with_restrict_xpaths ...                                 [OK]\n    test_deny_extensions ...                                               [OK]\n    test_encoded_url ...                                                   [OK]\n    test_encoded_url_in_restricted_xpath ...                               [OK]\n    test_extraction ...                                                    [OK]\n    test_extraction_using_single_values ...                                [OK]\n    test_matches ...                                                       [OK]\n    test_process_value ...                                                 [OK]\n    test_restrict_xpaths ...                                               [OK]\n    test_restrict_xpaths_concat_in_handle_data ...                         [OK]\n    test_restrict_xpaths_encoding ...                                      [OK]\n    test_urls_type ...                                                     [OK]\nscrapy.tests.test_utils_url\n  UrlUtilsTest\n    test_canonicalize_url ...                                              [OK]\n    test_url_is_from_any_domain ...                                        [OK]\n    test_url_is_from_spider ...                                            [OK]\n    test_url_is_from_spider_class_attributes ...                           [OK]\n    test_url_is_from_spider_with_allowed_domains ...                       [OK]\n    test_url_is_from_spider_with_allowed_domains_class_attributes ...      [OK]\nDoctest: scrapy.utils\n  url\n    escape_ajax ...                                                        [OK]\n\n-------------------------------------------------------------------------------\nRan 25 tests in 0.031s\n\nPASSED (successes=25)\n\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12889138/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3659280", "body": "I think this issue can be closed as part of work for #82\n\nsee 2840865746a207afe7e98c3139032df0242c2dda\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3659280/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12839451", "body": "fixed by #190 \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12839451/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12840474", "body": "unlikely, reopen if there is a valid use case. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12840474/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12840655", "body": "it works now\n\n```\n~$ PYTHONPATH=~/src/testspiders SCRAPY_SETTINGS_MODULE=testspiders.settings scrapy list\ndummy\nfollowall\nnoop\nmad\ntimed\nloremipsum\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12840655/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12840825", "body": "Dupe filter is part of scheduler now, and any redirected request goes trough it too and will be discarded if already seen.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12840825/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12840935", "body": "noone has proposed a solution and there are workarounds like using lxml itertag instead of xmliter function which is regex based.  \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12840935/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12841026", "body": "command line option to `parse` or `shell` command?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12841026/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/26352771", "body": "I think nobody is working on it. you welcome.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/26352771/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/27695177", "body": "@pedrofaustino: news from who? I stated on https://github.com/scrapy/scrapy/issues/36#issuecomment-26352771 that no known developer is working on it.\n\nfrom comments on other issues I suppose you (@pedrofaustino) and @srmaximiano are working together, so if you guys aren't implementing this feature, nobody is.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/27695177/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/27698559", "body": "@pedrofaustino : thanks for pointing to PR #447, I left some comments there.\n\nI think it worth researching on supporting a reduced set of curl options, another inspiring interface is https://github.com/jkbr/httpie     \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/27698559/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12842115", "body": "superseded by #45 \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12842115/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/26274347", "body": "@nramirezuy: so true.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/26274347/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/4326668", "body": "Scrapy core supported `start_requests` generator for a long time, consuming start requests on demand but default implementation in BaseSpider returned a list of requests instead of a generator.\n\nPrevious feature combined with persistence of requests at scheduler reduced memory footprint and removed the limitation of scheduling lot of requests in start_requests.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/4326668/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3854708", "body": "I think this pull request looks fair and clean, the only worry is that it changes default behaviour. In the other side been \"secure\" by default is not bad at all.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3854708/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3854725", "body": "Oh, could be cool to squash and rebase it to latest trunk after agreement on it going to be merged.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3854725/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/4231197", "body": "looking good, thanks for squashing it. I did a minor comment on documentation wording.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/4231197/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12831607", "body": "I think the problems described by this issue are already covered now because:\n- Scrapy now uses disk based queues for scheduler to reduce memory footprint caused by scheduling tons of requests\n- Core flow control should not consume all iterator values while it is saturated downloading or processing responses\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12831607/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11955390", "body": "I noticed this was fixed 3 months ago by https://github.com/scrapy/scrapy/commit/dd13dfe82bd6ae0d0d15235e8505c0ea905dd841\n\nthanks for pointing it long time ago.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11955390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11954987", "body": "Default http proxy middleware was designed to obey common proxy environment variables like `http_proxy`, `https_proxy` and `noproxy`.  Any other proxy functionality like proxy rotation or custom auth can be easily implemented as part of other middleware.\n\nAny serious attempt to modify this design goal must be backward compatible, real self contained use case, provide documentation and test cases.   \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11954987/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11955549", "body": "I think the goal on this pull request can be implemented pretty much like AutoThrottle extension does using changes introduced by https://github.com/scrapy/scrapy/pull/206\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11955549/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11957733", "body": "I can understand `0` can be a valid value, but for what case an empty string is better than non-empty? \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11957733/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5265153", "body": "default OffsiteMiddleware is thought to be use with less than 10 domains, the common use case are vertical crawls that extract information from a single site per spider. \n\nthe middleware can be completely disabled or extended as you did for wide crawls.\n\nI am happy to merge any proposal that doesn't compromise simplicity and speed for few domains and still handle a large list of domains.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5265153/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12831643", "body": "dusty and the real goal was to run scrapy spiders inside scraperwiki infrastructure (which is cool) but there are better ways to integrate them.  \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12831643/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3264790", "body": "I see you are using scrapy under scraperwiki https://scraperwiki.com/scrapers/scrapy_test/ (great!)\n\nfrom scraper source at https://scraperwiki.com/editor/raw/scrapy_test, you are overriding ITEM_PIPELINES settings:\n\n```\n    settings.overrides['ITEM_PIPELINES'] = [scrapy_utils.SWPipeline]\n```\n\ncan you explain why using standard string based imports like the following doesn't work:\n\n```\n    settings.overrides['ITEM_PIPELINES'] = [\"scrapy_utils.SWPipeline\"]\n```\n\nI am OK with functionality added by your patch, but we already consider settings handling a bit bloated than adding more functionality is against our simplification goal for it. \nBut in the other side it is my desire to see scrapy running under scraperwiki too. :)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3264790/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3272331", "body": "Hi Johtso, I found that scraper file is named \"script.py\" always, try running:\n\n``` python\nimport sys\nprint sys.argv\n```\n\nit outputs:\n\n```\n['/home/startup/exec.py', '--script=/home/scriptrunner/script.py']\n```\n\nthen the following lines at the end of the scraper script are enough to run a scrapy spider:\n\n``` python\nif __name__ == 'scraper':\n    from scrapy.cmdline import execute\n    execute(['scrapy', 'runspider', 'script.py'])\n```\n\nAlso, there is no need to use a pipeline to store scraped items, you can connect to `item_scraped` signal and store items as simple as:\n\n``` python\nfrom scrapy import signals\nfrom scrapy.xlib.pydispatch import dispatcher\n\ndef _item_scraped(item, spider):\n    scraperwiki.sqlite.save(['name'], data=dict(item), verbose=0)\n\ndispatcher.connect(_item_scraped, signals.item_scraped)\n```\n\nand if you really think buffering items before saving is an issue (and doing it in scraperwiki library isn't possible), then a modified version of your pipeline code using signals will do.\n\nAn example scraper using this technique can be found at https://scraperwiki.com/scrapers/pydaybot/ ([mirror](https://gist.github.com/1520383)), it is an example bot made for Python Day Uruguay, original scrapy project at https://github.com/insophia/pydaybot\n\nAt last, as scraper is always named \"script.py\", you can use string based imports like \"script.NameOfMyPipeline\" in ITEM_PIPELINES or any other similar setting like middlewares. Of course this doesn't work for modules imported using `scraperwiki.utils.swimport`.\n\nIMO the best and simple way to run scrapy spiders in scraperwiki will be by adding a contrib module to scrapy, and from scraper script use something like:\n\n``` python\nif __name__ == 'scraper':\n    from scrapy.contrib.scraperwiki import run_scraper\n    run_scraper()\n```\n\n`run_scraper` will save items by default but can be disabled and/or buffering configured by passing keyword arguments.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3272331/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11959733", "body": "I am open to review any integration solution to run scrapy spiders in ScraperWiki, in the meantime, I will close this PR as it doesn't address the goal of running scrapy spider in ScraperWiki completely.   \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11959733/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11956232", "body": "I'm not sure if this was a bug or not, but it works now as tested by https://github.com/scrapy/scrapy/blob/master/scrapy/tests/test_contrib_exporter.py#L185\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11956232/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12651988", "body": "You were right about XML, it is not working\n\nusing this spider\n\n``` python\nfrom scrapy.spider import BaseSpider\nfrom scrapy.item import Item, Field\n\n\nclass MyItem(Item):\n    name = Field()\n    subitem = Field()\n\n\nclass MySpider(BaseSpider):\n    name = 'nested'\n    start_urls = ['http://www.example.com']\n\n    def parse(self, response):\n        subitem = MyItem(name='foo')\n        item = MyItem(name='bar', subitem=subitem)\n        yield item\n```\n\nrunning it as:\n\n```\n$ scrapy runspider nesteditems.py --output-format xml -o out.xml \n2013-01-24 11:51:46-0200 [scrapy] INFO: Scrapy 0.17.0 started (bot: scrapybot)\n2013-01-24 11:51:46-0200 [scrapy] DEBUG: Enabled extensions: FeedExporter, LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState\n2013-01-24 11:51:46-0200 [scrapy] DEBUG: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2013-01-24 11:51:46-0200 [scrapy] DEBUG: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2013-01-24 11:51:46-0200 [scrapy] DEBUG: Enabled item pipelines: \n2013-01-24 11:51:46-0200 [nested] INFO: Spider opened\n2013-01-24 11:51:46-0200 [nested] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2013-01-24 11:51:46-0200 [scrapy] DEBUG: Telnet console listening on 0.0.0.0:6023\n2013-01-24 11:51:46-0200 [scrapy] DEBUG: Web service listening on 0.0.0.0:6080\n2013-01-24 11:51:46-0200 [nested] DEBUG: Redirecting (302) to <GET http://www.iana.org/domains/example/> from <GET http://www.example.com>\n2013-01-24 11:51:47-0200 [nested] DEBUG: Redirecting (302) to <GET http://www.iana.org/domains/example> from <GET http://www.iana.org/domains/example/>\n2013-01-24 11:51:48-0200 [nested] DEBUG: Crawled (200) <GET http://www.iana.org/domains/example> (referer: None)\n2013-01-24 11:51:48-0200 [nested] DEBUG: Scraped from <200 http://www.iana.org/domains/example>\n    {'name': 'bar', 'subitem': {'name': 'foo'}}\n2013-01-24 11:51:48-0200 [nested] INFO: Closing spider (finished)\n2013-01-24 11:51:48-0200 [nested] INFO: Stored xml feed (1 items) in: out.xml\n2013-01-24 11:51:48-0200 [nested] INFO: Dumping Scrapy stats:\n    {'downloader/request_bytes': 688,\n     'downloader/request_count': 3,\n     'downloader/request_method_count/GET': 3,\n     'downloader/response_bytes': 1204,\n     'downloader/response_count': 3,\n     'downloader/response_status_count/200': 1,\n     'downloader/response_status_count/302': 2,\n     'finish_reason': 'finished',\n     'finish_time': datetime.datetime(2013, 1, 24, 13, 51, 48, 274570),\n     'item_scraped_count': 1,\n     'log_count/DEBUG': 10,\n     'log_count/INFO': 5,\n     'response_received_count': 1,\n     'scheduler/dequeued': 3,\n     'scheduler/dequeued/memory': 3,\n     'scheduler/enqueued': 3,\n     'scheduler/enqueued/memory': 3,\n     'start_time': datetime.datetime(2013, 1, 24, 13, 51, 46, 274229)}\n2013-01-24 11:51:48-0200 [nested] INFO: Spider closed (finished)\n```\n\nIt doesn't store the nested items:\n\n```\n$ xmllint -format out.xml \n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n  <item>\n    <subitem>\n      <value>name</value>\n    </subitem>\n    <name>bar</name>\n  </item>\n</items>\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12651988/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5265399", "body": "`process_value` is good enough to workaround this rare case, and this is a dusty issue without fix proposal. feel free to reopen if you are able contribute a working patch with test cases.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5265399/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12831799", "body": "cough cough, an year of dust here :bath: :)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12831799/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/47848072", "body": "+1 to close and focus on lxml backend only.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/47848072/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3763568", "body": "I think the issue is httpcompression middleware not handling the invalid declared compressed body, also by switching the order of the middlewares it will miss meta redirects in compressed bodies. And moving redirect middleware also brings some issues with cookie handling set by redirect responses.\n\nThe proposed patch is just a workaround that hides the real issue with badly declared bodies not handled correctly by httpcompression middleware.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3763568/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3763877", "body": "As the issue also affects non-redirects, what about leaving the body as-is if httpcompression middleware fails to decompress it (maybe logging a warning) and handling the redirection as we normally do.\n\nAnother option is to split redirection middleware, one based on response status and hooked before httpcompression and another post httpcompression only handling meta redirects.\n\nI still prefer first option that is general enough to fix the issue for non-redirects.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3763877/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11962578", "body": "I have checked Firefox and Chrome behavior for 200, 301 and 302 response status with `Content-Encoding: gzip` header and invalid compressed body.\n\nBoth browsers shows an error for status 200 about undecodeable body content, and both redirects to `Location` header ignoring body completely on status 301 and 302.\n\nI think we should mimic this as much as possible, possible options:\n1. Split redirection middleware into _status redirections_ and _meta html redirection_, prioritazing the former before response is uncompressed and the latter just after.  \n2. Do not uncompress body for non-200 responses (or only skipping 3xx responses)\n3. Do not fail uncompressing 3xx responses but fail for any other status code.\n\nI prefer 1, it looks as the most clean approach to me\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11962578/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11994582", "body": "hey @darkrho, I merged one of your commits as part of the final fix. thanks!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11994582/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3655285", "body": "I don't get why scrapy needs to handle `about:` scheme, but in case we agree it does, this patch is rendering a blank page for every `about:` scheme request it gets. IMHO rendering 404 for other than `about:blank` makes more sense.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3655285/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11995600", "body": "The change by itself looks fine, but I still can't see what is the advantage of supporting `about:blank`.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11995600/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3692902", "body": "now it closes fine\n\n```\n2012-01-27 17:19:11-0200 [scrapy] INFO: Scrapy 0.15.1 started (bot: testbot)\n2012-01-27 17:19:11-0200 [spidername.com] INFO: Spider opened\n2012-01-27 17:19:11-0200 [spidername.com] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2012-01-27 17:19:11-0200 [spidername.com] ERROR: Obtaining request from start requests\n    Traceback (most recent call last):\n      File \"/home/daniel/envs/mytestenv/local/lib/python2.7/site-packages/twisted/internet/base.py\", line 1169, in run\n        self.mainLoop()\n      File \"/home/daniel/envs/mytestenv/local/lib/python2.7/site-packages/twisted/internet/base.py\", line 1178, in mainLoop\n        self.runUntilCurrent()\n      File \"/home/daniel/envs/mytestenv/local/lib/python2.7/site-packages/twisted/internet/base.py\", line 800, in runUntilCurrent\n        call.func(*call.args, **call.kw)\n      File \"/home/daniel/src/scrapy/scrapy/utils/reactor.py\", line 41, in __call__\n        return self._func(*self._a, **self._kw)\n    --- <exception caught here> ---\n      File \"/home/daniel/src/scrapy/scrapy/core/engine.py\", line 108, in _next_request\n        request = slot.start_requests.next()\n      File \"/home/daniel/src/testbot/testbot/spiders_dev/myspider.py\", line 32, in start_requests\n        'spidername.com does not support url mapping'\n    exceptions.AssertionError: spidername.com does not support url mapping\n\n2012-01-27 17:19:11-0200 [spidername.com] INFO: Closing spider (finished)\n2012-01-27 17:19:11-0200 [spidername.com] INFO: Dumping spider stats:\n    {'finish_reason': 'finished',\n     'finish_time': datetime.datetime(2012, 1, 27, 19, 19, 11, 981009),\n     'start_time': datetime.datetime(2012, 1, 27, 19, 19, 11, 973632)}\n2012-01-27 17:19:11-0200 [spidername.com] INFO: Spider closed (finished)\n2012-01-27 17:19:11-0200 [scrapy] INFO: Dumping global stats:\n    {'memusage/max': 111972352, 'memusage/startup': 111972352}\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3692902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/26022028", "body": "tests added by 5eb429999e9e6a49eb9721d2dc0a47e0100345c3\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/26022028/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5265091", "body": "works for me too, I am closing this issue due to lack of response.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5265091/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5265092", "body": "close...\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5265092/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12832309", "body": "lacks test cases\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12832309/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/4145968", "body": "The fix was merged and will be released in Scrapy 0.14.2\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/4145968/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/4346183", "body": "Looks good to me, I couldn't spot issues.\n\nI see a trend to connect signals in `from_crawler` instead of `__init__`, can we take it as good practice and port others pipelines/middelwares in Scrapy codebase to do the same?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/4346183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5265095", "body": "I can't find it but I remember that there is a similar issue reported in IPython bug tracker, could be good to link it from here.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5265095/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12832136", "body": "This looks like a contrib pipeline that implements the basis for item type delegation, users still need to extend it to add its projects functionality.\n\nI don't think this worth the pain of maintaining another contrib as part of Scrapy project, the functionality described is easily implementable and there is no concensus about the approach to handle multiple item types. Others have proposed building an item pipeline per type instead.\n\nIMHO this base pipeline is more for a blog post, recipe or external scrapy cookrecipes project.\n\nthanks \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12832136/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5265388", "body": "Duplicate requests are very common for spiders implementing aggressive link extraction like those based in CrawlSpider, which is the default spider class. Logging every discarded request in this case is too much and the reason not only to discard them silently but also to don't notify the spider by triggering its request's errback.\n\nIn the past we were logging too much for Items and downloaded pages, and we settle to log rates instead of per downloaded-page/scraped-item, what about adding the discarded-requests to logstats log line?\n\nAttached commit to this issue implements that and logging of first N requests discarded by dupefilter.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5265388/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5265394", "body": "this is how it looks using the attached patch:\n\n```\n2012-04-22 00:37:30-0300 [followall] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min), discarded 0 requests (at 0 req/min)\n2012-04-22 00:37:32-0300 [followall] DEBUG: Discarded duplicate request: <GET http://scrapinghub.com/>\n2012-04-22 00:37:32-0300 [followall] DEBUG: Discarded duplicate request: <GET http://scrapinghub.com/scrapy-cloud.html>\n2012-04-22 00:37:32-0300 [followall] DEBUG: Discarded duplicate request: <GET http://scrapinghub.com/tour.html>\n2012-04-22 00:37:32-0300 [followall] DEBUG: Discarded duplicate request: <GET http://scrapinghub.com/about.html>\n2012-04-22 00:37:32-0300 [followall] DEBUG: Discarded duplicate request: <GET http://scrapinghub.com/autoscraping.html>\n2012-04-22 00:37:32-0300 [followall] DEBUG: Discarded duplicate request: <GET http://scrapinghub.com/proxyhub.html>\n2012-04-22 00:37:32-0300 [followall] DEBUG: Discarded duplicate request: <GET http://scrapinghub.com/pricing.html>\n2012-04-22 00:37:32-0300 [followall] DEBUG: Discarded duplicate request: <GET http://scrapinghub.com/faq.html>\n2012-04-22 00:37:32-0300 [followall] DEBUG: Filtered offsite request to 'doc.scrapy.org': <GET http://doc.scrapy.org/en/latest/topics/spiders.html>\n2012-04-22 00:37:32-0300 [followall] DEBUG: Discarded duplicate request: <GET http://scrapinghub.com/services.html>\n2012-04-22 00:37:32-0300 [followall] DEBUG: Discarded duplicate request: <GET http://scrapinghub.com/consulting-faq.html>\n2012-04-22 00:37:32-0300 [followall] DEBUG: Disabled logging of duplicate requests\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5265394/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5026058", "body": "overall it is looking great!\n\nmy notes:\n- `WebClientContextFactory` needs to be configurable as it is after #82\n- HEAD is handled as a special case, is this really needed?\n- [`twisted.web._newclient.HTTPClientParser`](http://twistedmatrix.com/documents/11.1.0/api/twisted.web._newclient.HTTPClientParser.html) (used by HTTP11ClientFactory) handles chunked transfers clashing with `scrapy.contrib.downloadermiddleware.chunked`.\n- HTTPClientParser is a bit strict about content-length headers, it could cause problems crawling badly implemented servers.\n- HTTP11ClientFactory doesn't handle redirections as it did in HTTPClientFactory. good! \n\nin summary:\n- lot of code reused from twisted\n- clear separation of agent, protocol and parser\n- still extensible\n- no persistent connections yet but the path looks clean\n- `ProxyAgent` doesn't support SSL proxies (CONNECT) but looks great as future improvement to twisted\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5026058/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5071010", "body": "thanks for submitting this pull request and keep working on it!\n\nAs for HEAD method, I prefer to test the body length too instead of handling it specially in downloader handler.\n\nAbout `RedirectAgent`, Scrapy already have a downloader middleware to handle redirections `scrapy.contrib.downloadermiddleware.redirect` and it is the canonical way to implement this feature because it can be easily disabled, extended and tested. I only foresee using two twisted agents in Scrapy, they are basic `Agent` and `ProxyAgent` because they offer the minimal set of features that doesn't clash with scrapy builtins.\n\nRegarding chunked-encoding transfer, `scrapy.contrib.downloadermiddleware.chunked` can't handle all the cases that `HTTPClientParser` does trough `_ChunkedTransferDecoder` as reported in https://groups.google.com/d/topic/scrapy-developers/Ss2XDTRdSb4/discussion. So using twisted chunked decoder instead of scrapy middleware is a big plus in this case.\n\nFor gzip and others content-encodings there is `scrapy.contrib.downloadermiddleware.httpcompression` that works very well so far, do you know of case not handled by this middleware?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5071010/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5080293", "body": "On Wed, Apr 11, 2012 at 6:57 PM, paul <\nreply@reply.github.com\n\n> wrote:\n> \n> Refactored a bit. Explicitly removed `Trasnfer-Encoding` header.\n> I admit I haven't really tested this patch with chunked encoded bodies\n> from real servers.\n\nI will give it a try later or tomorrow\n\n> As `ProxyAgent` doesn't handle CONNECT, I guess it's better to restrict\n> the use of Twisted's `Agent` for http scheme only (why I removed the 443\n> port part)\n\nPlease, restore it. Current http downloader handler can't CONNECT either\nbut forward method just like it do under plain http.\nAnd if we improve ProxyAgent to handle CONNECT later then it is easier to\nhack on it.\n\n> As for your question about gzip and\n> `scrapy.contrib.downloadermiddleware.httpcompression`, I simply haven't\n> played with Scrapy enough to have an opinion.\n\nOk, np.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5080293/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5739604", "body": "hi @redapple, I have been working on your patch, see https://github.com/dangra/scrapy/tree/http11\n\nas you are the original developer for this feature, can I ask you for a quick review?\n\nIt requires twisted 12.1 because it is using HTTPConnectionPool for persistent connections\n\ncompare link https://github.com/dangra/scrapy/compare/master...http11\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5739604/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5743490", "body": "thanks for reviewing it!\n\nI agree on your comment to body producer, wrote a simpler alternative line of code that follows your idea.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5743490/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5743661", "body": "here: https://github.com/dangra/scrapy/commit/e1ec4bb898f7cdf90cc0b7a666a59689a3dc811a\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5743661/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5743736", "body": "Can you merge my branch into yours and push to github, that's the only way my commits will show in this pull request. otherwise I will have to open another PR.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5743736/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5744686", "body": "looks good to me, thanks.\n\nthere is still a broken test case that I want to fix before merging this into master branch.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5744686/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/8367160", "body": "hi @paulproteus, thanks for following up on this\n\nI have a fork myself on this pull request on https://github.com/dangra/scrapy/tree/http11\n\nIt is rebased on top of master too and include a fix for unclean reactor messages, basically trial complains about HTTPConnectionPool not cleaned before closing reactor dangra/scrapy@698105b0fc90bf28d2b8efce607e9ccb88c36f0c\n\nI am OK with removing the `log.err` line in `_agentrequest_failed` callback, it is there just to help debugging new issues with http11 downloader, isn't something permanent.\n\nAs said before in previous PR comments, the only test case failing is `scrapy.tests.test_downloader_handlers.Http11TestCase.test_timeout_download_from_spider`\n\nyou can see a full test run with all tests passing except this here http://travis-ci.org/#!/dangra/scrapy/jobs/2369075\n\n```\n$ trial scrapy.tests.test_downloader_handlers.Http11TestCase\nscrapy.tests.test_downloader_handlers\n  Http11TestCase\n    test_download ...                                                      [OK]\n    test_download_head ...                                                 [OK]\n    test_host_header_not_in_request_headers ...                            [OK]\n    test_host_header_seted_in_request_headers ...                          [OK]\n    test_payload ...                                                       [OK]\n    test_redirect_status ...                                               [OK]\n    test_redirect_status_head ...                                          [OK]\n    test_timeout_download_from_spider ...                               [ERROR]\n\n===============================================================================\n[ERROR]\nTraceback (most recent call last):\nFailure: twisted.internet.error.TimeoutError: User timeout caused connection failure.\n\nscrapy.tests.test_downloader_handlers.Http11TestCase.test_timeout_download_from_spider\n-------------------------------------------------------------------------------\nRan 8 tests in 0.087s\n\nFAILED (errors=1, successes=7\n```\n\nby this time I was hoping this twisted ticket to be closed but it isn't http://twistedmatrix.com/trac/ticket/4330\nI have been trying workarounds by extending Agents and even HTTPConnectionPool but I always ends discarding it because I don't like the results\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/8367160/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/8369494", "body": "I changed the pull request HEAD to my fork using:\n\n```\ncurl --user \"dangra\" --request POST --data '{\"issue\": \"109\", \"head\": \"dangra:http11\", \"base\": \"master\"}' https://api.github.com/repos/scrapy/scrapy/pulls\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/8369494/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/18911989", "body": "Yes! we have it ready, just need to backport some twisted bits to support old releases.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/18911989/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/18912175", "body": "@redapple: you did the initial work, you are the rock star here :)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/18912175/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/18931633", "body": "moved to PR #318 \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/18931633/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5068976", "body": "thanks! don't hesitate to report typos :)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5068976/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12842947", "body": "@fguilpain: paste.pocoo.org is down and I want to restore the proposed patch if possible.\ncan you paste it as a comment in this issue if is still with you?\n\nthanks!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12842947/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5080464", "body": "I completely regret on this change, the proper fix must happen in spider xpaths.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5080464/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5197909", "body": "I have not gentoo at hand, but under my py2.7 system scrapy pass all tests\n\n```\n$ python --version\nPython 2.7.2+\n$ scrapy version\nScrapy 0.14.2\n$ bin/runtests.sh --reporter=text scrapy/tests/test_utils_*py \n.....................................................................................................................................................................................\n-------------------------------------------------------------------------------\nRan 181 tests in 0.345s\n\nPASSED (successes=181)\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5197909/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5220428", "body": "Ok, I don't discard that there can be some hidden bugs in testing suite that shows up in your system and not in mine.\n\nto be clear, I am cloning scrapy from github and checking out 0.14.2 tag, and all tests are passing successfully:\n\n```\n$ bin/runtests.sh \n.............................................................S.....S..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n===============================================================================\n[SKIPPED]\nNo FTP server available for testing\n\nscrapy.tests.test_contrib_feedexport.FTPFeedStorageTest.test_store\n===============================================================================\n[SKIPPED]\nAWS keys not found\n\nscrapy.tests.test_contrib_feedexport.S3FeedStorageTest.test_store\n-------------------------------------------------------------------------------\nRan 894 tests in 35.569s\n\nPASSED (skips=2, successes=892)\n```\n\nThe main difference is that my system counts 894 tests in total not 845. not sure what is the difference.\n\nand about the other errors with `testegg` and `mybot.egg`, those files are generated by testsuite.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5220428/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5220530", "body": "I downloaded Scrapy-0.14.2 from pypi and it miss `mybot.egg` and `test.egg`, to amend myself these files are not generated by testsuite.\n\nThe bug is in setup.py not shipping egg files used in testsuite as datafiles. So uploaded tarball to pypi doesn't contain this files.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5220530/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5220783", "body": "missing files comparing pypi tarball with github clone:\n\n``` diff\n--- github-cloned   2012-04-19 09:49:21.763577174 -0300\n+++ pypi-tarball    2012-04-19 09:49:17.715557107 -0300\n@@ -34,7 +34,6 @@\n scrapy/tests/test_utils_misc\n scrapy/tests/test_utils_misc/test_walk_modules\n scrapy/tests/test_utils_misc/test_walk_modules/mod\n-scrapy/tests/test_utils_misc/test.egg\n scrapy/tests/test_djangoitem\n scrapy/tests/test_spidermanager\n scrapy/tests/test_spidermanager/test_spiders\n@@ -58,7 +57,6 @@\n scrapy/commands\n scrapy/xlib\n scrapy/xlib/pydispatch\n-scrapy/xlib/pydispatch/license.txt\n scrapy/templates\n scrapy/templates/spiders\n scrapy/templates/spiders/csvfeed.tmpl\n@@ -75,4 +73,3 @@\n scrapyd\n scrapyd/default_scrapyd.conf\n scrapyd/tests\n-scrapyd/tests/mybot.egg\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5220783/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5236836", "body": "FYI I just released Scrapy 0.14.3 with a fix to include egg files in pypi tarball http://pypi.python.org/pypi/Scrapy\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5236836/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5236919", "body": "> Having observed the sources further, the github source has far more content than the tarball. \n> I suppose the tarball was  cutdown to just what was required.\n> \n> The 932 tests come from the github source.\n\n932 tests cases are in in master branch, aka development branch or Scrapy 0.15\n\n> Ran 894 tests in 28.456s comes from the tarball.\n\nthis is the number of test cases in stable branch, aka Scrapy 0.14\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5236919/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5236940", "body": "> -- the args --reporter=text scrapy/tests/test_utils*py appear to impede when run from an ebuild. \n> It does ok with just bin/runtests.sh.\n> \n> exceptions.AssertionError: '{\"spider:16b6ad0:name1\": \"1000.12\"}' != '{\"spider:16b6ad0:name1\": 1000.12}'\n> \n> appears to be a red herring. It was initially the constant fail but it passes now anyway. \n> For some reason it was attempting to compare a number with a string, hence the sed statement.\n\nif the sed statement is required to pass this test in your system then it is a bug in Scrapy, but I understand this failure is not happening anymore may after w3lib 1.1 upgrade?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5236940/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5237012", "body": "btw, the weird message about xmlXPathEval failure:\n\n> ...........S......................S..xmlXPathEval: evaluation failed [not sure what that means]\n\nit is harmless and comes from [this test case](https://github.com/scrapy/scrapy/blob/master/scrapy/tests/test_selector.py#L202) that checks failure in case of invalid xpath query\n\neasily reproducible using:\n\n```\n$ bin/runtests.sh scrapy.tests.test_selector_libxml2.Libxml2XPathSelectorTestCase.test_selector_invalid_xpath\nscrapy.tests.test_selector_libxml2\n  Libxml2XPathSelectorTestCase\n    test_selector_invalid_xpath ... Unfinished literal\nxmlXPathEval: evaluation failed\n                                       [OK]\n\n-------------------------------------------------------------------------------\nRan 1 tests in 0.008s\n\nPASSED (successes=1)\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5237012/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5237115", "body": "> However this may not be over.  Running from the ebuild, it reports tests passing, but it errors out and bails out of the emerge.  Running the exaggerated bin/runtests.sh --spew gives some info but I don't understand it.\n> \n> method **delete of threading._MainThread at 140347049744656\n> function _run_exitfuncs in /usr/lib64/python2.7/atexit.py, line 13\n> function shutdown in /usr/lib64/python2.7/logging/__init**.py, line 1616\n> Exception ImportError: 'No module named twisted.python' in <function _remove at 0x7fa51c501938> ignored\n> - ERROR: dev-python/scrapy-0.14.2 failed (test phase):\n> -   Testing failed with CPython 2.7 in testing() function\n\nI have no idea, the only pointer there is the failed import for `twisted.python`. \nI grep'ed around stdlib logging module, twisted and scrapy and couldn't find a function named `_remove` either. \nwhat twisted version are you using?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5237115/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5245053", "body": "Hi idella, can you confirm that Scrapy 0.14.3 tarball from pypi doesn't fail to pass tests in your ebuild, at least for missing egg files?\n\nnot sure how to help with the other ebuild failure you report, I am not familiar with emerge myself.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5245053/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5247996", "body": "found `_remove` function in twisted 12, but what bring my attention from your output was:\n\n```\narchtester dev-python # qlist twisted | grep \"twisted/python\" | grep 2.7\n/usr/lib64/python2.7/site-packages/twisted/python/roots.py\n/usr/lib64/python2.7/site-packages/twisted/python/randbytes.py\n```\n\nit suggest that `twisted/python/__init__.py` is missing and that can lead to ImportError of `twisted.python`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5247996/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5251744", "body": "hey idella, thanks for following this issue to the end.\n\nI don't like the idea of requiring patching in your side, at least after 0.14.4 release you can drop ebuild's runtest.sh patch 66de3d175781b47ee0e7bdd2975dbdab694a31d8\n\nthanks again!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5251744/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5261961", "body": "```\n{'category': <Category: Electro-m\u00e9nager :: TV>,\n```\n\nHas `Category` class a `__unicode__` method instead of `__str__` ?\n\nwhat is the output of `unicode(item['category'])`?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5261961/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5313412", "body": "What about the exception, what is the output of repr(unicode(exception)) ? \n\nthere aren't too many options in that line, I can keep guessing but without details won't work.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5313412/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5739174", "body": "thanks for reporting but I have to close this issue 'cos lack of feedback\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5739174/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5359839", "body": "I suppose you are scraping with scrapy 0.14, can you provide the url to test this in scrapy 0.15?\n\nwe have reworked encoding detection in 0.15 (development branch) and I want to try it with your issue.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5359839/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5372116", "body": "I just tried the url in Scrapy 0.15 (master branch)\n\n```\n$ scrapy shell http://people.tamu.edu/~afayazi/cr/main.html\n2012-04-26 22:58:50-0300 [scrapy] INFO: Scrapy 0.15.1 started (bot: scrapybot)\n...\n>>> response.encoding\n'utf-8'\n\n>>> response.body[:10]\n'\\xef\\xbb\\xbf<!DOCTY'\n\n>>> response.body_as_unicode()[:10]\nu'\\ufeff<!DOCTYPE'\n\n>>> response._body_inferred_encoding()\n'utf-8'\n\n>>> response._body_declared_encoding()\n\n>>> response._headers_encoding()\n\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5372116/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5372146", "body": "and Scrapy 0.14.3 (0.14 branch)\n\n```\n$ scrapy shell http://people.tamu.edu/~afayazi/cr/main.html\n2012-04-26 23:04:47-0300 [scrapy] INFO: Scrapy 0.14.3 started (bot: scrapybot)\n...\n\n>>> response.encoding\n'ascii'\n\n>>> response.body[:10]\n'\\xef\\xbb\\xbf<!DOCTY'\n\n>>> response.body_as_unicode()[:10]\nu'\\ufffd\\ufffd\\ufffd<!DOCTY'\n\n>>> response._body_inferred_encoding()\n'ascii'\n\n>>> response._body_declared_encoding()\n\n>>> response._headers_encoding()\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5372146/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5373082", "body": "I tried HtmlXpathSelector from 0.14.3 and it works for me\n\n```\n>>> response.encoding\n'ascii'\n>>> response.body_as_unicode()\nu'\\ufffd\\ufffd\\ufffd<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\\r\\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\\r\\n\\r\\n<head>\\r\\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\" />\\r\\n<title>Untitled 1</title>\\r\\n</head>\\r\\n\\r\\n<body>\\r\\n\\r\\n<p><a href=\"sub1.html\">sub1.html</a></p>\\r\\n<p><a href=\"sub2.html\">sub2.html</a></p>\\r\\n<p><a href=\"sub3.html\">sub3.html</a></p>\\r\\n\\r\\n</body>\\r\\n\\r\\n</html>\\r\\n'\n>>> hxs.select('//a/text()').extract()\n[u'sub1.html', u'sub2.html', u'sub3.html']\n>>> hxs.__class__\n<class 'scrapy.selector.libxml2sel.HtmlXPathSelector'>\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5373082/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5725820", "body": "Bugfix released as part of 0.14.4 http://doc.scrapy.org/en/latest/news.html#id1\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5725820/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5725882", "body": "it is detected as body declared encoding:\n\n``` pycon\n$ scrapy shell http://people.tamu.edu/~afayazi/cr/main.html\n>>> response.encoding\n'utf-8'\n>>> response._body_declared_encoding()\n'utf-8'\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5725882/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12843138", "body": "right, except DOWNLOAD_DELAY should be a float: `60 / 40.0`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12843138/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5625183", "body": "+1 too\n\nalong this changes, what do you think about dropping [_request_fingerprint_cache](https://github.com/scrapy/scrapy/blob/master/scrapy/utils/request.py#L17)\nand store the computed fingerprint in the meta as suggested in this ticket\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5625183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5625843", "body": "just figured out a problem with storing fingerprint in `request.meta` :-(\n\n`request.meta` is propagated by `request.replace()` which is commonly used to redirect, retry or simply propagate itermediate results stored in meta to other urls\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5625843/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12843228", "body": "dusty.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12843228/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5552652", "body": "I can't find a test case for the bugfix, changes to test_selectors.py are good and can pass as part of this pull request, but they don't test the regression. \n\ncan you add a test case for this?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5552652/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5552830", "body": "thanks! \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5552830/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12843316", "body": "I can't find the cause and we lack feedback.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12843316/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5673663", "body": "I don't get why 0 should disable extensions of any kind, it is an acceptable priority value to order downloader and spider middlewares, negative priorities are also accepted. None has been the only documented way to do it, what is wrong with it?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5673663/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5791391", "body": "do you really need multiples start_urls to be passed from command line argument? \n\nI'd prefer using arg_to_iter in start_requests instead\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5791391/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12843912", "body": "dusty\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12843912/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12843394", "body": "`canonicalize_url` is following expected behavior \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12843394/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11953251", "body": "although the functionality looks OK, I don't see an use case for this pull request, closing.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11953251/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11955612", "body": "This PR doesn't merge anymore and noone else has showed interest in it.\nWe can reopen it if an updated patch is proposed.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11955612/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/6302225", "body": "removing `<noscript>` tags for meta refresh is considered a feature because it is commonly used to trap bots that doesn't support javascript. \n\nrelevant change ec1ef0235f9deee0c263c9b31652d3e74a754acc\n\nthat said, I am OK to accept a patch that controls noscript tag removal on a per request basis\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/6302225/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12843548", "body": "lack of feedback.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12843548/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/6338335", "body": "something like the following still test for the correct behaviour of urlparse\n\n``` diff\ndiff --git a/scrapy/xlib/urlparse_monkeypatches.py b/scrapy/xlib/urlparse_monkeypatches.py\nindex 5228b65..b614119 100644\n--- a/scrapy/xlib/urlparse_monkeypatches.py\n+++ b/scrapy/xlib/urlparse_monkeypatches.py\n@@ -1,7 +1,10 @@\n-from urlparse import urlparse, uses_netloc, uses_query\n+from urlparse import urlparse\n\n # workaround for http://bugs.python.org/issue7904\n if urlparse('s3://bucket/key').netloc != 'bucket':\n+    from urlparse import uses_netloc\n     uses_netloc.append('s3')\n+\n if urlparse('s3://bucket/key?key=value').query != 'key=value':\n+    from urlparse import uses_query\n     uses_query.append('s3')\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/6338335/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11959894", "body": "what can we do with this PR?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/11959894/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/6734060", "body": "You can easily extend ImagePipeline in your project and override HEADERS class attribute to your own needs.\n\n``` python\n\nfrom scrapy.contrib.pipeline.images import ImagesPipeline\n\nclass MyImagesPipeline(ImagesPipeline):\n\n    HEADERS = {'myheader': 'myvalue'}\n\n```\n\nremember to add it to your ITEM_PIPELINES settings\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/6734060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355950", "body": "@pablohoffman: \"a global limit **and** a per-domain limit\"\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355950/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355969", "body": "\"to run **Scrapy** from a script\" right?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355969/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355981", "body": "\"if possible, use...\"\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355981/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379551", "body": "The issue this commit tries to address is real, but the fix introduces a new bug when items contains unicode values not encodeable with default encoding.\n\nIt's a twisted shame that `log.err` doesn't call `_safeFormat` on `_why`  here http://twistedmatrix.com/trac/browser/tags/releases/twisted-12.3.0/twisted/python/log.py#L328\n\nThe good news is that we can call _safeFormat ourself on our `scrapy.log.err` wrapper\n\nwhat do you think?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379551/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379680", "body": "to make it more clear, this is what I am proposing https://gist.github.com/4445963\n\nit has an obvious dislike because of private function import and it doesn't improve on lazy evaluation side\n\nafter all, I am not sure. :sake: \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379680/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379712", "body": "Now I figured out that previous Gist was an implementation of `format` instead of `_why`\n\nhere is the `_why` version https://gist.github.com/4446084 \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379712/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2966628", "body": "I guess you refer to Scrapy 0.16. is it the same case on Scrapy 0.18? \n\nThe motivation was to avoid hiding the real error that makes debugging images error a bit harder.\nCan you provide an test case that reproduces \"image file is truncated\" error and that is valid case to silence instead of propagating and logging the full traceback?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2966628/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2976147", "body": "hey @a7ch3r, you welcome.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2976147/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3048440", "body": "unused import?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3048440/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3049106", "body": ":beers:\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3049106/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4647682", "body": "@redapple: what about gzip,x-gzip,deflate?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4647682/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4647837", "body": "Is it time to move forward and remove `x-gzip` from default `Accept-Encoding`? :)\n\nChrome doesnt send x-gzip anymore:\n\n```\nGET / HTTP/1.1\nHost: example.com\nConnection: keep-alive\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.76 Safari/537.36\nDNT: 1\nAccept-Encoding: gzip,deflate,sdch\nAccept-Language: en-US,en;q=0.8,es-419;q=0.6,es;q=0.4\n```\n\nFirefox neither:\n\n```\nGET / HTTP/1.1\nHost: example.com\nUser-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:24.0) Gecko/20100101 Firefox/24.0\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: en-US,en;q=0.5\nAccept-Encoding: gzip, deflate\nConnection: keep-alive\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4647837/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4956151", "body": "IDEA: as classes are declared in order, with parents been defined before childs, it's possible to set a class attribute into the first class built from this metaclass.\n\nand I think we can simplify finding the new class name by always pointing to the ~~first~~ second element of the MRO.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4956151/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104487", "body": "PYPI says it was released 2014-01-17 :) \n\n![image](https://f.cloud.github.com/assets/37369/1943916/f8783836-7fb1-11e3-9348-0ec972f5b626.png)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104487/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/7238585", "body": "nice!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/7238585/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/7239229", "body": "wohoo! \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/7239229/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/7430073", "body": "good catch.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/7430073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/11352162", "body": "> or does this commit mean that starting from this point features would go to 1.1 release?\n\nthat's right.  \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/11352162/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13024812", "body": "what do you think if instead of checking response type it checks the for response instance identity. \n\nI mean, create a Response object in test case scope, return it from `process_request()` and check it is the same response with `self.assertIs()` in this line.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13024812/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13114145", "body": "I think it is sensible to escape 25 too, can you provide an example url and even better a fix ?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13114145/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13176598", "body": "Scrapy can't use FifoDiskQueueu or FifoSQLiteQueue directly, it wraps queuelib queues with a serializer, see https://github.com/scrapy/scrapy/blob/master/scrapy/squeues.py#L31-L34\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13176598/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13176617", "body": "the settingi s https://github.com/scrapy/scrapy/blob/master/scrapy/settings/default_settings.py#L227\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13176617/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13176831", "body": "No need to modify Scrapy, you can do that in your project and point to it\nusing settings.\n\n2015-09-11 4:40 GMT-03:00 foresightyj notifications@github.com:\n\n> Oh. I completely overlooked this file and set SCHEDULER_DISK_QUEUE to\n> FifoSQLiteQueue directly . Thanks.\n> \n> I manually added the following two lines into squeues.py:\n> \n> PickleFifoSQLiteQueue = _serializable_queue(queue.FifoSQLiteQueue,\n>                                         _pickle_serialize, pickle.loads)\n> PickleLifoSQLiteQueue = _serializable_queue(queue.LifoSQLiteQueue,\n>                                         _pickle_serialize, pickle.loads)\n> \n> And used SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleFifoSQLiteQueue' in\n> the settings.py.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/scrapy/scrapy/commit/9a64d8ff97f2b188d2be8a1d7944a47790adda2c#commitcomment-13176815\n> .\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13176831/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13299547", "body": "@Zephor5 I think you are right, it is a rare dependency only because of Scrapy extending queuelib's testsuite but it is fair to point this dependency for tests requirements. Fix https://github.com/scrapy/scrapy/commit/a3390afc66134e77f98ae3bfae7bc23479ed8566\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13299547/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13311048", "body": "`from_crawler` is the standard way to do this, this method is supported by all components that load middlewares, extensions and spiders. Even core components like scrapy.core.scheduler.Scheduler use it.\n\nIn any case, the backward compatible import for pydispatch is a good idea\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13311048/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/15694635", "body": "@kalessin do you mind providing a test case?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/15694635/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/20681078", "body": "what about using a hostname that implies failure intend: `http://dns.resolution.falure./` or `http://nonexistent.domain-name.`?\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/20681078/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670796", "body": "Can we avoid checking for \" xmlns \" in every loop iteration? it is an invariant check. move it to the top of the function.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670796/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670810", "body": "alternative and less boilerplate: `inputs.extend(formdata.iteritems())`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/675942", "body": "I tried removing the call to `_nons()` and still passed all tests.\nWhat about removing it completely of figuring out a test case that justifies its use? \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/675942/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/847002", "body": "not testing for string before splitting?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/847002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2516457", "body": "I dont find in what cases the cached response can be a Request object\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2516457/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2563966", "body": "This change attach the `request` to the exception under `response.value` where `response` is a `Failure` instance\n\nI don't think we need another way to access the `request` from spider errbacks, there is another way already implemented that attaches `request` directly to `Failure` instance  at [scrapy/core/scraper.py#L139](https://github.com/scrapy/scrapy/blob/master/scrapy/core/scraper.py#L139)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2563966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2831509", "body": "I think \"Jinja2\" should be an optional requirement, it is not needed to run scrapy (not d), and it is even better if we do client side templating IMHO. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2831509/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3877816", "body": "deepcopy is imported but not used\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3877816/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3964547", "body": "to keep the reference to slot alive :(\nit needs a big comment.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3964547/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3964624", "body": "there are two main references that can keep slot alive:\n1-  _process_queue is particulary important for slots with delays\n2- and _deactivate closure added as callback in _enqueue_request\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3964624/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5269044", "body": "it is simpler to use \"hostname\" attribute from `urlparse_cached` result\n\n```\n>>> urlparse('http://scrapytest.org:1104/path').netloc\n'scrapytest.org:1104'\n>>> urlparse('http://scrapytest.org:1104/path').hostname\n'scrapytest.org'\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5269044/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6115017", "body": "I think it is better to let BaseSpider handle the special keyword, it sets keyword arguments as spider attributes already.\nAlso, `use_alternate_links` is a better name IMHO.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6115017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6115027", "body": "checking for `True` equality is considered a mistake, you check for identity like `if alt is True:` or usually just for bool `if alt:` \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6115027/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6115038", "body": "worth testing for an alternate link without `href`.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6115038/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6229389", "body": "`remove_comments=True` makes sense but it is new and unrelated to this change right?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6229389/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6229392", "body": "perfect! docs are great :)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6229392/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6300930", "body": "It's ok, but if remove_comment is required we should add its own test case (aside of leaving the comment in this test)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6300930/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6355041", "body": "+1 to do it like it is done for download_timeout\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6355041/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6526601", "body": "This is not really backward compatible, the dict will randomize the pipelines as all are converted with the same priority changing expected pipeline order. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6526601/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6952246", "body": "it's depends on the content from the url right?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6952246/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6952321", "body": "Good point. Happens that we are not documenting how to instanciate with \"text\" argument.\n\nAnd this is the tutorial, things should be clearer on selector's reference documentation \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6952321/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6958314", "body": "@stav: thanks, I'll fix.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6958314/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7131763", "body": "I'd fix the library as part of this PR, bundled pydispatch library already suffered modifications.\n\nWe can look for alternatives like proposed by #8 or using an non-bundled library later.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7131763/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7761521", "body": "This is correct.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7761521/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7764579", "body": "Debug why `scrapy.responsetypes.ResponseTypes` doesn't detect it as text/xml.\n\ncan you provide more details, an url would be ideal, but headers should be enough. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7764579/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7761701", "body": "`response.body` is always an encoded string (strictly speaking a bytearray) but I am not sure default decoding from UTF-8 is a good way to deal with this case, as most of the times when the response is not TextResponse it can't be parsed as html/xml, and in case it is xml/html parseable there is a bug in the code that detects response types.\n\nMy vote is to raise ValueError instead of forcing UTF-8 decoding.  \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7761701/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7767231", "body": "I must admit there is nothing useful on those headers :)\n\n`scrapy.utils.response.body_or_str` is used only by `scrapy.utils.iterators`, I am temped to move it there as a private function.\n\nAnd in that case may be assume UTF-8 in case of unknown encoding for `Response`. \n\nAnother option is to retry detection based on body content with `responsetypes.from_args(body=response.body)`, I am not sure why this is not done in http handlers. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7767231/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7770740", "body": "It should be reduced to:\n\n``` python\n\n    if isinstance(obj, Response):\n        return obj.body.decode('utf-8') if unicode else obj.body\n\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7770740/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7770963", "body": "@llonchj : Do you want to update this PR or should I take care of it?\n\nit should move `body_or_str` as private function to `scrapy.utils.iterators` and try to decode \"binary\" responses using `utf-8` as best effort. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7770963/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7792003", "body": "s/some_xpath_selector/some_selector/\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7792003/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7811832", "body": "I think it is possible that someone is using it because even in this case `response` becomes part of the context\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7811832/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7812168", "body": "I am tempted to suggest moving `self._check_selector_method()` check here. :)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7812168/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7812180", "body": "and check for selector instance also here of course.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7812180/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7812213", "body": "`RuntimeError` coverage needs to be improved for all the methods that may raise it \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7812213/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7812245", "body": "`_get_values` is asking for a renaming to `_get_xpathvalues` to be consistent with `_get_cssvalues`. Although, I am tempted to deprecate it as people extending `XpathItemLoader` may be using it. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7812245/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7898661", "body": "Any reason not to reuse `XPath` class to compile the query expression in case it is not one?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7898661/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6577049", "body": "It's probably better to return the proxy response including the http status: 403s, 500s, ... whatever proxy returns that is not a 200.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6577049/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6577314", "body": "In case the response status line isn't a 200, I think we should restore `dataReceived` and feed it with `bytes`, that should returns a http response with correct status to the client.\n\nDo you think we need a check on what `bytes` are discarded, just in case we switch transport before all proxy related bytes are read?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6577314/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8406038", "body": "hey @kmike, sorry for summoning you. I wanted to ask if you can review for Python 3 forward compatibility before merging. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8406038/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8497745", "body": "what is the advantage of using a class attribute for this settings instead of directly referencing it in the constructor like `AJAXCRAWLABLE_MAXSIZE` ? \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8497745/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8409118", "body": "fair point, although the real issue with #473 is that Referrer is set using setdefault() \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8409118/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8409203", "body": "addressed by https://github.com/dangra/scrapy/commit/f7aa0dd51825b33ad9b33b2648a1488893b6b8c5\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8409203/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8409223", "body": "addressed by https://github.com/dangra/scrapy/commit/3e84c70ca7097b067df1b6faabc485fd2012fe67\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8409223/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574692", "body": "Autothrottle extension doesn't alter slot concurrency in any way (it used to but it doesn't now), it only changes slot's download delay.\n\nI think the missunderstanding with concurrency limits is because current algorithm alter delays so it follows response latency which leads to 1 concurrent request with rare increases to 2 or 3 concurrent requests when latency changes abruptly.   \n\nConcurrency limits are imposed by the downloader itself, it is not managed by this extension at all.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574692/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574708", "body": "The change is good, it reflects current state.\n\nBut to be honest, I think both settings (CONCURRENT_REQUESTS_PER_IP and CONCURRENT_REQUESTS_PER_DOMAIN) should be deprecated in favour or a single setting name CONCURRENT_REQUESTS_PER_SLOT, plus another to control the slot assignment policy.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574708/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574716", "body": "\"website\" is not accurate, the real delay is applied to the requests classified per slot, happens that slots are assigned per website (~domain) by default. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574716/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574753", "body": "seems so.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574753/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574763", "body": "ok, website should be enough to explain the idea.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574763/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574873", "body": "but we do have a global concurrent limit and no global download delay\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574873/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574887", "body": "I like `CONCURRENT_REQUESTS` analogy to `DOWNLOAD_DELAY`, what about a better name for the global concurrency setting?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574887/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574905", "body": "I think it is a documentation task defining what is a slot, it's a direct reference to how downloader enqueue requests. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8574905/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8572220", "body": "Instead of the magic number 4, don't you prefer to compute it at module import `len(Spider.__mro__) + 1` ? \n\nobject_ref is an artifact there and we may make it optional (shortcut for object) or directly drop it.\nAnd as this PR doesn't add a testcase for the warning it will be hard to catch when MRO changes. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8572220/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8572266", "body": "the approach looks very appealing! I learnt a new trick today. thanks.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8572266/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8588799", "body": "`clsdict or {}`  to avoid propagating default None value to type() \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8588799/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8588901", "body": "seems the dict is copied so updates to clsdict are not propagated\n\n```\n>>> clsdict = {'baz': 1}\n>>> cls = type('Foo', (), clsdict)\n>>> clsdict['other'] = 'qux'\n>>> cls.__dict__\n<dictproxy {'__dict__': <attribute '__dict__' of 'Foo' objects>,\n '__doc__': None,\n '__module__': '__main__',\n '__weakref__': <attribute '__weakref__' of 'Foo' objects>,\n 'baz': 1}>\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8588901/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7899075", "body": "> And there's the concern about further modifications to .namespaces if it's a public attribute.\n\nI think this is enough reason not to assign `self._default_namespaces` to `self.namespaces` :)\n\nany post namespace registered post selector creation will propagate to future selectors \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7899075/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8857568", "body": "s/midrodata/microdata/?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8857568/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8968653", "body": "fixed\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8968653/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8968668", "body": "good catch\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8968668/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8968851", "body": "indeed, the sentences are collected from pull request titles and commit messages. \n\nI'll try to reword all entries in present tense.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8968851/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9010097", "body": "good catch, somehow I removed `&&` from that line. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9010097/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9010354", "body": "any ideas on a replacement for that sentence? \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9010354/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9010800", "body": "Updating package lists is required for the first time, I think the warning tries to cope with package upgrades.\n\nNowadays Ubuntu does a better job keeping package lists uptodate automatically, the warning may be more confusing than useful.\n\nwhat about removing it or go for something like:\n\n```\n.. note:: Please note that these packages are updated frequently. If you are trying \n   to upgrade Scrapy, run ``sudo apt-get update`` to update package list first.\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9010800/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9010848", "body": "or more clear:\n\n```\n.. note:: Please note that these packages are updated frequently, repeat step 3 If you are trying \n   to upgrade Scrapy.\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9010848/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9225783", "body": "Let collect stats by default, and no need for this flag.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/9225783/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "jesuslosada": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/61c0b1478284b02a4fcfd2cc4931587c348c5d3a", "message": "Fix typo in comment"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/a0836b8fd9720a9439cb3b940aca53b6844a094b", "message": "Fix link in news.rst"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "raphapassini": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/a1cc5a63d3e253c325159fdc6ebf4cd3faa37c49", "message": "Add mention to dont_merge_cookies in CookiesMiddlewares docs (#2999) (#3030)\n\nAdd mention to dont_merge_cookies in CookiesMiddlewares docs (#2999)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hugovk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/cbcf80b98ff66db1ccf625fa52c4de8935331972", "message": "Fix typo\n\n[CI skip]"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f11c21c6fc62b64a2bbee0e19e2098ed6257cf19", "message": "Test on Python 3.4"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/44623687ab8936c5696f68f74e438a2891880c82", "message": "Drop support for EOL Python 3.3"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Jane222": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/22c68baf990f15d249f38c481f24a984977be3e5", "message": "url_pattern is now being compiled before entering the loop"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/91ff194d1e9477d2196817ea1dc8beb220c3e058", "message": "looping over allowed_domains directly instead of via index"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/62a626102877de4998538717f34e61d2f7d2622c", "message": "Issues a warning when user puts a URL into allowed_domains (#2250)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Jane333": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/454d5e57333e9f33c8d684e4e21f8f7e9493f310", "message": "checking for subclass of URLWarning instead of checking error message text when URL in allowed_domains"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/8ec3b476b03d6b8424f6dfc556758392e7a5a61f", "message": "triggering a warning when user puts URL in allowed_domains now covered by test"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "KosayJabre": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/5441cc18e43cba6b8196ece49d6b454badb246f0", "message": "Separated import statements\n\nJust separated the import statements. Tiny change - testing GitHub!"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "colinmorris": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/23e571e860729fad1f4351cde69b77d88837e628", "message": "fix issues identified in review"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/8a7552370de2d31fbd3564e47fa92049c8efee6f", "message": "revise/modernize item exporter example in docs"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "codeaditya": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/df7e0a4315f9db2c74fa9e9a0654f44277da2e55", "message": "Use https link in default user agent"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/dae7b1cdd06649db3c692eccd70195a499733448", "message": "Migrate all subdomains on readthedocs.org to readthedocs.io"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/97d047a055b3af080047768b196ce677fbfaa12e", "message": "Fix link for Tox"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/23c7437e4629199e8ee1ae6bcdf75b7062466010", "message": "Fix link for 'XPath and XSLT with lxml'"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9d9d83a8c31b6a18d7aaac35a30ffb69db4bb81d", "message": "Use https links wherever possible"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9dd680d5c94340ac308f1450d9d3dc226a015326", "message": "Use https for external links wherever possible in docs"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9cdf34b7c791359b1f86678f758cf64368723c53", "message": "Link \"Debugging in Python\" article to its new location\n\nReference: https://web.archive.org/web/20170203104051/http://www.ferg.org/papers/debugging_in_python.html"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "weldon0405": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/95815d27e89a6eea4676358697e2846959e4725b", "message": "updated file structure to include middlewares.py"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/169dc2860e9f7054c50c84f5adcd8a0d5afe161e", "message": "Update tutorial.rst startproject files\n\nAdded middlewares.py to accurately reflect the file structure created by \"scrapy startproject tutorial\""}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "NoExitTV": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/9b8503011e1da0507e82e15631194ed99b7e699a", "message": "Changed log message to include information about request as user djunzu commented"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/345d948f2f55ad81ec7de7cb1f5619d80971a6c0", "message": "Changed the log message to make it more clear. As requested in issue #2927"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/938bc18405ca2cf60836bb8490c391c0fe445af1", "message": "Changed the log message to make it more clear. As requested in issue #2927"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/e914556adf8e556d4184db415c49266cc4c91bf5", "message": "Changed the log message to make it more clear. As requested in issue #2927"}], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2955", "title": "Handle \"invalid\" relative URL issue #1304", "body": "Did some minor tweaks on how scrapy handle relative URL's as discussed in #1304 \r\n\r\nTested it with some basic code in the scrapy shell:\r\n```\r\n>>> resp = scrapy.http.response.html.HtmlResponse('http://www.example.com',\r\n...      body='''<html>\r\n...                  <body>\r\n...                      <a href=\"../index1.html\">Link1</a>\r\n...                      <a href=\"../../index2.html\">Link2</a>\r\n...                      <a href=\"index3.html\">Link3</a>\r\n...                      <a href=\"other_html/../index4.html\">Link4</a>\r\n...                      <a href=\"other_html/folder2/../index5.html\">Link5</a>\r\n...                      <a href=\"other_html/index6.html\">Link6</a>\r\n...                      <a href=\"../other_html/index7.html\">Link7</a>\r\n...                 </body>\r\n...          </html>''')\r\n>>> links = scrapy.linkextractors.LinkExtractor().extract_links(resp)\r\n>>> links\r\n[Link(url='http://www.example.com/index1.html', text='Link1', fragment='', nofollow=False), Link(url='http://www.example.com/index2.html', text='Link2', fragment='', nofollow=False), Link(url='http://www.example.com/index3.html', text='Link3', fragment='', nofollow=False), Link(url='http://www.example.com/index4.html', text='Link4', fragment='', nofollow=False), Link(url='http://www.example.com/other_html/index5.html', text='Link5', fragment='', nofollow=False), Link(url='http://www.example.com/other_html/index6.html', text='Link6', fragment='', nofollow=False), Link(url='http://www.example.com/other_html/index7.html', text='Link7', fragment='', nofollow=False)]\r\n```\r\n\r\nPassed the same unit tests as the original code when running tox\r\n```\r\n========================================================= 1679 passed, 6 skipped, 14 xfailed in 458.89 seconds =========================================================\r\n\r\n_______________________________________________________________________________ summary ________________________________________________________________________________\r\n\r\n  py27: commands succeeded\r\n  congratulations :)\r\n```\r\n\r\nI believe that scrapy now handle relative url's as expected in python 2.7.13\r\nWhat are your thoughts?", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rodrigc": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/fc406801f1783392935fcd7faf603d8339c74675", "message": "ESMTPSenderFactory takes a message of bytes"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/00c81a32ee58d0a59c14373471bf152c46131aec", "message": "Bump Twisted requirement to 17.9.0 to catch many Python 3 fixes."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lagenar": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/59c3f6f095d7605a825a90f376d41493a78e5da7", "message": "Fix typos in tests"}], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2889", "title": "Refactoring to make possible to optimize item loaders' performance", "body": "Hello, the aim of these changes is to allow to optimize item loaders that don't need to use a context.\r\nI'm working on a project that has spiders that load hundreds/thousands of items per request and they are using item loaders and I noticed that there's a big difference in performance when using plain dicts instead of item loaders for these cases.\r\n\r\nAfter profiling I found that a good part of the time was spent in the  wrap_loader_context function which inspects the processors arguments to see if it accepts the loader_context param. The change I suggest to do is to move this function to a method(also this is done for the MapCompose loader) so that it's easier to subclass ItemLoader and redefine that method to just return the same function it's given when the item loader doesn't use contexts.\r\n\r\nHere's the test I did to verify the performance improvements. In my pc the first loop runs in 46 seconds and the second one in 21 seconds.\r\n\r\n```import scrapy\r\nimport time\r\nfrom scrapy.loader import ItemLoader\r\n\r\nclass OptimizedLoader(ItemLoader):\r\n    def _wrap_loader_context(self, proc):\r\n        return proc\r\n\r\nclass Loader(ItemLoader):\r\n    pass\r\n\r\nclass Product(scrapy.Item):\r\n    item = scrapy.Field()\r\n    \r\nclass Spider(scrapy.Spider):\r\n    name = 'test'\r\n    start_urls = ['http://google.com']\r\n\r\n    def parse(self, response):\r\n        count = 500000\r\n        start = time.time()\r\n        for x in range(count):\r\n            loader = Loader(item=Product())\r\n            loader.add_value('item', x)\r\n            i = loader.load_item()\r\n\r\n        end = time.time()\r\n        self.log('non optimized {}'.format(end - start))\r\n        self.log('average {}'.format((end - start) / count))\r\n\r\n        start = time.time()\r\n        for x in range(count):\r\n            loader = OptimizedLoader(item=Product())\r\n            loader.add_value('item', x)\r\n            i = loader.load_item()\r\n\r\n        end = time.time()\r\n        self.log('optimized {}'.format(end - start))\r\n        self.log('average {}'.format((end - start) / count))\r\n```", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rhoboro": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/d4555b2bcc387292e5fd5bd8321c946e2e374fb7", "message": "update docs for supporting google cloud storage"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/ee166ec44f38da7f5b99c6c164a7c6ff02b37c16", "message": "Support for ImagesPipeline"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/e5d4364b2a0e7ae205605905ac0c5ac6fd8d15db", "message": "Add tests for GCS Storage"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/d71a0634039d637dc10509ebb63fa2f4ef595ebb", "message": "Support for Google Cloud Storage"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "superyyrrzz": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/088b80d41a12a7b79e440cc4d4a3aae678d5c4af", "message": "minor fix typo"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stav": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/3637b75a6702cb3fb4962477c0f2ec38e366f3e2", "message": "[Doc] Update Response.body type"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/12718894", "body": "5000-th commit, nice.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/12718894/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2853057", "body": "Actually yes, this code is necessary because the defaultdict will get created if no entry exists and empty defaultdicts do not equate to False as described in comment [11469555](https://github.com/scrapy/scrapy/pull/208#issuecomment-11469555):\n\n> This is because get_output_value() calls proc(self._values[field_name]) and since self._values is a defaultdict, self._values[field_name] gets created, which then overwrites the self.item[field_name] data later in load_item().\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2853057/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3039860", "body": "I was thinking of using flatten:\n\n```\nfor f, v in values.iteritems():\n    self.add_value(f, flatten(arg_to_iter(v)))\n```\n\nbut perhaps some users may not want it flattened recursively, otherwise looks good as you have it Pablo (vales=values).\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3039860/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6952492", "body": "```\nit automatically choice\n```\n\nit automatically chooses\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6952492/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "revolter": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/aab98080a06281ca3a88646990b81b92f492c517", "message": "Add option to disable automatic log handler install"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cclauss": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/b8fabeed8652d22725959345700b9e7d00073de4", "message": "ur'string' not needed in Py 2, syntax error in Py3\n\nThis instance was missed in #2909 --> ur'Scrapy developers' --> u'Scrapy developers'"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/b7022360824cd1a8aa19fb0d65ea01700f06a208", "message": "ur'string' not needed in Py 2, syntax error in Py3\n\nConvert `u'(.*)\\:\\d+\\:\\s\\[(.*)\\]\\s(?:(.*)\\sto\\s(.*)|(.*))'`--> `u'(.*)\\:\\d+\\:\\s\\[(.*)\\]\\s(?:(.*)\\sto\\s(.*)|(.*))'`to be compatible with both Python 2 and Python 3.  See #2891"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9f16f040b661f18f2e61a35427199c099bfd2f90", "message": "ur'string' not needed in Py 2, syntax error in Py3\n\nConvert `ur'Scrapy Documentation'`--> `u'Scrapy Documentation'`to be compatible with both Python 2 and Python 3.  See #2891"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/0a69a32b5a2cab7575fdbb5f2cd4b7c7b900aabc", "message": "Force Travis CI to test again"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/c016a4309dbb045c17842e329043b7d9951e8f14", "message": "# noqa to close #2836\n\nMarks #2836 as will not fix."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/33dfac50185b1940dda89bec3e3480a7e76e9ca7", "message": "xrange() --> range() for Python 3\n\nEither this PR or #2845."}], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2892", "title": "Define TelnetConsole.port = None in __init__()", "body": "Fixes  #2702 by only calling self.port.stopListening() if self.start_listening() has already been called.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pablohoffman": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/a429d78019a379fff29c7aa3fff0a0f0427b6995", "message": "update scrapinghub.com urls to use https"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2184054", "body": "When you access `settings['FEED_URI']` Scrapy ends up looking at the `SCRAPY_FEED_URI` environment variable.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2184054/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379561", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379561/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2701990", "body": "Indeed, shorter and faster. Change pushed, thanks!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2701990/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104504", "body": "Only UTC matters, time zones should be deprecated! :P\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104504/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/6127229", "body": "curly braces are added by lxml namespace normalization\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/6127229/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/10998978", "body": "my bad, forgot to uncomment it before pushing (I'm on an old sphinx version). just fixed it.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/10998978/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670863", "body": "Is there a way to avoid calling a protected method of lxml.html?. As this may raise some compatibility issues on future/past lxml versions.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670863/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/676433", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/676433/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455702", "body": "touple -> tuple\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455702/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455727", "body": "should we make these settings dicts (like extension and middlewares) and use `scrapy.utils.conf.build_component_list` to load them?.\n\nany particular reason why you went with lists instead?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455727/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455737", "body": "I think this should inherit from `AssertionFailure`, to appear as test failures (and not errors) in some testing frameworks.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455737/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455755", "body": "I think I would prefer two contracts for these:\n\n```\n@minitems 1\n@minrequests 2\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455755/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455772", "body": "Why do we need this method for?. I don't see it used, and we already have `adjust_request_args`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455772/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455784", "body": "what about just ignoring those who don't?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455784/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455786", "body": "perhaps in the future we can add a way to list contracts per spider\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455786/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455791", "body": "I'm not sure a separte register method is needed. How about the constructor receiving the contracts? Like middlewares and extension do. See `scrapy.middleware` module.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455791/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455796", "body": "these changes should go into a separate PR\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455796/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465460", "body": "I think we should reuse the convention of using dicts. (we were planning to port pipelines too).\n\nIt's true that priorities won't add much in in this case, but it's not only priorities what the dict mechanism provides, but also being to disable specific components of the (presumably most sensible default). Priorioties wont' add much to this (as they don't to extensions) but don't harm and it could be useful if someone ever needs them. We should make sure internal code in an intuitive behaviour. Like pre_process being called in priority-order.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465460/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465558", "body": "I wasn't considering all cases. In that case, how about this:\n\n```\n@returns <type> [M [N]]\n```\n\nHere are some examples to illustrate:\n- `@returns request` - returns at least one request\n- `@returns request 2` - returns at least two requests\n- `@returns request 2 10` - returns at least two requests and no more than 10 requests\n- `@returns request 0 10` - returns no more than 10 requests\n\nSame goes for items.\n\nMaybe make aliases (`request` -> `requests` , `item` -> `items`) so that both plurals and singulars work.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465558/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570903", "body": "I can't see that comment on github now, did you remove it?\n\nOn Mon, Sep 10, 2012 at 6:24 PM, Alexandru Cepoi\nnotifications@github.comwrote:\n\n> In scrapy/contracts/default.py:\n> \n> > +# contracts\n> > +class UrlContract(Contract):\n> > -    \"\"\" Contract to set the url of the request (mandatory)\n> > -        @url http://scrapy.org\n> > -    \"\"\"\n> >   +\n> > -    name = 'url'\n> >   +\n> > -    def adjust_request_args(self, args):\n> > -        args['url'] = self.args[0]\n> > -        return args\n> >   +\n> >   +class ReturnsContract(Contract):\n> > -    \"\"\" Contract to check the output of a callback\n> > -        @returns items, 1\n> > -        @returns requests, 1+\n> \n> @returns requests 2 - returns at least two requests\n> \n> -- wouldn't this be considered unexpected behaviour?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/pull/167/files#r1570864.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570903/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614908", "body": "this one looks very similar to `scrapy.utils.get_func_args` (which is tested and supports methods, function, classes, etc) - could that one be reused?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614908/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614916", "body": "`from scrapy.conf import settings` is deprecated  API, use `self.settings` within the command methods,  that attribute is assigned by the `scrapy.cmdline` mechanics.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614920", "body": "unused import? (check for others too)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614920/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614925", "body": "shouldn't this section be called \"Spider contracts\"?.\n\nMainly for explicitly sake, and also consistency - for example the \"XPath Selectors\" section is not called \"Extracting data\".\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614925/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2851353", "body": "Is this change needed?. Isn't `_values` a `defaultdict(list)` already?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2851353/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2851397", "body": "should we support multi-valued values?. By adding something like this:\n\n```\nfor f, v in vales.iteritems():\n    for vv in arg_to_iter(v):\n        self.add_value(f, vv)\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2851397/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3225954", "body": "I would use `pkg_resources.iter_entry_points(\"scrapy\", \"commands\")` for consistency with how scrapy settings are  handled in scrapyd. Scrapy settings entry point uses \"scrapy\" as the group and \"settings\" as the name. Do you think \"scrapy.commands\" is a better convention?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3225954/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2553613", "body": "Here it uses `recursive` but below it uses `all`, should we normalize the argument names?. Also, the documentation only refers to `all`.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2553613/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3964413", "body": "why is this change needed?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3964413/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3964491", "body": "It may be worth adding a comment on why WeakValueDictionary is used, and where the references to slots are kept (_process_queue scheduled call, since it's far from obvious.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3964491/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/4920335", "body": "it says spider1, but it should say spider4.\n\nhow does this test that `from_crawler()` is called btw?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/4920335/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5896688", "body": "we should probably add a (boolean) class attribute to cache this and avoid comparing tuples every time\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5896688/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5901128", "body": "oh right, it's defining a different method based on python version, i missed that!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5901128/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6308399", "body": "I would say \"For example, to limit the response size to 1 Mb\". To avoid getting it confused with the default value.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6308399/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6308412", "body": "missing space before \"=\"\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6308412/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6308461", "body": "why is this test removed?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6308461/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6308719", "body": "does this closes the connection? should we? have we considered raising a custom exception? (ie. ResponseTooLarge)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6308719/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6354899", "body": "what information does this comment?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6354899/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6354985", "body": "This raises CancelledError but where does it call:\n\n```\ntxresponse._transport._producer.loseConnection()\n```\n\nThe canceller (_cancel) is not yet set here.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6354985/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6355023", "body": "Lines 35 & 36 can be replaced by a single line:\n\n```\nrequest.meta.setdefault('download_maxsize', self._default_download_maxsize)\n```\n\nBut do we need to always populate request.meta instead of doing something like this when it's used:\n\n```\nmaxsize = request.meta.get('download_maxsize', self._default_download_maxsize)\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6355023/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6384589", "body": "If this raises here, the deferred with `_cancel` canceller (line 123) is never created, and thus can never be called.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6384589/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6384628", "body": "both of these should use `settings.getint()`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6384628/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6384707", "body": "Passing the maxsize/warnsize as initial required position arguments sounds counter-intuitive to me, I would have added them at the end as optional keyword arguments. Even if this API isn't public there's no need to make it intentionally less future compatible (remember that warnsize will be going away eventually).\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6384707/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6384756", "body": "any reason not to use the more standard form:\n\n```\nrequest.meta.get('download_maxsize', self._download_maxsize) ?\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6384756/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/4366664", "body": "This is not portable (ie. doesn't work on windows). One option is to port the test to Twisted Trial and use self.mktemp(), otherwise tempfile.mkstemp() should work too.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/4366664/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7065286", "body": "Not intended, can be safely removed.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7065286/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7065333", "body": "It's needed by `url_has_any_extension` function but was imported as a side effect of the wildcard import below. This one needs to be added regardless of what we do with the `w3lib.url` wildcard import.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7065333/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7065361", "body": "That's correct. It has been a while since the move to `w3lib.url` though...\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7065361/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7133123", "body": "`mock` should only be required for tests, isn't that the case?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7133123/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7133663", "body": "cssselect should definitely be there, but I'm not sure about `mock`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7133663/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7985692", "body": "I'd keep this on a single line (just a minor thing though)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7985692/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6611047", "body": "unused import\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6611047/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6611050", "body": "unused import\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6611050/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6611122", "body": "We should only check for the response code, not the reason (since that is bound to vary among different servers). Something like `if bytes.startswith('200 ')` should do.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6611122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6611152", "body": "We should return some exception here (using `self._tunnelReadyDeferred.errback()` possibly with a custom exception) instead of just closing the connection.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6611152/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6611184", "body": "Unnecessary change, let's remove to keep the commit cleaner.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6611184/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6611238", "body": "Is there really a need for the `openTunnel` argument?. Since this is a tunneling class (`TunnelingTCP4ClientEndpoint`) it may as well always tunnel, right?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6611238/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6611364", "body": "We should also support the old (insecure) mechanism that proxies HTTPS over plain HTTP. I discussed it with Dan and we propose to use an argument in the proxy url to indicate that the old mechanism should be used.\n\nFor example, to use the new (recommended) mechanism the proxy url would be:\n\n```\nhttp://localhost:8080\n```\n\nWhile for using the new mechanism it would be:\n\n```\nhttp://localhost:8080?noconnect\n```\n\nThis `if` would then check both the scheme and the presence of `noconnect` argument.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6611364/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6639104", "body": "Right, here is where reusing the twisted HTTP client protocol would help, but a regex should be fine for now.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6639104/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6639173", "body": "It would be propagated through the downloader middleware and ultimately handled by the spider/request errback, if any.\n\nit would be better to return the actual response we got from the proxy but in order to do that we'd need to do more parsing (like parsing HTTP headers) so this is probably something that could be improved when we port the patch to a cleaner approach using a twisted protocol.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6639173/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6639204", "body": "I'm in favor of removing that hacky retrial too.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6639204/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6639269", "body": "Totally understandable, see http://doc.scrapy.org/en/latest/contributing.html\n\nIt's also a good idea, in general, to separate aesthetic from functional changes, although you probably know that :) \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6639269/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6639325", "body": "It shouldn't reach the proxy, I think, since there's no need to pass the `proxyArgs` to `_TunnelingAgent` method.\n\nRemember this is to specify the proxy in the configuration, not the request url to visit.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6639325/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6728319", "body": "maybe we should run this as a python module?. like `python -m mitmproxy.tool` or something like that. It's typically more portable than running the raw command, because it only depends on the python path and not the system path. Remember this should also run on windows (although it's not tested so frequently there)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6728319/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6728676", "body": "Great. Also, did you consider using `libmtproxy` on a thread instead of spawning a separate process?\nhttp://mitmproxy.org/doc/scripting/libmproxy.html\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6728676/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8599524", "body": "nice helper @kmike!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8599524/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8779430", "body": "`Selector` should be imported from `scrapy.selector`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8779430/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8779437", "body": "SgmlLinkExtractor is duplicated in the line below (where it should be imported)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8779437/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896015", "body": "wouldn't this cause a conflict between the imported function and method name?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896015/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7896983", "body": "How about `dict(self._default_namespaces)` or `self._default_namespaces.copy()`?. Both would achieve the same without requiring the `copy` import.\n\nI think the `ns` variable is also unnecessary, consider this:\n\n```\nself.namespaces = self._default_namespaces.copy()\nself.namespaces.update(namespaces or {})\n```\n\nFinally, how about optimizing the common case to avoid duplicating the `_default_namespaces` dict on every `Selector` instantiatation? (which are a lot)\n\n```\nif namespaces is None:\n    self.namespaces = self._default_namespaces\nelse:\n    self.namespaces = self._default_namespaces.copy()\n    self.namespaces.update(namespaces)\n```\n\nI'm really unsure about the impacts of this dict duplication, I would do some profiling to confirm it's worthwhile.\n\nAnd there's the concern about further modifications to `.namespaces` if it's a public attribute.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7896983/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896187", "body": "Even when they don't run in the same process, they may still affect other spiders running in different processes through scrapyd. If a spider leaks too much, it will fill memory and affect all spiders (and other processes in the system).\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896187/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896324", "body": "No need to change, thanks about the clarification. Good refactoring!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896324/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896376", "body": "Btw, in order to become more broadly useful (and officially supported), we should document this ability to override the dupe filter fingerprint.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896376/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896904", "body": "How about updating it to:\n\nIn big projects, the spiders are typically written by different people and some\nof those spiders could be \"leaking\" memory and potentially affecting other\n(well-written) spiders running simultaneously in the same server (for example,\nvia `scrapyd`_).\n\n(with a link to scrapyd)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8896904/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8897638", "body": "Why remove this?. It's still important to deallocate (on `spider_closed`) all resources allocated on `spider_opened` regardless of whether the spiders run in the same process or not - we plan to make them run in the same process in the future, but with a different architecture (using multiple Crawler instances in a single twisted reactor), and it's also the right thing to do, and often forgotten.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8897638/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8886649", "body": "it's `scrapy.selector` not `scrapy.selectors`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8886649/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8886749", "body": "that's what pull requests & CRs are for :)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8886749/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8731448", "body": "Why not warn once by default? (ie. `warn_multiple=False` kwarg). Looks like the most sensible behavior to me, so I would make it the default.\n\nAlternatively, we could consider supporting the \"action\" field of warnings filter:\nhttp://docs.python.org/2/library/warnings.html#the-warnings-filter\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8731448/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8933140", "body": "I'd add it back.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8933140/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "iamminji": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/1dcea6a9d4615afc463fa9839d002d861aed5274", "message": "fix typo"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ngash": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/fd27cde24d273e30f72f53f2711515403270838a", "message": "Update asserts to use more generic ones"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/4ca61a20512b7306e7266e6667a0c02ae5ebe557", "message": "Update deprecated test aliases\n\n- change ``failIf`` to ``assertFalse``\n- change ``asertEquals`` to ``assertEqual``\n- change ``assert_`` to ``assertTrue``\n\nhttps://docs.python.org/2/library/unittest.html#deprecated-aliases"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kirankoduru": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/2960c9b5683dceb149823e7f927d8c86ee83deb8", "message": "Use self.__class__.__name__ instead of showing generic Spider class name"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/12409a0cf6c37ff5c19588bb064690549798bb37", "message": "Fix broken encoding on text for py 3"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/7adab61a7a5f88c78311cc44a468c7b8d0a4c954", "message": "Added test for NotImplemented Spider.parse method"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/be71f98e92688c759d3af48101617229dcdfe05f", "message": "Explicit message for scrapy parse callback\n\nThe scrapy parse method raises a NotImplementedError when not defined,\nbut for new comers it can be hard to debug what might be going wrong.\n\nAdding an explicit message for NotImplementedError will help new users."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jenya": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/01ac8838934071b89c5c711688f3d833387e27d4", "message": "Follow alternate link for all types of sitemaps"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "davidthewatson": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/71d5b7d75a579360ab02b9f9594199993ff91f61", "message": "fix typo (#2867)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "andreip": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/6e6b5cc29f15dbf4f1941fca70dd9c126e4ba556", "message": "Use getfullargspec under the scenes for py3 to stop DeprecationWarning (#2864)\n\nUse getfullargspec under the scenes for py3 to stop DeprecationWarning. \r\n\r\nCloses #2862"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "simik-ru": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/a65fec050ae4a07d233ae886457daf10f14929fe", "message": "Small fix in description of startproject arguments"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dguo": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/26c488970c51c06506cfc88c4a76440c009cdd48", "message": "Fix a typo in the Items documentation"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/dedc4a8b8f7cd3870f89bec2bf89a5ec11a95e4f", "message": "Tweak the CSVFeedSpider documentation"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cconrad": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/18b96dd82af0ac58a2fc6e1ba2227b4468ab2bb7", "message": "Spelling mistake"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "crasker": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/ae679f6499b7d63061f11ca11592dcaff5919a00", "message": "Create item-pipeline.rst"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chuanjin": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/3f8542eb566dd06d35b17574d65955de2682a29e", "message": "Update extensions.rst\n\n#2759"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "khpeek": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/af2963d0eb79b1241e04b9fc7972bc4a31adf67c", "message": "Update autothrottle.rst\n\nAdded missing bullet point for the AUTOTHROTTLE_TARGET_CONCURRENCY setting."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/548a432951ef48f142d6091ecffd8e54eaab3fc4", "message": "Minor grammatical changes"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Granitosaurus": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/1a452c038cc0547924051a7cd0786215ec7c2104", "message": "increase ptpython priority since it can use other shells as backend"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/7ba4b0a21b0bb2efe0523bec0c1db07771816347", "message": "add support for embeded ptpython shell"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "eliat123": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/b74b98fa3e7149c27bd3a541940457864a28d5d1", "message": "cleanup: removed unused MEMUSAGE_REPORT\n\nSigned-off-by: Eli Atzaba <eliat123@gmail.com>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "luzfcb": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/df7a5c4aa4f5898de3c70cef17c3c5031f7e05a6", "message": "Add support for executing scrapy using -m option of python\n\npython -m scrapy"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "HarrisonGregg": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/ffef828a8deb86520e3bd6a50d76b2a4ecf3ae71", "message": "Add test for dropping fields in from_response request body"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/45a323024c7a7008df3e319a8a1437fae53826f9", "message": "Add documentation for dropping fields in from_response request body"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/26f723e4e63185b35b2f11285f50cfc3ac52b8fc", "message": "Allow formdata value to be None to drop field generated from response"}], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2918", "title": "Fix SIGINT handling when using inspect_response", "body": "Currently, after calling `scrapy.shell.inspect_response` and then closing the opened shell, SIGINT (Ctrl-C) no longer works to terminate the spider.  This is because `Shell.start`, called in `inspect_response`, removes the signal handler.  To fix this, save the SIGINT handler before calling `Shell.start` in `inspect_response` and add it again after the shell has closed.\r\n\r\nIt doesn't appear that there are any tests for `inspect_response`, so I haven't added any tests for this fix.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yandongxu": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/4966dd7a7fa544f6c5b3bbeb1f09426d9adcdb16", "message": "Fix doc: open file with \"wb\" mode will get an error in python 3"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "liusy182": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/6c1cacb5d5f0e5523d429cb89808dabfd824dcd5", "message": "[MRG+1] doc: fix documentation error in link-extractor.rst (#2676)\n\n* fix doc error in link-extractor.rst\r\n\r\n* remove the import clause\r\n\r\n* update based on suggestion"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mgaitan": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/4bc0c6b0f4c5dae033bd83549b6d0a573fcf4805", "message": "Update practices.rst\n\nfix a typo"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tbcardoso": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/00ee9eaeafcd3b3af43132c4513754627f97a886", "message": "Mention how to disable request filtering in documentation of DUPEFILTER_CLASS setting"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "JulienPalard": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/30eec559104649653d4c0e182fb2d3e2252dca1e", "message": "[PEDANTIC] FIX trailing whitespaces in LICENSE."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "LMKight": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/05ce1296c6a60f23e81af7ec38ac1855e78be79f", "message": "changed code according to request"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/6352c2e9b2028473acdbd58175bbc5638258e29d", "message": "fixed command list"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "simongartz": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/21d794d35ae1347918e6bf5b2ffe13515ea28795", "message": "Fixes conversion of transparent PNG with palette images to jpg #2452"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "qhuang872": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/8ecc307e8f04b058f2fc8a1a47759203ff6ebca0", "message": "Update spiders.rst\n\nAdded note to allowed_domain attribute with an example explaining what goes in the list"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/c5f74f7d1a5245b65e5c7863b02a379667bee0dd", "message": "Update spiders.rst\n\nAdded a note to allowed_domains attribute, reminding users not to add urls into the list."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "otobrglez": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/11cf6ad4258250b8a2410029eb794ed892dc4cea", "message": "Comments for AWS_ENDPOINT_URL setting."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/605691792f5198d75524a1ad952489193970d4a7", "message": "Updating media-pipeline docs for S3-like storage."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tchiotludo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3072", "title": "Handle Webp Image transparency", "body": "Webp transparent image have a dark background : \r\n\r\nreference image : https://www.lavera.de/typo3temp/fl_realurl_image/105115-4021457609482-glossylips-deliciouspeach09-1294b-43402bf213-6cf9c.png", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "BrandonSmithJ": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3062", "title": "Use as a library: Reactor decorator code + MWE", "body": "First, let me say thanks for the extremely useful tool - it really has made a lot of things much simpler than they otherwise would have been. \r\n\r\nWith that in mind, I'd like to give back what I can if it's useful. The problem I'm having is I don't know exactly where to post this. I'm not familiar enough with scrapy internals to suggest where to put the feature, or if it's even necessary enough to put into the main code base. It also likely needs one more modification before being ready for a main branch contribution. \r\n\r\nThe problem this solves is something at least a few people seem to have run into:\r\n\r\n- https://stackoverflow.com/questions/35289054/scrapy-crawl-multiple-times-in-long-running-process\r\n\r\n- https://stackoverflow.com/questions/22116493/run-a-scrapy-spider-in-a-celery-task/22202877#22202877\r\n\r\nMy use case is a website which requires a virtually endless process able to use various scrapy functionalities at whim. To solve the problem of restarting the twisted reactor, I essentially took the solutions I could find and rolled everything into an extremely simple decorator. Basically all the end-user needs to do is label the function they'd like to run as a scrapy function, and the decorator handles everything necessary in order to use scrapy in a library capacity:\r\n\r\n```\r\n\t@reactor_process(timeout=10)\r\n\tdef execute(self, keyword_list):\r\n\t\t''' Crawl the site specifically for certain keywords '''\r\n\t\tfrom crawlers.backend.spider_interface import construct_spider\r\n\t\tfrom scrapy.crawler import CrawlerProcess\r\n\r\n\t\tspider = construct_spider(self)\r\n\t\tspider.create_start_urls(keyword_list)\r\n\t\t\r\n\t\tcrawler  = CrawlerProcess(self.settings)\r\n\t\tdeferred = crawler.crawl(spider)\r\n\t\tdeferred.addBoth(lambda _: crawler._stop_reactor())\r\n```\r\n\r\nWithout the decorator, the function above would only be able to run once - the reactor engine would complain about being restarted. The one problem is that the necessary imports have to be made *within* the function itself - though I think this can be solved with a multiprocessing manager (or worst case, global inspection). \r\n\r\nI also have an interface which allows dynamic creation of contract docstrings / crawlers with different settings inside the same process, which might be useful; but that's something that would probably be better as its own discussion. \r\n  ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Matthijsy": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3061", "title": "Make autothrottle slow down on HTTP 429 response", "body": "When a response has status 429 (Too Many Requests) it is ignored by the AutoThrottle (because the latency is low, so otherwise the spider will speed up). I think that the wanted behaviour is that the spider will slow down when a 429 is received, but currently the spider will stay at a constant scraping speed. \r\n\r\nThis PR adds a new setting `AUTOTHROTTLE_429_DELAY` when it is set the download delay will be increased with this value every time a 429 is received. It still respects the 'MAX_DOWNLOAD_DELAY` value.  ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "parlays": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3051", "title": "The URI gets set again before feed export store gets called.  This al\u2026", "body": "\u2026lows the spider to change settings in the parse method based on the content scraped.  The IFeedStorage interface has been changed for the store method to allow uri to be passed.  The BlockingFeedStorage class now sets the new uri settings in the store method.\r\n\r\nIn my personal scraper I overwrite the FeedExporter object and S3FeedStorage object.  But it would be great if this was part of the core.  It may be a tough change though because it requires a change to the IFeedStorage interface and that is a breaking change for older code.\r\n\r\nIt is common for me to scrape a site and then decide the folder name to use in the S3 bucket.  To be able to change the bucket name or directory in the parse method is important.  If this change is not agreed upon, let's think of another way to accomplish this.  Thanks!", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "munderseth": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3041", "title": "Add Test Reporting to Travis CI", "body": "**Test Reporting for Travis using [Testspace.com](https://testspace.com)**\r\n\r\nHi `scrapy` team. We added test reporting to your repo. We have a blog article on [why we are staging repos](https://blog.testspace.com/testspace-and-open-source).\r\n\r\nNote that we contacted you in the past via an email. We thought it would be easier for you (if interested) by using a Pull Request.   \r\n\r\nFew of the benefits using Testspace:\r\n- view all your test results from a single dashboard\r\n- triage and manage your test failures faster\r\n- add more metrics\r\n- leverage built-in analytics \r\n- get a [test badge](https://help.testspace.com/how-to:get-badge) \r\n[![Space Health](https://open.testspace.com/spaces/74470/badge?token=2f48b759d52023674602ea42e90a5508cad6c953)](https://open.testspace.com/spaces/74470?utm_campaign=badge&utm_medium=referral&utm_source=test \"Test Cases\")\r\n\r\nCheckout **your** test results: https://open.testspace.com/projects/TryTestspace:scrapy from our fork.\r\n\r\n**Why** we are doing this: https://blog.testspace.com/testspace-and-open-source \r\n\r\n----\r\n\r\n**Give Testspace a try?**\r\n\r\n1. Create **Open** (free) account: https://testspace.com/pricing\r\n2. Add a **New Project** from the list of Github repo's\r\n3. Add a Travis **Environment Variable**: Name: `TS_ORG`  Value: `organization name` - based on subdomain selected in step 1\r\n4. Invite others: https://help.testspace.com/how-to:invite-other-users\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jslay88": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3040", "title": "Fix for Issue #2919", "body": "Splits out url params and updates them with formdata. Passes local tests.\r\n\r\nFirst PR, please be nice :)\r\n\r\nFixes Issue #2919 ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mylh": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3028", "title": "Update practice.rst docs", "body": "added tips to avoid getting banned", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chainly": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3027", "title": "fix Spider.log to record right caller information", "body": "Spider.log always log itself in `pathname), funcName, lineno, etc.` format. Because `currentframe = lambda: sys._getframe(3)` and `f = f.f_back` in ``logging.Logger.findCaller`.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "isra17": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2996", "title": "Add signals to handle error from downloader or pipeline", "body": "As of right now, there's is no simple way to handle exception coming from items pipeline or a the `process_response` of a downloader middleware.\r\n\r\nThis PR adds two signals for those use case. Note that the downloader signals also catch any exception from the downloader engine, including `process_request` and `process_exception`.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2995", "title": "Allow passing Failure object to middlewares", "body": "Trying to handle errors coming from middleware, it happens that Twisted strip traceback from an exception returned from a completed deferred (Needed to avoid GC issues). This mean that trying to use `exception.__traceback__` always yield `None`.\r\n\r\nThis PR adds a decorator that can be used on middleware `process_exception` or `process_spider_exception` to get a Failure object instead of an Exception. The Failure object provides a `getTracebackObject` to get a `Traceback`-like object which come really handy when trying to pinpoint an issue.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2962", "title": "Add signal `request_downloading` called right before the download handler", "body": "It was needed for one of our project, maybe it can be useful for core as well.\r\n\r\nThe `request_downloading` signal is sent when the engine is about to download a request. If one handler raise an exception, the download is aborted. The signal supports returning deferreds from their handlers.\r\n\r\nThis is an alternative to the Downloader middlewares where there might be a significant delay between the middleware call and the download handler in case of slow queue processing.\r\nThis even handler allow some extension to tamper the request right before the download time and possibly cancel the request by raising an exception.\r\n\r\n`signal.send_deferred` was needed for the exception handling raised by signal handler in the download manager.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "aitoehigie": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2972", "title": "Add a note to allowed_domains", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kaplun": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2954", "title": "spiders: add OAI-PMH support WIP", "body": "Signed-off-by: Samuele Kaplun <samuele.kaplun@cern.ch>", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "phnk": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2953", "title": "Added debug message in spiders/crawl", "body": "As requested in #2925.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mGalarnyk": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2931", "title": "Update install.rst", "body": "Updated installation instructions for installing scrapy using conda.\r\n\r\nYou can now just do: \r\n\r\n```\r\nconda install scrapy\r\n```", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zhaojiedi1992": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2911", "title": "Update exporters.rst", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sergiobellon": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2901", "title": "Fixed mail mimetype when it has attachments", "body": "An email cannot be sent in html format if there are attachments because it's set always to 'plain'", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gtseres": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2887", "title": "Add debug print of config file path", "body": "Fixes #2825\r\n\r\ntests/test_utils_conf.py and tests/test_utils_log.py passed. Would need some help if I need to write any unit tests for this change, and how I can get the scrapy.log file in the testcase.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yongzx": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2838", "title": "Bring back documentation for methods in CrawlSpider", "body": "#2727 \r\nThis brings back the docstring comments in CrawlSpider that were removed in [e2290a5](https://github.com/scrapy/scrapy/commit/e2290a5359ee80c75d8bcf8de8b6894e61fad83a) to the [spiders.rst](https://github.com/scrapy/scrapy/blob/master/docs/topics/spiders.rst)\r\n\r\nCompared to #2819, private methods are not included in spiders.rst. Docstrings are not included in the CrawlSpider class (in response to the duplication issue pointed out by @kmike in #2819  and  #2814 ).", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2819", "title": "Documentation of CrawlSpider methods in Spiders.rst ", "body": "#2727 \r\nAdded the documentation from CrawlSpider source code to the [spiders.rst](https://github.com/scrapy/scrapy/blob/master/docs/topics/spiders.rst)", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2814", "title": "Add documentation to CrawlSpider", "body": "#2727 \r\nThis brings back the docstring comments in CrawlSpider that were removed in [e2290a5](https://github.com/scrapy/scrapy/commit/e2290a5359ee80c75d8bcf8de8b6894e61fad83a). \r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "umrashrf": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2827", "title": "[WIP] Added a feature to allow setting of custom log config file", "body": "Added a feature to allow setting of custom log config file from project's settings file.\r\n\r\nNeed ideas for:\r\n- Tests\r\n- Whether or not it should be YAML or other supporting config file types", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1466", "title": "[MRG+1] Support auth credentials in netloc for HTTP and FTP handlers", "body": "Fixes #669 \n\nSolution provided by @dangra https://github.com/scrapy/scrapy/pull/670#issuecomment-38921769\n\nI can add tests but should I add this dependency for FTP tests? https://github.com/giampaolo/pyftpdlib \n\nCurrent FTP tests uses Twisted so I'm afraid they won't work for PY3 https://github.com/scrapy/scrapy/blob/master/tests/test_downloader_handlers.py#L523-L543\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/499", "title": "added JavaScript FAQ", "body": "I have seen questions popup in Scrapy google group that boils down to \"Scrapy can't execute JavaScript\" so I think it should be added to Scrapy official FAQ page.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "katz": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2809", "title": "extract file extension properly", "body": "Remove query string part from the url first so that the logic can file extension properly.\r\n\r\nThe original code cannot extract the file extension properly if the media url contains query string like below:\r\nhttps://example.net/some.jpg?ab=1e9b29b5d3d0aa13d601505c6a67ee3e&dc=59C43AAF", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "anaisabel7": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2807", "title": "python-attrs support to Scrappy Items", "body": "Fixes #2749 \r\n\r\nProviding new ways to populate Scrapy items and adding the possibility of having @attr.s decorated Scrappy items.\r\n\r\nThis PR tries to solve the suggestions presented in issue #2749 \r\n\r\nCertain arguments need to be passed to the decorator in order to make the decorated class (a subclass of Scrapy Item) compatible with the functioning of Scrapy Items. This is covered in the form of tests in the PR.\r\n\r\nAt the moment, the passing of default values to the attributes of the class (equivalent to Fields in normal Scrapy Items) is covered, but the passing of other metadata, such as validators, has not been considered.\r\n\r\nThis PR also covers the dot notation access to the Fields in Scrapy Items. ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shirk3y": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2799", "title": "Async HTTP cache using Deferreds", "body": "Leverages Twisted `Deferred`s to make HTTP cache storage optionally asynchronous. Useful when using non-local services to store/retrieve responses.\r\n\r\nNo docs/tests yet, just piece of code proven to work in several projects.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/735", "title": "OffsiteMiddleware that filters start_requests", "body": "When we use additional Middleware that for example schedules requests from DB in start_requests, we want have guarantee that requests are limited to 'allowed_domains'.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rocioar": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2788", "title": "Check if attribute has a value before moving forward with the link extraction", "body": "I run into this bug using a link extractor to get all the images in a web page. The link extractor was returning the base_url for some reason. Investigated, and found out that the logic is not checking if the attribute is empty before url joining it with the base url.\r\n\r\nAs attr_val was empty this line:\r\n\r\n`attr_val = urljoin(base_url, attr_val) `\r\n\r\nwas returning just the base_url.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nerogit": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2780", "title": "Support azure blob storage to image pipeline", "body": "I add test based on #2064\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "aperezalbela": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2773", "title": "Spider name colliding with existing module in runspider Command. Fixes #2680", "body": "Created a project named **testingcsvname** with a spider name **csv.py**\r\n\r\n```\r\n\u279c tree\r\n.\r\n\u251c\u2500\u2500 scrapy.cfg\r\n\u2514\u2500\u2500 testingcsvname\r\n    \u251c\u2500\u2500 __init__.py\r\n    \u251c\u2500\u2500 items.py\r\n    \u251c\u2500\u2500 middlewares.py\r\n    \u251c\u2500\u2500 pipelines.py\r\n    \u251c\u2500\u2500 settings.py\r\n    \u2514\u2500\u2500 spiders\r\n        \u251c\u2500\u2500 __init__.py\r\n        \u2514\u2500\u2500 csv.py\r\n```\r\n\r\nUsed **runspider** to run the Spider file\r\n\r\n```\r\n\u279c scrapy runspider testingcsvname/spiders/csv.py\r\n2017-06-01 04:45:40 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: testingcsvname)\r\n2017-06-01 04:45:40 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'testingcsvname.spiders', 'SPIDER_LOADER_WARN_ONLY': True, 'SPIDER_MODULES': ['testingcsvname.spiders'], 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'testingcsvname'}\r\nUsage\r\n=====\r\n  scrapy runspider [options] <spider_file>\r\n\r\nrunspider: error: No spider found in file: testingcsvname/spiders/csv.py\r\n```\r\n\r\nAfter applying this fix:\r\n\r\n```\r\n\u279c scrapy runspider testingcsvname/spiders/csv.py\r\n2017-06-01 04:50:57 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: testingcsvname)\r\n2017-06-01 04:50:57 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'testingcsvname.spiders', 'SPIDER_LOADER_WARN_ONLY': True, 'SPIDER_MODULES': ['testingcsvname.spiders'], 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'testingcsvname'}\r\n2017-06-01 04:50:57 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.logstats.LogStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.corestats.CoreStats']\r\n2017-06-01 04:50:57 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\r\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2017-06-01 04:50:57 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2017-06-01 04:50:57 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2017-06-01 04:50:57 [scrapy.core.engine] INFO: Spider opened\r\n2017-06-01 04:50:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2017-06-01 04:50:57 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\r\n2017-06-01 04:50:59 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://scrapinghub.com/robots.txt> from <GET http://scrapinghub.com/robots.txt>\r\n2017-06-01 04:51:01 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://scrapinghub.com/robots.txt> (referer: None)\r\n2017-06-01 04:51:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://scrapinghub.com/> from <GET http://scrapinghub.com/>\r\n2017-06-01 04:51:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://scrapinghub.com/> (referer: None)\r\n2017-06-01 04:51:02 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2017-06-01 04:51:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'downloader/request_bytes': 872,\r\n 'downloader/request_count': 4,\r\n 'downloader/request_method_count/GET': 4,\r\n 'downloader/response_bytes': 9368,\r\n 'downloader/response_count': 4,\r\n 'downloader/response_status_count/200': 1,\r\n 'downloader/response_status_count/301': 2,\r\n 'downloader/response_status_count/404': 1,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2017, 6, 1, 9, 51, 2, 561073),\r\n 'log_count/DEBUG': 5,\r\n 'log_count/INFO': 7,\r\n 'memusage/max': 49393664,\r\n 'memusage/startup': 49393664,\r\n 'response_received_count': 2,\r\n 'scheduler/dequeued': 2,\r\n 'scheduler/dequeued/memory': 2,\r\n 'scheduler/enqueued': 2,\r\n 'scheduler/enqueued/memory': 2,\r\n 'start_time': datetime.datetime(2017, 6, 1, 9, 50, 57, 503991)}\r\n2017-06-01 04:51:02 [scrapy.core.engine] INFO: Spider closed (finished)\r\n```\r\n\r\nThe main idea is to check that imported module with:\r\n\r\n`module = import_module(fname)`\r\n\r\nHas the same path as the absolute path of the provided Spider(s) file", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "OlivierKnell": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2765", "title": "DOC not supporting python 3 yet", "body": "Specified that it doesn't support python 3 yet", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "woisnow": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2760", "title": "fix s3 connection bug", "body": "support region/host selection (botocore and boto)\r\nbotocore use v4 auth to connection s3\r\nadd two settings options: AWS_REGION_NAME and AWS_REGION_TO_HOST\r\nplease read the 'file/image' code for the detail info ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shitian-ni": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2757", "title": "Update request-response.rst #2733", "body": "fixed issue #2733 ,\r\n`class scrapy.http.Request(url[, callback, method='GET', headers, body, cookies, meta, encoding='utf-8', priority=0, dont_filter=False, errback, flags])`\r\nnow read as\r\n`class scrapy.Request(url[, callback, method='GET', headers, body, cookies, meta, encoding='utf-8', priority=0, dont_filter=False, errback, flags])`", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shaform": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2738", "title": "Allow custom files storage class", "body": "Recently I found that it is too restrictive to only have S3 storage when using scrapy on scrapinghub.\r\nHopefully we could use any custom storage classes.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "RajatGoyal": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2707", "title": "allow specifying max_active_size for scraper.slot in settings", "body": "fixes #1410\r\n\r\nFor some cases when I have many big (~20-30mb)  files to scrape, the spider just waits until everything of one file is scraped and links are enqueued and then takes on the new file, which makes my scraping almost synchronous. If I increase the `max_active_size` to a big value (x100) since we have machines with large ram, it works fine. \r\n\r\nThis parameter needs to be configurable for the project/spider.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kooy": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2672", "title": "Update exporters.py", "body": "In windows, the exported csv will produce extra blank lines. Add **lineterminator = '\\ n'**, to solve this problem.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "codinguncut": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2565", "title": "using custom request types for sitemap spider", "body": "I am deriving from SitemapSpider for exploratory crawls and only want to see sitemap links, without necessarily following them.\r\nFor this reason it's important to be able to differentiate whether a Request is from a SitemapIndex or from a UrlSet.\r\nI have derived (named) classes from Request to make this easily discoverable in derived classes and/or pipeline.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "funkyfuture": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2532", "title": "CsvItemExporter: don't write headers to files when appending", "body": "this handles the case when an export feed uri points to an existing csv file, which would otherwise contain repeated header lines. and hardly any of them on top.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "eLRuLL": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2494", "title": "[WIP] adding LOG_FILEMODE settings for enabling mode in logging FileHandler", "body": "I have been seeing a lot of questions on Stackoverflow related to controlling more the scrapy logs. One in particular is the ability to rewrite the logs in the same file, which would be easily handled with this improvement by adding `LOG_FILEMODE='w'` in settings.\r\n\r\nThis is still WIP, so if you agree with this improvement I will continue working on it (add documentation, tests, etc.)\r\n\r\nAnother possibility I was thinking is to add the ability to override an entire LogHandler (just like `DUPEFILTER_CLASS`), that way there would be more control over it, as we currently only support `FileHandler`, `StreamHandler` and `NullHandler`.\r\n\r\nPlease let me know what do you think.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "w495": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2482", "title": "LOG_DICT_CONFIG for custom logging", "body": "Add LOG_DICT_CONFIG option to achive ability add custom logging. For example, for colorlog.ColoredFormatter", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "samj1912": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2471", "title": "Allow image conversion/preservation in ImagePipeline #2452 #1705", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "syucream": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2415", "title": "Enable FilesPipeline to determine filename from 'Content-Disposition' header.", "body": "`file_path()` usually doesn't return a human readable filename so I'd like to enable to change it based on `Content-Disposition` header value.\r\n\r\nIn case of a server responds the header, `media_to_download()` can't determine an actual filename. So this behavior may be changeable by settings ... ?", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "korsunowk": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2372", "title": "S3FilesStore enable SSL ( #2085 )", "body": "#2085 Hey everyone. Added two variables in config, this should be it. Looking forward to any feedback!", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "iserko": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2355", "title": "Enable LOG_CUSTOM_LEVELS in order to specify 3rd party logger log levels", "body": "When you use 3rd party libraries inside crawler code, which log all sorts of information at various logging levels, it can become quite noisy. This code change helps out by moving the LOGGING dictionary with the list of `loggers` which we can override. Example:\r\n\r\n```\r\n    LOG_LEVEL = 'DEBUG'\r\n    LOG_CUSTOM_LEVELS = {\r\n        'boto': 'ERROR'\r\n    }\r\n    ```\r\n\r\nWould silence boto while allowing you to keep your `LOG_LEVEL` at `DEBUG`.\r\n\r\nThis ties into issue #2231 as it would enable us to modify loggers. It does not touch handlers, but it does give you the ability to set them up however you want and assign loggers to specific handlers (if you know how to configure logging properly).\r\n\r\ncc @redapple\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lfmattossch": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2336", "title": "Adding an example on the documentation of writing your own middleware (fixes #1789)", "body": "Sup guys?\n\nFollows up an example in the documentation on how to access settings and other crawler attributes.\n\nThnaks\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ahlinc": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2268", "title": "Make crawler.engine available in scrapy's Spider.__init__ method.", "body": "Hi,\nI've had situation when I needed to extend `Spider.__init__` method and access to low-level http-cache middleware instance methods to manually control scrapy's work with the cache. But in current implementation the engine will be created after spider instance and crawler property will be assigned after spider instance creation, so the next possible point where engine may be accessed is only _start_requests_ method but in my opinion it's irrational place. So in this PR I suggest to change the order of components creation and as result the engine will be available at the end of spider instance `__init__` method just by `self.crawler.engine` reference.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "taito": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2242", "title": "Add LOG_HANDLER to settings, so that other file handlers can be chosen", "body": "Title here is self explanatory, but wanted to add possibility to use other file handlers for logging.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kedark3": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2207", "title": "[MRG+1] PEP8-ize scrapy/commands/", "body": "See #2144\nChanging multiple imports on one line to one line one import scheme. removed unused import from check.py. This is my first Pull request on Github and I am just starting my journey in open source world. Let me know if I did anything wrong in this commit. \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mayouf": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2179", "title": "new itemloader function: create nested loader by inserting a selector", "body": "By using nested Item loader, we should be able to do it with a selector as an input. Not only a xpath or css.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "maksimbormot": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2085", "title": "S3FilesStore enable SSL", "body": "Enable SSL because of this python bug is fixed on 2.6 and 3.x Python versions.\n\nFixes #2083\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mengbiping": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2080", "title": "Turn off use_ssl for botocore interface.", "body": "use_ssl=True would be too expensive when calling spiders configured with ImagePipeline. In my experience, it slows down scrapy by 5X and more.\n\nIn addition, this keeps the same behaviour as we had for boto interface, as in\n\n```\nc = self.S3Connection(..., is_secure=False)\n```\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "anatolykazantsev": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2067", "title": "Updated the list of ignored extension in the linkextractor component", "body": "- Added following extension to the list: .7z, .7zip, .bz2, .gz, .tar, .xz, .cdr, .ico, .apk\n- Moved archives into the separate section\n\nSee scrapy/scrapy#1837 issue for details and discussion.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "josericardo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2045", "title": "[WIP] Change BaseSettings#__getitem__ to behave like a regular dict's one", "body": "The previous implementation broke the MutableMapping contract.\n`setdefault`, for one, doesn't work anymore.\n\n`__getitem__` is expected to raise KeyError for non-existing keys, unless\nwe're trying to emulate a defaultdict, which should be explicit in the\ndocs and is backwards incompatible.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rootAvish": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2030", "title": "Moving signals away from PyDispatcher", "body": "Fixes https://github.com/scrapy/scrapy/issues/8\n\nThis is by no means complete, specially as of the changes committed to this point,  most of the effort to this point dealt with porting relevant parts of `django.dispatch` to Scrapy and writing parts of it without dependency on `django.utils` and introducing methods for API consistency, but opening this PR so that the changes can be monitored by the community. I'll update this as the changes come along in detail about what the individual changes concern themselves with.\n\nTo this point we have:\n- Introduced the `scrapy.dispatch` module\n- Changed all standard signals to be objects of the `Signal` class instead of the generic `object`.\n\nMost of the other stuff is outside the mainline and I'll be pushing it as soon as I'm done with it, I'll preferably have a working prototype by the end of this week(Saturday), latest by the start of the next one(the Monday after that). \n\n/cc @jdemaeyer \n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mgachhui": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2029", "title": "#1915: Multiple items through item pipelines", "body": "My solution to #1915. Approach:\n- All functions in a pipeline are put inside closures and a process chain is made out of them.\n- For the n<sup>th</sup> pipeline, it's output is handled by the n+1<sup>th</sup> pipelineHandler.\n- When an iterable is encountered, a parallel call is generated and a DeferredList is returned. According to the Twisted docs, if a callback returns a deferred, then the callchain is paused until that deferred completes. So the concurrency limits should be maintained in my approach.\n- When the result of a DeferredList is encountered, a SplitItem exception is raised to exit the callchain.\n\nHiccups:\n- I had to add two extra functions at the end of the pipeline just to handle corner cases. Not sure if there's another way around it.\n- My plan was to show the split items when the exception is thrown, but I'm not sure how to do that.\n- When an item is split, scraper._itemproc_finished should show it like a normal item, as being scraped from the original response. I don't know how to access the response without changing the function signature.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "aron-bordin": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1954", "title": "[WIP] support multipart/form-data form encoding and file upload", "body": "### PR Overview\n\nThis PR add support for multipart/form-data form encoding.\n\nFixes https://github.com/scrapy/scrapy/issues/1897\n- [x] Handle request header\n- [x] Encode formdata\n- [x] support file upload\n- [ ] docs\n1. Form fields and form files are present in the `formdata` parameter. To add a file in the `formdata`, it's necessary to use the new `MultipartFile` object (this is not yet documented, because I'd like to confirm if this api design is good first.) From the [multipart/form-data](https://www.w3.org/TR/html401/interact/forms.html#h-17.13.4) spec, files requires a special treatment, thats why we have the `MultipartFile` object. Non-files can be used as usual in the `formdata` parameter, including binary content.\n2. Adds the `class MultipartFormRequest(FormRequest)`, designed to work exclusively with multipart requests.\n3. A sample Spider using  this feature:\n   \n   ``` python\n   # coding: utf-8\n   import scrapy\n   from scrapy.http.request.form import MultipartFormRequest, MultipartFile\n   \n   \n   class SampleSpider(scrapy.Spider):\n   \n       name = 'sample'\n       start_urls = ['http://0.0.0.0:5001']\n   \n       def parse(self, response):\n           sample_file = MultipartFile('example \u00a3.png', open('/tmp/a.png').read())\n           data = {\n               'name': 'Test',\n               'file': sample_file,\n           }\n   \n           return MultipartFormRequest.from_response(response, formdata=data)\n   ```\n\n**updated at Oct 13**\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1649", "title": "make spider name optional #1157", "body": "## PR Overview\n\nThis pr makes the name attribute optional(see https://github.com/scrapy/scrapy/issues/1157#issue-68467857).\n### New behavior\n\nIf the class attr `name` is None, the class name will be used. \n\nSome scrapy components access the class name before creating an instance of the Spider. So the name attr and its default value must be visible before/after creating the obj instance.\n\nI started using a classmethod `get_spider_name`, however I needed to edit some points to make it compatible(https://github.com/scrapy/scrapy/blob/master/scrapy/spiderloader.py#L25, https://github.com/scrapy/scrapy/blob/master/scrapy%2Fcommands%2Fcheck.py#L78, and more). And then it was failing in the tests because some methods use Spider classes and instances(https://github.com/scrapy/scrapy/blob/master/scrapy%2Futils%2Furl.py#L32), so classmethod was not compatible here. I just found this one because it failed in a test, so I think that it's  safer to implement something that is compatible with the old name attr.\n\nSo I used a new approach, a class property.\n\nSome tests were summing that Spider0 was not valid because it had no name. These tests were updated to handle this class, and there is a test to check if the default name is being used. \n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kneufeld": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1909", "title": "[MRG+1] added faq entry regarding ssl errors on osx", "body": "@redapple here's that pull request you... requested.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "DecKen": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1860", "title": "add notes when use MongoPipeline", "body": "see:**http://stackoverflow.com/questions/33524517/mongodb-invaliddocument-cannot-encode-object**\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mjsiegfried": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1797", "title": "Added DeltaLeveldbCacheStorage class", "body": "Our computer science capstone team decided to create a storage backend for scrapy to solve our sponsor's specific problem. We decided to build a backend that uses delta encoding as a means to save space when storing lots of web pages that are self similar, so we built a backend using bsdiff4 to do the encoding and built our class based on the existing leveldb backend. \n\nThe backend will decompress data that is sent compressed by the server, serialize all the header data and body together in order to get better performance when calculating the diff, which it stores in LevelDB. On retrieval, it reconstructs the response from this form. \n\nWe will have some more formal test data available, but as an example we took a crawl from a popular web comic and were able to take a 39M (FilesystemStorageCache, without accounting for block size) or 5.7M (LeveldbCacheStorage) to a 1.6M cache size, so we think this may be a pretty useful tool for other Scrapy users.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "felixonmars": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1757", "title": "Use mitmproxy and netlib 0.15", "body": "Some changes to make test_proxy_connect more friendly to netlib/mitmproxy 0.15 series. However, there is still one failure here that I don't know how to fix:\n\n```\n__________________________________________________________ ProxyConnectTestCase.test_https_noconnect __________________________________________________________\n\nresult = None, g = <generator object test_https_noconnect at 0x7f36400c54b0>, deferred = <Deferred at 0x7f363505fd40 current result: None>\n\n    def _inlineCallbacks(result, g, deferred):\n        \"\"\"\n        See L{inlineCallbacks}.\n        \"\"\"\n        # This function is complicated by the need to prevent unbounded recursion\n        # arising from repeatedly yielding immediately ready deferreds.  This while\n        # loop and the waiting variable solve that by manually unfolding the\n        # recursion.\n\n        waiting = [True, # waiting for result?\n                   None] # result\n\n        while 1:\n            try:\n                # Send the last result back as the result of the yield expression.\n                isFailure = isinstance(result, failure.Failure)\n                if isFailure:\n                    result = result.throwExceptionIntoGenerator(g)\n                else:\n>                   result = g.send(result)\n\n/usr/lib/python2.7/site-packages/twisted/internet/defer.py:1128:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n/home/felix/projects/scrapy/tests/test_proxy_connect.py:64: in test_https_noconnect\n    self._assert_got_response_code(200, l)\n/home/felix/projects/scrapy/tests/test_proxy_connect.py:104: in _assert_got_response_code\n    self.assertEqual(str(log).count('Crawled (%d)' % code), 1)\n/usr/lib/python2.7/site-packages/twisted/trial/_synctest.py:437: in assertEqual\n    super(_Assertions, self).assertEqual(first, second, msg)\nE   FailTest: 0 != 1\n```\n\nI tried to print the logs for that test, and got:\n\n```\nboto DEBUG\n  Using access key found in config file.\nboto DEBUG\n  Using secret key found in config file.\nscrapy.middleware INFO\n  Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, HttpProxyMiddleware, ChunkedTransferMiddleware, DownloaderStats\nscrapy.middleware INFO\n  Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\nscrapy.middleware INFO\n  Enabled item pipelines:\nscrapy.core.engine INFO\n  Spider opened\nscrapy.extensions.logstats INFO\n  Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\nscrapy.telnet DEBUG\n  Telnet console listening on 127.0.0.1:6023\nscrapy.downloadermiddlewares.retry DEBUG\n  Retrying <GET https://localhost:8999/status?n=200> (failed 1 times): 400 Bad Request\nscrapy.downloadermiddlewares.retry DEBUG\n  Retrying <GET https://localhost:8999/status?n=200> (failed 2 times): 400 Bad Request\nscrapy.downloadermiddlewares.retry DEBUG\n  Gave up retrying <GET https://localhost:8999/status?n=200> (failed 3 times): 400 Bad Request\nscrapy.core.engine DEBUG\n  Crawled (400) <GET https://localhost:8999/status?n=200> (referer: None)\nscrapy.spidermiddlewares.httperror DEBUG\n  Ignoring response <400 https://localhost:8999/status?n=200>: HTTP status code is not handled or not allowed\nscrapy.core.engine INFO\n  Closing spider (finished)\nscrapy.statscollectors INFO\n  Dumping Scrapy stats:\n{'downloader/request_bytes': 804,\n 'downloader/request_count': 3,\n 'downloader/request_method_count/GET': 3,\n 'downloader/response_bytes': 774,\n 'downloader/response_count': 3,\n 'downloader/response_status_count/400': 3,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 2, 4, 5, 42, 56, 719896),\n 'response_received_count': 1,\n 'scheduler/dequeued': 3,\n 'scheduler/dequeued/memory': 3,\n 'scheduler/enqueued': 3,\n 'scheduler/enqueued/memory': 3,\n 'start_time': datetime.datetime(2016, 2, 4, 5, 42, 56, 556138)}\nscrapy.core.engine INFO\n  Spider closed (finished)\n```\n\nPlease let me know what else should be adjusted to fix this. Thanks!\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jdemaeyer": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1756", "title": "Fix Windows tests", "body": "Based on #1736.\n\nFixes #1758.\n\nA couple of minor changes to the tests to make them compatible with Windows.\n\nAlso addresses two smaller issues within Scrapy:\n- Small change to output foramatting of the `startproject` command. (Project name and template path were outputted as `%r` instead of `'%s'`, which screwed up paths that contain backslashes)\n- In `scrapy.utils.trackref`, on Windows (only), use `time.clock` instead of `time.time` for timestamping to restore deterministic ordering of tracked references\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1736", "title": "Add Appveyor integration", "body": "Resolves #1684 \n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1605", "title": "Add from_crawler constructor for feed exporters and storages", "body": "Addresses #1567. This moves the \"which constructor should I call\" logic from the middleware manager into a util function, and re-uses it in the feed exporter extension.\n\nFeed exporters and storages can now provide a `from_crawler()` class method and properly access settings.\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1591", "title": "[MRG+1] Allow updating dictionary-like settings with magic prefix", "body": "This PR would ease updating, not replacing, dictionary settings from the command line and from `Spider.custom_settings` by introducing a magix prefix:\n\n``` python\n>>> mysettings = BaseSettings({'MYDICT': {'key': 'val'}})\n>>> mysettings['MYDICT']\n{'key': 'val'}\n>>> mysettings['update:MYDICT'] = {'new_key': 'new_val'}\n>>> mysettings['MYDICT']\n{'new_key': 'new_val', 'key': 'val'}\n```\n\nI think this is nice-to-have but not must-have. Dictionaries can already be updated by overwriting `Spider.update_settings()`, and I'm not sure if there's many use cases for updating from the command line.\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1580", "title": "Change extensions/spiders/settings initialisation order", "body": "This is an implementation of @chekunkov's suggested changes from #1305. I will be uploading consecutive commits one-by-one in the next minutes so we can see which tests break at what point.\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1442", "title": "[WIP] Add-on callbacks for spiders", "body": "### [**>> View commits of this PR only <<**](https://github.com/jdemaeyer/scrapy/compare/enhancement/addons...jdemaeyer:enhancement/spider-addon-callbacks)\n\nBased on #1272 \n\nAllows spiders to implement the add-on interface, i.e. to provide `update_addons()` and `check_configuration()` callbacks in addition to the already existent `update_settings()`.\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1272", "title": "Add-ons", "body": "Based on ~~#1149 and #1423~~ ~~#1586~~ master\n\nCloses #591 and #1215\n\nImplementation of SEP-021 ([updated version from this PR](https://github.com/jdemaeyer/scrapy/blob/enhancement/addons/sep/sep-021.rst), [original version](https://github.com/scrapy/scrapy/blob/master/sep/sep-021.rst))\n\n[**Design decision to-do here**](https://github.com/scrapy/scrapy/pull/1272#issuecomment-152191871)\n\n**Old to-do:**\n- Design decisions\n  - [x] Configuration entry point (settled on `INSTALLED_ADDONS` setting and additional support for `scrapy.cfg` so far)\n  - [x] Integration into start-up process\n  - [x] Add-on interactions (can add-ons configure other add-ons?)\n  - [x] Find better name for `AddonManager`?\n  - [x] Update SEP\n- Code\n  - [x] Add-on interface\n  - [x] Base `Addon` class\n  - [x] Load add-on configuration from `scrapy.cfg`\n  - [x] Load add-on configuration from settings module\n  - [x] Search add-ons by name\n    - [x] Improve `get_project_path()` robustness (**done partially**, still fails when settings module does not live in project's root)\n  - [x] Load & verify add-ons\n  - [x] Add-on \"holding\" (a place where loaded add-ons live)\n  - [x] Helper functions to call add-on callbacks\n  - [x] Dependency clash management\n    - [x] ~~Add version support~~ Use `pkg_resources`' extensive dependency management\n    - [x] ~~Should `pkg_resources` be used throughout the manager and not just within the dependency checking?~~ Will go to separate PR if we decide to\n  - [x] Integrate into standard start-up process\n    - [x] Integrate dependency clash checks\n  - [x] Allow putting objects (not just class paths) in component lists, e.g. `PIPELINES` (check #1215)\n  - [x] Write add-ons for built-in components\n    - [x] Fix add-ons for components which do not prepend their settings with their name\n  - [x] ~~Move settings for built-in components from global namespace to add-ons (is this possible without making them harder to configure?)~~ Allow setting global namespace settings via add-on framework\n  - [x] Allow spider to implement add-on callbacks (like it already does with `update_settings()`)\n- Tests\n  - [x] Base `Addon` class\n  - [x] Loading configuration from `cfg` files\n  - [x] Loading configuration from Python modules\n  - [x] Search add-ons\n  - [x] Load and verify add-ons\n  - [x] Callback helper functions\n  - [x] Dependency clash management\n  - [x] Integration into start-up process\n    - [x] Fix [test for `check_configuration()`](https://github.com/scrapy/scrapy/pull/1272#issuecomment-118411511)\n  - [x] Putting objects in component lists\n- Documentation\n  - [x] Docstrings for `AddonManager`, then use autodoc in `docs/topics/api.rst`\n  - [x] New topic on add-ons\n    - [x] Mention `_addon` attribute, more versatile examples (using `_addon`, module as add-on)\n\n<!-- Reviewable:start -->\n\n[<img src=\"https://reviewable.io/review_button.png\" height=40 alt=\"Review on Reviewable\"/>](https://reviewable.io/reviews/scrapy/scrapy/1272)\n\n<!-- Reviewable:end -->\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wilfre": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1663", "title": "Append like behavior to item on different callbacks", "body": "Allows to add values on different callbacks to the same field of an item using the 'item=' constructor of the ItemLoader class\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rdowinton": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1641", "title": "Improved Selectors documentation", "body": "Rewrote introduction and 'Using selectors' (renamed to 'How to use selectors'). \n\nAdded the following sections and examples:\n- Testing selectors\n- Selecting images\n- Selecting by content\n- Selecting specific nodes\n- Scrapy-specific CSS pseudo-classes and pseudo-elements\n- Further reading\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1509", "title": "Added 'Why use Scrapy?' section to overview", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mkcor": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1640", "title": "[MRG+1] Let allowed domains be updated in offsite middleware", "body": "Hi,\n\nThis is a very small change code-wise but it allows for some interesting flexibility.\n\nSo far, `get_host_regex()`, which reads spider attribute `allowed_domains`, is only called when _opening_ the spider (by method `spider_opened()`). With this change, if/when `allowed_domains` are updated after opening the spider, these updates can be taken into account. This allows for more control.\n\nCredits for inspiration go to @nhuray.\n\nThank you for your consideration.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jersub": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1559", "title": "Add support for multipart upload to the S3 feed storage", "body": "This PR adds support for multipart upload to the S3 feed storage. It aims to allow uploading of large files (e.g. items containing HTML from a big crawl) and to fix the issue #960.\n\nSource snippets:\n- http://boto.readthedocs.org/en/latest/s3_tut.html#storing-large-data\n- boto/boto#2704\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jschnurr": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1548", "title": "[MRG+1] Fix #1287 - FilesPipeline properly handles file extensions when URL has parameters", "body": "First (original) method extracts extension from entire URL, but if that extension is null or has non-alpha characters, fall back to second (new) method of extracting extension from base url (without parameters).  Maintains existing behavior for previous tests.\n\n_Before_\n\n``` python\n>>> file_path(Request(\"http://localhost:8050/render.png?url=http://www.test.ca&timeout=30wait=3\"))\n'full/1691f03855fb23bc1e3be2618889a8d0d7ce15f8.ca&timeout=30wait=3'\n\n>>> file_path(Request(\"http://foo.bar/baz.txt?fizz\"))\n'full/a2b4913a62f65445aeae2bac08cd8c3b41d7195e.txt?fizz'\n```\n\n_After_\n\n``` python\n>>> file_path(Request(\"http://localhost:8050/render.png?url=http://www.test.ca&timeout=30wait=3\"))\n'full/1691f03855fb23bc1e3be2618889a8d0d7ce15f8.png'\n\n>>> file_path(Request(\"http://foo.bar/baz.txt?fizz\"))\n'full/a2b4913a62f65445aeae2bac08cd8c3b41d7195e.txt'\n```\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "agreen": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1484", "title": "Update exporters.py", "body": "Fixes #1428\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cyberplant": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1439", "title": "Update doc with our package distribution process", "body": "Related to #1438.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "barraponto": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1345", "title": "[MRG+1] Add optional external dependencies to extras_require", "body": "Having the external dependencies in extras_require would be useful to raise\nawareness of our optional features, let projects that depend on those features\ndeclare it in the proper syntax (like `scrapy[JMESpath]`) and have us thinking\ntwice before adding more of those.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sardok": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1310", "title": "Redirect downloadermiddleware: handle cyclic redirection.", "body": "Cyclic redirection's die by reaching **max_redirect_times** in current redirect middleware. This change makes this middleware, to ignore those kind of responses. Another possible behavior could be raising IgnoreRequest exception.\n\nThis PR also includes minor refactoring on RedirectMiddleware.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/897", "title": "Parameters from file", "body": "Passing many arguments to spider is tedious work. This PR gives scrapy the ability to use file content as argument. Influenced by '-'d parameter of curl command.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/644", "title": "Telnet find biggest obj function", "body": "I use this function quite a lot when investigating about memory issues so thought it could be useful to others as well.\n\nSince this function uses, low level functions from sys and gc, i could not find proper way of writing unit test to it.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rmuellerb": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1264", "title": "Update broad-crawls.rst", "body": "Added section on how to treat memory consumption problems of broad crawls.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mikeumus": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1261", "title": "Reads a little better.", "body": "From: To enable your images pipeline you must first add it to your project ITEM_PIPELINES setting:\nTo: To enable your images pipeline you must first add ITEM_PIPELINES to your project's settings:\n\nExplicitly saying `settings.py` may be even more helpful.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1221", "title": "Little docs fixes ", "body": ":smiling_imp: : :+1: \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stphivos": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1255", "title": "Add support to Selectors for querying by tag name and attribute filters", "body": "Hi, I thought it would be nice to have an alternative way to xpath/re/css selectors when searching for elements, that uses python built-in dictionaries in a declarative manner. While it feels simple and familiar it still provides a lot of flexibility and expressiveness. I have included an example in scrapy/selector/querytranslator.py. If you like my idea I can also update the unit tests and docs in a new pull-request. Thanks!\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tonal": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1182", "title": "Add check sitemap_follow conditions sutemsp url of robots.txt", "body": "Sitemap urls from robots.txt do not filtred by sitemap_follow regex.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/918", "title": "Meta var for skip cache read for request", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nramirezuy": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1087", "title": "New DownloaderMiddleware flow", "body": "Older flow:\n![downloadermdw flow](https://cloud.githubusercontent.com/assets/1042865/6755373/657d54ee-cf01-11e4-9570-9b9c1dd977fb.png)\n\nProposed flow:\n![downloadermdw flow_new](https://cloud.githubusercontent.com/assets/1042865/6755413/aff5aab2-cf01-11e4-9e82-08619d979721.png)\n\nImplementation might not be the best, but I'm open to suggestions.\n- `process_request` (nothing changed)\n- `process_response`\n  - returning None will keep the same response going in the chain.\n  - Response objects are queued in the `process_response` chain.\n  - can raise exceptions and these are sent to the `process_exception` chain.\n- `process_exception` (keep one or split in two - process_request_exception, process_response_exception).\n  - Will can a request or response instead of just a request.\n", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/876", "title": "sargs available on update_settings", "body": "With sargs available on updated_settings we can make use of more dynamic configurations.\n\nOne example could be `scrapy crawl spider -a debug=yes` where `debug=yes` disables a `FeedExporter` that upload to s3.\n", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/861", "title": "[WIP] fix xmliter namespace on selected node", "body": "This PR was triggered by [scrapy-users](https://groups.google.com/forum/#!topic/scrapy-users/VN6409UHexQ)\n\nActually `xmliter` populates a `Selector` with everything from the position 0 to the tag start, so if we had 100mb before the tag we want to iter it copy those 100mb across all the `Selector` objects. Also it just extract this info for the first tag and embed the rest on that, this can cause info crossing.\n\nIn this PR I kept the regex stuff even tho I think we should use something like [`iterparse`](https://docs.python.org/2/library/xml.etree.elementtree.html#xml.etree.ElementTree.iterparse).\n\nCurrently `xmliter_lxml` tests are failing due to it has a different API.\n", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/829", "title": "[WIP] optparse to argparse", "body": "It uses argparse whenever it can, based on `any('add_arguments' not in vars(cmd.__class__) and 'add_options' in vars(cmd.__class__) for cmd in cmds.values())`\n\n```\n$ scrapy --help\nusage: scrapy [-h]  ...\n\nScrapy 0.25.1 - no active project.\n\noptional arguments:\n  -h, --help    show this help message and exit\n\ncommands:\n  More commands available when run from project directory.\n\n\n    runspider   Run a self-contained spider (without creating a project)\n    shell       Interactive scraping console\n    settings    Get settings values\n    bench       Run quick benchmark test\n    version     Print Scrapy version\n    startproject\n                Create new project\n    fetch       Fetch a URL using the Scrapy downloader\n    view        Open URL in browser, as seen by Scrapy\n```\n\n```\n$ scrapy bench --help\nusage: scrapy bench [-h] [--logfile FILE] [-L LEVEL] [--nolog]\n                    [--profile FILE] [--lsprof FILE] [--pidfile FILE]\n                    [-s NAME=VALUE] [--pdb]\n\nRun quick benchmark test\n\noptional arguments:\n  -h, --help            show this help message and exit\n\nglobal arguments:\n  --logfile FILE        log file. if omitted stderr will be used\n  -L LEVEL, --loglevel LEVEL\n                        log level (default: DEBUG)\n  --nolog               disable logging completely\n  --profile FILE        write python cProfile stats to FILE\n  --lsprof FILE         write lsprof profiling stats to FILE\n  --pidfile FILE        write process ID to FILE\n  -s NAME=VALUE, --set NAME=VALUE\n                        set/override setting (may be repeated)\n  --pdb                 enable pdb on failure\n```\n", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/741", "title": "[MRG] item loader let in empty values different to none", "body": "Right now we cant insert values that evaluate `False`.\n\n``` python\n>>> from scrapy.contrib.loader import ItemLoader\n>>> from scrapy.contrib.loader.processor import TakeFirst, Identity\n>>> il = ItemLoader(item={})\n>>> il.price_in = TakeFirst()\n>>> il.add_value('price', 0.0)\n>>> il.load_item()\n{}\n>>> il.price_in = Identity()\n>>> il.price_out = TakeFirst()\n>>> il.add_value('price', 0.0)\n>>> il.load_item()\n{'price': 0.0} \n```\n", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/424", "title": "[MRG] added required option to settings module", "body": "", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/346", "title": "[WIP] linkfilters added", "body": "This is in stage of review, I want some feedback.\n\nPlease do not merge.\n", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/3048968", "body": "ssh! :dancer: \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3048968/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104492", "body": "It is 2014-01-17 on UTC :P\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104492/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/10262042", "body": "isn't 0 different from None in this case?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/10262042/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/15678552", "body": "[`TemporaryFile`](https://docs.python.org/2/library/tempfile.html#tempfile.TemporaryFile) gets deleted when is closed, so it just needs to be closed. On Unix doesn't look like it even creates the actual file.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/15678552/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3977462", "body": "Why this code delay? may not be better if is a setting? \n\nclosedelay = (slot.delay or .1) \\* DOWNLOADER_CLOSEDELAY\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3977462/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/4776636", "body": "You can leave size_limit as None. Everything is less than None.\n\n``` python\n>>> None < 'a'\nTrue\n>>> None < -10\nTrue\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/4776636/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6952175", "body": "and the `default response's type` is?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6952175/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6916320", "body": "`request middleware` should be `downloader middlewares`\nInstead of `Once the newly ...` why not something like `downloader middlewares, reschedule the returned request and whole process happens again`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6916320/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6917310", "body": "This isn't true, when you raise IgnoreRequest or any other exception It calls process_exception of every downloader middleware from the downloader side to the engine side, if not handled on the downloader middlewares it will try to call errback of the request. Spider Middlwares are never called.\n\nThe only difference between raise IgnoreRequest instead of a normal exception is that the IgnoreRequest is silenced and not showed on logs if nobody can handle it.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6917310/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6919305", "body": "This is really happening plus the errback is called. And the difference is between IgnoreRequest and normal Exceptions is the same as in process_request.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6919305/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6919931", "body": "There is no `immediate redirection` if a request is returned here it is rescheduled, not matters where the exception was.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6919931/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6943363", "body": "When you rise an exception on process_response the `process_exception` methods are not called at all. When I reference to `errback`I'm referring to the `Request.errback`. This part is fine: `If no code handles the raised\n-      exception, it is ignored and not logged (unlike other Exceptions).`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6943363/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "markbaas": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1067", "title": "url.py/_unquotepath: added '25' to reserved", "body": "The url \"/cmp/Supermercados-Dia%25\" was changed into \"/cmp/Supermercados-Dia%\". This is incorrect. Adding 25 to the reserved list fixes it.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1043", "title": "adding option to use html5lib instead of default htmlparser", "body": "#1039 using html5lib invalid chars as <, >, etc are not stripped. Safer, but a bit slower.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ChienliMa": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1051", "title": "Shortcut for idle", "body": "related to issue #740\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "h4ck3rm1k3": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1032", "title": "allow for direct classes", "body": "This is more of a hack but would like top open a discussion.\n\nThe patch will allow you to specify a class directly instead of a path name for loading. \n\nUsage here  https://github.com/flosokaks/scrapy_ks/blob/master/scrapy_openkansas.py\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gatufo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1014", "title": "Added periodic stats. Allows periodical stats generation and processing.", "body": "Added support to periodically selecting, exporting/processing scrapy stats.\n\nSome use cases can be:\n- Monitor and graph scraping data over time (memory usage, http codes, errors, scraped items, etc).\n- Get notified when a crawl job has finished or alerted when something happens.\n- Use your own stat values to monitor custom data or events.\n\nTime-based graphs can be generated using a 3rd party tool like [Kibana](http://www.elasticsearch.org/overview/kibana/).\n\nJob is done by a `StatCollector` class called `PeriodicStatsCollector`. It can be configurated with:\n- `Value Observers`: To select and configure the filtered periodic stats.\n- `Stats Pipelines`: To process the generated stats.\n\nAll configuration is done via settings, an example that will log responses count every 30 seconds:\n\n``` python\nfrom scrapy.contrib import periodicstats as stats\n\nSTATS_CLASS = 'scrapy.contrib.periodicstats.PeriodicStatsCollector'\nPERIODIC_STATS_OBSERVERS = [\n    stats.Observer(key='downloader/response_count', export_interval=30),\n]\nPERIODIC_STATS_PIPELINES = [\n    'scrapy.contrib.periodicstats.PeriodicStatsLogger',\n]\n```\n\nThat will produce the following output:\n\n```\n2015-01-16 11:29:10+0100 [scrapy] INFO: Dumping Scrapy periodic stats:\n    {'interval': 1,\n     'spider': 'example',\n     'stats': {'downloader/response_count': 51}}\n2015-01-16 11:29:40+0100 [scrapy] INFO: Dumping Scrapy periodic stats:\n    {'interval': 2,\n     'spider': 'example',\n     'stats': {'downloader/response_count': 489}}\n...\n```\n## Value Observers\n\nThey allow to select data from the available Stats Collection and define when it should be generated. \n\nSome examples...\n\nYou can define many observers:\n\n``` python\nPERIODIC_STATS_OBSERVERS = [\n    stats.Observer(key='downloader/request_count'),\n    stats.Observer(key='downloader/response_count'),\n    stats.Observer(key='httpcache/hit'),\n]\n\n\"\"\"\nGenerated stats:\n{'downloader/request_count': 10, 'downloader/request_count': 10, 'httpcache/hit': 0}  # for t=1\n{'downloader/request_count': 23, 'downloader/request_count': 20, 'httpcache/hit': 3}  # for t=2\n{'downloader/request_count': 23, 'downloader/request_count': 20, 'httpcache/hit': 3}  # for t=3\n{'downloader/request_count': 34, 'downloader/request_count': 28, 'httpcache/hit': 7}  # for t=4\n...\n\"\"\"\n```\n\nTo use a different name for the stats value use the `export_key` parameter:\n\n``` python\nPERIODIC_STATS_OBSERVERS = [\n    stats.Observer(key='downloader/request_count', export_key='requests'),\n    stats.Observer(key='downloader/response_count', export_key='responses'),\n    stats.Observer(key='httpcache/hit', export_key='from_cache'),\n]\n\n\"\"\"\nGenerated stats:\n{'requests': 10, 'responses': 10, 'from_cache': 0}   # for t=1\n{'requests': 23, 'responses': 20, 'from_cache': 3}   # for t=2\n{'requests': 23, 'responses': 20, 'from_cache': 3}   # for t=3\n{'requests': 34, 'responses': 28, 'from_cache': 7}   # for t=4\n...\n\"\"\"\n```\n\nYou can use several observers at the same key, but then `export_key` can\u2019t be repeated:\n\n``` python\nPERIODIC_STATS_OBSERVERS = [\n    stats.Observer(key='downloader/request_count', export_key='requests_counter'),\n    stats.Observer(key='downloader/request_count', export_key='another_request_counter'),\n]\n\n\"\"\"\nGenerated stats:\n{'requests_counter': 10, 'another_request_counter': 10}  # for t=1\n{'requests_counter': 23, 'another_request_counter': 23}  # for t=2\n{'requests_counter': 23, 'another_request_counter': 23}  # for t=3\n{'requests_counter': 34, 'another_request_counter': 34}  # for t=4\n...\n\"\"\"\n```\n\nBy default values are accumulated, but we can also use just differences between intervals with the `use_partial_values` parameter:\n\n``` python\nPERIODIC_STATS_OBSERVERS = [\n    stats.Observer(key='downloader/request_count', export_key='requests', use_partial_values=True),\n]\n\n\"\"\"\nGenerated stats:\n{'requests': 10}  # for t=1\n{'requests': 13}  # for t=2\n{'requests': 0}   # for t=3\n{'requests': 11}  # for t=4\n...\n\"\"\"\n```\n\nTo minimize generated data we can choose to export data only when it changes with the `only_export_on_change` parameter:\n\n``` python\nPERIODIC_STATS_OBSERVERS = [\n    stats.Observer(key='downloader/request_count', export_key='requests', only_export_on_change=True),\n]\n\n\"\"\"\nGenerated stats:\n{'requests': 10}  # for t=1\n{'requests': 13}  # for t=2\n{}                # for t=3\n{'requests': 11}  # for t=4\n...\n\"\"\"\n```\n\nWe can define export intervals per key with the `export_interval` parameter:\n\n``` python\nPERIODIC_STATS_OBSERVERS = [\n    stats.Observer(key='downloader/request_count', export_key='requests', export_interval=1),\n    stats.Observer(key='downloader/response_count', export_key='responses', export_interval=2),\n]\n\n\"\"\"\nGenerated stats:\n{'requests': 10, 'responses': 10}  # for t=1\n{'requests': 23}                   # for t=2\n{'requests': 23, 'responses': 20}  # for t=3\n{'requests': 34}                   # for t=4\n...\n\"\"\"\n```\n\nTo export only stats when a spider is finished use the `only_export_on_close` parameter:\n\n``` python\nPERIODIC_STATS_OBSERVERS = [\n    stats.Observer(key='downloader/request_count', export_key='requests', only_export_on_close=True),\n]\n\n\"\"\"\nGenerated stats:\n{}                  # for t=1\n{}                  # for t=2\n{}                  # for t=3\n{}                  # for t=4\n...\n{'requests': 2759}  # on spider_close\n\"\"\"\n```\n\nRegular expressions can be used to group/filter data with the `use_re_key` parameter::\n\n``` python\nPERIODIC_STATS_OBSERVERS = [\n    stats.Observer(key='downloader/response_status_count/2..', use_re_key=True, export_key='2xx'),\n    stats.Observer(key='downloader/response_status_count/3..', use_re_key=True, export_key='3xx'),\n    stats.Observer(key='downloader/response_status_count/4..', use_re_key=True, export_key='4xx'),\n    stats.Observer(key='downloader/response_status_count/5..', use_re_key=True, export_key='5xx'),\n]\n\n\"\"\"\nGenerated stats:\n{'2xx': 10}                               # for t=1\n{'2xx': 34, '3xx': 1, '4xx':1}            # for t=2\n{'2xx': 67, '3xx': 4, '4xx':2, '5xx': 1}  # for t=3\n{'2xx': 90, '3xx': 8, '4xx':2, '5xx': 1}  # for t=4\n...\n\"\"\"\n```\n## Stats Pipelines\n\nBasically work like item pipelines, they have a `process_stats` method where they will receive the generated stats from the observers for every interval.\n\n``` python\nclass PeriodicStatsPipeline(object):\n    \"\"\"\n    Base class for periodic stats pipeline\n    \"\"\"\n    def __init__(self, crawler):\n        pass\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(crawler)\n\n    def open_spider(self, spider):\n        pass\n\n    def close_spider(self, spider, reason):\n        pass\n\n    def process_stats(self, spider, period, stats):\n        raise NotImplementedError\n\n\nclass PeriodicStatsLogger(PeriodicStatsPipeline):\n    \"\"\"\n    Stats processor that print stats debug info\n    \"\"\"\n    def process_stats(self, spider, interval, stats):\n        log.msg('Dumping Scrapy periodic stats:\\n' + pprint.pformat({\n            'spider': spider.name,\n            'interval': interval,\n            'stats': stats,\n        }))\n        return stats\n```\n\nNot implemented yet, but some uses may be exporting to ELK through [redis](http://redis.io/)>[logstash](http://logstash.net/) or exporting stats to [hubstorage](https://github.com/scrapinghub/hubstorage) using its API.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "aliowka": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/1002", "title": "Offsite awared redirect ", "body": "Here is my attempt to solve this issue https://github.com/scrapy/scrapy/issues/15.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/10210624", "body": "This fixes issue #1074\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/10210624/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "adon-at-work": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/991", "title": "dupefilter should record requests even with dont_filter", "body": "dupefilter must see and record all requests even with dont_filter, and\nthus a subsequent duplicate request happened to have no dont_filter can\nbe filtered out.\n\nExample: consider having some start_urls marked as dont_filter, any\nself-referencing urls will not be de-duplicated.\n\nApparently this problem was introduced since Jun 2011 after a refactoring at\nhttps://github.com/scrapy/scrapy/commit/03751749a80526f3d97f60f0f63892501ef5de19\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "aufziehvogel": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/984", "title": "added test cases for exporting collection items like dict, list", "body": "Test cases for issue #532, which has already been fixed some time ago.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6235359", "body": "Was required for the test to run with the comment \"wrong tag without href\". But if it's better to keep such a change out of the merge, I can remove the comment from the test and undo this change.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6235359/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "jtwaleson": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/956", "title": "pep8 and pyflakes applied", "body": "I applied PEP8 to most places where deviations did not seem necessary.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mattfullerton": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/949", "title": "Use Reppy instead of robotsparser", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bernardotorres": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/941", "title": "Adding support to FTP file listing", "body": "Make possible to scan FTP repositories. If the path of the request ends in / in a FTP URL, calls FTP list.\n\nIt assigns a variable: response.files with a list, each element with the structure of a FTPFileListProtocol (see: http://twistedmatrix.com/documents/8.2.0/api/twisted.protocols.ftp.FTPFileListProtocol.html):\n\nfiletype: e.g. 'd' for directories, or '-' for an ordinary file\nperms: e.g. 'rw-r--r--'\nnlinks: e.g. 1\nowner: e.g. 'root'\ngroup: e.g. 'other'\nsize: e.g. 531\ndate: e.g. 'Jan 29 03:26'\nfilename: e.g. 'README'\nlinktarget: e.g. 'some/file'\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "orian": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/915", "title": "Adds parsing filename from HTTP header.", "body": "When downloading a file from URL, the HTTP server may return a name of file the content should be saved as.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "timfeirg": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/884", "title": "modify the autothrottle extension so that it respects spider.download_delay the whole time", "body": "### The problem\n\nmy spider really need to run with the ability to change the delay between each request on the run to avoid getting banned, autothrottle extension wasn't helping because a lot of people on my LAN was also trying to reach that domain, autothrottle can't see all that traffic towards a particular domain **on my LAN** based only on download elapse.\n### My solution\n\n**( which didn't work due to the behavior of autothrottle )**\n\nI have to write an extension to modify spider.download_delay based on `datetime.now().weekday()`, if it's a working day, my spider should slow down during working hours, but it won't work because **autothrottle extension ignores spider.download_delay** the whole time except at spider_opened signal.\n### My changes to autothrottle.py\n\nI modified this extension a bit so that it refreshes `self.mindelay` at each `response_downloaded` signal, this way autothrottle respects spider.download_delay the whole time.\n### why I think it's important\n\notherwise if somebody wants to implement some sort of dynamic download_delay, they'll have to wrote another extension to deal with the whole `request slot` thing, which can be totally avoided if autothrottle looks at spider.download_delay the whole time.\n\nI believe this is the right thing to do because people do change spider.download_delay during a spider run, download_delay should be able to describe the actual download delay the whole time and that's the way things should be.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kyzi007": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/773", "title": "add signal name", "body": "From debug or mass subscription\n\ndef event_handler(self, **kwargs):\n    event_name = kwargs.get('signal').name\n    self.log(event_name)\n\n```\nif(event_name == \"engine_started\"):\n    ...\n```\n\ncrawler.signals.connect(event_handler)\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bertinatto": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/772", "title": "Add method that allows joining the extracted result into a string", "body": "Would it be ok if we add a method to join the extracted result into a string?\n\nFor example, usually, we want the result of a field to be a string. To get this, we often do something like this: \n\n`var = ''.join(sel.xpath('//div[@id=\"title\"]/text()').extract())`\n\nOr when we're sure that there is only one item in the list (can throw an exception if the list is empty):\n\n`var = sel.xpath('//div[@id=\"title\"]/text()').extract()[0]`\n\nI think it would be more convenient if we could do something like this:\n\n`var = sel.xpath('//div[@id=\"title\"]/text()').join()`\n\nOr even:\n\n`var = sel.xpath('//div[@id=\"title\"]/text()').join(',')`\n\nIn my opinion, this would increase the code readability.\n\nWhat do you guys think of this?\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alexcepoi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/768", "title": "[MRG] contracts support for multiple requests and batches", "body": "This allows to:\n- have multiple batches of contracts for each method (run separately)\n- have multiple requests in each batch of contracts (i.e. multiple @url contracts)\n\nChanges:\n- create_request and adjust_request replace adjust_request_args (all custom contracts using adjust_request_args will stop working)\n- Contract.args deprecated in favour of Contract.input_string\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1457975", "body": "what about maxitems, maxrequests? Or the case where you expect to receive exactly one request (the original sep describes a returns_request contract which checks this).\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1457975/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458004", "body": "Ignoring sounded unfriendly to me at first (i.e. you may wonder why it does not work). Also eliminating the assertion gives a very misleading error.\nListing contracts sounds good to me maybe a \"--list / -l\" option?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458004/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458025", "body": "This is a reminiscence from the original implementation. I thought it could prove useful, but now that I think about it's hard to find a scenario in which adjust_request_args is insufficient.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458025/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458066", "body": "I did not think priorities would be useful, or that we should encourage users to rely on contracts priorities.\nEspecially since for some hooks (pre_process comes into mind) the last contract hooked in is the first one to be executed.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458066/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458090", "body": "note: also change docstring\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458090/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570864", "body": "`@returns requests 2` - returns at least two requests\n\n-- wouldn't this be considered unexpected behaviour?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570864/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570929", "body": "It's an outdated diff (i.e. comment on a line of code which has been modified). You should still be able to find it in the issue\n\nOn Sep 10, 2012, at 11:30 PM, Pablo Hoffman notifications@github.com wrote:\n\n> In scrapy/contracts/default.py:\n> \n> > +# contracts\n> > +class UrlContract(Contract):\n> > -    \"\"\" Contract to set the url of the request (mandatory)\n> > -        @url http://scrapy.org\n> > -    \"\"\"\n> >   +\n> > -    name = 'url'\n> >   +\n> > -    def adjust_request_args(self, args):\n> > -        args['url'] = self.args[0]\n> > -        return args\n> >   +\n> >   +class ReturnsContract(Contract):\n> > -    \"\"\" Contract to check the output of a callback\n> > -        @returns items, 1\n> > -        @returns requests, 1+\n> >   I can't see that comment on github now, did you remove it? On Mon, Sep 10, 2012 at 6:24 PM, Alexandru Cepoi notifications@github.comwrote:\n> >   \u2026\n> >   \u2014\n> >   Reply to this email directly or view it on GitHub.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570929/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1656221", "body": "`get_func_args` does not return defaults (which I am intersted in) but only the name of the arguments, so I can't reuse it. I'm gonna move `get_spec` in scrapy.utils.python and improve it and it's doctests.\n\n`get_func_args` is also untested (formally at least) and does not work with bounded methods (should I fix that in a separate PR?). There's also a global exception handler there whose role I don't understand.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1656221/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1961493", "body": "I don't really like this call to '_makeResult', but could not find a better way\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1961493/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7133189", "body": "definitely, but shouldn't requirements.txt set up a full development environment?\ncssselect is already included in setup.py\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7133189/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "tpeng": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/766", "title": "show all the missing field when scrapes contract fails", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/4776859", "body": "yep. that's a good idea. thanks\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/4776859/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6343218", "body": "since there is a return above (line 137)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6343218/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6343229", "body": "it will close the connection and the idea was to still use `CancelledError`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6343229/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6370512", "body": "called by `_cancel` in `_cb_bodyready`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6370512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6370517", "body": "yep. it's better!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6370517/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6995411", "body": "i see! so do you think adding `txresponse._transport._producer.loseConnection()` after line 118 is ok?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6995411/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "bijzz": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/756", "title": "Changed feed to HTMLParser to unicode instead of str.", "body": "A lot of sites yielded `UnicodeDecodeError` when using `HtmlParserLinkExtractor().extract_links(response)`. When the `HTMLParser` receives the `response.body` as `unicode` the exceptions dissappear. Maybe you can still replicate this with one or the other url i posted on [stackedit](https://stackoverflow.com/questions/24351023/scrapy-linkextractors-fail) (but these might work on your system depending on the system default encoding).\n\nAlso have a look at the HTMLParser documentation in Python 2 [docs.python.org/2.7](https://docs.python.org/2.7/library/htmlparser.html?highlight=parser#HTMLParser.HTMLParser.feed) stating `data can be either unicode or str, but passing unicode is advised.`.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "oliverrahner": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/751", "title": "Fix handling of empty clickables in FormRequest.from_respone()", "body": "Empty Clickables lead to an exception, because unicode_to_str is called with an argument of None, which should be of type unicode.\nThis is fixed by checking for None values and setting them to u''\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yhager": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/716", "title": "  Added --overwrite-output (-O) option", "body": "How about this for #547? This is my first time writing code for scrapy, so please scrutinize.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vatsalj": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/653", "title": "Resolved issue #508 final url getting quoted if | present in url. Also added tests for the same.", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "illarion": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/605", "title": "Fixed memory issue in sitemap spider by implementing less memory consuming sitemap class", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "arijitchakraborty": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/487", "title": "Implementation of inheriting of all parents' fields during multiple inheritance of Item class", "body": "# SEP-21\n\n---\n\nItems when inherited from multiple item classes do not inherit all of its parents'  fields. It inherits fields only from its first parent. This is because of the way Scrapy handle's the Item class. All fields are stored using a dict attr - 'fields' implemented in the DictItem class. So during multiple inheritance this 'fields' attr is inherited from the first parent.\n\nThis proposal aims to inherit all parents' fields during multiple inheritance of a Item class.\n\nPlease refer to https://gist.github.com/arijitchakraborty/7868842 for details.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5270491", "body": "Included this in the new commit : https://github.com/arijitchakraborty/scrapy/commit/66ff34cf05f5ed1624e8efb39d2b311ffe3ba5f4\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5270491/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "srmaximiano": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/447", "title": "Command line option post", "body": "Here's a post option for the commands fetch, view and shell. \nPlease review and comment.\nSee issue description on #36\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pedrofaustino": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/413", "title": "Probe command (fix issue #33)", "body": "Here's a first take at adding the probe command. Please review and comment.\n\nHow can we use the MockServer to add tests?\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2516634", "body": "I just followed the documentation,\nhttp://doc.scrapy.org/en/0.16/topics/downloader-middleware.html#writing-your-own-downloader-middleware\n\nMy rationale was to implement a rfc2616-aware storage that based on the\nrequest would return different values, which back at the middleware would\ntrigger certain behavior on either the scheduler or other middleware.\n\nOriginally I've added cache logic to both the middleware and the storage.\nWith my limited knowledge of Scrapy I don't see how we can have cache\npolicy totally decoupled from storage, since it will need to access\nResponse and Request headers.\n\nExample, a 'no-store' directive should be handled at the middleware level\nwithout storage ever knowing about it. But a 'Expires' header should be\nhandled at the storage level.\n\nPerhaps we need two policies, one at the Middleware level and another at\nthe Storage level?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2516634/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "dfdeshom": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392118", "body": "Hi, to disable `scrapy.contrib.feedexport.FeedExporter`, I had to set it to `None` in my `EXTENSIONS` dict instead of `0`. I am getting this error when using scrapyd.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392118/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392185", "body": "off-topic but: in general to disable any extension, middleware, etc I have to set it to `None`, which can be a little confusing since you would think that `0` would work too\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392185/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/6111375", "body": "@dangra unfortunately, I don't see a documented way to disable an extension and I consider myself pretty familiar with scrapy and its docs. Maybe it's already there in the docs, but simply needs to be more prominent.\n\nFor me there is a larger issue: some extensions have settings associated with them that make it confusing to know exactly how/where to disable them. For example, the cookies extension seems to have 2 ways to disable it:\n-  set `COOKIES_ENABLED` to `False` (https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/downloadermiddleware/cookies.py)\n- set the priority to `None`, ie set `scrapy.contrib.downloadermiddleware.CookiesMiddleware` to `None`\n\nIt would be nice to have just one way to disable extensions.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/6111375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "NicolasP": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/179992831", "body": "Hi, any chance for a review of this PR?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/179992831/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "knaveofdiamonds": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/220300951", "body": "@kmike - ah, ok. I'm working on an inherited codebase that doesn't use the `LinkExtractors` directly, so this was obviously just a misunderstanding of the API/using something non-public. I'll close this issue - agree with backwards incompatible change reasons.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/220300951/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "nside": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29349706", "body": "Seeing the same issue\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29349706/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32618703", "body": "@dangra crawl() fixed it for me. Still I'd expect any requests scheduled to have the same treatment, whatever their \"entry point\" in the pipeline is. Feel free to close if you disagree.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32618703/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32620941", "body": "I'm on 0.21 (dev). These are good subtleties to know!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32620941/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "robyoung": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/736606", "body": "Hi,\n\nI've copied a URL below which failed with the original function because it has no url argument and the arguments are in an unexpected order. I can certainly add some unit tests, I'll have a look into it now. It's the end of the day and I want some food!\n\nCheers,\nRob\n\nhttp://www.firstchoice.co.uk/fcsun/page/search/searchresults?sttrkr=mthyr:02/2011_durT:7/n_ls:true_tuidesc:000832_day:15_mps:9_isvid:false_pconfig:1|2|0|0|0|_tchd:0_rating:0_act:0_jsen:true_resc:_attrstr:||||||||null|null_mdest:false_tinf:0_mnth:02_desc:_bc:17_margindt:7_tadt:2_numr:1_spp:mainSearch_depm:7_tuiresc:004287_dur:7_dtx:0_df:false_dxsel:0_imgsel:0_dac:MAN_loct:0_tsnr:0_year:2011_dta:false_tuiacc:028367_acc:\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/736606/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "italomaia": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/65951615", "body": "Which distro are you using? Here in ubuntu, works fine. Try:\n\napt-get install -y build-essential python-dev\napt-get install -y libssl-dev libxml2-dev libxslt1-dev libssl-dev libffi-dev\n\nthen pip install scrapy in a brand new virtualenv\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/65951615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/66092982", "body": "Let me try again:\n\nI want to use scrapy like this:\n\n```\n# prints options\nscrapy crawl -t csv|sql|mongo|etc -h\n\n# uses a particular item exporter\nscrapy crawl -t csv -f somefile.csv\nscrapy crawl -t mongo --db somedb --col somecollection\n```\n\nWhat do you guys think of this? Is it desired behavior? Does not feel like a whole lot of code to modify. I could do it. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/66092982/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ariddell": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/18024661", "body": "@nramirezuy there's a reference implementation for pep 3156 here: https://code.google.com/p/tulip/\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/18024661/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "sabren": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/2305629", "body": "Here you go: \n\nhttps://github.com/scrapy/scrapy/pull/45\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/2305629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/165490", "body": "Thanks, Scotty! \n\nI'm sure my client would appreciate it.\n\nCan you make a combined pull request, or should I pull from you and open another pull request here?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/165490/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "mvj3": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/156869848", "body": "Thanks @curita for the careful review! I correct it in a new commit.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/156869848/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "RFDAJE": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/314142937", "body": "I had been facing the same issue, so far the simplest solution I found is inside `item_completed`, after getting all things done, reset downloaded to empty. `self.spiderinfo.downloaded = {}`, memory leak issue resolved. ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/314142937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "paulproteus": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/8343573", "body": "I did some work refreshing these patches against current origin/master.\n\nhttps://github.com/paulproteus/scrapy/tree/revise-pullrequest-109\n\nSome notes:\n\nSee 3077ac8b6f592b044ad67f15af3b065d06f27cf7 for a fix where Http11DownloadHandler needs to accept second argument, since that's how the tests use it.\n\nSee 888dfadd24f7d325bffab367acc9dd5a7405e5bc for a fix where the call to log.err() creates an exception in the log that test runner notices, so tests that call this method begin to fail. For now I've disabled the call to log.err(). If we want to keep logging the error, we'll need to call flushLoggedErrors() -- see http://twistedmatrix.com/documents/current/core/howto/trial.html#auto11 . (The test this makes fail is scrapy.tests.test_downloader_handlers.Http11TestCase.test_timeout_download_from_spider )\n\nCommit bdd2a1b02c944845fec4d6142fcb63367b17cc11 (rebased from the original, but otherwise the same) makes many tests fail because there is work left in the reactor. One test you can see this with is scrapy.tests.test_engine.EngineTest.test_crawler\n\nFor now I can't promise I'll have time to figure out what's going on with leaving the reactor unclean, but I thought I'd leave a comment with what I have found in a few hours of looking into this pull request!\n\n-- Asheesh.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/8343573/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "berkerpeksag": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/121303843", "body": "Sure, but that list is a bit outdated. For example, `twisted.web.static` has already been [ported to Python 3](https://github.com/twisted/twisted/commit/ab110cc815c31eed17bb5ef06f1037a8ede0b395). You may want to check `twisted/python/dist3.py` first.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/121303843/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/77748093", "body": "Good point, thanks! Updated PR.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/77748093/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/85468979", "body": "Sorry for the cryptic commit message. I didn't propose to switch to unittest or something else. I needed to reproduce the issue to work on it, but couldn't see any deprecation warnings on my machine or on Travis.\n\nunittest and probably other test frameworks set Python's warning level to \"default\" or \"all\" by default. pytest completely ignores warnings and doesn't show any warnings unless they are raised by pytest itself (you can see those warnings by using the `-rw` option). `py.test -s` doesn't work either.\n\nThere is an open issue about this: https://bitbucket.org/pytest-dev/pytest/issue/253/pytest-should-integrate-with-warnings\n\n(recwarn doesn't work either since it's use case is different.)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/85468979/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/108508498", "body": "> [...] in that case while your code is accepted by Twisted developers Scrapy codebase has to point the tox env to your twisted fork.\n\nGood point!\n\n> There is not significant work to merge this PR, it's better if you work on a branch named gsoc-py3-port (or similar) and submit a single PR with ported code in following commits. \n\nI was thinking to send incremental patches(especially for non-Twisted ones)  to avoid potential merge conflicts.\n\n(My main work will be on the `python3` branch.)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/108508498/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/110585428", "body": "Good idea, will open a new PR for your suggestions. Thanks! Also, I noticed that --set is a global option, but --get\\* options are only usable with the settings command. Is this intentional?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/110585428/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "panaggio": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/25048688", "body": "Yeah, totally agree with @dangra. Joining is just one special case. There are several others. Use loaders :)\n\nFrom my experience, `w3lib` can help with 90% of the special cases, trivial or not (besides some too trivial cases where you shouldn't even need to rely on a external lib). If you find another common special case, it's a good place to add them :)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/25048688/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/25047845", "body": "Minimal awesome changes. +1\n\nNot that it was intended, but Ruby people (that know Nokogiri) will love this change.\n\nIn the spirit of Tenderlove, <3 <3 <3 <3 <3 <3 <3 <3 <3 \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/25047845/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/34211300", "body": "Adding jinja2 to scrapy's dependency (even if just test) seems too much to me.\n\nI have tried to reproduce that myself too but couldn't either till now.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/34211300/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "djm": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/426681", "body": "@pablohoffman Cheers!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/426681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "aalvarado": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/749296", "body": "```\nfound in the dmoz directory\n```\n\nI think this wasn't updated. Should say `tutorial` now.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/749296/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "mohsinhijazee": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2180902", "body": "The settings have FEED_URI whereas here it is picking up from SCRAPY_FEED_URI. I think this is kind of conflicting  here.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2180902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "nuklea": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2701979", "body": "`isinstance(arg, (dict, BaseItem))` short.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2701979/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1245155", "body": "What about pep8?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1245155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "brunsgaard": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2853556", "body": "Ahh yeah.. good point. I will write up another commit in the near future taking the settings instance from the crawler. Had totally forgotten about overrides :/\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2853556/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "archerhu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2965291", "body": "can you explain why you remove the try catch?\n\nat 1.6 version, using ImagePipeline may raise a lot \"exceptions.IOError: image file is truncated\"\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2965291/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2976024", "body": "I get your\u00a0motivation,thanks very much\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2976024/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "pombredanne": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/5690419", "body": "@pablohoffman : \nI cannot make up which use case this line is supposed to support. \nThere are no tests for it BTW. Why splitting a tag name on curly braces } ?  \n\nThere is similar code in https://github.com/scrapy/scrapy/commit/cea0dae1b2fe52629ab2be26e11876473b3c5b3a#diff-b7bb18a9cf07dcc374b08c0b05746850R26\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5690419/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "shaneaevans": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/6978256", "body": "nice commit message :+1: \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/6978256/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3072601", "body": "Can we rely on the order of fields as this test does? Is there something preventing \"child\" being first and \"name\" being second?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/3072601/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "jojje": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/9748419", "body": "Was cleaning out my unwieldy list of repos today, deleting any I thought I hadn't yet committed to. By mistake I killed off my the scrapy fork, after which I noticed I had this active pull request.\n\nWell it seems to still be in your pull queue for the master repo and if you were to appreciate it, then I guess you would be able to merge / rebase based off that information. If not, just ping me and I'll re-fork, re-apply the changes and update the PR.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/9748419/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "jpcormiergoogle": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/13113726", "body": "Dumb question: should we also extend ('2f', '2F', '3f', '3F') to ('2f', '2F', '3f', '3F', '25').  I found some urls with %25XX in the path and those were also removed by the unquote function.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13113726/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13114249", "body": "Here is an example url\n\nhttp://schools-wikipedia.org/wp/m/Maxwell%2527s_equations.htm\n\nOn Tue, Sep 8, 2015 at 3:57 PM Daniel Gra\u00f1a notifications@github.com\nwrote:\n\n> I think it is sensible to escape 25 too, can you provide an example url\n> and even better a fix ?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/scrapy/scrapy/commit/0d3e4b4c43711c4b02db6813ebad99933ee8858d#commitcomment-13114145\n> .\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13114249/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "foresightyj": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/13176549", "body": "I encountered errors with both \"FifoDiskQueue`and`FifoSQLiteQueue`. \n\n![image](https://cloud.githubusercontent.com/assets/1296736/9809134/4798776c-5898-11e5-90c6-ee8cfa258482.png)\n\n`request_to_dict` is not a real serializer because it returns a `dict`.\n\n```\n reqd = request_to_dict(request, self.spider)\n self.dqs.push(reqd, -request.priority)\n```\n\nSo a `dict` is pushed into a queue instead of bytes.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13176549/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13176815", "body": "Oh. I completely overlooked this file and set `SCHEDULER_DISK_QUEUE` to `FifoSQLiteQueue` directly . Thanks.\n\nI manually added the following two lines into squeues.py:\n\n```\nPickleFifoSQLiteQueue = _serializable_queue(queue.FifoSQLiteQueue,\n                                        _pickle_serialize, pickle.loads)\nPickleLifoSQLiteQueue = _serializable_queue(queue.LifoSQLiteQueue,\n                                        _pickle_serialize, pickle.loads)\n```\n\nAnd used `SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleFifoSQLiteQueue'` in the settings.py.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13176815/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13177111", "body": "That is a better idea. Thanks.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13177111/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "Zephor5": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/13278338", "body": "Maybe requirements.txt should be fixed with version limitation of queuelib\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13278338/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "liangerleunger": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/13460317", "body": "Binary file not shown\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/13460317/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "raintan": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/14395371", "body": "This comment should be removed or changed to: # 'testspider' is the name...\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/14395371/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/14430024", "body": "In the latest version of the docs, followall isn't part of the code example anymore. \nsee: http://doc.scrapy.org/en/1.0/topics/practices.html right after \"What follows is a working example of how to do that, using the testspiders project as example.\"\n(I'm new here, so I'm not sure if this was the proper place to put this comment.)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/14430024/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "curita": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/14428934", "body": "Not sure if I'm misunderstanding what you're saying @raintan, but `testspiders` is the name of the project (which you can found here: https://github.com/scrapinghub/testspiders, should be linked in the docs as well), and `followall` is the name of the spider inside that project that we're using (You can check the code in the line below).\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/14428934/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/14430358", "body": "Oh, I see what you meant, thanks for the notice!\n\nThis issue seems to be fixed already (that's why these docs are correct), but the latest available version of scrapy docs is this one: http://doc.scrapy.org/en/master/topics/practices.html (I know, rather confusing that is not `latest`). We'll see to backport the fix to 1.0 accordantly.\n\nGenerally opening an issue for any kind of bugs is the way to go, it's more flexible and github offers more features for them (can be referenced easier, we can modify their status to track the work done on them, milestones can be assigned to each of them, etc), so feel free to open an issue for your next bug report :)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/14430358/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "kalessin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/15693798", "body": "@eliasdorneles @dangra this change break the exporter on values that are not string. Previously, for example, if value was an int or a bool, then the same value was returned. Now, an exception is raised because to_bytes and to_unicode does not accept them as value. Tests in our ds-toolbox lib are failing because of this.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/15693798/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "mtwilliams": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/16965386", "body": "Just ran into this! :100: \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/16965386/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "villamarinella": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/17781808", "body": " class WebsiteLoader(XPathItemLoader):\nscraper.py:177: ScrapyDeprecationWarning: scrapy.settings.CrawlerSettings is deprecated, instantiate scrapy.settings.Settings instead.\n  settings = CrawlerSettings(values=settings)\n/usr/local/lib/python2.7/dist-packages/scrapy/settings/**init**.py:479: ScrapyDeprecationWarning: `Settings.overrides` attribute is deprecated and won't be supported in Scrapy 0.26, use `Settings.set(name, value, priority='cmdline')` instead\n  if opt_name in self.overrides:\n/usr/local/lib/python2.7/dist-packages/scrapy/settings/**init**.py:483: ScrapyDeprecationWarning: `Settings.defaults` attribute is deprecated and won't be supported in Scrapy 0.26, use `Settings.set(name, value, priority='default')` instead\n  if opt_name in self.defaults:\n2016-06-08 05:56:52 [scrapy] INFO: Scrapy 1.1.0 started (bot: scrapybot)\n2016-06-08 05:56:52 [scrapy] INFO: Overridden settings: {'FEED_URI': 'proxylist.json'}\nTraceback (most recent call last):\n  File \"scraper.py\", line 212, in <module>\n    main()\n  File \"scraper.py\", line 195, in main\n    run_spider(HideMyAssSpider(), options)\n  File \"scraper.py\", line 180, in run_spider\n    crawler.install()\nAttributeError: 'CrawlerProcess' object has no attribute 'install'\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/17781808/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "Jesse-Bakker": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/25716718", "body": "In python, you can loop over collections directly, using\r\n```\r\nfor domain in allowed_domains:\r\n    # now you can use domain the same way you were using allowed_domains[domainIndex]\r\n```\r\nThis is always better, as it more clearly conveys what you are trying to do (iterating over a list) and gives less chance for errors. ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/25716718/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "scottyallen": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/163154", "body": "Thanks for the patch - I was in the process of trying to fix this, and it saved me a ton of time:)  However, I don't think line 191 is quite right for the tunnel case.  It results in sending a GET request with the full url to the destination webserver, which is technically wrong and some sites refuse to handle.  Instead, self.path should remain unchanged for the tunnel case.  I can send a patch to your patch, if you like...\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/163154/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "Mimino666": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2564248", "body": "Oh, I never noticed that. In that case it is of course useless.\nThanks for pointing that out.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2564248/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "llonchj": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2832086", "body": "which js client templating do you suggest?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/2832086/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7764353", "body": "I have a use case when the response is a Response object and XML is valid. @dangra, What's your suggestion?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7764353/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7765159", "body": "Cache-Control: no-cache\nPragma: no-cache\nContent-Type: application/octet-stream\nExpires: -1\nServer: Microsoft-IIS/7.5\nX-AspNet-Version: 4.0.30319\nX-SiteConHost: P508\nX-Powered-By: ASP.NET\nX-Powered-By: ARR/2.5\nX-SiteConHost: P508\nX-Powered-By: ASP.NET\nDate: Tue, 19 Nov 2013 17:40:30 GMT\nTransfer-Encoding:  chunked\nConnection: keep-alive\nConnection: Transfer-Encoding\nSet-Cookie: tj_recruiters_ssl#sc_wede=1; path=/\nSet-Cookie:\nAnonymousUser=MemberId=2a3bc8aa-64dc-4c1f-b382-22b2122cd891&IsAnonymous=True;\nexpires=Thu, 19-Nov-2043 00:00:00 GMT; path=/\n\n2013/11/20 Daniel Gra\u00f1a notifications@github.com\n\n> In scrapy/utils/response.py:\n> \n> > ```\n> >      return obj.body_as_unicode() if unicode else obj.body\n> > ```\n> > -    elif isinstance(obj, Response):\n> > -        body = obj.body\n> > -        if isinstance(body, str):\n> > -            return body.decode('utf-8') if unicode else body\n> > -        return body if unicode else body.encode('utf-8')\n> \n> Triage why scrapy.responsetypes.ResponseTypes doesn't detect it as\n> text/xml.\n> \n> can you provide more details, an url would be ideal, but headers should be\n> enough.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/pull/462/files#r7764579\n> .\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7765159/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7770449", "body": "Will then be an option to :\n\n> -        body = obj.body\n> -        if isinstance(body, str):\n> -            return body.decode('utf-8') if unicode else body\n> -        return body if unicode else body.encode('utf-8')\n> -        return body obj.body\n\nDoes this make any sense?\n\n2013/11/20 Daniel Gra\u00f1a notifications@github.com\n\n> In scrapy/utils/response.py:\n> \n> > ```\n> >      return obj.body_as_unicode() if unicode else obj.body\n> > ```\n> > -    elif isinstance(obj, Response):\n> > -        body = obj.body\n> > -        if isinstance(body, str):\n> > -            return body.decode('utf-8') if unicode else body\n> > -        return body if unicode else body.encode('utf-8')\n> \n> I must admit there is nothing useful on those headers :)\n> \n> body_as_str is only used in scrapy.utils.iterators, I am temped to move\n> it there as a private function.\n> \n> And in that case may be assume UTF-8 in case of unknown encoding for\n> Response.\n> \n> Another option is to retry detection based on body content with\n> responsetypes.from_args(body=response.body), I am not sure why this is\n> not done in http handlers.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/pull/462/files#r7767231\n> .\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7770449/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7770471", "body": "Sorry:\n\n> -        return obj.body\n\n2013/11/20 Jordi Llonch llonchj@gmail.com\n\n> Will then be an option to :\n> \n> > -        body = obj.body\n> > -        if isinstance(body, str):\n> > -            return body.decode('utf-8') if unicode else body\n> > -        return body if unicode else body.encode('utf-8')\n> > -        return body obj.body\n> \n> Does this make any sense?\n> \n> 2013/11/20 Daniel Gra\u00f1a notifications@github.com\n> \n> > In scrapy/utils/response.py:\n> > \n> > > ```\n> > >      return obj.body_as_unicode() if unicode else obj.body\n> > > ```\n> > > -    elif isinstance(obj, Response):\n> > > -        body = obj.body\n> > > -        if isinstance(body, str):\n> > > -            return body.decode('utf-8') if unicode else body\n> > > -        return body if unicode else body.encode('utf-8')\n> > \n> > I must admit there is nothing useful on those headers :)\n> > \n> > body_as_str is only used in scrapy.utils.iterators, I am temped to move\n> > it there as a private function.\n> > \n> > And in that case may be assume UTF-8 in case of unknown encoding for\n> > Response.\n> > \n> > Another option is to retry detection based on body content with\n> > responsetypes.from_args(body=response.body), I am not sure why this is\n> > not done in http handlers.\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/pull/462/files#r7767231\n> > .\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7770471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7771561", "body": "@dangra, not sure about what's your idea. Kindly update PR yourself.\n\n2013/11/20 Daniel Gra\u00f1a notifications@github.com\n\n> In scrapy/utils/response.py:\n> \n> > ```\n> >      return obj.body_as_unicode() if unicode else obj.body\n> > ```\n> > -    elif isinstance(obj, Response):\n> > -        body = obj.body\n> > -        if isinstance(body, str):\n> > -            return body.decode('utf-8') if unicode else body\n> > -        return body if unicode else body.encode('utf-8')\n> \n> @llonchj https://github.com/llonchj : Do you want to update this PR or\n> should I take care of it?\n> \n> it should move body_or_str as private function to scrapy.utils.iteratorsand try to decode \"binary\" responses using\n> utf-8 as best effort.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/pull/462/files#r7770963\n> .\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7771561/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "loucash": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5920915", "body": "done\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5920915/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5920916", "body": "done\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5920916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5920920", "body": "done\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5920920/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5920921", "body": "done\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5920921/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5922591", "body": "yup, fixed\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5922591/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5922732", "body": "Well, I would like to treat those changes rather as a refactorization that extracts FilesPipeline and keep the interface. \n\nI think new functionalities is a separated topic for a separated pull request, probably built on this one.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5922732/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5922798", "body": "definitely\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5922798/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5923370", "body": "yup, fixed within last commit\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5923370/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5923710", "body": "Yes, I thought about it, and maybe it is better to keep file_key. Let me explain:\nImagesPipeline inherits from FilesPipeline and FilesPipeline internals requires file_key.\n\nIf someone, by accident removes \"def file_key\" from ImagesPipeline then image_key will never be used and results will be wrong.\n\nSo, here, by using file_key, you test also a pointer from file_key to image_key in ImagesPipeline.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/5923710/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "plainas": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6374584", "body": "To generate a different setup.py with different configs is the whole purpose of this pull request. The setup.py IS generated by the deploy command anyway, based on an hardcoded template.\n\nHowever I notice now that the deploy command has been moved to scrapyd.  Any chance this would be integrated in scrapyd?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6374584/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6375005", "body": "I am not aware of what most of the people customize on their setup.py. What do people use it for in this case?\n\nCouldn't they just edit the template in that case? (Honest question) \nIt is just editing a single python script anyway.\n\nAlso related, this pull request adds the possibility of adding a distinct settings module per deploy target, I would guess that would cover  many steup.py tuning many people do, but again, I am not sure about what people use it for.\n\nEither way, any suggestion on how this could be added in a more smooth way? I.e. without this impact?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6375005/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6391216", "body": "It is hardcoded into the source yes. AFAIK the reason why it was put there it was because it provides an easy mean of packing so the project can be deployed. I don't think the need to build eggs arises from anything else. Wouldn't it be for the deploy command, would the template be there at all?\nI have yet to see the first scrapy project distributed as package or egg. The documentation only mentions packaging as a necessary step to deployment.\n\nAnd it also says this:\n\n> The simplest way to deploy your project is by using the deploy command, which automates the process of building the egg uploading it using the Scrapyd HTTP JSON API.\n> \n>  It is not a good idea to change the installed Scrapy version for multiple reasons: template can be shared with other scrapy projects, scrapy updates will overwrite the changes, these changes won't be in VCS, etc.\n\nOk, here I can tell your for a fact that you got confused. The template is not changed as you can see in the pull request. What is changed is the settings definitions passed to it. There is no possible way  project could impact another project regardless this changes are used or not.\nAlso, the setup.py we are talking about is is the one of the project, not scrapy's setup.py. Which I think nobody really cares about or ever used manually. Specially if it is under VCS this won't be a problem.\n\nBut we are discussing over a deprecated implementation, ideally this should be integrated with new scrapyd-deploy. This problem is possibly non-existent (I haven't checked).\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6391216/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "audiodude": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6929486", "body": "That's extremely interesting and important information. I will update this doc to incorporate it (here and below).\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6929486/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7019901", "body": "I changed the doc to remove the \"immediate redirection\" language.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7019901/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7019962", "body": "So is it true that `process_response` and `process_request` behave differently when IgnoreRequest is raised? If I understand correctly in `process_request`, all the `process_exception` methods are called, followed by the `Request.errback` if nothing handles it. Whereas for `process_response` the `process_exception` methods are NOT called. Is that right? \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7019962/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "alexanderlukanin13": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7070874", "body": "Restored import *, added comment to make sure nobody breaks it in the future.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7070874/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7081529", "body": "Good point!\nI vote for `from __future__ import print_function` because it was designed just for that. I think we should add it to all files which use `print`.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7081529/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7132891", "body": "But there is a `from __future__ import print_function`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7132891/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7133014", "body": "It's just like you said: hard to tell if `print_function` is in effect in particular file. I think we should create a test which will run 2to3 on whole project automatically, and fail if there are old-style statements, functions and imports. Then we will gradually increase set of 2to3 flags used.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7133014/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7395251", "body": "```\n>>> str(b'1.2.3')\n\"b'1.2.3'\"\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/7395251/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "duendex": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6578375", "body": "We can't just feed the restored dataReceived with bytes as the client is not expecting a response from the server yet (the connection is still 'pending' until I trigger self._tunnelReadyDeferred). I will commit a change where I just trigger the deferred and not switch to TLS when the response from the proxy is not a 200. In this case, the request from the client will be sent and the proxy will return an http status 500 to the client. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6578375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6614663", "body": "removed.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6614663/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6614669", "body": "removed.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6614669/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6614711", "body": "Ok, but the response is of the form HTTP/1.x 200 Connection established. I will use a re to match 'HTTP/1.x 200' at the beginning.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6614711/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6614767", "body": "My bad, I let pylint convince me...\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6614767/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6614862", "body": "Actually it is needed when we restore the connection after the request to open the tunnel fails. Look at the connectionLost method.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6614862/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6615071", "body": "I will implement this. It may make things a little bit uglier if we decide we have to modify the URL to remove the <code>noconnect</code> parameter form the request... I guess we don't want that parameter to reach the proxy. WDYT?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6615071/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6615297", "body": "Note that the connection is closed by the proxy and not by us (at least that's Squid's behaviour) when the tunnel can't be opened. My idea here is that we restore the connection and allow the client to send his request to the proxy. The proxy will finally respond with an error that will be returned to the client. If we do an errback here, who will handle it?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6615297/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6640338", "body": "I totally misread your comment. I thought the <code>noconnect</code> parameter was going to be added to the list of URLs to crawl... ?!? \nIt makes total sense now!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6640338/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6640642", "body": "Ok. I did the hacky retrial for the client to get some kind of HTTP response (an error) instead of getting a closed connection. I will remove the retrial and just raise an error using an errback for now.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6640642/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6728403", "body": "Excellent point, I will correct that.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/6728403/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "hobson": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8796706", "body": "Yea, noticed that once I got to know scrapy a bit better.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8796706/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8796708", "body": "good catch\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8796708/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8886741", "body": "right you are... again\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8886741/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "andresp99999": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8968676", "body": "Most sentences are in present tense, but a few in past tense, this looks a bit odd.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/8968676/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}}}}