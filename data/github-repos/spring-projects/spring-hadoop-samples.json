{"_default": {"1": {"sumanth35": {"issues": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/31", "title": "Mapreduce program could only be replicated to 0 nodes instead of minReplication (=1)", "body": "I got the following stacktrace when i ran the mapreduce program:\r\n![image](https://user-images.githubusercontent.com/9468673/32426202-a58fbb3c-c286-11e7-8b7d-438b150e2b3f.png)\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/31/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/339535077", "body": "Hi I have installed hadoop 2.7.4 on windows 7. I tried to run the spring hadoop map reduce wordcount program but could not run on windows as sh ./target/appassembler/bin/wordcount cannot be run on windows.\r\n\r\nWhen I tried to run the wordcount class as a standalone class I get the following exception:\r\n\r\n`log4j:WARN No appenders could be found for logger (org.springframework.context.support.ClassPathXmlApplicationContext).\r\nlog4j:WARN Please initialize the log4j system properly.\r\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\r\nException in thread \"main\" org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'wordcountJob' defined in null: Could not resolve placeholder 'app.repo' in string value \"file:${app.repo}/hadoop-examples-*.jar\"; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'app.repo' in string value \"file:${app.repo}/hadoop-examples-*.jar\"\r\n\tat org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:211)\r\n\tat org.springframework.context.support.PropertySourcesPlaceholderConfigurer.processProperties(PropertySourcesPlaceholderConfigurer.java:180)\r\n\tat org.springframework.context.support.PropertySourcesPlaceholderConfigurer.postProcessBeanFactory(PropertySourcesPlaceholderConfigurer.java:155)\r\n\tat org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:265)\r\n\tat org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:162)\r\n\tat org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:606)\r\n\tat org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:462)\r\n\tat org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:197)\r\n\tat org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:172)\r\n\tat org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:158)\r\n\tat org.springframework.samples.hadoop.mapreduce.Wordcount.main(Wordcount.java:28)\r\nCaused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'app.repo' in string value \"file:${app.repo}/hadoop-examples-*.jar\"\r\n\tat org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:174)\r\n\tat org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)\r\n\tat org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:204)\r\n\tat org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:178)\r\n\tat org.springframework.context.support.PropertySourcesPlaceholderConfigurer$2.resolveStringValue(PropertySourcesPlaceholderConfigurer.java:175)\r\n\tat org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveStringValue(BeanDefinitionVisitor.java:282)\r\n\tat org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:209)\r\n\tat org.springframework.beans.factory.config.BeanDefinitionVisitor.visitList(BeanDefinitionVisitor.java:228)\r\n\tat org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:192)\r\n\tat org.springframework.beans.factory.config.BeanDefinitionVisitor.visitPropertyValues(BeanDefinitionVisitor.java:141)\r\n\tat org.springframework.beans.factory.config.BeanDefinitionVisitor.visitBeanDefinition(BeanDefinitionVisitor.java:82)\r\n\tat org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:208)\r\n\t... 10 more\r\n`\r\n\r\nHow can i run this program?\r\n\r\nPlease advise", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/339535077/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/339976330", "body": "Figured this out by providing the complete path", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/339976330/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "MichalMichalak": {"issues": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/30", "title": "Unable to initialize 'javax.el.ExpressionFactory' after update Spring Boot to 1.5.2", "body": "Hello. I am not sure this is the right place to open an issue. If it's not, I'd gladly appreciate if you could share the right person / project.\r\n\r\nWe are using Spring Boot and Hadoop as shown in Gradle script below.\r\n\r\n```groovy\r\nplugins {\r\n    // before update it was: '1.4.4.RELEASE'\r\n    id 'org.springframework.boot' version '1.5.2.RELEASE'\r\n}\r\ndependencies {\r\n    compile 'org.springframework.data:spring-data-hadoop-boot:2.4.0.RELEASE-cdh5'\r\n}\r\n```\r\n\r\nWhen trying to get validator, we got following exception.\r\n\r\n```java\r\nimport javax.validation.Validator;\r\nclass Utils {\r\n    Validator v = Validation.buildDefaultValidatorFactory().getValidator();\r\n}\r\n```\r\n\r\n```\r\njava.lang.ExceptionInInitializerError\r\n\tat com.rakuten.felix.listnormalizer.broker.MessageProcessor.handleIncomingMessage(MessageProcessor.java:110)\r\n\tat com.rakuten.felix.listnormalizer.test.broker.MessageProcessorTest.handleIncomingMessageTest(MessageProcessorTest.java:115)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\nCaused by: javax.validation.ValidationException: HV000183: Unable to initialize 'javax.el.ExpressionFactory'. Check that you have the EL dependencies on the classpath, or use ParameterMessageInterpolator instead\r\n\tat org.hibernate.validator.messageinterpolation.ResourceBundleMessageInterpolator.buildExpressionFactory(ResourceBundleMessageInterpolator.java:102)\r\n\tat org.hibernate.validator.messageinterpolation.ResourceBundleMessageInterpolator.<init>(ResourceBundleMessageInterpolator.java:45)\r\n\tat org.hibernate.validator.internal.engine.ConfigurationImpl.getDefaultMessageInterpolator(ConfigurationImpl.java:423)\r\n\tat org.hibernate.validator.internal.engine.ConfigurationImpl.getDefaultMessageInterpolatorConfiguredWithClassLoader(ConfigurationImpl.java:575)\r\n\tat org.hibernate.validator.internal.engine.ConfigurationImpl.getMessageInterpolator(ConfigurationImpl.java:364)\r\n\tat org.hibernate.validator.internal.engine.ValidatorFactoryImpl.<init>(ValidatorFactoryImpl.java:144)\r\n\tat org.hibernate.validator.HibernateValidator.buildValidatorFactory(HibernateValidator.java:38)\r\n\tat org.hibernate.validator.internal.engine.ConfigurationImpl.buildValidatorFactory(ConfigurationImpl.java:331)\r\n\tat javax.validation.Validation.buildDefaultValidatorFactory(Validation.java:110)\r\n\tat com.rakuten.felix.listnormalizer.ValidatorUtils.<clinit>(ValidatorUtils.java:11)\r\n\t... 4 more\r\nCaused by: javax.el.ELException: Provider com.sun.el.ExpressionFactoryImpl not found\r\n\tat javax.el.FactoryFinder.newInstance(FactoryFinder.java:101)\r\n\tat javax.el.FactoryFinder.find(FactoryFinder.java:197)\r\n\tat javax.el.ExpressionFactory.newInstance(ExpressionFactory.java:189)\r\n\tat javax.el.ExpressionFactory.newInstance(ExpressionFactory.java:160)\r\n\tat org.hibernate.validator.messageinterpolation.ResourceBundleMessageInterpolator.buildExpressionFactory(ResourceBundleMessageInterpolator.java:98)\r\n\t... 13 more\r\nCaused by: java.lang.ClassNotFoundException: com.sun.el.ExpressionFactoryImpl\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat javax.el.FactoryFinder.newInstance(FactoryFinder.java:87)\r\n\t... 17 more\r\n```\r\n\r\nAfter an investigation we found out that two `javax.el.ExpressionFactory` classes exist in class path. One from `tomcat-embed-el:8.5.11` and other from `jsp:jsp-api:2.1`. I assumed `jsp-api` contains old version of `ExpressionFactory` class and excluded JSP API, which solved the issue.\r\n\r\nThe question is - shouldn't this be solved in Spring itself? Since I wasn't excluding anything before and it was working fine, I would expect the same behavior after update. Any ideas how to proceed with this? Thank you.", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/30/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/345682700", "body": "I may be wrong here, but since it's Spring's library it should be fixed by Spring team. Either exclude conflicting libs, maybe create some custom package instead, etc. OR they should communicate with Apache and solve the issue together / ask them to use different version.", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/345682700/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ashishkatiyar16": {"issues": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/29", "title": "hbase running app", "body": "log4j:WARN No appenders could be found for logger (org.springframework.samples.hadoop.hbase.UserApp).\r\nlog4j:WARN Please initialize the log4j system properly.\r\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\r\nException in thread \"main\" java.io.IOException: Attempt to start meta tracker failed.\r\n\tat org.apache.hadoop.hbase.catalog.CatalogTracker.start(CatalogTracker.java:201)\r\n\tat org.apache.hadoop.hbase.client.HBaseAdmin.getCatalogTracker(HBaseAdmin.java:230)\r\n\tat org.apache.hadoop.hbase.client.HBaseAdmin.tableExists(HBaseAdmin.java:277)\r\n\tat org.apache.hadoop.hbase.client.HBaseAdmin.tableExists(HBaseAdmin.java:288)\r\n\tat org.springframework.samples.hadoop.hbase.UserUtils.initialize(UserUtils.java:36)\r\n\tat org.springframework.samples.hadoop.hbase.UserApp.main(UserApp.java:22)\r\nCaused by: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/meta-region-server\r\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:99)\r\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\r\n\tat org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1045)\r\n\tat org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:199)\r\n\tat org.apache.hadoop.hbase.zookeeper.ZKUtil.watchAndCheckExists(ZKUtil.java:425)\r\n\tat org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.start(ZooKeeperNodeTracker.java:77)\r\n\tat org.apache.hadoop.hbase.catalog.CatalogTracker.start(CatalogTracker.java:197)\r\n\t... 5 more\r\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/29/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alleyZ": {"issues": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/28", "title": "issue while reading data on hdfs from remote ", "body": "## I want to read the files on the remote HDFS\uff0cbut there was an exception.\n\n> ## local ENV.\n> - OS: windows 10\n> - JDK 1.8\n> - SHDP 2.4.0\n> - hadoop 2.7.1\n## 1.  code\n\n``` java\nAbstractApplicationContext context = new ClassPathXmlApplicationContext(\n                    \"/application.xml\", XmlUserApp.class);\n            HdfsResourceLoader loader = context.getBean(HdfsResourceLoader.class);\n            Resource resource = loader.getResource(\"hdfs://hd-23:6000/user/alleyz/hdfs.txt\");\n            System.out.println(resource.exists()); // true\n            System.out.println(resource.lastModified()); // 1469696205365\n            System.out.println(resource.isReadable()); // true\n            System.out.println(resource.contentLength()); // 41\n            System.out.println(resource.isOpen()); // true\n            System.out.println(resource.isReadable()); // true\n            File file = resource.getFile(); // throw exception\n```\n## 2. exception\n\n```\njava.lang.UnsupportedOperationException: Cannot resolve File object for HDFS Resource for [hdfs://hd-23:6000/user/alleyz/hdfs.txt]\n    at org.springframework.data.hadoop.fs.HdfsResource.getFile(HdfsResource.java:160)\n    at com.alleyz.spring.hadoop.hdfs.XmlHdfsApp.main(XmlHdfsApp.java:31)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)\n```\n## How to solve it?\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/28/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "crazedmeph": {"issues": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/27", "title": "Hbase Connection Issue?", "body": "Does this code base still work or is it out dated? I have tried to connect to hortonworks sandbox VM 2.2 2.3 and 2.4 and all are giving me issues. \n\nOne of the issues is...\n\n```\nTue Jun 14 11:42:29 EDT 2016, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=75740: row 'users,,' on table 'hbase:meta' at region=hbase:meta,,1.1588230740, hostname=sandbox.hortonworks.com,60020,1418759208042, seqNum=0\n\n    at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.throwEnrichedException(RpcRetryingCallerWithReadReplicas.java:271)\n    at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:195)\n    at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:59)\n    at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:200)\n    at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:320)\n    at org.apache.hadoop.hbase.client.ClientScanner.nextScanner(ClientScanner.java:295)\n    at org.apache.hadoop.hbase.client.ClientScanner.initializeScannerInConstruction(ClientScanner.java:160)\n    at org.apache.hadoop.hbase.client.ClientScanner.<init>(ClientScanner.java:155)\n    at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:821)\n    at org.apache.hadoop.hbase.MetaTableAccessor.fullScan(MetaTableAccessor.java:602)\n    at org.apache.hadoop.hbase.MetaTableAccessor.tableExists(MetaTableAccessor.java:366)\n    at org.apache.hadoop.hbase.client.HBaseAdmin.tableExists(HBaseAdmin.java:303)\n    at org.apache.hadoop.hbase.client.HBaseAdmin.tableExists(HBaseAdmin.java:308)\n    at org.springframework.samples.hadoop.hbase.UserUtils.initialize(UserUtils.java:34)\n    at org.springframework.samples.hadoop.hbase.UserApp.main(UserApp.java:36)\nCaused by: java.net.SocketTimeoutException: callTimeout=60000, callDuration=75740: row 'users,,' on table 'hbase:meta' at region=hbase:meta,,1.1588230740, hostname=sandbox.hortonworks.com,60020,1418759208042, seqNum=0\n    at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:159)\n    at org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture.run(ResultBoundedCompletionService.java:64)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.ConnectException: Connection refused: no further information\n    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n    at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n    at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)\n    at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)\n    at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.setupConnection(RpcClientImpl.java:424)\n    at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.setupIOstreams(RpcClientImpl.java:748)\n    at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.writeRequest(RpcClientImpl.java:920)\n    at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.tracedWriteRequest(RpcClientImpl.java:889)\n    at org.apache.hadoop.hbase.ipc.RpcClientImpl.call(RpcClientImpl.java:1222)\n    at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:213)\n    at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:287)\n    at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.scan(ClientProtos.java:32651)\n    at org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:372)\n    at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:199)\n    at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:62)\n    at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:200)\n    at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:346)\n    at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:320)\n    at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:126)\n    ... 4 more\n```\n\nHave you tested this code on the Hortonworks Sandbox VM?\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/27/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xhe": {"issues": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/26", "title": "Exceptions for Running HiveApp in Mac", "body": "I started my local HDFS and then tried to run HiveApp, the \"show tables\" commands can be run successfully, but when running HSQL, exceptions happened, this is the output in the console:\n\n2015-07-26 21:56:51.744 java[5042:457346] Unable to load realm mapping info from SCDynamicStore\nOK\n[grpshell, passwords]OK\nOK\nOK\nCopying data from file:/etc/passwd\nCopying file: file:/etc/passwd\nLoading data to table default.passwords\nTable default.passwords stats: [numFiles=1, numRows=0, totalSize=5581, rawDataSize=0]\nOK\nOK\nTotal jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks not specified. Estimated from input data size: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapreduce.job.reduces=<number>\norg.apache.hive.com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Unable to create serializer \"org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer\" for class: org.apache.hadoop.hive.ql.exec.FileSinkOperator\nSerialization trace:\nchildOperators (org.apache.hadoop.hive.ql.exec.SelectOperator)\nchildOperators (org.apache.hadoop.hive.ql.exec.GroupByOperator)\nreducer (org.apache.hadoop.hive.ql.plan.ReduceWork)\n    at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:82)\n    at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:474)\n    at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:614)\n    at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:78)\n    at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18)\n    at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:538)\n    at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:61)\n    at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:474)\n    at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:538)\n    at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:61)\n    at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:474)\n    at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:520)\n    at org.apache.hadoop.hive.ql.exec.Utilities.serializeObjectByKryo(Utilities.java:895)\n    at org.apache.hadoop.hive.ql.exec.Utilities.serializePlan(Utilities.java:799)\n    at org.apache.hadoop.hive.ql.exec.Utilities.serializePlan(Utilities.java:811)\n    at org.apache.hadoop.hive.ql.exec.Utilities.setBaseWork(Utilities.java:601)\n    at org.apache.hadoop.hive.ql.exec.Utilities.setReduceWork(Utilities.java:578)\n    at org.apache.hadoop.hive.ql.exec.Utilities.setMapRedWork(Utilities.java:569)\n    at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:372)\n    at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:136)\n    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)\n    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)\n    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1503)\n    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1270)\n    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1088)\n    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)\n    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)\n    at org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:198)\n    at org.apache.hadoop.hive.service.ThriftHive$Processor$execute.getResult(ThriftHive.java:644)\n    at org.apache.hadoop.hive.service.ThriftHive$Processor$execute.getResult(ThriftHive.java:628)\n    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\n    at java.lang.Thread.run(Thread.java:695)\njava.lang.OutOfMemoryError: PermGen space\n    at java.lang.Throwable.getStackTraceElement(Native Method)\n    at java.lang.Throwable.getOurStackTrace(Throwable.java:591)\n    at java.lang.Throwable.printStackTraceAsCause(Throwable.java:481)\n    at java.lang.Throwable.printStackTrace(Throwable.java:468)\n    at java.lang.Throwable.printStackTrace(Throwable.java:451)\n    at org.apache.hadoop.hive.ql.exec.Utilities.setBaseWork(Utilities.java:626)\n    at org.apache.hadoop.hive.ql.exec.Utilities.setReduceWork(Utilities.java:578)\n    at org.apache.hadoop.hive.ql.exec.Utilities.setMapRedWork(Utilities.java:569)\n    at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:372)\n    at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:136)\n    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)\n    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)\n    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1503)\n    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1270)\n    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1088)\n    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)\n    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)\n    at org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:198)\n    at org.apache.hadoop.hive.service.ThriftHive$Processor$execute.getResult(ThriftHive.java:644)\n    at org.apache.hadoop.hive.service.ThriftHive$Processor$execute.getResult(ThriftHive.java:628)\n    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\n    at java.lang.Thread.run(Thread.java:695)\nFAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. PermGen space\nException in thread \"org.apache.hadoop.hdfs.PeerCache@4db323af\" java.lang.OutOfMemoryError: PermGen space\nException in thread \"main\" java.lang.OutOfMemoryError: PermGen space\nException in thread \"LeaseRenewer:frankhe@localhost:9000\" java.lang.OutOfMemoryError: PermGen space\n\nThe config is as follows:\n\nhd.fs=hdfs://localhost:9000\nhd.rm=localhost:50070\nhd.jh=localhost:8088\n\nhive.host=localhost\nhive.port=10000\nhive.url=jdbc:hive://${hive.host}:${hive.port}\nhive.table=passwords\n\nMy OS is Mac, I already tried to udpate STS.ini  to:\n\n-Xms128m\n-Xmx768m\n-XX:MaxPermSize=4096m\n\nthe exception is the same.\n\nAny idea on how to fix the exception?\n\nThanks\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/26/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "honeyc0der": {"issues": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/23", "title": "Hadoop Spring mapreduce multiple inputs and mappers in a job", "body": "How to specify multiple input files and their respective format in a Job tag?\n\n<code><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<code>&lt;beans:beans xmlns=\"http://www.springframework.org/schema/hadoop\"\n        xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n        xmlns:beans=\"http://www.springframework.org/schema/beans\"\n        xmlns:context=\"http://www.springframework.org/schema/context\"\n        xsi:schemaLocation=\"http://www.springframework.org/schema/beans               http://www.springframework.org/schema/beans/spring-beans.xsd\n       http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\n       http://www.springframework.org/schema/hadoop http://www.springframework.org/schema/hadoop/spring-hadoop.xsd\"&gt;\n\n<code> &lt;context:property-placeholder location=\"hadoop.properties\"/&gt;\n\n<code> &lt;configuration&gt; \n      fs.default.name=${hd.fs} \n      yarn.resourcemanager.address=${hd.rm} \n      mapreduce.framework.name=${mr.fw}  \n              &lt;/configuration&gt;\n\n<code> &lt;job id=\"wordcountJob\"\n      input-path=\"${wordcount.input.path}\" \n      output-path=\"${wordcount.output.path}\"\n      mapper=\"org.apache.hadoop.examples.WordCount.TokenizerMapper\"\n      reducer=\"org.apache.hadoop.examples.WordCount.IntSumReducer\"/&gt;\n\n<code> &lt;/beans:beans&gt;\n\nAs we can a specify in java program. Like we this.\n\n<code>MultipleInputs.addInputPath(job, firstPath, FirstInputFormat.class, FirstMap.class);\n MultipleInputs.addInputPath(job, sencondPath, SecondInputFormat.class, SecondMap.class);</code>\n\nI goggled a lot even i checked its xsd file. I did not find any attribute so how can we specify multiple inputs in a job?\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/23/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "quux00": {"issues": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/21", "title": "spring-yarn examples not finding / using hadoop classpath", "body": "I'm trying to run the yarn examples.  I tried both `simple-command` and `batch-files` on a Hortonworks HDP-2.1 multi-node (non-secured) cluster.\n\nThe job submits fine, but it fails with:\n\n```\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/commons/logging/LogFactory\n    at org.springframework.yarn.launch.AbstractCommandLineRunner.<clinit>(AbstractCommandLineRunner.java:60)\nCaused by: java.lang.ClassNotFoundException: org.apache.commons.logging.LogFactory\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n    ... 1 more\n```\n\nThe only modifications to the `application-context.xml` and `appmaster-context.xml` was to edit the paths to where I copied the jars that get built with `gradlew`.  For example, here is (part of) the simple-master application-context.xml:\n\n```\n    <yarn:configuration>\n            fs.defaultFS=${hd.fs}\n            yarn.resourcemanager.address=${hd.rm}\n            fs.hdfs.impl=org.apache.hadoop.hdfs.DistributedFileSystem\n    </yarn:configuration>\n\n    <yarn:localresources>\n            <yarn:hdfs path=\"/user/u070072/spring-yarn/app/simple-command/*.jar\"/>\n            <yarn:hdfs path=\"/user/u070072/spring-yarn/lib/*\"/>\n    </yarn:localresources>\n\n    <yarn:environment>\n            <yarn:classpath use-yarn-app-classpath=\"true\"/>\n    </yarn:environment>\n\n    <util:properties id=\"arguments\">\n            <prop key=\"container-count\">4</prop>\n    </util:properties>\n\n    <yarn:client app-name=\"simple-command\">\n            <yarn:master-runner arguments=\"arguments\"/>\n    </yarn:client>\n```\n\nand the appmaster-context.xml:\n\n```\n    <yarn:configuration>\n            fs.defaultFS=${hd.fs}\n            yarn.resourcemanager.address=${hd.rm}\n            fs.hdfs.impl=org.apache.hadoop.hdfs.DistributedFileSystem\n    </yarn:configuration>\n\n   <yarn:localresources>\n            <yarn:hdfs path=\"/user/u070072/spring-yarn/app/simple-command/*.jar\"/>\n            <yarn:hdfs path=\"/user/u070072/spring-yarn/lib/*\"/>\n    </yarn:localresources>\n\n    <yarn:environment>\n            <yarn:classpath use-yarn-app-classpath=\"true\" delimiter=\":\">\n                    ./*\n            </yarn:classpath>\n    </yarn:environment>\n\n    <yarn:master>\n            <yarn:container-allocator/>\n            <yarn:container-command>\n                    <![CDATA[\n                    date\n                    1><LOG_DIR>/Container.stdout\n                    2><LOG_DIR>/Container.stderr\n                    ]]>\n            </yarn:container-command>\n    </yarn:master>\n```\n\nI invoked it with:\n\n```\n$ ./gradlew -q run-yarn-examples-simple-command -Dhd.fs=hdfs://trvlapp0049:8020 \\\n-Dhd.rm=http://trvlapp0050.tsh.thomson.com:8050 -Dlocalresources.remote=hdfs://trvlapp0049:8020\n```\n\nThe apache-commons-logging jar it wants is in `/usr/lib/hadoop/lib`:\n\n```\nu070072@TST yarn$ ls -1 /usr/lib/hadoop/lib/ | grep commons-logging\ncommons-logging-1.1.3.jar\n```\n\nand that location is in the standard hadoop classpath on the HDP platform:\n\n```\n$ hadoop classpath\n/etc/hadoop/conf:/usr/lib/hadoop/lib/*:/usr/lib/hadoop/.//*:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/*:/usr/lib/hadoop-hdfs/.//*:/usr/lib/hadoop-yarn/lib/*:/usr/lib/hadoop-yarn/.//*:/usr/lib/hadoop-mapreduce/lib/*:/usr/lib/hadoop-mapreduce/.//*::/usr/share/java/mysql-connector-java-5.1.17.jar:/usr/share/java/mysql-connector-java.jar:/usr/lib/hadoop-mapreduce/*:/usr/lib/tez/*:/usr/lib/tez/lib/*:/etc/tez/conf\n```\n\nSo why isn't the spring-yarn setup finding the commons-logging jar?  I've run other YARN apps (not with spring-yarn) and everything works fine.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/21/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/59924065", "body": "Thanks that fixed the immediate problem.  But that uncovered another configuration problem, so I still don't have it working.\n\n> AFAIK there is no reliable way for the application to detect these classpaths, so you have to provide them.\n\nThe way to do it is start the application with `yarn jar my.jar my.YarnClient arg1 argN`.  That puts the Hadoop classpath into the CLASSPATH of the YarnClient, which then should put that classpath into the environment hashmap that gets passed to the Yarn AppMaster, which does the same in passing that to its child containers.  It is not clear how to run these examples that way.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/59924065/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "sagpid": {"issues": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/20", "title": "I need a SpringBatch Hive 13, HiverServer2 sample", "body": "I am trying to get the sample working when  running against hadoop 2.4.1, Hiveserver2, Hive 13.\n\nI am having problems and please find below details of a second instance of failure.  This is in hive-batch project from spring-hadoop-samples.\n\nThanks for any help.  \n\nMy properties are\n\n```\nhd.fs=hdfs://aa.0.11.120:8020\nlocalSourceFile=data/nbatweets-small.txt\ntweets.input.path=/tweets/input\n```\n\n The stack trace is as given below.\n\n```\nlocalSourceFile = /home/sagar/Downloads/spring-hadoop-samples/hive-batch/target/appassembler/data/nbatweets-small.txt\ninputDir = /tweets/input\nabout to execute the file copying \nexiting\n01:06:34,304  INFO amework.samples.hadoop.hive.HiveBatchApp:  37 - Batch Tweet Influencers Hive Job Running\n01:06:34,411  INFO ch.core.launch.support.SimpleJobLauncher: 133 - Job: [FlowJob: [name=hiveJob]] launched with the following parameters: [{}]\n01:06:34,471  INFO amework.batch.core.job.SimpleStepHandler: 146 - Executing step: [influencer-step]\n01:06:34,589 ERROR ngframework.batch.core.step.AbstractStep: 225 - Encountered an error executing step influencer-step in job hiveJob\norg.springframework.dao.DataAccessResourceFailureException: Invalid method name: 'execute'; nested exception is org.apache.thrift.TApplicationException: Invalid method name: 'execute'\n    at org.springframework.data.hadoop.hive.HiveUtils.convert(HiveUtils.java:69)\n    at org.springframework.data.hadoop.hive.HiveTemplate.convertHiveAccessException(HiveTemplate.java:99)\n    at org.springframework.data.hadoop.hive.HiveTemplate.execute(HiveTemplate.java:82)\n    at org.springframework.data.hadoop.hive.HiveTemplate.executeScript(HiveTemplate.java:261)\n\n```\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/20/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/19", "title": "I need a SpringBatch Hive 13, HiverServer2 sample", "body": "I posted a message earlier on this board and it got deleted and so posting again.\n\nI down loaded the samples and modified some of them for Hive13, hadoop 2.4.1 (hrtonworks 2.1).\nPlease take aa look and see if you can help in spotting the problem.  Or Do you have a project that works for hive13 on Hiveserver2.\n\nThanks for any help.\n\nI get the following error\n\n```\n[sagar@devsagar hive]$ sh ./target/appassembler/bin/hiveApp\n00:47:15,902  INFO t.support.ClassPathXmlApplicationContext: 513 - Refreshing org.springframework.context.support.ClassPathXmlApplicationContext@5ab2f56: startup date [Wed Aug 20 00:47:15 CEST 2014]; root of context hierarchy\n00:47:16,057  INFO eans.factory.xml.XmlBeanDefinitionReader: 316 - Loading XML bean definitions from class path resource [META-INF/spring/hive-context.xml]\n00:47:16,449  INFO eans.factory.xml.XmlBeanDefinitionReader: 316 - Loading XML bean definitions from class path resource [META-INF/spring/jdbc-context.xml]\n00:47:16,889  INFO ort.PropertySourcesPlaceholderConfigurer: 172 - Loading properties file from class path resource [hadoop.properties]\n00:47:16,890  INFO ort.PropertySourcesPlaceholderConfigurer: 172 - Loading properties file from class path resource [hive.properties]\n00:47:16,898  INFO ion.AutowiredAnnotationBeanPostProcessor: 141 - JSR-330 'javax.inject.Inject' annotation found and supported for autowiring\n00:47:17,433  INFO ans.factory.config.PropertiesFactoryBean: 172 - Loading properties file from class path resource [hive-server.properties]\nException in thread \"main\" org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'hiveServer': Invocation of init method failed; nested exception is org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address 0.0.0.0/0.0.0.0:10000.\n    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1553)\n    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539)\n\n```\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/19/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/52704277", "body": "I use hadoop 2.4.1 from Hortonworks.   Spring 4.0.6.  \n\nI have the following properties.   \n\n```\nhive.exec.drop.ignorenonexistent=true\nhive.host=aa.bb.cc.dd\nhive.port=10000\n# hive.url=jdbc:hive2://${hive.host}:${hive.port}/default;auth=noSasl\nhive.table=passwords\n\n```\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/52704277/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/52711747", "body": "hive.exec.drop.ignorenonexistent=true\n\n``` hive.host=10.0.11.120\nhive.port=10000\n#hive.url=jdbc:hive2://${hive.host}:${hive.port}/default;auth=noSasl\nhive.table=passwords\n```\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/52711747/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "pawelantczak": {"issues": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/17", "title": "hiveAppWithApacheLogs demo hangs on: Returning cached instance of singleton bean 'hiveRunner'", "body": "Hello.\nI'm trying to run Hive demo on Hortonworks Sandbox 2.1.\nI'm connecting to Hive from host (outside of VM).\nWhen running hiveAppWithApacheLogs, logs looks like this:\n\n```\n21:10:51,481 DEBUG ctory.support.DefaultListableBeanFactory: 477 - Finished creating instance of bean 'hadoopConfiguration'\n21:10:51,482 DEBUG ctory.support.DefaultListableBeanFactory: 220 - Creating shared instance of singleton bean 'hiveClientFactory'\n21:10:51,482 DEBUG ctory.support.DefaultListableBeanFactory: 449 - Creating instance of bean 'hiveClientFactory'\n21:10:51,486 DEBUG ctory.support.DefaultListableBeanFactory: 523 - Eagerly caching bean 'hiveClientFactory' to allow for resolving potential circular references\n21:10:51,492 DEBUG ctory.support.DefaultListableBeanFactory: 477 - Finished creating instance of bean 'hiveClientFactory'\n21:10:51,492 DEBUG ctory.support.DefaultListableBeanFactory: 220 - Creating shared instance of singleton bean 'hiveRunner'\n21:10:51,492 DEBUG ctory.support.DefaultListableBeanFactory: 449 - Creating instance of bean 'hiveRunner'\n21:10:51,493 DEBUG ctory.support.DefaultListableBeanFactory: 523 - Eagerly caching bean 'hiveRunner' to allow for resolving potential circular references\n21:10:51,493 DEBUG ctory.support.DefaultListableBeanFactory: 249 - Returning cached instance of singleton bean 'hiveClientFactory'\n21:10:51,497 DEBUG ctory.support.DefaultListableBeanFactory: 449 - Creating instance of bean '(inner bean)#198ddef7'\n21:10:51,500 DEBUG ctory.support.DefaultListableBeanFactory: 449 - Creating instance of bean '(inner bean)#ab14733'\n21:10:51,511 DEBUG ctory.support.DefaultListableBeanFactory: 477 - Finished creating instance of bean '(inner bean)#ab14733'\n21:10:51,515 DEBUG ctory.support.DefaultListableBeanFactory: 477 - Finished creating instance of bean '(inner bean)#198ddef7'\n21:10:51,517 DEBUG ctory.support.DefaultListableBeanFactory:1595 - Invoking afterPropertiesSet() on bean with name 'hiveRunner'\n21:10:51,518 DEBUG ctory.support.DefaultListableBeanFactory: 477 - Finished creating instance of bean 'hiveRunner'\n21:10:51,519 DEBUG t.support.ClassPathXmlApplicationContext: 700 - Unable to locate LifecycleProcessor with name 'lifecycleProcessor': using default [org.springframework.context.support.DefaultLifecycleProcessor@39a3014f]\n21:10:51,519 DEBUG ctory.support.DefaultListableBeanFactory: 249 - Returning cached instance of singleton bean 'lifecycleProcessor'\n21:10:51,521 DEBUG core.env.PropertySourcesPropertyResolver:  81 - Searching for key 'spring.liveBeansView.mbeanDomain' in [systemProperties]\n21:10:51,521 DEBUG core.env.PropertySourcesPropertyResolver:  81 - Searching for key 'spring.liveBeansView.mbeanDomain' in [systemEnvironment]\n21:10:51,521 DEBUG core.env.PropertySourcesPropertyResolver: 103 - Could not find key 'spring.liveBeansView.mbeanDomain' in any property source. Returning [null]\n21:10:51,521  INFO amples.hadoop.hive.HiveAppWithApacheLogs:  31 - Hive Application Running\n21:10:51,522 DEBUG ctory.support.DefaultListableBeanFactory: 249 - Returning cached instance of singleton bean 'hiveRunner'\n```\n\nOn my own app based on existing Hive existing server approach there is also problem.\nMy logs:\n\n```\n21:15:54,121  INFO  org.apache.hadoop.fs.TrashPolicyDefault:  92 - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n21:15:54,121 DEBUG ctory.support.DefaultListableBeanFactory: 477 - Finished creating instance of bean 'setupScript'\n21:15:54,121 DEBUG ctory.support.DefaultListableBeanFactory: 220 - Creating shared instance of singleton bean 'hiveClientFactory'\n21:15:54,121 DEBUG ctory.support.DefaultListableBeanFactory: 449 - Creating instance of bean 'hiveClientFactory'\n21:15:54,121 DEBUG ctory.support.DefaultListableBeanFactory: 523 - Eagerly caching bean 'hiveClientFactory' to allow for resolving potential circular references\n21:15:54,125 DEBUG ctory.support.DefaultListableBeanFactory: 477 - Finished creating instance of bean 'hiveClientFactory'\n21:15:54,125 DEBUG ctory.support.DefaultListableBeanFactory: 220 - Creating shared instance of singleton bean 'hiveRunner'\n21:15:54,125 DEBUG ctory.support.DefaultListableBeanFactory: 449 - Creating instance of bean 'hiveRunner'\n21:15:54,126 DEBUG ctory.support.DefaultListableBeanFactory: 523 - Eagerly caching bean 'hiveRunner' to allow for resolving potential circular references\n21:15:54,126 DEBUG ctory.support.DefaultListableBeanFactory: 247 - Returning cached instance of singleton bean 'hiveClientFactory'\n21:15:54,130 DEBUG ctory.support.DefaultListableBeanFactory: 449 - Creating instance of bean '(inner bean)#672e34d8'\n21:15:54,130 DEBUG ctory.support.DefaultListableBeanFactory: 449 - Creating instance of bean '(inner bean)#a8f85d4'\n21:15:54,131 DEBUG ctory.support.DefaultListableBeanFactory: 477 - Finished creating instance of bean '(inner bean)#a8f85d4'\n21:15:54,132 DEBUG ctory.support.DefaultListableBeanFactory: 477 - Finished creating instance of bean '(inner bean)#672e34d8'\n21:15:54,134 DEBUG ctory.support.DefaultListableBeanFactory:1595 - Invoking afterPropertiesSet() on bean with name 'hiveRunner'\n```\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/17/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "betht1220": {"issues": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/16", "title": "Protocol message contained an invalid tag", "body": "I've been struggling with this example trying to make it work.\nI am running Hadoop Version: 2.0.5-alpha-gphd-2.1.0.0 with namenode at port 8020\nI've run the application as described in the README file\n mvn clean package -Pphd20\nsh ./target/appassembler/bin/wordcount\n\nI got the error\nCaused by: com.google.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero).\n\nHere's the gist containing the stacktrace\nhttps://gist.github.com/betht1220/e9aab0b241778fe93758\n\nCan you please help me out with this\nThanks\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/16/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/49561846", "body": "Thank you Thomas, it works now.  And yes, I though my VM is running PHD2.0 since on PCC, it\nhas hadoop 2.0.5-alpha-gphd-2.1.0.0 - This is referring to the hadoop client version, correct?\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/49561846/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ajjadrapc": {"issues": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/14", "title": "Issue while writing data to hdfs from remote client", "body": "Hi, I'm trying out the spring for hadoop sample provided to write data to HDFS running on Amazon EC2 cluster from my local machine(windows-from eclipse). From the documentation provided here \nhttp://docs.spring.io/spring-hadoop/docs/1.0.x/reference/html/appendix-amazon-emr.html,\nI  have created a SOCKS proxy using the below command \nssh -i kp1.pem -ND 6666 ubuntu@ec2-54-191-18-136.us-west-2.compute.amazonaws.com\n\nand then tried to connect to remote cluster. but ,it gives me the below exception\n![ec2](https://cloud.githubusercontent.com/assets/5454294/3398048/39fe5548-fd29-11e3-8756-54025cb96810.png)\n\nAlso, as per the information in the blog(http://blog.cloudera.com/blog/2008/12/securing-a-hadoop-cluster-through-a-gateway/), I have set up the below properties in core-site.xml on client side and on server side, I have made the property \"hadoop.rpc.socket.factory.class.default\"  final.\n <property> \n    <name>hadoop.socks.server</name> \n    <value>localhost:6666</value> \n</property>\n<property> \n    <name>hadoop.rpc.socket.factory.class.default</name> \n    <value>org.apache.hadoop.net.SocksSocketFactory</value> \n</property>\n\nI'm using hadoop-2.4.0 and in all the hadoop related configuration files, I  have used the amazon public DNS name as the hostname both on the client and the server side. For example,\n<property>\n    <name>fs.default.name</name>\n    <value>hdfs://ec2-54-191-18-136.us-west-2.compute.amazonaws.com:8020</value>\n  </property>\n\nCan you please let me know the reason why I get the attached error?\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/14/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/47245524", "body": "Yes, you can access the AWS instance. I have included the resources core-site.xml, hdfs-site.xml and mapred-site.xml to hadoop configuration object. please find the attached images of the configuration files. please let me know if you need any further information\n![core-site](https://cloud.githubusercontent.com/assets/545\n![hdfs-site]%28https://cloud.githubusercontent.com/assets/5454294/3401055/c2bd9a48-fd4b-11e3-9e9d-9f45408f3599.png%29\n4294/3401045/aabeaa4a-fd4b-11e3-81c5-5efe760c47cb.png)\n![mapred-site](https://cloud.githubusercontent.com/assets/5454294/3401047/b5363a92-fd4b-11e3-93da-7be8a8c817dd.png)\n![hadoop-context](https://cloud.githubusercontent.com/assets/5454294/3401064/d5b82c30-fd4b-11e3-90c4-3554709a4cf7.png)\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/47245524/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/47245976", "body": "Hi, Just wanted to tell you that the AWS instance is charging me as I was done with my free tier. so, I would like to keep it running till 3:30ET. It would be great if you can help me before that.\n\nThanks in advance and sorry about that\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/47245976/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/47353997", "body": "Hi Thomas, Thanks for the response.Yes, I see that namenode is giving EC2 internal IP address for datanodes and that's the reason, I changed my approach to use SOCKS proxy configuration. hostname gives \"triconnode173\" on my local machine.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/47353997/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "pooleja": {"issues": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/7", "title": "Windows Client Support?", "body": "Does Spring Hadoop support running from a Windows client?  I assume it does, since I see windows specific batch files to execute in the map reduce example.\n\nWhen I build and run on a Windows client, connecting to my cluster, it fails.  First it says it can't load native libs and then it submits the job but fails after that.\n\n```\n11:40:41,919  INFO t.support.ClassPathXmlApplicationContext: 510 - Refreshing org.springframework.context.support.ClassPathXmlApplicationContext@659297ab: startup date [Tue Feb 11 11:40:41 EST 2014]; root of context hierarchy\n11:40:42,176  INFO eans.factory.xml.XmlBeanDefinitionReader: 315 - Loading XML bean definitions from class path resource [META-INF/spring/application-context.xml]\n11:40:42,895  INFO ort.PropertySourcesPlaceholderConfigurer: 172 - Loading properties file from class path resource [hadoop.properties]\n11:40:42,922  INFO ctory.support.DefaultListableBeanFactory: 596 - Pre-instantiating singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@74ab6b5: defining beans [org.springframework.context.support.PropertySourcesPlaceholderConfigurer#0,hadoopConfiguration,wordcountJob,runner]; root of factory hierarchy\n11:40:43,166  INFO he.hadoop.conf.Configuration.deprecation: 840 - fs.default.name is deprecated. Instead, use fs.defaultFS\n11:40:44,706 ERROR             org.apache.hadoop.util.Shell: 303 - Failed to locate the winutils binary in the hadoop binary path\njava.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.\n    at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)\n    at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)\n    at org.apache.hadoop.util.Shell.<clinit>(Shell.java:293)\n    at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:76)\n    at org.apache.hadoop.conf.Configuration.getTrimmedStrings(Configuration.java:1546)\n    at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:519)\n    at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:453)\n    at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:136)\n    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2433)\n    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:88)\n    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2467)\n    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2449)\n    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:367)\n    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:166)\n    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:351)\n    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:287)\n    at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath(FileInputFormat.java:466)\n    at org.springframework.data.hadoop.mapreduce.JobFactoryBean.afterPropertiesSet(JobFactoryBean.java:208)\n    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1547)\n    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1485)\n    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:524)\n    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:461)\n    at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:295)\n    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:223)\n    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:292)\n    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)\n    at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:608)\n    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:932)\n    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:479)\n    at org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:197)\n    at org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:172)\n    at org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:158)\n    at org.springframework.samples.hadoop.mapreduce.Wordcount.main(Wordcount.java:28)\n11:40:45,142  INFO    org.apache.hadoop.yarn.client.RMProxy:  56 - Connecting to ResourceManager at hd-dn-01.grcrtp.local/10.6.64.232:8050\n11:40:45,245  INFO ramework.data.hadoop.mapreduce.JobRunner: 192 - Starting job [wordcountJob]\n11:40:45,302  INFO    org.apache.hadoop.yarn.client.RMProxy:  56 - Connecting to ResourceManager at hd-dn-01.grcrtp.local/10.6.64.232:8050\n11:40:45,971  WARN org.apache.hadoop.mapreduce.JobSubmitter: 258 - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n11:40:46,080  INFO doop.mapreduce.lib.input.FileInputFormat: 287 - Total input paths to process : 1\n11:40:46,422  INFO org.apache.hadoop.mapreduce.JobSubmitter: 394 - number of splits:1\n11:40:46,441  INFO he.hadoop.conf.Configuration.deprecation: 840 - user.name is deprecated. Instead, use mapreduce.job.user.name\n11:40:46,442  INFO he.hadoop.conf.Configuration.deprecation: 840 - fs.default.name is deprecated. Instead, use fs.defaultFS\n11:40:46,444  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapred.mapoutput.value.class is deprecated. Instead, use mapreduce.map.output.value.class\n11:40:46,444  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapred.used.genericoptionsparser is deprecated. Instead, use mapreduce.client.genericoptionsparser.used\n11:40:46,450  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapreduce.map.class is deprecated. Instead, use mapreduce.job.map.class\n11:40:46,450  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapred.job.name is deprecated. Instead, use mapreduce.job.name\n11:40:46,450  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n11:40:46,451  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapreduce.reduce.class is deprecated. Instead, use mapreduce.job.reduce.class\n11:40:46,451  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n11:40:46,452  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n11:40:46,452  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n11:40:46,454  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapred.mapoutput.key.class is deprecated. Instead, use mapreduce.map.output.key.class\n11:40:46,454  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir\n11:40:46,820  INFO org.apache.hadoop.mapreduce.JobSubmitter: 477 - Submitting tokens for job: job_1391711633872_0022\n11:40:47,127  INFO      org.apache.hadoop.mapred.YARNRunner: 368 - Job jar is not present. Not adding any jar to the list of resources.\n11:40:47,225  INFO doop.yarn.client.api.impl.YarnClientImpl: 174 - Submitted application application_1391711633872_0022 to ResourceManager at hd-dn-01.grcrtp.local/10.6.64.232:8050\n11:40:47,291  INFO          org.apache.hadoop.mapreduce.Job:1272 - The url to track the job: http://http://hd-dn-01.grcrtp.local:8088/proxy/application_1391711633872_0022/\n11:40:47,292  INFO          org.apache.hadoop.mapreduce.Job:1317 - Running job: job_1391711633872_0022\n11:40:50,330  INFO          org.apache.hadoop.mapreduce.Job:1338 - Job job_1391711633872_0022 running in uber mode : false\n11:40:50,332  INFO          org.apache.hadoop.mapreduce.Job:1345 -  map 0% reduce 0%\n11:40:50,356  INFO          org.apache.hadoop.mapreduce.Job:1358 - Job job_1391711633872_0022 failed with state FAILED due to: Application application_1391711633872_0022 failed 2 times due to AM Container for appattempt_1391711633872_0022_000002 exited with  exitCode: 1 due to: Exception from container-launch: \norg.apache.hadoop.util.Shell$ExitCodeException: /bin/bash: line 0: fg: no job control\n\n    at org.apache.hadoop.util.Shell.runCommand(Shell.java:464)\n    at org.apache.hadoop.util.Shell.run(Shell.java:379)\n    at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)\n    at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:195)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:283)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:79)\n    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n    at java.lang.Thread.run(Thread.java:662)\n\n\n.Failing this attempt.. Failing the application.\n11:40:50,434  INFO          org.apache.hadoop.mapreduce.Job:1363 - Counters: 0\n11:40:50,470  INFO ramework.data.hadoop.mapreduce.JobRunner: 202 - Completed job [wordcountJob]\n11:40:50,507  INFO    org.apache.hadoop.yarn.client.RMProxy:  56 - Connecting to ResourceManager at hd-dn-01.grcrtp.local/10.6.64.232:8050\n11:40:50,590  INFO ctory.support.DefaultListableBeanFactory: 444 - Destroying singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@74ab6b5: defining beans [org.springframework.context.support.PropertySourcesPlaceholderConfigurer#0,hadoopConfiguration,wordcountJob,runner]; root of factory hierarchy\n```\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/7/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/33623258", "body": "Did you try looking at the attempt logs at the following location?\n http://Admins-MacBook-Pro.local:8088/proxy/application_1390012296433_0009/ \n\nI just tried running the map reduce job against a generic hadoop cluster and go the same error.  When I looked at the attempt logs, I see this:\n\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/hadoop/mapreduce/v2/app/MRAppMaster\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.mapreduce.v2.app.MRAppMaster\n\nAccording to SO, it looks like there might be something wrong with the Yarn class path???\n\nhttp://stackoverflow.com/questions/20699632/hadoop-2-2-word-count-example-failing-on-windows-7\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/33623258/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/34358366", "body": "Yep, that was it.  Thanks!  \n\nIs there any guidance from the Spring Hadoop team on the best way to ensure all the properties are correct?  For example, would it make sense to copy a yarn-site.xm and mapred-site.xml to the client machine and pull it into the spring config?  Or could that cause other types of problems?\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/34358366/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/34924318", "body": "I had edited the batch file so the class path was defined as:\n\n```\nset CLASSPATH=\"%BASEDIR%\"\\etc;\"%REPO%\"\\*\n```\n\nThis allowed all the files to be on the class path without making the command too long.\n\nI found related open bugs:\nhttps://issues.apache.org/jira/browse/YARN-1298\nhttps://issues.apache.org/jira/browse/MAPREDUCE-4052\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/34924318/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/34924385", "body": "My hadoop cluster is a Linux based one.  I am trying to go from a Windows client to a Linux cluster.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/34924385/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "brendangreene": {"issues": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/4", "title": "Having issues with your mapreduce example", "body": "I've installed Hadoop-2.2.0 and I'm able to run the hadoop samples just fine from the command line.   Interested in using Spring though so I tried this sample.  Whenever I run the job I get this output:\n\nsh ./target/appassembler/bin/wordcount\n09:24:53,391  INFO t.support.ClassPathXmlApplicationContext: 510 - Refreshing org.springframework.context.support.ClassPathXmlApplicationContext@370410a7: startup date [Sat Jan 18 09:24:53 MST 2014]; root of context hierarchy\n09:24:53,540  INFO eans.factory.xml.XmlBeanDefinitionReader: 315 - Loading XML bean definitions from class path resource [META-INF/spring/application-context.xml]\n09:24:53,914  INFO ort.PropertySourcesPlaceholderConfigurer: 172 - Loading properties file from class path resource [hadoop.properties]\n09:24:53,938  INFO ctory.support.DefaultListableBeanFactory: 596 - Pre-instantiating singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@4fe596de: defining beans [org.springframework.context.support.PropertySourcesPlaceholderConfigurer#0,hadoopConfiguration,wordcountJob,setupScript,runner]; root of factory hierarchy\n09:24:54,096  INFO he.hadoop.conf.Configuration.deprecation: 840 - fs.default.name is deprecated. Instead, use fs.defaultFS\n2014-01-18 09:24:54.573 java[4237:1703] Unable to load realm info from SCDynamicStore\n09:25:28,945  WARN  org.apache.hadoop.util.NativeCodeLoader:  62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n09:25:29,488  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapred.used.genericoptionsparser is deprecated. Instead, use mapreduce.client.genericoptionsparser.used\n09:25:29,501  INFO  org.apache.hadoop.fs.TrashPolicyDefault:  92 - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n09:25:30,707  INFO    org.apache.hadoop.yarn.client.RMProxy:  56 - Connecting to ResourceManager at localhost/127.0.0.1:8032\n09:25:30,759  INFO ramework.data.hadoop.mapreduce.JobRunner: 192 - Starting job [wordcountJob]\n09:25:30,790  INFO    org.apache.hadoop.yarn.client.RMProxy:  56 - Connecting to ResourceManager at localhost/127.0.0.1:8032\n09:25:31,055  WARN org.apache.hadoop.mapreduce.JobSubmitter: 258 - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n09:25:31,111  INFO doop.mapreduce.lib.input.FileInputFormat: 287 - Total input paths to process : 1\n09:25:31,260  INFO org.apache.hadoop.mapreduce.JobSubmitter: 394 - number of splits:1\n09:25:31,271  INFO he.hadoop.conf.Configuration.deprecation: 840 - user.name is deprecated. Instead, use mapreduce.job.user.name\n09:25:31,272  INFO he.hadoop.conf.Configuration.deprecation: 840 - fs.default.name is deprecated. Instead, use fs.defaultFS\n09:25:31,275  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapred.mapoutput.value.class is deprecated. Instead, use mapreduce.map.output.value.class\n09:25:31,276  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapreduce.map.class is deprecated. Instead, use mapreduce.job.map.class\n09:25:31,276  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapred.job.name is deprecated. Instead, use mapreduce.job.name\n09:25:31,276  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n09:25:31,277  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapreduce.reduce.class is deprecated. Instead, use mapreduce.job.reduce.class\n09:25:31,277  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n09:25:31,277  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n09:25:31,278  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n09:25:31,278  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapred.mapoutput.key.class is deprecated. Instead, use mapreduce.map.output.key.class\n09:25:31,279  INFO he.hadoop.conf.Configuration.deprecation: 840 - mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir\n09:25:31,412  INFO org.apache.hadoop.mapreduce.JobSubmitter: 477 - Submitting tokens for job: job_1390012296433_0009\n09:25:31,641  INFO      org.apache.hadoop.mapred.YARNRunner: 368 - Job jar is not present. Not adding any jar to the list of resources.\n09:25:31,705  INFO doop.yarn.client.api.impl.YarnClientImpl: 174 - Submitted application application_1390012296433_0009 to ResourceManager at localhost/127.0.0.1:8032\n09:25:31,748  INFO          org.apache.hadoop.mapreduce.Job:1272 - The url to track the job: http://Admins-MacBook-Pro.local:8088/proxy/application_1390012296433_0009/\n09:25:31,749  INFO          org.apache.hadoop.mapreduce.Job:1317 - Running job: job_1390012296433_0009\n09:25:35,778  INFO          org.apache.hadoop.mapreduce.Job:1338 - Job job_1390012296433_0009 running in uber mode : false\n09:25:35,780  INFO          org.apache.hadoop.mapreduce.Job:1345 -  map 0% reduce 0%\n09:25:35,796  INFO          org.apache.hadoop.mapreduce.Job:1358 - Job job_1390012296433_0009 failed with state FAILED due to: Application application_1390012296433_0009 failed 2 times due to AM Container for appattempt_1390012296433_0009_000002 exited with  exitCode: 127 due to: Exception from container-launch: \norg.apache.hadoop.util.Shell$ExitCodeException: \n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:464)\n        at org.apache.hadoop.util.Shell.run(Shell.java:379)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)\n        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:195)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:283)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:79)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:166)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\n\n.Failing this attempt.. Failing the application.\n09:25:35,850  INFO          org.apache.hadoop.mapreduce.Job:1363 - Counters: 0\n09:25:35,858  INFO ramework.data.hadoop.mapreduce.JobRunner: 202 - Completed job [wordcountJob]\n09:25:35,876  INFO    org.apache.hadoop.yarn.client.RMProxy:  56 - Connecting to ResourceManager at localhost/127.0.0.1:8032\n09:25:35,914  INFO ctory.support.DefaultListableBeanFactory: 444 - Destroying singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@4fe596de: defining beans [org.springframework.context.support.PropertySourcesPlaceholderConfigurer#0,hadoopConfiguration,wordcountJob,setupScript,runner]; root of factory hierarchy\nException in thread \"main\" org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'runner': Invocation of init method failed; nested exception is java.lang.IllegalStateException: Job wordcountJob] failed to start; status=FAILED\n        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1488)\n        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:524)\n        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:461)\n        at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:295)\n        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:223)\n        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:292)\n        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)\n        at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:626)\n        at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:932)\n        at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:479)\n        at org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:197)\n        at org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:172)\n        at org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:158)\n        at org.springframework.samples.hadoop.mapreduce.Wordcount.main(Wordcount.java:28)\nCaused by: java.lang.IllegalStateException: Job wordcountJob] failed to start; status=FAILED\n        at org.springframework.data.hadoop.mapreduce.JobExecutor$2.run(JobExecutor.java:223)\n        at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:49)\n        at org.springframework.data.hadoop.mapreduce.JobExecutor.startJobs(JobExecutor.java:172)\n        at org.springframework.data.hadoop.mapreduce.JobExecutor.startJobs(JobExecutor.java:164)\n        at org.springframework.data.hadoop.mapreduce.JobRunner.call(JobRunner.java:52)\n        at org.springframework.data.hadoop.mapreduce.JobRunner.afterPropertiesSet(JobRunner.java:44)\n        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1547)\n        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1485)\n        ... 13 more\n\nAny thoughts on this?  Looks like the job is connection to hadoop and the resource manager...\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/4/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/32990801", "body": "I'm seeing this in my resourcemanager logs:\n\n2014-01-21 20:34:26,801 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Application application_1390064045818_0006 failed 2 times due to AM Container for appattempt_1390064045818_0006_000002 exited with  exitCode: 127 due to: Exception from container-launch: \norg.apache.hadoop.util.Shell$ExitCodeException: \n    at org.apache.hadoop.util.Shell.runCommand(Shell.java:464)\n    at org.apache.hadoop.util.Shell.run(Shell.java:379)\n    at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)\n    at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:195)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:283)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:79)\n    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:166)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\n\n.Failing this attempt.. Failing the application.\n2014-01-21 20:34:26,801 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1390064045818_0006_02_000001, NodeId: 192.168.1.135:56710, NodeHttpAddress: 192.168.1.135:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.1.135:56710 }, ] resource=<memory:2048, vCores:1> queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>\n2014-01-21 20:34:26,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing info for app: application_1390064045818_0006\n2014-01-21 20:34:26,801 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1390064045818_0006 State change from ACCEPTED to FAILED\n2014-01-21 20:34:26,802 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>\n2014-01-21 20:34:26,802 WARN org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=bgreene  OPERATION=Application Finished - Failed TARGET=RMAppManager RESULT=FAILURE  DESCRIPTION=App failed with state: FAILED   PERMISSIONS=Application application_1390064045818_0006 failed 2 times due to AM Container for appattempt_1390064045818_0006_000002 exited with  exitCode: 127 due to: Exception from container-launch: \norg.apache.hadoop.util.Shell$ExitCodeException: \n    at org.apache.hadoop.util.Shell.runCommand(Shell.java:464)\n    at org.apache.hadoop.util.Shell.run(Shell.java:379)\n    at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)\n    at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:195)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:283)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:79)\n    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:166)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\n\n.Failing this attempt.. Failing the application.    APPID=application_1390064045818_0006\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/32990801/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "trisberg": {"issues": [], "commits": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/48686e7eead6afde1191359409a2d25861da86e7", "message": "Fixing setupScript to remove directories in Hive output directory"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/b1569e5f9f1fdfde9530e44bf0b32c0d1d3798d1", "message": "Updating Hive example to use HiveServer2\n\n- upgrade to Spring for Apache Hadoop 2.3.0.M1\n\n- update JDBC URL for HiveServer2\n\n- add hiveDataSource to Hive context files and modify hiveClientFactory bean definition to use it\n\n- modify code using HiveCLient"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/58e405318d88e2e94bbf79d2b17c52505aba6ee8", "message": "Reset hive properties to default to localhost"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/1ea06737ee31565b21f9ae51fb751ad42107c84c", "message": "rename user (reserved word) column to userName"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/ad064a23523bf97b923b444636243991129055dc", "message": "Remove unused files"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/6cccd6ef768045d040360cdeb52d1ca4c16bcff4", "message": "Reset hadoop properties to default to localhost"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/d968b1efa6b2414963b3c3253862de5e743f6710", "message": "Update README for Hive example"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/d836fa8dc9ce0de5f8beed8ff24aae8f05ea7c7d", "message": "update Hive example to run all operations against remote HiveServer"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/e1e48a72c5327e52fe631482a805db36f904008b", "message": "Adding note examples are buildt with 2.2.0.RELEASE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/b7b4521a5aae9e0c77f4628854a0a153bdd41fe2", "message": "Updating dependency versions and Gradle wrapper"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/b11607475d4084cc39aa3039b7b4985aa6559047", "message": "Updating dependency versions and logging dependency"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/2906c5607eead872f7df34adfc0221ff9ecf7573", "message": "Updating dependency versions and Gradle wrapper"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/ef9320d42c0055500729db3fd138f3a235eed996", "message": "Updating Spring version to 4.1.6"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/827d68d53575e7e5ad292e98486f6e6b0a01b0a7", "message": "Adding Spring Batch MapReduce example"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/c1a9ff68bf24d34dde354c51f155389cee621933", "message": "Updating hive-batch example"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/42fa6440176482f63ae85aac97ee3eb7f512bfcb", "message": "Removing old config directories"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/c99771aed9206510474d1468c9f6bd102bbcd86d", "message": "Updating pig sample to 2.2.0.RELEASE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/c53d50454809aadb82ed4d5f60e075e979972d51", "message": "Updating hive sample to 2.2.0.RELEASE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/99f8b26cb50827d698ff9abf83a313275d3c0385", "message": "Updating gitignore"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/aff269c9c8552051a64965433f339390db95e953", "message": "Updating mapreduce sample to 2.2.0.RELEASE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/4fc151bb8713c4cec6fe93303174995debf55a36", "message": "Adding HBase example from Spring Data book and updating it to recent versions"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/181aebda818163534f5807ec3b1771ca46d611af", "message": "Adding HBase example from Spring Data book and updating it to recent versions"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/e61b7b6fe8e59cf69fb6ab0f4f20bf7ed8cd66da", "message": "Adding HBase example from Spring Data book and updating it to recent versions"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/9c2bda260a20da0412d184703ce11e131e56ae9e", "message": "update Hive example to SHDP 2.1.0.RELEASE and Hadoop 2.6.0"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/6b53415e58d9b3fcd694517d27ce3dc1f33bfa50", "message": "Update gitignore"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/25716e6531ef81a64c7bf558b2cd048be9512590", "message": "Updating versions"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/56b1a069a27e587b5c76c74bb55c7731e5043167", "message": "Update README.asciidoc"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/9fda091d9657c872193637fb05c2d38f8a59d5e1", "message": "Update CDH5 instructions"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/c51a741337447f339d9a336f88be153993b18832", "message": "Updating to SHDP 2.0.0.RELEASE and Spring Boot 1.1.0"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/42b6a7a07f31eb5665507f09ee6b957fd7973e1e", "message": "Updating to SHDP 2.0.0.RELEASE and Spring IO versions"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/1469ac3fd200b558c96edf1b536c7052fbd7da0b", "message": "Adding dataset example"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/adc78ee8d97c002bd559b6dbb5c7deee5594ca41", "message": "Update README.asciidoc"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/5796aad712fb37b2c3edfcb1a143ae987808f907", "message": "Switching to use Hadoop 2.2.0 and a separate Hive server"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/f799a4dc983bf5f13ec57ac5a9d58e8b285fb140", "message": "Updated Hive instructions"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/bab772dfaaa66e30e04bfe70711053855e1641c3", "message": "Adding output"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/77d733666d4ba553e34f3b12278b7ca713b62416", "message": "Switching to use Hadoop 2.2.0 as default"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/ec3de94ba48892cbd868bcf3aa95a50f4eef14fb", "message": "Adding new Hive Batch example"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/e65a504b579b20b0d3cc29108bcf2d11b41c2a56", "message": "Adding CDH5 and HDP21 profiles"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/cf9b1e3266a0969257d95c511ebeddc847d5ed62", "message": "Update README.asciidoc"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/5be31651b29e1eee2b657dde32905f91aa752db0", "message": "Adding instructions for phd20 profile"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/981f794422f3c6027399baae189f27db5f558698", "message": "Adding Pivotal HD 2.0 profile"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/623498fa378205d25eed0c6315ba0c4169d08440", "message": "Updating to match latest 2.0.0.RC3 release of Spring for Apache Hadoop"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/75d86e06740a08021526da39a865ae938a9e9b76", "message": "removing mavanLocal() to make build more reliable"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/4e9211cf7a68bedb4ddf8d17ff30c42909bea9a0", "message": "Switching from app.home to basedir property too enable testing on Windows"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/1d39c86eb25e7a66be013271a5ecc400a2301a23", "message": "Update pom.xml"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/f737d36140984d4c87aa23a19d7d3057297e2489", "message": "Adding Hadoop 2.2.0 profile and updating to Pivotal HD 1.1"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/41c29e40bfacdb515d43e23444aed5a9bdf3a8dc", "message": "Update Hive sample to use Apache Hadoop 1.2.1"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/1dba440c35fac23b7d055f996cca95c40602b488", "message": "Updating to 1.0.1.RELEASE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/93010f17d1a0429a571083dfa9e559475a46d927", "message": "SHDP-160 Adding another sample app - pig"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/bc61202f60849ecac9beb7d089ea21f95cf235d8", "message": "Polish"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/9a47950f4ab018939d9b40b69ff1ad16d9c6e932", "message": "SHDP-160 Adding second sample app - hive"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/74f85781a12a7683b9fb7556ea8e33927444bbf3", "message": "Moving log4j.properties to separate config directory to be placed first on classpath"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/c7248b9ffd5364e0fdd7179993df2ecec7de98c4", "message": "Adding log4j to parent pom"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/eb90a302002b6650ce93d2eacb6ba821ff7a5d67", "message": "SHDP-160 Updating logging to use log4j"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/e314ecb18181f8fe2f794ec75d756908152353fe", "message": "SHDP-160 Moving parent pom to parent directory"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/d67312158c257d3a115504e4750e50bed9d99c71", "message": "SHDP-160 Adding dependencies and config for running on YARN"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/a949742544affdf5ceb67888e49f64bc3ea2eccb", "message": "Fixing build issue for hadoop-examples jar"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/ce812d1f15dac0caf46e9cbabb518c5182312352", "message": "Formatting"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/9327833888c83de76b3c8d87c6144509ca283ff8", "message": "SHDP-160 First new sample app - mapreduce"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/25908749", "body": "It's more of a framework for do-it-yourself object-mapping - look at the RowMapper.java interface in the org.springframework.data.hadoop.hbase package - you would have to implement that. Maybe we should rephrase the readme for the samples. \n\nThe samples are being re-organized and we don't have a current HBase example.  Here is a link to an older sample that might need to have some config setting adjusted - https://github.com/spring-projects/spring-hadoop-samples-old/tree/master/original-samples/hbase-crud\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/25908749/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/32872040", "body": "Hi,\n\nI'm assuming you built using the '-P hadoop22' profile.\n\nIt looks like the application is starting but then failing - anything in the resourcemanager or nodemanager logs indicating a failure to run the job?\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/32872040/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/33026076", "body": "So, it looks like you are missing some permissions on the file system. You could try to change the following lines in hadoop.properties\n\nwordcount.input.path=/user/gutenberg/input/word/\nwordcount.output.path=/user/gutenberg/output/word/\n\nchange that to some directories that your user has write permissions for.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/33026076/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/34353552", "body": "Could you try setting this property in your config:\n\nyarn.resourcemanager.hostname\n\nSet that to the hostname where the RM is running.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/34353552/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/34362654", "body": "I've used both ways for providing config options and the net effect is the same, so pick the one that you are more comfortable using. I tend to prefer to collect my configurations in a properties file that's part of my application.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/34362654/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/34906699", "body": "How are you building and running the example? The batch file that is generated is built via Maven Appassembler plug-in. They don't seem to work right if you have a deep directory structure - got some error about command being too long. \n\nAlso see some other error -  Could not resolve placeholder 'app.home' - so not sure how well these generated batch files actually work. Not sure if anyone has run these examples successfully on Widows. \n\nIs your Hadoop cluster on Windows as well?\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/34906699/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/34923688", "body": "I just updated the samples to use $basedir instead of $app.home since the generated batch file for Windows doesn't set the app.home system property. Ran the wordcount sample successfully on Windows.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/34923688/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/34926828", "body": "Nice find on that bug. My test ran fine since I was running against a Windows based Hadoop cluster, haven't tried going from Windows client to Linux cluster.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/34926828/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/45889397", "body": "Hi, thanks for trying this. I know I tested the sample on the CDH5 Quickstart VM and don't think I changed that setting. Did you use the Quickstart VM or did you use your own install? \n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/45889397/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/45890205", "body": "Strange. Looking at my config -\n\nContainer Memory Minimum 1 GiB default value\nyarn.scheduler.minimum-allocation-mb\n\nSo my mapreduce example runs in less than 1GB.\n\nI assume you have the latest with the Spring for Apache Hadoop 2.0 and build using the cdh5 profile:\n\nmvn clean package -Pcdh5\nsh target/appassembler/bin/wordcount\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/45890205/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/45927844", "body": "I just tried a new install of the Quickstart VM and I'm able to reproduce this - must have made some config change on the other VM I tried - I'll add a note in the instructions.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/45927844/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/45931769", "body": "Thanks for reporting this.\n\nI have updated the README\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/45931769/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/47231669", "body": "Looks like a connectivity issue. What do you have in  src/main/resources/application.properties? I could try accessing this from my system - let me know if it is ok for me to access your AWS instance.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/47231669/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/47346319", "body": "Took a quick peek yesterday but didn't have enough time to test it. From what I could tell the datanodes seemed to use an AWS internal address rather than a public address which could be the cause. What does `hostname` return on your VM?\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/47346319/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/47776106", "body": "What's your environment? Haven't had a problem running this example as is.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/47776106/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/49551850", "body": "Try using the phd1 profile by specifying -Pphd1. The 2.0.5-alpha-gphd-2.1.0.0 version corresponds to Pivotal HD 1.1, so that's why the phd1 profile is the one to use. You could use the phd20 profile with the latest Pivotal HD 2.0 singlenode VM which has a version of 2.2.0-gphd-3.0.1.0. The Pivotal HD versioning is a bit confusing.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/49551850/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/49562394", "body": "The server and client versions are the same, 2.0.5-alpha-gphd-2.1.0.0 is Pivotal HD 1.1 even though it's hard to guess that from the version.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/49562394/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/59833105", "body": "Try specifying the following properties in your `<yarn:configuration>`:\n\n```\nyarn.application.classpath=/etc/hadoop/conf,/usr/lib/hadoop/*,/usr/lib/hadoop/lib/*,/usr/lib/hadoop-hdfs/*,/usr/lib/hadoop-hdfs/lib/*,/usr/lib/hadoop-yarn/*,/usr/lib/hadoop-yarn/lib/*\nmapreduce.application.classpath=/usr/lib/hadoop-mapreduce/*,/usr/lib/hadoop-mapreduce/lib/*\n```\n\nYou can also include yarn-site.xml and mapred-site.xml on your classpath. Check the clusters yarn-site.xml/mapred-site.xml for the correct paths.\n\nAFAIK there is no reliable way for the application to detect these classpaths, so you have to provide them.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/59833105/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/118951726", "body": "Created hbase example based on the Spring Data book example. Updated to current versions. Continue discussion on StackOverflow - http://stackoverflow.com/questions/31232486/spring-hbasetemplate-throws-java-lang-illegalargumentexception-not-a-hostport\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/118951726/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "jvalkeal": {"issues": [], "commits": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/70c6e993153095c5eed752b0bde31245c4ec7e84", "message": "Update yarn-store-groups to shdp 2.2.0"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/45c2c8836c22f002db345f5a0750e11045f0b27e", "message": "Add yarn-store-groups sample"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/0e8f85b29f1563a8c2160fd6c4b951b0a381d28a", "message": "update yarn-boot-simple to shdp 2.1.0.RELEASE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/3beebf1b0dae47c7ee8812a2dc149240df808848", "message": "update boot-fsshell to shdp 2.1.0.RELEASE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/c8b4869fc08470f7ecae25ba1bddd78ada3cb306", "message": "update yarn samples to shdp 2.1.0.RELEASE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/ae83cca2e70b03dca272eda939f4590bf9b020e5", "message": "Fix typo"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/ddf8c9780d853751738c371061a403f12ca0d985", "message": "Adding boot-fsshell sample"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/f916cd06ffcb6614b51fa437a811be8f60a7c5ba", "message": "Upgrade SHDP 2.0.0.RELEASE for yarn samples"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/1cf7aa7027b32bc801541b48d555ec92275d0f4e", "message": "Update yarn and boot samples to use 2.0.0.RC4"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/2a8b39fd21f73808b8970f1bd0243ca189863522", "message": "Upgrade yarn samples to RC2"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/dad0a8e709f86ad751a3ceb5b919e52b1b9277ed", "message": "Yarn sample updates for RC1"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/a6dd1c86f58bf0bde42fbfe1a8fd9284ceff72ff", "message": "Yarn sample updates for M6"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/f98d6c07dffa97ab4559c8cf655c04c71e883569", "message": "Fix samples for M5"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/3f75e76e9596fbd5d6cf70e812791efd230c310a", "message": "Yarn sample updates for M4"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/63863e767c18e2021ba5623743ff98dc6b1d41d7", "message": "Merge remote-tracking branch 'jvalkeal/m2-updates'"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/d2903c5b824fd93e4fc2d48fc5150b1dc7a92e24", "message": "Fix broken custom-amservice sample"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/57a0eeae2802562a7e3ba9b8ef905d2f0b7f190f", "message": "Build yarn samples with 2.0.0.M2-hadoop20"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/529bc89d2a91595aafb81e5ab163f41ff86312dc", "message": "work for SHDP-190"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/fe642f1f3bb3d0ef5d2f4de757b29551f60d1c25", "message": "switch to local shdp build"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/0b942bfcdccebe3590d6d1cb9dbc9aa272ac8878", "message": "SHDP-179 adding quick command instructions"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/33fb9c09f82b86d48ffa2e20e5867fa430814670", "message": "Update readme"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/01cf45c880012b35b366d44d68b32e9ee5ee549f", "message": "Modify spring-hadoop build version\n\nNow using Spring Hadoop 2.x M1 with\nhadoop20 and phd1 flavors. Local maven repo\nreplaced with spring milestone repo."}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/605351bf19f1fb9f292d85247d813e0ac3b9a1bb", "message": "Changes for SHDP-173"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/7c690e6b304d4ee09bcbac1d2455bbb408796312", "message": "Yarn examples mods for SHDP-170"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/5df95c6fd9bb5e3aef6d4d207d8629b798181e36", "message": "Yarn samples from hadoop 2.0.5 to 2.0.6"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/36eaa9236dcc94b132540a28276c6908303fb558", "message": "Change MultiContextTests timeout"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/549aca5c7c2c8cc17fa5b84c6a769f1d2b6e2957", "message": "Update readme"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/736f7461594b21955719858f1a0b3e71aedeb7e8", "message": "Adding restart-context example"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/10589d6e18d99fa598d42345d1aea85cd4fdba0a", "message": "Fixing dependency copy for build and test\n\nNow hadoop deps are copied and resolved for both\ntest and build tasks."}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/c7f0bcf25f1401eba4ca47de0c9fc1f815b25ad6", "message": "Import of original spring-yarn samples"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/59887067", "body": "Might be something to do with build because samples are still on vanilla hadoop 2.2.0 and cdh5 might be newer. Did you found container logs(stdout and stderr logs for application master) which might tell more. You could also try our IO guides https://spring.io/guides?filter=yarn. Building Spring YARN Projects with Gradle/Maven guides have instructions howto build against different distros and cdh5 is one supported.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/59887067/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/60062387", "body": "Classpath in yarn apps is one complex monster if being honest. Passing it from client would work if environment is exactly same as in the cluster which would be the case if you run the client from one of the cluster nodes. There is no universal way to find out what is the classpath so that you could run client from any location and with any env settings.\n\nIn our io guides https://spring.io/guides?filter=yarn, we pretty much package all jars in boot executable fat jar which forces classpath to be whatever we have in that executable jar. Of course this will make your 'app' a bigger but it isolates the whole classpath if you want to use something would not work with jars already in hadoops classpath. Guava, protobuf, thrift libs are usually the ones which are really old in hadoop distros and may not work with your versions.\n\nIf you want to re-use jars from hadoop distro then in reality you always need to hardcode something because distros are different.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/60062387/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "trevormarshall": {"issues": [], "commits": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/commits/2c766eabe5302e5d9bbd9a763f66dfb4d80c4c1b", "message": "Initial commit"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zelinzheng": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/pulls/15", "title": "Update application-context.xml", "body": "The libs property has to be file:/// + jar-path\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/47814564", "body": "Oh, I was using windows. Maybe that's environment specific setting. Never mind. Hi, Thomas, I just saw you answered my question in stackoverflow. How to configure MultiOutputFormat from HCatalog API in spring-hadoop project? I upload more details. Maybe you can look at it again. Thanks.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/47814564/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "chinatimwu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/25944990", "body": "Thx for reply. \n\nTo other concerns,\nCurrently, spring-data-book (https://github.com/spring-projects/spring-data-book) also has a simply sample about HBase.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/25944990/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "SaiPrasannaAnnamalai": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/36781095", "body": "Hi, Same problem...but coudnt solve....please help...i have a 5 node cluster with one master and 4 slaves.\nI have set the ip-address of the master node [in fact even tried hard-coding] for 'yarn.resourcemanager.hostname' in the yarn-site.xml file. But even then i get the following in the log files. \nERROR:........Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.\n2014-03-05 20:15:50,597 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.\n2014-03-05 20:15:50,603 INFO [main] org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8030\n2014-03-05 20:15:56,632 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n\nWhat could be the reason. Why is not hadoop picking up the parameter that i had set...???\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/36781095/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ameizi": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/41520155", "body": "It may be in _yarn-site.xml_ you had set _yarn.application.classpath_ and in _/etc/profile_ you had set _yarn.application.classpath_ too. the _yarn.application.classpath_ you defined two times. you should delete one of _yarn.application.classpath_ and restart hadoop   clusters then run again.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/41520155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "renaud": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/45889711", "body": "Thanks Thomas for writing back. \nI used the Quickstart VM.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/45889711/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/45892256", "body": "Yes, I did build with the cdh5 profile. So if you can't reproduce let's close this issue. It's working on my end, so all good.\nThanks, Renaud\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/45892256/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/45944769", "body": "Ok. Thanks too for the prompt reply!\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/45944769/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "chang-chao": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/59898236", "body": "Finally,I made it work.(Though still there is something that I cannot understand)\n\nThere was ClassNotFound(log4j,org.apache.hadoop.conf.Configuration) exception in stderr(Appmaster.stderr) for application master log,So I commented out the \"hadoopruntime.exclude\" declaration in build.gradle,after that the app finished and succeeded.\n\nBy the way,the following should be noticed.\n1.the following env viariables ,which seems to be used when master is started,are not set automatically when cdh is installed,I had to set them manually.\n- HADOOP_CONF_DIR\n- HADOOP_COMMON_HOME\n- HADOOP_HDFS_HOME\n- HADOOP_MAPRED_HOME\n- HADOOP_YARN_HOME\n\n2.As with this cloudera issue:[Container erases temporary file and shell script immediately after execution](https://issues.cloudera.org/browse/DISTRO-538),you have to monitor file to get the log file.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/59898236/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/60028838", "body": "> which then should put that classpath into the environment hashmap that gets passed to the Yarn AppMaster.\n\nI doubt on that.\n\nWhat is the value of \"yarn.application.classpath\" in your env,does it look like something like below?\nand make sure the env variables used are all set.\n\n<pre>\n        $HADOOP_CONF_DIR,\n        $HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,\n        $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,\n        $HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,\n        $HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*\n</pre>\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/60028838/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/60209788", "body": "After digging nearly two days,I think I found the reason why ClassNotFound(class in commons-logging lib) exception occured in the simple yarn app.\n\nFirstly,we should  ****definitely set the yarn.application.classpath value****  (through adding yarn-site.xml to the classpath of the client) according to cluster env in the  ****app client**** .If not so,the default value(eg.$HADOOP_COMMON_HOME/share/hadoop/common/*,...) will be used,which is not the right path in many distros(at least CDH5)\n\nSecondly,****Don't use multiple-line values  for \"yarn.application.classpath\"**** ,in yarn-site.xml,in my cluster (cdh5,java7,centos),classpath could not be fully and correctly parsed if we did so.\nunfortunately the yarn-site.xml in cdh5 cluster is containing the multi-line value,which I think is a bug.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/60209788/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/60209942", "body": "I think I found the reason,you can see the explanation in issue 22[https://github.com/spring-projects/spring-hadoop-samples/issues/22]\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/60209942/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/118865890", "body": "As is shown in the log,the hbase client cannot conenct to zookeeper port(localhost:2181).\nDid you start hbase in the same machine with the client?\nIf not,change https://github.com/spring-projects/spring-data-book/blob/master/hadoop/hbase/src/main/resources/hbase.properties to relfect your hbase server configuration.\n\nBy the way,I don't think this is not the proper place to ask a question about spring-data-book project.\nMaybe you should post your question to stackoverflow or somewhere else.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/118865890/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "akshatthakar": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/71959275", "body": "I faced similar issue.\nI traced the roor cause of issue by checking logs on History server using Web Url.\n\nThis exception is in launching Container for MR job. This was due to classpath issue, I had some jars(avro jars) which were clashing with Yarn framework jars. I removed those jars from job jar and it worked.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/71959275/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "vvelayutham": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/87981185", "body": "i am still facing the issue. We have CDH5.1.2 and today we deployed kerberos security on top of it. The installation was done almost 8 months back without kerberos and we have a MR running for past 6 months. After the kerberos setup was done, when i run the same MR, its failing to load the third-party library that we set in the HADOOP_CLASSPATH variable.\n\n[ec2-user@etl ]$ hadoop classpath\n/etc/hadoop/conf:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop/lib/_:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop/.//_:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop-hdfs/./:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop-hdfs/lib/_:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop-hdfs/.//_:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop-yarn/lib/_:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop-yarn/.//_:/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/lib/_:/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/.//_\n[ec2-user@etl ]$ export HADOOP_CLASSPATH=`hadoop classpath`:/home/ec2-user/framework/framework.jar:/home/ec2-user/framework/lib/java-json.jar:/home/ec2-user/framework/lib/log4j-core-2.0.1.jar:/home/ec2-user/framework/lib/log4j-api-2.0.1.jar:/home/ec2-user/framework/lib/commons-dbcp-1.4.jar:/home/ec2-user/framework/lib/commons-pool-1.6.jar:/home/ec2-user/framework/lib/mysql-connector-java-5.1.31-bin.jar:\n[ec2-user@etl ]$ \n[ec2-user@etl ]$ hadoop classpath\n/etc/hadoop/conf:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop/lib/_:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop/.//_:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop-hdfs/./:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop-hdfs/lib/_:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop-hdfs/.//_:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop-yarn/lib/_:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop-yarn/.//_:/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/lib/_:/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/.//_:/etc/hadoop/conf:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop/lib/_:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop/.//_:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop-hdfs/./:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop-hdfs/lib/_:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop-hdfs/.//_:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop-yarn/lib/_:/opt/cloudera/parcels/CDH-5.1.2-1.cdh5.1.2.p0.3/lib/hadoop/libexec/../../hadoop-yarn/.//_:/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/lib/_:/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/.//_:/home/ec2-user/framework/framework.jar:/home/ec2-user/framework/lib/java-json.jar:/home/ec2-user/framework/lib/log4j-core-2.0.1.jar:/home/ec2-user/framework/lib/log4j-api-2.0.1.jar:/home/ec2-user/framework/lib/commons-dbcp-1.4.jar:/home/ec2-user/framework/lib/commons-pool-1.6.jar:/home/ec2-user/framework/lib/mysql-connector-java-5.1.31-bin.jar:\n\nHadoop environment is set in the path and when i run the hadoop/yarn command it fails and the application log says:\n2015-03-31 00:19:19,625 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster\njava.lang.NoClassDefFoundError: org/json/JSONException\n    at java.lang.Class.forName0(Native Method)\n    at java.lang.Class.forName(Class.java:190)\n    at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.isChainJob(JobImpl.java:1292)\n    at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.makeUberDecision(JobImpl.java:1234)\n    at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.access$3700(JobImpl.java:139)\n    at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1435)\n    at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1373)\n    at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)\n    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n    at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:986)\n    at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:138)\n    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1249)\n    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1049)\n    at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$1.run(MRAppMaster.java:1460)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)\n    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1456)\n    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1389)\nCaused by: java.lang.ClassNotFoundException: org.json.JSONException\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n    ... 22 more\n\nThis was a running MR and it starts failing since today. Only change done was adding the kerberos setup in the configuration object and setting userinformationobject. After running the build and updating the JAR, we started having failures. Any clues from anyone would be really helpful.\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/87981185/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "DRUNK2013": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/98689794", "body": "HI, when i run this command:\n[root@drunk hive]# sudo -u hive sh ./target/appassembler/bin/hiveApp\n04:04:45,672  INFO t.support.ClassPathXmlApplicationContext: 513 - Refreshing org.springframework.context.support.ClassPathXmlApplicationContext@15e337bf: startup date [Mon May 04 04:04:45 PDT 2015]; root of context hierarchy\n04:04:45,965  INFO eans.factory.xml.XmlBeanDefinitionReader: 316 - Loading XML bean definitions from class path resource [META-INF/spring/hive-context.xml]\n04:04:47,186  INFO eans.factory.xml.XmlBeanDefinitionReader: 316 - Loading XML bean definitions from class path resource [META-INF/spring/jdbc-context.xml]\n04:04:48,267  INFO ort.PropertySourcesPlaceholderConfigurer: 172 - Loading properties file from class path resource [hadoop.properties]\n04:04:48,271  INFO ort.PropertySourcesPlaceholderConfigurer: 172 - Loading properties file from class path resource [hive.properties]\n04:04:48,303  INFO ion.AutowiredAnnotationBeanPostProcessor: 141 - JSR-330 'javax.inject.Inject' annotation found and supported for autowiring\n04:04:49,551  INFO ans.factory.config.PropertiesFactoryBean: 172 - Loading properties file from class path resource [hive-server.properties]\n04:04:58,357  INFO ontext.support.DefaultLifecycleProcessor: 341 - Starting beans in phase -2147483648\n04:04:58,369  INFO ingframework.samples.hadoop.hive.HiveApp:  31 - Hive Application Running\n04:04:58,369  INFO ingframework.samples.hadoop.hive.HiveApp:  32 - 000000000000000000\n04:04:58,369  INFO ingframework.samples.hadoop.hive.HiveApp:  34 - Here.........................\n04:04:58,369  INFO ingframework.samples.hadoop.hive.HiveApp:  36 - show tables...................\n\nThis is stop in template.query(\"show tables\");  \nlog.info(\"show tables...................\");\ntemplate.query(\"show tables\");  \nlog.info(\"show tables end..............\");\n\nCan you give some suggest, god bless you,\n", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/98689794/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "whitfin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/301275378", "body": "I found this issue whilst looking for something similar I was seeing with Hadoop. This leads me to believe it's coming from the Hadoop libraries rather than the Spring libraries? \r\n\r\nIt's weird though, excluding JSP API didn't fix it for me.", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/301275378/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "yibanguaiqi": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/327749152", "body": "i got this error 2", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/327749152/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "bric3": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/345660179", "body": "Foud this issue after upgrading a simple spring boot web project as well.", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/345660179/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "digz6666": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/357884010", "body": "Try include following to your dependencies:\r\n```\r\n<dependency>\r\n    <groupId>javax.el</groupId>\r\n    <artifactId>javax.el-api</artifactId>\r\n    <version>3.0.0</version>\r\n</dependency>\r\n<dependency>\r\n    <groupId>org.glassfish</groupId>\r\n    <artifactId>javax.el</artifactId>\r\n    <version>3.0.0</version>\r\n</dependency>\r\n```", "reactions": {"url": "https://api.github.com/repos/spring-projects/spring-hadoop-samples/issues/comments/357884010/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}}}}