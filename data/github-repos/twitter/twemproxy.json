{"_default": {"1": {"unisqu": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/546", "title": "can implement xxhash as hash?", "body": "can implement xxhash as hash?", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/546/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stanisavs": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/543", "title": "Stats is empty", "body": "Hello!\r\nHow can I see stats?\r\n``src/nutcracker -d --mbuf-size=1024 --stats-interval=30000 --verbose=6 --output=/var/log/nginx/proxy.log --stats-port=22222``\r\nThen I use this connection on production for 1 minute (5k qps).\r\nLogs:\r\n```...\r\n...\r\n[2017-12-06 11:05:52.734] nc_connection.c:360 recv on sd 10 eof rb 567 sb 124\r\n[2017-12-06 11:05:52.734] nc_request.c:430 c 10 is done\r\n[2017-12-06 11:05:52.734] nc_core.c:237 close c 10 '127.0.0.1:20964' on event 00FF eof 1 done 1 rb 567 sb 124  \r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76105 done on c 8 req_time 0.226 msec type REQ_REDIS_ZREVRANGEBYSCORE narg 7 req_len 86 rsp_len 97 key0 'DB11' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76107 done on c 11 req_time 0.181 msec type REQ_REDIS_SETEX narg 4 req_len 79 rsp_len 5 key0 '456:51:d8437695e7a446210fc356b730d5e909:tmp' peer '127.0.0.1:20968' done 1 error 0\r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76111 done on c 8 req_time 0.140 msec type REQ_REDIS_HGET narg 3 req_len 100 rsp_len 13 key0 'tables:domain' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.734] nc_connection.c:360 recv on sd 11 eof rb 456 sb 159\r\n[2017-12-06 11:05:52.734] nc_request.c:430 c 11 is done\r\n[2017-12-06 11:05:52.734] nc_core.c:237 close c 11 '127.0.0.1:20968' on event 00FF eof 1 done 1 rb 456 sb 159  \r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76114 done on c 8 req_time 0.156 msec type REQ_REDIS_HINCRBY narg 4 req_len 90 rsp_len 7 key0 'imps:420155' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.735] nc_request.c:96 req 76116 done on c 8 req_time 0.151 msec type REQ_REDIS_HINCRBY narg 4 req_len 81 rsp_len 8 key0 'campaigns:ads:stats_day:420144' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.735] nc_request.c:96 req 76118 done on c 8 req_time 0.139 msec type REQ_REDIS_SETEX narg 4 req_len 79 rsp_len 5 key0 '459:51:489c90dd70cc6bf3292a1722ac0f46ee:tmp' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.735] nc_connection.c:360 recv on sd 8 eof rb 436 sb 130\r\n[2017-12-06 11:05:52.735] nc_request.c:430 c 8 is done\r\n....\r\n```\r\n\r\n\r\nStats (I checked it every 5 second):\r\n```\r\nsrc/nutcracker --describe-stats\r\nThis is nutcracker-0.4.1\r\n\r\npool stats:\r\n  client_eof          \"# eof on client connections\"\r\n  client_err          \"# errors on client connections\"\r\n  client_connections  \"# active client connections\"\r\n  server_ejects       \"# times backend server was ejected\"\r\n  forward_error       \"# times we encountered a forwarding error\"\r\n  fragments           \"# fragments created from a multi-vector request\"\r\n\r\nserver stats:\r\n  server_eof          \"# eof on server connections\"\r\n  server_err          \"# errors on server connections\"\r\n  server_timedout     \"# timeouts on server connections\"\r\n  server_connections  \"# active server connections\"\r\n  server_ejected_at   \"timestamp when server was ejected in usec since epoch\"\r\n  requests            \"# requests\"\r\n  request_bytes       \"total request bytes\"\r\n  responses           \"# responses\"\r\n  response_bytes      \"total response bytes\"\r\n  in_queue            \"# requests in incoming queue\"\r\n  in_queue_bytes      \"current request bytes in incoming queue\"\r\n  out_queue           \"# requests in outgoing queue\"\r\n  out_queue_bytes     \"current request bytes in outgoing queue\"\r\n```\r\n\r\n```\r\nps aux | grep nut\r\nroot      5985  1.0  0.0  18584  2660 ?        Sl   11:04   0:44 src/nutcracker -d --mbuf-size=1024 --stats-interval=30000 --verbose=6 --output=/var/log/nginx/proxy.log --stats-port=22222\r\n```\r\n\r\n```\r\nnetstat -tulpn | grep 22222\r\ntcp        0      0 0.0.0.0:22222           0.0.0.0:*               LISTEN      5985/nutcracker \r\n```\r\n\r\nBTW connection with redis took 0.15ms, connection with twemproxy took 0.03ms, but 4 pipes with redis was faster (1.0-1.15ms) than the same operations with twemproxy (1.0-1.5ms). I was hoping for more acceleration.\r\n\r\nAnd also how to understand the logs?", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/543/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "filippog": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/540", "title": "Prometheus stats support?", "body": "Hi,\r\nwould you be interested in having native Prometheus stats support? I'm not sure what would be the best way implementation-wise (a separate port?).", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/532", "title": "Double quotes in server name result in invalid json stats", "body": "Hi,\r\nwe're using nutcracker/twemproxy with server names with double quotes, though this results in invalid json because the double quotes are not escaped but should (`src/nc_stats.c` at `stats_begin_nesting` if I'm reading the code correctly)", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/532/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "praveenbalaji-blippar": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/539", "title": "Twemproxy (Redis) for large batch/small # of connections", "body": "I use Redis in our offline processing (large batch/small # of connections). I want to use twemproxy to partition data. I see two issues which I want to address:\r\n\r\n- I notice that the performance drops ~2x when I use twemproxy.\r\n- Of greater concern to me is that twemproxy seems to drop the connection intermittently. I tried various `--mbuf-size` ranging from the default (16K) to 16M.\r\n\r\nI looked through several Issues on github, but most use cases seem to be large # of connection/online use cases. I want to figure out the configuration to handle large batch/small # of connections instead. Suggestions are much appreciated.\r\n\r\n\r\nOn an 8-core machine, `top` shows that available memory is not a problem. However, twemproxy appears to consume >90% CPU. The Redis instances consume ~10% CPU. When I execute the same commands in one of the Redis instances, the Redis instance consumes >90% CPU.\r\n\r\nThe configuration is as follows:\r\n```\r\ntest:\r\n  listen: 0.0.0.0:6378\r\n  hash: fnv1a_64\r\n  distribution: ketama\r\n  auto_eject_hosts: true\r\n  redis: true\r\n  server_retry_timeout: 2000\r\n  server_failure_limit: 1\r\n  servers:\r\n   - 127.0.0.1:7378:1\r\n   - 127.0.0.1:7379:1\r\n```\r\n\r\nRequest payloads have one of the following characteristics:\r\n\r\n- pipelined request with `del`. Each pipelined request has 200,000 - 2,000,000 `del` commands. Each key is ~10 bytes.\r\n- `mget` batches of size 100,000 - 1,000,000. Each key and value are ~10 bytes.\r\n- pipelined request with `set` and `sadd`. Each pipelined request has 200,000 - 2,000,000 commands (set + sadd). Each key and value is ~10 bytes.\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/539/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "inter169": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/538", "title": "OOM on the mbuf freelist (100+GB)", "body": "the nutcracker process consumed 100+GB phisycal memory on my production box after a data migration from another redis to this one (nutcracker).\r\nand the gdb console showed below:\r\n```\r\n(gdb) p nfree_mbufq\r\n$1 = 6365614\r\n(gdb) p mbuf_chunk_size\r\n$1 = 16384\r\n```\r\n\r\nthe memory consumption was nfree_mbufq * mbuf_chunk_size = 101GB approx. \r\nI have read some code fixes (pr)s about the similar phenomenon, like:\r\nhttps://github.com/twitter/twemproxy/pull/461\r\nhttps://github.com/twitter/twemproxy/issues/203\r\n\r\nbut such fixes didn't set the limitation of the mbuf chunks, so the OOM was still here, \r\nI coded a fix, and the nutcracker can pass a command param ('-n ',  in my fix) to set the max number of mbuf chunks, once exceeded the limitation it can free the mbuf after processing one req immediately.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/538/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jhwillett": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/537", "title": "Twemproxy hangs parsing nested Array responses from Redis", "body": "Twemproxy hangs when processing a Redis Lua script via EVALwhich returns nested sub-arrays. \r\nThis is easy to reproduce.\r\n\r\nSetup:\r\n* On Mac OS 10.12.6.\r\n* Running redis-server 2.8.24 at localhost:7379 (installed via Homebrew).\r\n* Running nutcracker-0.4.1 at localhost:221221 which is configured with:\r\n```\r\ndevelopment:\r\n  redis:            true\r\n  auto_eject_hosts: false\r\n  listen:           127.0.0.1:22121\r\n  servers:\r\n    - 127.0.0.1:7379:1\r\n```\r\n\r\nDemo of direct connection to Redis being happy:\r\n```\r\n$ redis-cli -p 7379\r\n127.0.0.1:7379> EVAL 'return {1,2,3}' 1 x\r\n1) (integer) 1\r\n2) (integer) 2\r\n3) (integer) 3\r\n127.0.0.1:7379> EVAL 'return {1,{2},3}' 1 x\r\n1) (integer) 1\r\n2) 1) (integer) 2\r\n3) (integer) 3\r\n127.0.0.1:7379>\r\n```\r\n\r\nDemo of connection through Twemproxy being unhappy:\r\n```\r\n$ redis-cli -p 221221\r\n127.0.0.1:221221> EVAL 'return {1,2,3}' 1 x\r\n1) (integer) 1\r\n2) (integer) 2\r\n3) (integer) 3\r\n127.0.0.1:221221> EVAL 'return {1,{2},3}' 1 x\r\n^C\r\n```\r\nThe last command hangs as long as I was willing to wait (several minutes), until I cancel it.\r\n\r\nI see no messages in the twemproxy logs during this time.\r\n\r\nI notice that src/proto/nc_redis.c has special cases for nested multi bulk reply element from sscan/hscan/zscan.  Perhaps eval and evalsha are simply missing the special case treatment?\r\n\r\nI apologize for phrasing this as a bug report instead of as a pull request with a fix, but I am afraid it may be a long time before I can dig deeper.\r\n\r\nThank you for a wonderful tool.  It is a critical piece of our infrastructure at ProsperWorks and has made our lives much easier.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/537/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "BlueCatFlord": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/535", "title": " Whether the project is in maintenance", "body": "Whether the project is in maintenance", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/535/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jslusher": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/534", "title": "twemproxy only sees one of the memcached servers in the pool", "body": "It's my understanding that when a server in the twemproxy pool gets ejected, the other server in the pool should still be available for caching. It seems that when I take out *memcached-1 only*, the proxy itself becomes unavailable. If I take out memcached-2 from the pool, everything operates normally, except that there doesn't seem to be any indication in the logs that the server leaves or returns to the pool. \r\n\r\nI have tested that both memcached servers are available directly. If I put one or the other memcached sever by itself in the pool configuration, they're available using the proxy, but only memcached-1 is available if I have them both in the pool. I've tried ordering them differently and it doesn't seem to make a difference. A tcpdump only ever shows traffic to memcached-1 when they are both in the pool. When nutcracker is restarted, I only see arp traffic going to one of the two servers, but never both.\r\n\r\nTo reproduce:\r\n(nutcracker version 0.4.1 on centos 7)\r\n/etc/nutcracker/nutcracker.yml\r\n``` yaml\r\nbad_pool:\r\n  listen: 127.0.0.1:22122\r\n  hash: fnv1a_64\r\n  distribution: ketama\r\n  auto_eject_hosts: true\r\n  timeout: 400\r\n  server_retry_timeout: 30000\r\n  server_failure_limit: 3\r\n  servers:\r\n   - 10.10.10.33:11211:1 memcached-1\r\n   - 10.10.10.34:11211:1 memcached-2\r\n```\r\n\r\ntelnet 127.0.0.1 22122\r\n```\r\nTrying 127.0.0.1...\r\nConnected to 127.0.0.1.\r\nEscape character is '^]'.\r\nset testing 1 0 3\r\none\r\nSTORED\r\n```\r\n\r\nssh 10.10.10.33:\r\n```\r\nsudo systemctl stop memcached\r\n```\r\n\r\ntelnet console:\r\n```\r\nget testing\r\nSERVER_ERROR Connection refused\r\nConnection closed by foreign host.\r\n```\r\n\r\nnutcracker logs for sequence:\r\n```\r\n[2017-08-18 11:08:46.894] nc_core.c:43 max fds 1024 max client conns 989 max server conns 3\r\n[2017-08-18 11:08:46.894] nc_stats.c:851 m 4 listening on '0.0.0.0:22222'\r\n[2017-08-18 11:08:46.894] nc_proxy.c:217 p 6 listening on '127.0.0.1:22122' in memcache pool 0 'bad_pool' with 2 servers\r\n[2017-08-18 11:08:56.457] nc_proxy.c:377 accepted c 8 on p 6 from '127.0.0.1:41122'\r\n[2017-08-18 11:09:11.595] nc_request.c:96 req 1 done on c 8 req_time 1160.716 msec type REQ_MC_SET narg 2 req_len 24 rsp_len 8 key0 'testing' peer '127.0.0.1:41122' done 1 error 0\r\n[2017-08-18 11:14:00.115] nc_response.c:118 s 9 active 0 is done\r\n[2017-08-18 11:14:00.116] nc_core.c:237 close s 9 '10.50.20.35:11211' on event 00FF eof 1 done 1 rb 8 sb 24\r\n[2017-08-18 11:14:06.887] nc_core.c:237 close s 9 '10.50.20.35:11211' on event FFFFFF eof 0 done 0 rb 0 sb 0: Connection refused\r\n[2017-08-18 11:14:06.887] nc_request.c:96 req 4 done on c 8 req_time 0.597 msec type REQ_MC_GET narg 2 req_len 13 rsp_len 33 key0 'testing' peer '127.0.0.1:41122' done 1 error 0\r\n[2017-08-18 11:14:06.887] nc_core.c:237 close c 8 '127.0.0.1:41122' on event FF00 eof 0 done 0 rb 37 sb 41: Operation not permitted\r\n```", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/534/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Vibe6": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/531", "title": "Change to grow yr Power", "body": "Hello twitter I what to give you advice please change \"follow\" text on yr button type. Add or only show a + income with profile it will help you believe me.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/531/reactions", "total_count": 3, "+1": 0, "-1": 3, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mishtika": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/530", "title": "Twemproxy \" Connection refused \" on failure of one redis instance", "body": "I have configured twem proxy with 2 redis servers.\r\nWhen one of these redis server fails the twem proxy gives a error saying \" Connection refused\"\r\n\r\nMy twem conf \r\nbeta:\r\n  listen: 127.0.0.1:22122\r\n  hash: fnv1a_64\r\n  hash_tag: \"{}\"\r\n  distribution: ketama\r\n  auto_eject_hosts: true\r\n  timeout: 400\r\n  redis: true\r\n  servers:\r\n   - 127.0.0.1:6381:1\r\n   - 127.0.0.1:6379:1\r\n\r\ni am trying to get a key from redis. If i kill one instance of redis and then try to run the get command then it gives the following error:\r\n[2017-07-26 16:25:24.548] nc_core.c:237 close s 15 '127.0.0.1:6381' on event FFFFFF eof 0 done 0 rb 0 sb 0: Connection refused\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/530/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yuetianle": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/528", "title": "is it  support the docker redis service ", "body": "when use the docker redis service ,it don't work well.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rposky": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/526", "title": "Add additional timeout option to apply to time between server request and response", "body": "The current \"timeout clock starts ticking the instant the message is enqueued into the server [in queue]\" and not at the time that the message is actually sent to the server. While this may be appropriate for some use cases, I suggest that it is problematic for others that wish to react to server response time irrespective of server demand.\r\n\r\nI have observed that in the presence of many requests using the same key, and thus mapped to the same server, a denial of service scenario is possible due to timeout of requests that may not have even left the server in-queue. Server ejection naturally follows, despite that the proxied service is still running and responding normally. It just could not respond to all queued requests within the configured timeout, again some of which it may not have even technically began servicing.\r\n\r\nIt would be beneficial were there an additional timeout option that could be evaluated specifically against server response times and used to influence server ejections in lieu of server_timeout errors. In the meantime, I have found that adjusting the existing timeout option to a much larger value can help to mitigate this error scenario, though it unfortunately means that the service will be slower to detect actual server issues.\r\n\r\nThanks ahead of time for any consideration and/or suggestions.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/526/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "danielkraaijbax": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/525", "title": "Redis PING command is sometimes slow.", "body": "Hi There,\r\n\r\nWe have a setup of Twemproxy and 3 Redis masters with 3 Redis Slaves  and i have found an issue with the PING command that we were using.\r\nFirst of, We use the PING command to check if a Redis server is correctly connected. We found some issues with this in the past where we would connect to Twemproxy, ask for a key and that the server did not respond. We fixed that and is totally not related to this ticket.\r\nIn our codebase (PHP) there was however still a line where we would issue a PING command after each connect. \r\nWe were sometimes experiencing slowness on our pages. Somehow our connection wrapper managed to wait 200ms before continuing.\r\n\r\nI created a couple of test scripts to try and resolve this issue.\r\n```php\r\n<?php\r\n\r\n$start = (microtime(true) * 1000);\r\n$redis = new Redis();\r\n$redis->connect('<twemproxyIP>', 6379, 1);\r\n$redis->ping();\r\n$end = (microtime(true) * 1000);\r\n\r\necho $end-$start.PHP_EOL;\r\n```\r\nThis resulted in the following times. \r\n\r\n> for i in {1..20}; do php redis-ping.php; sleep 1; done\r\n202.45825195312\r\n0.625\r\n1.968994140625\r\n0.60693359375\r\n1.89501953125\r\n202.28198242188\r\n0.452880859375\r\n1.438720703125\r\n201.91772460938\r\n2.072021484375\r\n201.50170898438\r\n202.705078125\r\n0.783935546875\r\n2.90966796875\r\n1.487060546875\r\n0.831787109375\r\n0.52587890625\r\n0.593017578125\r\n0.634033203125\r\n1.114990234375\r\n\r\nI made some adjustments to the script.\r\n```php\r\n<?php\r\n\r\n$start = (microtime(true) * 1000);\r\n$redis = new Redis();\r\n$redis->connect('<twemproxyIP>', 6379, 1);\r\n$redis->get('FooBar');\r\n$end = (microtime(true) * 1000);\r\n\r\necho $end-$start.PHP_EOL;\r\n```\r\nThis resulted in the following times.\r\n> for i in {1..20}; do php redis.php; sleep 1; done\r\n1.2978515625\r\n1.47607421875\r\n2.22216796875\r\n2.47509765625\r\n2.26708984375\r\n1.2548828125\r\n1.25\r\n0.85986328125\r\n4.005859375\r\n3.97802734375\r\n0.876953125\r\n1.739990234375\r\n0.784912109375\r\n0.760986328125\r\n1.220947265625\r\n3.2080078125\r\n2.267822265625\r\n0.810791015625\r\n2.806884765625\r\n0.744140625\r\n\r\nSo it seems that the PING command is sometimes slow.\r\nI assume this could be an issue? \r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/525/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kigster": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/523", "title": "On newer SmartOS Twemproxy chooses epoll instead of event ports", "body": "Looks like SmartOS now [implements `epoll`](http://blog.shalman.org/exploring-epoll-on-smartos/), as well as their native event ports.\r\n\r\nBuild system prefers `epoll` when both are available, but it would be better if it chose native `event ports`. We are currently experiencing random sporadic lockups of twemproxy built with `epoll` support. Currently testing the event ports version.\r\n\r\nThanks!", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/523/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vishalsharma13": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/522", "title": "Redis rename command", "body": "hi,\r\n\r\nWhy twemproxy not used rename command? Is there any possibilty to use rename command through twemproxy.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/522/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/516", "title": "Twemproxy Server Timedout", "body": "Hi,\r\n\r\nCan you please help to identify the reason of error with redis server.\r\n\r\nTwemproxy logs :- \r\n\r\n[2017-02-09 12:06:24.188] nc_request.c:96 req 219840 done on c 27903 req_time 1000.594 msec type REQ_REDIS_HMGET narg 7 req_len 117 rsp_len 27 key0 'USER:SESSION:8c5ab24a-c0b0-4ccd-a377-02c0b1d728ed' peer '10.247.74.50:58653' done 1 error 1\r\n\r\nTwemproxy Configurations :- \r\n\r\nalpha:\r\n  auto_eject_hosts: false\r\n  client_connections: 40000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8511\r\n  preconnect: false\r\n  redis: true\r\n  server_connections: 10000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6382:1\r\n  timeout: 500\r\nbeta:\r\n  auto_eject_hosts: false\r\n  client_connections: 75000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8510\r\n  preconnect: false\r\n  redis: true\r\n  server_connections: 20000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6380:1 slave1\r\n  - 0.0.0.0:6381:1 slave2\r\n  timeout: 1000\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/516/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/512", "title": "Twemproxy : Server Connections not closed properly", "body": "Hi,\r\n\r\nI am using Redis with 3 servers configuration i.e one master(to write data) and two slaves(to read data) and using Twemproxy for load balancing.\r\n\r\nAnd using Jedis with connection pooling.\r\n\r\nNow i doing load testing but i am facing below errors\r\n\r\n1) Twemproxy Server connections are not properly closed on time and increased upto double or triple of  request(Like for 10k requests server connections opened upto 30k connections)\r\n\r\n2) Getting exception like unable to get resource from pool however my poolsize limit is not exceeded and after one command connection returns back to pool.\r\n\r\nServer Configurations :- \r\n\r\n1) 64GB RAM\r\n2) 16 core processor\r\n\r\nTwemproxy Configurations :- \r\n\r\nalpha:\r\n  auto_eject_hosts: false\r\n  client_connections: 40000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8511\r\n  preconnect: true\r\n  redis: true\r\n  server_connections: 10000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6380:1\r\n  timeout: 5000\r\nbeta:\r\n  auto_eject_hosts: true\r\n  client_connections: 75000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8510\r\n  preconnect: true\r\n  redis: true\r\n  server_connections: 50000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6382:1 slave1\r\n  - 0.0.0.0:6381:1 slave2\r\n  timeout: 5000\r\n\r\n\r\nJedis Pool Configuration :- \r\n\r\npublic class RedisReadManager {\r\n    private static final RedisReadManager instance = new RedisReadManager();\r\n    private static JedisPool pool;\r\n    private RedisReadManager() {}\r\n    public final static RedisReadManager getInstance() {\r\n        return instance;\r\n    }\r\n    public void connect(String host, int port) {\r\n        try {\r\n        \tif(pool==null || pool.isClosed()){\r\n\t\t\t\tJedisPoolConfig poolConfig = new JedisPoolConfig();\r\n\t\t\t\tpoolConfig.setMaxTotal(60000);\r\n\t\t\t\tpoolConfig.setTestOnBorrow(true);\r\n\t\t\t\tpoolConfig.setTestOnReturn(true);\r\n\t\t\t\tpoolConfig.setMaxIdle(3000);\r\n\t\t\t\tpoolConfig.setMinIdle(100);\r\n\t\t\t\tpoolConfig.setTestWhileIdle(true);\r\n\t\t\t\tpoolConfig.setNumTestsPerEvictionRun(10);\r\n\t\t\t\tpoolConfig.setTimeBetweenEvictionRunsMillis(10000);\r\n\t\t\t\tpool = new JedisPool(poolConfig, host, port);\r\n        \t}\r\n\t\t} catch (Exception e) {\r\n\t\t\te.printStackTrace();\r\n\t\t\tthrow e;\t\t\t\r\n\t\t}\r\n    }\r\n    public void releasePool() {\r\n        try {\r\n\t\t\tif(pool!=null&&!pool.isClosed()){\r\n\t\t\t\tpool.destroy();\r\n\t\t\t\t}\r\n\t\t} catch (Exception e) {\r\n\t\t\te.printStackTrace();\r\n\t\t\tthrow e;\r\n\t\t}\r\n    }\r\n    public Jedis getJedis() {\r\n        return pool.getResource();\r\n    }\r\n\r\n\tpublic void returnJedis(Jedis jedis) {\r\n    \ttry {\r\n\t\t\tif(jedis!=null&jedis.isConnected()){\r\n\t\t\t\tjedis.disconnect();\r\n\t\t\t\tjedis.close();\r\n\t\t\t}\r\n\t\t} catch (Exception e) {\r\n\t\t\te.printStackTrace();\r\n\t\t\tthrow e;\r\n\t\t}\r\n    }\r\n}\r\n\r\nNutcracker-web :- \r\n\r\n![nutcracker-web](https://cloud.githubusercontent.com/assets/22707039/22018052/ebaf5d92-dcd2-11e6-91c6-463af69e960a.jpg)\r\n\r\nPlease help to resolve the issue.\r\n \r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ArcticSnowman": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/521", "title": "Any recommendations For Using KeepaliveD with Twemproxy?", "body": "Anyone got recommendations For Using KeepaliveD with Twemproxy?\r\n\r\nUse a simple tcp check against the port?  Something more sophisticated with a check on the stats port?\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/521/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/518", "title": "Please support the server command 'COMMAND' as 'redis-cli' uses it for non-interactive commands", "body": "Please support the server command 'COMMAND' as 'redis-cli' uses it for non-interactive commands.\r\n\r\n`redis-cli GET <key>`  does not work for twemproxy as redis-cli issues a `COMMAND` command to check that you have a valid command to send. \r\n\r\nThis works in interactive mode...\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/518/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mcanonic": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/519", "title": "Support for BRPOP and BLPOP", "body": "Hi,\r\nI saw the notes/redis.md file and I'm wondering if the unsupported commands are somethin that you are working on it or not. Just to know if the (near?) future these commads like BRPOP e BLPOP will be supported.\r\nThanks\r\nM", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/519/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hjhart": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/514", "title": "Twemproxy cluster not accepting clients", "body": "On a machine running twemproxy, it is no longer accepting client connections. We're not totally sure why, at this point. The logs look interesting enough, ending with an `eof`. I think I'm just not understanding 1) how twemproxy would ever \"go down\" and 2) choose not to come back up.\r\n\r\nThe redis machines are responsive, because we have five other twemproxy machines with exactly the same configuration and they are still up. There is a process that runs JUST before this problem occurs, and it is hitting redis incredibly hard. We're just not sure conceptually _why_ twemproxy goes down, and what configuration we can change to make it stop. \r\n\r\nUnderstanding, of course, that we are hitting redis incredibly hard from a single process and we could choose _not_ to do that, but we want to first understand why twemproxy goes down, and then why it _stays_ down.\r\n\r\nAnother thing to note, is that we've recently updated from twemproxy 0.3.0 to 0.4.1, and this didn't appear to happen on 0.3.0.\r\n\r\nThe end of the logs looks like this: \r\n\r\n## Logs\r\n\r\n```\r\n> tail /var/log/twemproxy-daily-sale.log\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782534 done on c 30 req_time 27.163 msec type REQ_REDIS_EXPIRE narg 3 req_len 50 rsp_len 4 key0 'ds:20888845:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782535 done on c 30 req_time 27.181 msec type REQ_REDIS_DEL narg 2 req_len 34 rsp_len 4 key0 'ds:20889139:mp' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782536 done on c 30 req_time 27.200 msec type REQ_REDIS_RPUSH narg 12 req_len 177 rsp_len 5 key0 'ds:20889139:mp' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782539 done on c 30 req_time 27.209 msec type REQ_REDIS_EXPIRE narg 3 req_len 49 rsp_len 4 key0 'ds:20889139:mp' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782540 done on c 30 req_time 27.227 msec type REQ_REDIS_DEL narg 2 req_len 35 rsp_len 4 key0 'ds:20889503:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782541 done on c 30 req_time 27.246 msec type REQ_REDIS_RPUSH narg 3 req_len 51 rsp_len 4 key0 'ds:20889503:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782544 done on c 30 req_time 27.256 msec type REQ_REDIS_EXPIRE narg 3 req_len 50 rsp_len 4 key0 'ds:20889503:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:57.393] nc_connection.c:360 recv on sd 30 eof rb 830971428 sb 32948345\r\n[2017-01-27 05:44:57.393] nc_request.c:430 c 30 is done\r\n[2017-01-27 05:44:57.393] nc_core.c:237 close c 30 '10.100.17.85:56791' on event 00FF eof 1 done 1 rb 830971428 sb 32948345\r\n```\r\n\r\n## Configuration\r\n\r\nOur configuration on twemproxy looks like this:\r\n<details>\r\n  <summary>Click to expand configuration</summary>\r\n\r\n\r\n```\r\n> cat /etc/twemproxy/twemproxy-daily-sale.yml\r\ndaily-sale:\r\n  hash: fnv1_32\r\n  hash_tag: \"::\"\r\n  distribution: ketama\r\n  timeout: 1000\r\n  auto_eject_hosts: false\r\n  server_retry_timeout: 2000\r\n  server_failure_limit: 3\r\n  redis: true\r\n  listen: 10.100.18.248:22139\r\n  servers:\r\n  - 10.100.17.85:37001:1 daily-sale001\r\n  - 10.100.17.85:37002:1 daily-sale002\r\n  - 10.100.17.85:37003:1 daily-sale003\r\n  - 10.100.17.85:37004:1 daily-sale004\r\n  - 10.100.17.241:37005:1 daily-sale005\r\n  - 10.100.17.241:37006:1 daily-sale006\r\n  - 10.100.17.241:37007:1 daily-sale007\r\n  - 10.100.17.241:37008:1 daily-sale008\r\n  - 10.100.18.19:37009:1 daily-sale009\r\n  - 10.100.18.19:37010:1 daily-sale010\r\n  - 10.100.18.19:37011:1 daily-sale011\r\n  - 10.100.18.19:37012:1 daily-sale012\r\n  - 10.100.18.100:37013:1 daily-sale013\r\n  - 10.100.18.100:37014:1 daily-sale014\r\n  - 10.100.18.100:37015:1 daily-sale015\r\n  - 10.100.18.100:37016:1 daily-sale016\r\n```\r\n\r\n</details>\r\n\r\n## Connections\r\n\r\n`netstat -an` shows that there is a single `CLOSE_WAIT` connection stuck.\r\n\r\n```\r\n> netstat -an | grep 22139\r\n10.100.18.248.22139        *.*                0      0 1048576      0 LISTEN\r\n10.100.18.248.22139  10.100.104.187.23453 1049792      0 1049800      0 CLOSE_WAIT\r\n```\r\n\r\nAnd on the corresponding machine (10.100.104.187) there is a `SYN_SENT` connection stuck:\r\n\r\n```\r\n> netstat -an | grep 10.100.18.248\r\n10.100.104.187.28052 10.100.18.248.22136  1049792      0 1049800      0 ESTABLISHED\r\n10.100.104.187.38585 10.100.18.248.22139      0      0 1049740      0 SYN_SENT\r\n10.100.104.187.34939 10.100.18.248.22135  1049792      0 1049800      0 ESTABLISHED\r\n```\r\n\r\n## Statistics\r\n\r\nOur statistics do not indicate any servers are out of the hash ring...\r\n\r\n<details>\r\n  <summary>Click to expand statistics</summary>\r\n\r\n```\r\n> nc localhost 22227 | json\r\n{\r\n  \"service\": \"nutcracker\",\r\n  \"source\": \"twemproxy100.prod\",\r\n  \"version\": \"0.4.1\",\r\n  \"uptime\": 90618,\r\n  \"timestamp\": 1485566119,\r\n  \"total_connections\": 459796,\r\n  \"curr_connections\": 17,\r\n  \"daily-sale\": {\r\n    \"client_eof\": 3927,\r\n    \"client_err\": 455852,\r\n    \"client_connections\": 0,\r\n    \"server_ejects\": 0,\r\n    \"forward_error\": 0,\r\n    \"fragments\": 0,\r\n    \"daily-sale001\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 499925,\r\n      \"request_bytes\": 52043467,\r\n      \"responses\": 499925,\r\n      \"response_bytes\": 2109629,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale002\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 425855,\r\n      \"request_bytes\": 44208662,\r\n      \"responses\": 425855,\r\n      \"response_bytes\": 1791852,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale003\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 516402,\r\n      \"request_bytes\": 53579106,\r\n      \"responses\": 516402,\r\n      \"response_bytes\": 2170560,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale004\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 571640,\r\n      \"request_bytes\": 59475059,\r\n      \"responses\": 571640,\r\n      \"response_bytes\": 2405288,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale005\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 488641,\r\n      \"request_bytes\": 50937149,\r\n      \"responses\": 488641,\r\n      \"response_bytes\": 2060132,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale006\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 595062,\r\n      \"request_bytes\": 61596411,\r\n      \"responses\": 595062,\r\n      \"response_bytes\": 2514791,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale007\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 539622,\r\n      \"request_bytes\": 56245033,\r\n      \"responses\": 539622,\r\n      \"response_bytes\": 2274943,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale008\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 450732,\r\n      \"request_bytes\": 47025684,\r\n      \"responses\": 450732,\r\n      \"response_bytes\": 1898874,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale009\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 461595,\r\n      \"request_bytes\": 47897394,\r\n      \"responses\": 461595,\r\n      \"response_bytes\": 1938910,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale010\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 502702,\r\n      \"request_bytes\": 52229759,\r\n      \"responses\": 502702,\r\n      \"response_bytes\": 2124118,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale011\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 489467,\r\n      \"request_bytes\": 50950224,\r\n      \"responses\": 489467,\r\n      \"response_bytes\": 2060700,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale012\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 510860,\r\n      \"request_bytes\": 53141344,\r\n      \"responses\": 510860,\r\n      \"response_bytes\": 2145320,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale013\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 446362,\r\n      \"request_bytes\": 46440311,\r\n      \"responses\": 446362,\r\n      \"response_bytes\": 1872555,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale014\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 427993,\r\n      \"request_bytes\": 44477521,\r\n      \"responses\": 427993,\r\n      \"response_bytes\": 1803340,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale015\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 508857,\r\n      \"request_bytes\": 52884727,\r\n      \"responses\": 508857,\r\n      \"response_bytes\": 2141577,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale016\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 564176,\r\n      \"request_bytes\": 58592099,\r\n      \"responses\": 564176,\r\n      \"response_bytes\": 2376014,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n</details>\r\n\r\n## Connecting\r\n\r\nAnd similarly trying to establish a connection to twemproxy hangs indefinitely...\r\n\r\n```\r\n> telnet 10.100.18.248 22139\r\nTrying 10.100.18.248...\r\n```\r\n\r\nAny and all help is appreciated. Thank you!", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/514/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jennyfountain": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/513", "title": "Questions about twemproxy and performance", "body": "We are seeing a few issues that I was hoping someone could help me resolve or point me in the right direction.\r\n\r\n1.  During high loads, we are seeing a lot of backup in the out_queue_bytes.  On normal traffic loads, this is 0. \r\n\r\nExample (sometimes goes into 2k/3k range as well):\r\n    \"out_queue_bytes\": 33\r\n    \"out_queue_bytes\": 91\r\n    \"out_queue_bytes\": 29\r\n    \"out_queue_bytes\": 29\r\n    \"out_queue_bytes\": 174\r\n\r\nIn addition, it shows that our time spent in memcache goes up from 400 ms to 1000-2000 ms. This seriously affects our application. \r\n\r\n2.  auto eject also seems to not work as expected. Server goes down and our app freaks out - saying it cannot access a memcache server.\r\n\r\nhere is an example of a config:\r\n\r\nweb:\r\n  listen: /var/run/nutcracker/web.sock 0777\r\n  auto_eject_hosts: true\r\n  distribution: ketama\r\n  hash: one_at_a_time\r\n  backlog: 65536\r\n  server_connections: 16\r\n  server_failure_limit: 3\r\n  server_retry_timeout: 30000\r\n  servers:\r\n   - 1.2.3.4:11211:1\r\n   - 1.2.3.5:11211:1\r\n   - 1.2.3.6:11211:1\r\n  timeout: 2000\r\n\r\nsomaxconn = 128\r\n\r\nWhat we tried and didn't help.\r\n1.  mbuf to 512\r\n2.  server connection from 1 to 200\r\n\r\nThank you for any guidance on this problem.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/513/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/482", "title": "Did tcpkeepalive make it into the current codebase?", "body": "I am see a couple references to it in issues and thought I saw a merge but do not see documentation.\n\nThanks\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/482/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/450", "title": "Send all additions/sets to all memcached servers in pool", "body": "What setting do I need to have nutcracker send all sets/additions to every memcached server in the pool?\n\nWe have four servers that need to be exactly identical in data.\n\nThanks!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/450/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "y123456yz": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/509", "title": "in function redis_fragment_argx, if msg->fragment() return NC_ERROR, behind req_forward_error , it should return.", "body": "status = msg->fragment(msg, pool->ncontinuum, &frag_msgq);\r\n    if (status != NC_OK) { //redis_fragment \r\n        if (!msg->noreply) {\r\n            conn->enqueue_outq(ctx, conn, msg);\r\n        }\r\n        req_forward_error(ctx, conn, msg);\r\n    }\r\n........\r\nstatus = req_make_reply(ctx, conn, msg); \r\n    if (status != NC_OK) {\r\n        if (!msg->noreply) {\r\n            conn->enqueue_outq(ctx, conn, msg);\r\n        }\r\n        req_forward_error(ctx, conn, msg);\r\n    }\r\n..........\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nI think it should be following:\r\n\r\nstatus = msg->fragment(msg, pool->ncontinuum, &frag_msgq);\r\n    if (status != NC_OK) { //redis_fragment \r\n        if (!msg->noreply) {\r\n            conn->enqueue_outq(ctx, conn, msg);\r\n        }\r\n        req_forward_error(ctx, conn, msg);\r\n        return;\r\n    }\r\n........\r\nstatus = req_make_reply(ctx, conn, msg); \r\n    if (status != NC_OK) {\r\n        if (!msg->noreply) {\r\n            conn->enqueue_outq(ctx, conn, msg);\r\n        }\r\n        req_forward_error(ctx, conn, msg);\r\n        return\r\n    }\r\n..........\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/509/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "haihq88": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/507", "title": "how twemproxy write to memcache server ", "body": "I have following setting for PHP session \r\n```\r\nsession_cached:\r\n  listen: 127.0.0.1:22123\r\n  hash: fnv1a_64\r\n  distribution: ketama\r\n  timeout: 400\r\n  backlog: 1024\r\n  preconnect: true\r\n  auto_eject_hosts: true\r\n  server_retry_timeout: 2000\r\n  server_failure_limit: 3\r\n  servers:\r\n    - 10.16.223.249:11211:1 b1\r\n    - 10.16.223.254:11211:1 b2\r\n    - 10.16.223.221:11211:1 b3\r\n    - 10.16.223.204:11211:1 b4\r\n```\r\nWhen session created, how does it write to each memcache server? \r\nAs i know, twemproxy is consistent hashing. Can we define number of memcache server will be written? for example, memcache has option memcache.session_redundancy=N\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/507/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "carlwgeorge": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/505", "title": "minimum version of autoconf", "body": "Your configure.ac file specifies a [minimum autoconf version of 2.64](https://github.com/twitter/twemproxy/blob/v0.4.1/configure.ac#L8).  Your RPM spec file [reduces that to 2.63](https://github.com/twitter/twemproxy/blob/v0.4.1/scripts/nutcracker.spec#L25).  If autoconf 2.63 is acceptable for building nutcracker, then why not just correct the configure.ac directly?", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/505/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/504", "title": "allow building against system yaml", "body": "Nutcracker builds against a [bundled copy of yaml](https://github.com/twitter/twemproxy/blob/v0.4.1/contrib/yaml-0.1.4.tar.gz).  Linux distributions typically have guidelines recommending or requiring that packaged applications use system copies of libraries, not bundled ones.\r\n\r\nhttps://fedoraproject.org/wiki/Bundled_Libraries\r\nhttps://www.debian.org/doc/debian-policy/ch-source.html#s-embeddedfiles\r\nhttps://en.opensuse.org/openSUSE:Factory_dos_and_donts#..._include_bundled_libraries\r\n\r\nIs it possible to make building against the system copy of yaml as simple as a configure flag or an environment variable?  For example, in Fedora I can install libyaml-devel, which gives me the file `/usr/include/yaml.h` that I would like to compile nutcracker with.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/504/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Moln": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/503", "title": "Can add 'INFO' command or mock?", "body": "Add `INFO` command .", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/503/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "porunov": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/501", "title": "Can twemproxy load balance read requests?", "body": "Hello,\n\nI want to use redis with master and slaves. I.e. several shards. Each shard has one master and several slaves.\n\nIs it possible to configure twemproxy to use masters to write and slaves to read? Or all requests will go to the master?\n\nSincerely\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/501/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/456", "title": "Is twemproxy unscalable?", "body": "Hello,\n\nWill we lose all data in all nodes if we will lose only one master node?\nIf I've understood correctly, twemproxy uses consistent hashing. So, if we increase or decrease size of our cluster then all keys (or most of them) will be placed on incorrect nodes. Twemproxy can't automatically migrate/rebalance the cluster, so we will lose a lot of data if we just add or remove several nodes.\nHave I understood it correctly?\nAre there any tools which can migrate/rebalance automatically our data when we change size of the cluster?\n\nBest Regards, Alexandr\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/456/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kerrick-lyft": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/500", "title": "Redis mget with no arguments blocks instead of returning error", "body": "When you run `mget` with no arguments against a normal Redis instance, it returns an error:\n\n```\n127.0.0.1:6379> mget\n(error) ERR wrong number of arguments for 'mget' command\n127.0.0.1:6379> \n```\n\nWhen you run mget without arguments against Twemproxy, however, it blocks forever:\n\n```\nredis /var/run/nutcracker/my_redis_cluster.sock> mget\n[... control is never returned to the command prompt ...]\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/500/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "NeetiMittal": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/499", "title": "Read / Write Split", "body": "Hi,\nI am using twemproxy for data sharding between multiple master nodes. I have 2 master nodes and 2 slave connected to each. I want to use different ports for read(8801) and write(8802). \nBelow are the read and and write configuration for twemproxy:\n\nread:\nlisten: 0.0.0.0:8801\nhash: crc32a\ndistribution: ketama\nauto_eject_hosts: true\nhash_tag: \"{}\"\nredis: true\nserver_retry_timeout: 2000\nserver_failure_limit: 1\nservers:\n- 127.0.01:8504:1 slave1\n- 127.0.01:8502:1 slave2\n- 127.0.01:8602:1 slave3\n- 127.0.01:8603:1 slave4\n\nwrite:\nlisten: 0.0.0.0:8802\nhash: crc32a\nhash_tag: \"{}\"\ndistribution: ketama\nauto_eject_hosts: false\ntimeout: 400\nredis: true\nservers:\n127.0.01:8503:1 master1\n127.0.01:8601:1 master2\n\nslave1 and slave2 are slaves of master1\nslave3 and slave4 are slaves of master2\nwhen i connected to 8802 for writing,  data is distributed between master1 and master2.\nHowever, when i connected to 8801 for getting the same data, it does not get retrieved.\nSo, can anyone please help me in getting the data from the slave..(multiple slaves per master) .\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/499/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/498", "title": "Problem while connecting with multiple slave nodes", "body": "Hi,\n\nI am using twemproxy for data sharding between multiple master nodes. I have 2 master nodes and 2 slave connected to each. I want to use different ports for read and write.\nI am getting problem while i trying to read request from multiple slave nodes connected to single master\nbelow is my configuration:- \n\nalpha:\n  listen: 0.0.0.0:8801\n  hash: crc32a\n  distribution: ketama\n  auto_eject_hosts: true\n  hash_tag: \"{}\"\n  redis: true\n  server_retry_timeout: 2000\n  server_failure_limit: 1\n  servers:\n    - 127.0.01:8504:1 slave1\n    - 127.0.01:8502:1 slave2\n    - 127.0.01:8602:1 slave3\n    - 127.0.01:8603:1 slave4\n\nbeta:\n  listen: 0.0.0.0:8802\n  hash: crc32a\n  hash_tag: \"{}\"\n  distribution: ketama\n  auto_eject_hosts: false\n  timeout: 400\n  redis: true\n  servers:\n- 127.0.01:8503:1 master1\n- 127.0.01:8601:1 master2\n\nslave1 and slave2 are slaves of master1\nslave3 and slave4 are slaves of master2\n\nwhen i connected to 8802 data is distributed between master1 and master2 however when i connected to 8801 for getting data sometimes it will respond with nil as data is written on another master node.\nis there any possible solution for load balancing between multiple slaves.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/498/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "freedom1989": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/495", "title": "PFCOUNT does not work for multiple keys", "body": "I'm using version 0.4.1.\n`PFCOUNT` is working for one key, but not working for multiple keys:\n\n``` bash\npfcount key1\n> pfcount {user1}-hll1\n(integer) 3\n> pfcount {user1}-hll2\n(integer) 2\n> pfcount {user1}-hll1 {user1}-hll2\nError: Server closed the connection\n```\n\nis multiple keys for `PFCOUNT` not supported? or do I need to do some extra configuration to enable it?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/495/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Somebi": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/491", "title": "NOAUTH Authentication required", "body": "Using Ubuntu 16.04.1 LTS, nutcracker version 0.4.1 \n\n`nc_redis.c:2936 SELECT 10 failed on redis-pool | my_ip_here:6379: NOAUTH Authentication required.`\n\n`nutcracker -d -c /var/local/nutcracker.yml -a 0.0.0.0 -s 20100 -v 11 -o /var/log/nutcracker.log`\n\n```\nredis-pool:\n  listen: 127.0.0.1:20001\n  distribution: random\n  redis: true\n  servers:\n   - my_ip_here:6379:1\n  redis_db: 10\n  redis_auth: my_password_here\n```\n\nWith preconnect: true throwing right away.\n\nSelect command is not supported, so i fearing that it's not selecting 10 redis_db and using 0 db as default one.\n\nCould you please look into it?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/491/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Aditya-Chowdhry": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/490", "title": "Operation timeout on 2000 concurrent connections", "body": "Why am I getting operation timeout on 2000 concurrent get requests ?\nyml file config:\n\n```\nbeta:\n  listen: 127.0.0.1:22122\n  hash: fnv1a_64\n  hash_tag: \"{}\"\n  distribution: ketama\n  auto_eject_hosts: false\n  timeout: 500\n  redis: true\n  servers:\n   - 127.0.0.1:6380:1 server1\n   - 127.0.0.1:6380:1 server2\n   - 127.0.0.1:6381:1 server3\n   - 127.0.0.1:6382:1 server4\n   - 127.0.0.1:6383:1 server5\n   - 127.0.0.1:6384:1 server6\n```\n\nThe request time is increasing with the concurrent requests. What should be the ideal timeout value ?\nDecreasing mbuff value is not helping either.\nSome logs:\n\n```\nparsed command 'get'\n[2016-08-25 16:39:40.192] nc_redis.c:1661 parsed req 2007 res 0 type 42 state 0 rpos 36 of 36\n00000000  2a 32 0d 0a 24 33 0d 0a  67 65 74 0d 0a 24 31 36   |*2..$3..get..$16|\n00000010  0d 0a 6d 69 67 72 61 74  65 5f 74 65 73 74 31 39   |..migrate_test19|\n00000020  39 39 0d 0a                                        |99..|\n[2016-08-25 16:39:40.192] nc_server.c:706 key 'migrate_test1999' on dist 0 maps to server '127.0.0.1:6380:1'\n[2016-08-25 16:39:40.192] nc_message.c:172 insert msg 2007 into tmo rbt with expiry of 500 msec\n[2016-08-25 16:39:40.192] nc_request.c:611 forward from c 7 to s 13 req 2007 len 36 type 42 with key 'migrate_test1999'\n[2016-08-25 16:39:40.192] nc_core.c:300 req 5 on s 8 timedout\n[2016-08-25 16:39:40.192] nc_message.c:190 delete msg 5 from tmo rbt\n[2016-08-25 16:39:40.192] nc_core.c:237 close s 8 '127.0.0.1:6381' on event 0000 eof 0 done 0 rb 0 sb 0: Operation timed out\n[2016-08-25 16:39:40.192] nc_server.c:378 close s 8 swallow req 5 len 32 type 0\n[2016-08-25 16:39:40.192] nc_server.c:399 close s 8 schedule error for req 3 len 33 type 42 from c 7: Operation timed out\n[2016-08-25 16:39:40.192] nc_server.c:399 close s 8 schedule error for req 4 len 33 type 42 from c 7: Operation timed out\n[2016-08-25 16:39:40.192] nc_server.c:399 close s 8 schedule error for req 6 len 33 type 42 from c 7: Operation timed out\n[2016-08-25 16:39:40.193] nc_server.c:399 close s 8 schedule error for req 7 len 33 type 42 from c 7: Operation timed out\n```\n\nCode snippet:\n\n```\nfor ( var i = 1; i<2000; ++i ) {\n    var key = \"migrate_test\" +  String(i);\n    var value = \"val \" + i;\n    a = client.get(key, function(err, reply) {\n             if (reply == null){\n                count = count + 1\n                console.log(\"----------*****---------\",count);\n             }else{\n                console.log(reply);\n             }\n\n        });\n}\n\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/490/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kidambisrinivas": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/488", "title": "Partition keys by key-type", "body": "Q1: Is it possible to partition redis keys by key type and say:\nhash: To go to redis-backend1 at (host1,port1)\nall-other-types: Go to redis-backend2 at (host2,port2)\n\nQ2: Is ssdb supported as a backend?\nAm planning to use [ssdb](http://ssdb.io/) for storing huge memory intensive hashes (with >10MM keys). Do let me know if ssdb backend is supported by twemproxy. Ssdb supports redis protocol. One minor difference in commands is `hsize` instead of `hlen`. Rest are the same.\n\nThanks in advance!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stralytic": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/485", "title": "ubuntu package doesn't run under systemd", "body": "the systemd service file in the ubuntu packages is incorrect. I had to modify it to below to get it to work on xenial.\n- The override file needs a - before its filename to make it optional.\n- The ExecStart line was looking for the binary in /usr/bin instead of /usr/sbin\n\n```\n[Unit]\nDescription=nutcracker/twemproxy\n\n[Service]\nEnvironmentFile=/etc/default/twemproxy\nEnvironmentFile=-/etc/default/twemproxy.override\n\nExecStart=/usr/sbin/nutcracker $DAEMON_ARGS\n\n[Install]\nWantedBy=multi-user.target\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/485/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jgarlow": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/481", "title": "Core dump in memcache_fragment code", "body": "We have had two coredumps in our production environment since May and I'm posting information here to see if anyone else has seen a crash like these.   I've looked through other open and closed core dump issues and don't see any references to \"memcache_fragment\" in the backtrace so I think (but am in no way sure) that this is something different.  We also have not be able to recreate in our staging environment so I am really unclear what the cause is.  \n\nStill - if you have seen something like this then let me know and make we can work together on a cause.\n\nHere are the core dumps:\n\nMay 24th\nroot@prod-web-18:~# cat /var/log/nutcracker/nutcracker.log\n/usr/sbin/nutcracker(nc_stacktrace_fd+0x2a)[0x40f80a]\n/usr/sbin/nutcracker(signal_handler+0x49)[0x40d539]\n/lib/x86_64-linux-gnu/libpthread.so.0(+0xf8d0)[0x7fab8195c8d0]\n/usr/sbin/nutcracker(memcache_fragment+0x109)[0x413329]\n/usr/sbin/nutcracker(req_recv_done+0xc0)[0x409630]\n/usr/sbin/nutcracker(msg_recv+0x28e)[0x408b7e]\n/usr/sbin/nutcracker(core_core+0x81)[0x405851]\n/usr/sbin/nutcracker(event_wait+0xe7)[0x419097]\n/usr/sbin/nutcracker(core_loop+0x19)[0x405cc9]\n/usr/sbin/nutcracker(main+0x5e0)[0x405270]\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf5)[0x7fab812c4b45]\n/usr/sbin/nutcracker[0x405638]\n[.......................] signal 11 (SIGSEGV) received, core dumping\nJuly 11\n<didn't grab the very start> nc_response.c:118 s 7 active 0 is done\n/usr/sbin/nutcracker(nc_stacktrace_fd+0x2a)[0x40f80a]\n/usr/sbin/nutcracker(signal_handler+0x49)[0x40d539]\n/lib/x86_64-linux-gnu/libpthread.so.0(+0xf8d0)[0x7f28ea92f8d0]\n/usr/sbin/nutcracker(mbuf_insert+0xc)[0x409e3c]\n/usr/sbin/nutcracker(msg_ensure_mbuf+0x61)[0x4086e1]\n/usr/sbin/nutcracker(memcache_fragment+0x11c)[0x41333c]\n/usr/sbin/nutcracker(req_recv_done+0xc0)[0x409630]\n/usr/sbin/nutcracker(msg_recv+0x28e)[0x408b7e]\n/usr/sbin/nutcracker(core_core+0x81)[0x405851]\n/usr/sbin/nutcracker(event_wait+0xe7)[0x419097]\n/usr/sbin/nutcracker(core_loop+0x19)[0x405cc9]\n/usr/sbin/nutcracker(main+0x5e0)[0x405270]\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf5)[0x7f28ea297b45]\n/usr/sbin/nutcracker[0x405638]\n[.......................] signal 11 (SIGSEGV) received, core dumping\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/481/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "okapies": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/479", "title": "Is it actively maintained?", "body": "Someone mentioned this great software, but I'm concerned if it is still maintained by authors or not. Does anyone know the roadmap of twemproxy?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/479/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shanicky": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/477", "title": "client_connections doesn't work?", "body": "Hi, any\n\nI'm using twemproxy in v0.4.1\n\n```\n# nutcracker -V\nThis is nutcracker-0.4.1\n```\n\nAnd I'm using this config\n\n``` yaml\nproxy01:\n  listen: 0.0.0.0:8001\n  auto_eject_hosts: false\n  hash: fnv1a_64\n  distribution: modula\n  redis: true\n  preconnect: true\n  timeout: 300\n  client_connections: 1\n  servers:\n  - localhost:7001:1 redis7001\n  - localhost:7002:1 redis7002\n```\n\nI've set `client_connections` to 1, but when I run redis-benchmark with `-c` flag to start multi clients like this \n`redis-benchmark -t get -c 5 -p 8001 -n 10000000`\n\nIt didn't get any errors. The result of status port is like this \n\n```\n\n{\n   \"source\" : \"twemproxy01\",\n   \"curr_connections\" : 8,\n   \"version\" : \"0.4.1\",\n   \"total_connections\" : 48,\n   \"service\" : \"nutcracker\",\n   \"proxy01\" : {\n      \"redis7001\" : {\n         \"request_bytes\" : 0,\n         \"server_timedout\" : 0,\n         \"requests\" : 0,\n         \"server_ejected_at\" : 0,\n         \"out_queue_bytes\" : 0,\n         \"in_queue_bytes\" : 0,\n         \"out_queue\" : 0,\n         \"in_queue\" : 0,\n         \"response_bytes\" : 0,\n         \"server_err\" : 0,\n         \"responses\" : 0,\n         \"server_eof\" : 0,\n         \"server_connections\" : 1\n      },\n      \"client_eof\" : 35,\n      \"forward_error\" : 0,\n      \"fragments\" : 0,\n      \"redis7002\" : {\n         \"request_bytes\" : 23035068,\n         \"server_timedout\" : 0,\n         \"requests\" : 639863,\n         \"server_ejected_at\" : 0,\n         \"out_queue_bytes\" : 72,\n         \"in_queue_bytes\" : 0,\n         \"out_queue\" : 2,\n         \"in_queue\" : 0,\n         \"response_bytes\" : 3199305,\n         \"server_err\" : 0,\n         \"responses\" : 639861,\n         \"server_eof\" : 0,\n         \"server_connections\" : 1\n      },\n      \"client_err\" : 5,\n      \"client_connections\" : 5,\n      \"server_ejects\" : 0\n   },\n   \"timestamp\" : 1467099733,\n   \"uptime\" : 126\n}\n\n```\n\nIt looks like that `client_connections` does not work.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/477/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thenewguy": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/475", "title": "python-memcached warning is now out of date", "body": "I have been reading about twemproxy for a python environment.  I noticed your warning about [python-memcached](https://github.com/twitter/twemproxy/blob/master/notes/recommendation.md#twemproxy-and-python-memcached) and investigated.  It looks like this is now fixed per https://github.com/linsomniac/python-memcached/commit/81cd83d713738f4d707c404d89f4544d89895943\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/475/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bymaximus": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/472", "title": "key remap not working", "body": "I'm testing with 2 servers\n\n```\nroot:\n  listen: 192.168.0.3:16378\n  hash: fnv1a_64\n  distribution: ketama\n  preconnect: true\n  timeout: 5000\n  auto_eject_hosts: true\n  server_retry_timeout: 30000\n  server_failure_limit: 2\n  redis: true\n  servers:\n   - 127.0.0.1:6379:1 server1\n   - xxxx.com:6379:1 server2\n```\n\nI add key 'aaa' with some value and then I check with redis-cli which one of both servers have the key 'aaa' and then I stop the target server. At twemproxy I try get the key 'aaa' which return connection dropped and then in the next get key request returns null, is ok. \n\nThen I set the key 'aaa' again with some other value and then I start the other server, twemproxy shows the key 'aaa' new value until it reconnects to the server I stopped and then start showing the key 'aaa' first value. It should not remap this key 'aaa' to the last server I set the key value?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/472/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sophnim": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/471", "title": "Twemproxy Hash-algorythm change on different machine?", "body": "Hello.\n\nI am using twemproxy in production service, so far there are no problem but recently I encountered some weird issue.\n\nMy production service have 4 redis instance and combined with twemproxy. I copied these 4 redis instance to my test machine and construct twemproxy for test environment. When I try to get data from test twemproxy, It says there are no such a data. \n\nbecause of some reason, hash has been changed so twemproxy could not find data.. but why? same data, same configuration. Only machine changed.  Indeed redis has data, but twemproxy could not find that data.\n\nI can find out what's problem. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xingkaihu": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/470", "title": "support redis3.0 ?", "body": "", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/470/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Blasterdick": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/469", "title": "Can I use twemproxy for balancing by redis queue size?", "body": "Can you please tell me if I can use twemproxy as a load balancer for a pool of redis instances, according to size of redis queue (amount of keys per instance), so twemproxy be able to turn off from the upstream one of the redis instances, if it reaches a preconfigured maximum amount of keys in it's database? \n\nIf so, how can I do that (I'm very new to redis and stuff, so I just possibly don't get this from documentation)?\n\nThanks in advance.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/469/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "JeffXue": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/468", "title": "why nutcracker occput port 26379. conflict with redis sentinel", "body": "![image](https://cloud.githubusercontent.com/assets/3280855/14780067/a53a6db2-0b0d-11e6-98ba-6678f44a9e5c.png)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/468/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/467", "title": " 0.4.1 release support USR1 signal?", "body": "I am using 0.4.1 release version. But when i change my config file and send USR1 signal, it did't wok?\nIf i kill the process and restart it ,if will be fine. That why?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/467/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rudy2015": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/466", "title": "does twemproxy supports pub\\sub featrues in redis", "body": "Thank you very much\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/466/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "deep011": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/465", "title": "We support multi-theads for twemproxy", "body": "We support multi-theads for twemproxy. See the bellow:\nhttps://github.com/vipshop/twemproxies\n\nWe run a simple test for twemproxies\n\nUsed 20 threads, qps can be 550,000 per second.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/465/reactions", "total_count": 5, "+1": 5, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/430", "title": "When \"server_connections > server_failure_limit\"", "body": "We set server_connections bigger than server_failure_limit.\nSometime the connections to mc/redis are not stable, suddenly mc/redis close some connections that  more then server_failure_limit but less than server_connections, this mc/redis will be eject!\n@manjuraj, maybe something is wrong?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/430/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/409", "title": "addition features", "body": "Hi, dear @manjuraj @idning \n\nWe add some new features for twemproxy, and running very well.\n\nI want you help me review the features, and check whether some feature i can pull request or not?\n\nwiki: https://github.com/vipshop/twemproxy-vip/wiki/twemproxy-maintained-and-used-at-vipshop\n\ngithub: https://github.com/vipshop/twemproxy-vip\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/409/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/329", "title": "Twemproxy always has 'ESTABLISHED' tcp connection that In fact didn't exist.", "body": "When i execute \"netstat\" command on the twemproxy machine, and find many 'ESTABLISHED' tcp connections, some of them actually can not be found on the client machine.That means, the client tcp connections had been closed, but some of the twemproxy tcp connections still alive and state is 'ESTABLISHED'.\nHow can i deal with this problem?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/329/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/558e0d40ad79f423c4784565648e6c83cf035777", "message": "fix a memory leak bug for mset command"}], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/462", "title": "Fix a bug for msg->mlen", "body": "Function msg_append() already added the dst->mlen.\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/390", "title": "Update recommendation.md", "body": "add a recommendation for tcpkeepalive option.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/10512396", "body": " I think it would be better to support \u201cdelete key 0\\r\\n\u201d command, because memcached support this command. What is your opinion?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10512396/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "rohitjoshi": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/464", "title": "Question: Writing to multiple masters", "body": "Is it possible to configure Twemproxy to write to multiple masters instead of sharding? Ideally pipeline multiple requests and write to two masters.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/464/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "charsyam": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/459", "title": "[Unofficial] fix bugs and geo commands support version.", "body": "I just merge some patches that I think clear.\nfor merging some patches easily.\n\nso If you want geo commands and some bug fix version of twemproxy, try this.\nhttps://github.com/charsyam/twemproxy/tree/tags/unofficial-0.4.1-cs1\n\nthis version just merged below issues.\n1] Fix Bug: redis_auth don't work with redis_db\nhttps://github.com/twitter/twemproxy/pull/458\n2] Fix parsing bug when error body contains no spaces\nhttps://github.com/twitter/twemproxy/pull/423\n3] support nested multibulk reply\nhttps://github.com/twitter/twemproxy/pull/395\n- additional: geo commands support with this\n  4] allow exists can variodic\n  https://github.com/twitter/twemproxy/pull/388\n\nIf there are some bugs. I will fix it :)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/459/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/20a53e7957aa392266dfaea9ecfd720394a5fd25", "message": "redis_auth"}], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/458", "title": "Fix Bug: redis_auth don't work with redis_db", "body": "currently, twemproxy ignore \"redis_db\" when \"redis_auth\" is set.\nselect command is set as noforward because of auth.\n\nthis patch fix this.\nand move add_auth to post_connected.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/428", "title": "rebase heartbeat branch ", "body": "It had a bug when redis is loading.\nbut after patch of https://github.com/twitter/twemproxy/commit/ef453130e321974b332dd99d585ae7285eee4b5d\n(handle loading state as error)\nI think it fixed.\n\nIn my tests. this works fine :)\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/423", "title": "Fix parsing bug when error body contains no spaces", "body": "This is only copy for https://github.com/twitter/twemproxy/pull/406 (by @tom-dalton-fanduel )\nfor testing. I just PR again.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/395", "title": "support nested multibulk reply", "body": "@manjuraj This is part of https://github.com/twitter/twemproxy/pull/393\n\nfirst. I divided it that is only supporting nested multi bulk reply.\n\nafter merging this :)\n\nI will PR second of them :)\n\nThanks for your reivew :)\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/393", "title": "support geo commands and nested multibulk reply.", "body": "1. support GEO Commands\n   -> geoadd, geohash, geodist, georadius, georadiusbymember\n2. support nested multibulk reply\n   -> geo commands return nested multibulk reply\n\ncurrently we only support this type of multibulk\n\n```\n                 * - mulit-bulk\n                 *    - cursor\n                 *    - mulit-bulk\n                 *       - val1\n                 *       - val2\n                 *       - val3\n```\n\nbut georadius and georadiusbymember returns maximum 3 depth multibulk. so this patch supports this kind of multibulk also.\n\n```\n                 * - mulit-bulk\n                 *    - multi-bulk\n                 *       - val1\n                 *       - val2\n                 *       - val3\n                 *    - mulit-bulk\n                 *       - val1\n                 *       - val2\n                 *       - val3\n                 *\n                 * - mulit-bulk\n                 *    - multi-bulk\n                 *       - val1\n                 *       - multi-bulk\n                 *          - val2\n                 *          - val3\n                 *    - mulit-bulk\n                 *       - val1\n                 *       - multi-bulk\n                 *          - val2\n                 *          - val3\n```\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/388", "title": "allow exists can variodic", "body": "today, redis changed exists command can be variodic\n\n```\nrequest> exists ab a c\nresponse> :3\n```\n\nbut this feature just is in unstable.\n@manjuraj Does twemproxy prefer to follow unstable branch of redis ? or stable branch?\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/379", "title": "remove some compile warninig in clang(on mac)", "body": "just fix some compile warning in clang(on mac)\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/178", "title": "Fix bug: crashing when there is no lived server in heartbeat branch", "body": "I found one important bug. and fixed it. Thank you.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/29", "title": "Robust hash ring failure retry mechanism", "body": "There are 2 step to check connection.\n\nstep 1] trying to connect in server_failure\n\n  If it succeed,  going to step 2\n\nstep 2] trying to send heartbeat command to fail server\n\n If it succeed, it will update hash ring.\n\nand it uses array just to connect failed servers to prevent infinity loop for connecting\n\nand in heartbeat stage. It uses msg_two_insert for timeout\n\nbut, if there is no timeout setting in conf, It will just wait for response like other message.\n", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10983176", "body": "@caniszczyk  thank you for your response\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10983176/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11058378", "body": "@manjuraj I have a question. if server->port is 11211, then why don't you attach port number in hash string? is there special issue?\n\n``` c\n           if (server->port == KETAMA_DEFAULT_PORT) {\n                hostlen = snprintf(host, KETAMA_MAX_HOSTLEN, \"%.*s-%u\",\n                                   server->name.len, server->name.data,\n                                   pointer_index - 1);\n            } else {\n                hostlen = snprintf(host, KETAMA_MAX_HOSTLEN, \"%.*s:%u-%u\",\n                                   server->name.len, server->name.data,\n                                   server->port, pointer_index - 1);\n            }\n\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11058378/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11078239", "body": "@antirez @manjuraj it is complicated problem. I also think ignoring priority is good way when redis is true. but it can also cause some misconception because twemproxy also has to support memcache.\n\nlike craiglist. some can use like below too.\n\n``` c\n192.168.1.3:2000:1 server1-1\n192.168.1.3:2001:1 server1-2\n192.168.1.3:2002:1 server1-3\n192.168.1.3:2003:1 server1-4\n```\n\nbut, no one can deny that users will easily make a mistake.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11078239/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11149601", "body": "Plz review my code :) For your advice, code became simple.\nCheck -t option with below case\n\n``` c\n Valid case\n   - 127.0.0.1:11211:1 server1\n   - 127.0.0.1:11212:1 server2\n\n   - 127.0.0.1:11211:1\n   - 127.0.0.1:11212:1 \n\n   - 127.0.0.1:11211:1\n   - 127.0.0.1:11212:1 server2 \n```\n\n``` c\n  Invalid case\n\n   - 127.0.0.1:11211:1 server1\n   - 127.0.0.1:11212:1 server1 <-- same name\n\n   - 127.0.0.1:11211:1 server1\n   - 127.0.0.1:11211:1 server2 <-- same pname with different name # I fixed this case with 3rd patch.\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11149601/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11314107", "body": "@manjuraj Hi, Could you review my code for this issue. I don't send patch for this yet.\nI think I have to get code review from you.\nhttps://github.com/charsyam/twemproxy/commit/4748ed4d2132d78e72db37d45be2deddaa19c77f\n\nbriefly speacking,\n1] it is using core_timeout.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11314107/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11400273", "body": "@AllenDou In my personal opinion, It is because it is hard to garther and summarize all redis server's info result.\nIf twemproxy gathers them, It's result is totally different from redis original result. so It can hurt backward Compatibility.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11400273/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11413147", "body": "@AllenDou I recommend you to use another redis monitoring tools. like redis stat https://github.com/junegunn/redis-stat It is more helpful to understand the facts.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11413147/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11475134", "body": "@AllenDou I have a question. How did you implement it? for example, there are 3 server in a pool.\nso if you send \"INFO\" command, in this case, twemproxy only send it to one server, but you might want to get results from all servers. and how to distinguish results? and how to use it? \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11475134/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11475145", "body": "@AllenDou so, you should gather and modify results which response from each redis server.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11475145/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11500731", "body": "@manjuraj, I add a function which check servers' status with 'get command'\n\nThere are 2 steps.\n\n1] check connection\n -> until now, I still use failures' array in retry_connection to connection test.\n\n2] check command\n -> if connection is established, at that time send 'get command'\n    If this operation is success, and then update the hash_ring.\n\nBut there is a limitation. it needs timeout setting\n\nhttps://github.com/charsyam/twemproxy/commit/f2bf742b880bc5fe66dfdd0bd4957f9f72ecd1b3\n\nI think it is better that implementing cron module like redis's serverCron and regularly check all servers' status. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11500731/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11586219", "body": "https://github.com/twitter/twemproxy/issues/14\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11586219/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11587323", "body": "I don't know why travis-ci failed. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11587323/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11587476", "body": "Yes. fortunately, It was a transient failure.\nThank you @caniszczyk \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11587476/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11595457", "body": "@manjuraj Thank you. I will wait for your advice. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11595457/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11600778", "body": "see the conf/nutcracker.leaf.yml :)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11600778/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11618936", "body": "hi @highkay , There is a trick. \n1] run new redis server \n2] and run \"slaveof original-ip original-port\" on new node\n3] run twemproxy\n4] run \"slaveof no one\" on new node\n\nBut, this solution is somewhat dangerous.\nbecause one of them fails, It can show old version data which it has.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11618936/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11634948", "body": "@highkay hi. so I said it is a kind of trick. \n\nfirst of all, twemproxy will hash your keys to distribute your queries.\n\nso if you make a full copy of your original node.\n\nIt looks like rehashed two nodes.\n\nIt is because twemproxy will distribute your queries to 2 nodes.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11634948/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11655971", "body": "hi @hamgua, I think \"select\" command is very dangerous in twemproxy.\n\nas you know, twemproxy just use one connection with each redis server. and it runs like redis client.\n\nso if twemproxy allows \"select\" command.\n\nother clients are affected by changing db.\n\nin redis server, client connections have their own db id\n\n``` c\nint selectDb(redisClient *c, int id) {\n    if (id < 0 || id >= server.dbnum)\n        return REDIS_ERR;\n    c->db = &server.db[id];\n    return REDIS_OK; \n}\n```\n\nso I think twemproxy can't support \"select\" command. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11655971/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11658003", "body": "@hamgua Hmm. I think the best way to use redis with twemproxy in your situation.\n\njust use db 0 in redis, Using multiple db in one redis doesn't support better performance.\n\nand redis will stop support \"SELECT\" command in cluster mode( yes, it's not implemented yet, maybe from redis 3.0 )  \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11658003/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11726567", "body": "In my test, it is ok.\n\nwhere did you execute nutcracker?\n\nI think you just executed nutcracker in src folder.\n\nIf you do that, maybe nutcracker couldn't find conf folder\n\nexecute in parent folder of src and conf, \n\nlike this\n\n``` c\nsrc/nutcracker -t\n```\n\nIt might work well.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11726567/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11747078", "body": "@manjuraj It's my pleasure \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11747078/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11871652", "body": "@NoDurex I think your configuration is right.  \n\nJust check your clients code.\n\nor check manually\n\n``` c\ntelnet 192.168.0.22 24444\nset test 0 0 4\ntest\n```\n\nif it works well, then you should check your client code. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11871652/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12078645", "body": "@aaronuu hi, guys.\nYou should give another name for it. guanyu is the same name of 152.121.123.24:6379:1\ntwemproxy allows only different server name.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12078645/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12078854", "body": "@aaronuu I think some datum look hidden. it is because twemproxy rehashing keys. It is correct behavior.\nIt doesn't move data to another server. it just sends your request to another server through key hashing. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12078854/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12131076", "body": "could you show me some inaccurate cases?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12131076/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12147639", "body": "@highkay I recommend you to read this article about consistent-hashing(http://www.codeproject.com/Articles/56138/Consistent-hashing)\n\nIn Consistent Hashing, it is correct. because, consistent hashing doesn't move items position. It just changes where to find key with minimum rehashing of keys.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12147639/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12147653", "body": "@aaronuu I recommend you to read this article about consistent-hashing(http://www.codeproject.com/Articles/56138/Consistent-hashing)\n\nIn Consistent Hashing, it is correct. because, consistent hashing doesn't move items position. It just changes where to find key with minimum rehashing of keys.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12147653/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12228326", "body": "@LD250, twemproxy already supports \"MGET\" and \"DEL\"\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12228326/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12406785", "body": "@dcartoon I have a question about this patch.\n\nDoes it work well?\n\nI think it can cause many problems.\n\nfor example, RPOPLPUSH command needs 2 listnames.\n\nRPOPLPUSH list1 list2\n\nat that time this patch will push the value to the same server.\n\nand if hashed servers are different, it can't find the collect server.\n\nso if you make it correctly, you have to split RPOPLPUSH to RPOP and LPUSH\n\nand there is another problem that how to handle one of servers failed.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12406785/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12477315", "body": "@srned good job!. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12477315/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12978455", "body": "@xqpmjh I just recommend making delete script to send delete command to all server.\nit is not difficult.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12978455/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12978911", "body": "I think in general situation. it might be not problem, only when more than one server die, and that time will do it.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12978911/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13255555", "body": "I think. just sending auth command to all connected redis is easy. but Handling irregular cases are hard, for example, one server's setting is different, or and when it failed, I think it can cause many problems which is hard to solve.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13255555/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13270043", "body": "@manjuraj I have an idea about this. it is using my former patch about reconnection.\nIt used \"set\" operation to check. if I change this to use \"auth\" command. I might think I will work well.\nOf course, there is another problem, I should implement extra code for first connection.\nreconnection code is only for reconnection. but I think it is not difficult. I will try. and tell you. Thank you.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13270043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13904276", "body": "@mezzatto I sent a pull request now. but, CRC16 hash has many implementations. so I wonder it is right your system? If you like, change it. Thank you.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13904276/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13904889", "body": "@caniszczyk Thank you for your teaching. You're right I also worried about it.\n@mezzatto also sent a pull request. \nbut I will try to look for BSD crc16 implementation. Thank you. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13904889/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13905098", "body": "@mezzatto Are you using CCITT implementation?, I didn't know what type of CRC16 you use. :)\nThank you.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13905098/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14474116", "body": "@manjuraj, @ejc3 Hi, guys, I sent a pull request to support redis auth.\nand It doesn't need \"liveness code\"\n\nhttps://github.com/twitter/twemproxy/pull/81\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14474116/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/10031155", "body": "@BrandonBrowning Could you give me a script to test this? Thank you.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10031155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10032839", "body": "@BrandonBrowning Thank you. I found the reason. and I will fix it soon. Thank you.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10032839/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10033615", "body": "@BrandonBrowning could you try this version?\nhttps://github.com/charsyam/twemproxy/tree/feature/issue-323\nThank you.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10033615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991572", "body": "How was just initializing with CONF_DEFAULT_SELECT?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991572/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991575", "body": "I just think that add new conf file for this patch like this: nutcracker.select.yml\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991575/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991579", "body": "in twemproxy, it is better to change the variable name to is_select_msg\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991579/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991597", "body": "and conn->sd is nonblocking socket. so write can be failed.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991597/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/36986443", "body": "@ton31337 twemproxy can run on mac or freebsd also. so I think you should check other os too not only linux :)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/36986443/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "pavel-shpilev": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/455", "title": "Feature request: reads/writes routing with Redis.", "body": "Hi.\n\nA couple of projects I am currently working on would greatly benefit if twemproxy supported routing for reads and writes. I imagine this could be a quite useful feature for many others as well.\n\nLet me explain what I mean. I have a Redis cluster with one master and a bunch of slaves. Reads to writes ratio in my scenario is at least 100:1. Immediate availability of writes across all the slaves isn't a concern, and Redis replication serves the purpose just fine. I want to have a single nutcracker setup that would look something like this:\n\n```\nalpha:\n  auto_eject_hosts: true\n  read_distribution: random\n  write_distribution: one_at_a_time\n  hash: murmur\n  listen: 127.0.0.1:6379\n  preconnect: true\n  redis: true\n  redis_db: 1\n  server_failure_limit: 3\n  server_retry_timeout: 5000\n  timeout: 20000\n  read_servers:\n  - read-001.84omyv.0001.use1.cache.amazonaws.com:6379:1\n  - read-002.84omyv.0001.use1.cache.amazonaws.com:6379:1\n  - read-003.84omyv.0001.use1.cache.amazonaws.com:6379:1\n  write_servers:\n  - write-001.84omyv.0001.use1.cache.amazonaws.com:6379:1\n```\n\nCurrently I do routing on application level by handling two connections: one direct to write node and one to nutcracker balancing the reads. This task, however, appears to me common enough to be implemented as twemproxy feature. It would allow to decouple infrastructure configuration and business logic.\n\nI can start working on the feature myself, but before that I would like to know the maintainers opinion whether it worth looking at. Perhaps there are some obvious obstacles that defeat the purpose, that I am missing.\n\nThank you.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/455/reactions", "total_count": 7, "+1": 7, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "anhhuyla": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/454", "title": "twemproxy get timed out when add more redis nodes", "body": "Hello Pros,\n\nI need to use twemproxy to shard keys on my redis nodes, so first i take some benchmark :  \n- in 1st case, twemproxy will connect to only one redis node, as below : \n\n```\nnutcracker01:\n        listen: 0.0.0.0:7000\n        hash: fnv1a_64\n        distribution: ketama\n        auto_eject_hosts: true\n        server_retry_timeout: 60000\n        server_failure_limit: 3\n        timeout: 500\n        redis: true\n        backlog: 1024\n        servers:\n        - 10.90.7.132:7001:1 server1\n```\n\nwith data size at 20 kbyte, ccu = 1000, the result looks very well.\n\n```\n# redis-benchmark -p 7000 -c 1000 -n 10000 -d 20480 -k 1 -r 10000 -t set,get -q -l\nSET: 4697.04 requests per second\nGET: 4757.37 requests per second\n\nSET: 4750.59 requests per second\nGET: 4775.55 requests per second\n\nSET: 4866.18 requests per second\nGET: 4175.37 requests per second\n\n```\n\nNow I add more redis nodes to shard the keys, so the twemproxy config as this : \n\n```\nnutcracker01:\n        listen: 0.0.0.0:7000\n        hash: fnv1a_64\n        distribution: ketama\n        auto_eject_hosts: true\n        server_retry_timeout: 60000\n        server_failure_limit: 3\n        timeout: 500\n        redis: true\n        backlog: 1024\n        servers:     \n        - 10.90.7.132:7001:1 server1\n        - 10.90.7.132:7002:1 server2\n        - 10.90.7.132:7003:1 server3\n        - 10.90.7.132:7004:1 server4\n```\n\nI re-benchmark again with the same command : \n\n`# redis-benchmark -p 7000 -c 1000 -n 10000 -d 20480 -k 1 -r 10000 -t set,get -q -l`\n\nNow the twemproxy log get lot of timed out errors : \n\n```\n# tail -f /var/log/nutcracker/nutcracker.log | grep timed\n[2016-02-16 22:47:03.096] nc_core.c:237 close s 1009 '10.90.7.132:7001' on event 00FF eof 0 done 0 rb 87666159 sb 93215034: Connection timed out\n[2016-02-16 22:47:04.655] nc_core.c:237 close s 1009 '10.90.7.132:7001' on event FFFF eof 0 done 0 rb 700 sb 3204424: Connection timed out\n[2016-02-16 22:47:05.218] nc_core.c:237 close s 1009 '10.90.7.132:7001' on event 00FF eof 0 done 0 rb 290 sb 1255416: Connection timed out\n[2016-02-16 22:47:30.733] nc_core.c:237 close s 1009 '10.90.7.132:7001' on event FFFF eof 0 done 0 rb 205052911 sb 204343314: Connection timed out\n[2016-02-16 22:47:31.291] nc_core.c:237 close s 1009 '10.90.7.132:7001' on event FFFF eof 0 done 0 rb 2497146 sb 24084: Connection timed out\n[2016-02-16 22:47:31.837] nc_core.c:237 close s 1009 '10.90.7.132:7001' on event 00FF eof 0 done 0 rb 2869944 sb 19260: Connection timed out\n[2016-02-16 22:47:33.282] nc_core.c:237 close s 1009 '10.90.7.132:7001' on event FFFF eof 0 done 0 rb 390 sb 1736152: Connection timed out\n[2016-02-16 22:47:33.857] nc_core.c:237 close s 1009 '10.90.7.132:7001' on event FFFF eof 0 done 0 rb 355 sb 1633344: Connection timed out\n[2016-02-16 22:47:35.763] nc_core.c:237 close s 1009 '10.90.7.132:7001' on event FFFF eof 0 done 0 rb 2079415 sb 12237198: Connection timed out\n[2016-02-16 22:47:36.356] nc_core.c:237 close s 1009 '10.90.7.132:7001' on event FFFF eof 0 done 0 rb 1355336 sb 21996: Connection timed out\n[2016-02-16 22:47:36.939] nc_core.c:237 close s 1009 '10.90.7.132:7001' on event FFFF eof 0 done 0 rb 1258320 sb 23616: Connection timed out\n[2016-02-16 22:47:38.804] nc_core.c:237 close s 1009 '10.90.7.132:7001' on event FFFF eof 0 done 0 rb 6495890 sb 2571476: Connection timed out\n[2016-02-16 22:47:39.372] nc_core.c:237 close s 1009 '10.90.7.132:7001' on event FFFF eof 0 done 0 rb 390 sb 1798416: Connection timed out\n[2016-02-16 22:47:41.275] nc_core.c:237 close s 1009 '10.90.7.132:7001' on event FFFF eof 0 done 0 rb 2472126 sb 12587364: Connection timed out\n[2016-02-16 22:47:41.820] nc_core.c:237 close s 1008 '10.90.7.132:7004' on event FFFF eof 0 done 0 rb 411086668 sb 445947444: Connection timed out\n[2016-02-16 22:47:41.822] nc_core.c:237 close s 1009 '10.90.7.132:7001' on event 00FF eof 0 done 0 rb 3171128 sb 21780: Connection timed out\n```\n\nI check the redis slow log but found nothing : \n\n```\n# echo \"config get slowlog-log-slower-than\" | redis-cli -h 10.90.7.132 -p 7001\n1) \"slowlog-log-slower-than\"\n2) \"200000\"\n[root@twemproxy7-176 ~]# echo \"slowlog get 100\" | redis-cli -h 10.90.7.132 -p 7001\n(empty list or set)\n\n# tail /var/log/redis/redis7001.log :\n Accepted 10.90.7.176:52896\n- Reading from client: Connection reset by peer\n- Accepted 10.90.7.176:52897\n- Reading from client: Connection reset by peer\n- Accepted 10.90.7.176:52898\n- Reading from client: Connection reset by peer\n```\n\nIt looks like that the twemproxy get worse when add more redis nodes => it should be good at this because it was born to do this :)\n\nIs it normal or I have missed something ?\n\nPlz show advices, I really need to use twemproxy to scale out my redis cluster and I plan to add dozens of redis nodes behind a twemproxy.\n\nVersion info : \n\n```\n# nutcracker -V\nThis is nutcracker-0.4.1\n\n# redis-server -v\nRedis server v=3.0.7 sha=00000000:0 malloc=jemalloc-3.6.0 bits=64 build=653c024c355853e5\n```\n\nMore advices are also appreciate.\n\nThank you & Brs.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "TrumanDu": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/453", "title": "How to dynamically add a redis instance and migrate data?", "body": "@idning \nHow to dynamically add a redis instance and migrate data?\nWhat I mean is extended.looking forward to your reply.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/453/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "codej99": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/451", "title": "Twemproxy + jedis. pipeline command is not work", "body": "Hi.\nI am currently studying a \"Spring OAuth2\".\nI was implemented Oauth TokenStore using the \"Twemproxy\" and \"Redis\".\nRedis TokenStore provided by the Spring Oauth is implemented by pipeline.\n# I was tested in the following environments\n- Twemproxy0.4.0 + Redis 2.8.0 , Jedis 2.8.0\n# Test\n\nRedisConnection conn = jedisConnectionFactory.getConnection();\nconn.openPipeline();\nconn.set(\"accessKey\".getBytes(),\"AccessTokenValue\".getBytes());\nconn.set(\"authKey\".getBytes(), \"AuthValue\".getBytes());\nconn.expire(\"accessKey\".getBytes(), 100);\nconn.expire(\"authKey\".getBytes(), 200);\nconn.closePipeline();\nconn.close();\n\nwork fine.\nTwemproxy Log below\n\n[2016-02-05 09:54:18.201] nc_request.c:96 req 10 done on c 19 req_time 0.140 msec type REQ_REDIS_SET narg 3 req_len 51 rsp_len 5 key0 'accessKey' peer '10.77.164.82:65002' done 1 error 0\n[2016-02-05 09:54:18.201] nc_request.c:96 req 11 done on c 19 req_time 0.144 msec type REQ_REDIS_SET narg 3 req_len 47 rsp_len 5 key0 'authKey' peer '10.77.164.82:65002' done 1 error 0\n[2016-02-05 09:54:18.201] nc_request.c:96 req 12 done on c 19 req_time 0.123 msec type REQ_REDIS_EXPIRE narg 3 req_len 40 rsp_len 4 key0 'accessKey' peer '10.77.164.82:65002' done 1 error 0\n[2016-02-05 09:54:18.201] nc_request.c:96 req 13 done on c 19 req_time 0.116 msec type REQ_REDIS_EXPIRE narg 3 req_len 38 rsp_len 4 key0 'authKey' peer '10.77.164.82:65002' done 1 error 0\n\nset command excuted\nBut expire command not excuted\n\n127.0.0.1:6380> keys *\n1) \"accessKey\"\n2) \"authKey\"\n127.0.0.1:6380> ttl \"accessKey\"\n(integer) -1\n127.0.0.1:6380> ttl \"authKey\"\n(integer) -1\n\n---\n\nI was ReTested\nNot Use Twemproxy, Only Use Redis + Jedis\n\nRedisConnection conn = jedisConnectionFactory.getConnection();\nconn.openPipeline();\nconn.set(\"accessKey\".getBytes(),\"AccessTokenValue\".getBytes());\nconn.set(\"authKey\".getBytes(), \"AuthValue\".getBytes());\nconn.expire(\"accessKey\".getBytes(), 100);\nconn.expire(\"authKey\".getBytes(), 200);\nconn.closePipeline();\nconn.close();\n\nAll Work Fine.\n\n127.0.0.1:6379> keys *\n1) \"authKey\"\n2) \"accessKey\"\n127.0.0.1:6379> ttl \"authKey\"\n(integer) 194\n127.0.0.1:6379> ttl \"accessKey\"\n(integer) 80\n\n---\n\nI was Re Re Tested\nUse Twemproxy + Redis + Jedis, But pipeline command devided...\n\n---\n\nRedisConnection conn = jedisConnectionFactory.getConnection();\n\nconn.openPipeline();\nconn.set(\"accessKey\".getBytes(),\"AccessTokenValue\".getBytes());\nconn.set(\"authKey\".getBytes(), \"AuthValue\".getBytes());\nconn.closePipeline();\n\nconn.openPipeline();\nconn.expire(\"accessKey\".getBytes(), 100);\nconn.expire(\"authKey\".getBytes(), 200);\nconn.closePipeline();\n## conn.close();\n\nTwemproxy Log below\n\n[2016-02-05 10:18:47.196] nc_request.c:96 req 46 done on c 46 req_time 0.090 msec type REQ_REDIS_SET narg 3 req_len 51 rsp_len 5 key0 'accessKey' peer '10.77.164.82:65349' done 1 error 0\n[2016-02-05 10:18:47.196] nc_request.c:96 req 47 done on c 46 req_time 0.094 msec type REQ_REDIS_SET narg 3 req_len 47 rsp_len 5 key0 'authKey' peer '10.77.164.82:65349' done 1 error 0\n[2016-02-05 10:18:47.207] nc_server.c:539 connected on s 58 to server '127.0.0.1:6380:1'\n[2016-02-05 10:18:47.207] nc_server.c:539 connected on s 57 to server '127.0.0.1:6380:1'\n[2016-02-05 10:18:47.207] nc_request.c:96 req 50 done on c 46 req_time 0.071 msec type REQ_REDIS_EXPIRE narg 3 req_len 40 rsp_len 4 key0 'accessKey' peer '10.77.164.82:65349' done 1 error 0\n[2016-02-05 10:18:47.207] nc_request.c:96 req 51 done on c 46 req_time 0.076 msec type REQ_REDIS_EXPIRE narg 3 req_len 38 rsp_len 4 key0 'authKey' peer '10.77.164.82:65349' done 1 error 0\n\nAll Work fine.\n\n127.0.0.1:6380> keys *\n1) \"accessKey\"\n2) \"authKey\"\n127.0.0.1:6380> ttl \"accessKey\"\n(integer) 90\n127.0.0.1:6380> ttl \"authKey\"\n(integer) 186\n\nI don't know problem of the Jedis or problem of the Twemproxy\n\nCan you help me?\n\nThanks.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/451/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Cphilo": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/449", "title": "redis client connection not closed", "body": "When I use twemproxy, I connect twemproxy with python  redis client, the client connection grows to the limit 10000. But I use it as  a connection pool with max connections 100, it should not happened. Instead, I directly insert data into redis server, the client connection keep stable not grows constantly. I guess that the client connection cannot be closed from the redis client. Twemproxy should do that.\n\n``` bash\nroot@node1:~/ssdb_benchmark# nutcracker --version\nThis is nutcracker-0.4.1\n```\n\n Here is my twemproxy `nutcracker.yml` file\n\n```\nssdb_cluster:\n  listen: 192.168.0.202:4001\n  hash: fnv1a_64\n  distribution: ketama\n  auto_eject_hosts: false\n  redis: true\n  server_connections: 10000\n  preconnect: true\n  server_retry_timeout: 2000\n  server_failure_limit: 1\n  servers:\n   - 192.168.0.201:8888:1\n   - 192.168.0.202:8888:1\n\nredis_cluster:\n  listen: 192.168.0.202:4002\n  hash: fnv1a_64\n  distribution: ketama\n  auto_eject_hosts: false\n  redis: true\n  server_connections: 10000\n  preconnect: true\n  server_retry_timeout: 2000\n  server_failure_limit: 1\n  servers:\n   - 192.168.0.201:6379:1\n   - 192.168.0.202:6379:1\n```\n\nAnd here is my python test script\n\n``` python\nimport os, sys\nfrom sys import stdin, stdout\nfrom random import randint\nimport redis\n\nf = open(\"test.html\").read()\nsettings = {\n    \"SSDBIp\": \"xx\",\n    \"SSDBPort\": xx\n}\nconn_pool = redis.ConnectionPool(host=settings[\"SSDBIp\"], port=settings[\"SSDBPort\"], max_connections=100)\nr = redis.Redis(connection_pool=conn_pool)\n\ndef bench_mark(content):\n    r.set(str(randint(1, 10000000000)), content)\n\nif __name__ == \"__main__\":\n    cnt = 0\n    for i in xrange(100000):\n        cnt += 1\n        if cnt % 1000 == 0:\n            print cnt\n        bench_mark(f)\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/449/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gabhi": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/446", "title": "ssl support", "body": "Does twemproxy support https mode? if yes how to configure it?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/446/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zzgang": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/445", "title": "https://github.com/zzgang/kconnp/wiki/LCP%28Linux-Connection-Pool%29-document", "body": "helpful?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/445/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "princeap": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/444", "title": "redirect msg", "body": "Hi,\n\nI have a redis cluster (10.54.78.20  7001 - 7006) and twemproxy (10.54.78.21) installed. \nI'm connecting to redis cluster thru twemproxy.\nBut still at redis client I get redirect msg. I thought Proxy will take care handling redirection and client will only get final response. \nAm I missing something?\n\nredis-cli -c -h 10.54.78.21 -p 22121\n10.54.78.21:22121> get name\n-> Redirected to slot [5798] located at 10.54.78.20:7002\n\"prince\"\n\nrgds,\nPrince\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/444/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "BinWu1989": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/443", "title": "Is it possible to use the redis Sentinel as the solution about the failover and use the twemproxy for loading balance to the slaves ?", "body": "hi, I am the new starter of redis ,and just want to to know if it is possible for using the redis Sentinel as the solution for the failover and using the twemproxy for loading balance to the slaves , I just check the notes and didn't find the answer , so I rised the issue . May someone kindly helps to answer it ? thank you very much\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/443/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "arnecls": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/442", "title": "Blocking connect on host down", "body": "**Overview**\nWe encountered a strange behavior when a memcached host out of a ketama pool is taken down (by pulling the network cable).  \nTwemproxy is returning timeouts directly after the server becomes unreachable. However, after the ARP cache is flushed (every 20min on FreeBSD) twemproxy suddenly starts to return server errors and the response time increases for every reconnection attempt.\n\n**Version and OS**\nTwemproxy 0.4.1\nFreebsd 10.1\n\n**Webserver load metrics**\n![response](https://cloud.githubusercontent.com/assets/6131989/11688263/df160c06-9e8b-11e5-9c95-9d849e8f4e98.png)\n\n**Twemproxy error metrics**\n![errors](https://cloud.githubusercontent.com/assets/6131989/11688250/c489b0a4-9e8b-11e5-8301-1fdfcd33e1b6.png)\n\n**Twemproxy config**\n24 memcached pools like this:\n\n```\nmemcached_default:\n  listen: /var/run/twemproxy/memcached_default.socket 0777\n  hash: md5\n  distribution: ketama\n  auto_eject_hosts: true\n  backlog: 2048\n  preconnect: true\n  server_retry_timeout: 30000\n  server_failure_limit: 3\n  timeout: 300\n  servers:\n    - 1.2.3.1:11211:1\n    - 1.2.3.2:11211:1\n    - 1.2.3.3:11211:1\n    - 1.2.3.4:11211:1\n    - 1.2.3.5:11211:1\n    - 1.2.3.6:11211:1\n```\n\n**example error log entry**\n\n```\n[2015-12-02 9:30:58.884] nc_server.c:531 connect on s 3462 to server '1.2.3.5:11211:1' failed: Host is down\n```\n\n**Investigation**\nThe error above is produced by nc_server.c:552 (the log entry is misleading), which is strange as the socket is created as non-blocking (nc_server.c:516). In case of a non blocking socket a \"Host is down\" error should have been returned by select() or in this case kevent() as in nc_kqueue.c:273 ff..  \nWe actually see this happen before the ARP cache flush. When kevent encounters a connection related error, a timeout and a forward error are being logged. \nThis is what we see in the first part of the twemproxy error graph.\nThe second part however is likely caused by connect() from nc_server.c as the corresponding error messages appear exactly when the server errors start appearing and the response times go up.\n\nThe conclusion we draw from this is that connect() is directly or indirectly blocking the twemproxy mainloop after the ARP cache is flushed:  \nWith a blocking code path timeouts are multiplied by the set retry count, i.e. 3x300ms before a server is removed. In contrast to that an async code path can handle 3 or more events in parallel, i.e. there is only a total timeout of ~300ms until the server is removed.  \nThis explains why we see an increase of repsonse times after the ARP flush.\nThe only problem here is that no timeout is ever set for connect(). So this call is probably not directly responsible for a blocking behavior.\n\nWe think that the failing connect() is indirectly responsible for triggering an early timeout on kevent because of an empty queue (or sth. like that). In addition to that connect() might actually return an error from a previous connection attempt that timed out (e.g. kevent() giving EINPROGRESS).\n\nIt would be great if someone with a little more knowledge on kevent and/or async sockets could have a look at this.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/442/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/b103e75b0da33c98a8bd9fa421d7011b79131306", "message": "Warning on failed select\n- added a callback that can inspect a req/rsp before it is swallowed\n- this callback is now used with redis so it can check on failed selects"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/74d0bb1183eb25274b362473daf4a2132838f643", "message": "Adjusted code based on recommedations:\n- server_conn_init moved to redis plugin (redis_conn_init)\n- renamed conn->initialize to conn->init\n- fixed typos and comments\n- removed a now obsolete check from #217 in nc_request.c"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/cb9f6418618d82cdaabfe38126be2426256218f3", "message": "- Fixed select message type\n- Added log notice for triggering a select\n- Fixed a missing prototype warning"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/9c093e7ec8beeff14c45cf71cba118208ceefda1", "message": "Configuration options"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/3dd3802876b5339ac55d57ea4150ea845b3f1f3d", "message": "Redis database select upon connect"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/70bd5b0d660350a25b47389fef7da7ad91e29f04", "message": "Build support for freebsd 10 (adding libexecinfo)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21963644", "body": "As far as I know there is no cap on the db setting in redis\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21963644/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21964214", "body": "Of course we could, but as this command is not heavily frequented it should not be much of a performance issue. So I guess adding a field to the config is more wasteful than doing the calculation in place, especially as the value is not used elsewhere.\nSadly the redis protocol integer notation does not work here : /\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21964214/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "ingtarius": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/441", "title": "Strange connect issue", "body": "Hi all!\nI have strange issue in my environment:\ntwemproxy -> redis\nSome times i have null response from twemproxy and see this error in log \"Connection reset by peer\"\nAt this moment in strace i can find this message:\ngetpeername(79, 0x65bd48, [112])        = -1 ENOTCONN (Transport endpoint is not connected)\n\non Redis side everything is clean and fine, no errors, no restarts or same.\nHere is nutcracer conf file:\nnutcracker-redis-pa:\n  listen: 0.0.0.0:8374\n  hash: testtest\n  hash_tag: \"{}\"\n  distribution: modula\n  timeout: 1000\n  auto_eject_hosts: false\n  server_retry_timeout: 30000\n  server_failure_limit: 3\n  server_connections: 1\n  redis: true\n  servers:\n\n```\n- server01:6301:1 server01\n- server02:6302:1 server02\n```\n\nI will try to dump network traffic, but it have a lot of packages between this hosts...\nDoes anybody have any ideas?...\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/441/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "leunamnauj": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/438", "title": "auto_eject_host not working. ", "body": "I have 3 redis servers + sentinel and I thought on put twemproxy over for load balancing and HA somehow. \nI understand that auto_eject_host directive is used to remove a server from the pool when is failing but this is not the case, when I put down one of Redis servers twemproxy still trying to query on it and I got \"(error) ERR Connection refused\"\n\nHonestly I'm not sure if I set something wrong of if I misunderstand the behavior of the tool.\nThanks in advance\n\nThis is my config:\n\nredis1:\n  listen: 0.0.0.0:6379\n  redis: true\n  hash: fnv1a_64\n  distribution: ketama\n  auto_eject_hosts: true\n  timeout: 400\n  server_retry_timeout: 2000\n  server_failure_limit: 1\n  preconnect: true\n  servers:\n- privip1:6380:1\n- privip1:6380:1\n- privip1:6380:1\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ChinaXing": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/437", "title": "why not support zookeeper/etcd", "body": "I am consider of using twemproxy as my proxy of redis, but I found the problem of people must config the server list in configure file. which was not friendly to maintains.\n\nbecause each time we adjust the redis servers, we must change the configure and restart the proxy.\n\nThe codis was developed at https://github.com/wandoulabs/codis, it use zk / etcd to coordinator and make the configure minimal .\n\nso , why don't let twemproxy support this feature ? that will be cool !\n\nthanks for your explain.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/437/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "helifu": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/436", "title": "function 'memcache_copy_bulk()' in nc_memcache.c", "body": "the format of the response for get/gets is:\n\"VALUE key1 0 len1\\r\\nval1\\r\\nVALUE key2 0 len2\\r\\nval2\\r\\nEND\\r\\n\"\n\nis it safe while you are eating 'VALUE key 0'? (line 1450)\nmaybe string 'VALUE key 0' is fragmented into two mbuf?\n\n```\nfor (i = 0; i < 3; i++) {                 /*  eat 'VALUE key 0 '  */\n    for (; *p != ' ';) {\n        p++;\n    }\n    p++;\n}\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/436/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rubenpapovyan": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/434", "title": "Problem with running 2 instances on same server. ", "body": "Hi,\nIm running 2 services of nutcracker\nIm using different ports and different config files \n\nSee `ps axu | grep nutcr`\nroot      4234  6.5  0.6 445672 428184 ?       Ssl  Nov03 1688:02 /usr/local/sbin/nutcracker -c /etc/nutcracker_22122_22222.yml -s 22222 -o /var/log/redis/nutcracker_22122.log\nroot     28281  7.2 <b>76.6</b> 69583152 50568728 ?   Ssl  01:19  44:55 /usr/local/sbin/nutcracker -c /etc/nutcracker_22124_22224.yml -s 22224 -o /var/log/redis/nutcracker_22124.log\n\nSo problem is that second process is using a lot of RAM. \nAfter restart it goes down but after few hour it going up \n\nPlease help me to solve this issue.\n\nThanks in advance. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sidprak": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/433", "title": "Twemproxy becomes unresponsive", "body": "Hello,\n\nWe just experienced an issue across multiple of our Twemproxy servers that caused them to hang and stop responding to Redis commands. The servers both started around the same time (if that matters) and both stopped responding within 30 seconds of each other. I was wondering if we could get some of your insight into what happened.\n\nThe server seemed to be operating normally, suddenly started consuming a lot of memory and got into a hung state where it didn't respond to any commands. It also didn't log anything during the time it was unavailable. We run automated monitoring on the backend Redis servers and we've verified that they were all alive and responding to commands during this time. The issue resolved itself when I restarted Twemproxy. The `connection timed out` message indicates that it may be a problem with the Redis server not responding in time but it is also odd that 1) the server was responding to our monitoring and 2) the issue resolved itself after restarting the proxy. Is there a certain Redis command that could cause something like this?\n### Configuration\n\n```\nredis_cluster:\n  listen: 0.0.0.0:22111\n  hash_tag: \"{}\"\n  hash: fnv1a_64\n  distribution: ketama\n  timeout: 8000\n  auto_eject_hosts: false\n  preconnect: true\n  redis: true\n  servers:\n    - ...\n```\n### Twemproxy log\n\n```\n[Fri Nov 20 15:30:23 2015] nc_proxy.c:336 accepted c 313 on p 38 from '127.0.0.1:55683'\n[Fri Nov 20 15:30:23 2015] nc_core.c:201 close c 313 '127.0.0.1:55683' on event FFFF eof 1 done 1 rb 0 sb 0\n[Fri Nov 20 15:30:25 2015] nc_core.c:201 close c 355 '10.153.161.11:42144' on event 00FF eof 1 done 1 rb 9337489 sb 196610\n[Fri Nov 20 15:30:25 2015] nc_proxy.c:336 accepted c 313 on p 38 from '127.0.0.1:55684'\n[Fri Nov 20 15:30:25 2015] nc_core.c:201 close c 313 '127.0.0.1:55684' on event FFFF eof 1 done 1 rb 0 sb 0\n[Fri Nov 20 15:30:27 2015] nc_proxy.c:336 accepted c 313 on p 38 from '127.0.0.1:55685'\n[Fri Nov 20 15:30:27 2015] nc_core.c:201 close c 313 '127.0.0.1:55685' on event FFFF eof 1 done 1 rb 0 sb 0\n[Fri Nov 20 15:30:30 2015] nc_proxy.c:336 accepted c 313 on p 38 from '127.0.0.1:55686'\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close c 313 '127.0.0.1:55686' on event FFFF eof 1 done 1 rb 0 sb 0\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 34 '10.81.147.121:16383' on event FFFF eof 0 done 0 rb 6500516968 sb 6597409394: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 7 '10.101.176.67:16397' on event FFFF eof 0 done 0 rb 16519790645 sb 8436631958: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 101 '10.101.176.67:16400' on event FFFF eof 0 done 0 rb 14482299157 sb 7437663567: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 8 '10.81.147.121:16394' on event FFFF eof 0 done 0 rb 7192546334 sb 2862473209: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 23 '10.81.147.121:16379' on event FFFF eof 0 done 0 rb 10140341308 sb 3541861302: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 25 '10.81.147.121:16390' on event FFFF eof 0 done 0 rb 6222270100 sb 2575703776: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 13 '10.101.176.67:16402' on event FFFF eof 0 done 0 rb 5656067711 sb 3208107440: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 29 '10.81.147.121:16392' on event FFFF eof 0 done 0 rb 16375822129 sb 7491696210: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 109 '10.101.176.67:16403' on event FFFF eof 0 done 0 rb 7033188937 sb 3165248734: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 20 '10.101.176.67:16405' on event FFFF eof 0 done 0 rb 5049494906 sb 2912358728: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 24 '10.101.176.67:16395' on event FFFF eof 0 done 0 rb 14582409223 sb 7437543343: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 11 '10.101.176.67:16398' on event FFFF eof 0 done 0 rb 8493865699 sb 3749516296: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 30 '10.101.176.67:16396' on event FFFF eof 0 done 0 rb 5690542864 sb 2879933765: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 35 '10.81.147.121:16389' on event FFFF eof 0 done 0 rb 9868206863 sb 11974818980: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 19 '10.101.176.67:16399' on event FFFF eof 0 done 0 rb 3640723229 sb 2266408893: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 22 '10.81.147.121:16380' on event FFFF eof 0 done 0 rb 2840311416 sb 2499551356: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 26 '10.101.176.67:16401' on event FFFF eof 0 done 0 rb 7856895940 sb 3450868310: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 18 '10.81.147.121:16382' on event FFFF eof 0 done 0 rb 7785360347 sb 3647852458: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 28 '10.101.176.67:16409' on event FFFF eof 0 done 0 rb 10876816017 sb 4636879042: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 14 '10.81.147.121:16391' on event FFFF eof 0 done 0 rb 5296392258 sb 2886882817: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 6 '10.81.147.121:16384' on event FFFF eof 0 done 0 rb 5529048908 sb 2391134191: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 17 '10.81.147.121:16387' on event FFFF eof 0 done 0 rb 6629604594 sb 3930064817: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 31 '10.101.176.67:16404' on event FFFF eof 0 done 0 rb 2684417151 sb 2525815006: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 33 '10.101.176.67:16406' on event FFFF eof 0 done 0 rb 13894211803 sb 6363319093: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 21 '10.81.147.121:16385' on event FFFF eof 0 done 0 rb 14175947919 sb 5886369400: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 37 '10.101.176.67:16407' on event FFFF eof 0 done 0 rb 7767051139 sb 2896583329: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 27 '10.81.147.121:16388' on event FFFF eof 0 done 0 rb 5308005719 sb 3467601963: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 9 '10.81.147.121:16381' on event FFFF eof 0 done 0 rb 56969140366 sb 17668770195: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 32 '10.81.147.121:16386' on event FFFF eof 0 done 0 rb 10626964628 sb 4409789478: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close s 12 '10.101.176.67:16408' on event 00FF eof 0 done 0 rb 8699313888 sb 3574219519: Connection timed out\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 7 on p 38 from '127.0.0.1:55687'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 8 on p 38 from '127.0.0.1:55688'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 9 on p 38 from '127.0.0.1:55689'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 11 on p 38 from '127.0.0.1:55690'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 12 on p 38 from '127.0.0.1:55691'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 13 on p 38 from '127.0.0.1:55692'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 14 on p 38 from '127.0.0.1:55693'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 17 on p 38 from '127.0.0.1:55694'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 18 on p 38 from '127.0.0.1:55695'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 19 on p 38 from '127.0.0.1:55696'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 20 on p 38 from '127.0.0.1:55697'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 21 on p 38 from '10.141.254.27:59567'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 22 on p 38 from '10.157.164.165:38110'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 23 on p 38 from '10.229.35.80:57003'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 24 on p 38 from '10.157.125.35:58347'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 25 on p 38 from '10.150.112.106:53063'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 26 on p 38 from '10.141.254.27:59570'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 27 on p 38 from '10.229.35.80:57010'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 28 on p 38 from '10.141.254.27:59571'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 29 on p 38 from '10.167.81.60:34398'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 30 on p 38 from '10.229.35.80:57013'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 31 on p 38 from '10.184.8.180:39252'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 32 on p 38 from '10.167.81.60:34399'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 33 on p 38 from '10.229.35.80:57014'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 34 on p 38 from '10.141.254.27:59574'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 35 on p 38 from '10.141.254.27:59575'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 37 on p 38 from '10.164.103.223:39581'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 101 on p 38 from '10.203.145.203:41854'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 109 on p 38 from '10.167.81.60:34400'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 313 on p 38 from '10.167.81.60:34401'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 318 on p 38 from '10.229.35.80:57019'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 349 on p 38 from '10.167.81.60:34402'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 351 on p 38 from '10.203.145.203:41855'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 352 on p 38 from '10.229.35.80:57020'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 354 on p 38 from '10.203.145.203:41856'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 355 on p 38 from '10.203.145.203:41857'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 357 on p 38 from '10.141.254.27:59576'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 358 on p 38 from '10.141.254.27:59577'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 359 on p 38 from '10.167.81.60:34403'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 363 on p 38 from '10.203.145.203:41860'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 365 on p 38 from '10.150.112.106:53066'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 370 on p 38 from '10.203.145.203:41863'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 376 on p 38 from '10.229.35.80:57021'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 377 on p 38 from '10.167.81.60:34404'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 378 on p 38 from '10.141.254.27:59578'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 379 on p 38 from '10.203.145.203:41864'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 380 on p 38 from '10.167.81.60:34405'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 381 on p 38 from '10.150.112.106:53067'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 382 on p 38 from '10.229.35.80:57022'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 383 on p 38 from '10.167.81.60:34406'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 384 on p 38 from '10.141.254.27:59579'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 385 on p 38 from '10.229.35.80:57023'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 386 on p 38 from '10.157.125.35:58349'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 387 on p 38 from '10.229.35.80:57024'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 389 on p 38 from '10.167.81.60:34407'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 391 on p 38 from '10.203.145.203:41912'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 392 on p 38 from '10.229.35.80:57025'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 393 on p 38 from '10.167.81.60:34410'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 394 on p 38 from '10.229.35.80:57026'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 395 on p 38 from '10.141.254.27:59580'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 396 on p 38 from '10.157.164.165:38112'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 397 on p 38 from '10.167.81.60:34411'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 398 on p 38 from '10.203.145.203:41913'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 399 on p 38 from '10.141.254.27:59581'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 400 on p 38 from '10.165.130.54:56135'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 401 on p 38 from '10.203.145.203:41914'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 402 on p 38 from '10.229.35.80:57029'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 403 on p 38 from '10.203.145.203:41915'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 404 on p 38 from '10.167.81.60:34412'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 405 on p 38 from '10.229.35.80:57030'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 406 on p 38 from '10.141.254.27:59582'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 407 on p 38 from '10.150.112.106:53068'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 408 on p 38 from '10.203.145.203:41918'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 409 on p 38 from '10.141.254.27:59583'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 411 on p 38 from '10.141.254.27:59584'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 412 on p 38 from '10.157.125.35:58350'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 413 on p 38 from '10.167.81.60:34413'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 414 on p 38 from '10.203.145.203:41919'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 415 on p 38 from '10.203.145.203:41920'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 416 on p 38 from '10.141.254.27:59587'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 417 on p 38 from '10.229.35.80:57031'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 418 on p 38 from '10.157.121.26:46460'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 419 on p 38 from '10.229.35.80:57032'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 420 on p 38 from '10.167.81.60:34418'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 421 on p 38 from '10.203.145.203:41923'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 422 on p 38 from '10.157.125.35:58351'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 423 on p 38 from '10.150.112.106:53071'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 424 on p 38 from '10.167.81.60:34419'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 425 on p 38 from '10.203.145.203:41924'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 426 on p 38 from '10.229.35.80:57033'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 427 on p 38 from '10.167.81.60:34422'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 428 on p 38 from '10.167.81.60:34423'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 429 on p 38 from '10.141.254.27:59588'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 430 on p 38 from '10.203.145.203:41927'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 431 on p 38 from '10.157.164.165:38114'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 432 on p 38 from '10.229.35.80:57038'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 433 on p 38 from '10.157.164.165:38115'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 434 on p 38 from '10.167.81.60:34424'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 435 on p 38 from '10.229.35.80:57039'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 436 on p 38 from '10.203.145.203:41928'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 437 on p 38 from '10.157.121.26:46467'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 438 on p 38 from '10.150.112.106:53072'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 439 on p 38 from '10.203.145.203:41929'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 440 on p 38 from '10.141.254.27:59591'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 441 on p 38 from '10.157.125.35:58354'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 442 on p 38 from '10.141.254.27:59592'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 443 on p 38 from '10.229.35.80:57040'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 444 on p 38 from '10.141.254.27:59593'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 445 on p 38 from '10.157.164.165:38119'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 446 on p 38 from '10.157.121.26:46470'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 447 on p 38 from '127.0.0.1:55698'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 448 on p 38 from '10.157.125.35:58357'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 449 on p 38 from '10.150.112.106:53075'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 450 on p 38 from '10.157.125.35:58358'\n[Fri Nov 20 15:30:55 2015] nc_proxy.c:336 accepted c 451 on p 38 from '10.157.164.165:38125'\n[Fri Nov 20 15:30:55 2015] nc_core.c:201 close c 175 '10.37.180.132:42442' on event FFFF eof 1 done 1 rb 70228199 sb 340403456\n\n# Manually restarted #\n[Fri Nov 20 15:57:44 2015] nc.c:187 nutcracker-0.3.0 built for Linux 3.13.0-36-generic x86_64 started on pid 5700\n[Fri Nov 20 15:57:44 2015] nc.c:192 run, rabbit run / dig that hole, forget the sun / and when at last the work is done / don't sit down / it's time to dig another one\n```\n### `ps` log\n\nThis is a list of `ps` entries for Nutcracker every minute during the window.\n\n```\ntwemproxy       806  4.0 18.4 739108 709616 ?       Ssl  Nov03 1027:37  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.0 18.4 739108 709616 ?       Ssl  Nov03 1027:39  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.0 78.1 4896524 3000608 ?     Rsl  Nov03 1028:12  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.0 78.1 4896524 3000608 ?     Rsl  Nov03 1029:18  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.0 78.1 4896524 3000608 ?     Rsl  Nov03 1030:25  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.0 78.1 4896524 3000608 ?     Rsl  Nov03 1031:32  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.0 78.1 4896524 3000608 ?     Rsl  Nov03 1032:35  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.0 78.1 4896524 3000608 ?     Rsl  Nov03 1033:41  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.0 78.1 4896524 3000608 ?     Rsl  Nov03 1034:33  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.0 78.1 4896524 3000608 ?     Rsl  Nov03 1034:33  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.0 78.1 4896524 3000608 ?     Rsl  Nov03 1035:52  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.0 78.1 4896524 3000608 ?     Rsl  Nov03 1035:52  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.0 78.1 4896524 3000608 ?     Rsl  Nov03 1038:00  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.0 78.1 4896524 3000608 ?     Rsl  Nov03 1038:00  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.1 78.1 4896524 3000608 ?     Rsl  Nov03 1040:13  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.1 78.1 4896524 3000608 ?     Rsl  Nov03 1040:13  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.1 78.1 4896524 3000608 ?     Rsl  Nov03 1042:23  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.1 78.1 4896524 3000608 ?     Rsl  Nov03 1042:23  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.1 78.1 4896524 3000608 ?     Rsl  Nov03 1044:36  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.1 78.1 4896524 3000608 ?     Rsl  Nov03 1044:36  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.1 78.1 4896524 3000608 ?     Rsl  Nov03 1044:36  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.1 78.1 4896524 3000608 ?     Rsl  Nov03 1047:02  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.1 78.1 4896524 3000608 ?     Rsl  Nov03 1047:02  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.1 78.1 4896524 3000608 ?     Rsl  Nov03 1049:58  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.1 78.1 4896524 3000608 ?     Rsl  Nov03 1049:58  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.1 78.1 4896524 3000608 ?     Rsl  Nov03 1049:58  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.1 78.1 4896524 3000608 ?     Rsl  Nov03 1053:12  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.1 78.1 4896524 3000608 ?     Rsl  Nov03 1053:12  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy       806  4.1 78.1 4896524 3000608 ?     Rsl  Nov03 1053:12  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy      5700  6.0  0.1  30884  7640 ?        Ssl  15:57   0:01  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\ntwemproxy      5700  6.5  2.5 154192 97136 ?        Ssl  15:57   0:05  \\_ /usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml -o /var/log/nutcracker/nutcracker.log\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/433/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "csrazvan": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/432", "title": "Openresty + twemproxy error", "body": "Hi,\n\nI'm trying to use twemproxy in order to allow openresty/lua-resty-redis to write to different cache nodes ( basically a bit of sharding / load balancing). \n\nI've created my nutcracker.yml configuration and all looks pretty good when talking through redis-cli for example but as soon as I start making requests from openresty I get errors. \nThe error i get seems to be \"connection closed\". \n\nThe twemproxy logs look like this but I'm not sure how to interpret them and what's the error cause:\n\nFirst error caused by a \"script load\" command \n[2015-11-18 16:42:12.297] nc_redis.c:1092 parsed unsupported command 'script'\n[2015-11-18 16:42:12.297] nc_redis.c:1680 parsed bad req 60 res 1 type 0 state 5\n00000000  2a 33 0d 0a 24 36 0d 0a  73 63 72 69 70 74 0d 0a   |*3..$6..script..|\n00000010  24 34 0d 0a 4c 4f 41 44  0d 0a 24 31 30 34 0d 0a   |$4..LOAD..$104..|\n00000020  6c 6f 63 61 6c 20 76 20  3d 20 72 65 64 69 73 2e   |local v = redis.|\n00000030  63 61 6c 6c 28 27 49 4e  43 52 27 2c 20 41 52 47   |call('INCR', ARG|\n00000040  56 5b 31 5d 29 20 69 66  20 76 20 3d 3d 20 31 20   |V[1]) if v == 1 |\n00000050  74 68 65 6e 20 72 65 64  69 73 2e 63 61 6c 6c 28   |then redis.call(|\n00000060  27 45 58 50 49 52 45 27  2c 20 41 52 47 56 5b 31   |'EXPIRE', ARGV[1|\n00000070  5d 2c 20 41 52 47 56 5b  32 5d 29 20 65 6e 64 20   |], ARGV[2]) end |\n00000080  72 65 74 75 72 6e 20 76  0d 0a                     |return v..|\n[2015-11-18 16:42:12.297] nc_core.c:198 recv on c 16 failed: Invalid argument\n[2015-11-18 16:42:12.297] nc_core.c:237 close c 16 '10.76.130.107:48194' on event FFFF eof 0 done 0 rb 138 sb 0: Invalid argument\n[2015-11-18 16:42:12.297] nc_client.c:147 close c 16 discarding pending req 60 len 138 type 0\n\nSecond error caused by an evalsha command:\n\n[2015-11-18 16:42:12.297] nc_redis.c:1680 parsed bad req 61 res 1 type 126 state 11\n00000000  2a 35 0d 0a 24 37 0d 0a  65 76 61 6c 73 68 61 0d   |*5..$7..evalsha.|\n00000010  0a 24 2d 31 0d 0a 24 31  0d 0a 31 0d 0a 24 31 33   |.$-1..$1..1..$13|\n00000020  0d 0a 31 32 33 3a 2f 74  65 73 74 2f 75 72 6c 0d   |..123:/test/url.|\n00000030  0a 24 32 0d 0a 31 30 0d  0a                        |.$2..10..|\n[2015-11-18 16:42:12.297] nc_core.c:198 recv on c 8 failed: Invalid argument\n[2015-11-18 16:42:12.297] nc_core.c:237 close c 8 '10.76.130.107:48193' on event 00FF eof 0 done 0 rb 57 sb 0: Invalid argument\n[2015-11-18 16:42:12.297] nc_client.c:147 close c 8 discarding pending req 61 len 57 type 126\n\nThank you very much for your help,\nRazvan \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/432/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sethrosenblum": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/429", "title": "Stats namespaces aren't properly segregated", "body": "On the stats endpoint, the pool name and server name are being added as keys to the same object that has pre-defined stats in it:\n\n```\n$ curl localhost:22222\n{\n    \"service\":\"nutcracker\",\n    \"source\":\"i-deadbeef\",\n    \"version\":\"0.4.1\",\n    \"uptime\":4018,\n    ...global stats...\n    \"poolA\": {\n        \"client_eof\":221,\n        \"client_err\":0,\n        \"client_connections\":15,\n        \"fragments\":0,\n        ...pool stats...\n        \"serverA\": {\n            ...server stats...\n        }\n    }\n}\n```\n\nAs a result I can clobber the higher-level stats by naming a pool `uptime` or naming a server `fragments`:\n\n```\n$ curl localhost:22222\n{\n    \"service\":\"nutcracker\",\n    \"source\":\"i-deadbeef\",\n    \"version\":\"0.4.1\",\n    \"uptime\": {\n        \"client_eof\":221,\n        \"client_err\":0,\n        \"client_connections\":15,\n        ...pool stats...\n        \"fragments\": {\n            ...server stats...\n        }\n    }\n    ...global stats...\n}\n```\n\nAs a result, `uptime` and `fragments` are no longer reported.  What I'd really like to do is be able to iterate over a dedicated JSON object or array where I don't need to know ahead of time what the pool or server names are in order to know that they're pool or server stats respectively, Like this:\n\n```\n{\n    \"service\":\"nutcracker\",\n    \"source\":\"i-deadbeef\",\n    ...global stats...\n    \"pools\" {\n        \"poolA\": {\n            ...pool stats...\n            \"forward_error\":0,\n            \"fragments\":0,\n            \"servers\": {\n                \"serverA\": {\n                    ...server stats...\n                    \"server_eof\":0,\n                    \"server_err\":0,\n                },\n                \"serverB\": {\n                   ... other server stats...\n                }\n            }\n        }\n    }\n}\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/429/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "digitalprecision": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/427", "title": "PHP - Retry through proxy is not successful", "body": "http://stackoverflow.com/questions/33487641/twitter-twemproxy-retry-not-working-as-expected\n\nWondering if anyone had insight into this? I even tried to sleep and create a new instance and I can't get it to hit a known good cache node. I set server retry to 1, and in code have retries set to 2 per the docs ( retries have to be > server retry).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/427/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "purplegrape": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/426", "title": "add systemd support", "body": "nutcracker.service\n\n[Unit]\nDescription=nutcracker - High performance proxy server for memcached/redis\nAfter=network.target\n\n[Service]\nType=simple\nExecStartPre=/usr/sbin/nutcracker -t -c /etc/nutcracker/nutcracker.yml\nExecStart=/usr/sbin/nutcracker -c /etc/nutcracker/nutcracker.yml\nExecReload=/bin/kill -HUP $MAINPID\n\n[Install]\nWantedBy=multi-user.target\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/426/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "buglomi": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/422", "title": "need a read only feature", "body": "Hello :\nIs it possible to have a configure about global read model ?\nAs below ,the \"Permission\" has three value :  r(read), w(write),rw\n\nr : the proxy only forward read operation to the backend,and return a message to the client for all write operation.\n\nbeta:\n  listen: 127.0.0.1:22122\n  hash: fnv1a_64\n  hash_tag: \"{}\"\n  distribution: ketama\n  auto_eject_hosts: false\n  timeout: 400\n  Permission: rw\n  redis: true\n  servers:\n- 127.0.0.1:6380:1 server1\n- 127.0.0.1:6381:1 server2\n- 127.0.0.1:6382:1 server3\n- 127.0.0.1:6383:1 server4\n\nThanks!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/422/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tfarina": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/419", "title": "nc_set_reuseaddr: len can be inlined.", "body": "We can just call sizeof(reuse) directly in the setsockopt() function.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/419/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xiaosuo": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/416", "title": "possible starvation", "body": "When reading the source code, I find that if we use edge trigger instead to monitor fd events, and we use a relative simple event processing framework without event caching but drain sockets, then if there is a huge response, it will take much time to process it, and during that time, all the other events will be blocked.\n\nI think we can add a quota for each event, and if the bytes processed for one connection is over its quota, we'll cache this event for the next round and process the others.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/416/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "EasonYi": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/415", "title": "redis-benchmark failed on my Mac with the latest repo sources", "body": "After I cloned the latest repo and built it successfully, I ran the redis-benchmark 2 times and got the same error.  And I checked that the redis server was still running normally.\n\n```\n\u279c  ~  redis-benchmark -q -t set,get,incr,lpush,lpop,sadd,spop,lpush,lrange -c 100 -p 22121\nSET: 9943.32 requests per second\nGET: 11759.17 requests per second\nINCR: 12838.62 requests per second\nLPUSH: 12484.39 requests per second\nLPOP: 13073.60 requests per second\nSADD: 12603.98 requests per second\nSPOP: 8874.69 requests per second\nLPUSH (needed to benchmark LRANGE): 5545.39 requests per second\nError: Server closed the connection55.43\n\n[2015-09-23 07:13:15.209] nc_request.c:96 req 1652720 done on c 70 req_time 0.281 msec type REQ_REDIS_LRANGE narg 4 req_len 43 rsp_len 906 key0 'mylist' peer '127.0.0.1:51440' done 1 error 0\n[2015-09-23 07:13:15.209] nc_request.c:96 req 1652721 done on c 71 req_time 0.257 msec type REQ_REDIS_LRANGE narg 4 req_len 43 rsp_len 906 key0 'mylist' peer '127.0.0.1:51441' done 1 error 0\n[2015-09-23 07:13:15.209] nc_request.c:96 req 1652722 done on c 72 req_time 0.283 msec type REQ_REDIS_LRANGE narg 4 req_len 43 rsp_len 906 key0 'mylist' peer '127.0.0.1:51442' done 1 error 0\n[2015-09-23 07:13:15.209] nc_util.c:324 assert 'conn->send_active' failed @ (nc_message.c, 872)\n[2015-09-23 07:13:15.219] nc_util.c:302 [0] 2   nutcracker                          0x000000010fef7a6c msg_send + 92\n[2015-09-23 07:13:15.219] nc_util.c:302 [1] 3   nutcracker                          0x000000010fef1a9d core_core + 557\n[2015-09-23 07:13:15.219] nc_util.c:302 [2] 4   nutcracker                          0x000000010ff13cdc event_wait + 476\n[2015-09-23 07:13:15.219] nc_util.c:302 [3] 5   nutcracker                          0x000000010fef1d60 core_loop + 32\n[2015-09-23 07:13:15.219] nc_util.c:302 [4] 6   nutcracker                          0x000000010ff04558 main + 1416\n[2015-09-23 07:13:15.219] nc_util.c:302 [5] 7   libdyld.dylib                       0x00007fff8f51a5c9 start + 1\n[1]    17426 abort      src/nutcracker -i 2000 -a 127.0.0.1\n\n\u279c  twemproxy git:(master) git status\nOn branch master\nYour branch is up-to-date with 'origin/master'.\nnothing to commit, working directory clean\n\n\u279c  twemproxy git:(master) uname -a\nDarwin Eason 14.5.0 Darwin Kernel Version 14.5.0: Wed Jul 29 02:26:53 PDT 2015; root:xnu-2782.40.9~1/RELEASE_X86_64 x86_64\n\n\u279c  ~  redis-server -v\nRedis server v=3.0.4 sha=00000000:0 malloc=libc bits=64 build=ee774adfcab9032f\n\n\u279c  ~  redis-benchmark -q -t set,get,incr,lpush,lpop,sadd,spop,lpush,lrange -c 100 -p 6379\nSET: 50100.20 requests per second\nGET: 50150.45 requests per second\nINCR: 51255.77 requests per second\nLPUSH: 49975.02 requests per second\nLPOP: 49850.45 requests per second\nSADD: 50632.91 requests per second\nSPOP: 50251.26 requests per second\nLPUSH (needed to benchmark LRANGE): 49627.79 requests per second\nLRANGE_100 (first 100 elements): 16129.03 requests per second\nLRANGE_300 (first 300 elements): 8232.49 requests per second\nLRANGE_500 (first 450 elements): 5983.72 requests per second\nLRANGE_600 (first 600 elements): 4517.94 requests per second\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/415/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "IYism": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/414", "title": "SERVER_ERROR Connection timed out", "body": "Memcached server failure, twemproxy will not automatically remove the server. \nStill SERVER_ERROR Connection timed out\n\nconf\n\nmemcached:\n  listen: 0.0.0.0:11211\n  hash: md5\n  distribution: ketama\n  timeout: 100\n  auto_eject_hosts: true\n  server_retry_timeout: 2000\n  server_failure_limit: 1\n  servers:\n- front-cache1001.memcached:11211:5\n- front-cache1002.memcached:11211:5\n- front-cache1003.memcached:11211:5\n- front-cache1004.memcached:11211:5\n- front-cache1005.memcached:11211:5\n- front-cache1006.memcached:11211:5\n- front-cache1007.memcached:11211:5\n- front-cache1008.memcached:11211:5\n- front-cache1009.memcached:11211:5\n- front-cache1010.memcached:11211:5\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/414/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rohitpaygude": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/408", "title": "Uneven cache hits", "body": "Hi all,\n\nI have integrated twemproxy into web layer and I have 6 Elasticache(1 master , 5 read replicas) I am getting issue that the all replicas have same keys everything is same but cache hits on one replica is way more than others and I performed several load testing still on every test I am getting same result. I have separate data engine that writes on the master of this cluster and remaining 5 replicas get sync with it. So I am using twemproxy only for reading data from Elasticache not for sharding purpose. So my simple question is why i am getting 90% of hits on single read replicas of Elasticache it should distribute the hits evenly among all read replicas? right?  \nThank you in advance.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/408/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "haukebruno": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/405", "title": "best practice for slot distribution in redis backends", "body": "Hi there,\n\nI want to use some nutcrackers in front of a small set of Redis (cluster) backends. Currently this will be 5 backends and in summary 5 Masters with 4 Slaves each.\n\nCan someone explain to me, what the _better_ option will be? To use 5 independend clusters containing 1 master with 16384 hash slots mapped to each cluster or to use on big cluster containing all masters with a distributed set of hash slots?\n\nAs I want to use sharding from nutcrackers site, I wonder what the best practice will be in that case.\n\ncheers,\nhauke\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/405/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tom-dalton-fanduel": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/404", "title": "0.4.1 Response corruption with timeout and lua script", "body": "We've found an issue whereby Twemproxy seems to get into a situation where responses are corrupted and mixed incorrectly between clients. The issue seems to be triggered by evaluation of a lua script combined with a response timeout. After a request is times out (and the client receives an \"-ERR Connection timed out\" Redis response), subsequent requests can see partial responses from other clients requests.\n\nThe minimal example I've been able to create that exhibits the issue:\n\n``` lua\n    local id_counter_key = KEYS[1]\n    local x_key = KEYS[2]\n    local id = -1\n    local id_key = \"\"\n    local x = ARGV[1]\n    if redis.call('exists', x_key) == 1 then\n        return {err=\"10001\"}\n    end\n    id = redis.call('incr', id_counter_key)\n    id_key = 'test_data:' .. id\n    redis.call('hset', id_key, 'id', id)\n    redis.call('hset', id_key, 'x', x)\n    redis.call('set', x_key, id)\n    return id\n```\n\nCombined with a configuration like:\n\n``` yaml\nuser:\n  listen: 0.0.0.0:22120\n  redis: true\n  hash: fnv1a_64a\n  distribution: ketama\n  auto_eject_hosts: false\n  timeout: 5000\n  server_retry_timeout: 1000\n  server_failure_limit: 1 \n  servers:\n   - redis.test.com:6379:1 server\n```\n\nThe problem appears using eval or evalsha, I can't reproduce it without a lua script that times out. It is sufficient to force timeouts by setting the twemproxy pool timeout value sufficiently low (e.g. 10ms) to see the issue, but the issue has been seen with 'real' response timeouts of both 5 and 20 secs.\n\nI am also only able to reproduce this when 2 cases hold:\n- The script has a error-condition exit such as the one above if the key exists;\n- And also there is a separate process making set and get commands through twemproxy that have data of the form \"nn:nn:nn\" e.g. \"12:34:56\".\n\nA minimal example of that 'second process':\n\n``` sh\nwhile true\ndo\n  echo\n  date\n  nslookup $ENDPOINT | grep Address: | tail -n+2\n  echo -e \"get test_counter\\nset test `date +%H:%M:%S`\\nget test\" \\\n    | redis-cli -h $ENDPOINT -p $PORT\n\n  sleep 1\ndone\n```\n\nI have been trying to get a minimal example together but struggling to reproduce the situation when there isn't a secondary client, but I will keep working on this on Monday to get to the bottom of this, but thought it was worth reporting now due to the severity.\n\nHere is a Python script that should demonstrate the issue when run in conjunction with the above shell loop:\n\n``` python\nfrom multiprocessing import Process\nimport random\nimport signal\nimport sys\nimport time\n\nfrom redis import StrictRedis, ResponseError\n\n\nWORKER_DELAY_SECS = 0.0001\nNUM_WORKERS = 10\n\n\nSCRIPT = \"\"\"\n    local id_counter_key = KEYS[1]\n    local x_key = KEYS[2]\n    local id = -1\n    local id_key = \"\"\n    local x = ARGV[1]\n    if redis.call('exists', x_key) == 1 then\n        return {err=\"10001\"}\n    end\n    id = redis.call('incr', id_counter_key)\n    id_key = 'test_data:' .. id\n    redis.call('hset', id_key, 'id', id)\n    redis.call('hset', id_key, 'x', x)\n    redis.call('set', x_key, id)\n    return id\n\"\"\"\n\n\ndef get_twemproxy_redis():\n    return StrictRedis(\"localhost\", 22120)\n\n\ndef worker():\n    user_redis = get_twemproxy_redis()\n\n    while True:\n        try:\n            x = random.randint(1, 1000)\n\n            result = user_redis.eval(SCRIPT, 2, \"test_counter_key\", \"test_data_key\", x)\n            int(result)\n\n        except ResponseError as e:\n            pass\n\n        time.sleep(WORKER_DELAY_SECS)\n\n\ndef create_data():\n    subprocesses = []\n    for i in range(NUM_WORKERS):\n        p = Process(target=worker, name=\"worker-{}\".format(i))\n        subprocesses.append(p)\n\n    for p in subprocesses:\n        p.start()\n\n\nif __name__ == \"__main__\":\n    create_data()\n```\n\nWithin a few seconds of both scripts running:\n\n```\n(venv)root@app-test:/opt/app# python run_twemproxy_soak_test.py \nProcess worker-9:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n    self._target(*self._args, **self._kwargs)\n  File \"run_twemproxy_soak_test.py\", line 44, in worker\n    int(result)\nValueError: invalid literal for int() with base 10: '-ERR Inv'\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/404/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/6f32a928b4830d218fef67aa5d22bc0fd44000f6", "message": "Add testcase"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ee48b75e7898ef3dfe2eec2c0ff6356afe8e5521", "message": "Fix parsing bug when error body contains no spaces"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/0f69053c6c763084553812ef805191aa24c8ed5d", "message": "Document the client_connections config option"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/2fd5a746c8e6d7574e5a5d52089957699d0514d0", "message": "Document the redis_db config setting."}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/53e77123574ed9eef0030342b13e4cd85099faee", "message": "Update readme - company list"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gerasimhovhannisyan": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/401", "title": "nc_core close c, Invalid argument", "body": "Hi all\nI have errors in nutcracer.log\nIt is running in debug mode and here are logs\n\n2015-08-12 14:08:19.281] nc_core.c:237 close c 529 '127.0.0.1:17958' on event 00FF eof 0 done 0 rb 993790 sb 399415742: Invalid argument\n[2015-08-12 14:08:19.538] nc_proxy.c:377 accepted c 529 on p 19 from '127.0.0.1:18198'\n[2015-08-12 14:08:45.414] nc_core.c:237 close c 529 '127.0.0.1:18198' on event 00FF eof 0 done 0 rb 224930 sb 61853847: Invalid argument\n[2015-08-12 14:08:45.671] nc_proxy.c:377 accepted c 529 on p 19 from '127.0.0.1:18252'\n[2015-08-12 14:08:46.050] nc_core.c:237 close c 535 '127.0.0.1:18038' on event 00FF eof 0 done 0 rb 1705800 sb 415883481: Invalid argument\n[2015-08-12 14:08:46.307] nc_proxy.c:377 accepted c 535 on p 19 from '127.0.0.1:18253'\n[2015-08-12 14:08:56.348] nc_core.c:237 close c 541 '127.0.0.1:17788' on event 00FF eof 0 done 0 rb 2637728 sb 984732541: Invalid argument\n[2015-08-12 14:08:56.606] nc_proxy.c:377 accepted c 541 on p 19 from '127.0.0.1:18285'\n[2015-08-12 14:09:00.518] nc_core.c:237 close c 28 '127.0.0.1:18010' on event 00FF eof 0 done 0 rb 805366 sb 343271145: Invalid argument\n[2015-08-12 14:09:00.774] nc_core.c:237 close c 541 '127.0.0.1:18285' on event 00FF eof 0 done 0 rb 27984 sb 16251824: Invalid argument\n[2015-08-12 14:09:00.776] nc_proxy.c:377 accepted c 28 on p 19 from '127.0.0.1:18299'\n[2015-08-12 14:09:01.032] nc_proxy.c:377 accepted c 541 on p 19 from '127.0.0.1:18300'\n[2015-08-12 14:09:11.612] nc_core.c:237 close c 529 '127.0.0.1:18252' on event 00FF eof 0 done 0 rb 364633 sb 98015102: Invalid argument\n[2015-08-12 14:09:11.870] nc_proxy.c:377 accepted c 529 on p 19 from '127.0.0.1:18329'\n[2015-08-12 14:09:32.215] nc_core.c:237 close c 541 '127.0.0.1:18300' on event 00FF eof 0 done 0 rb 808399 sb 146614443: Invalid argument\n[2015-08-12 14:09:32.472] nc_proxy.c:377 accepted c 541 on p 19 from '127.0.0.1:18371'\n\nI have run it by:\nnutcracker -c /etc/nutcracker.yml -d -o /var/log/redis/nutcracker.log \n\nhear is the configuration\nredis-users:\n  listen: 0.0.0.0:22122\n  redis: true\n  hash: fnv1a_64\n  distribution: ketama\n  auto_eject_hosts: false\n  server_retry_timeout: 30000\n  server_failure_limit: 3\n  timeout: 500000\n  backlog: 4096\n  preconnect: true\n  server_connections: 32\n  servers:\n- 10.10.10.10:6379:1 server0\n- 10.10.10.10:6378:1 server1\n- 10.10.10.10:6377:1 server2\n- 20.20.20.20:6379:1 server3\n- 20.20.20.20:6378:1 server4\n- 20.20.20.20:6377:1 server5\n- 30.30.30.30:6379:1 server6\n- 30.30.30.30:6378:1 server7\n- 30.30.30.30:6377:1 server8\n- 40.40.40.40:6379:1 server9\n- 40.40.40.40:6378:1 server10\n- 40.40.40.40:6377:1 server11\n\nin application site we have lots of errors\nerror_message: 'Redis connection to 127.0.0.1:22122 failed - connect ECONNREFUSED',\n  stack: 'Error: Redis connection to 127.0.0.1:22122 failed - connect ECONNREFUSED\\n    at RedisClient.on_error (/var/www/node_modules/redis/index.js:196:24)\\n    at Socket.<anonymous> (/var/www/node_modules/redis/index.js:106:14)\\n    at Socket.emit (events.js:95:17)\\n    at net.js:440:14\\n    at process._tickCallback (node.js:419:13)',\n\nPlease suggest what be a cause of problem?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/401/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "priya23": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/399", "title": "Reg:Dalli client connection to twenproxy in binary_memcache branch", "body": "", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/399/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wooparadog": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/376", "title": "SIGSEGV in mset when one redis server is down.", "body": "Hi,\n\nWe are using redis as cache servers, and put twemproxy on top of them as load balancers. And we've discovered a bug that twemproxy would crash when one of our redis server is down. \n\nWe are using 0.4.0 downloaded from github release page. This is a pretty serious bug in our production environment when a single redis instance is taken down, it'll take the whole cluster with it.\n## Reproduce\n1. Don't start any redis-server(those configured in twemproxy, or you can start all of them, kill one during 3.)\n2. Start twemproxy.\n3. Run:\n   `redis-benchmark -h localhost -p 5401 -c 500 -n 10000000 -r 1045231 -d 256 -t mset`\n\ntwemproxy config:\n\n``` config\nredis-cache:\n  listen: 0.0.0.0:5401\n  hash: fnv1a_64\n  distribution: modula\n  timeout: 1000\n  preconnect: true\n  redis: true\n  server_connections: 1\n  auto_eject_hosts: true\n  #server_retry_timeout: 10000\n  #server_failure_limit: 5\n  servers:\n\n    - localhost:6300:1 redis-localhost-00\n    - localhost:6301:1 redis-localhost-01\n    - localhost:6302:1 redis-localhost-02\n    - localhost:6303:1 redis-localhost-03\n    - localhost:6304:1 redis-localhost-04\n```\n## bt\n\nThere are different dump locations:\n\n1.\n\n``` coredump\n#0  redis_append_key (keylen=16, key=0x1487172 \"key:000000066829\\r\\n$256\\r\\n\", 'x' <repeats 176 times>..., r=0x7fb2a1d7a7d8 <main_arena+120>) at nc_redis.c:2253\n#1  redis_fragment_argx (r=0x1486dd0, ncontinuum=4, key_step=2, frag_msgq=<optimized out>) at nc_redis.c:2379\n#2  0x000000000040e878 in req_recv_done (ctx=0x128d090, conn=0x12953c0, msg=0x1486dd0, nmsg=<optimized out>) at nc_request.c:613\n#3  0x000000000040dae9 in msg_parsed (msg=0x1486dd0, conn=0x12953c0, ctx=0x128d090) at nc_message.c:591\n#4  msg_parse (msg=0x1486dd0, conn=0x12953c0, ctx=0x128d090) at nc_message.c:626\n#5  msg_recv_chain (msg=0x1486dd0, conn=0x12953c0, ctx=0x128d090) at nc_message.c:681\n#6  msg_recv (ctx=0x128d090, conn=0x12953c0) at nc_message.c:714\n#7  0x0000000000409531 in core_recv (conn=0x12953c0, ctx=0x128d090) at nc_core.c:194\n#8  core_core (arg=0x12953c0, events=255) at nc_core.c:329\n#9  0x000000000041f39d in event_wait (evb=0x128d570, timeout=1000) at nc_epoll.c:269\n#10 0x0000000000409b29 in core_loop (ctx=ctx@entry=0x128d090) at nc_core.c:352\n#11 0x0000000000408e48 in nc_run (nci=0x7fffd4a71470) at nc.c:530\n#12 main (argc=<optimized out>, argv=<optimized out>) at nc.c:579\n```\n\n2.\n\n``` core\n\nProgram received signal SIGSEGV, Segmentation fault.\nredis_fragment_argx (r=0x806450, ncontinuum=4, key_step=2, frag_msgq=<optimized out>) at nc_redis.c:2378\n2378            sub_msg->narg++;\n(gdb) bt\n#0  redis_fragment_argx (r=0x806450, ncontinuum=4, key_step=2, frag_msgq=<optimized out>) at nc_redis.c:2378\n#1  0x000000000040e878 in req_recv_done (ctx=0x63e090, conn=0x6460c0, msg=0x806450, nmsg=<optimized out>) at nc_request.c:613\n#2  0x000000000040dae9 in msg_parsed (msg=0x806450, conn=0x6460c0, ctx=0x63e090) at nc_message.c:591\n#3  msg_parse (msg=0x806450, conn=0x6460c0, ctx=0x63e090) at nc_message.c:626\n#4  msg_recv_chain (msg=0x806450, conn=0x6460c0, ctx=0x63e090) at nc_message.c:681\n#5  msg_recv (ctx=0x63e090, conn=0x6460c0) at nc_message.c:714\n#6  0x0000000000409531 in core_recv (conn=0x6460c0, ctx=0x63e090) at nc_core.c:194\n#7  core_core (arg=0x6460c0, events=255) at nc_core.c:329\n#8  0x000000000041f39d in event_wait (evb=0x63e570, timeout=1000) at nc_epoll.c:269\n#9  0x0000000000409b29 in core_loop (ctx=ctx@entry=0x63e090) at nc_core.c:352\n#10 0x0000000000408e48 in nc_run (nci=0x7fffffffe080) at nc.c:530\n#11 main (argc=<optimized out>, argv=<optimized out>) at nc.c:579\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/376/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nikolay": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/354", "title": "Support for Disque", "body": "Not released yet, maybe there's no need to change anything except the documentation acknowledging that it's supported.\n\nhttps://github.com/antirez/disque\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/354/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "idning": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/353", "title": "should route to backend according to the key if server_connections > 1", "body": "in the case of `server_connections > 1`\n\nif one proxy receive 2 pipelined request:\n\n```\nset k v\ndel k\n```\n\nit has possibility that the `del` and `set` cmd will go to different server_connection.\n\nand `del` may executed before `set`, so after this pipeline returned, we may find that the command in the pipeline is out of order. \n\nthis will be a problem especially in the case of import lot's of data using pipeline. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/353/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/350", "title": "core on mac", "body": "most cases works well on mac, but not thisone:\n\nin a `kevnet()` call, it may trigger more than one time for a single fd::\n\n```\n[2015-04-23 14:24:16.949] nc_kqueue.c:276 kevent return 1\n[2015-04-23 14:24:16.949] nc_kqueue.c:287 kevent 0021 with filter -1 triggered on sd 11\n\n[2015-04-23 14:24:16.949] nc_kqueue.c:276 kevent return 3\n[2015-04-23 14:24:16.949] nc_kqueue.c:287 kevent 8021 with filter -2 triggered on sd 8\n[2015-04-23 14:24:16.949] nc_connection.c:439 sendv on sd 8 failed: Broken pipe\n[2015-04-23 14:24:16.949] nc_core.c:237 close s 8 '127.0.0.1:2100' on event FF00 eof 0 done 0 rb 0 sb 0: Broken pipe\n[2015-04-23 14:24:16.949] nc_kqueue.c:287 kevent 0000 with filter 0 triggered on sd 8\n[2015-04-23 14:24:16.949] nc_kqueue.c:287 kevent 8021 with filter -2 triggered on sd 8\n0   nutcracker                          0x0000000106ae62b3 nc_stacktrace_fd + 51\n1   nutcracker                          0x0000000106ae3945 log_stacktrace + 53\n2   nutcracker                          0x0000000106ae1cb1 signal_handler + 481\n3   libsystem_platform.dylib            0x00007fff844f0f1a _sigtramp + 26\n4   ???                                 0x000000a900000000 0x0 + 725849473024\n5   nutcracker                          0x0000000106ac8980 core_core + 32\n6   nutcracker                          0x0000000106afe021 event_wait + 673\n7   nutcracker                          0x0000000106ac9240 core_loop + 32\n8   nutcracker                          0x0000000106ae780a nc_run + 58\n9   nutcracker                          0x0000000106ae6e98 main + 296\n10  libdyld.dylib                       0x00007fff84e6c5c9 start + 1\n[.......................] signal 11 (SIGSEGV) received, core dumping\n```\n\non the first trigger, the fd 8 is closed and unrefed.\n\nbut on the second triger, we are using a freeed conn object! so we got a core.\n\nthis can reproduce with the test case::\n\n```\nnosetests -v test_redis.test_mget_mset:test_mget_on_backend_down\n```\n\ni'm not familiar with kqueue. anyone can help?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/350/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/222", "title": "setup travis-ci with test-twemproxy", "body": "on travis-ci, it will automatic run https://github.com/idning/test-twemproxy. I have use it in `idning/twemproxy`\n\nhere is some building history: \nhttps://travis-ci.org/idning/twemproxy\n\nsee this patch:\n\nhttps://github.com/idning/twemproxy/pull/1/files\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/222/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/216", "title": "Error response is not forward to client on redis mulit del command", "body": "if one of the redis server is read only slave:\n\n`del key1` will got Err: \n\n```\nREADONLY You can't write against a read only slave.\n```\n\n`del key1 key2 key3` will got Err:\n\n```\nInvalid argument\n```\n\nhere is the test case:\n\n```\ndef test_multi_delete_on_readonly():\n    all_redis = [\n            RedisServer(USER, '127.0.0.5:2100', '/tmp/r/redis-2100/'),\n            RedisServer(USER, '127.0.0.5:2101', '/tmp/r/redis-2101/'),\n        ]\n    for r in all_redis:\n        r.args['cluster_name'] = 'ttt'\n        r.args['server_name'] = TT('redis-$port', r.args)\n        r.deploy()\n        r.start()\n\n    nc = NutCracker(USER, '127.0.0.5:4100', '/tmp/r/nutcracker-4100/', all_redis)\n    nc.args['cluster_name'] = 'ttt'\n    nc.deploy()\n    nc.start()\n\n\n    all_redis[0].slaveof(all_redis[1].args['host'], all_redis[1].args['port'])\n    r = redis.Redis('127.0.0.5', 4100)\n    keys = ['key-1', 'key-2', 'kkk-3']\n\n    r.delete('key-1') #got READONLY You can't write against a read only slave.\n    r.delete('key-2')\n    r.delete('key-3')\n    print r.delete(*keys) #got Invalid argument\n```\n\nhere is the code make this different:\n\nhttps://github.com/twitter/twemproxy/blob/master/src/proto/nc_redis.c#L2063-L2074\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/216/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/1f4e0dae62baa379209e4cdfe64f2224dadce632", "message": "Merge pull request #410 from vincentve/master\n\noptimize performance when single key del"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/6c618baa5b93acdd5c8b4e1a3789f5a90d11662e", "message": "add test for \"mset key\""}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/bf9803bf4c2525542a6a1e070105dd90a944a34b", "message": "Merge pull request #361 from charsyam/feature/bugfix\n\nfix crash when mset has invalid pair"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/786c6d4345e98d8f2bdb9a3e63cd666fc6f57615", "message": "add testcase for issue_323"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/8ebc32bb8e782b260491f804b4a7925dfb498450", "message": "Merge branch 'charsyam-feature/issue-323'"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/23da7de6d3f685f6d5582824299bb2bf9f1252da", "message": "Merge branch 'feature/issue-323' of https://github.com/charsyam/twemproxy into charsyam-feature/issue-323"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/8c8fd05df5446dfad6404c978d1d4057dacded32", "message": "clean on test setup"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/4ac15978d956a4a696d40b5a76412075058888c5", "message": "add update travis.sh"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/53fbeecf90b0a59d80a5fc329f3f7c96f1d103cb", "message": "add -v for nosetests"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/93c538fa47db4607c52a750f2584d2453b95a8eb", "message": "update travis.sh"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ecb21543e2f4251a20907b08790f5501f98d718f", "message": "update travis.sh"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/52ad27e75e85fa4d7b76626724e2ea5ba0bba769", "message": "add test-twemproxy to travis"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/cfbc0bbf653e5f32f4f0ae84cd90cd1a79a88f79", "message": "temp fix a core on kqueue,\n\nsee: https://github.com/twitter/twemproxy/issues/350"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/0dbb3a915d746d8b4fb625c58daf1583968e5ed2", "message": "Merge pull request #311 from atdt/socket-perms\n\nAllow file permissions to be set for UNIX domain listening socket"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/a089c9ea5b499e79070caf10ee4548d24ee264b3", "message": "fix req_make_reply on msg_get, mark it as response"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/161b49cb7094efda5a4a7775468de45ee1ae844e", "message": "Merge pull request #289 from charsyam/feature/verbose\n\nchange verbosity option to verbose"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/e4908d73ca8758e8e3bab27b165ec640f7dec908", "message": "update max-passwd-length in redis.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ed7c9b931e60d7d3fd32e45e4974d08a7973884c", "message": "use constant for redis reply"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/bf7b239555406668abfd16941ea76beef3035360", "message": "Merge branch 'auth'"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/e5368c39e6499cac6532ac330688f0a6d0bc001a", "message": "naming, error chk, and remove useless code"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/6788a21f958c1c04ac630893fbdaa19e0fe43dbf", "message": "Merge pull request #265 from guilhem/ppa\n\nAdd PPA informations to README"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/0c2308ed4072418d7b80c512dfb116baf2017f17", "message": "allow null key(empty key)"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/f722820dc77890154c7d3ec281fb82bb9e1bcaab", "message": "Merge branch 'master' of github.com:twitter/twemproxy"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/1f17bc18543989b24cb7b4ad743a70f335fe62fd", "message": "add GCC option -fno-strict-aliasing\n\nsee: https://github.com/twitter/twemproxy/issues/276"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/155262b0718d30cdeac53669382a6595db5b6163", "message": "Merge pull request #274 from rhoml/rhoml/add-debian-init\n\nAdds init script for debian servers"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/eceae63f85e2f17ee7f45b76447767c2c392b070", "message": "Merge pull request #268 from ngaut/master\n\nfix redis.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/591b25eaabb3a2bd345a3ce7800fe5728e743dd1", "message": "fix core on invalid mset like \"mset a a a\""}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/1d61c594dde56bdaea8edab5ced8fe8bea49c0f0", "message": "update ChangeLog"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/533ab483d73b31c41dbb9eaa6164900551824c39", "message": "update version 0.3.0 => 0.4.1"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/91fa10bce4ff976cc73aae2e3aec16d39a4d9043", "message": "Merge pull request #255 from idning/ping_quit\n\nping & quit supported"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/4175419288ef66d95e082cfa2124e77fe6d4fe6d", "message": "ping & quit supported"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/c19c9f26bb9a732826c2d1e340c06299789c35df", "message": "Merge pull request #263 from areina/add-support-for-sort-command\n\nAdd support for SORT command in Redis."}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/b72c722ec430dbef0912402f15d98150eeef2b18", "message": "Merge pull request #254 from idning/xscan\n\nhscan/sscan/zscan supported"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/efece2e242c9d30a6b8eff698e9aa25757b82d83", "message": "update comment"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/1e3a5d092e1c807a5068c3798c48a56a3e1d8902", "message": "handle xscan replay"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/6e172fcec4b4a62782ffb36da307280a2e3a88b5", "message": "add hscan/sscan/zscan cmd, but the rsp parse need upgrade"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/f6cc55bc8cd27bd4c47b8b60c048d65449f2fb6d", "message": "Merge pull request #210 from idning/mget-improve\n\nMGET improve \r\n\r\nMerge pull request #210 from idning/mget-improve"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/257b564a6037524348364ea478373d0f8ea3aaaf", "message": "use ms in _log()"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/8e719c054f3d273f95d5a1a0faab197bab55526a", "message": "use msg_append to avoid small mbuf fragments"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/da2170f09a9e11bdf1d487bc19b323b9cf089f9b", "message": "use array(keypos) to trace all key positions (start and end) in the message struct,\nand del key_start/key_end."}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/191cdcf72995ddec86fbab9728437e5ab02f4696", "message": "modifies base on Manj's review advise."}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/79059c7f841b5a90d2073853e86f1c6cc8380515", "message": "formating & compile warning"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/4a99a4c1ff4f49f831f436693e4e0385e5711cd7", "message": "remove dead code (pre/post splitcopy)"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/f89c7d9eae93855c3098e805c65ea88d0fbb9834", "message": "error handling on fragment/coalesce"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/1f0a133d41db3314d47ac49219878d7f734f485e", "message": "remove dead code\n\nremove:\n- msg_fragment\n- MSG_PARSE_FRAGMENT:\n- first_fragment/last_fragment"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/576ff39645f35e48638acf57ac263166de4dc3ac", "message": "formating"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/122e9e64c65339c4de87b2188e82bdb8e7f6c468", "message": "reduce mem usage in fragement"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/2ccd2858b7614d0ee77756db2f2ef835ab271159", "message": "put hash_tag logic into server_pool_idx"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/de902c117ce148d8c1ce9074e5268065d85d406a", "message": "add msg->fragement handler, main logic of mget-improve is put into proto/"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/a1ca20134703fcc006292c4e398e6e4e0133241a", "message": "adjust style"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/cbe547ec87bd609f1ea9e52edb5a344a297d7cc0", "message": "MSETNX should not support\n\nMSETNX Return:\n\n- 1 if the all the keys were set.\n- 0 if no key was set (at least one key already existed)."}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/d6e27980c547850139bdb7ce286665a1c694ba41", "message": "1. do not rewrite the orig mget as a ping msg for redis, and del useless code\n2. nfrag_done work for memcache"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/1c9229896e97e8f3ff32e628806c3b243698f280", "message": "1. get/gets for memcache passed cases\n2. fix req_done(nfrag_done) for memcache"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/68860c98df3c41fa966e94aac883faddc30771fb", "message": "fix bug on msg_fragment_argx when key_len=mbuf_size-1"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/a909fcd530d5b75be7d2f53d93b1bc4b576c8e2a", "message": "enhance error handling on backend down"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/9eda07d237d93a75f3c02766b3190e81be718d40", "message": "follow the coding style"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/50fd57aa307f9de08176279733c236a00677a300", "message": "MSET/MSETNX support now"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/9556a70d4bc5229d54987af73e7e5362d88526c6", "message": "add benchmark code"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/3064dc29e01646ba4eb4715946610cbaf6c735cd", "message": "fix multi DEL, a bug on parse key, and remove assert on conn_sendv"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/9739f3a40c24173fadf53d943be2efa532606426", "message": "After rebase: mget split to many mget and send to backend\n\nreabse following::\n\n    * 0fd32bb - (HEAD, idning/mget-improve, mget-improve) unchange (10 hours ago) <idning>\n    * 705acab - clean code (10 hours ago) <idning>\n    * 32e8559 - correct, but not clean. (18 hours ago) <idning>\n    * fde1f12 - mget-improve almost done. (2 days ago) <idning>\n    * b30fa4f - mget split to many mget and send to backend (3 days ago) <idning>\n    * 7cf5625 - add frag_owner counter (4 days ago) <idning>\n\n    * 8a4f5c0 - (origin/master, origin/HEAD, idning/master) Update README.md (12 days ago) <Manju Rajashekhar>"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/4801705e55e10b120a2fdaa2a8e3ed4ece7d8114", "message": "add frag_owner counter"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/81cb8e26a11cce2ee46c8e17cb21357cee952265", "message": "change all loga to log_safe in signal handler."}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/59dd2a9dd9509b68b3373642b9ffadd184a7e05a", "message": "add safe_snprintf"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ecfad5ff1d8635d601ae177305a5e8f9ddf71e9d", "message": "more fix for sighandler"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/d638039e48bdcbff47e0f031664f056d93419d0b", "message": "fix deadlock in sighandler"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/1a8ba757b58392bbaa6389d45bf200b3211382fa", "message": "notice-log upgrade\n\n1. add peer ip:port on notice log\n2. fix missing of notice_log in pipeline"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/387389b2b4a648a2eb2710560815c4d34a6aabd8", "message": "adjust coding style after Manju's review\n\n1. use MSG_TYPE_CODEC instead py script,\n2. del colon in notice log"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/5550942b185e70d293e972e023b5e9e69d86f527", "message": "move notice to a function"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/9000ef3cab658f49f2e8c6cdea3e304d51d3ba45", "message": "add notice-log for every request:\n\n[Wed May 28 15:06:12 2014] nc_request.c:59 req 3 done on c 11 req_time: 0.353 type: SET narg: 3 mlen: 35 key0: kkk-3, done: 1, error:0"}], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/257", "title": "hot-reload", "body": "how it works:\n1. when we receive USR1 signale, we will fork and exec with new binary and new config.\n2. the new process will inherited all listen-socket from the old process, including:\n   - stat socket\n   - listen socket for each pool\n3. after the new process is already running, the old process will close all listen socket.\n   and wait for 3 seconds (for those who has already connected with old process)\n   3 seconds later, it close all client socket and shut down\n   \n   (however, the keep-alive connectin will be closed, we can not wait forever)\n\nTODO:\n- make it work for `unix domain socket`\n- need test on nc_kqueue\n\ntest cases:\n\nhttps://github.com/idning/test-twemproxy/blob/master/test_system/test_reload.py\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13062350", "body": "if current msg is a sub-msg, the `msg->frag_owner` may be freed, and reset by `_msg_get`, this is not correct, especially for `req_error`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13062350/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14004378", "body": "macro create clean code but break cscope/ctags :( \n\nclean code is more important, I will have a try.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14004378/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326415", "body": "we may see something like this:\n\n```\n[Sun Jun 29 05:39:03 2014] nc_redis.c:1527 parsed req 13 res 0 type 24 state 0 rpos 124 of 124\n00000000  2a 31 31 0d 0a 24 33 0d  0a 44 45 4c 0d 0a 24 35   |*11..$3..DEL..$5|\n...\n00000060  6b 6b 2d 34 0d 0a 24 35  0d 0a 6b 6b 6b 2d 39 0d   |kk-4..$5..kkk-9.|\n00000070  0a 24 35 0d 0a 6b 6b 6b  2d 38 0d 0a               |.$5..kkk-8..|[Sun Jun 29 05:39:03 2014] nc_util.c:228 malloc(16) at 0x11a3210 @ nc_message.c:625\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326415/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326419", "body": "for dump larger nc_msg.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326419/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14436271", "body": "get_mbuf is also used in twemproxy_auth_and_select branch. I think we will merge it later. so I keep this a function. \n\nhttps://github.com/twitter/twemproxy/blob/twemproxy_auth_and_select/src/nc_request.c#L383-L400\n\nand we got smaller `msg_make_reply` function body.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14436271/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14436425", "body": "this function is copy from https://github.com/twitter/twemproxy/blob/twemproxy_auth_and_select/src/nc_request.c#L403\n\nin `auth-select` branch, we can reply immediately, so `event_add_out` is called.\n\nin mget case, we use this function to prepare a reply mgs, then we do real msg dispatch.\nso we can not enable EV_OUT.  \n\nI comment it to let us remember this when we merge these two branches.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14436425/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14436849", "body": "the hash_tag logic is implement here now: \nhttps://github.com/twitter/twemproxy/blob/twemproxy_auth_and_select/src/nc_request.c#L589-L606\n\nI think it's a good idea to move both `nc_message.c#482-495`  and  `nc_request.c#L589-L606` into server_pool_idx:)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14436849/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14436984", "body": "yes!  \nto many waste of `malloc` calls here, \n\nI will optimize this. this can make us faster on small mget with lots of backend !\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14436984/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14437129", "body": "there is no 'last_fragment' now:)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14437129/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/20070477", "body": "you can not compare p against \"quit\" like this, it's maybe in two mbufs.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/20070477/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/22346151", "body": "we do dup here and in `core_cleanup_inherited_socket` we will close all FS in NC_ENV_FDS.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/22346151/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "jhmartin": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/352", "title": "Selectively disable functionality", "body": "It would be useful to be able to disable certain redis commands, such as to make a Redis end-point effectively insert-only (for example as a target for Logstash clients in less-trusted network zones). In this example I would only need RPUSH to work, and all others would be disabled.\n\nThe applications needing to perform other operations would access Redis via a separate network path.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/352/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jonhattan": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/338", "title": "Keep rpm spec in sync with releases", "body": "[scripts/nutcracker.spec](https://github.com/twitter/twemproxy/blob/master/scripts/nutcracker.spec) references version 0.3.0. I was to propose a PM to update to 0.4.0 but it may be available in 0.4.1. \n\nSomehow it needs that maintainers updated the spec file before releasing.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/338/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "NickMeves": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/337", "title": "Hashing Algorithms Sending everything to Different Nodes than Before", "body": "Using fnv1a_64 with modula.\n\n5 days ago we had a network outage on our switch/router (unannounced maintance, yay!) for 15ish minutes.\n\nSince networking has come back up, Nutcracker is hashing all keys (or subsets of the keys in the hash tag) to different nodes.  If before key foo always went to node1, now it is always going to node2.\n\nI have tried restarting the cluster, realigning which backend nodes are master/slave, recompiling nutcracker -- all keys all still hashing to the new nodes, rather than the previous proper nodes.  This is using named nodes in the nutcracker conf generated by redis-mgr.\n\nAny ideas on how to get hashing to go to back to how it was before? Or do I need to aof-replay everything and just accept the new hashing that is happening?\n\nI'm still perplexed how every key is all of the sudden hashing to new nodes.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/337/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ghost": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/303", "title": "show current backend status in json statistics (feature request)", "body": "Hi.\n\nI`m setting up my monitoring for twemproxy, and find, that there is no way to determine,\nif backend alive now or not...\n\nSo, i think it would be very useful, if you add field ('alive', for example) for current backend status in json statistic, like this:\n\n``` bash\n  \"172.16.76.132\": {\n\n    \"alive\" : 1,\n\n    \"server_eof\": 0,\n    \"server_err\": 0,\n    \"server_timedout\": 0,\n    \"server_connections\": 8,\n    \"server_ejected_at\": 0,\n    \"requests\": 1709,\n    \"request_bytes\": 224692,\n    \"responses\": 1709,\n    \"response_bytes\": 7628058,\n    \"in_queue\": 0,\n    \"in_queue_bytes\": 0,\n    \"out_queue\": 0,\n    \"out_queue_bytes\": 0\n  },\n\n```\n\nIf i understand correctly how twemproxy works, logic may be like this:\n\nalive:       (now - ejected_time) >= server_retry_timeout\nnot alive: (now - ejected_time) <   server_retry_timeout\n\nThanks in advance!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/303/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "andyqzb": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/297", "title": "A patch makes twemproxy work with redis-sentinel", "body": "I\u2019m doing a patch for twemproxy. It makes twemproxy work with redis-sentinel to auto detect redis instance failover, and change its forward address.\n\nThe design of the patch is shown below.\n1. configure sentinels in configuration like servers.\n   \n   Sentinel address can be configured one or more like servers. Twemproxy pick one, and connect.\n2.  fetch redis addresses and maintain it consistency with sentinel\n   \n   twemproxy will send info sentinel and subscribe requests to sentinel when it connects to sentinel.\n   twemproxy update servers\u2019s addresses from info sentinel response, and fetch redis failover event from subscribe channel. So twemproxy can maintain the consistency of redis addresses with redis-sentinel.\n3. sentinel reconnect\n   \n   Twemproxy will pick an sentinel to reconnect When sentinel connection is done.\n   A different sentinel will be picked if multiple sentinels are configured. So twemproxy can switch to a good sentinel when some sentinels are done.\n   Twemproxy will send info sentinel and subscribe requests to sentinel when the new connection is established, just like it connect to sentinel at the first time.\n4. Identify the same redis between proxy and sentinel\n   \n   We configure the servername in the proxy as same as the master-name configured in sentinel. The servername is the redis identification between proxy and sentinel.\n   For example, configuration in proxy:\n   \n   ```\n   servers:\n    - 127.0.0.1:6379:1 server1\n   ```\n   \n   configuration in sentinel:\n   \n   ```\n   sentinel monitor server1 127.0.0.1 6379 2\n   ```\n5. some small feature\n   \n   Twemproxy will dump a new configuration when it changes the redis address. It can let user know the status in the proxy and avoid some problem when proxy is restarted.\n\nThat's design of the patch. I\u2019m glad to hear your advices about the patch.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/297/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/241", "title": "msg leak in reply function of twemproxy_auth_and_select branch", "body": "  The reply function maybe failed because of ENOMEM, while the return value of the function is void. So we don't do anything when it failed in req_filter, this will lead to request msg leak.\n  Another problem in reply is we don't check the return value of event_add_out, while event_add_out maybe failed.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/241/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/330f43a430261aa48d4063771ed70fe191177154", "message": "Merge pull request #486 from deep011/deep011-patch-1\n\nfix a memory leak bug for mset command"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ced2044980e4b9dd0c91b25fc64ce127879a1491", "message": "Merge pull request #484 from postwait/patch-1\n\nFix typo circunous -> circonus"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/a6b28c610d386837da24a59f0768b89a4ad22265", "message": "fix the bug in string_compare"}], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/324", "title": "twemproxy work with redis-sentinel", "body": "Hi, @manjuraj @idning , I made a patch for twemproxy to let it work with redis-sentinel.\nThis patch is implemented as the design mentioned in issue 297.\n\nIn addition, I add keepalive to the sentinel pub/sub connection. Because sentinel pub/sub connection don't have heartbeat, we must add keepalive to check if the connection is dead. The implemention of nc_set_keepalive is copied from redis.\n\nYou can configure a server_pool like sigma in conf/nutcracker.yml to use this feature.\n\nI hope to hear your feedback about this patch.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/76415609", "body": "What is the intention of this line?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/76415609/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/25775677", "body": "@mishan You can configure any number of sentinels no matter how many sentinels you use. Becase twemproxy just need connect to one of them to fetch the redis address. \n\nOf cause, configuring all the sentinels you use is better. Because if some of the sentinels are dead, twemproxy can connect to the alive sentinels.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/25775677/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/25776456", "body": "To avoid being confused, I added one sentinel address to the example configuration.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/25776456/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/27093408", "body": "@TheRealBill In the partition, if twemproxy can only connect a part of the sentinels, and some told you new master, some told you old master. Twemproxy don't know whick is right if the both two sides don't have instances over half of all. Another, the design of sentinel doesn't work well in network partition, so I think take consideration of network partition doesn't make much sense.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/27093408/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/27143045", "body": "@TheRealBill  Thanks for your suggestions.\nFor the first suggestion, twemproxy should connect to all the sentinels. I think we shouldn't make twemproxy too complicated. It's just a client which supports sentinels. The problems in network partition should be solved by sentinel. For example, if the minority of sentinels don't server the clients, twemproxy will connect to a sentinel sunccessfully until it find a sentinel in majority part. Doing this in sentinel is simpler than doing it in twemproxy.\nThe podname suggestion is a good idea. I have thought to make mastername in sentinel as a mixed type like poolname-servername. So we can manage all the servers in different pool in the same sentinel. But I think the idea of specifying each server a different sentinel pool is not needed. I don\u2019t think there are people want to config one pool's servers into different sentinel pools.\nThe weight in sentinel config is not used. I just want to reuse the code of loading server config in sentinel config load. Modifying the code to drop weight in sentinel load is better.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/27143045/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/27409140", "body": "@TheRealBill I have seen the guidelines of sentinel clients. It said client just need to connect one of the sentinels just like what the patch does. Just like what the guidelines said below, It won't work well in the network partition.\n\n> Note: it is possible that a stale master returns online at the same time a client contacts a stale Sentinel instance, so the client may connect with a stale master, and yet the ROLE output will match. However when the master is back again Sentinel will try to demote it to slave, triggering a new disconnection. The same reasoning applies to connecting to stale slaves that will get reconfigured to replicate with a different master.\n\nI think twemproy shouldn't do too much things about distributed system. Sentinel is responsible for the cluster. So if you want the cluster works well under the network partition, the sentinels of minority should refuse to server the clients just like zookeeper or chubby.\nI have seen the redis issue 2257. I think it's a suggestion to reduce the connections of sentinels. Sentinel is ok when it has 2000+ connections(under the issue condition). The epoll can process tens of thousands of connections easily. Maybe It's a problem when the cluster has thousands of redis.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/27409140/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "72squared": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/271", "title": "redis cluster support?", "body": "redis cluster is in RC1 but sadly many of the clients are nowhere close to being able to support it. I've been helping out on getting a python client into shape:\n\nhttps://github.com/Grokzen/redis-py-cluster/tree/unstable\n\nHowever it seems to me that if twemproxy supported redis cluster, then any number of existing clients could connect over localhost to twemproxy and then twemproxy could do the work of tracking redis cluster state and packing the commands to the correct nodes in the cluster to improve efficiency.\n\nIs this something you might consider supporting in the future?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/271/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "diegoroccia": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/262", "title": "replicas", "body": "Hi\nis it  possible to have replicas of keys?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/262/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "willshulman": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/249", "title": "Question about consistent hashing ", "body": "Let's say I want to use nutcracker as a redis proxy for a set of persistent-less redis nodes that I will use as a cache. Let's also say I use a consistent hashing scheme. \n\nMy understanding is that if a node leaves the cluster the hashing algorithm will result in keys that were on the original node eventually being stored elsewhere. But what happens when the original node comes back (let's say it was just a temporary network partition)? How does nutcracker now deal with the fact that there may be the same key more than one node in the ring?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/249/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "danielgpm": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/245", "title": "Saving the name information on all redis nodes to query them inside redis function", "body": "Is there a way to save the common data (E.g. \"product info\") on all nodes using twemproxy?\nThe idea is having a particular data (not a huge amount) always available not matter if only 1 node remains alive on the cluster.\n\nFor example:\nLet's assume that all the data except \"product info\" are sharded using the  {subscriber_xxxx}\nBut If I do something like: \n\n``` lua\nset {all-nodes}:product:xx \"product's values...\"\n```\n\nThis \"product:xx\" would be saved on all nodes.\nThis way if  I have the following function (on a sharded node), this function will be able to query the product information even though the node only contains certain subscribers (as defined by the hashing)\nE.g: \n\n``` lua\nredis.evalsha(get_sub_and_prod_sha, :keys => [\"{subscriber_xx}\"], :argv => [\"{all-nodes}:product:xx\"])\n```\n\nIs this possible? Any ideas?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/245/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "teknogecko": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/233", "title": "Support SELECT / WATCH command", "body": "Hi, I've tried to get this proxy going with our current setup but ServiceStack v3 says SELECT is not supported and Harbour.RedisSessionStateStore needs WATCH.\n\nWe use SELECT to isolate certain data from others. A workaround would be to put is on a separate instance entirely.\n\nI don't know why Harbour needs WATCH and if it is even compatible with Sharding. But it would be really nice not having to rewrite it's code to get it done. :)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/233/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shibo0305": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/227", "title": "why some listening ports gone?", "body": "Hello, manjuraj:\nwe use nutcracker for redis proxy. However, sometimes we found that several ports which nutcracker is listening suddenly gone, It seems that nutcracker not listening on those ports any longer! Then we have to restart nutcracker. \nCould you tell me the possible reason for this problem and how to track and solve it? Thanks very much!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/227/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bitthegeek": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/223", "title": "Support for vBuckets", "body": "I have read about vBuckets and find it really interesting. _(See http://dustin.sallings.org/2010/06/29/memcached-vbuckets.html)_\n\nSome benefits of using vBuckets:\n- scaling up and down at will\n- high availability (no cache misses when a redis server is down)\n- no need for \"even\" server capacities\n\nIt would be really ~~great~~ awesome if twemproxy can support vBuckets as well.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/223/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/240", "title": "Added specifying ordinal hash tag positions", "body": "if the start hash tag character is the same as the end hash tag character, only the first hash tag will be used\n\n```\nhash_tag: __\n1_2_3_4 -> 2\n```\n\nApplying ordinal hash tag positions, you can get the nth hash tag if multiple hash tags exist, reverts to whole key if nth hash tag is not found\n\n```\nhash_tag: __\nhash_tag_pos: 2\n1_2_3_4 -> 3\n\nhash_tag:__\nhash_tag_pos: 2\n1_2_3 -> 1_2_3\n```\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "allenlz": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/221", "title": "stray message on server connection if \\0 in the key for memcache", "body": "Example:\necho -en \"set a\\0 0 0 1\\r\\na\\r\\n\"\n\n\\0 is invalid in the key for memcache. If \\0 is included, memcache will return 'ERROR' before continue to read the data in the connection. It will cause twemproxy to find stray message. This case is much like #149 .\n\nIn most cases, client side should do the validation for the key.\nHowever, its also needed to check the key again in twemproxy. Because, if twemproxy find stray message, it will close the server connection. 'Doing so would affect other clients that have request pending/outstanding on this given server connection.'\n\nThe fix is simple.\nhttps://github.com/allenlz/twemproxy/commit/5fddf251919a402f30690651ff20439c3fbbf34d\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/221/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/b854e1ed4fd3d7e488e6cd38e0d936f532b3ca68", "message": "fix to handle max open file limit"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13274133", "body": "All fragements will be freed at the same time if some error happens (`rsp_make_error`). If there is no error, they will also be freed at the same time.\nIf I can't believe `frag_owner` is valid, which you think it may be a wild pointer, that will be a bug of twemproxy upstream.\nWill it happen?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13274133/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "matthewdresden": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/220", "title": "Can this be load balanced for redundancy?", "body": "If so, can a general high-level recommended route be posted here? I will post how I did it in detail after I have it working. I have access to F5 LTMs, just about anything else.\n\nI am building a clustered demo of LogStash with redis clusters for my Company Cengage Learning in an effort to  offset or replace the extremely expensive Splunk tool we are currently using.\n\nI am not looking for step by step how to, just a path that the developers think would work or better yet had tested.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/220/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nainam": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/215", "title": "Twemproxy addition and deletion of servers in config file", "body": "How to automate twemproxy config to add or delete servers in the list when an autoscale up or down happens.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/215/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mezzatto": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/213", "title": "auto_eject_hosts mode design", "body": "Background:\n- modula distribution\n- using twemproxy in front of redis servers that are acting as databases, not as caches\n- data is sharded between these redis servers\n- master / slave configuration\n- application is 99% reads, almost no writes\n- client balances the load between the masters and the slaves instances. For every request the client chooses if it goes to the master or to the slave pool in twemproxy (50/50 weight)\n\nIf one or more redis server goes offline and I choose the twemproxy pool that has the failing redis server and the key is hashed to this redis server, I have to wait the timeout before I get my reply and try the other twemproxy pool. Every request that goes to this redis server has to wait. This is bad... and I cannot use auto_eject_hosts since it changes the shards / hash ring.\n\nI though about implementing a new config, called `auto_eject_drop` with values `true` (current behavior, default) and `false` (my proposal) that lets you:\n- when a server fails, it wont decrement the live servers of the continuum (this way the shards wont change)\n- immediately replies with \"ERR timeout\" if some key tries to ask a failing redis server\n\nThe way to do that is maybe changing req_forward() so that if a faling server got asked, the message will not be enqueued and. I dont know if this is the best solution but is the one that has minimal code change.\n\nAny thoughts?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/213/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/99", "title": "\"server_connections\", in stats, returning a negative value", "body": "The \"server_connections\" fields, in stats (port 22222), is returning a negative value. What does it means?\n\nExample:\n\n```\n\"Backup_05\": {\n  \"server_eof\": 2,\n  \"server_err\": 0,\n  \"server_timedout\": 1,\n  \"server_connections\": -3,\n  \"requests\": 49175590,\n  \"request_bytes\": 2505043890,\n  \"responses\": 49175576,\n  \"response_bytes\": 10950549909,\n  \"in_queue\": 0,\n  \"in_queue_bytes\": 0,\n  \"out_queue\": 0,\n  \"out_queue_bytes\": 0\n},\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/99/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/84", "title": "Detailed stats", "body": "Before nutcracker, my system was able to generate some detailed info of each server, apart from the stats that nutcracker already generate. They were (everyone related to the past hour requests):\n- total time that we waited for a redis response\n- average time of the requests\n- slowest request (with date)\n- fastest request (with date)\n- moment of the peak request (with the date and the amount of requests)\n- time of the requests in \"buckets\" (< 50us, >=50us < 100us, >= 100us < 150us etc)\n- size of the requests in \"buckets\" (< 128 bytes, >=128 bytes < 256 bytes, etc)\n\nDo you think that these stats would be possible to be generated by nutcracker? Would you accept a pull request with this implementation?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/84/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/214", "title": "New option \"auto_eject_drop\"", "body": "A boolean value that controls if auto ejected hosts should be dropped from the hash ring. If set to false, failing hosts will immediately reply timeout. Defaults to true.\n\nSee https://github.com/twitter/twemproxy/issues/213 for more information\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9613884", "body": "Thanks! Will test it ASAP\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9613884/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9709936", "body": "Hey Manju, now it works!\nThanks for the fix.\nClosing...\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9709936/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10127371", "body": "+1 for this request, but being done in the background...\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10127371/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13904589", "body": "Whoa... I got the exact same implementation :)\nYour pull request laked the actual use of the crc16 hash\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13904589/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13904795", "body": "Right. It is the crc16 that we use.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13904795/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14251003", "body": "I ran some tests some time ago and using TCP sockets gave me a little performance overhead compared to local sockets.\n\nMy Redis setup used the answer every request within 180 microsecs with RH EL 6.0. Changed to local sockets and the requests time went to 150 microsecs. Updated to RH EL 6.3 and the timing increased to 170 microsecs. Put twemproxy in the middle and the timing went to 230 microsecs (connecting to twemproxy via TCP sockets). My hope is that connecting to twemproxy via local sockets (still in my todo list) and with twemproxy connecting to redis through a local socket can decrease the request's time to something around 200 microsecs.\n\nWhen you are fighting for the microseconds performance boost per request, every little thing counts...\n\nBy the way, my requests are 99% HMGETs\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14251003/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14262868", "body": "I did a crazy config like this:\n\n```\npool:\n  listen: 127.0.0.1:22121\n  hash: crc32\n  distribution: modula\n  redis: true\n  servers:\n   - 127.0.0.1:6379:1 Server0\n   - 127.0.0.1:6379:2 Server1\n   - 127.0.0.1:6380:1 Server2\n   - 127.0.0.1:6380:2 Server3\n```\n\nChanging the weight fools nutcracker into thinking they are different servers and let me use the configuration. But a unique TCP connection is opened for each server ip:port:weight tuple.\n\nIt seems that my only solution is if nutcracker would accept multiple names per tuple. Something like this:\n\n```\npool:\n  listen: 127.0.0.1:22121\n  hash: crc32\n  distribution: modula\n  redis: true\n  servers:\n   - 127.0.0.1:6379:1 Server0 Server1\n   - 127.0.0.1:6380:1 Server2 Server3\n```\n\nThis way, if a key maps to Server0 OR Server1, it goes to 127.0.0.1:6379 and Server2 OR Server3, it goes to 127.0.0.1:6380.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14262868/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14294753", "body": "As I understood, it is possible to configure twemproxy with the following config:\n\n```\n   distribution: modula\n   servers:\n    - 127.0.0.1:6379:1 Server0\n    - 127.0.0.1:6379:2 Server1\n    - 127.0.0.1:6380:1 Server2\n    - 127.0.0.1:6380:2 Server3\n    - 127.0.0.1:6380:3 Server4\n```\n\nBut that would gave me 5 TCP connections. I want just 2, since 127.0.0.1:6379:1 and 127.0.0.1:6379:2 are the same redis instance.\n\nI was able to get 2 TCP connections with something like this:\n\n```\n   distribution: ketama\n   servers:\n    - 127.0.0.1:6379:2 Machine0\n    - 127.0.0.1:6380:3 Machine1\n```\n\nBut ketama has a terrible performance compared to modula (something like 40%+ slower).\n\nThe problem here is that looks like I want a weighted modula. I have multiple Redis machines that runs a variable amount of redis instances each one.\n\nThat why I thought about multiple names to the same ip:port:weight tuple. Like this:\n\n```\n   servers:\n    - 127.0.0.1:6379:1 Server0 Server1\n    - 127.0.0.1:6380:1 Server2 Server3 Server4\n```\n\nBut I have no clue if it is possible to implement something like it.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14294753/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14294929", "body": "I did some tests with the following configs:\n\n1) Apache ->                           (NET) ->                                  Redis\n2) Apache -> TWEMPROXY_CRC16_MODULA -> (NET) ->                                  Redis\n3) Apache -> TWEMPROXY_CRC16_MODULA -> (NET) ->            Proxy_CRC16_MODULA -> Redis\n4) Apache -> TWEMPROXY_CRC16_KETAMA -> (NET) -> Machine -> Proxy_CRC16_MODULA -> Redis\n\nResults:\n\n1) 170us per req (14K TCP connections across the datacenter)\n2) 230us per req (4K TCP connections across the datacenter)\n3) 290us per req (4K TCP connections across the datacenter)\n4) 390us per req (550 TCP connections across the datacenter)\n\nI want 1) performance (or something close to it) with 4) amount of TCP connections :(\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14294929/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14295105", "body": "By the way, I have 32 Apache machines (3 httpd process per machine) and 17 Redis machines (total of 128 redis instances across those 17 machines)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14295105/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14306161", "body": "Can you please fix this? That would be great!\n\nIts basically \"pointer_per_server = server->weight;\" instead of \"pointer_per_server = 1;\", or something else need to be changed?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14306161/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14441019", "body": "No one. The request should fail with some error. The host is failing. You should have some redundancy (master/slave in Redis).\n\nSomething like 2 pools with 5 hosts on each. If a host in pool 1 fails and gets ejected, the client should retry on pool 2.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14441019/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14442055", "body": "That just makes sense if you are using memcached / redis as a caching layer, not as a storage.\n\nIf I wrote some data to a host, its only available in that host (and at some other copies, like a redis slave). If ketama sends me to another host when the original host fails, it is sure that I wont be able to retrieve my data.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14442055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14525318", "body": "Hey @manjuraj , I studied the code and this support seems pretty straightforward to implement. Would you accept a pull request if I implement it?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14525318/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14528132", "body": "https://github.com/mezzatto/twemproxy/commit/32d178856b7dcea8ee2987bfd1dbe28bc4a9b908\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14528132/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/1693973", "body": "This is awesome! Any known issues?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/1693973/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "siddo420": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/211", "title": "questions about HA and key distribution", "body": "For HA, I am thinking of setting up multiple nutcracker servers (that may be used together in parallel). All nutcracker servers may be connected to the same redis+memcached servers on the backend. My questions are:\n- Is this supported?\n- In the scenario above, if I plan on using data partitioning using hash_tag: \n  - Will partitioning work using any distribution mode or hash function or I have a limited set to choose from?\n  - If one or more nutcracker servers die (for any reason), will the remaining nutcracker server(s) correctly route clients to the right backend servers based on the hash_tag values? In other words, the hash function and distribution modes are enough to tell nutcracker which server to use or it stores some data locally about key distribution etc?\n\nPlease let me know\n\nThanks.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/211/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thedrow": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/198", "title": "Describe how automatic sharding works", "body": "twemproxy specifies that it supports automatic sharding but I can't see any documentation about it anywhere.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/198/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/197", "title": "Add a test suite", "body": "Currently twemproxy has almost no tests. I can see that it has some basic smoke tests at https://github.com/twitter/twemproxy/tree/master/scripts but it's not enough.\n\nFirst of all the smoke tests should run on each build. I don't see a walkthrough for running the smoke tests so it would be nice to have one so I can create a basic build process for twemproxy.\n\nSecond of all we should select a testing framework/tool. Functional black box testing is where we should begin so I suggest something BDD oriented. Maybe https://github.com/arnaudbrejeon/cspec which looks maintained.\n\nAfter we finish blackbox testing we can use something like http://cmocka.org/ for unit testing if there's any value in it.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/197/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/196", "title": "Add automatically generated documentation", "body": "It will help people to contribute if you generate an API reference.\nMaybe doxygen can be useful?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/196/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "neeleshkorade": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/158", "title": "mbuf_split() splits by copying data in quadratic instead of linear time", "body": "Hi All\n\nWe are looking to use TwemProxy with Redis for sharding. We have use cases where we may need to fetch about 10k keys in one go from across multiple shards. However, when I try this with TwemProxy on a test setup (described below), it takes about 1.7 seconds to return. If I fired the same request on a single Redis instance directly, it returns in about 16ms.\n\nWould someone know if I have missed out some obvious performance tuning option? Any ideas on what we could do to better this number? We need mget response time to be within sub 300ms range.\n\nNote that the hosts are shared with other processes (and not dedicated for my performance tests). I am giving the details below to give an indication of the leftover capacity.\n\nSetup Details-\nHost One- 8 core, 3.3 GHz (5% utilization during test run), 16GB RAM (6GB available) hosts two redis instances and a TwemProxy instance\n\nHost Two- 8 core, 3.3 GHz (5% utilization during test run), 16GB RAM (9.5GB available) hosts two redis instances\n\nTwemProxy config- \nhash: murmur\ndistribution: ketama\nservers:\n- host1:6379:1\n- host1:6380:1\n- host2:6379:1\n- host2:6380:1\n\nThank you\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/158/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "elfring": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/155", "title": "reserved identifier violation", "body": "I would like to point out that identifiers like \"[`_NC_CONNECTION_H_`](https://github.com/twitter/twemproxy/blob/a31b72202272a381df8bee1f3eb7328de23a4a67/src/nc_connection.h#L18)\" and \"[`_NC_MESSAGE_H_`](https://github.com/twitter/twemproxy/blob/d5dff38989c700a3eb66128b85764bfba052571d/src/nc_message.h#L18)\"  [do not fit](https://www.securecoding.cert.org/confluence/display/seccode/DCL37-C.+Do+not+declare+or+define+a+reserved+identifier#DCL37-C.Donotdeclareordefineareservedidentifier-NoncompliantCodeExample%28HeaderGuard%29) to the expected naming convention of the C language standard.\nWould you like to adjust your selection for unique names?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/154", "title": "Complete build options for Pthread API", "body": "Would you like to add the configuration script \"[AX_PTHREAD](http://www.gnu.org/software/autoconf-archive/ax_pthread.html)\" to [your build specification](https://github.com/twitter/twemproxy/blob/e0f939b5d2af6378b5a45840756d18e364f7c195/configure.ac#L77)?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/154/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/185", "title": "Complete build options for Pthread API", "body": "The configuration script \"[AX_PTHREAD](http://www.gnu.org/software/autoconf-archive/ax_pthread.html)\" should be added to the build specification so that corresponding special parameters will be taken into account.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/184", "title": "Make include guards unique.", "body": "Some include guards do not fit to the expected naming convention of the C language standard. This detail can be fixed by the deletion of leading underscores.\n\nThe probability for name clashes can also be reduced by the addition of a kind of universally unique identifier as a suffix.\n\nThis update suggestion corresponds to the issue \"[reserved identifier violation](https://github.com/twitter/twemproxy/issues/155)\".\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "harishd": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/152", "title": "Twemproxy + host discovery", "body": "Does twemproxy has any support for host discovery eg: serversets? If not any plans in the near future. \n\nCurrently we give the ip addresses in the config file. Rather we could give a serverset path.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/152/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dkong": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/148", "title": "Request: Allow multiple servers to use same node hash name", "body": "I'd like to have multiple distributed redis slaves associated with the same node name for a couple of reasons:\n1. Distribute read-only load across multiple instances\n2. Redundancy in case master goes down (one or more slave should be ready to be promoted)\n\nAn example topology with three different servers with 1 redis master and 3 redis slaves.\n\n```\nServer1\n\n---\nmaster1\nslave1a\n\nServer2\n\n---\nslave1b\n\nServer3\n\n---\nslave1c\n```\n\nProposed config file:\n\n```\nmasters:\n  listen: 127.0.0.1:22122\n  hash: fnv1a_64\n  hash_tag: \"{}\"\n  distribution: ketama\n  auto_eject_hosts: false\n  timeout: 400\n  redis: true\n  servers:\n   - 192.168.0.1:6379:1 server1\n\nslaves:\n  listen: 127.0.0.1:22123\n  hash: fnv1a_64\n  hash_tag: \"{}\"\n  distribution: ketama\n  auto_eject_hosts: false\n  timeout: 400\n  redis: true\n  servers:\n   - 192.168.0.1:6380:1 server1\n   - 192.168.0.2:6379:1 server1\n   - 192.168.0.3:6379:1 server1\n```\n\nOnce a redis key is hashed, twemproxy would have 3 servers to choose from (servers 1, 2, or 3).  At that point it could randomly choose, use round-robin, or some other load balancing scheme.\n\nCurrently twemproxy [rejects](https://github.com/twitter/twemproxy/blob/master/src/nc_conf.c#L1150) the config file when multiple servers have the same node name.\n\nAre there any gotchas or problems with this idea?\n\nDoes anyone have suggestions on the best approach to implement this?\n\nThanks.\nDara.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/148/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "therealbill": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/142", "title": "Feature request: sFlow source support", "body": "While this would likely be non-trivial, I would love to see nutcracker publish metrics via sFlow.  Probably the host flow route since it it isn't networking. Have the ability to sample commands just as the Nginx and apache sFlow modules can would be simply stellar. Especially when dealing with thousands of nodes and clusters. It would also provide a pure metrics source which could be easily integrated into various data collection and alerting/monitoring systems virtually our if the box as it were.  \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/142/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/37", "title": "Redis AUTH Support", "body": "There doesn't seem to be a mailing list (that I've found) so I ask here: are there any plans to support the Redis AUTH command? I've got a set of servers that I'd like to migrate to twemproxy but we need to be able to authenticate.\n\nI think the general use case would be to try the auth against any node in the pool and assume that any one of them can be considered authoritative. Thus it seems like a relatively simple addition, though there may be other concerns I've not considered yet (I've just run into this).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/37/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11741504", "body": "Functionally, yes it would work. However, it is in use for a PHP session backend (slightly modified phpredis) which handles the PHP sessions for thousands of different clients. Thus, if someone were to open a direct connection from within PHP to the Redis nodes they would have access to other clients' session data. Thus, the authentication (compiled into the modified phpredis module) prohibits that.\n\nThus functionally, yes, but it isn't something that can be allowed for non-technical reasons.\n\nI suppose I could write an auth proxy ....\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11741504/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11741563", "body": "<tongue in cheek>\nOr one could treat the auth argument as a key and hash on it. ;) \n</tongue in cheek>\n\nThat would cause a problem if the node it hashed to went down.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11741563/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11751858", "body": "The with proxy idea would essentially handle just the auth and do a direct proxy for all non-auth commands. Less than ideal but potentially workable. It would be ok if the twemcache to redis were unauthenticated in that case because the systems would be isolated from customer reachable nodes.\n\nUltimately, since the concern is potential unauthorized clients and authorized clients connecting from the same servers there has to be something sitting between the nodes and redis, or redis itself (or the proxy) needs to authenticate with the client.\n\nAnother possible route is to locally modify twemproxy to reject any commands other than get, del, and setex - the only commands our custom phpredis sends. That could potentially ameliorate the concerns with unauthorized connections have visibility to the keys and thus the session data from other customers. Though that might still be a significant patch we would have to maintain. At that point I'd have to consider writing a \"smaller\" version of twemproxy in Python that either handles the authentication from clients and proxies authenticated connections, or simply proxies a subset of commands and does the node key balance and routing operations.\n\nOr the third option of hacking auth into twemproxy myself. Might be an excuse to dust off my C chops.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11751858/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/26949404", "body": "There is actually a good reason to connect to multiple Sentinels. If the Sentinel you are connected to is on the minority side of a network partition, and the original master is in that partition, it could report it's \"old\" master. By querying all of the Sentinels you'd detect it and route to the majority master.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/26949404/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/27094869", "body": "> On Mar 24, 2015, at 10:48 PM, andy notifications@github.com wrote:\n> \n> In conf/nutcracker.yml https://github.com/twitter/twemproxy/pull/324#discussion_r27093408:\n> \n> > @@ -65,3 +65,18 @@ omega:\n> >    servers:\n> >     - 127.0.0.1:11214:100000\n> >     - 127.0.0.1:11215:1\n> > +\n> > +sigma:\n> > -  listen: 127.0.0.1:22125\n> > -  hash: fnv1a_64\n> > -  distribution: ketama\n> > -  auto_eject_hosts: false\n> > -  redis: true\n> > -  server_retry_timeout: 2000\n> > -  server_failure_limit: 1\n> > -  servers:\n> > -    - 127.0.0.1:6379:1 server1\n> > -    - 127.0.0.1:6380:1 server2\n> > -  sentinels:\n> >   @TheRealBill https://github.com/TheRealBill In the partition, if twemproxy can only connect a part of the sentinels, and some told you new master, some told you old master. While twemproxy don't know whick is right if the both two sides don't have instances over half of all. Another, the design of sentinel doesn't work well in network partition, so I think take consideration of network partition is not needed.\n\nI think it is quite the contrary. If Twemproxy can\u2019t verify the current master it should refuse to proxy commands for the server. If you\u2019ve only got one sentinel you can communicate with, it would be wise to assume the current master is no longer the master, but minimally cautious to simply refuse the connections for that slot. That is preferable to having data go to multiple places. One of the _big_ benefits to using Twemproxy+sentinel is the ability, if done correctly, to make split-brain scenario something the clients are immune to. I\u2019ve done it outside of Twemproxy so it is certainly doable - and not terribly complicated.\n\nConceptually the best place to run those sentinels will be the Twemproxy nodes themselves. However that isn\u2019t alway possible. As such your network between Sentinels and Sentinels+Redis can partition but twemproxy could still see the whole picture. But regardless by having the proxy _not_ accept commands (or minimally not accept write commands) when you aren\u2019t getting results which are confirmable you remove split-brain by nipping it at the very point it can happen - you stop proxying that node/slot.\n\nThe design of sentinel works fine in network partitions, it is the client which needs to be informed of changes, and Twemproxy is functionally the client in this scenario. Ideally, Twemproxy would subscribe to each sentinel\u2019s master-change event channel.[1] Then, on a master-change event it would update that slot\u2019s master setting and proxy to the new master. By only checking one sentinel you can\u2019t reliably handle the case - and you don\u2019t get the information as fast when you do get it. But by checking each of the sentinel servers you will either know you can\u2019t communicate and can fail to deny-to-safe-mode, or catch the event from the majority\u2019s side and know you need to reconfigure even though the other sentinel(s) hasn\u2019t caught the change. Incidentally there is a lag between the failover and the non-leader Sentinels so even outside of a network split the proper route is to catch it from the leader.\n\nIdeally you want these actions to be idempotent as well. In other words if you get a change notification multiple times, or the current pull, and the \"new-master\" is the same as last-updated-master, don\u2019t make changes. I suspect you\u2019ve already done that but sometime people miss that so I mention it solely out of that possibility. But _always_ communicate with more than just one sentinel.\n\nPerhaps to do this we could either:\n1) specify a quorum of sentinels to communicate with\n2) pull the list of known sentinels from sentinel and use that discovery to learn of and communicate with all of the other sentinels.\n\nOn the idea of not listing servers, perhaps we could do something like:\n<podname> <servername> \nWhere server name  is use by twemproxy for slot identification, but podname is the name you pass to sentinel to get the master & slaves (remember, a sentinel can manage multiple Redis master/slave combos). Then you can do _all_ discovery via sentinel. \n\nOn a related note I\u2019m not seeing which code maps a given pod (\u201cserver\u201d in Twemproxy parlance) to a given set of Sentinels. Each pod (server) may be managed by different sentinel pools. I\u2019ve also not found where we allow you to use a pod-name other than the server name in the servers section. We should cover that as well. Perhaps something along the lines of (assuming we do not do full discovery via sentinel):\nsentinel: true\nservers:\n   <servername> <podname> <list of sentinel:port pairs>\nOr (better, IMO):\nsentinel: true\nservers:\n   <servername> <podname1> <constellation1>\n   <servername> <podname2> <constellation2>\nsentiinels:\n  IP:PORT:constellation1\n  IP:PORT:constellation1\n  IP:PORT:constellation1\n  IP:PORT:constellation2\n  IP:PORT:constellation2\n  IP:PORT:constellation2\n\nWhere constellation = \u201cthe group of sentinels managing  a given pod\u201d. This latter option could also work well for full-discovery in sentinel. I also think it looks the most clean and readable. We might consider quoting pod-name. IIRC you can pretty much use any ascii character other than a space in a name in sentinels.\n\nJust throwing out some ideas to hopefully express better what I find as missing or ways we can do it even better. To say I use sentinel heavily might be a bit of an understatement. \n\nI see we currently have IP:port:weight for sentinels, but I hope we aren\u2019t using weight at all - ideally we should not have it at all to avoid confusion.\n\nIt\u2019d been a long day and I\u2019ll have more time of the next week or two to really dig into this. I apologize for not getting to it sooner, and am really glad to see someone taking the first steps in making what I maintain to be a significant improvement to Twemproxy! So thanks for getting this ball rolling. :D\n\nCheers,\nBill\n1. OK, _Ideally_ I\u2019d love to be able to fire twemproxy up unconfigured, then configure it entirely via API calls the same way you can w/Redis and Sentinel.\n\nP.S. I\u2019ll probably work on or find someone to work on adding Red Skull (http://redskull.io) connectivity as an alternative to Sentinel connectivity to simplify things - but not until I\u2019ve added non-JSON API interfaces to Red Skull. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/27094869/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/27147701", "body": "> On Mar 25, 2015, at 12:14 PM, andy notifications@github.com wrote:\n> \n> In conf/nutcracker.yml https://github.com/twitter/twemproxy/pull/324#discussion_r27143045:\n> \n> > @@ -65,3 +65,18 @@ omega:\n> >    servers:\n> >     - 127.0.0.1:11214:100000\n> >     - 127.0.0.1:11215:1\n> > +\n> > +sigma:\n> > -  listen: 127.0.0.1:22125\n> > -  hash: fnv1a_64\n> > -  distribution: ketama\n> > -  auto_eject_hosts: false\n> > -  redis: true\n> > -  server_retry_timeout: 2000\n> > -  server_failure_limit: 1\n> > -  servers:\n> > -    - 127.0.0.1:6379:1 server1\n> > -    - 127.0.0.1:6380:1 server2\n> > -  sentinels:\n> >   @TheRealBill https://github.com/TheRealBill Thanks for your suggestions.\n> >   For the first suggestion, twemproxy should connect to all the sentinels. I think we shouldn't make twemproxy too complicated. It's just a client which supports sentinels.\n> \n> Not quite - it is a client of sentinel. Thus it should be following the guidelines at http://redis.io/topics/sentinel-clients\n> \n> The problems in network partition should be solved by sentinel. For example, if the minority of sentinels don't server the clients, twemproxy will connect to a sentinel sunccessfully until it find a sentinel in majority part.\n> \n> How does it determine if the sentinel it connects to is in a minority? In order for it to do that it would have to compare master settings across the sentinel constellation. You can\u2019t just ask sentinel if it is in a minority network partition. You also can\u2019t query the \u201cknown sentinels\u201d info for the pod because that isn\u2019t updated except on new sentinel discovery and pod resets.\n\nIf a net split does occur and the original master is in the minority partition, twemproxy will still be connected to it. When sentinel initiates a fail-over normally, it sends a client kill to the old master (if available) to DC existing connections. However, in this scenario that won\u2019t DC twemproxy. Thus Twemproxy needs to be checking/monitoring for failovers and updating/reconnecting as appropriate. Anything short of that means to have reliable redundancy and avoid or minimize the split-brain scenario you have to use the current method of rewriting the config and restarting twemproxy.\n\nIt just occurred to me you might be trying to implement this as \u201cjust\u201d a TCP proxy/load balancer to Sentinels. I surely hope that isn\u2019t the case as you can\u2019t do it reliably. The way the Sentinel and client setup works clients need direct access to every sentinel for the reasons I listed above regarding why Twemproxy would need to do the things I\u2019m talking about. Clients need to connect to the sentinel and do more than just get the current master. They need PUBSUB and the ability to talk to each sentinel directly to ensure they aren\u2019t talking to a minority sentinel which contained the original master. Please tell me you\u2019re not trying to make Twemproxy a load balancer for Sentinel. :) It would be either as complicated or, more likely,  more complicated than what I am talking about above. If you\u2019re using twemproxy you don\u2019t want to talk directly to the backend redis instances - that defeats the main purpose of twemproxy.\n\n> Doing this in sentinel is simpler than doing it in twemproxy.\n> \n> Except that Sentinel doesn\u2019t control the clients directly. The mechanism for Sentinel helping clients is by either 1) having each client talk to and monitor every sentinel, or 2) having a script on the sentinels which reaches out to reconfigure the clients, or 3) by disconnecting clients connected after a reconfiguration of the pod (such as failovers). \n> \n> The podname suggestion is a good idea. I have thought to make mastername in sentinel as a mixed type like poolname-servername. So we can manage all the servers in different pool in the same sentinel. But the idea of specifying each server a different sentinel pool is not needed. I don\u2019t think there are people want to config one pool's servers into different sentinel pools.\n> \n> While it isn\u2019t the pattern I generally recommend, especially in the case of twemproxy supporting sentinel, it is actually very common for people to have each pod run it\u2019s own sentinels, and I consistently run into objections to doing it any other way. Furthermore in environments where a tool such as Red Skull is used to provide a cluster of managed sentinel constellations it would be the case there as well. Sentinel does start running into issues when you have high amounts of pods being monitored. So breaking that out across a banks of sentinel instances is also an expected use case. Not supporting it backs a potential user into a corner where you can increase the time it takes to failover. Given that Twemproxy is by design setup to proxy to essentially a bank of Redis instances there is a higher than normal likelihood of multiple sentinels being used.\n\nYou can learn some of these issues as https://github.com/antirez/redis/issues/2257 and https://github.com/antirez/redis/issues/2045 \n\n> The weight in sentinel config is not used. I just want to reuse the code of loading server config in sentinel config load. Modifying the code to drop weight in sentinel load is better.\n> \n> Ok, sounds reasonable. You should update the docs for that section to be quite clear that weight isn\u2019t used for sentinels. A few lines in the docs can be worth many questions as to why Twemproxy isn\u2019t respecting the weights assigned. ;)\n\nCheers,\nBill\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/27147701/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/27450141", "body": "> On Mar 30, 2015, at 12:00, andy notifications@github.com wrote:\n> \n> In conf/nutcracker.yml:\n> \n> > @@ -65,3 +65,18 @@ omega:\n> >    servers:\n> >     - 127.0.0.1:11214:100000\n> >     - 127.0.0.1:11215:1\n> > +\n> > +sigma:\n> > -  listen: 127.0.0.1:22125\n> > -  hash: fnv1a_64\n> > -  distribution: ketama\n> > -  auto_eject_hosts: false\n> > -  redis: true\n> > -  server_retry_timeout: 2000\n> > -  server_failure_limit: 1\n> > -  servers:\n> > -    - 127.0.0.1:6379:1 server1\n> > -    - 127.0.0.1:6380:1 server2\n> > -  sentinels:\n> >   @TheRealBill I have seen the guidelines of sentinel clients. It said client just need to connect one of the sentinels just like what the patch does. Just like what the guidelines said below, It won't work well in the network partition.\n> \n> But the client of twemproxy can't use sentinel directly because twemproxy is \"in the way\".  Since that was written we have developed operational knowledge and wisdom that tells us how to do it correctly, and it really isn't that hard. At a bare minimum you connect to each sentinel and pick the master reported by a majority of sentinels. If you can't reach said quorum, don't proxy. You can do it in a relatively simple script in HAProxy, so to argue it is overly complicated to do in Twemproxy doesn't hold IMO. It really is the least you should do. \n> Note: it is possible that a stale master returns online at the same time a client contacts a stale Sentinel instance, so the client may connect with a stale master, and yet the ROLE output will match. However when the master is back again Sentinel will try to demote it to slave, triggering a new disconnection. The same reasoning applies to connecting to stale slaves that will get reconfigured to replicate with a different master.\n> \n> I think twemproy shouldn't do too much things about distributed system.\n\nThen don't use it with sentinel. By placing Twemproxy in front and having sentinel hidden from the clients you're preventing the clients from doing what you describe. You take on the responsibility of protecting the clients when you prevent them from protecting themselves. \n\nDoing the simple thing is not always the right thing. If you're going to make it impossible for the client to protect itself, you must take the precautions on it's behalf. Twemproxy partitions data, and with the addition of sentinel support it thus becomes a core part of a distributed system. At that point the assertion it shouldn't do the right thing with regard to proper behavior in distributed systems is rather moot. \n\n> Sentinel is responsible for the cluster. So if you want the cluster works well under the network partition, the sentinels of minority should refuse to server the clients just like zookeeper or chubby.\n> \n> Again, how exact do you think the clients can learn the sentinel they are talking to is in a minority partition? If you're going to propose they can, you have to show how. It also would be how sentinel would do it. There is one way: ask all sentinels you can connect to, and you can't get a majority of known sentinels, stop proxying the connections. \n\nYou can wax on all day about what sentinel could or should do, but you're dealing with Twemproxy, not sentinel. I'm not sure you understand what sentinel is, as it doesn't serve clients it is a lookup service. \n\nWhen a network partition happens, sentinel has no way to know if a new master has been elected, and as such takes no action. It doesn't know if it is alone and all sentinels are alone (thus no master change happens) or it is in the only minority. And even then, it doesn't know who the new master is. Thus it can't know it is \"in the minority\" partition. Therefore it can't tell the clients to stop talking to the server as they may still be talking to the right server. \n\nBut Twemproxy can figure it out. Any client can, until you put Twemproxy in front of it and prevent it from doing so. \n\n> I have seen the redis issue 2257. I think it's a suggestion to reduce the connections of sentinels. Sentinel is ok when it has 2000+ connections(under the issue condition). The epoll can process tens of thousands of connections easily. \n\nThe point of this issue, Andy, is showing that the condition you think is rare is actually common and in a non-trivial amount of the time required. This is just one of many conditions I've seen. I've seen thousands of these setups, multiple sets of sentinels across a bank of pods is not a rare condition. Nor is it going to become one anytime soon. \n\nCheers,\nBill\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/27450141/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "ryanc4": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/134", "title": "Can twemproxy a solution for stale data handling when using consistent hashing", "body": "Hi,\n\nI am new to twemproxy but we have extensively using memcached in the past.\n\nOne of the problem that force us not using memcached with consistent hashing is the network partitioning problem:\n\na. we have a lot of memcached clients and servers\nb. if a particular client failed to connect to one server (say. OldM), consistent hashing will map into another server (NewM), new data will write to the NewM from the client\nc. if other servers are able to connect to the OldM, so now clients have different hashing.\nd. Stale data then will occur\n\nI am thinking if using twemproxy in a ACTIVE-PASSIVE would solve the issue for me. Anyone mind to share your experience.?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/134/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mthenw": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/130", "title": "PubSub in Redis - any plans?", "body": "Hi,\n\ndo you plan providing pubsub feature for redis backend? I would like to use twemproxy but this feature is crucial for me.\n\nBTW thanks for great project. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/130/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jbmclaughlin": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/118", "title": "Method to know which server was selected for a key", "body": "Hey all-\n\nFirst off, excellent job on Twemproxy.  Our company is looking for an easy-to-use, consistent hashing solution for Redis, and this tool meets all our needs.\n\nQuestion: Is there a way to know which server was selected for a particular key?\n\nThe reason I'm asking is because we might want to map several virtual servers to the same redis instance to establish our initial hash-ring and simultaneously reduce costs.  Knowing which server was initially selected will help us with data migration in the future.\n\nThanks again for releasing this to the community...it's a big help for start-ups looking for an out-of-the-box solution before redis cluster.\n\nCheers,\nJ\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/118/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Niteesh": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/103", "title": "plan to support select command in redis", "body": "one way ) when select command  comes set client connection property <db> to selected db\nkeep an array  of connection pools to redis server each for db <max number can be specified in config>. \nselect conn from appropriate pool to that db\n\nif this plan sounds  ok, we can start working on it\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/103/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991739", "body": "if cp->select  is initlized by CONF_DEFAULT_SELECT, the function conf_set_num will throw error that select is duplicate,\nthus to avoid writing very similar function  and  let conf_set_num work fine, this is done, later if cp->select remains unset default value is  assigned in conf_validate_pool\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991748", "body": "i made sure that when i call this funtion, connection is connected.\neven than i should  have kept it in loop to try more than once if EAGAIN error is encountered\n\nor Ill appritiate  your comments if there is a better way to do it\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991748/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3992055", "body": "```\nvoid  setSelectDb(struct conn *conn,struct server *server){\n     ASSERT(!conn->client && conn->connected )\n     int n, i;\n     char selectCommand[25];\n     sprintf(selectCommand,\"*2\\r\\n$6\\r\\nSELECT\\r\\n$1\\r\\n%d\\r\\n\",server->owner->select);\n     n = write(conn->sd,selectCommand,strlen(selectCommand));\n if (n < 0) {\n     log_error(\"ERROR selecting db on  socket for socket %d-> error %d \", conn->sd, strerror(errno) );\n     conn->err = errno;\n }\n```\n\nI am really not sure about this  so i want to ask you if this will do the needful        \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3992055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "manjuraj": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/97", "title": "handle max open file limit reached gracefully", "body": "when we reach max open file limit, nutcracker accept() loop errors out:\n\nSee: https://github.com/twitter/twemproxy/blob/master/src/nc_proxy.c#L291\n\nwe should handle this scenario gracefully\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/97/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/34eb60fb977da9aa8bcac784dee1e7fb04c79d47", "message": "Merge pull request #517 from takayamaki/fix_typo\n\nfix typo in README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/0f9e1baf06db478065ba43402b975c57a567d00f", "message": "Merge pull request #492 from kalifg/patch-1\n\nFix typo in notes/memcache.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/6fd922039bc696e07b7ef18946732df6b7cf2a5d", "message": "Merge pull request #493 from dennismartensson/patch-1\n\nUpdate README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/017d44503344ecdf2439747476556f4b9fee3e21", "message": "Merge pull request #494 from mortonfox/patch-1\n\nUpdate the sensu-metrics link"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/e5739338ddf228b3b900aa116646cb9654fa5f65", "message": "Merge pull request #489 from rohitpaulk/update-redis-docs\n\nUpdate redis docs for PING and QUIT"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/74af2fb2d5d3e214d8c0741a4b0ebb7d93572fc8", "message": "Merge pull request #439 from Krinkle/patch-1\n\nreadme: Link to HTTPS for wikimedia.org"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/9a2611e5992e65e6c15cd23bce06ffed3901f4f8", "message": "Merge pull request #425 from charsyam/feature/typos\n\nfix typos"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/619b55b6a1dcd21ffe16f312b52cd576c6a4caa0", "message": "Merge pull request #421 from charsyam/feature/upgrade-redis-lib\n\nupgrade redis-py version from 2.9.0 to 2.10.3"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/592c1b6b157721f66789bca6a3d463f32e6c59fc", "message": "Merge pull request #420 from esindril/master\n\nAdd script to build SRPM and fix spec file"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/defda674cdfb94fef31bdf8cc7a1dd766ea0a05a", "message": "Merge pull request #418 from twitter/revert-406-bugfix/redis-error-response-parser\n\nRevert \"Fix parsing bug when error body contains no spaces\""}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/65bb2ac5bbf0acce3fa52469752046c388e56ef5", "message": "Revert \"Fix parsing bug when error body contains no spaces\""}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/34f369ff91076b5d1b1229eca51bc3a2ed8f6d4a", "message": "Merge pull request #406 from tom-dalton-fanduel/bugfix/redis-error-response-parser\n\nFix parsing bug when error body contains no spaces"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/38ff5bdb0f0228b7f479f1fed41839730f709dd3", "message": "Merge pull request #397 from tom-dalton-fanduel/doc-client-connections\n\nDocument the client_connections config option"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/013b5313dfa015dd6d2cbf2476302c75be20e1c8", "message": "Merge pull request #398 from androidmj/master\n\nAdded Wayfair to list of companies that use this."}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/4bfcc42d1941993ae41ae16f6640622cc38e52d7", "message": "updated README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/09a4b08c4038c1310d49134c2bf53debbc712d54", "message": "Merge pull request #389 from charsyam/feature/bitpos\n\nsupport redis bitpos command"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/e1960982540e93282eb361983e583ed682aa5f35", "message": "Merge pull request #387 from anubhavmishra/master\n\nAdded Hootsuite as twemproxy users"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/29e13cc04dfdab83a5dc118c2076a160a04a58f4", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/5dd8757587f424d539ee97cd125baf54664a07ab", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/452b606ba2ab89e9018475a67f7b35289e364471", "message": "updated README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/6d1fc92dd27d6810c6bbc1917a3b1ec589451325", "message": "fixed formatting"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/f449b5e9fa48c118167814a226d3e8e2868263d4", "message": "Merge branch 'charsyam-feature/KEEPALIVE'"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/5f3098a43cf8f561293e710b5765bafb3f696066", "message": "Add switch to use TCPKEEPALIVE option"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ba06d32298fdbff82c1078c3dd6e8ed5861adc21", "message": "all keys in yaml file must have a non-empty value - issue 381"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/a51959c9db802f7b1a84a7d06f1ea3d2c0dc0d4d", "message": "updated ChangeLog"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/7e50f0c04d7eac6a2efac7e161d76706d917b04b", "message": "updated nutcracker version to 0.4.1"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/0beb3b550787270a12f6451df26b2e60abdac482", "message": "remove unused function"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/adf00c8b89dc20163ba2882ad5d34aa642a2c969", "message": "updated nutcracker.spec"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/fc0be81153822331b2435a6ae417a682d8cf2c74", "message": "refactored how redis_auth is handled through authenticated option; redis_auth is only enabled for a redis pool"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/d2e1721636827860a64aa250a926259bb4b9ced8", "message": "redis_auth directly is only valid for a redis pool"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/cf4ea7368f11973b4918d54e035663a584be6309", "message": "unquote strings in the RSP_STRING table"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/cbaced3e3e05e331c8f1e6e120b792d0d35e8429", "message": "redis_add_auth_packet -> redis_add_auth"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/cae528d08ad5d6df996854472762cab8f79f8d50", "message": "added a #define table for redis response string"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/12b5a660cad52daf32747b30bb4f3909ce6c0df6", "message": "formatting fixes"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/7e04cd85b7e4be1e2f88ec1564fec553f40abb25", "message": "updated comments"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/00def85c82740acd2bed0927a1b4d8e86a90f3e6", "message": "updated comment"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/fd34c29bb69861b37eea786bd84a7e437ab8bb7c", "message": "backend server hostnames are resolved lazily"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/d3c19adcd11bbe2f3531baeab07fdb3d99721e69", "message": "getaddrinfo returns non-zero +ve value on error"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/036b0e64097a3c34ec3947fcc4efd651c510b6e6", "message": "Merge pull request #366 from begeekmyfriend/rbtree-optimization\n\nRemove redundant conditional judgement in rbtree deletion"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/01ba86a982dca02973132aae3cc13727f4d5e8c6", "message": "Merge branch 'master' of github.com:twitter/twemproxy"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/5b04feebd85eaf0d843b87a8af0e169814478d90", "message": "use sockinfo field in server and server_pool struct, instead of storing individual (int:family, socklen_t:addrlen, sockaddr:addr)"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/bcf721ad658d02b911757ff1ce41b1c30ea3defb", "message": "Merge pull request #375 from charsyam/feature/fix-hang-when-command-only\n\nissue: #362 fix bug hang when command just has type only"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ea7190b2956795035ed3304665e5273cbaa13316", "message": "fixed log message"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/5c897fb6fd19ae6e5abc598dbc78530e45f3c824", "message": "log when memcache request have empty or non-existent keys"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/4f717266e7e77093baf0641dd58990f560cbf82f", "message": "Merge pull request #363 from charsyam/feature/bug-fix-memcache-crash\n\nfix bug crash when get command without key and whitespace"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ef453130e321974b332dd99d585ae7285eee4b5d", "message": "mark server as failed on protocol level transiet failures like -OOM, -LOADING, etc."}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/b55301d4d66e95c41a3ef80f0ac7dac8a0408ee4", "message": "Merge pull request #372 from tom-dalton-fanduel/readme-redis-db\n\nDocument the redis_db config setting."}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/3f04ea0e97129cd6c820f5a758d373f2c8378ebf", "message": "Merge pull request #371 from tom-dalton-fanduel/update-readme-companies\n\nUpdate readme - company list"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ac410fcaea4bb18caa9b89fd79501a3fc9c7ac3c", "message": "updated README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/5e082b8d3ae0479b8f019a5f53e1e289f66ddfe9", "message": "implemented support for parsing fine grained redis error response"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/b5bfa6077bca2357e592f6dffa7a91b85560f5c2", "message": "updated memcached.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/687d25a7148212ec7682567e08c29e8f7d78df6e", "message": "udpated memcache.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/2f925959bd041f5b5a656afa2ed3bfc7f866a815", "message": "updated memcached.txt"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/c287ba65aded54ce80c4cee48d3e118e6e7a05c6", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/b694cc9a51955fb07f17ba4fc1c7137ba3cee91a", "message": "Merge pull request #349 from davis/patch-1\n\nUpdate README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/b8a76c03b0358c7068ac8d54e29b6b2dcc469527", "message": "Merge pull request #344 from mckelvin/touch\n\nSupport \"touch\" command for memcached and allow negative exptime"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/6d376e7820fd8bc1d826cf95ef37237a392bfc68", "message": "Merge pull request #333 from Serekh/master\n\nAdded beholder to the Utils section, a python agent that works together ..."}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/aead8731a6593ea1d456b02e957952501122bd68", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/5941059587fcbb1b8bad029422e47d51fef848e7", "message": "updated README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/4d45bac85bf7df528404bc510a074c4057981c3a", "message": "updated redis.md since SORT command takes only a single key"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/d372c3432ade35388b144e0702bf4e11540b345c", "message": "SORT command can take multiple arguments. So it should be part of redis_argn() and not redis_arg0()"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/658c825e24a9380def037e8e71493f207c5f327c", "message": "updated README.md and recommendation.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/a36fdae2a8a3c166d9d580fc5973f35c02713c39", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/d75168c7a1eebcbaab0ca9977573641e0a1ac15d", "message": "updated README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/a5c182bda5a4994df65d02e7c21af698a50a9ae8", "message": "remove incorrect assert because client could send data after sending a quit request which must be discarded"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/f0a6c3baed6526d14f80a492a148e4d9ee4b7cdc", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/1bfbca4b5d2d36e7a1a22224e0b2dabb745d945a", "message": "Merge pull request #322 from yak0/master\n\nUtils Update"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/d8e6c1173b864b73ee22c7ec4e0f330a0ae41bbe", "message": "rename conn_init to post_connect"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ca7d0168560dfede5204c5fdd92a0324dc4d7f21", "message": "make redis_conn_init() use msg_prepend_format()"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/2b0d5beae7b1e62d0f1acb1e0d0eb01aec869368", "message": "Return error if formatted is greater than mbuf size by using nc_vsnprintf() in msg_prepend_format()"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/76da36179a2209a5bf7cf72653bcde15422ea547", "message": "added comments on snprintf() and scnprintf()"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/0da2d99d1451e72f260d72d59792a5f8338f697c", "message": "fixed formatting"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/98c8dfe1c7049479ac4d56d3dc063c72b74597c9", "message": "Merge branch 'master' of github.com:twitter/twemproxy"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/a9f1417a8a29d5846f911ec531f82951b9fbde25", "message": "create default implementations for init() and swallow_msg() handlers in memcache"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/0161f288633cb7f3f1abfeb8c7490ccd46f0c95f", "message": "Merge pull request #302 from charsyam/feature/stat\n\nfix stat when response"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/0d7d2fcd12d48971365768ec124bc303ca1bf102", "message": "Updated recommendation.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/37ca913e0ff426224a76816848df8d3e4c90471e", "message": "Merge pull request #294 from arnecls/RedisSelectDB\n\nRedis SELECT on connect"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/4d9a8a0c250fb1a00b3e76efc55ff935503ecbef", "message": "Merge pull request #300 from charsyam/feature/add_note_for_python_memcached\n\nadd note for python memcached"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/9cf0e3afa729ca4605470b641fd6913a30ffc0f7", "message": "Merge pull request #292 from zwChan/master\n\nCheck the configuration before start in the nutcracker.init script."}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/139c5278b05b3736ba885c6e6f47968c29569520", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/94a538438d9a411b274d3c8cccaf0b003e89cb62", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/87b708ed6fc95452cc69f0f117d6871b180cb5c7", "message": "Merge pull request #295 from charsyam/feature/misspells\n\nfix misspellings"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/b0a940720bae9041ddc06bf05ef5fd10c0cbf43d", "message": "Merge pull request #293 from arnecls/FreeBSD10Support\n\nBuild support for freebsd 10 (adding libexecinfo)"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/36122ec09462e5d127d08b649004cd42c107ad1c", "message": "Update redis.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/3ed7701578ca970b42ccfe9f58be2202c0411768", "message": "Merge pull request #270 from guilhem/chef\n\nAdd chef cookbook in `utils` section"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/d4d8b38b6dc7e52b84d56c3b4813503ed38c3de7", "message": "Merge pull request #284 from realzeitmedia/master\n\nAdd `redis_auth` to the README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/4e35da43c11bf9c9896a9fa42dc0b837292b6417", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/df5c5ba32e2992d2c12e88ff03d085aa8daeae30", "message": "Update redis.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/f1b2bad07b2826a56a4c149a81f9db350e75bc40", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/8305438ac6bf3b79e9681f4ea7cbf439996c5848", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/79441177e981ec4d68e2d1e2d94fd9eb36594455", "message": "Merge pull request #247 from mkadin/patch-1\n\nAdjust doc title"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/faa8db72a9e213acf943cb14eca1895a240b659c", "message": "Merge branch 'allenlz-safe_max_connections'"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ffc13ad7ce372d5cfa19fc74cef5a2cff3f6078a", "message": "format changes"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/e99c79123975713abf49dc9a0930ca404e3985ec", "message": "Merge branch 'safe_max_connections' of github.com:allenlz/twemproxy into allenlz-safe_max_connections"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/bf43a2d39819a6c4b1db011495b42ee6e6b8cd40", "message": "disallow zero weight in the servers section of conf"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/0d3084be2c4d3d8cb64cb6199db3413d2cbcda42", "message": "resolved conflicts for merge from mattrobenolt-hyperloglog"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/8bd5face4af20d1f9087ca3b04f6ca0dffca5714", "message": "Merge pull request #243 from mattrobenolt/lex\n\nAdd LEX sorted set operations"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/8a4f5c0278708e0b00b2e7314750bfc828e78524", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/c8ae34a62edf173650ee764ed02f2593c7df3630", "message": "Merge pull request #200 from rhoml/rhoml/adds-sensu-plugin\n\nAdds sensu plugin more  users"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ef7dd1f69a8a20e5518ceeed1a4cd84c4134b8c3", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/b4749eb0db2e8c6b13fd0b2963a59717ed1d2b24", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/5a330fef71b53ee40a45df14f912a49c9d699b9b", "message": "treat ECONNABORTED from accept() syscall as a transient error"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/f2c561322f14a281b5e3b7ff07f018f63a5bd775", "message": "Merge pull request #190 from oremj/fix_spec_changelog\n\nfix(nutcracker.spec): Fails to build on RHEL 6"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/69413ff3ed724477c448f60f0d45d6add63372f3", "message": "Merge pull request #188 from dksidana/patch-1\n\nFixing project URL in nutcracker.spec"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/4013d6cb34becb8779edb6a47cae569b3c623c9b", "message": "Merge pull request #189 from andyqzb/master\n\nfix the bug in string_compare"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/c87dc995a36e5e410f4586858f442efc20a63532", "message": "Update nutcracker.spec"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ddbc0ef322d3f4d7d966a3f394a445ec4ef4d042", "message": "Merge pull request #132 from oldmantaiter/master\n\nRPM SPEC Changes"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ecfd0fa0ea802692071acc4c99c933f21f33aca7", "message": "Merge pull request #176 from nikai3d/patch-1\n\nfix typos in stats description and README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/bdc9c78ec0b3cc24624a58e31a74c7a747f2bba0", "message": "updated redis.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/b2cd3adfbe892005a1a564bd6a0b6137fe42e46f", "message": "Update recommendation.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/7e75e6f57c073cff1e011af151906f384c28c4cd", "message": "updated comment"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/0d8afb36b08c1527bdc6560de50a110f1b3b35ba", "message": "Merge pull request #150 from jdi-tagged/early-response\n\nHandle case where server responds while the request is still being sent. You encounter this scenario with memcache when you send it an item that exceeds maximum slab size item"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ed4cca87a2be8a6abfd415a6288c67c6e35c213e", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/dabd19a42f1421036f46a48e58221820e626ae01", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/b8c4b2c7a0a84a2127871b20a2879b5b164f47bb", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/483aec042f94d6eefdc60a831f90cc786b9848c8", "message": "Update recommendation.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/749f8b2bcbbc746929a107aa08986826372f28fb", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ea3f896e1d04f47265cbc502770f2210fef24ac8", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/5f6d8a56e6a56bf7dad7d0ffee61b2d12c191368", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/683ae64b45e76a503cc6fd21376b39d6b7464129", "message": "Merge pull request #127 from eranb/patch-1\n\nUpdate README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/97ced21b0405cc0a46b002c9074b34abee15acb7", "message": "formatting fixes"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/54e4fe76ab1523c2ed92ac13f5adbb92290fe68f", "message": "Merge pull request #231 from idning/sighandler\n\nfix deadlock in sighandler"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/a747b9b97ad91f03838fb67eb3b895b1193a20a2", "message": "Merge pull request #163 from mkhq/master\n\nSRANDMEMBER support for the optional count argument"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/395f17a5d853febab0a6ea70e1430a083d4420fb", "message": "introducing msg_type_strings[] array of message type string reprs"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/7a84643b01db62e6d5e3225d2919e66cd72ac238", "message": "Merge pull request #237 from eleusive/master\n\nUpdate mbuf max size in recommendation.md, add Twitch to Users list"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/1353eae78d822da4d7e8b34891712af9e6101afb", "message": "increased mbuf max size to 16 MB"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/b53eb8d62717c737bc8b8eed92e5857fee75bb96", "message": "Merge pull request #234 from eleusive/master\n\nincrease maximum mbuf size"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/a273d7ce09dc964d854bcbca4018dcf819e1029d", "message": "Merge pull request #226 from idning/req-notice-log\n\nadd notice-log for every request"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/002721be2e56223246549d1d02e6a28b83625812", "message": "Merge pull request #228 from ivmaykov/master\n\nAdded Ooyala to list of users in README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/f57392e4535f3eb2431a6002a38a9f8fa5e1f408", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/3383e09eaff3d0c348acb99d71e03d23b0d91937", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/03e9a3c35a0da4cbb7250c37084828335defae9e", "message": "version 0.3.0 release"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/e93e8165be96aa2274fcb1a072ea21abb9371b50", "message": "add the timestamp when the server was ejected"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/a0c49ec68ed5927ec3c0cb2816450c0b17c27ddc", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/8c8d5fed6652e28f22b3fef18a5f0393f057c013", "message": "Merge pull request #139 from MaximF/patch-1\n\nUpdate nutcracker.spec"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/b549eeb86cfbabc79179dc146758c8559e9d2ac2", "message": "Update README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/9bf48a1e1f84b5e8024976309620ea91a9aa76d6", "message": "Update recommendation.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/b068585fffc3b03675375c523128129e43d09817", "message": "event ports debugging notes"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/a3fd599dd3593289480c56f898e75c4733a2b3c2", "message": "changed the signature of core_core"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/d2d7df3535f695627813694a0d7f866e00988a85", "message": "add nc_evport.c to libevent_a_SOURCES"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/80eedf6feb8732ecb0d6b52a1b28f3d55597a0c8", "message": "comments on why we need to check for ENOENT in event port's event_del_conn()"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/f07f6b148c65da51a3d19652d1b12101f662aab7", "message": "s/compiled for/built for/"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/04380021e4d3c804f7f9d5ba984c5f4f065aa905", "message": "delete event_add_st() from nc_evport.c"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/e0f939b5d2af6378b5a45840756d18e364f7c195", "message": "event ports (solaris/smartos) support"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/46f2e09b0b6680ad0f5820ae413e840fca860f33", "message": "remove nc_evport for now"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/d6b8274ecba8f5ccbcea2f6aa2039c101192b6e8", "message": "formatting fixes"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887954", "body": "I would be willing to take patches for kqueue support in twemproxy\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887954/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887968", "body": "Thank you for the patches. I will merge this in the next version\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887968/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6456668", "body": "twemproxy does not do replication. If you are doing replication, you would have to do it on the client side\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6456668/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6798106", "body": "> 1. Before you started work on twemproxy, did you consider using moxi: https://github.com/steveyen/moxi? If yes, why did you reject moxi?\n>    with nutcracker we have:  \n>    a) protocol pipelining which works really well for us, as we wanted to introduce minimal latency degration with a proxy sitting between a client and server.\n>    b) mbuf, which essentially enables zero copy when moving data from client to server (and vice versa)\n>    c) observability which was fairly important to us in our production environment\n> 2. Are you planning to support binary protocol in addition to ascii?\n>    no (see 'thoughts' section in notes/memcache.txt)\n> 3. Do you see any need for multi-threaded support? Moxi supports both single and multi-threaded configurations.\n>    no; if a run proxy of proxy (client --> proxy --> (proxy)+ --> server) you can actually make use of all cores and would probably be network bound in this scenario\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6798106/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6864681", "body": "This is a good idea.\n\nThe start up time of twemproxy is really small, especially when preconnect is not set. So, if your clients had retries built into it, then you can trivially propagate configuration changes by doing a rolling restarts of twemprox'ies. This is a good enough solution, imo\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6864681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373486", "body": "> > Would you be open to a patch for this?\n> > I am always open to accepting patches :) \n\nI just think that reloading of configuration file on-the-fly is tricky to get right.\n\n> >  The problem we have is that if the server is busy enough the ports for twem may become used in the short restart window, causing twem to not start. In that case we have to shutdown apache, restart twem, start apache.\n\nI am curious as how this is happening. I set SO_REUSEADDR address option on the twemcache's listening socket, which means that we would reuse ports even if they are busy (See: proxy_reuse() function in nc_proxy.c)\n\nCould you paste me the log file dump with the error that you seeing? \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373486/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373606", "body": "Thanks for the patch using libevent API. \n\nI am willing to accept patches if they don't use libevent but directly call bsd's kqueue API. All you might have to do is to implement abstraction in the event interfaces, which would be in nc_event.[ch]. Let if know if you have any questions. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373606/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7387377", "body": "you can always change the ulimit of the shell from which you launch nutcracker\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7387377/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448857", "body": "blake the patch looks great. One thing I realized is that memcache has some item header overhead and slab header overhead even for the largest item. So, even if you configure memcache with 1MB slab, the largest item that can be stored in the slab is < 1MB. Furthermore the item size not only includes the value length, but also key length. \n\nGiven this, do you think we should have two extra keys in yml configuration\nitem_max_kvlen: (maximum key + value length)\nitem_overhead:\n\nand we discard requests whose key + value length + overhead > item_max_kvlen\n\nthoughts?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448857/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448944", "body": "the reason I don't use libevent api is because I wanted to build nutcracker without any dependency on 3rd party libraries. furthermore, I use ET semantics, which I believe is not available in 1.4 version of libevent.\n\ncould i get a pull request of your changes?\n\nI also looked at your changes. I think you might want to do few cleanups before submitting a pull request. Few suggestions:\na) abstract out the event interface and wire to the underlying event call using a function pointer\nb) maybe create event/ directory that contains event/nc_epoll.c and event/nc_kqueue.c files. \nc) follow style conventions outlined in: https://github.com/twitter/twemproxy/blob/master/notes/c-styleguide.txt\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448944/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456133", "body": "> SO_REUSEADDR will only reuse a TIME_WAIT socket, not an ESTABLISHED socket. If twem is running on a busy web server (as a local proxy), and the port gets used, reuseaddr is no help.\n\nIs there a test case that would reproduce this scenario? I am unable to reproduce this scenario, even if I bombard twemproxy with a constant stream of traffic and restart it.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456133/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456361", "body": "blake, I wonder if this issue would go away if we set l_linger = 0 by calling nc_set_linger(sd, 0) on the listening sockets (see notes/socket.txt for details on it)? \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456361/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7733080", "body": "Thanks for the test case; this is really useful\n\nI am still actively working on the redis branch. hopefully we should have a stable build out by end of this month\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7733080/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7792159", "body": "should be straight forward if aws conforms to memcache ascii protocol\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7792159/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/8287813", "body": "is there a reason why you would prefer inline command over unified command besides the obvious ease of issuing such command from telenet;\n\njust supporting unified protocol makes parsing for req / rsp easy and simple in twemproxy.\n\nif you would like to contribute, you could look at nc_parse.[ch]\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/8287813/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315651", "body": "twemproxy should just work with kestrel. I guess you might be well off using \"distribution: random\" instead of default of \"distribution: ketama\" for a pool of kestrel severs\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315651/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315677", "body": "apologies @jsholmes ; I am still working on this\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315677/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9319262", "body": "distribution random, assigns key to a server randomly on every interaction.\n\nyou can read:\nhttps://github.com/twitter/twemproxy/blob/master/README.md\nhttps://github.com/twitter/twemproxy/blob/master/notes/recommendation.md\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9319262/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9553043", "body": "@rbranson mbuf memory leak is false positive. Please read \"read, writev and mbuf\" section in:  https://github.com/twitter/twemproxy/blob/master/notes/recommendation.md\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9553043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9611737", "body": "@mezzatto @jsholmes redis_unstable branch HEAD has updated changes, which I believe is close to stable. \n- For list of commands supported, see: https://github.com/twitter/twemproxy/blob/redis_unstable/notes/redis.md\n- Unit test cases, see: https://github.com/twitter/twemproxy/blob/redis_unstable/scripts/redis-check.sh\n\n@mezzatto let me know if the SIGSEGV goes away with the latest changes.\n\nPerf numbers against a local twemproxy to redis 2.6 looks like this:\n\n#### without twemproxy\n\n```\n $ ./redis-benchmark -h localhost -p 6379 -t set,get,incr,lpush,lpop,sadd,spop,lrange -q\n\nSET: 75187.97 requests per second\nGET: 70921.98 requests per second\nINCR: 74626.87 requests per second\nLPUSH: 72463.77 requests per second\nLPOP: 76923.08 requests per second\nSADD: 76335.88 requests per second\nSPOP: 75757.58 requests per second\nLPUSH (needed to benchmark LRANGE): 72992.70 requests per second\nLRANGE_100 (first 100 elements): 29498.53 requests per second\nLRANGE_300 (first 300 elements): 11778.56 requests per second\nLRANGE_500 (first 450 elements): 8298.75 requests per second\nLRANGE_600 (first 600 elements): 6443.30 requests per second\n```\n\n#### with local twemproxy\n\n```\n $ ./redis-benchmark -h localhost -p 22121 -t set,get,incr,lpush,lpop,sadd,spop,lrange -q\nSET: 60240.96 requests per second\nGET: 64102.56 requests per second\nINCR: 64102.56 requests per second\nLPUSH: 63291.14 requests per second\nLPOP: 64935.07 requests per second\nSADD: 63291.14 requests per second\nSPOP: 66225.16 requests per second\nLPUSH (needed to benchmark LRANGE): 60240.96 requests per second\nLRANGE_100 (first 100 elements): 18796.99 requests per second\nLRANGE_300 (first 300 elements): 7564.30 requests per second\nLRANGE_500 (first 450 elements): 5170.63 requests per second\nLRANGE_600 (first 600 elements): 4098.36 requests per second\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9611737/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9692379", "body": "@mezzatto any updates?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9692379/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9710038", "body": "thanks a ton @mezzatto \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9710038/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9717058", "body": "twemproxy will not support redis inline commands\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9717058/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9828665", "body": "yes, in the 5 minute window, twemproxy A and twemproxy B have a different ring and as a result they may send traffic to different backends\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9828665/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9879987", "body": "@xaratt this is really odd. \n\nwhat happens when you try to connect to the AWS cache servers from \"my.proxy.server\" directly. So, in one try you use \n\n printf \"get foo\\r\\n\" | nc  myserver.0001.use1.cache.amazonaws.com 11211\n\nAnd in the next try you do\n\nprintf \"get foo\\r\\n\" | nc  ec2-xx-xx-xx-xx.compute-1.amazonaws.com 11211\n\ndo both of them work for you? \n\nAlso does ping of \"ec2-xx-xx-xx-xx.compute-1.amazonaws.com\" and \" myserver.0001.use1.cache.amazonaws.com\" resolve to different addresses?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9879987/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10086645", "body": "@shapirus thanks for such a detail explanation on the bug. This is great stuff!!\n\nI think, I know what the issue is. The issue happens when the \"END\\r\\n\" token spread across two mbufs. For example, the first mbuf has \"E\" and the second mbuf has \"ND\\r\\n\" token. When this scenario happens the response parser - memcache_parse_rsp in nc_memcache.c ends up in a wrong state leading to the \"Invalid Argument\" error\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10086645/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10165048", "body": "@litao941 is there a reson why \"localhost:22222\" will not work for you? \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10165048/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10173961", "body": "Ah! Looks like stats_listen wasn't using STATS_ADDR. I fixed this in this commit: https://github.com/twitter/twemproxy/commit/fd328015805b407c35c9b6995330fcd686765e41\n\nyou can now use STATS_ADDR #defined to \"127.0.0.1\" for your instance to only listen on 127.0.0.1\n\nlet me know if this works for you?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10173961/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10174768", "body": "great to hear that; I will add a command line param soon.\n\nTo reduce cpu load use (-m 512)\n\nalso, if your company is using twemproxy, let me know and I will add you to https://github.com/twitter/twemproxy#users\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10174768/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10316364", "body": "fixed on HEAD - https://github.com/twitter/twemproxy/commit/8216f667ec88cb2eecab1353907751826df10e5c\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10316364/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10412920", "body": "@shapirus would you be interested to submit a patch for this? :)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10412920/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10419366", "body": "server failure handing routing is in nc_server.c:server_failure(). This is triggered whenever a server is closed - nc.sever.c:server_close() which gets invoked from conn->close() in core_close()\n\nThe server struct in nc_server.h maintains two fields to keep track of failures:\n\n```\nint64_t            next_retry;    /* next retry time in usec */\nuint32_t           failure_count; /* # consecutive failures */\n```\n\nThe server_pool struct in nc_server.h keep track of how many failures to allow before letting dead/live servers to be ejected and/or put-back\n\n```\nuint32_t           ncontinuum;           /* # continuum points */\nuint32_t           nserver_continuum;    /* # servers - live and dead on continuum (const) */\nstruct continuum   *continuum;           /* continuum */\nuint32_t           nlive_server;         /* # live server */\nint64_t            next_rebuild;         /* next distribution rebuild time in usec */\n\nint                timeout;              /* timeout in msec */\nint64_t            server_retry_timeout; /* server retry timeout in usec */\nunsigned           auto_eject_hosts:1;   /* auto_eject_hosts? */\n```\n\nWhenever a server is ejected or put back, we have to rebuild the hash ring. See server_pool_run() and server_pool_update(). We call server_pool_update() when:\n\n```\nserver is ejected - server_failure()\nserver needs to be added back - server_pool_conn()\n```\n\nOne idea in terms of implementing what @yashh is asking for is to use the \"normal traffic\" to eject a server on a failure. But once a server is ejected we use a timer to do background \"heartbeat traffic\" on the bad server and add the server back only if these hearbeat checks succeed. Timers and timer expiry is implemented in twemproxy use event loop. See core_timeout() \n\n@shapirus let me know if you have questions or thoughts\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10419366/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10767369", "body": "@poison I'm glad you are finding it useful\n\nThe reason multiget request is split into single get is because in \"get user-1 user-2\\r\\n\", user-1 key maybe routed to server-x and user-2 key maybe routed to server-y \n\nEven though multiget is translated to single get, the request on the wire is pipelined and batched and hence achieving the same performance benefits of multiget. Put in a different way, if you only had a single backend memcache server in the cluster, the request \"get user-1 user-2 user-3\\r\\n\" gets translated to \"get user-1\\r\\nget user-\\r\\nget user-3\\r\\n\"\n\npipelining is also the reason why twemproxy ends up doing better in terms of throughput even though it introduces an extra hop between the client and server\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10767369/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10983213", "body": "good catch; updated the file\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10983213/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11002137", "body": "@xqpmjh I am assuming that you are using the config file https://github.com/twitter/twemproxy/blob/master/conf/nutcracker.yml\n\nin this file, the pool listening on \"127.0.0.1:22121\" is a redis pool and hence only parses redis requests and responses\n\nyou either want to delete the line \"redis: true\" or use \"redis:false\"\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11002137/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11003241", "body": "your problem may also be the following: http://stackoverflow.com/questions/4745345/how-do-i-stop-phpmemcachedelete-from-producing-a-client-error\n\nfrom spec: https://github.com/memcached/memcached/blob/master/doc/protocol.txt, delete format is 'delete <key> [noreply]'\n\nbut it seems like php delete sends 'delete <key> timeout [noreply]\"\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11003241/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11031940", "body": "Thanks @charsyam; I will merge this soon\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11031940/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11048607", "body": "You can use the option --stats-addr to set the stats monitoring port\n\n```\n-a, --stats-addr=S     : set stats monitoring ip (default: 0.0.0.0)\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11048607/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11051346", "body": "Thanks for fixing this :)\n\nLet me know if you would like to help contribute to more upcoming features in twemproxy. There is one called Issue-14: https://github.com/twitter/twemproxy/issues/14 that I think would be really useful in keep the twemproxy server cluster even more stable in presence of rack switch failures and machine failures.\n\nI could definitely use some help :) \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11051346/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11056346", "body": "I like this idea of using the \"node name\" (when specified) instead of \"host:port\" pair as input to consistent hashing. I also believe that this should be fairly easy to implement\n\nRegarding the open problem of priority, we can just use the priority from the \"host:port:priority\" triplet. For example, for a input like \"127.0.0.1:6382:1 server4\" we will use \"1\" as the priority of server4\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11056346/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11068214", "body": "@charsyam This code exists for backward compatibility reasons.\n\nWhen we deployed twemproxy inside twitter for memcached protocol, for a while we would do dual reads - read data through proxy and read data directly from backend server cluster and ensure that we read the same data from both code paths. Since the client was using libmemcached, we had to make sure that we used the same consistent hashing algorithm as that used by libmemcached library to ensure that keys get mapped to the same server. \n\nI guess, we can now update this code to not attach a port number only if the server pool is a memcache server pool\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11068214/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11068517", "body": "@antirez priority refers to the weight of a server. For example, if I am running redis on a server1 with 4G and another redis on server2 with 8G, I would want to give server2 twice the weight given to server1 in order for the keys to distribute evenly across the total cluster memory\n\nSo, if a server migrates from \"127.0.0.1:6379:X server1\" to \"1.2.3.4:8888:Y server1\", we ensure that we keep the weights X and Y same to keep the key mapping stable \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11068517/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11101114", "body": "wow! thanks for doing this so quickly @charsyam  :)\n\nI will take a look at your diff and merge it in if everything looks good\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11101114/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11212477", "body": "fixed by @charsyam; docs updated: https://github.com/twitter/twemproxy/blob/master/notes/recommendation.md#node-names-for-consistent-hashing\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11212477/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11406723", "body": "@charsyam is spot on. twemproxy only supports commands that take a \"key\" as argument and is \"distributable\". See https://github.com/twitter/twemproxy/blob/master/notes/redis.md for list of all supported commands\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11406723/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11457259", "body": "We do have plans to support EVAL.\n\nBut before we support EVAL/EVALSHA, we need to support https://github.com/twitter/twemproxy/issues/9. This is because, if sharding is done on the \"keynames\", we would require all the \"keys\" in EVAL command  to map to the same server. Unlike mget and del, we cannot fragment EVAL command. \n\nwhat do you think? \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11457259/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11475123", "body": "@AllenDou the issue with \"info\" command is that it is not clear how to distribute it. For example, if you had server cluster of size 4, sending an info command to twemproxy would require us to send the command to all the 4 backend servers and aggregate the response from all the 4 servers\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11475123/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11545552", "body": "fixed by this: https://github.com/twitter/twemproxy/commit/f08088f7ed8a370a45365723ba64fb6fc463a0b9\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11545552/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11595395", "body": "@charsyam  I will take a look at this in next two days\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11595395/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11600566", "body": "you need to specify pool name. So you config file should look like:\n\nPOOL_NAME:\n  listen: 127.0.0.1:22123\n  hash: fnv1a_64\n  distribution: ketama\n  timeout: 400\n  backlog: 1024\n  preconnect: true\n  auto_eject_hosts: true\n  server_retry_timeout: 2000\n  server_failure_limit: 3\n  servers:\n- 127.0.0.1:11212:1\n- 127.0.0.1:11213:1\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11600566/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11600568", "body": "let me know if this works? \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11600568/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11600734", "body": "Use the following\n\n```\nola:\n  listen: 127.0.0.1:22123\n  hash: fnv1a_64\n  distribution: ketama\n  timeout: 400 \n  backlog: 1024\n  preconnect: true\n  auto_eject_hosts: true\n  server_retry_timeout: 2000\n  server_failure_limit: 3\n  servers:\n   - 127.0.0.1:11212:1\n   - 127.0.0.1:11213:1\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11600734/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11601287", "body": "twemproxy should solve your needs if you use memcache ascii protocol\n\nplease read this: https://github.com/twitter/twemproxy/blob/master/README.md\n\nand this: https://github.com/twitter/twemproxy/blob/master/notes/recommendation.md\n\nbefore starting to use twemproxy\n\nhth\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11601287/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11603519", "body": "ketama is not saying that it is going to distribute keys evenly; please google and read about ketama and consistent hashing. here is a start: http://en.wikipedia.org/wiki/Consistent_hashing\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11603519/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11617491", "body": "You have to write separate client app, that reads all the keys from existing 8 g redis server and write those keys to a cluster of backend redis server through twemproxy\n\nlet me know if this works\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11617491/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11630054", "body": "@charsyam I like your idea\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11630054/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11727049", "body": "Please read the README.md. You can use -c or --conf-file to specify the location of the configuration file. Otherwise by default it uses $PWD/conf/nutcracker.yml\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11727049/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11727216", "body": "Twemproxy can only support redis commands that takes \"key\" as an argument. This is the case because sharding layer of twemproxy routes redis commands based on consistent hashed values on \"key\".\n\nSince AUTH command does not take key as an argument we cannot support this in twemproxy.\n\nIs there a reason why you use AUTH in your setup? Can you make it work without Redis AUTH?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11727216/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11745671", "body": "github recently took away the ability to upload tarballs :(  (https://github.com/blog/1302-goodbye-uploads)\n\nAnway, I moved all the distribution tarballs to google code to make this easy. You will find the 0.2.2 tarball there in the \"Dowloads\" section.\n\nSee: http://code.google.com/p/twemproxy/\n\nAbout \"nutcracker\" verus \"twemproxy\", note that only the top level directory name is twemproxy and everything else is nutcracker. So, I don't think this should cause an issue with creating spec files\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11745671/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11746977", "body": "@TheRealBill I believe the way redis authentication works is that once a connection to a redis node has been established, the client sends the \"AUTH password\" command to the redis node before sending any other commands. A failure to do so would mean that the redis server configured with requirepass directive would reject any unautenticated connections.\n\nAn auth proxy will not solve the issue, because connections made my twemproxy to backend redis nodes would still be unauthenticated.\n\nThe right way to solve this problem IMHO, would be to implemented authentication on the client side so that you have an architecture where clients talking to twemproxy / redis nodes are trusted and hence don't required authentication.\n\nthoughts?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11746977/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11747041", "body": "thanks @charsyam \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11747041/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11747084", "body": "@hamgua As @charsyam suggested, the best way to use twemproxy in your situation is to use a single db (which by default would be db 0)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11747084/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11766272", "body": "call the rpm nutcracker\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11766272/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11766326", "body": "@conmame Looks good. Thanks for doing this!\n\nIf you could resubmit a pull request with the following changes, I will go ahead and merge it:\n- Revert the last commit -- \"Remove unuse define\"\n- Use 4 space indentiing in your changes\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11766326/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11771030", "body": "@TheRealBill I like the first option of \"python proxy\" only for commands that need auth. I believe this design is clean.\n\nThe second option would require hacking of twemproxy. You can easily achieve this by modifying (commenting out) some of the code between https://github.com/twitter/twemproxy/blob/master/src/proto/nc_redis.c#L396 and https://github.com/twitter/twemproxy/blob/master/src/proto/nc_redis.c#L805\n\nBtw, let me know if rackspace is using twemproxy. If so, I will add you guys to the users list (https://github.com/twitter/twemproxy/blob/master/README.md#users)\n\nI will close this issue for now. Let me know once you finish implementing the solution and how you ended up solving your auth issue.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11771030/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11799965", "body": "Looks good overall :) Could you resubmit after incorporating the following comments:\n- move the changes from contrib/ folder to scripts/ folder as the contrib folder corresponds to 3rd party libraries\n- instead of including nutcracker.yml, could you just refer to nutcracker.yml in conf folder (Is that possible?)\n- use 4 spaces indentation\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11799965/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11817086", "body": "I fixed the non-zero exit status issue here: https://github.com/twitter/twemproxy/commit/67ad4951488e2ce1561add92839754d7cbc29359\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11817086/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11817365", "body": "Please read this: https://github.com/twitter/twemproxy/issues/14 and this https://github.com/twitter/twemproxy/blob/master/notes/recommendation.md#liveness\n\n@charsyam has an oustanding pull request for this\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11817365/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11872388", "body": "Could you use twemproxy  v0.2.2 and run twemproxy with verbosity level 8 (-v 8) and print logs here\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11872388/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11872627", "body": "Could you please print twemproxy logs here by running the binary at -v 8 level\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11872627/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11872646", "body": "The problem is that your client is sending \"version\" string which is not recognized by twemproxy \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11872646/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11872768", "body": "Which language?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11872768/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11873838", "body": "just configure xmemcached to not send \"version\" command. Maybe use TextCommandFactory? \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11873838/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11874179", "body": "maybe post on xmemcached google group list\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11874179/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11875634", "body": "post on xmemcached mailing list\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11875634/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11893457", "body": "take a look at finagle-memcached or spymemcached for client library; Alternately you can also hack xmemcached to not send the version string\n\nI'm closing this issue as twemproxy does not support version command by design\n\nfeel free to ping if you have questions.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11893457/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12101782", "body": "why not have separate pools for this case? \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12101782/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12232147", "body": "twemproxy does not provide an automatic way to move the affected keys. To need to code up a workaround to make this happen.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12232147/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12232179", "body": "thanks @charsyam. See https://github.com/twitter/twemproxy/blob/master/notes/redis.md for list of supported commands\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12232179/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12232220", "body": "this should be easy to support, as long as you use hash tags (https://github.com/twitter/twemproxy/blob/master/notes/recommendation.md#hash-tags) \n\nI will keep this issue open until we have this fixed\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12232220/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12232274", "body": "@highkay you might have to code up a manual workaround to make this happen.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12232274/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12232377", "body": "we have 'hash tags' now on head. so this should be easy to support\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12232377/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12257220", "body": "the support for \"mset\" would be using hash tags\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12257220/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12556143", "body": "@srned thanks! I am little bit overloaded right now. But, I promise I will take a look at your branch as soon as possible. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12556143/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12688036", "body": "if configuration file is invalid, then twemproxy does die with no zero exit status (see this: https://github.com/twitter/twemproxy/commit/67ad4951488e2ce1561add92839754d7cbc29359)\n\nfor \"-t\" option, we print \"valid\" or \"invalid\" conf file which is the correct behavior because you are testing the syntax validity of the conf file\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12688036/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12693504", "body": "@dcartoon the patch looks good. thanks!\nhere are my thoughts:\n\nFor commands that take multiple keys and are not easily distributable (because of atomicity guarantee like msetx or because implementing it is tricky like sinter), the only way to make these 'non-distributable' commands to work with the sharding layer is to use the SAME 'hash tag' for every key in the command. \n\nNow the question is how do we make sure that all the keys in a given command have a hash tag and have the same hash tag. For example, an input like 'SINTER {abc}foo {abc}bar' is valid but an input like 'SINTER {cba}foo {abc}bar' is not. \n\nDoing this check on the twemproxy side would make the parsing code unnecessarily complicated. So, my take would be to keep the parsing code simple, document the above scenario and do sharding only on the first key. So, if the client fails to put the same hash tag in all the keys of a multi-key command , then the client might end up seeing unintended behavior\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12693504/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12693650", "body": "@ferenyx send a pull request :)\n\nI think for EVAL and EVALSHA commands taking >= 1 key, we should do  sharding on the 1st key with the hope that client uses 'hash tags' and has the same hash tag for all the keys in the EVAL and EVALSHA command.\n\nSee https://github.com/twitter/twemproxy/pull/54#issuecomment-12693504 for more discussion on this\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12693650/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12693776", "body": "if you are using cluster as a cache only (where items expire or get evicted) you can use a solution that I call \"dual write and single read\". \n\nInitially you have a single cluster say A. Now you want to expand the cluster. Lets say the expanded cluster is B. So, once cluster A and cluster B is set up, on the client side you write to cluster A (original) and cluster B (new one) but read only from cluster A (for a period of time). After enough 'time' has passed you can switch reading to cluster B and monitor the hit-rate. If you get the same hit rate as before, this switch of read traffic from A to B is reasonable. So you can stop dual writing, and retire cluster A. Now your setup is only writing and reading from cluster B\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12693776/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12693875", "body": "not a bug. see this: https://github.com/twitter/twemproxy/blob/master/notes/recommendation.md#liveness\n\nand this excerpt:\n\n_To ensure that requests always succeed in the face of server ejections (auto_eject_hosts: is enabled), some form of retry must be implemented at the client layer since nutcracker itself does not retry a request. This client-side retry count must be greater than server_failure_limit: value, which ensures that the original request has a chance to make it to a live server._\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12693875/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12693906", "body": "could you run twemproxy with verbosity level 8 and paste the logs here\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12693906/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12712848", "body": "It is possible to answer which servers are not in the pool looking by looking at the following stats\nhttps://github.com/twitter/twemproxy/blob/master/src/nc_stats.h#L38\nhttps://github.com/twitter/twemproxy/blob/master/src/nc_stats.h#L38\nhttps://github.com/twitter/twemproxy/blob/master/src/nc_stats.h#L38\n- server_eof is incremented when the server close the connection normally (which should not happen because we use persistent connection)\n- server_timedout is incremented when the connection / request to server timedout\n- server_err is incremented for any other kinds of error\n\nSo, on a given server, the cumulative number of times server is ejected can be computed as (server_err + server_timedout + server_eof) / server_failure_limit\n\nA diff of the above value between two successive minutes would give you a nice timeseries graph\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12712848/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12713016", "body": "Also if you run twemproxy with \"-v 6\" option you would see the following log line when a server is ejected. \n\nhttps://github.com/twitter/twemproxy/blob/master/src/nc_server.c#L278\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12713016/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12822637", "body": "pgrep nutcracker\n\nOr,\n\nnc localhost 22222\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12822637/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12823845", "body": "Based on the logs, I don't see any issues!\n\ncould you reproduce the issue by sending small amounts of data\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12823845/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12824396", "body": "so, when a string is > 2000 bytes what is the exact error you see on the client side\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12824396/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12873481", "body": "The patch looks good. if you do the following changes, I will go ahead and merge this:\n- for (1) lets close the connection (i.e. error out) if the command 'EVAL' has 0 keys specified.\n- I am fine with the (2) behavior\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12873481/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12899749", "body": "Thanks @ferenyx :)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12899749/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12908973", "body": "Fixed by @ferenyx. See: https://github.com/twitter/twemproxy/commit/4e117f814caa9e336b959fec0000f77d73abb449\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12908973/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12909685", "body": "here is one idea: take dump of all  your redis caches. the create your new expanded cluster and use redis-cli to populate the new cluste with the dump that you just took\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12909685/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12919102", "body": "Merge this manually https://github.com/twitter/twemproxy/commit/5144e0c94b39b39aa377f6efb37123aa2056e730\n\nThanks @dcartoon! \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12919102/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12919128", "body": "Fixed by @dcartoon here https://github.com/twitter/twemproxy/commit/5144e0c94b39b39aa377f6efb37123aa2056e730\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12919128/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13255011", "body": "Absolutely! How do you intend to implement AUTH support?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13255011/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13256006", "body": "So, you are saying that when a connection is established to a server, you first send out an AUTH command on that server and only if that succeeds do you use that connection\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13256006/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13270104", "body": "thanks @charsyam! sorry I haven't reviewed your \"liveness\" code yet. but it is on my plate though\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13270104/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13395647", "body": "@yashh ran into this issue\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13395647/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13409161", "body": "Eg of a sample failure\n\n[Sat Feb  9 16:20:43 2013] nc.c:177 nutcracker-0.2.2 started on pid 15339\n[Sat Feb  9 16:20:43 2013] nc.c:181 run, rabbit run / dig that hole, forget the sun / and when at last the work is done / don't sit down / it's time to dig another one\n[Sat Feb  9 16:20:43 2013] nc_util.c:481 address resolution of node '\"abc' service '11211' failed: Name or service not known\n[Sat Feb  9 16:20:43 2013] nc_conf.c:490 conf: directive \"servers\" has an invalid value\n[Sat Feb  9 16:20:43 2013] nc.c:187 done, rabbit done\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13409161/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13503668", "body": "The supported/unsupported redis commands are detailed in notes/redis.md, which is linked from README.md\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13503668/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13507595", "body": "Glad you liked it @eveiga \n\nI believe using the external process the way you described makes sense. In fact you can have two twemproxy processes running - one routing the traffic to all the masters and the other to all the slaves. On a failover event, you switch from one twemproxy to the other\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13507595/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13902306", "body": "Yes :)\n\nAdd code to src/hashkit\nplease read the coding style guidelines: https://github.com/twitter/twemproxy/blob/master/notes/c-styleguide.txt\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13902306/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13904720", "body": "looks good @mezzatto  .. I guess I can assume that this crc16 works in your environment, right?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13904720/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13905110", "body": "Yes that seems like one possible way to do it.\n\nAlternatively for commands that aren't key-specific we could execute them outside twemproxy. For example the \"SCRIPT LOAD\" command can be executed by an external proces on all the redises since it is not a command that you would run really frequently\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13905110/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13986401", "body": "your telnet client should send crlf and I believe it is not. See man telnet\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13986401/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13991679", "body": "> > This uses the new github relative links I'd be happy to switch the rest of them if you'd like.\n\nthat would be great :)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13991679/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14122609", "body": "it you guys can make this generic enough, we can check this into the scripts/ folder of twemproxy \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14122609/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14249260", "body": "yeah we could add that support.\n\nThough using TCP socket over unix domain socket is not necessarily a perf overhead\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14249260/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14264131", "body": "this is easy to fix\n\njust update conf_validate_server() in nc_conf.c\n\nSee https://github.com/twitter/twemproxy/blob/master/src/nc_conf.c#L1157-L1161\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14264131/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14264468", "body": "I believe the right fix would be to only validate \"name\" and not \"pname\". Because if you look at conf_add_server(), name is equal to pname, when name is not present.\n\nNote that in your case name would be Server0 | Server1 | Server1 | Server3, while pname would be the 127.0.0.1:6379:1, 127.0.0.1:6379:2,...\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14264468/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14266586", "body": "fixed: https://github.com/twitter/twemproxy/commit/10b2f14135e15a89aaaead66dd9724d07feafa38\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14266586/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14302830", "body": "@mezzatto instead of something like this:\n\n```\n   servers:\n    - 127.0.0.1:6379:1 Server0 Server1\n    - 127.0.0.1:6380:1 Server2 Server3 Server4\n```\n\nyou do something like this:\n\n```\n   servers:\n    - 127.0.0.1:6379:2\n    - 127.0.0.1:6380:3\n```\n\nThe later configuration gives server \"127:0.0.1:6380\" 1.5 times more weight when compared to \"127.0.0.1:6379\" and in the end you will have you 2 TCP connections\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14302830/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14303097", "body": "@mezzatto of course, we need to fix \"modula\" distribution to work properly with weights. The fix should be easy, you would have to change -- https://github.com/twitter/twemproxy/blob/master/src/hashkit/nc_modula.c#L113\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14303097/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14528308", "body": "send a pull request :)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14528308/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/1694665", "body": "I am still actively testing this branch...I should have a stable build out in few weeks.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/1694665/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829605", "body": "hmm...this is a wrong commit from my end. ignore this. apologies\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829605/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829681", "body": "I chose the verbose way because the log message prints the line number which helps in deciphering the condition that was triggered\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899536", "body": "No need for a py script. You can generate the strings using macro magic called strigificaion :) This way your code never gets outdate\n\nSee this for reference -- \n- https://github.com/twitter/twemproxy/blob/master/src%2Fnc_stats.h#L23-L49\n- https://github.com/twitter/twemproxy/blob/master/src%2Fnc_stats.h#L117-L122 \n- http://gcc.gnu.org/onlinedocs/cpp/Stringification.html\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899536/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899553", "body": "s/char */uint_8 */\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899558", "body": "usec */\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899558/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899586", "body": "space between the closing paren and curly brace. So `log_loggable(LOG_NOTICE) {`\nHere are the coding guidelines -- https://github.com/twitter/twemproxy/blob/master/notes/c-styleguide.txt. Would really appreciate if you follow it\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899586/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899603", "body": "just call it req_log()\n\nalso format is `struct msg *req`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899603/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899612", "body": "formatting -- `/* a fragment */`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899612/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899643", "body": "formatting - `struct msg *req`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899643/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899649", "body": "formatting - `if (rsp) {`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899649/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899660", "body": "call this `int_64_t start_ts /\\* request start timestamp in usec */\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899660/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436410", "body": "Rename\ntotal_connections to ntotal_conn\ncurr_connections to nconn\ncurr_client_connections to nclient_conn\n\ncurr_connections should be int32_t\ncurr_client_connections should be int32_t\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436410/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436414", "body": "how did we come up with RESERVED_FDS number as 32\nrename it as NC_NUM_RESERVED_FD\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436414/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436416", "body": "I think we might still hit this scenario - One instance where I see this happening is that you close the connection() from the code (decrement the counters), but the sockets are still in TIME_WAIT state and hence are not available for accept() sys call.\n\nSo, instead of panic, we should log() and return NC_ERROR and not set p->recv_ready to 0\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436416/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326121", "body": "what is this for?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326121/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326123", "body": "why did we bump this up?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326123/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326127", "body": "[formatting] `status: %d`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326127/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326172", "body": "is get_mbuf() function required? It seems to be called only in msg_make_reply(). Prefer not having this function\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326172/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326178", "body": "why are these commented out? Isn't it required to trigger the out event?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326178/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326216", "body": "Should we move the lines 482 - 495 into server_pool_idx()? It seems that only place where the tag logic is not used is in server_pool_server(), which is surprising, because I think that tag logic should also apply there equally.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326216/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326257", "body": "In this piece of code, you allocate message structs for every server in the pool - ncontinuum messages, even if incoming messages are destined to < ncontinuum servers.\n\nOnly at line 818-819, do you some of these allocated message structs. Can we refactor this differently - maybe two loops where the first loop aggregates all the keys destined per server and the second loop has the logic\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326257/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326266", "body": "why commented out?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326266/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326270", "body": "[format] delete new line\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326270/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326272", "body": "[format] delete new line\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326272/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326309", "body": "We should refactor this piece of logic. All the memcache specific logic should reside in nc_memcache.c and all the redis logic should reside in nc_redis.c. This separation of concerns is a good thing because it divorces the protocol handling from the core connection/routing logic enabling us to plug-in other protocols (like memcache binary impl) easily\n\nWe can use callbacks to achieve this abstraction. Look at pre_splitcopy(), post_splitcopy(), pre_coalesce() and post_coalesce() for reference\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326309/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21946628", "body": "can we call this init?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21946628/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21946653", "body": "s/Only/only/\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21946653/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21946669", "body": "we only use comments of form /\\* */ in twemproxy\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21946669/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21946725", "body": "since this only affects redis, can we have two init handlers for memcache (memcache_conn_init) and redis (redis_conn_init) and move the code from server_conn_init to redis_conn_init\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21946725/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21946883", "body": "can you use nc_snprintf\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21946883/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21946971", "body": "use mbuf_\\* methods to do the copy\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21946971/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21946982", "body": "use /\\* */ comment\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21946982/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21947064", "body": "should we put a cap on maximum number of redis db -- like redis_db must be less than 64\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21947064/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21947322", "body": "can we compute number of digits in nc_conf.c and store it in conf structure -- no need to recompute it on every connection request\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21947322/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21947599", "body": "use uint8_t\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/21947599/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/32696311", "body": "delete this line\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/32696311/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/35277176", "body": "all functions in twemproxy must be namspaced like redis_ \n\nalso the return value must be either rstatus_t or bool\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/35277176/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/35277206", "body": "code must follow: https://github.com/twitter/twemproxy/blob/master/notes/c-styleguide.txt\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/35277206/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/35277248", "body": "even though this works, stylistically returning int and comparing boolean is not good. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/35277248/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/33058056", "body": "instead of creating a the all possible combinations of connection (client | proxy | server) and protocol (redis | memcache), I think it would make sense to keep the original design of three types of connection (client | proxy | server) and a separate field for protocol.\n\nThe questions then becomes how to you express CONN_SIGNAL. For that I propose we should introduce a new structure (struct signal or something like that) with signatures something like this:\n\nstruct conn {\n  uint32_t tag;  /\\* initialized to some conn specific magic let's say 0x00000001 */\n}\n\nstruct signal {\n  uint32_t tag; /\\* initialized to some unique signal magic let's say 0x0000002 */\n}\n\nThis way when you in the event callback you can have different handlers for conn based events and signal based events\n\nAlso I've been doing cleanups based on your code in the branch \"config-reload\". Would be nice to incorporate those cleanups. thanks!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/33058056/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}]}, "sdarwin": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/93", "title": "redis HA Automatic Failover", "body": "From Antirez:\n\n\"twemproxy is already able to monitor instance errors, count the number of errors, and eject the node when enough errors are detected. Well it is a shame it is not able to take slave nodes as alternatives, and instead of eject nodes use the alternate nodes just after sending a SLAVE OF NOONE command. This would turn it into an HA solution as well.\"\n\nJust for a little context, consider ideas from haredis, https://github.com/carlos8f/haredis:\n\n\"Easily build a cluster out of 3 or more (default-configured) redis servers\nAuto-failover due to connection drops\nMaster conflict resolution (default your servers to master, and haredis will elect the freshest and issue the SLAVEOF commands)\nFreshness judged by an opcounter (incremented on write)\"\n\nProposed solution:\n\ntwemproxy monitors a group of shards , each one composed of a group of master-slave nodes.      If it's going to eject a node (which it's already capably of), then at this time, it issues SLAVE OF NOONE to a slave, and makes that slave active in the list. \n\nThe result of all this is somewhat similar to mongodb with sharding + replicasets, at least in the final outcome.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/93/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "matschaffer": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/68", "title": "SCRIPT LOAD support", "body": "It seems like it'd be a reasonable implementation to have any SCRIPT LOAD get sent to all servers. Possibly other commands that aren't key-specific as well.\n\nAm I missing something or is this a reasonable idea that could be accepted as a PR?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/68/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13966242", "body": "Yep, just proved it to myself with experimentation. I'll take a crack at adding that note to the documentation.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13966242/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14113842", "body": "@eveiga thanks for the redis-sentinel reminder. So far it looks like this will work well.\n\nHas anyone built the bits to update twemproxy when redis-sentinel finishes a failover?\n\n@manjuraj would you recommend anything more graceful than simply rewriting the twemproxy config and restarting it?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14113842/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14117747", "body": "@eveiga any chance of sharing what you've come up with?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14117747/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14118192", "body": "Sure! Even just a gist is great. Always nicer to have some collaboration. :)\n\nOn Feb 26, 2013, at 9:47, eveiga notifications@github.com wrote:\n\nNo problem. It's on node.js and a bit tight with our structure but, still\nwant it?\n\n\u2014\nReply to this email directly or view it on\nGitHubhttps://github.com/twitter/twemproxy/issues/67#issuecomment-14118050\n.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14118192/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14250682", "body": "@eveiga how's yours panning out? Over here it seems to work if I'm careful about the startup order. But if the agent comes up before the sentinel the agent seems to deadlock after a certain number of retries. Have you run into that or are you controlling start order more carefully.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14250682/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14250779", "body": "@eveiga btw, I have this up at https://github.com/matschaffer/redis_twemproxy_agent as something I can pack with npm and get some rough testing around. I took out the email notifier though since we'll probably want to notify via other means.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14250779/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14305542", "body": "No problem! After further testing I'm not sure that's the case (with the startup order issue). Not sure what caused the lack of reconfiguration on my first test but I haven't been able to replicate it. My latest commit logs a lot to stdout in hopes that I can tell what's up if it happens again.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14305542/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "eveiga": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/67", "title": "HA - Automatic Failover", "body": "Hi! First of all, thanks for the proxy, it has been really helpful :)\n\nI'm in need of a decent solution for automatic failover and already stated that twemproxy doesn't support it. Any thoughts or ideas on it? \n\nI was thinking on a external process that would leverage the use o redis-sentinel and on a master-switch event updates the IP address on nutcracker.conf and restarts the service.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/67/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13520660", "body": "@manjuraj I've already thought on that solution. Can I use the slaves cluster to perform read operations? Or the hashes wont pair with the ones for the master cluster?\n\n@bmatheny are you using my sugestion or manjuraj one?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13520660/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13524365", "body": "@bmatheny sorry for the boring questions :) dont you experience a window of downtime during that restart? If yes, How do you cope with that?\n\nBTW, are you using any pool of twemproxy just with slaves for reading?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13524365/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13524486", "body": "Humm, I forgot you are using it with memcache, dont know if the last question fits your use case!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13524486/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13541937", "body": "@bmatheny Thanks for the tips, I'll go on with that solution!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13541937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14115240", "body": "@matschaffer Yes, I've developed a simple service that attaches a handler to the \"master-switch\" event emitted by redis-sentinel, updates twemproxy.conf with the new info and restartes the service. So far so good with the tests, I'll put it in production in a short time.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14115240/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14118050", "body": "No problem. It's on node.js and a bit tight with our structure, still want it?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14118050/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14118809", "body": "https://gist.github.com/eveiga/5039007\n\nAs I said, it's pretty tight with our structure (init scripts path, mails, etc) and could be a lot configurable, but it can give you a starting point.\n\nSugestions are welcome!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14118809/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14302361", "body": "Hey @matschaffer, I've assumed that the sentinel was already running, but indeed we should have some kind of reaction on a failed startup. Thanks for packing this in a new repo, I'll take a look at it during the weekend and try to do some contribution!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14302361/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "highkay": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/55", "title": "How to move keys (data as well) between nodes added or removed?", "body": "According to issue https://github.com/twitter/twemproxy/issues/49 , I am informed to make a manual workaround to fix the problem. But I don't know how to manually move affacted keys and data from existing nodes to new added nodes for the hashing policy. Could somebody give some hints for the situation?\n\nAlso I am interesting in the way Twitter or other team shard exist redis data during scaling with the twemproxy. Full copy would not be a nice answer for me.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/55/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11634901", "body": "hi @charsyam If I am not misunderstand your idea, after 1,2 and 4, it made a full copy of my original node on the new node, while the two full copies are not rehashed at all!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11634901/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11634970", "body": "hi @manjuraj I am sure the twemproxy would be convenience to run on a fresh redis cluster, but in my situation, it seems I would have to write my resharding programme. I wonder if there could be a solution like https://github.com/kni/redis-sharding-hs.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11634970/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11634994", "body": "@charsyam First of all, I am sorry for maybe I am not clear my situation. Now we are not using twemproxy, only direct redis connections in our code. So the keys on the original node are not hashed at all. If I made the slave fresh new node fully copy the keys/data, it helped nothing.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11634994/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12156194", "body": "@charsyam Does it mean there has none solution to access the some of the \"old\" data after I added a new node in the twemproxy? Because the twemproxy could not find some \"old\" data on a new node (in fact they are on the old nodes) following the key hashing policy. Does the twemproxy have any automatic way to move the (affacted) data on exist nodes to new nodes?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12156194/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12749617", "body": "@manjuraj thank you for your reply.Unfortunately, we use redis cluster as storage than cache. To avoid data \"losing\", your advice would not work. Could you give more hints?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12749617/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12907559", "body": "@fifsky Thank you for your mention. We are not ready for data losing and rehashing all data is not accepted also.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12907559/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "tomponline": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/42", "title": "Connection re-try after failed instance before accepting commands", "body": "I have tried setting up a 3-node cluster of nutcracker and 2 redis servers using the config below:\n\n alpha:\n  listen: 127.0.0.1:22121\n  hash: fnv1a_64\n  distribution: ketama\n  auto_eject_hosts: true\n  redis: true\n  preconnect: true\n  server_retry_timeout: 2000\n  server_failure_limit: 1\n  servers:\n- tp.dev.infinity.local:6379:1\n- rn.dev.infinity.local:6379:1\n\nThis works fine, until I stop one of the redis servers.\n\nThen requests through nutcracker for keys that are hashed to the downed redis server are generating an error to the requesting client each time the server_retry_timeout passes and an attempt to reconnect is made.\n\nIs there a way of having nutcracker try re-connecting to the downed redis server before re-adding it to the hash ring. I had thought that this was what the preconnect option was for, but I think I have misunderstood its use.\n\nThanks\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/42/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11729868", "body": "Also, the package name is nutcracker in the tarballs, rather than twemcache, this makes it rather confusing when coming to build RPM packages.\n\nI will post my SPEC and init scripts here once I can get the build working.\n\nThanks\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11729868/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11764402", "body": "Hi, thanks for the quick response, I hadn't realised that the code was hosted on google, thats great.\n\nShould the RPM be called nutcracker or twemproxy, what is the actual product name?\n\nThanks again\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11764402/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11802421", "body": "Hi, no problem I'll get those changes made.\n\nIncluding the custom config from the source tarball will be fine, and I'll do the same for the init script too.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11802421/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11802533", "body": "One thing I have noticed is that if the config file is wrong, or if the program cannot start for another reason, perhaps there is something already listening on the same port. The program does not seem to return a non-zero error code and so the init script says \"Starting nutcracker OK\", even though it has not started.\n\nIs it possible for nutcracker to return non-zero?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11802533/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "yashh": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/14", "title": "Robust hash ring failure retry mechanism", "body": "So once a host failed and we have a server_retry_timeout of 30 secs nutcracker retries the failed host on production traffic. I think nutcracker needs to perform a background hearbeat request like fetch a simple key and assure that host is up.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/14/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/6", "title": "Support to reload configuration file without killing process", "body": "Having a signal like HUP or something to reload the configuration file will be really helpful since we do swap instances sometimes. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/6/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mortonfox": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/fe68175e0200e3c2589139438ff3efa392042aa6", "message": "Update the sensu-metrics link"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dennismartensson": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/80ef6a7444fd5ae97fcab9606c1abedc19f00824", "message": "Update README.md\n\nAdded Greta to the list of companys"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kalifg": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/b87ba1abfe6a814999279e69af7ce07ba0ff6c68", "message": "Fix typo"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rohitpaulk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/eed195341a02fa688b0dfb784d120f337f15a454", "message": "Update redis docs for PING and QUIT\n\nSupport for these was introduced in\n@4175419288ef66d95e082cfa2124e77fe6d4fe6d."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "postwait": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/1e078e9e9d97560825ae4f1245177a0af29e3c82", "message": "Fix typo circunous -> circonus"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Krinkle": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/91a68d3c42638eb8178001f4d67d2606dcd80f51", "message": "readme: Link to HTTPS for wikimedia.org"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "esindril": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/51c5228acdd8a324a43107330f6b936de028dc0c", "message": "Fix spec file to work for RHEL >= 6 and wrong changelog date"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/205323f87deb0f0004963717f2d7a80eed8e9c3b", "message": "Add script to build the SRPM package for RPM-based distributions"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "caniszczyk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/0e8708be4d365163a1061c2a89ecc344e21203d6", "message": "Add Uber to the list of Adopters\n\nhttp://highscalability.com/blog/2015/9/14/how-uber-scales-their-real-time-market-platform.html"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/fbe945c96780cb64e72d927c18fbbbb8bf4b8664", "message": "Merge pull request #346 from stoph/master\n\nAdding AOL"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/fbf15a90bdb19cf719913d3f461183400787b701", "message": "Merge pull request #335 from dentarg/patch-1\n\nFix typo in README"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ebd0a9b9269c3a4ea35d8521f78d2e1713fcace5", "message": "Merge pull request #291 from bmonkman/master\n\nAdded Hootsuite to list of twemproxy users in readme"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/9063042b6ba5d11e2dcfa5d19f5c476d06465419", "message": "Add Committers section to reflect reality"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426128", "body": "Any updates on this patch @manjuraj ?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426128/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426280", "body": "Just to note that this is somewhat of a hack, but it works for now. I didn't find any tests to run via gtest or whatever harness.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426280/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10983017", "body": "We were using Travis CI before they had formal support for C... so we had to hack it...\n\nhttp://about.travis-ci.org/docs/user/languages/c/\n\nI guess we can update the .travis.yml file now ;)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10983017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11587418", "body": "I restarted the build... looks like a transient failure...\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11587418/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13902387", "body": "Could you also add yourself to the \"Users\" section in the README too? That would be awesome!\n\nhttps://github.com/twitter/twemproxy/blob/master/README.md#users\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13902387/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13904378", "body": "Please don't copy the CRC16 impl from the Linux Kernel. Can you find one that is under a BSD license or a more liberal license? We can't include GPL code within twemproxy without changing the license.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13904378/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "invalid-email-address": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/b6ac1bbc34aa0f470b55893090c7cf6a696ec184", "message": "optimize performance when single key del"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "anubhavmishra": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/8e702ce619d47d6ccb704895a4dbb1e1b9228b33", "message": "Added Hootsuite as twemproxy users"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "begeekmyfriend": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/8aeb6f34b818bdbc7dce79e4a0ff4900239874d7", "message": "Remove redundant conditional judgement in rbtree deletion\n\nIn binary tree deletion, as is defined, first we need to find the\nminimum node of its right sub-tree to replace the location of the node\nto be deleted when it has both its left and right children. Then we\nshall trace its sub-tree to find the minimum node in `ngx_rbtree_min`\nfunction. Since the terminal condition of the tracing loop has already\nbeen set as `node->left == sentinel`, there is no need to leave the\nfollowing if-condition judging whether the substitute node's left child\nis the sentinel any more because the program will never run here. And\nthe code will be cleaner as well as may run faster in some situations\nafter removing this redundant conditional branch.\n\nIn C++ SGI STL stl_tree.c source, we can see there is no if-condition\njudgement in rbtree deletion:\n\nhttps://github.com/dutor/stl/blob/master/sgi/stl_tree.h#L317\n\nBy the way, the current released Nginx source still has this issue but\nI have no idea how to commit this patch to the upstream.\n\nSigned-off-by: Leo Ma <begeekmyfriend@gmail.com>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "davis": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/26fe203b9d69edbb075eb9f6247237cdbf0b324e", "message": "Update README.md\n\nFix small typo"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mckelvin": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/d17cdf46eb98bb110287d582d953c21baaac164d", "message": "Allow touch key with negative number"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/73ed131efa6574906cd4774dc58c34e3661ae7db", "message": "Support \"touch\" command for memcached"}], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/315", "title": "Support unittest using google-test.", "body": "- Support unittest using google-test.\n- test_hashkit as an example.\n- enable unittest in travis.\n\nThe gtest-1.7.0.zip is downloaded from [googlecode](https://code.google.com/p/googletest/downloads/detail?name=gtest-1.7.0.zip&can=2) , please verify the SHA1 Checksum before merging this PR.\n\nALSO SEE:\n- https://github.com/twitter/twemproxy/issues/197\n- https://github.com/twitter/twemproxy/issues/222\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/313", "title": "hashkit: Add correct fnv1* implementations.", "body": "Old fnv1\\* implementations(fnv1_64/fnv1a_64/fnv1_32/fnv1a_32) are not correct as per spec but are compatible with libmemcachd. The corresponding correct implementations are fnv1_64a/fnv1a_64a/fnv1_32a/fnv1a_32a.\n\n~~This issue is similar to 653b05f861ae3ce0ff143e1067af378f677a4d29 (@manjuraj).~~\n\nhttp://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function says:\n\n```\nThe XOR is an 8-bit operation that modifies only the lower 8-bits of the hash value.\n```\n\nSuppose key is a utf-8 encoded string `\"\\xe4\\xb8\\xad\"`(Chinese Character`\u4e2d` in UTF-8). Let's take `\\xe4`(`0xe4 >= 0b10000000`) as an example:\n\n``` c\n(uint32_t)(char)0xe4 = 4294967268 // buggy\n(uint32_t)(uint8_t)(char)0xe4 = 228 // correct\n(uint32_t)(unsigned char)(char)0xe4 = 228 // also correct\n```\n\ncc @windreamer\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stoph": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/e39f16a56607d3874b94997a471692020791db70", "message": "Adding AOL"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dentarg": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/c8fe367b9f1f5b9512da11a70aa3af494da524cd", "message": "Fix typo in README\n\ndisributed => distributed"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Serekh": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/cbc50aefbf66e0d042c98196a6899ca3052f5126", "message": "Added beholder to the Utils section, a python agent that works together redis sentinel used to extend the HA capabilities of twemproxy even after a redis node has failed."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yak0": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/67ac941db9fd13937f85b6aeb65a03d7fcfa4d06", "message": "Utils Update\n\nTwemsentinel is added"}], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/320", "title": "Twemsentinel", "body": "Python twemproxy agent for the master-change event and nutcracker config/restart\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "atdt": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/11c8d219ca4f283ef18a10e3b67b8d498da3340a", "message": "Allow file permissions to be set for UNIX domain listening socket\n\nWhen 'listen:' starts with a slash, it is interpreted as a UNIX domain socket\npath for twemproxy to bind. This patch allows file permissions for the socket\nto be specified via a space-separated permission field following the path.\n\nExamples:\n\nPermissions not set, so use defaults (same as before, still supported):\n\n  listen: /var/run/nutcracker/nutcracker.sock\n\nSet socket permissions to ugo+rw:\n\n  listen: /var/run/nutcracker/nutcracker.sock 0666"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zwChan": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/1f1c7d43da1eed45e74e90f9392bcda54c6f9f3c", "message": "Check the configuration before start in the nutcracker.init script."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bmonkman": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/dbb63434c71e2292d0a0df448c40ea33b2d9dbc7", "message": "Added Hootsuite to list of twemproxy users in readme"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alicebob": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/d10d6bec9de6ef92893a90f67ed8ca9a90d5ba79", "message": "Add `redis_auth` to the README.md"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rhoml": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/67eebc7ec274332f8f12eb6bf45dd669d015e51a", "message": "Update description"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/79c0c68aa089be4deb7452d9ec4d19f19a137209", "message": "Adds init script for debian servers"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/790e1ad8c77fb5db34dd975beeea27276ccc5291", "message": "Adds documentation"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "guilhem": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/bc9eb4948d414d04ddb02a8fbdbf47c52047924c", "message": "Add chef cookbook in `utils` section"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/0c2bf6ea94e09728d2632886384e7b50fae88310", "message": "Add PPA informations to README"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ngaut": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/867e3f326372c731357b2e13fd7c2ed0a58c8a64", "message": "fix redis.md"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "areina": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/abdf20475341b56eebe8c996beb510150dca87e4", "message": "Update Redis notes with info about SORT support."}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/a97c8e23620e449ae2db480d8a0165ae022f69fe", "message": "Add support for SORT command in redis."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mkadin": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/672045f915bb7b4394df3745b42da15f53c1fb31", "message": "Adjust doc title\n\n'Redis Commands Supported' seems to indicate that all of the commands below are supported, i didn't read the 'Supported?' column and got tripped up.  I think 'Redis Command Support' seems to indicate that more info about what commands are supported is available in this section."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mattrobenolt": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/f0f6e6b08ca8c356f653b1998f0585aeb85779cb", "message": "Add LEX sorted set operations"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "oremj": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/5e676b033a37915eaaf1f4693f33cffe446d22d9", "message": "fix(nutcracker.spec): Fails to build on RHEL 6\n\nRunning \"rpmbuild -ba nutcracker.spec\" on RHEL 6 fails with:\r\nerror: bad date in %changelog: Tue July 30 2013 Tait Clarridge <tait@clarridge.ca>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dksidana": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/c34aaa12fe44e634a3ad00a48ebc978f95e1c65a", "message": "Fixing project URL in nutcracker.spec"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nikai3d": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/8a0df437419e779bb7474be7e65f0515f707a090", "message": "fix typos in README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/17ff75da24c91c043b6547b8dbaaa99a81ca4dd2", "message": "fix typo in stats description"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "oldmantaiter": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/a929fc2099787aaf84b87e879c2ccdc471ecddec", "message": "Forgot to add CentOS 6 specific if statement for binary directory"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/6bc06cae5baf18d5da778f9aea7b6047febec731", "message": "Adding CentOS hooks for spec to build on 6.x"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "eranb": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/e6eb72c4cc4f372bfa02524c596a56d7a5bb33f9", "message": "Update README.md"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mkhq": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/7032ed7490e581997efbe4d349ea4dbe48258fa8", "message": "SRANDMEMBER support for the optional count argument"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "eleusive": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/c8384c9e9315ff8cef9c7f7d3655645f1482d35f", "message": "Update README.md\n\nAdd Twitch to Users"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/952fc9e0564e95bff9b8428324a9183ba0f5fdcd", "message": "Update recommendation.md\n\nUpdate max mbuf size"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ivmaykov": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/b3af606c1b3ad3820c4972e74dc93fe6953c51e1", "message": "Added Ooyala to list of users in README.md"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "TysonAndre": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/545", "title": "Always initialize file permissions field for unix domain socket", "body": "It seems like field->perm might be uninitialized memory\r\ndepending on how it is allocated.\r\nThe intended behavior is to only change file permissions from the default if a permission was specified in the config:\r\nhttps://github.com/twitter/twemproxy/pull/311/files#diff-f74ea9da930e79a9573455a0cbe4785d\r\n\r\nI ran into an issue where different sockets had different file\r\npermissions, and some of those sockets weren't readable by the user\r\nwhich created it. (I specified *only* the path to the unix domain socket)\r\n\r\nThis behavior probably started in\r\nhttps://github.com/twitter/twemproxy/pull/311/files", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/122809210", "body": "This will/does have issues if redis-sentinel adds new fields to the response, changes the order of the fields, or sends more lines than this expects. See https://github.com/twitter/twemproxy/issues/527 (wrong repo)\r\n\r\nAfter upgrading from redis 2.x to 3.2.6 , I noticed this. I'll try to find a solution\r\n\r\n`cd path/to/src/of/redis/; git diff 2.8.9 3.2.8 -- src/sentinel.c`\r\n\r\n```patch\r\n@@ -2729,11 +3242,13 @@ void sentinelInfoCommand(redisClient *c) {\r\n             \"sentinel_masters:%lu\\r\\n\"\r\n             \"sentinel_tilt:%d\\r\\n\"\r\n             \"sentinel_running_scripts:%d\\r\\n\"\r\n-            \"sentinel_scripts_queue_length:%ld\\r\\n\",\r\n+            \"sentinel_scripts_queue_length:%ld\\r\\n\"\r\n+            \"sentinel_simulate_failure_flags:%lu\\r\\n\",\r\n             dictSize(sentinel.masters),\r\n             sentinel.tilt,\r\n             sentinel.running_scripts,\r\n-            listLength(sentinel.scripts_queue));\r\n+            listLength(sentinel.scripts_queue),\r\n+            sentinel.simfailure_flags);\r\n```", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/122809210/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "phamhongviet": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/544", "title": "Add systemd service file", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "essanpupil": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/541", "title": "fix list indentation in README", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "idirouhab": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/536", "title": "Add Foodora as company who uses it in prod", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pataquets": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/524", "title": "Add Docker support", "body": "Add Dockerfile to enable image building.\r\nUsing the official GCC image, latest tag. More info at https://hub.docker.com/_/gcc/\r\n\r\nJust adding files, setting working dir and running make instructions.\r\n\r\nBuild:\r\n\r\n```\r\n$ docker build -t twemproxy .\r\n```\r\n\r\nRun:\r\n\r\n```\r\n$ docker run --rm -it -p [ext_port1:port1] -p [ext_port2:port2] -p [...] -v [your_host_config_yml]:/etc/nutcracker.yml:ro twemproxy -c /etc/nutcracker.yml --verbose=6 [other options...]\r\n```\r\n\r\nFYI, there's a still quicker to test, already built image on my Docker Hub. Test it by running:\r\n\r\n```\r\n$ docker run --rm -it -p [ext_port1:port1] -p [ext_port2:port2] -p [...] -v [your_host_config_yml]:/etc/nutcracker.yml:ro pataquets/twemproxy  -c /etc/nutcracker.yml --verbose=6 [other options...]\r\n```\r\n\r\nUsing `--rm` instead of `-d` makes the container not go background and it to be deleted after stop. Should stop by `CTRL+C`'ing it.\r\nIn order for the Docker container to connect to external memcached or Redis instances, either them should be contactable as external IPs or hosts or be linked to other previously run Docker containers via Docker's ```--link``` option.\r\n\r\nHere it is an example Docker Compose file I'm using (I can submit it with the PR also if you find it useful):\r\n```\r\nredis1:\r\n  image: redis:3.2\r\n  command: --save \"\" --maxmemory 128mb\r\n  ports:\r\n    - 63791:6379\r\n\r\nredis2:\r\n  image: redis:3.2\r\n  command: --save \"\" --maxmemory 128mb\r\n  ports:\r\n    - 63792:6379\r\n\r\ntwemproxy:\r\n  image: pataquets/twemproxy\r\n  command: -c /etc/nutcracker.yml --verbose=6\r\n  links:\r\n    - redis1\r\n    - redis2\r\n  ports:\r\n    - 6379:6379\r\n    - 22222:22222\r\n  volumes:\r\n    - ./conf/nutcracker.redis.yml:/etc/nutcracker.yml:ro\r\n```\r\nNotice that it is tuned for two Redis instances (yml file not included, mount yours)\r\n\r\nOptional improvement to come (maybe in another issue):\r\n- Create an 'official', based on your repo, automated build at Docker Hub for the image: https://docs.docker.com/docker-hub/builds/ . Just requires a free Docker Hub account and a following a quick 'Create automated build' process. I'll be happy to help on it, if needed.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "galusben": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/520", "title": "JFrog is using twemproxy on production", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "santoshsahoo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/515", "title": "Added CircleHD to companies using Twemproxy", "body": "Updated README.md, we are using of twemproxy on aws.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "voetsjoeba": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/511", "title": "Fix build failure with --disable-stats", "body": "A build with ./configure --disable-stats currently fails with the following error (gcc 4.8.5 on CentOS 6):\r\n\r\n```\r\nnc_server.c: In function \u2018server_failure\u2019:\r\nnc_server.c:291:5: warning: implicit declaration of function \u2018stats_server_set_ts\u2019 [-Wimplicit-function-declaration]\r\n     stats_server_set_ts(ctx, server, server_ejected_at, now);\r\n     ^\r\nnc_server.c:291:38: error: \u2018server_ejected_at\u2019 undeclared (first use in this function)\r\n     stats_server_set_ts(ctx, server, server_ejected_at, now);\r\n                                      ^\r\nnc_server.c:291:38: note: each undeclared identifier is reported only once for each function it appears in\r\n```\r\n\r\nThe reason appears to be that some functions are not being nopped out in `nc_stats.h` when `NC_STATS` is 0. This change adds the missing entries. Technically only `stats_server_set_ts` is needed to fix the build error, but from the intent of the code it's clear that `stats_pool_set_ts` should also be nopped out, despite never being called directly.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/510", "title": "Allow negative exptime values in memcached storage commands", "body": "Memcached allows negative exptime values in storage commands to immediately expire the stored values, but this wasn't very well documented until recently (here's the commit that adds the documentation from May 2016: https://github.com/memcached/memcached/commit/e7d4521cd8b27f7ebc6e4c1b9aee9eb3544f6af5).\r\n\r\nThis patch allows the memcached parser to support negative exptime values in storage commands. The server still responds with the usual STORED if the exptime is negative, so no additional response handling is needed.\r\n\r\n\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ugurengin": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/502", "title": "update memcache populate script", "body": "update memcache populate script to improve test cause of memcached for get and set operations", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mahdi-hdi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/497", "title": "Added Geo Functionality", "body": "New Redis functionality for geo date type now supported:\n\n```\nGEOADD\nGEODIST\nGEOHASH\nGEOPOS\nGEORADIUS\nGEORADIUSBYMEMBER\n```\n\n**Mutli bulk array** was implemented in Redis response parser to support response of **GEORADIUS**. \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "willfitch": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/480", "title": "Implement ERROR protocol", "body": "Rather than severing connections upon an invalid event, this patch adds the ERROR portion of the Memcached protocol.  This does not, however, add CLIENT_ERROR or SERVER_ERROR.  That can be discussed.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "VishalRocks": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/476", "title": "Update README.md", "body": "Added Codechef in Companies using Twemproxy in Production\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "artursitarski": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/474", "title": "Wikia as twemproxy user", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "flygoast": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/463", "title": "Implemented client connections limiting of server pool.", "body": "Implemented client connections limiting of server pool.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "huachaohuang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/461", "title": "Fix memory leak for redis mset.", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ofirule": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/435", "title": "Added support for listening to same port from multiple twemproxy processes", "body": "My team (from IBM Security Trusteer) has ran into some bottlenecks when using twemproxy and developed this patch.\n\nThis patch allows to use socket option SO_REUSEPORT on twemproxy socket. This option is available on Linux kernel version 3.9+.\n\nAfter stress testing our redis-server we found that handling many clients consumes too much processing from redis but not redis code itself rather its networking code (epoll/etc.).\n\nIt seems that the single thread nature of redis maxes out the networking part with one core in extreme conditions (like many connections scenario, no pipelining, etc.).\n\nUsing this patch we can connect many twemproxy instances (which runs on the same computer) to a redis server using the same socket, we were able to get a much greater throughput and still have one logical redis server. \n\nThese twemproxy instances can all be configured with the same configuration file, and only needs to have a different stats port. (we don't enable SO_REUSEPORT for that socket)\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rosmo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/431", "title": "Allow ignoring SELECT from client", "body": "Even if you are just using database 0, some developers want to call Redis' SELECT command always up front. This add a new redis_ignore_select option to pools to simply return OK for any SELECT command.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "umegaya": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/424", "title": "add support of script load command by broadcast", "body": "refs #68 \nthis pull request aims to add support for SCRIPT LOAD command, by following fix.\n- define msg type for SCRIPT LOAD and parse correctly (8158d2e)\n- able to give 'broadcast' attribute to certain kind of msg type, and give broadcast attribute to SCRIPT command (8158d2e)\n- do broadcast correctly (cb50953)\n\nalso original python test seems to be broken, add new test by shell script (dfba9f8), you can run new test like following:\n\n```\ncd tests/test_redis_sh\n./run ./test_script_load.sh\n```\n\nit is only passed limited test case, and I'm very new to twemproxy, so not enough confidence about correctness. can you review and if it looks good, merge to master? thanks in advance.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dec5e": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/412", "title": "Removing duplicate Hootsuite mention in list of companies using Twemproxy", "body": "Hootsuite was added twice: in #291 and #387.\n\nPS. Maybe it's good to sort companies list alphabetically to prevent such duplication in future. I can do it if you agree.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ideal": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/411", "title": "add rbtree_entry to fetch a struct pointer from a rbtree node pointer", "body": "So the code is more generic.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "susman": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/400", "title": "add rhel7 compatibility", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/76194303", "body": "ExecStop is optional\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/76194303/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/76194448", "body": "I guess many things have changed since the PR was opened\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/76194448/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "ton31337": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/396", "title": "Add SO_REUSEPORT support to socket", "body": "Add SO_REUSEPORT support to socket. Introduce new feature for Twemproxy running on newer kernels. With this feature you are able to do upgrades, restarts without any downtime. Kernel does load balancing between processes with the same host:port pairs. \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/37003301", "body": "@susman by the way, ExecStop isn't needed?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/37003301/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/36987216", "body": "@charsyam :+1: of course will fix this, but need to be sure if it's worth to spend more time. I mean if it's planned to be merged or not.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/36987216/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/37294849", "body": "@charsyam are you able to test this on FreeBSD ? :)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/37294849/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "nanzhushan": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/391", "title": "add", "body": "\u4e2d\u6587\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "maralla": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/380", "title": "fix fragment index bug, resolves #376", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "MOON-CLJ": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/367", "title": "fix crc16", "body": "nc_crc16 impl is not correct, eg, \"FM|ivector_2129920\" -> 2859837793, the result should be 43361.\n\ncrc16's return value should never beyond 65536, @manjuraj @idning, pls review.\n\nrefer to https://github.com/twitter/twemproxy/pull/313, and i dont know how do you think about this kind of error.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wtcross": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/343", "title": "Fix debian init script default loading.", "body": "DAEMON_ARGS needs to be set after defaults are loaded to facilitate overrides. The following order is now in place:\n1. load defaults\n2. check if daemon is executable\n3. set DAEMON_ARGS\n\nThis change is useful when installing in some other directory prefix other than `/opt/nutcracker`. I'm using [fpm](https://github.com/jordansissel/fpm) and would like to use the `scripts/nutcracker.init.debian` init script when packaging.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vlm": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/321", "title": "Run time configuration reload", "body": "This patch introduces a runtime configuration reload. `killall -USR1 nutcracker` would cause a configuration reload.\n#### Changes\n- The patch contains the test cases to test different failure scenarios. Unfortunately, to make the test work across platform (Linux, Mac OS X) I also had to fix a nc_kqueue bug which caused crashes on Mac OS X.\n- The patch switches separate .client, .redis bit fields in `nc_connection.h` to a more precise enumeration of what a connection is. This also allows significantly more descriptive logging.\n- The event loop in the original branch was crafted in tight coupling with `struct conn`, in a way that is not very conducive to allowing multiple different kinds of entities receiving file system events. This did not work well when I tried to safely pause the statistics gathering events. I restructured that part by removing the part of event loop which deals with statistics and moving it into the statistics thread itself.\n#### Model of operation\n\nThe new config is read into memory, and the new pools are allocated. If the new pools cannot be allocated for some reasons, the configuration change is not done and the configuration state is rolled back. Once the new pools are allocated, we pause all the ingress data transfers from the clients and drain the output queues. Once the client queues are drained (and we have sent out all the outstanding server responses), we replace the old pools with the newly configured pools. Then we unblock the clients so they can send new requests.\n\nI am ok restructuring this patch however you see fit. Looking forward to your feedback.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829590", "body": "Does it make sense to add CONN_KIND_AS_STRING(conn) instead of every `\"s` though?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829590/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829594", "body": "What's the reason for this commit?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829594/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11831080", "body": "Ack.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11831080/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/33113284", "body": "@manjuraj Re combinations of connections. One thing is that it eases the \"CONN_KIND_AS_STRING(c)\" implementation, so it is O(1). Do you recommend a different approach?\n\n@manjuraj re cleanups: I've added a couple of comments to your changes in the config-reload branch, would you please respond.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/33113284/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "hatmann1944": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/316", "title": "support poll", "body": "only use poll when epoll,kqueue or event port are all unavaliable\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "akopytov": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/314", "title": "Tarantool support", "body": "Changes in core code:\n- the \u2018redis\u2019 configuration directive is deprecated by the new\n  \u2018protocol\u2019 directive with the following possible values: \u2018memcached\u2019,\n  \u2018redis\u2019 and \u2018tarantool\u2019, with \u2018memcached\u2019 being the default. When the\n  \u2018redis\u2019 directive is used, a warning is printed on startup. Using both\n  \u2018redis\u2019 and \u2018protocol\u2019 directives with conflicting values results in\n  an error on startup.\n- Tarantool uses a binary protocol, so error messages generation is\n  moved from msg_get_error() to a new handler method, msg->get_error().\n- Tarantool clients expect a \u2018greeting\u2019 message on connect. To this end,\n  the post_connected() handler method is now also called on a client\n  connect. memcached and redis post_connect() methods are modified to\n  tell between client and server connections.\n- A Tarantool server can send out-of-order replies in certain cases. To\n  match responses to requests the \u2018sync\u2019 field from request/response\n  headers is used when a connection has a new \u2018need_sync\u2019 attribute set\n  to 1, instead if picking the first request from the server\u2019s outgoing\n  queue.\n- \u2018struct msg\u2019 has a new \u2018emitted\u2019 attribute to mark pseudo-request\n  generated on a client connect so that this message is not filtered as\n  empty.\n- \u2018struct msg\u2019 has a number of new Tarantool-specific fields\n- the Tarantool protocol requires a pseudo-random numbers\n  generator (implemented in nc_prng.ch) which is initialized on startup\n  and de-initialized on shutdown.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "amolrajoba": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/309", "title": "Redis key-value distribution with twemproxy compatible with phpredis redisArray", "body": "Implementation to use key-value distribution with twemproxy compatible with phpredis redisArray (https://github.com/phpredis/phpredis/blob/master/arrays.markdown)\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "paravoid": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/123", "title": "Use sysconfdir & localstatedir for conf/pid/logs", "body": "Use autoconf-supplied paths as the default values for the configuration\nfile, the pidfile and the logfile. So, for example:\n  /etc/nutcracker/nutcracker.yml\n  /var/run/nutcracker/nutcracker.pid\n  /var/log/nutcracker/nutcracker.log\n\nThe separate subdirectory under /var/run was picked since nutcracker\ndoesn't possess the capability of dropping privileges by itself, so we\nneed to have a system user writeable directory to place the pidfile.\n\nSimilarly, we need a separate /var/log subdir as logs can't go to\nsyslog, and out of convenience (need to be able to perform logrotate,\ncleanup upon removing a package etc.)\n\nThe separate subdirectory under /etc/ is just for being pretty\n(top-level /etc conffiles are ugly) and for future use (multiple configs\nfor the same daemon?)\n\nAll of the above can of course be used in the case where a single init\nscript spawns multiple nutcrackers, each with a separate config, pidfile\nand logfile, for example for extra redundancy reasons.\n\n(also fix an underlying bug where NC_PID_FILE was completely unused)\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "travisbot": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6685567", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/1743714) (merged bd16a4cc into 6a426fd1).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6685567/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6686512", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/1744434) (merged 12b9c896 into 6a426fd1).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6686512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407742", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/2003216) (merged 9a17a01d into b85ac82c).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407742/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407765", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/2003218) (merged f34559a3 into b85ac82c).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407765/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "bmatheny": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7306865", "body": "Would you be open to a patch for this? The problem we have is that if the server is busy enough the ports for twem may become used in the short restart window, causing twem to not start. In that case we have to shutdown apache, restart twem, start apache.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7306865/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7375063", "body": "SO_REUSEADDR will only reuse a TIME_WAIT socket, not an ESTABLISHED socket. If twem is running on a busy web server (as a local proxy), and the port gets used, reuseaddr is no help.\n\nI agree the on the fly reload is tricky, I'll put some thought into it.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7375063/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407782", "body": "One additional comment. Based on feedback from Manju I moved the old vlen values to vlen_rem, and made vlen be an immutable value representing the total size of the value. This allows a much cleaner calculation in the req_filter of whether the object value exceeds the configured item_size_max or not.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407782/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10560914", "body": "I'm going to close this and reopen with the changes you recommended and also the merge conflicts handled.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10560914/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11723036", "body": "This isn't part of the memcache spec and the PHP manual even says \"It's not recommended to use the timeout parameter. The behavior differs between memcached versions, but setting to 0 is safe. Other values for this deprecated feature may cause the memcache delete to fail.\"\n\nDoesn't seem like this should be part of master.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11723036/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13520200", "body": "@manjuraj @eveiga that's what we do for memcache when there are events like a total failure (external process). Works quite well.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13520200/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13523307", "body": "@eveiga the one you recommended. When the topology needs to change the config is updated by an external process and twem gets restarted.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13523307/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13524862", "body": "We do see a short burst of errors. The error type is detected by the app and retried, so we generally don't 'lose' writes, and reads will fall back to the DB.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13524862/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "Ayutthaya": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7364117", "body": "As a school project, I've made a patch to support kqueue, epoll and event ports, using the libevent API. There are also a few additional changes to make it work on my mac os x. (the patch is at github.com/ayutthaya/twemproxy ). I'm ready to improve my work if necessary, so don't hesitate to give feedback. Regards.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7364117/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7409583", "body": "I changed the patch. Now it uses kqueue API directly (but doesn't support event ports anymore). It needed changes mainly to nc_event.[ch] and nc_stats.[ch], since stats aggregation was also based on epoll. The patch is still at github.com/ayutthaya/twemproxy . one question: what is the reason for not using libevent API ? Thanks. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7409583/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7638593", "body": "I've tried to comply with the suggestions above as much as possible. In particular, I've abstracted out the event interface and used a function pointer to make all #ifdef HAVE_EPOLL/KQUEUE disappear in all files except nc_event.h nc_epoll.c and nc_kqueue.c. Don't hesitate if you think it needs further changes before doing a pull request. \nI also have a question about twemproxy: what is the number of sockets from which point one should use twemproxy / the performance of the cache server starts degrading seriously due to per-connection overhead? 65k? 200k? If you could simply give me a lead, it would help me a lot for my project. Thanks !\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7638593/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12475590", "body": "as far as I'm concerned this is a good job srned ! couldn't finish mine, though I appreciated hacking on this very well-made open source code. the way you reorganized core_loop and core_core among other pieces looks familiar to me ;-) Hope your branch will be merged ! :-)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12475590/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12487739", "body": "@srned it seems you could finish what I started, well done\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12487739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12799512", "body": "@ferenyx Thanks a lot for finishing/fixing/enhancing what I started :-) well done ! srned did nice work as well. I hope manjuraj will take the best from each contribution !\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12799512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "jsholmes": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9115243", "body": "Any update on that stable build?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9115243/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "cactus": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9318009", "body": "Does distribution random assign a key to a server randomly but persistently, or is it random on every interaction with the key?\n\nI tried finding some info on that in the docs, but don't recall running across it. I haven't yet poked at the source to find out.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9318009/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9324143", "body": "Thanks. I did read those, and saw only a single mention of `random` -- in the distribution description where it was just listed as an option.\n\nThanks for the responses!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9324143/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9328582", "body": "After thinking about this, the twemproxy's pipelining support would probably break the \"reliable read\" mechanism of kestrel. Should be fine if just doing gets and sets though I think (just not '/open's).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9328582/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "rbranson": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9550121", "body": "This is running on Ubuntu Server 10.04 natty: Linux 2.6.38, libc6 2.13.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9550121/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9551355", "body": "This hack^H^H^H^Hpatch \"fixes\" it, albeit with a pretty decent performance hit:\n\n```\ndiff --git a/src/nc_mbuf.c b/src/nc_mbuf.c\nindex 05ec25e..04f157c 100644\n--- a/src/nc_mbuf.c\n+++ b/src/nc_mbuf.c\n@@ -126,12 +126,13 @@ void\n mbuf_put(struct mbuf *mbuf)\n {\n     log_debug(LOG_VVERB, \"put mbuf %p len %d\", mbuf, mbuf->last - mbuf->pos);\n+    mbuf_free(mbuf);\n\n-    ASSERT(STAILQ_NEXT(mbuf, next) == NULL);\n+    /*ASSERT(STAILQ_NEXT(mbuf, next) == NULL);\n     ASSERT(mbuf->magic == MBUF_MAGIC);\n\n     nfree_mbufq++;\n-    STAILQ_INSERT_HEAD(&free_mbufq, mbuf, next);\n+    STAILQ_INSERT_HEAD(&free_mbufq, mbuf, next); */\n }\n\n /*\ndiff --git a/src/nc_message.c b/src/nc_message.c\nindex ca20be0..c989b8e 100644\n--- a/src/nc_message.c\n+++ b/src/nc_message.c\n@@ -320,8 +320,10 @@ msg_put(struct msg *msg)\n         mbuf_put(mbuf);\n     }\n\n-    nfree_msgq++;\n-    TAILQ_INSERT_HEAD(&free_msgq, msg, m_tqe);\n+    msg_free(msg);\n+\n+    /*nfree_msgq++;\n+    TAILQ_INSERT_HEAD(&free_msgq, msg, m_tqe);*/\n }\n\n void\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9551355/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "xaratt": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9902830", "body": "Thank you for response.\n\nYes, when I try to connect directly to cache servers, both of them works fine:\n\n<pre>\nuser@my.proxy.server:~$ printf \"get foo\\r\\n\" | nc myserver.0001.use1.cache.amazonaws.com 11211\nEND\nuser@my.proxy.server:~$ printf \"get foo\\r\\n\" | nc ec2-xx-xx-xx-xx.compute-1.amazonaws.com 11211\nEND\n</pre>\n\nAnd ping of both server names resolve to the same address. I can give you real IP of \"ec2-xx-xx-xx-xx\", but Amazon's FAQ say that \"Currently, all clients to an ElastiCache Cluster must be within the Amazon EC2 network\".\n\nI want to try to run twemproxy with memcached on my non-AWS server using CNAME and IP for connection. I'll post results here.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9902830/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9973803", "body": "I made few new attempts and found configuration which allow me use twemproxy with AWS Elasticaches. I created own CNAMEs which points on Amazon's myserver.000x.use1.cache.amazonaws.com servers and twemproxy works fine with this strange scheme:\n\n<pre>\ncache1.example.com -> myserver.0001.use1.cache.amazonaws.com -> ec2-xx-xx-xx-xx.compute-1.amazonaws.com\n</pre>\n\nIs this bug (feature?) in Amazon DNS system? I don't know.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9973803/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "shapirus": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10058071", "body": "I too thought about this. Implementing a background heartbeat check, however, may actually be too difficult and not worth the effort (I'm not sure that twmemproxy's design supports anything background at all; I may be wrong though). What could be done instead is something along these lines (pseudocode):\n\n```\n...\nif (time_passed > retry_timeout) {\n    if (try_connection(ejected_server)) {\n        inject_server_back(ejected_server);\n    } else {\n        leave_server_ejected(ejected_server);\n    }\n    do_the_job(key, value);\n}\n...\n```\n\nIt will allow this given request not to fail and be dispatched to another host if the original server is still down after the retry_timeout has expired. The cost of such approach will be one client request delayed for `timeout` ms once in every `server_retry_timeout` ms while the failed server stays down, which seems acceptable to me (it is better than the client having to redo its request anyway).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10058071/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10297815", "body": "Is anyone already working on fixing this? If not, I'll try to develop a patch myself. Really want to try this thing out in production :)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10297815/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10413998", "body": "Probably, but I will need to understand the current code and algorithms. Where do the server failure handling routines live and where do they get called from?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10413998/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "litao941": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10173641", "body": "Hello Manju,\n\nI have four IPs on my server, i just want the status port is only listens on 127.0.0.1:22222. i used the netstat command, and the below was the result(OS: CentOS 5.4):\n\n```\n  [root@virt]# netstat -tlnup |grep nut\n  tcp   0      0     10.11.80.50:22121    0.0.0.0:*    LISTEN   2268/nutcracker     \n  tcp   0      0      :::22222                  :::*            LISTEN   2268/nutcracker\n```\n\nso, the result was i can used any  IP (also include localhost) on my server to telnet the port 22222. i have already got some information from your code file nc_stats.h and  changed the row 52:\n\n```\n  #define STATS_ADDR      \"localhost\"      ------>   #define STATS_ADDR      \"127.0.0.1\"\n```\n\ni was also  re-compiled and re-made the source code. but it seemed not work. the result  as above :::22222 .\ni have no firewall on my OS, and need disable it listen on my internet IP. so is there any parameter for that or can i change  the code for myself ?\n\n BTW\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10173641/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10174377", "body": "Yea, i have made a test again. It works now.     __ ^ _ ^__\nBut also recommend that you can add a parameter for that.\nThank your for your help.\n Also appreciate your twemproxy. it's 40 times faster than use memcached direct and only just 2 times CPU(s) used.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/10174377/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "antirez": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11052002", "body": "+1 This could be useful.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11052002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11052139", "body": "Such a support would be great for the project because today most developers have osx computers, even if I doubt twemproxy is going to be deployed on osx if not in very rare cases, the ability to try it on osx is a good _get developers exposed_ feature.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11052139/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11054350", "body": "Thanks for the ACK Jeremy! I also did the same when trying to implement Dynamo concepts on top of Redis.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11054350/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11057586", "body": "@manjuraj doesn't the priority affect the way the hash ring is populated? (more repliacas of the same node if priority is higher)? If not I was addressing a non existing problem (that just changing the priority would change the map).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11057586/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11076323", "body": "@manjuraj yes, I and you understand this, but IMHO this is the random user interaction:\n\n\"Hey we got this new fast box with BIG RAM! Holy Shit let's move one of our instances there\"\n\n```\n- 192.168.1.3:6379:10 server1\n+ 192.168.1.5:6379:99 server1\n```\n\n\"Look, I updated the priority because this box is so much bigger!\"\n\nAnd the user ends with data shuffled around instances in a way that is very hard to recover.\n\nSo back to my proposals, honestly, both ignoring priority and putting it into the name sound wrong to me. For the following reasons:\n- Ignoring priority is a surprising behavior.\n- Forcing it to be part of the name could work but there is a numerical part anyway, like \"myserver:1000\", users may still think that the numerical part can be changed without problems.\n\nIt's probably better just to use warnings inside the documentation to make sure people understand that changing priority OR instance name will result in different mapping of keys.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11076323/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11078457", "body": "Maybe the ultimate solution is that:\n- If node ejection is false.\n- If redis is true\n- If for every node the user specified a node name\n\nTHEN -> Exit with an error if the specified priority is not always \"1\", with an error message that makes sense, like:\n\"You are proxying Redis protocol with node ejection disabled and explicit names for all the nodes. In this setup usually a static map between keys and hosts is needed, so all the instances must be configured with priority 1 (otherwise changing the priority may change how keys are mapped to servers).\"\n\nOptionally one may support an option to still allow non-1 priority with Redis server in this setup.\n\nOk I think so far this is absolutely the best option we have.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11078457/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "jzawodn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11053764", "body": "+1\n\nThis is pretty much exactly how we do it at craigslist with our sharding setup.  We has to a \"node name\" rather than directly to an IP:PORT pair, so it's possible to move data without losing any keys.\n\nhttp://blog.zawodny.com/2011/02/26/redis-sharding-at-craigslist/\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11053764/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "xqpmjh": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11075612", "body": "Thank you so much manjuraj~\n\nBut neithor delete(key, 0) nor upgrade the twemproxy could make it works.\n\nHere is what I got from the log when PHP's memcache sending the 'delete' command to twemproxy :\n\n[Thu Dec  6 15:23:45 2012] nc_proxy.c:207 p 7 listening on '127.0.0.1:22121' in memcache pool 0 'leaf' with 9 servers\n[Thu Dec  6 15:24:56 2012] nc_proxy.c:337 accepted c 8 on p 7 from '127.0.0.1:41348'\n[Thu Dec  6 15:24:56 2012] nc_core.c:207 close c 8 '127.0.0.1:41348' on event 0001 eof 0 done 0 rb 72 sb 48: Invalid argument\n[Thu Dec  6 15:25:09 2012] nc_proxy.c:337 accepted c 8 on p 7 from '127.0.0.1:41393'\n[Thu Dec  6 15:25:09 2012] nc_core.c:207 close c 8 '127.0.0.1:41393' on event 0001 eof 0 done 0 rb 72 sb 48: Invalid argument\n\nIt seems that twemproxy doesn't accept the command? But doesn't it just pass the commands to the group backend?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11075612/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11787144", "body": "In fact, finally I fixed it by removing the \"time\" parameter of delete() within the source code of memcache(php) extension then recompile.\ncommand_len = spprintf(&command, 0, \"delete %s %d\", key, time);\ncommand[command_len] = '\\0';\n->\ncommand_len = spprintf(&command, 0, \"delete %s\", key);\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11787144/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12358782", "body": "-C works\uff01Thank you so much.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12358782/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12823735", "body": "Sorry, it's so long ...\n\n[Tue Jan 29 15:11:52 2013] nc_proxy.c:337 accepted c 24 on p 23 from '127.0.0.1:59948'\n[Tue Jan 29 15:11:52 2013] nc_proxy.c:281 accept on p 23 not ready - eagain\n[Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 24 464 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:677 parsed req 1 res 4 type 5 state 14 rpos 464 of 464\n00000000  73 65 74 20 74 65 73 74  6b 65 79 31 32 33 20 38   |set testkey123 8|\n00000010  30 20 32 20 31 38 33 39  0d 0a 36 0b 00 00 1f 7b   |0 2 1839..6....{|\n00000020  22 43 6f 6e 74 65 6e 74  22 3a 22 5c 75 34 65 33   |\"Content\":\"\\u4e3|\n00000030  62 5c 75 36 66 31 34 5c  75 66 66 31 61 5c 75 03   |b\\u6f14\\uff1a\\u.|\n00000040  35 32 31 38 40 17 00 65  20 0b 0b 34 66 31 66 2d   |5218@..e ..4f1f-|\n00000050  2d 5c 75 39 39 37 30 20  25 02 37 39 37 20 1f 02   |-\\u9970 %.797 ..|\n00000060  39 32 39 20 05 07 62 39  64 5c 75 33 30 30 20 17   |929 ..b9d\\u300 .|\n00000070  e0 03 05 02 39 36 34 20  3d 09 36 30 32 31 5c 75   |....964 =.6021\\u|\n00000080  38 34 63 39 e0 01 3d 02  35 33 35 40 3d 01 62 61   |84c9..=.535@=.ba|\n00000090  20 6f 02 37 64 32 40 05  05 34 33 33 20 72 6e e0   | o.7d2@..433 rn.|\n000000a0  05 40 80 4c 03 38 64 37  35 20 46 01 35 38 40 32   |.@.L.8d75 F.58@2|\n000000b0  02 33 35 33 e0 02 46 01  65 62 20 13 80 25 02 37   |.353..F.eb ..%.7|\n000000c0  31 39 20 8a 80 0b e0 03  05 01 35 33 40 84 80 11   |19 .......53@...|\n000000d0  03 39 61 36 63 e0 01 3d  20 78 00 32 20 13 00 38   |.9a6c..= x.2 ..8|\n000000e0  40 bc 03 35 36 66 65 e0  0e 7e 02 37 33 38 20 9f   |@..56fe..~.738 .|\n000000f0  80 40 21 0f 00 61 e0 02  40 02 35 61 66 20 13 02   |.@!..a..@.5af ..|\n00000100  34 38 63 20 05 00 63 41  2f 80 25 e0 03 05 02 35   |48c ..cA/.%....5|\n00000110  66 32 21 15 02 39 34 63  21 03 41 33 e0 01 3d 02   |f2!..94c!.A3..=.|\n00000120  39 38 37 21 55 02 36 63  62 20 5d 03 35 65 31 64   |987!U.6cb ].5e1d|\n00000130  60 7e 02 35 31 38 20 eb  02 35 62 62 20 cb 02 37   |`~.518 ..5bb ..7|\n00000140  62 38 20 3a 02 34 65 63  20 20 41 8e 60 20 e0 04   |b8 :.4ec  A.` ..|\n00000150  5b 01 39 34 20 fe 02 35  66 64 20 05 21 65 40 4d   |[.94 ..5fd .!e@M|\n00000160  01 32 34 21 89 02 66 66  30 20 91 03 34 65 35 65   |.24!..ff0 ..4e5e|\n00000170  40 05 00 31 20 44 01 36  62 40 c3 02 37 35 62 20   |@..1 D.6b@..75b |\n00000180  11 02 37 65 66 20 b5 40  8b 20 05 60 2f 02 35 62   |..7ef .@. .`/.5b|\n00000190  35 20 23 01 34 65 40 41  80 05 03 39 [Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 24 464 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:677 parsed req 1 res 4 type 5 state 14 rpos 464 of 464\n00000000  20 82 20 58 03 37 32 30  36 20 05 01 62 31 20 11   | . X.7206 ..b1 .|\n00000010  80 9a 02 36 35 61 20 a6  01 36 38 41 99 02 35 32   |...65a ..68A..52|\n00000020  37 40 58 00 33 40 7c 02  36 32 64 20 eb 02 34 65   |7@X.3@|.62d ..4e|\n00000030  62 21 97 02 35 65 39 20  7c 02 38 62 64 20 64 02   |b!..5e9 |.8bd d.|\n00000040  35 39 30 20 0b 80 05 80  47 01 36 35 41 ed 02 37   |590 ....G.65A..7|\n00000050  36 37 20 d0 02 35 31 66  20 fa 02 35 65 37 20 53   |67 ..51f ..5e7 S|\n00000060  02 39 65 64 42 72 01 35  65 20 77 03 39 36 35 30   |.9edBr.5e w.9650|\n00000070  e0 0b 9a 01 30 32 40 f9  01 38 33 21 23 81 41 02   |....02@..83!#.A.|\n00000080  38 62 65 e0 02 9a 40 3a  02 38 64 35 20 b2 22 dd   |8be...@:.8d5 .\".|\n00000090  20 29 20 e1 20 4c e1 03  29 02 39 31 63 20 6a 03   | ) . L..).91c j.|\n000000a0  38 31 65 33 20 0b 01 36  63 20 29 01 34 65 42 fb   |81e3 ..6c ).4eB.|\n000000b0  01 38 65 42 cf 01 35 66  60 0b 01 66 39 20 d6 00   |.8eB..5f`..f9 ..|\n000000c0  37 22 af 20 17 00 39 40  23 02 35 32 36 20 4d 02   |7\". ..9@#.526 M.|\n000000d0  36 35 62 20 4d 02 39 38  63 42 6e e1 07 35 02 35   |65b M.98cBn..5.5|\n000000e0  35 39 40 94 e0 03 28 00  35 41 be 81 35 02 35 66   |59@...(.5A..5.5f|\n000000f0  65 40 40 01 30 61 20 76  01 36 30 41 23 20 0b 20   |e@@.0a v.60A# . |\n00000100  64 02 36 62 36 40 11 00  63 43 b6 e1 04 05 00 32   |d.6b6@..cC.....2|\n00000110  40 3b 80 9a 02 34 66 35  20 23 02 37 65 63 21 1d   |@;...4f5 #.7ec!.|\n00000120  02 37 65 64 20 0b 02 39  37 65 20 b8 21 d0 20 41   |.7ed ..97e .!. A|\n00000130  83 c0 02 36 35 66 20 11  23 ec 43 88 e0 08 9a 63   |...65f .#.C....c|\n00000140  6a 83 64 20 64 00 34 20  40 01 35 66 20 05 a0 9a   |j.d d.4 @.5f ...|\n00000150  e0 04 1d 00 31 21 1d 20  76 21 9a 20 05 20 ed 02   |....1!. v!. . ..|\n00000160  34 65 66 21 d6 04 35 30  65 37 28 20 06 01 62 39   |4ef!..50e7( ..b9|\n00000170  60 18 00 33 61 b3 43 bf  80 54 02 34 65 34 20 24   |`..3a.C..T.4e4 $|\n00000180  02 37 32 33 20 83 02 35  39 64 20 5a 04 36 64 33   |.723 ..59d Z.6d3|\n00000190  65 29 20 06 00 32 40 ba  02 38 64 34 [Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 24 464 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:677 parsed req 1 res 4 type 5 state 14 rpos 464 of 464\n00000000  60 02 35 33 31 20 6c 81  26 81 df 23 5d 22 7a 23   |`.531 l.&..#]\"z#|\n00000010  1b 20 6b 01 37 31 40 8a  80 71 01 35 31 42 15 80   |. k.71@..q.51B..|\n00000020  f7 20 4d 60 c0 e0 01 4d  80 ba 80 eb 02 32 30 31   |. M`...M.....201|\n00000030  40 3b 61 62 01 35 64 41  21 20 11 23 3f 80 65 02   |@;ab.5dA! .#?.e.|\n00000040  34 65 38 20 71 01 35 33  42 39 02 38 32 65 22 1b   |4e8 q.53B9.82e\".|\n00000050  21 c2 20 2f 24 05 21 92  00 37 23 11 20 1d 01 66   |!. /$.!..7#. ..f|\n00000060  30 20 05 82 5d a0 7d 00  33 41 aa 80 5f 02 37 36   |0 ..].}.3A.._.76|\n00000070  32 40 41 01 66 33 22 b1  02 38 30 30 20 41 02 34   |2@A.f3\"..800 A.4|\n00000080  66 65 41 d4 60 ad c0 65  61 5c 60 f5 25 ac 22 34   |feA.`..ea`.%.\"4|\n00000090  02 35 31 36 20 23 24 e4  21 07 a0 65 62 db 81 31   |.516 #$.!..eb..1|\n000000a0  02 35 31 39 21 7a 80 fb  02 35 31 37 40 5f a0 e3   |.519!z...517@_..|\n000000b0  44 d9 23 8e 21 6d 20 7d  20 f5 01 37 65 40 a7 20   |D.#.!m } ..7e@. |\n000000c0  23 21 d5 84 a4 80 c5 20  3b 20 8f 22 db 20 5f 24   |#!..... ; .\". _$|\n000000d0  b0 20 1d 01 34 66 40 a7  02 37 62 34 20 fb 01 35   |. ..4f@..7b4 ..5|\n000000e0  36 a3 d6 20 47 21 fe 40  89 64 3b 84 24 00 35 65   |6.. G!.@.d;.$.5e|\n000000f0  1b 80 e9 00 36 26 06 20  11 01 30 33 20 6b 01 36   |....6&. ..03 k.6|\n00000100  37 42 47 23 6b 20 0b a0  1d a2 d0 60 dd e2 02 3a   |7BG#k .....`...:|\n00000110  00 39 21 14 20 35 64 cb  02 37 65 61 20 0b 80 59   |.9!. 5d..7ea ..Y|\n00000120  20 b3 20 89 82 d6 a0 f5  01 66 61 20 a7 80 29 80   | . ......fa ..).|\n00000130  95 20 8f 20 b3 20 bf 20  35 80 11 80 ef 01 38 36   |. . . . 5.....86|\n00000140  44 4e 81 c7 a0 71 02 31  34 34 20 23 27 05 c0 8f   |DN...q.144 #'...|\n00000150  01 35 33 43 07 22 8d 21  61 84 4f 02 37 32 65 21   |.53C.\".!a.O.72e!|\n00000160  cd 21 c1 20 4d 80 b3 01  35 31 43 a8 01 37 61 85   |.!. M...51C..7a.|\n00000170  0e 42 87 84 fc 80 1d 80  9b 21 c7 20 35 02 36 34   |.B.......!. 5.64|\n00000180  32 20 89 02 35 65 32 20  ef 21 61 20 a7 01 38 66   |2 ..5e2 .!a ..8f|\n00000190  41 37 e0 09 29 02 34 65  39 20 5f 02 [Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 24 464 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:677 parsed req 1 res 4 type 5 state 14 rpos 464 of 464\n00000000  1d 82 6f 20 1d 20 83 02  38 66 38 21 85 21 79 20   |..o . ..8f8!.!y |\n00000010  e9 80 a1 80 83 80 ef 01  38 30 42 45 22 21 20 47   |........80BE\"! G|\n00000020  21 eb 40 fb 21 c2 20 11  01 66 66 20 5f 80 29 81   |!.@.!. ..ff _.).|\n00000030  31 24 92 40 c5 01 62 66  20 23 01 35 34 41 4f 40   |1$.@..bf #.54AO@|\n00000040  a1 e5 0a 92 e1 03 cc 21  a2 20 46 01 36 37 41 2a   |.......!. F.67A*|\n00000050  85 d9 02 38 62 62 20 88  80 58 02 36 37 32 20 6a   |...8bb ..X.672 j|\n00000060  02 36 30 66 20 a0 20 d0  20 f4 25 5c 20 be 26 a9   |.60f . . .%\\ .&.|\n00000070  20 17 02 38 32 31 20 3b  27 f1 40 70 01 65 37 60   | ..821 ;'.@p.e7`|\n00000080  23 44 55 02 38 30 64 20  1d 01 36 36 46 c2 80 47   |#DU.80d ..66F..G|\n00000090  80 ca 02 35 33 64 20 a0  01 37 33 45 92 24 00 20   |...53d ..73E.$. |\n000000a0  4d 02 35 32 33 20 05 80  35 86 6f 80 2f 01 38 64   |M.523 ..5.o./.8d|\n000000b0  64 72 62 bc 25 f1 20 35  21 00 20 95 20 9b 20 71   |drb.%. 5!. . . q|\n000000c0  20 89 23 40 27 16 00 34  20 1d 00 37 42 9e 21 3c   | .#@'..4 ..7B.!<|\n000000d0  21 48 80 05 80 41 01 35  37 42 38 02 38 63 36 23   |!H...A.57B8.8c6#|\n000000e0  04 01 36 30 43 64 00 39  20 30 20 0b 00 32 63 4c   |..60Cd.9 0 ..2cL|\n000000f0  00 66 67 8e 00 30 41 a8  01 35 38 41 cc 01 38 66   |.fg..0A..58A..8f|\n00000100  41 30 01 39 30 44 fc 41  36 20 17 e2 0e bc 01 36   |A0.90D.A6 .....6|\n00000110  62 61 a8 00 62 49 66 02  39 36 38 20 95 01 39 61   |ba..bIf.968 ..9a|\n00000120  67 17 63 10 01 35 66 48  45 80 83 e2 04 14 01 34   |g.c..5fHE......4|\n00000130  30 20 ad 02 35 63 35 24  42 87 06 21 01 20 b3 80   |0 ..5c5$B..!. ..|\n00000140  29 23 6a 22 3e 01 36 36  42 32 23 d5 20 53 25 13   |)#j\">.66B2#. S%.|\n00000150  20 e3 02 36 62 65 20 05  e0 03 23 26 95 21 31 01   | ..6be ...#&.!1.|\n00000160  35 32 42 08 01 35 35 61  01 6a 34 e0 04 23 00 36   |52B..55a.j4..#.6|\n00000170  4a 58 a7 b9 01 30 31 20  cb 02 39 61 38 20 05 e0   |JX...01 ..9a8 ..|\n00000180  03 23 02 38 38 63 20 89  a9 52 00 33 67 7e 00 30   |.#.88c ..R.3g~.0|\n00000190  49 1a 80 23 21 61 20 65  02 38 64 65 [Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 24 11 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:702 parsed req 1 res 0 type 5 state 0 rpos 11 of 11\n00000000  33 30 30 32 20 72 6e 22  7d 0d 0a                  |3002 rn\"}..|\n[Tue Jan 29 15:11:52 2013] nc_server.c:640 key 'testkey123' on dist 0 maps to server '10.11.80.55:20001:1'\n[Tue Jan 29 15:11:52 2013] nc_server.c:492 connecting on s 25 to server '10.11.80.55:20001:1'\n[Tue Jan 29 15:11:52 2013] nc_message.c:165 insert msg 1 into tmo rbt with expiry of 100 msec\n[Tue Jan 29 15:11:52 2013] nc_request.c:472 forward from c 24 to s 25 req 1 len 1867 type 5 with key 'testkey123'\n[Tue Jan 29 15:11:52 2013] nc_server.c:528 connected on s 25 to server '10.11.80.55:20001:1'\n[Tue Jan 29 15:11:52 2013] nc_connection.c:358 sendv on sd 25 1867 of 1867 in 5 buffers\n[Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 25 8 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:1159 parsed rsp 2 res 0 type 14 state 0 rpos 8 of 8\n00000000  53 54 4f 52 45 44 0d 0a                            |STORED..|\n[Tue Jan 29 15:11:52 2013] nc_message.c:183 delete msg 1 from tmo rbt\n[Tue Jan 29 15:11:52 2013] nc_connection.c:358 sendv on sd 24 8 of 8 in 1 buffers\n[Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 24 17 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:702 parsed req 3 res 0 type 1 state 0 rpos 17 of 17\n00000000  67 65 74 20 74 65 73 74  6b 65 79 31 32 33 20 0d   |get testkey123 .|\n00000010  0a                                                 |.|\n[Tue Jan 29 15:11:52 2013] nc_server.c:640 key 'testkey123' on dist 0 maps to server '10.11.80.55:20001:1'\n[Tue Jan 29 15:11:52 2013] nc_server.c:492 connecting on s 26 to server '10.11.80.55:20001:1'\n[Tue Jan 29 15:11:52 2013] nc_message.c:165 insert msg 3 into tmo rbt with expiry of 100 msec\n[Tue Jan 29 15:11:52 2013] nc_request.c:472 forward from c 24 to s 26 req 3 len 17 type 1 with key 'testkey123'\n[Tue Jan 29 15:11:52 2013] nc_server.c:528 connected on s 26 to server '10.11.80.55:20001:1'\n[Tue Jan 29 15:11:52 2013] nc_connection.c:358 sendv on sd 26 17 of 17 in 1 buffers\n[Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 26 464 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:1146 parsed rsp 4 res 4 type 19 state 10 rpos 464 of 464\n00000000  56 41 4c 55 45 20 74 65  73 74 6b 65 79 31 32 33   |VALUE testkey123|\n00000010  20 30 20 31 38 33 39 0d  0a 36 0b 00 00 1f 7b 22   | 0 1839..6....{\"|\n00000020  43 6f 6e 74 65 6e 74 22  3a 22 5c 75 34 65 33 62   |Content\":\"\\u4e3b|\n00000030  5c 75 36 66 31 34 5c 75  66 66 31 61 5c 75 03 35   |\\u6f14\\uff1a\\u.5|\n00000040  32 31 38 40 17 00 65 20  0b 0b 34 66 31 66 2d 2d   |218@..e ..4f1f--|\n00000050  5c 75 39 39 37 30 20 25  02 37 39 37 20 1f 02 39   |\\u9970 %.797 ..9|\n00000060  32 39 20 05 07 62 39 64  5c 75 33 30 30 20 17 e0   |29 ..b9d\\u300 ..|\n00000070  03 05 02 39 36 34 20 3d  09 36 30 32 31 5c 75 38   |...964 =.6021\\u8|\n00000080  34 63 39 e0 01 3d 02 35  33 35 40 3d 01 62 61 20   |4c9..=.535@=.ba |\n00000090  6f 02 37 64 32 40 05 05  34 33 33 20 72 6e e0 05   |o.7d2@..433 rn..|\n000000a0  40 80 4c 03 38 64 37 35  20 46 01 35 38 40 32 02   |@.L.8d75 F.58@2.|\n000000b0  33 35 33 e0 02 46 01 65  62 20 13 80 25 02 37 31   |353..F.eb ..%.71|\n000000c0  39 20 8a 80 0b e0 03 05  01 35 33 40 84 80 11 03   |9 .......53@....|\n000000d0  39 61 36 63 e0 01 3d 20  78 00 32 20 13 00 38 40   |9a6c..= x.2 ..8@|\n000000e0  bc 03 35 36 66 65 e0 0e  7e 02 37 33 38 20 9f 80   |..56fe..~.738 ..|\n000000f0  40 21 0f 00 61 e0 02 40  02 35 61 66 20 13 02 34   |@!..a..@.5af ..4|\n00000100  38 63 20 05 00 63 41 2f  80 25 e0 03 05 02 35 66   |8c ..cA/.%....5f|\n00000110  32 21 15 02 39 34 63 21  03 41 33 e0 01 3d 02 39   |2!..94c!.A3..=.9|\n00000120  38 37 21 55 02 36 63 62  20 5d 03 35 65 31 64 60   |87!U.6cb ].5e1d`|\n00000130  7e 02 35 31 38 20 eb 02  35 62 62 20 cb 02 37 62   |~.518 ..5bb ..7b|\n00000140  38 20 3a 02 34 65 63 20  20 41 8e 60 20 e0 04 5b   |8 :.4ec  A.` ..[|\n00000150  01 39 34 20 fe 02 35 66  64 20 05 21 65 40 4d 01   |.94 ..5fd .!e@M.|\n00000160  32 34 21 89 02 66 66 30  20 91 03 34 65 35 65 40   |24!..ff0 ..4e5e@|\n00000170  05 00 31 20 44 01 36 62  40 c3 02 37 35 62 20 11   |..1 D.6b@..75b .|\n00000180  02 37 65 66 20 b5 40 8b  20 05 60 2f 02 35 62 35   |.7ef .@. .`/.5b5|\n00000190  20 23 01 34 65 40 41 80  05 03 39 36 [Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 26 464 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:1146 parsed rsp 4 res 4 type 19 state 10 rpos 464 of 464\n00000000  82 20 58 03 37 32 30 36  20 05 01 62 31 20 11 80   |. X.7206 ..b1 ..|\n00000010  9a 02 36 35 61 20 a6 01  36 38 41 99 02 35 32 37   |..65a ..68A..527|\n00000020  40 58 00 33 40 7c 02 36  32 64 20 eb 02 34 65 62   |@X.3@|.62d ..4eb|\n00000030  21 97 02 35 65 39 20 7c  02 38 62 64 20 64 02 35   |!..5e9 |.8bd d.5|\n00000040  39 30 20 0b 80 05 80 47  01 36 35 41 ed 02 37 36   |90 ....G.65A..76|\n00000050  37 20 d0 02 35 31 66 20  fa 02 35 65 37 20 53 02   |7 ..51f ..5e7 S.|\n00000060  39 65 64 42 72 01 35 65  20 77 03 39 36 35 30 e0   |9edBr.5e w.9650.|\n00000070  0b 9a 01 30 32 40 f9 01  38 33 21 23 81 41 02 38   |...02@..83!#.A.8|\n00000080  62 65 e0 02 9a 40 3a 02  38 64 35 20 b2 22 dd 20   |be...@:.8d5 .\". |\n00000090  29 20 e1 20 4c e1 03 29  02 39 31 63 20 6a 03 38   |) . L..).91c j.8|\n000000a0  31 65 33 20 0b 01 36 63  20 29 01 34 65 42 fb 01   |1e3 ..6c ).4eB..|\n000000b0  38 65 42 cf 01 35 66 60  0b 01 66 39 20 d6 00 37   |8eB..5f`..f9 ..7|\n000000c0  22 af 20 17 00 39 40 23  02 35 32 36 20 4d 02 36   |\". ..9@#.526 M.6|\n000000d0  35 62 20 4d 02 39 38 63  42 6e e1 07 35 02 35 35   |5b M.98cBn..5.55|\n000000e0  39 40 94 e0 03 28 00 35  41 be 81 35 02 35 66 65   |9@...(.5A..5.5fe|\n000000f0  40 40 01 30 61 20 76 01  36 30 41 23 20 0b 20 64   |@@.0a v.60A# . d|\n00000100  02 36 62 36 40 11 00 63  43 b6 e1 04 05 00 32 40   |.6b6@..cC.....2@|\n00000110  3b 80 9a 02 34 66 35 20  23 02 37 65 63 21 1d 02   |;...4f5 #.7ec!..|\n00000120  37 65 64 20 0b 02 39 37  65 20 b8 21 d0 20 41 83   |7ed ..97e .!. A.|\n00000130  c0 02 36 35 66 20 11 23  ec 43 88 e0 08 9a 63 6a   |..65f .#.C....cj|\n00000140  83 64 20 64 00 34 20 40  01 35 66 20 05 a0 9a e0   |.d d.4 @.5f ....|\n00000150  04 1d 00 31 21 1d 20 76  21 9a 20 05 20 ed 02 34   |...1!. v!. . ..4|\n00000160  65 66 21 d6 04 35 30 65  37 28 20 06 01 62 39 60   |ef!..50e7( ..b9`|\n00000170  18 00 33 61 b3 43 bf 80  54 02 34 65 34 20 24 02   |..3a.C..T.4e4 $.|\n00000180  37 32 33 20 83 02 35 39  64 20 5a 04 36 64 33 65   |723 ..59d Z.6d3e|\n00000190  29 20 06 00 32 40 ba 02  38 64 34 21 [Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 26 464 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:1146 parsed rsp 4 res 4 type 19 state 10 rpos 464 of 464\n00000000  02 35 33 31 20 6c 81 26  81 df 23 5d 22 7a 23 1b   |.531 l.&..#]\"z#.|\n00000010  20 6b 01 37 31 40 8a 80  71 01 35 31 42 15 80 f7   | k.71@..q.51B...|\n00000020  20 4d 60 c0 e0 01 4d 80  ba 80 eb 02 32 30 31 40   | M`...M.....201@|\n00000030  3b 61 62 01 35 64 41 21  20 11 23 3f 80 65 02 34   |;ab.5dA! .#?.e.4|\n00000040  65 38 20 71 01 35 33 42  39 02 38 32 65 22 1b 21   |e8 q.53B9.82e\".!|\n00000050  c2 20 2f 24 05 21 92 00  37 23 11 20 1d 01 66 30   |. /$.!..7#. ..f0|\n00000060  20 05 82 5d a0 7d 00 33  41 aa 80 5f 02 37 36 32   | ..].}.3A.._.762|\n00000070  40 41 01 66 33 22 b1 02  38 30 30 20 41 02 34 66   |@A.f3\"..800 A.4f|\n00000080  65 41 d4 60 ad c0 65 61  5c 60 f5 25 ac 22 34 02   |eA.`..ea\\`.%.\"4.|\n00000090  35 31 36 20 23 24 e4 21  07 a0 65 62 db 81 31 02   |516 #$.!..eb..1.|\n000000a0  35 31 39 21 7a 80 fb 02  35 31 37 40 5f a0 e3 44   |519!z...517@_..D|\n000000b0  d9 23 8e 21 6d 20 7d 20  f5 01 37 65 40 a7 20 23   |.#.!m } ..7e@. #|\n000000c0  21 d5 84 a4 80 c5 20 3b  20 8f 22 db 20 5f 24 b0   |!..... ; .\". _$.|\n000000d0  20 1d 01 34 66 40 a7 02  37 62 34 20 fb 01 35 36   | ..4f@..7b4 ..56|\n000000e0  a3 d6 20 47 21 fe 40 89  64 3b 84 24 00 35 65 1b   |.. G!.@.d;.$.5e.|\n000000f0  80 e9 00 36 26 06 20 11  01 30 33 20 6b 01 36 37   |...6&. ..03 k.67|\n00000100  42 47 23 6b 20 0b a0 1d  a2 d0 60 dd e2 02 3a 00   |BG#k .....`...:.|\n00000110  39 21 14 20 35 64 cb 02  37 65 61 20 0b 80 59 20   |9!. 5d..7ea ..Y |\n00000120  b3 20 89 82 d6 a0 f5 01  66 61 20 a7 80 29 80 95   |. ......fa ..)..|\n00000130  20 8f 20 b3 20 bf 20 35  80 11 80 ef 01 38 36 44   | . . . 5.....86D|\n00000140  4e 81 c7 a0 71 02 31 34  34 20 23 27 05 c0 8f 01   |N...q.144 #'....|\n00000150  35 33 43 07 22 8d 21 61  84 4f 02 37 32 65 21 cd   |53C.\".!a.O.72e!.|\n00000160  21 c1 20 4d 80 b3 01 35  31 43 a8 01 37 61 85 0e   |!. M...51C..7a..|\n00000170  42 87 84 fc 80 1d 80 9b  21 c7 20 35 02 36 34 32   |B.......!. 5.642|\n00000180  20 89 02 35 65 32 20 ef  21 61 20 a7 01 38 66 41   | ..5e2 .!a ..8fA|\n00000190  37 e0 09 29 02 34 65 39  20 5f 02 37 [Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 26 464 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:1146 parsed rsp 4 res 4 type 19 state 10 rpos 464 of 464\n00000000  82 6f 20 1d 20 83 02 38  66 38 21 85 21 79 20 e9   |.o . ..8f8!.!y .|\n00000010  80 a1 80 83 80 ef 01 38  30 42 45 22 21 20 47 21   |.......80BE\"! G!|\n00000020  eb 40 fb 21 c2 20 11 01  66 66 20 5f 80 29 81 31   |.@.!. ..ff _.).1|\n00000030  24 92 40 c5 01 62 66 20  23 01 35 34 41 4f 40 a1   |$.@..bf #.54AO@.|\n00000040  e5 0a 92 e1 03 cc 21 a2  20 46 01 36 37 41 2a 85   |......!. F.67A*.|\n00000050  d9 02 38 62 62 20 88 80  58 02 36 37 32 20 6a 02   |..8bb ..X.672 j.|\n00000060  36 30 66 20 a0 20 d0 20  f4 25 5c 20 be 26 a9 20   |60f . . .%\\ .&. |\n00000070  17 02 38 32 31 20 3b 27  f1 40 70 01 65 37 60 23   |..821 ;'.@p.e7`#|\n00000080  44 55 02 38 30 64 20 1d  01 36 36 46 c2 80 47 80   |DU.80d ..66F..G.|\n00000090  ca 02 35 33 64 20 a0 01  37 33 45 92 24 00 20 4d   |..53d ..73E.$. M|\n000000a0  02 35 32 33 20 05 80 35  86 6f 80 2f 01 38 64 64   |.523 ..5.o./.8dd|\n000000b0  72 62 bc 25 f1 20 35 21  00 20 95 20 9b 20 71 20   |rb.%. 5!. . . q |\n000000c0  89 23 40 27 16 00 34 20  1d 00 37 42 9e 21 3c 21   |.#@'..4 ..7B.!<!|\n000000d0  48 80 05 80 41 01 35 37  42 38 02 38 63 36 23 04   |H...A.57B8.8c6#.|\n000000e0  01 36 30 43 64 00 39 20  30 20 0b 00 32 63 4c 00   |.60Cd.9 0 ..2cL.|\n000000f0  66 67 8e 00 30 41 a8 01  35 38 41 cc 01 38 66 41   |fg..0A..58A..8fA|\n00000100  30 01 39 30 44 fc 41 36  20 17 e2 0e bc 01 36 62   |0.90D.A6 .....6b|\n00000110  61 a8 00 62 49 66 02 39  36 38 20 95 01 39 61 67   |a..bIf.968 ..9ag|\n00000120  17 63 10 01 35 66 48 45  80 83 e2 04 14 01 34 30   |.c..5fHE......40|\n00000130  20 ad 02 35 63 35 24 42  87 06 21 01 20 b3 80 29   | ..5c5$B..!. ..)|\n00000140  23 6a 22 3e 01 36 36 42  32 23 d5 20 53 25 13 20   |#j\">.66B2#. S%. |\n00000150  e3 02 36 62 65 20 05 e0  03 23 26 95 21 31 01 35   |..6be ...#&.!1.5|\n00000160  32 42 08 01 35 35 61 01  6a 34 e0 04 23 00 36 4a   |2B..55a.j4..#.6J|\n00000170  58 a7 b9 01 30 31 20 cb  02 39 61 38 20 05 e0 03   |X...01 ..9a8 ...|\n00000180  23 02 38 38 63 20 89 a9  52 00 33 67 7e 00 30 49   |#.88c ..R.3g~.0I|\n00000190  1a 80 23 21 61 20 65 02  38 64 65 20 [Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 26 15 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:1159 parsed rsp 4 res 0 type 19 state 0 rpos 15 of 15\n00000000  30 30 32 20 72 6e 22 7d  0d 0a 45 4e 44 0d 0a      |002 rn\"}..END..|\n[Tue Jan 29 15:11:52 2013] nc_message.c:183 delete msg 3 from tmo rbt\n[Tue Jan 29 15:11:52 2013] nc_connection.c:358 sendv on sd 24 1871 of 1871 in 5 buffers\n[Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 24 464 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:677 parsed req 5 res 4 type 5 state 14 rpos 464 of 464\n00000000  73 65 74 20 74 65 73 74  6b 65 79 31 32 33 20 30   |set testkey123 0|\n00000010  20 32 20 31 35 32 34 0d  0a 7b 22 43 6f 6e 74 65   | 2 1524..{\"Conte|\n00000020  6e 74 22 3a 22 32 30 31  31 5c 75 35 65 37 34 39   |nt\":\"2011\\u5e749|\n00000030  5c 75 36 37 30 38 5c 75  66 66 30 63 5c 75 37 62   |\\u6708\\uff0c\\u7b|\n00000040  32 63 5c 75 34 65 30 30  5c 75 36 62 32 31 5c 75   |2c\\u4e00\\u6b21\\u|\n00000050  38 39 63 31 5c 75 34 66  36 30 5c 75 66 66 30 63   |89c1\\u4f60\\uff0c|\n00000060  5c 75 34 66 62 66 5c 75  38 39 63 39 5c 75 35 66   |\\u4fbf\\u89c9\\u5f|\n00000070  39 37 5c 75 34 65 63 65  5c 75 36 37 32 61 5c 75   |97\\u4ece\\u672a\\u|\n00000080  36 37 30 39 5c 75 34 65  62 61 5c 75 35 30 63 66   |6709\\u4eba\\u50cf|\n00000090  5c 75 34 66 36 30 5c 75  38 66 64 39 5c 75 38 32   |\\u4f60\\u8fd9\\u82|\n000000a0  32 63 5c 75 36 32 38 61  5c 75 37 36 37 64 5c 75   |2c\\u628a\\u767d\\u|\n000000b0  38 38 36 63 5c 75 38 38  36 62 5c 75 37 61 37 66   |886c\\u886b\\u7a7f|\n000000c0  5c 75 37 36 38 34 5c 75  35 39 38 32 5c 75 36 62   |\\u7684\\u5982\\u6b|\n000000d0  36 34 5c 75 35 39 37 64  5c 75 37 37 30 62 5c 75   |64\\u597d\\u770b\\u|\n000000e0  33 30 30 32 5c 75 36 32  31 31 5c 75 34 65 30 30   |3002\\u6211\\u4e00|\n000000f0  5c 75 37 36 66 34 5c 75  35 37 32 38 5c 75 34 66   |\\u76f4\\u5728\\u4f|\n00000100  36 30 5c 75 37 36 38 34  5c 75 38 65 61 62 5c 75   |60\\u7684\\u8eab\\u|\n00000110  38 66 62 39 5c 75 35 31  34 35 5c 75 35 66 35 33   |8fb9\\u5145\\u5f53|\n00000120  5c 75 37 37 34 30 5c 75  35 33 65 66 5c 75 36 37   |\\u7740\\u53ef\\u67|\n00000130  30 39 5c 75 35 33 65 66  5c 75 36 35 65 30 5c 75   |09\\u53ef\\u65e0\\u|\n00000140  37 36 38 34 5c 75 38 39  64 32 5c 75 38 32 37 32   |7684\\u89d2\\u8272|\n00000150  5c 75 66 66 30 63 5c 75  35 33 65 66 5c 75 36 36   |\\uff0c\\u53ef\\u66|\n00000160  32 66 5c 75 36 32 31 31  5c 75 35 33 37 34 5c 75   |2f\\u6211\\u5374\\u|\n00000170  34 65 30 64 5c 75 36 36  66 65 5c 75 35 34 30 65   |4e0d\\u66fe\\u540e|\n00000180  5c 75 36 30 39 34 5c 75  66 66 30 63 5c 75 34 65   |\\u6094\\uff0c\\u4e|\n00000190  35 66 5c 75 38 62 62 38  5c 75 36 37 [Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 24 464 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:677 parsed req 5 res 4 type 5 state 14 rpos 464 of 464\n00000000  37 65 63 38 5c 75 37 30  62 39 5c 75 33 30 30 32   |7ec8\\u70b9\\u3002|\n00000010  5c 75 35 33 37 33 5c 75  34 66 37 66 5c 75 34 66   |\\u5373\\u4f7f\\u4f|\n00000020  36 30 5c 75 35 66 63 33  5c 75 39 31 63 63 5c 75   |60\\u5fc3\\u91cc\\u|\n00000030  38 66 64 38 5c 75 36 37  30 39 5c 75 34 65 30 30   |8fd8\\u6709\\u4e00|\n00000040  5c 75 34 65 32 61 5c 75  35 39 37 39 5c 75 66 66   |\\u4e2a\\u5979\\uff|\n00000050  30 63 5c 75 35 33 37 33  5c 75 34 66 37 66 5c 75   |0c\\u5373\\u4f7f\\u|\n00000060  36 32 31 31 5c 75 35 65  33 38 5c 75 35 65 33 38   |6211\\u5e38\\u5e38|\n00000070  5c 75 38 38 61 62 5c 75  34 66 36 30 5c 75 35 66   |\\u88ab\\u4f60\\u5f|\n00000080  66 64 5c 75 38 39 63 36  5c 75 33 30 30 32 5c 75   |fd\\u89c6\\u3002\\u|\n00000090  34 65 30 30 5c 75 35 34  36 38 5c 75 35 65 37 34   |4e00\\u5468\\u5e74|\n000000a0  5c 75 37 65 61 61 5c 75  35 66 66 35 5c 75 36 35   |\\u7eaa\\u5ff5\\u65|\n000000b0  65 35 5c 75 36 35 66 36  5c 75 66 66 30 63 5c 75   |e5\\u65f6\\uff0c\\u|\n000000c0  34 66 36 30 5c 75 38 62  66 34 5c 75 35 62 66 39   |4f60\\u8bf4\\u5bf9|\n000000d0  5c 75 34 65 30 64 5c 75  38 64 37 37 5c 75 66 66   |\\u4e0d\\u8d77\\uff|\n000000e0  30 63 5c 75 36 32 31 31  5c 75 35 66 38 38 5c 75   |0c\\u6211\\u5f88\\u|\n000000f0  35 39 37 64 5c 75 66 66  30 63 5c 75 34 66 34 36   |597d\\uff0c\\u4f46|\n00000100  5c 75 36 32 31 31 5c 75  34 65 65 63 5c 75 34 65   |\\u6211\\u4eec\\u4e|\n00000110  30 64 5c 75 39 30 30 32  5c 75 35 34 30 38 5c 75   |0d\\u9002\\u5408\\u|\n00000120  66 66 30 63 5c 75 36 32  31 31 5c 75 34 66 31 61   |ff0c\\u6211\\u4f1a|\n00000130  5c 75 36 32 37 65 5c 75  35 32 33 30 5c 75 36 62   |\\u627e\\u5230\\u6b|\n00000140  64 34 5c 75 34 66 36 30  5c 75 36 36 66 34 5c 75   |d4\\u4f60\\u66f4\\u|\n00000150  35 39 37 64 5c 75 37 36  38 34 5c 75 34 65 62 61   |597d\\u7684\\u4eba|\n00000160  5c 75 33 30 30 32 5c 75  33 30 30 61 5c 75 36 32   |\\u3002\\u300a\\u62|\n00000170  31 31 5c 75 36 63 61 31  5c 75 36 37 30 39 5c 75   |11\\u6ca1\\u6709\\u|\n00000180  35 66 38 38 5c 75 36 30  66 33 5c 75 34 66 36 30   |5f88\\u60f3\\u4f60|\n00000190  5c 75 33 30 30 62 4a 50  4d 5c 72 5c [Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 24 464 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:677 parsed req 5 res 4 type 5 state 14 rpos 464 of 464\n00000000  32 31 31 5c 75 35 34 65  64 5c 75 34 65 38 36 5c   |211\\u54ed\\u4e86|\n00000010  75 34 65 30 30 5c 75 35  39 32 39 5c 75 34 65 30   |u4e00\\u5929\\u4e0|\n00000020  30 5c 75 35 39 31 63 5c  75 66 66 30 63 5c 75 36   |0\\u591c\\uff0c\\u6|\n00000030  32 31 31 5c 75 34 65 30  64 5c 75 36 31 63 32 5c   |211\\u4e0d\\u61c2|\n00000040  75 34 66 36 30 5c 75 34  65 33 61 5c 75 34 66 35   |u4f60\\u4e3a\\u4f5|\n00000050  35 5c 75 39 30 61 33 5c  75 34 65 34 38 5c 75 38   |5\\u90a3\\u4e48\\u8|\n00000060  66 37 62 5c 75 36 36 31  33 5c 75 35 63 33 31 5c   |f7b\\u6613\\u5c31|\n00000070  75 36 35 33 65 5c 75 35  66 30 33 5c 75 34 65 38   |u653e\\u5f03\\u4e8|\n00000080  36 5c 75 36 32 31 31 5c  75 34 65 65 63 5c 75 37   |6\\u6211\\u4eec\\u7|\n00000090  36 38 34 5c 75 37 32 33  31 5c 75 36 30 63 35 5c   |684\\u7231\\u60c5|\n000000a0  75 33 30 30 32 5c 75 35  37 32 38 5c 75 34 66 36   |u3002\\u5728\\u4f6|\n000000b0  30 5c 75 35 33 62 62 5c  75 37 36 66 38 5c 75 34   |0\\u53bb\\u76f8\\u4|\n000000c0  65 62 32 5c 75 37 36 38  34 5c 75 39 30 61 33 5c   |eb2\\u7684\\u90a3|\n000000d0  75 34 65 30 30 5c 75 35  39 32 39 5c 75 66 66 30   |u4e00\\u5929\\uff0|\n000000e0  63 5c 75 36 32 31 31 5c  75 35 65 32 36 5c 75 37   |c\\u6211\\u5e26\\u7|\n000000f0  37 34 30 5c 75 37 62 38  30 5c 75 35 33 35 35 5c   |740\\u7b80\\u5355|\n00000100  75 37 36 38 34 5c 75 38  38 34 63 5c 75 36 37 34   |u7684\\u884c\\u674|\n00000110  65 5c 75 66 66 30 63 5c  75 35 31 66 61 5c 75 35   |e\\uff0c\\u51fa\\u5|\n00000120  33 62 62 5c 75 36 64 34  31 5c 75 36 64 36 61 5c   |3bb\\u6d41\\u6d6a|\n00000130  75 33 30 30 32 5c 75 34  65 30 30 5c 75 34 65 32   |u3002\\u4e00\\u4e2|\n00000140  61 5c 75 34 65 62 61 5c  75 38 64 37 30 5c 75 35   |a\\u4eba\\u8d70\\u5|\n00000150  37 32 38 5c 75 35 66 30  32 5c 75 34 65 36 31 5c   |728\\u5f02\\u4e61|\n00000160  75 37 36 38 34 5c 75 38  38 35 37 5c 75 39 30 35   |u7684\\u8857\\u905|\n00000170  33 5c 75 66 66 30 63 5c  75 34 65 30 30 5c 75 34   |3\\uff0c\\u4e00\\u4|\n00000180  65 32 61 5c 75 34 65 62  61 5c 75 35 37 32 38 5c   |e2a\\u4eba\\u5728|\n00000190  75 35 66 30 32 5c 75 34  65 36 31 5c [Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 24 159 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:702 parsed req 5 res 0 type 5 state 0 rpos 159 of 159\n00000000  31 5c 75 37 37 34 30 5c  75 36 63 65 61 5c 75 33   |1\\u7740\\u6cea\\u3|\n00000010  30 30 32 5c 75 33 30 30  61 5c 75 36 63 65 61 5c   |002\\u300a\\u6cea|\n00000020  75 34 65 30 64 5c 75 35  30 35 63 5c 75 33 30 30   |u4e0d\\u505c\\u300|\n00000030  62 5c 75 36 65 32 39 5c  75 35 63 39 61 5c 72 5c   |b\\u6e29\\u5c9a\\r|\n00000040  6e 5c 75 34 66 36 30 5c  75 34 65 30 64 5c 75 36   |n\\u4f60\\u4e0d\\u6|\n00000050  36 66 65 5c 75 37 37 65  35 5c 75 39 30 35 33 5c   |6fe\\u77e5\\u9053|\n00000060  75 36 32 31 31 5c 75 35  34 65 64 5c 75 37 37 34   |u6211\\u54ed\\u774|\n00000070  30 5c 75 35 31 39 39 5c  75 35 62 38 63 5c 75 34   |0\\u5199\\u5b8c\\u4|\n00000080  65 30 30 5c 75 35 63 30  31 5c 75 35 31 65 30 5c   |e00\\u5c01\\u51e0|\n00000090  75 35 33 34 33 5c 75 35  62 35 37 22 7d 0d 0a      |u5343\\u5b57\"}..|\n[Tue Jan 29 15:11:52 2013] nc_server.c:640 key 'testkey123' on dist 0 maps to server '10.11.80.55:20001:1'\n[Tue Jan 29 15:11:52 2013] nc_server.c:492 connecting on s 27 to server '10.11.80.55:20001:1'\n[Tue Jan 29 15:11:52 2013] nc_message.c:165 insert msg 5 into tmo rbt with expiry of 100 msec\n[Tue Jan 29 15:11:52 2013] nc_request.c:472 forward from c 24 to s 27 req 5 len 1551 type 5 with key 'testkey123'\n[Tue Jan 29 15:11:52 2013] nc_server.c:528 connected on s 27 to server '10.11.80.55:20001:1'\n[Tue Jan 29 15:11:52 2013] nc_connection.c:358 sendv on sd 27 1551 of 1551 in 4 buffers\n[Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 27 8 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:1159 parsed rsp 6 res 0 type 14 state 0 rpos 8 of 8\n00000000  53 54 4f 52 45 44 0d 0a                            |STORED..|\n[Tue Jan 29 15:11:52 2013] nc_message.c:183 delete msg 5 from tmo rbt\n[Tue Jan 29 15:11:52 2013] nc_connection.c:358 sendv on sd 24 8 of 8 in 1 buffers\n[Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 24 17 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:702 parsed req 7 res 0 type 1 state 0 rpos 17 of 17\n00000000  67 65 74 20 74 65 73 74  6b 65 79 31 32 33 20 0d   |get testkey123 .|\n00000010  0a                                                 |.|\n[Tue Jan 29 15:11:52 2013] nc_server.c:640 key 'testkey123' on dist 0 maps to server '10.11.80.55:20001:1'\n[Tue Jan 29 15:11:52 2013] nc_server.c:492 connecting on s 28 to server '10.11.80.55:20001:1'\n[Tue Jan 29 15:11:52 2013] nc_message.c:165 insert msg 7 into tmo rbt with expiry of 100 msec\n[Tue Jan 29 15:11:52 2013] nc_request.c:472 forward from c 24 to s 28 req 7 len 17 type 1 with key 'testkey123'\n[Tue Jan 29 15:11:52 2013] nc_server.c:528 connected on s 28 to server '10.11.80.55:20001:1'\n[Tue Jan 29 15:11:52 2013] nc_connection.c:358 sendv on sd 28 17 of 17 in 1 buffers\n[Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 28 464 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:1146 parsed rsp 8 res 4 type 19 state 10 rpos 464 of 464\n00000000  56 41 4c 55 45 20 74 65  73 74 6b 65 79 31 32 33   |VALUE testkey123|\n00000010  20 30 20 31 35 32 34 0d  0a 7b 22 43 6f 6e 74 65   | 0 1524..{\"Conte|\n00000020  6e 74 22 3a 22 32 30 31  31 5c 75 35 65 37 34 39   |nt\":\"2011\\u5e749|\n00000030  5c 75 36 37 30 38 5c 75  66 66 30 63 5c 75 37 62   |\\u6708\\uff0c\\u7b|\n00000040  32 63 5c 75 34 65 30 30  5c 75 36 62 32 31 5c 75   |2c\\u4e00\\u6b21\\u|\n00000050  38 39 63 31 5c 75 34 66  36 30 5c 75 66 66 30 63   |89c1\\u4f60\\uff0c|\n00000060  5c 75 34 66 62 66 5c 75  38 39 63 39 5c 75 35 66   |\\u4fbf\\u89c9\\u5f|\n00000070  39 37 5c 75 34 65 63 65  5c 75 36 37 32 61 5c 75   |97\\u4ece\\u672a\\u|\n00000080  36 37 30 39 5c 75 34 65  62 61 5c 75 35 30 63 66   |6709\\u4eba\\u50cf|\n00000090  5c 75 34 66 36 30 5c 75  38 66 64 39 5c 75 38 32   |\\u4f60\\u8fd9\\u82|\n000000a0  32 63 5c 75 36 32 38 61  5c 75 37 36 37 64 5c 75   |2c\\u628a\\u767d\\u|\n000000b0  38 38 36 63 5c 75 38 38  36 62 5c 75 37 61 37 66   |886c\\u886b\\u7a7f|\n000000c0  5c 75 37 36 38 34 5c 75  35 39 38 32 5c 75 36 62   |\\u7684\\u5982\\u6b|\n000000d0  36 34 5c 75 35 39 37 64  5c 75 37 37 30 62 5c 75   |64\\u597d\\u770b\\u|\n000000e0  33 30 30 32 5c 75 36 32  31 31 5c 75 34 65 30 30   |3002\\u6211\\u4e00|\n000000f0  5c 75 37 36 66 34 5c 75  35 37 32 38 5c 75 34 66   |\\u76f4\\u5728\\u4f|\n00000100  36 30 5c 75 37 36 38 34  5c 75 38 65 61 62 5c 75   |60\\u7684\\u8eab\\u|\n00000110  38 66 62 39 5c 75 35 31  34 35 5c 75 35 66 35 33   |8fb9\\u5145\\u5f53|\n00000120  5c 75 37 37 34 30 5c 75  35 33 65 66 5c 75 36 37   |\\u7740\\u53ef\\u67|\n00000130  30 39 5c 75 35 33 65 66  5c 75 36 35 65 30 5c 75   |09\\u53ef\\u65e0\\u|\n00000140  37 36 38 34 5c 75 38 39  64 32 5c 75 38 32 37 32   |7684\\u89d2\\u8272|\n00000150  5c 75 66 66 30 63 5c 75  35 33 65 66 5c 75 36 36   |\\uff0c\\u53ef\\u66|\n00000160  32 66 5c 75 36 32 31 31  5c 75 35 33 37 34 5c 75   |2f\\u6211\\u5374\\u|\n00000170  34 65 30 64 5c 75 36 36  66 65 5c 75 35 34 30 65   |4e0d\\u66fe\\u540e|\n00000180  5c 75 36 30 39 34 5c 75  66 66 30 63 5c 75 34 65   |\\u6094\\uff0c\\u4e|\n00000190  35 66 5c 75 38 62 62 38  5c 75 36 37 [Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 28 464 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:1146 parsed rsp 8 res 4 type 19 state 10 rpos 464 of 464\n00000000  37 65 63 38 5c 75 37 30  62 39 5c 75 33 30 30 32   |7ec8\\u70b9\\u3002|\n00000010  5c 75 35 33 37 33 5c 75  34 66 37 66 5c 75 34 66   |\\u5373\\u4f7f\\u4f|\n00000020  36 30 5c 75 35 66 63 33  5c 75 39 31 63 63 5c 75   |60\\u5fc3\\u91cc\\u|\n00000030  38 66 64 38 5c 75 36 37  30 39 5c 75 34 65 30 30   |8fd8\\u6709\\u4e00|\n00000040  5c 75 34 65 32 61 5c 75  35 39 37 39 5c 75 66 66   |\\u4e2a\\u5979\\uff|\n00000050  30 63 5c 75 35 33 37 33  5c 75 34 66 37 66 5c 75   |0c\\u5373\\u4f7f\\u|\n00000060  36 32 31 31 5c 75 35 65  33 38 5c 75 35 65 33 38   |6211\\u5e38\\u5e38|\n00000070  5c 75 38 38 61 62 5c 75  34 66 36 30 5c 75 35 66   |\\u88ab\\u4f60\\u5f|\n00000080  66 64 5c 75 38 39 63 36  5c 75 33 30 30 32 5c 75   |fd\\u89c6\\u3002\\u|\n00000090  34 65 30 30 5c 75 35 34  36 38 5c 75 35 65 37 34   |4e00\\u5468\\u5e74|\n000000a0  5c 75 37 65 61 61 5c 75  35 66 66 35 5c 75 36 35   |\\u7eaa\\u5ff5\\u65|\n000000b0  65 35 5c 75 36 35 66 36  5c 75 66 66 30 63 5c 75   |e5\\u65f6\\uff0c\\u|\n000000c0  34 66 36 30 5c 75 38 62  66 34 5c 75 35 62 66 39   |4f60\\u8bf4\\u5bf9|\n000000d0  5c 75 34 65 30 64 5c 75  38 64 37 37 5c 75 66 66   |\\u4e0d\\u8d77\\uff|\n000000e0  30 63 5c 75 36 32 31 31  5c 75 35 66 38 38 5c 75   |0c\\u6211\\u5f88\\u|\n000000f0  35 39 37 64 5c 75 66 66  30 63 5c 75 34 66 34 36   |597d\\uff0c\\u4f46|\n00000100  5c 75 36 32 31 31 5c 75  34 65 65 63 5c 75 34 65   |\\u6211\\u4eec\\u4e|\n00000110  30 64 5c 75 39 30 30 32  5c 75 35 34 30 38 5c 75   |0d\\u9002\\u5408\\u|\n00000120  66 66 30 63 5c 75 36 32  31 31 5c 75 34 66 31 61   |ff0c\\u6211\\u4f1a|\n00000130  5c 75 36 32 37 65 5c 75  35 32 33 30 5c 75 36 62   |\\u627e\\u5230\\u6b|\n00000140  64 34 5c 75 34 66 36 30  5c 75 36 36 66 34 5c 75   |d4\\u4f60\\u66f4\\u|\n00000150  35 39 37 64 5c 75 37 36  38 34 5c 75 34 65 62 61   |597d\\u7684\\u4eba|\n00000160  5c 75 33 30 30 32 5c 75  33 30 30 61 5c 75 36 32   |\\u3002\\u300a\\u62|\n00000170  31 31 5c 75 36 63 61 31  5c 75 36 37 30 39 5c 75   |11\\u6ca1\\u6709\\u|\n00000180  35 66 38 38 5c 75 36 30  66 33 5c 75 34 66 36 30   |5f88\\u60f3\\u4f60|\n00000190  5c 75 33 30 30 62 4a 50  4d 5c 72 5c [Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 28 464 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:1146 parsed rsp 8 res 4 type 19 state 10 rpos 464 of 464\n00000000  32 31 31 5c 75 35 34 65  64 5c 75 34 65 38 36 5c   |211\\u54ed\\u4e86|\n00000010  75 34 65 30 30 5c 75 35  39 32 39 5c 75 34 65 30   |u4e00\\u5929\\u4e0|\n00000020  30 5c 75 35 39 31 63 5c  75 66 66 30 63 5c 75 36   |0\\u591c\\uff0c\\u6|\n00000030  32 31 31 5c 75 34 65 30  64 5c 75 36 31 63 32 5c   |211\\u4e0d\\u61c2|\n00000040  75 34 66 36 30 5c 75 34  65 33 61 5c 75 34 66 35   |u4f60\\u4e3a\\u4f5|\n00000050  35 5c 75 39 30 61 33 5c  75 34 65 34 38 5c 75 38   |5\\u90a3\\u4e48\\u8|\n00000060  66 37 62 5c 75 36 36 31  33 5c 75 35 63 33 31 5c   |f7b\\u6613\\u5c31|\n00000070  75 36 35 33 65 5c 75 35  66 30 33 5c 75 34 65 38   |u653e\\u5f03\\u4e8|\n00000080  36 5c 75 36 32 31 31 5c  75 34 65 65 63 5c 75 37   |6\\u6211\\u4eec\\u7|\n00000090  36 38 34 5c 75 37 32 33  31 5c 75 36 30 63 35 5c   |684\\u7231\\u60c5|\n000000a0  75 33 30 30 32 5c 75 35  37 32 38 5c 75 34 66 36   |u3002\\u5728\\u4f6|\n000000b0  30 5c 75 35 33 62 62 5c  75 37 36 66 38 5c 75 34   |0\\u53bb\\u76f8\\u4|\n000000c0  65 62 32 5c 75 37 36 38  34 5c 75 39 30 61 33 5c   |eb2\\u7684\\u90a3|\n000000d0  75 34 65 30 30 5c 75 35  39 32 39 5c 75 66 66 30   |u4e00\\u5929\\uff0|\n000000e0  63 5c 75 36 32 31 31 5c  75 35 65 32 36 5c 75 37   |c\\u6211\\u5e26\\u7|\n000000f0  37 34 30 5c 75 37 62 38  30 5c 75 35 33 35 35 5c   |740\\u7b80\\u5355|\n00000100  75 37 36 38 34 5c 75 38  38 34 63 5c 75 36 37 34   |u7684\\u884c\\u674|\n00000110  65 5c 75 66 66 30 63 5c  75 35 31 66 61 5c 75 35   |e\\uff0c\\u51fa\\u5|\n00000120  33 62 62 5c 75 36 64 34  31 5c 75 36 64 36 61 5c   |3bb\\u6d41\\u6d6a|\n00000130  75 33 30 30 32 5c 75 34  65 30 30 5c 75 34 65 32   |u3002\\u4e00\\u4e2|\n00000140  61 5c 75 34 65 62 61 5c  75 38 64 37 30 5c 75 35   |a\\u4eba\\u8d70\\u5|\n00000150  37 32 38 5c 75 35 66 30  32 5c 75 34 65 36 31 5c   |728\\u5f02\\u4e61|\n00000160  75 37 36 38 34 5c 75 38  38 35 37 5c 75 39 30 35   |u7684\\u8857\\u905|\n00000170  33 5c 75 66 66 30 63 5c  75 34 65 30 30 5c 75 34   |3\\uff0c\\u4e00\\u4|\n00000180  65 32 61 5c 75 34 65 62  61 5c 75 35 37 32 38 5c   |e2a\\u4eba\\u5728|\n00000190  75 35 66 30 32 5c 75 34  65 36 31 5c [Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 28 164 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:1159 parsed rsp 8 res 0 type 19 state 0 rpos 164 of 164\n00000000  31 5c 75 37 37 34 30 5c  75 36 63 65 61 5c 75 33   |1\\u7740\\u6cea\\u3|\n00000010  30 30 32 5c 75 33 30 30  61 5c 75 36 63 65 61 5c   |002\\u300a\\u6cea|\n00000020  75 34 65 30 64 5c 75 35  30 35 63 5c 75 33 30 30   |u4e0d\\u505c\\u300|\n00000030  62 5c 75 36 65 32 39 5c  75 35 63 39 61 5c 72 5c   |b\\u6e29\\u5c9a\\r|\n00000040  6e 5c 75 34 66 36 30 5c  75 34 65 30 64 5c 75 36   |n\\u4f60\\u4e0d\\u6|\n00000050  36 66 65 5c 75 37 37 65  35 5c 75 39 30 35 33 5c   |6fe\\u77e5\\u9053|\n00000060  75 36 32 31 31 5c 75 35  34 65 64 5c 75 37 37 34   |u6211\\u54ed\\u774|\n00000070  30 5c 75 35 31 39 39 5c  75 35 62 38 63 5c 75 34   |0\\u5199\\u5b8c\\u4|\n00000080  65 30 30 5c 75 35 63 30  31 5c 75 35 31 65 30 5c   |e00\\u5c01\\u51e0|\n00000090  75 35 33 34 33 5c 75 35  62 35 37 22 7d 0d 0a 45   |u5343\\u5b57\"}..E|\n000000a0  4e 44 0d 0a                                        |ND..|\n[Tue Jan 29 15:11:52 2013] nc_message.c:183 delete msg 7 from tmo rbt\n[Tue Jan 29 15:11:52 2013] nc_connection.c:358 sendv on sd 24 1556 of 1556 in 4 buffers\n[Tue Jan 29 15:11:52 2013] nc_connection.c:307 recv on sd 24 6 of 464\n[Tue Jan 29 15:11:52 2013] nc_memcache.c:702 parsed req 9 res 0 type 12 state 0 rpos 6 of 6\n00000000  71 75 69 74 0d 0a                                  |quit..|\n[Tue Jan 29 15:11:52 2013] nc_request.c:384 filter quit req 9 from c 24\n[Tue Jan 29 15:11:52 2013] nc_request.c:341 c 24 is done\n[Tue Jan 29 15:11:52 2013] nc_core.c:207 close c 24 '127.0.0.1:59948' on event 0001 eof 1 done 1 rb 3458 sb 3443  \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12823735/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12823903", "body": "Sure, sorry & please give me some minutes.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12823903/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12824282", "body": "Sorry, after testing, it seems can't be short!\n\nWhen the string is short, which is shorter than 2000, it works just well\nBut when longer than 2000, then it got messy.\n\nSeems like there is a \"cut\" or something like that? But not sure...\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12824282/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12824647", "body": "I see the messy string return by Get(), like :\n'G\ufffd\ufffd{\"Content\":\"2011\\u5e749\\u6708\\uff0c\\u7b2 4e00 b2 $89c 4f6 \ufffd#4fbf`\n\nBut I expect it to be a normal string like :\n{\"Content\":\"\\u6797--\\u9970\\u987a\\u6cbb\\u5e1d rn\n\nMy twemproxy version: 0.2.1\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12824647/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12826528", "body": "After comparing the logs, the string is messied before into twemproxy, it the problem of php ... closed\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12826528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12978868", "body": "In fact the problem is ... for example we have 100 nodes on backend, the KEY was saved in 49 of them, then it seems we have to take 51 WASTE operations...\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12978868/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13111372", "body": "I mean... if there could be a nice OPTION in the future which call \"auto_key_flush\"?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13111372/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "AllenDou": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11400306", "body": "yes, the old version ' info of redis is totally different form new version's.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11400306/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11413096", "body": "@charsyam @manjuraj  thank you.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11413096/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11475031", "body": "@charsyam @manjuraj  ,i saw the redis code , the 'info' command is just like 'get' command, and its argument is 'default' , so ,in this case, 'info default' is same as 'get key' command , i dont know if it is useful to your. thank you.\nby the way , i've already modified the code , it works for me .\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11475031/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11475086", "body": "but the old version of redis ,take v2.0.5 for example. it doesn't support \"info default\" command. so , my opinion above is only supported by new version of redis.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11475086/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "jgallartm": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11471963", "body": "It makes a lot of sense.\n\nThanks\n\nJavi\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11471963/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "jackylu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11600682", "body": "I have add POOL_NAME, also not works, the nutcracker.yml now is\n  POOL_NAME:twemproxy\n  listen: 127.0.0.1:22123\n  hash: fnv1a_64\n  distribution: ketama\n  timeout: 400\n  backlog: 1024\n  preconnect: true\n  auto_eject_hosts: true\n  server_retry_timeout: 2000\n  server_failure_limit: 3\n  servers:\n- 127.0.0.1:11212:1\n- 127.0.0.1:11213:1\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11600682/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11601153", "body": "I use the conf/nutcracker.leaf.yml, it works ! Thanks charsyam!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11601153/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "hamgua": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11657780", "body": "thanks for your email!\nwhen i know  how to work by twemroxy,i know how to use any other way!\nand now,i want to know how to config to share data in two or more redis\nserver !\n\nthank you very much!\n\n2012/12/24 charsyam notifications@github.com\n\n> hi @hamgua https://github.com/hamgua, I think \"select\" command is very\n> dangerous in twemproxy.\n> \n> as you know, twemproxy just use one connection with each redis server. and\n> it runs like redis client.\n> \n> so if twemproxy allows \"select\" command.\n> \n> other clients are affected by changing db.\n> \n> in redis server, client connections have their own db id\n> \n> int selectDb(redisClient *c, int id) {\n>     if (id < 0 || id >= server.dbnum)\n>         return REDIS_ERR;\n>     c->db = &server.db[id];\n>     return REDIS_OK; }\n> \n> so I think twemproxy can't support \"select\" command.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/twitter/twemproxy/issues/34#issuecomment-11655971.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11657780/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11668535", "body": "What is Twemproxy useful for? It can\n- reduce the number of connections to your cache server by acting as a\n  proxy\n- shard data automatically between multiple cache servers\n- support consistent hashing with different strategies and hashing\n  functions\n- be configured to disable nodes on failure\n- run in multiple instances, allowing client to connect to the first\n  available proxy server\n- Pipelining and batching of requests and hence saving of round-trips\n\nintroductio for twemproxy by infoq!\nurl:http://www.infoq.com/news/2012/12/twemproxy\n\nnum 2:shard data automatically between multiple cache servers\n\ni don't know how to use!can you help me!\n\n2012/12/24 charsyam notifications@github.com\n\n> @hamgua https://github.com/hamgua Hmm. I think the best way to use\n> redis with twemproxy in your situation.\n> \n> just use db 0 in redis, Using multiple db in one redis doesn't support\n> better performance.\n> \n> and redis will stop support \"SELECT\" command in cluster mode( yes, it's\n> not implemented yet, maybe from redis 3.0 )\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/twitter/twemproxy/issues/34#issuecomment-11658003.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11668535/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "yugene": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11704013", "body": "Guys, for those of you, who have problems with deletion through PHP memcache extension, there is a patch for twemproxy in my fork: https://github.com/yugene/twemproxy/commit/b965652166e1e67c2d7790c46f88d4c9897688ee\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11704013/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11726255", "body": "Agree, this should not be in the master.\n\nPhp extension manual is a bit misleading. You may set the timeout parameter to 0 or not use it at all, but it will appear in request to memcache (at least, in latest stable).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11726255/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12357040", "body": "That's cause of EOL symbols. AFAIK, nc sends \"\\n\" by default, while twemproxy requires \"\\r\\n\". Try telnet instead.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12357040/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "NoDurex": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11726687", "body": "yeah!  I  got  it .  Thank you   very  much!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11726687/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11871890", "body": "@charsyam  my client code   just  using  XMemcached client ,and  just  setting  a key-value to the port  23333 or  24444 ,the  log  is  showed  by  nutcracker.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11871890/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11872472", "body": "@manjuraj  i use the v0.2.2,but  not running with erbosity level 8 (-v 8),just the default setting.\n\nnow,with the verbosity level 8 ,the log is\n\n```\n[Fri Jan  4 12:49:28 2013] nc_proxy.c:281 accept on p 6 not ready - eagain\n[Fri Jan  4 12:49:28 2013] nc_proxy.c:337 accepted c 47 on p 7 from '192.168.0.155:52911'\n[Fri Jan  4 12:49:28 2013] nc_proxy.c:281 accept on p 7 not ready - eagain\n[Fri Jan  4 12:49:32 2013] nc_connection.c:307 recv on sd 8 9 of 16336\n[Fri Jan  4 12:49:32 2013] nc_memcache.c:712 parsed bad req 61 res 1 type 0 state 1\n00000000  76 65 72 73 69 6f 6e 0d  0a                        |version..|\n[Fri Jan  4 12:49:32 2013] nc_core.c:168 recv on c 8 failed: Invalid argument\n[Fri Jan  4 12:49:32 2013] nc_core.c:207 close c 8 '192.168.0.155:52867' on event 0001 eof 0 done 0 rb 9 sb 0: Invalid argument\n[Fri Jan  4 12:49:32 2013] nc_client.c:147 close c 8 discarding pending req 61 len 9 type 0\n[Fri Jan  4 12:49:33 2013] nc_connection.c:307 recv on sd 18 9 of 16336\n[Fri Jan  4 12:49:33 2013] nc_memcache.c:712 parsed bad req 62 res 1 type 0 state 1\n00000000  76 65 72 73 69 6f 6e 0d  0a                        |version..|\n[Fri Jan  4 12:49:33 2013] nc_core.c:168 recv on c 18 failed: Invalid argument\n[Fri Jan  4 12:49:33 2013] nc_core.c:207 close c 18 '192.168.0.155:52880' on event 0001 eof 0 done 0 rb 9 sb 0: Invalid argument\n[Fri Jan  4 12:49:33 2013] nc_client.c:147 close c 18 discarding pending req 62 len 9 type 0\n[Fri Jan  4 12:49:33 2013] nc_connection.c:307 recv on sd 33 9 of 16336\n[Fri Jan  4 12:49:33 2013] nc_memcache.c:712 parsed bad req 63 res 1 type 0 state 1\n00000000  76 65 72 73 69 6f 6e 0d  0a                        |version..|\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11872472/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11872692", "body": "@manjuraj  Is here  any  clinets  I  can  use  for  twemproxy?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11872692/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11872865", "body": "@manjuraj   java\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11872865/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11874102", "body": "No, it doesn't  help.\n\n2013/1/4 Manju Rajashekhar notifications@github.com\n\n> just configure xmemcached to not send \"version\" command. Maybe use\n> TextCommandFactory?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/twitter/twemproxy/issues/43#issuecomment-11873838.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11874102/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11874401", "body": "Does twemproxy  has  the  client project?\n\n2013/1/4 Manju Rajashekhar notifications@github.com\n\n> maybe post on xmemcached google group list\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/twitter/twemproxy/issues/43#issuecomment-11874179.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11874401/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11877062", "body": "@manjuraj hello,i don't want to share data automatically across multiple servers for this problem. just want to  close  this  feature,So how can i do ?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11877062/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "conmame": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11766435", "body": "@manjuraj Thanks!\nI changed indentiing on d157bab commit.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/11766435/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "aaronuu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12078965", "body": "@charsyam I have a lot of data distribution in the node 1 and node 2,I add a new node after, Inquires the data is wrong. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12078965/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12131123", "body": "@charsyam Three counter, record data for 10, 15, 30. After adding new node data into 10, 0, 0\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12131123/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "sscarduzio": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12102408", "body": "Your suggestion is valid. It would be tidier to introduce the feature at server pool level.\neg. Let it be the \"pubsub_pool\" vs the \"cache_pool\". And blacklist routing (or equivalent policy) to work separating traffic accordingly.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12102408/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "LD250": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12256073", "body": "When do you plan to introduce support for MSET command?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12256073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "agladysh": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12357100", "body": "On Thu, Jan 17, 2013 at 11:15 AM, Yevgeny Yegorov\nnotifications@github.comwrote:\n\n> That's cause of EOL symbols. AFAIK, nc sends \"\\n\" by default, while\n> twemproxy requires \"\\r\\n\". Try telnet instead.\n> \n> ... or try passing `-C` option to the `netcat`.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12357100/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "squamos": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12383451", "body": "+1 For this feature\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12383451/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12910080", "body": "Thanks!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12910080/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dcartoon": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12407116", "body": "You're correct, all my changes are based on the assumption that all the\nsupplied keys hash to the same server(since we have hash_tag support).\n\nIf that's not a safe assumption, then this won't work.\n\nOne could check that all the keys hash to the same server, but that would\nbe slower/harder to implement.  We could also require hash_tags for some\ncommands and raise some sort of error if they aren't configured. Both\nsolutions aren't that great, but they do make the hash_tag constraint more\napparent(so users aren't trying to figure out why their data disappears).\n\nI'm open to other approaches since I need set operations for an application\nthat I'm working on.\n\nDan\nOn Jan 17, 2013 7:58 PM, \"charsyam\" notifications@github.com wrote:\n\n> @dcartoon https://github.com/dcartoon I have a question about this\n> patch.\n> \n> Does it work well?\n> \n> I think it can cause many problems.\n> \n> for example, RPOPLPUSH command needs 2 listnames.\n> \n> RPOPLPUSH list1 list2\n> \n> at that time this patch will push the value to the same server.\n> \n> and if hashed servers are different, it can't find the collect server.\n> \n> so if you make it correctly, you have to split RPOPLPUSH to RPOP and LPUSH\n> \n> and there is another problem that how to handle one of servers failed.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/twitter/twemproxy/pull/54#issuecomment-12406785.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12407116/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12436597", "body": "I was looking at nc_redis.c this morning and realized that I missed the redis_argx function, which would be the better function to use for these commands and would allow the splitting that charsyam mentioned above.  \n\nAt the same time, RPOPLPUSH is supposed to be atomic, and we would lose atomicity if we split it into separate pop and push commands.  For the set operations, performance would be poor when working with large sets that are spread across multiple servers(and they also wouldn't be atomic).\n\nI think I need some guidance on what the best approach is here.\n\nDan\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12436597/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12711995", "body": "@manjuraj  - Thanks for the feedback!\n\nIf the code looks good and I update the Redis notes to include your points regarding hash tags, then is everything good to go?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12711995/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "srned": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12467466", "body": "@manjuraj I got a chance to work on this and the patch is at https://github.com/srned/twemproxy Let me know your thoughts and I can submit a pull request.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12467466/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12579449", "body": "@Ayutthaya I am glad you liked it :-)\n@manjuraj Thanks!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12579449/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ferenyx": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12617391", "body": "Hi manjuraj,\n\nI have written a patch that provides rudimentary EVAL/EVALSHA support for scripts that take exactly one key. I can expand that to more generic support, but I wanted to know if you had any thoughts about how that should look like.\n\nSpecifically:\n1) The sharding algorithm (or even the characters that determine the hash tag part of a key) isn't known in the redis protocol parser (nc_redis.c). We either have to expose this to the parser, or we have to expand struct msg to hold multiple keys so that either the parser or server_pool_conn() can determine if the query is well-formed. Do you have an opinion either way?\n2) Do we support EVAL of scripts that take 0 keys (e.g. EVAL \"return 0\" 0) - should we execute this on an arbitrary server, or make it mandatory for at least 1 key to be provided even if the script doesn't use the key so the server the script is executed on is deterministically picked?\n\nThanks\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12617391/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12700320", "body": "I sent a pull request, and that's now at issue #60. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12700320/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12897477", "body": "The change for (1) has been done:\n\n```\nredis 127.0.0.1:22121> eval \"return 123\" 0\nError: Server closed the connection\nredis 127.0.0.1:22121> eval \"return 123\" 1\nError: Server closed the connection\nredis 127.0.0.1:22121> eval \"return 123\" 1 1\n(integer) 123\nredis 127.0.0.1:22121> eval \"return {KEYS[1],KEYS[2]}\" 2 1 asdrf asdfw\n1) \"1\"\n2) \"asdrf\"\nredis 127.0.0.1:22121> eval \"return {KEYS[1],ARGV[1]}\" 2 1 asdrf asdfw\n1) \"1\"\n2) \"asdfw\"\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12897477/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14439901", "body": "which host would keys on the failing host hash to then?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14439901/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14441190", "body": "Doesn't that make the behavior for failed hosts between different distribution options different then? For instance, if you select ketama, keys that originally hashed to a failing host will get hashed to a different (working) host now, but if you select modula, those keys would permanently fail until that particular host is now available?\n\nIt seems like the problem you're facing, where all keys (instead of just keys on the failing host) get rehashed is precisely what consistent hashing aims to solve.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14441190/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "cofyc": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12688394", "body": "Hi, in restarting process, If init.d script can check new deployed configuration file is valid or not before stopping current twemporxy, It may avoid human mistake.\n\nexit code is used by init.d script.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12688394/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "xiefei": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12766529", "body": "thanks\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12766529/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "fifsky": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12769561", "body": "I think only the loss of a certain percentage Or recalculate the hash of new nodes\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/12769561/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ejc3": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13233678", "body": "Would you accept a pull request that adds auth support into twemproxy? In our deployment, we generally like to restrict the hosts that can connect to a particular system and using auth is a lightweight way of doing that (similar to putting a password on a mysql server).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13233678/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13328696", "body": "@manjraj, thanks for the go-ahead! We'll dig into this and get back to you, assuming charsyam@ doesn't beat us to it. Thanks for the effort on this software.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/13328696/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "kevwil": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14454323", "body": "At the risk of showing my naivety with C code, I would like to understand why @manjuraj has taken this stance. Please forgive me if this is a silly question.\n\nI don't understand the logic of asking contributors to implement event abstractions but not being willing to depend on existing event abstractions. Existing abstractions like libev and libuv are hardened and proven in real-world use, where a one-off abstraction implementation is neither, at least for a while. I don't think it's a performance issue, because accepting hand-coded abstractions from contributors may not yield any faster performance than existing abstraction libraries ... likely the performance might be a bit worse until the code is hardened. Abstractions like libev, or libuv even more so, would broaden the potential developer audience quite a bit. So ... why not just use libuv or libev? Aren't these hand-coded abstractions re-inventing the wheel?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/14454323/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "nitper": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/3668996", "body": "@manjuraj just wanted to let you know that we are fully using twemproxy with redis at bright.com and are loving it\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/3668996/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "1125449708": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/4628152", "body": "Fantastic.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/4628152/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "dominis": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/5512362", "body": "lol\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/5512362/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "GOPALYADAV": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/6924236", "body": "# hash_tag\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/6924236/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/6924245", "body": "+language:6\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/6924245/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "BrandonBrowning": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/10019622", "body": "I seem to have gotten a similar error with the latest code?\n\n```\n[2015-03-03 19:01:55.777] nc_redis.c:2076 parsed bad rsp 2 res 1 type 133 state 11\n00000000  2a 32 0d 0a 24 32 0d 0a  72 30 0d 0a 2b 4f 4b 0d   |*2..$2..r0..+OK.|\n00000010  0a                                                 |.|\n[2015-03-03 19:01:55.777] nc_core.c:198 recv on s 8 failed: Invalid argument\n```\n\nExpected\n\n```\n1) \"r0\"\n2) OK\n```\n\nCaused by running\n\n```\nevalsha 370ca495ee43ef26fae6c1d4dfa16baac375026d 1 a set 5\n```\n\n(it's a wrapper around normal command execution)\n(reference #108)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10019622/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10031992", "body": "Give twemproxy a single redis instance (only for simplicity; works with multiple obviously)\nBash variables represent ports\n\n```\n$ redis-cli -p $nutcracker set _name r0\n$ redis-cli -p $nutcracker eval \"local command = table.remove(ARGV, 1); table.insert(ARGV, 1, KEYS[1]); return { redis.call('get', '_name'), redis.call(command, unpack(ARGV)) }\" 1 a get\n1) \"r0\"\n2) (nil)\n\n# looking good\n\n$ redis-cli -p $nutcracker eval \"local command = table.remove(ARGV, 1); table.insert(ARGV, 1, KEYS[1]); return { redis.call('get', '_name'), redis.call(command, unpack(ARGV)) }\" 1 a set 5\n(error) ERR Invalid argument\n\n# oh no\n\n$ redis-cli -p $redis eval \"local command = table.remove(ARGV, 1); table.insert(ARGV, 1, KEYS[1]); return { redis.call('get', '_name'), redis.call(command, unpack(ARGV)) }\" 1 a set 5\n1) \"r0\"\n2) OK\n\n# works directly on client\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10031992/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10071652", "body": "Repro case is fixed!  Thanks :)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10071652/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "jdi-tagged": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/6936780", "body": "This does the opposite of what the comment above says; with this change having unique server names doesn't help if the pnames match.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/6936780/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "asharpe": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/20074044", "body": "Perhaps it'd be simpler if the parser read up to the first \\r\\n, that way we can detect whether it's a short command or a long one?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/20074044/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "superstarchenxin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/22337768", "body": "If new process inherited listen-socket from old one, why do you dup here?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/22337768/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "muayyad-alsadi": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/76193670", "body": "it should be `/etc/sysconfig/nutcracker.rc`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/76193670/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/76196925", "body": "Should have \n\nAfter=network.target\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/76196925/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/76196973", "body": "good to add\n\nAssertFileNotEmpty=/etc/nutcracker/nutcracker.yml\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/76196973/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "mishan": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/25754743", "body": "nit: don't you need three sentinels for quorum in this example?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/25754743/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}}}}