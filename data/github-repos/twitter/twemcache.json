{"_default": {"1": {"run": {"issues": [{"url": "https://api.github.com/repos/twitter/twemcache/issues/33", "title": "build.sh", "body": "Hi @thinkingfish \r\n\r\nOn my laptop, build.sh can't resolve https://artifactory.twitter.biz, maybe it's an internal url.\r\nSo build.sh can't download musl, and the same to libevent-2.0.22-stable.tar.gz\r\n\r\nThanks", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/33/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tfarina": {"issues": [{"url": "https://api.github.com/repos/twitter/twemcache/issues/32", "title": "constantify long_options array?", "body": "https://github.com/twitter/twemcache/blob/82c5d80e66a3c645b09721b3d78ef3aaf83902a8/src/mc.c#L115\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/32/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "abhilashshingane": {"issues": [{"url": "https://api.github.com/repos/twitter/twemcache/issues/27", "title": "memory slab calcification problem.", "body": "Once the object is allocated to some slabs and if the repartition of items size changes, some slabs will  miss space whereas others are full of empty pages.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/27/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/16707654", "body": "add a tool that will monitar a memory usage.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/16707654/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/16708777", "body": "we should have to maintain the free memory area for each slab this will defenately reduced the slab calcification problem.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/16708777/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/16731039", "body": "@manjuraj  sir ,\n\n```\nI have red most of the information about the project and it's code . now I am trying to create a small module for \"creating a memory slab \" . next what should I do ? \n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/16731039/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/17130515", "body": "@manjuraj sir ,\n\n group of requests according to their size and then stored it into the appropriate slab will little bit solve the problem of eviction.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/17130515/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "qbolec": {"issues": [{"url": "https://api.github.com/repos/twitter/twemcache/issues/25", "title": "Add Garbage Collector", "body": "I see that you've invested a lot of time in stackable Eviction Strategies.\nThat's good for me, because that means, that you've already made the first step : admitted that the default LRU is not as good as it could.\n\nPerhaps you would like to incorporate this changeset:\nhttps://groups.google.com/forum/?fromgroups#!topic/memcached/MdNPv0oxhO8\nwhich makes sure that expired items are never in memory, and does so in O(1).\nWe use it in nk.pl since years at it works great (evictions dropped to 0, and monitoring memory consumption provides more information now -- also, slabs now have a chance to become emptied and disposed).\n\nThe only drawback I can see with it is the additional O(1) memory per item for doubly linked list pointers. I believe that Twitter hires some tough hackers which could make this number smaller (how about the trick with XORed pointers?).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/25/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/16013123", "body": "Cool, please let me know if I could help somehow in the process.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/16013123/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "thinkingfish": {"issues": [{"url": "https://api.github.com/repos/twitter/twemcache/issues/22", "title": "Add metrics to track memory/heap consumption", "body": "Learning about the real memory consumption of Twemcache is important to correctly estimate overhead and avoid paging. To many people's surprise, slab memory doesn't account for the entire heap size in many cases, and it would be helpful to have metrics reflecting actual heap size and its composition.\n\nAside from slabs, large memcache instances usually allocate a lot of memory into hashtable(s); and for instances with a lot of connections, connection buffer is also a significant source of memory overhead. So it would be nice to have the following metrics for starters:\n\n``` C\nheap_curr /* total heap size, everything allocated through mc_*alloc */\nheap_hashtable /* size of the current hashtable, and if in transition, hashtables */\nheap_conn /* connection buffer related overhead */\n```\n\nThere are others that could be added, such as slab size (which can currently be computed from slab_curr and slab size), suffix buffer for reply messages, etc. It would be nice to come up a more comprehensive component list, but they probably aren't as important as the above ones.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/22/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commits": [{"url": "https://api.github.com/repos/twitter/twemcache/commits/5bfa26512520fc102be09a694007d60e680dc9d1", "message": "update README.md"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/7c846a16e8eb2930b35c7484720b8ef59125c277", "message": "Twemcache 2.6.3 release"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/82c5d80e66a3c645b09721b3d78ef3aaf83902a8", "message": "twemcache 2.6.2 release"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/a81ed22ddbb7def09b0fd03a61f6caf0521af0ba", "message": "Twemcache 2.6.0 release"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/1d0d8a58df7305406beb8ba68068cb49d63cf980", "message": "Merge pull request #21 from cloudflare/master\n\nTwemcache starts spinning with 100% CPU because of bad UDP packages"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/23b6c0e7647c89d5eb544936660585b4a847b90d", "message": "Twemcache 2.5.3 release"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/a647adfd5bcb7ff0d40c480f9c93ebff931ce179", "message": "Merge pull request #13 from soarpenguin/master\n\nlog message error"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/d1e46db6878772d61fb67052f73092fa97ca5052", "message": "Twemcache 2.5.2 release"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/33226247d45e1ef0868b925a3759d046c560987c", "message": "updating ChangeLog and README.md"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/aa195daef7fffafa2dd490d2b9ad7d4e7cf0abc5", "message": "Twemcache 2.5.1 release"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/9e49007ad7cc865584ceb512ef8e8e2a7f8a212b", "message": "updated ChangeLog"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/d4701e820b8fa6683fc28719644953a865372065", "message": "Twemcache 2.5.0 release ChangeLog update"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/37cfcf4774b481ed5fc1dd20d10803cbc754bb83", "message": "Twemcache 2.5.0 release"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/51456e149084c2259ea9e721976e69e5556bd8ab", "message": "updating README.md to include the new eviction strategy"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/771aaadb969d95aa937faa32bb1289f5676b5e39", "message": "code cleanups"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/151c53bbd969ca392d2673ae850010cc76013b6b", "message": "Merge pull request #10 from twitter/slab_lru\n\nSlab lru: \r\n\r\nEnable slab level lru (-M 4) for twemcache. A global slab lruq is used, a slab's position in the lruq is updated pretty much the same way item lruq is updated, invoked from the same place, except it works on a different target."}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/060a27228c2b77dacfb81cfeea7e2d0dd91740ec", "message": "Merge branch 'github_master' into slab_lru"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/3f1086d25312f83390434e8d6c1e5db05ec89bdc", "message": "updating ascii art, test & lru slab eviction"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/bf68171f23aa8df56c888146dba852387eaa0bb8", "message": "adding unittest for slab lru eviction"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/0fc4f5827b88a3d56aebe8fb5e534f31433abdfc", "message": "updating TODO list"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/f4864cc17b02afb9d27ae881db56df7522750a93", "message": "slab lrq eviction feature branch"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/06257199a561801ef1b4e40f63c8ae6e3fbd9879", "message": "Merge pull request #4 from twitter/github_master\n\nupdated twctop README"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/f43141f23087d54b49c36ee92e3158b9e85e6c49", "message": "updated twctop README"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/e20187e3e81f202a473e2acf9e524a9fb2062e07", "message": "Merge pull request #3 from monadbobo/pu\n\nremove some dead code. (merged with monadbobo/pu)"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6902575", "body": "Good catch!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6902575/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/8441579", "body": "We have evaluated all the changes since memcached 1.4.4 (but before memcached 2.0 as it's a completely different code base) and backported features that we think are useful for Twemcache. Closing this issue now.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/8441579/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/9082124", "body": "I believe your numbers are in the same ballpark as what others have done. And in our own throughput tests, we've observed similar performance between memcached trunk and twemcache.\n\nAddressing the existing bottlenecks (global locks, mostly) is on our roadmap. We've dealt with stats_lock (gotta do the yak-shaving before taking down the beast), but that alone won't prove of much help until we've taken care of the others as well. The next one will be cache_lock. Until then, we expect to have comparable performance with memcached. \n\nPerformance tuning we've done show that 4/8 threads are both reasonable numbers to use, beyond which point contention makes scaling hard. On CPU consumption, we notice that the binary itself does very little computation. Usually the CPU cycles are attributed to sending data over socket (sendmsg()), which is highly workload dependent.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/9082124/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/11275144", "body": "Thanks for pointing out the problem, this bug has been fixed in the 2.5.3 release. Please let us know if you are running into similar problems.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/11275144/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/16005568", "body": "I'll take a look tomorrow and get back to you.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/16005568/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/16005629", "body": "We have given garbage collection some serious thoughts and I agree with you on the benefits it could bring. What stopped us from simply incorporate something similar already was the memory overhead, which is noticeable for smaller items.\n\nI feel quite a few things could be done to make memory management better, unfortunately I have to shelf the development on Twemcache (other than bug fixes) due to lack of resources. Hopefully I can come back to this in the summer (Q3) and do an in-depth summary and planning about the memory management module as a whole.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/16005629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/18799790", "body": "I'm not sure what the actual problem/request is here. Please elaborate.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/18799790/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/18799870", "body": "I think the better approach, as Jamie suggested in the thread, is to use a background, latency oblivious thread to do the clean-ups. If you want to extend the current threading architecture of Twemcache, which uses background threads for various tasks, I'll be happy to review it and merge.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/18799870/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/18879978", "body": "I want to apologize for claiming this being fixed. Apparently it wasn't fixed for all cases and the claim was prematured. Now I've root-caused the bug and tested all cases, a proper fix will come very soon with another release.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/18879978/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/18884246", "body": "This patch helps when \"asc_respond_get\" returns with an error but is not the real reason why leak also happens on a normal path. It's a good observation though. I have generalized the solution to apply to other abnormal return status in the same function and will include the change in the next release. Thanks for pointing this out and I will give you credit in the release note.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/18884246/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/18884269", "body": "Will be fixed in Twemcache 2.6.0\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/18884269/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/18884460", "body": "good observation. Will introduce the change in Twemcache 2.6.0\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/18884460/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/19140813", "body": "Indeed, I've corrected it in my local repo and will push the change later. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/19140813/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/50709935", "body": "A memcached client should work. Such as http://php.net/manual/en/book.memcached.php\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/50709935/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemcache/comments/1897858", "body": "I did. Correcting now.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/comments/1897858/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "seckinsimsek": {"issues": [{"url": "https://api.github.com/repos/twitter/twemcache/issues/12", "title": "Scalability of Twemcache", "body": "Dear Twemcache team,\nI would like to ask about the scalability of Twemcache. What is the maximum throughput you could achieve in terms of concurrent requests per second? and how many cores could you scale up to?\n\nBased on my own experience, I could saturate only two (Xeon) cores with a single twemcache process achieving a maximum throughput of 320K requests per second. My request mix is 95% reads to 5% writes and each request  reads/writes a 1KB record.\n\nIs there any proof that Twemcache can be scaled-up to a higher number of cores? higher number of max concurrent requests?\nCould you please share your experience on the twemcache scalability issues? Are there any known scalability bottlenecks that I should consider in my tuning process?\n\nRegards,\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/12/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "soarpenguin": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemcache/commits/83840f27fa69a549c169cd713af6e1e4a8c6e09c", "message": "format the macro definiton"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/250836244878290dfc1b742e2e2c43a2e7b1a327", "message": "format the macro definiton"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/b97399bed835c2db6c460950c6e5285c4c25d663", "message": "fix macro definition strXcmp error for big endian"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/9639c2899e5bb6aec51adbdefb42b167220464d5", "message": "correct log message"}], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemcache/pulls/23", "title": "Mmstat", "body": "add some metrics for memory statistic. current heap memory usage, current hashtable size and connection buffer usage.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "monadbobo": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemcache/commits/b297dc2b8ab9ecdc9e7b9fb8de7ea21fb70e5b7f", "message": "Fix logical error of code and remove meaningless code."}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/66156e8e1b32cb6a1c414bfa6946dc77f920297d", "message": "remove some dead code."}], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemcache/pulls/8", "title": "Add linux accept4() support", "body": "Add linux accept4() support.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "caniszczyk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemcache/commits/a69d716d715ad13fec9dc7f8dcf739196882bd02", "message": "Update master"}, {"url": "https://api.github.com/repos/twitter/twemcache/commits/27d2f342d0927b3936d32c28c1d7528017566a80", "message": "Update master"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemcache/comments/1897843", "body": "Did you mess up the release date in the ChangeLog?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/comments/1897843/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "baldmountain": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemcache/pulls/11", "title": "Get the 64bit test working on MacOSX", "body": "MacOS X doesn't have /proc/meminfo so use sysctl -a\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dormando": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6888845", "body": "Most relevant, probably:\n\nhttp://code.google.com/p/memcached/wiki/ReleaseNotes1410 (massive performance boost)\nhttp://code.google.com/p/memcached/wiki/ReleaseNotes1411 (slab rebalancing)\n\nhttp://groups.google.com/group/memcached/browse_thread/thread/972a4cf1f2c1b017/d55dcfb56fba9592 (more notes on the performance changes. this version eventually became 1.4.10 instead of 1.4.9 though).\n\nIt's worth noting that I'd also tried something similar to your background thread aggregation, and was able to measure a _loss_ of performance rather than a boost, and disabling the per-thread locks didn't do anything at all. It should still be easy to remove the \"pointless\" per-thread locks and kill some of the few remaining global stats, but as I said above I wasn't able to measure much contention.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6888845/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6895790", "body": "I wasn't asking about justifying the work you did. Every time people release code I jump for joy and click my heels together. The only sadness is in the time scale, since I could've avoided doing some work if we had this stuff sooner.\n\nHere's a bit that's off:\n\n1) You are not the only company with traffic. I regularly tune clusters for top 20 websites. Memcached's own features are designed for scale, and are tested very thoroughly. Every time twitter seems to talk about memcached you have this disbelief that anyone could ever run into the same problems you have. We went through this years ago when you ran 1.2.3 and it was crashing, although I'm told your team was not aware of this.\n\n2) Your blog post is damaging since people will read that as though twitter's branch is the only one with slab reassignment and that it will have better performance (which I'm willing to bet it doesn't, if you haven't modified cache_lock much).\n\nAs for technical responses:\n- When you say \"client triggered/request based\" are you referring to our implementation of slab reassignment? The differences are pretty trivial. My case was designed to be more generic, in that people with evictions across all slabs won't have memory thrown about randomly, and if desired you can pre-move memory or aggressively move memory during a detected shift.\n\nIf your system truly relies on freeing slabs as soon (or perhaps right before) memory fills up, I think it's trivial to get that to happen via an option. I wasn't able to get slab rebalance to not cause microblocking without doing the background thread I did though.\n- The stats issue is confusing to me. The cache_lock granularity was 90% of the issue I ran into while tuning performance last year. I was able to fetch over 6 million keys per second while barely hitting contention with the remaining global locks? Did you not run into that at all, or did you also do similar fixes with cache_lock? I understand that you probably can't share production traffic numbers, but can you share benchmark results via your tools? I have my mc-crusher tool and it should be pretty trivial to compare the two\n\nthanks!\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6895790/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6918551", "body": "@tsuna https://gist.github.com/3088511 I did a really basic read test here. I need to modify my tool more to do set load tests since they dropped binary protocol support, and I have to add \"set with noreply\" tests to mc-crusher. Pre-1.4.10 memc had very poor read performance while mixed with significant sets. 1.4.10+ does very well by comparison.\n\nWhen I was doing the lock scaling and slab rebalance work I had only brief access to a NUMA machine, which showed a near 50% performance drop when spread across nodes (however it still had a 2-3x speedup compared to 1.4.9). I noted this in the 1.4.10/11 release notes. I didn't have enough access until recently to determine the biggest contributors of that drop (http://memcached.org/feedme you can see me unsuccessfully begging here).\n\n@manjuraj Ok, so that tells me how it's different. Slab rebalance in mainline has two background threads, actually: one that moves slab pages between slab classes, and one which observes and runs a crappy algorithm.\n\nIf you read the release notes and related mailing list messages, I went with this methodology:\n- Add commands to move memory from one class to another.\n- Add a rudimentary built-in automover\n- Include a perl script which also implements the automover algorithm, which people can modify for their own needs then submit patches/tunable ideas for the built-in automover.\n\nbecause we're like, a community or something, d'awg.\n\nIn your case you want triggered purges. We still can't do them fully inline because that would block the process and we want deterministic performance. I _can_ round out a few trivial things, then add a flag to allow triggering immediate moves in an almost identical way to yours. I'll use your example test as reference and see if we can match it. Should only take a few hours at most.\n\nThere is an important point here: I wrote the slab rebalancer _after_ cutting the cache_lock up severely. I've seen 6+ million key reads/sec, and 930,000 sets per second. I can make it go faster but wanted to limit how quickly we changed the locks around to limit potential impact to stability.\n\nI took a look through your code, and the slab mover just erases and shuffles memory while holding a global lock and stopping the world. We can't do that in the general case as some users prefer deterministic response times, but we can actually free items in bulk via a flag :) It was much harder to free memory out from under the system when you can't rely on holding the cache_lock _and_ want to preserve zero-copy client writes. I'm not sure yet what you've done with the latter.\n\nTL;DR in a few days I'll poke a few things on our end which should match your slab rebalance method while retaining our high performance.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6918551/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6919592", "body": "@tsuna Box I developed a feature on != the only box this software runs on. I validated it on NUMA but didn't have access to something I could crash without causing someone to pay for remote hands to reboot it.\n\nThat and NUMA tuning was basically a separate problem back then and would've taken a lot longer to code around. I made it 2-3x faster on NUMA and almost 9x faster on non-NUMA, so I released it. :)\n\nEDIT: Also to point out that if you have two-socket NUMA, starting two instances and binding them gave 100% performance, but a single instance was always overwhelming the network after my tuning. Needs 10ge to hit a bottleneck now\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6919592/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6998208", "body": "Haven't had a lot of hacking time this weekend, but I spent a few hours rearranging code and got this:\n\nhttps://gist.github.com/3120414\n\nIt's slightly different from twemcache in that it triggers a slab page move on an eviction, instead of moving memory at the time of an eviction. In practice that's completely negligable. Feel free to take the branch and beat on it a bit...\n\nI have a few patches to merge and want to give a whack at removing all the uncontested stats locks again and see how that changes on a NUMA box, then this'll turn into 1.4.14. I'm not sure how practical the automove=2 option is at all, since most of the clusters I've seen are just out of memory across the board and instead need something more like the less aggressive automover (which itself still needs more runtime options).\n\nHowever, in order to make it aggressive I've fixed a few legitimate issues I had promised to fix back in february anyway, so thanks for pushing on that :)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6998208/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7018979", "body": "I'm not sure what caused those errors yet. I'm sure it can be more aggressive, but given that my test ended up with 64 slabs in the 100k test, I think if you look at how many evictions each test had we're still much better than twemcache...\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7018979/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7019173", "body": "I don't mean to say that from a \"well if you look at THIS side of the bench...\" viewpoint. the original test wasn't so great. In designing this feature you're attempting to optimize the hit rate by minimizing the eviction rate. In this set only test you can only measure how many slab pages were moved and how many items got evicted.\n\nIn another test, you could load up 100,000 items, then set another 100,000 items into another slab class, then try to fetch the latter 100,000 items and see what the hit rate _may_ be\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7019173/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7019366", "body": "To contrive a test that my branch will lose:\n\nSet 100,000 items into class 3, so that memcached is fully out of slab pages to assign.\n\nSet 100,000 new items into class 2 with automove=2. Such that there should be more than enough slab pages to hold all 100,000 items. Then fetch all those items back. In that test my branch should not end up with 100,000 items available in class 2, but yours should be a lot closer, depending on how well the random selection works.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7019366/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7020255", "body": "(apologies for the spam, you made me curious about those errors :P)\n\nI took a verbose log of the run (doing each mcperf in order then recording the full log of the final run). The errors are \"out of memory storing object\", and almost all happen at the beginning of the session.\n\nThis is because you can't actually fit 10 100k items into a single slab page, you can fit 9 due to header overhead. So the first 9 items are being set, and the 10th connection spins on OOM errors as the other 9 are still uploading the 100k item. Since we store uploads directly into the memory assigned to an item, they appear \"locked\" and can't be evicted. Since it's not an eviction my little code didn't fire at first.\n\nAs a test I added a rebalance trigger to the OOM condition and that dropped the errors from 67 to 25. Still occassionally the item in the bottom LRU is locked due to upload/whatever and it OOM's. I'm actually fine with this, as it doesn't affect production folks very often.\n\nI did remove the item depth search as that turned out to be pointless and slow, however after talking with another user about a month back it seems like it's a good idea to add a short search back in that ticks up only if the bottom items are in use (being fetched or written to), but otherwise not searching for expired items. As is you can cause this OOM condition if you have very few pages in a very large slab class, but even when people hit it, it's extremely rare.\n\nWe can't avoid it entirely without doing tradeoffs I don't want... like buffering an item into temporary memory then copying it in or something. In order for twemcache to not error out on that you must've completely disabled zero copy reads/writes...\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7020255/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7022815", "body": "Even if you don't use a background thread, it should be impossible for you to enter this scenario without clobbering memory somewhere... but I think I see what's happening...\n- In your final test, you ended up with 54 pages into the 100k slab, but in order to minimize evictions you would've had to have 64. If you have 10 clients uploading into 9 available slab chunks, and you were not able to assign a new page via a random fetch, where does the extra memory come from? Though your slab distribution is slightly different, and you can fit 10 items in class #32.\n\n(answering my own question again: looks like twemcache will actually reassign a page from its own slab class?)\n\nAh, it looks like you're doing this by adding a refcount for the _entire_ slab class! (slab_acquire_refcount). Then you skip evicting a page from anything with a refcount incremented.\n\nSo if I fetch hard against all slabs on twemcache, it'll never be able to move pages around? Since at hardly any point in time each slab class with pages in it would be idle. It looks like if you hit this condition you return a NOMEM as well. If you're actually hitting twemcache fairly hard, shouldn't this be spewing NOMEM's instead of rebalancing pages? I guess if you suddenly set hard against another class you'll still hit enough times where other classes are fully idle...\n\nIf you look at mc-crusher's conf/ directory, you'll see \"slab_rebal_torture[1-4]\", which load up gets and sets on slab classes 1-14. On my desktop it'll be doing 1.4 million key fetches/sec while doing 400,000 sets per second. Then I run the \"slab-tosser\" script to throw memory between each class at random. Then to validate I run this for 12 hours... It found a few crashes and problems during development.\n\nI have a feeling twemcache would have a hard time with that test (which is similar to a highly loaded production box), wherein memcached is able to move slab pages arbitrarily regardless of read load. This is why we do it in the background. As I've been splitting the cache_lock and allowing higher and higher fetch rates, this becomes a more important feature. You'll run into similar issues when you look at splitting cache_lock as well.\n\nI can cause some server_error's on twemcache by using an even bigger item size. Perhaps we can make that better on our end, but I'm still not really convinced it matters for those large slabs, and our error rate should go down more by adding a search crawl on encountering a locked item.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7022815/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7023685", "body": "Ah, gotcha. Sorry for getting hung up on terminology in the code. You have a refcount per slab page, then randomly look for available pages within the entire pool. That'll scale a bit better for sure, though I still cringe at holding the cache_lock for so long.\n\nIf you have a higher percentage of pages in the slab class receiving a set, doesn't that end up mostly just evicting items randomly out of its own class? In that case I want to pull just one item from the bottom of the LRU in that class.\n\nOr is there something deeper going on about preserving a page distribution across classes despite pressure increasing in one area?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7023685/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "manjuraj": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6893300", "body": "twemcache essentially was built to make memcached suitable, robust and manageable for the large scale production environment at twitter. \n\nslab eviction changes have been running in our production since nov '10 and has gone through a lot of real world testing for our traffic; also because of the scale at which we operate, doing a client triggered, request-based slab eviction is not ideal for us.\n\nthe lock-less stat implementation have been running since may '11 and per-thread stats local-buffer is actually a precursor to any future changes that we would make cache_lock fine grained\n\nklog ends up being extremely useful to us in doing in doing in-depth analysis on the cached data access pattern across our cache cluster\n\nfinally, as we mentioned in the blog post, our initial goal was make memcached work well for us and in the long term, we look forward to sharing our code and ideas with the memcached community at large and push it upstream.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6893300/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6915544", "body": "Slab reassignment aka random eviction in twemcache is on-demand, that enables cache to adapt immediately to changing access patterns. We don't use a background thread because it does not give us the deterministic behavior that we would like in our use case. For comparison with memcached see: https://github.com/twitter/twemcache/blob/master/notes/random_eviction.md\n\nTo reiterate the stats feature, the thread-local stats is a precursor to any changes that would make cache_lock fine-grained.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6915544/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7018896", "body": "alan, the (100K size and 10 call case) still has 67 errors out of 1000 requests. I believe you might want to tune your background 'automove' algorithm to be more aggressive so that it can keep up with the request rate.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7018896/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7019969", "body": "server errors signify that 67 out of 1000 'set' requests failed and didn't make it to the cache\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7019969/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7021830", "body": "we didn't disable zero copy in twemcache, and just like memcached we copy directly into the item struct carved out of slab; the reason we don't hit this in twemcache is because we don't use a background thread and our eviction is inline\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7021830/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7023249", "body": "refcount are on slab not slabclass; refcount on slab implies that the slab is being used by some other thread; this is necessary to support zero copy into item structs\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7023249/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7081782", "body": "In random eviction, all slabs across all slab classes are treated as equal class citizens and the the algorithm for choosing the candidate slab is O(1). Random eviction works well when data access pattern is changing and not predetermined. For a LRU like access pattern, we have a 'LRU slab eviction' available in 2.5.0\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/7081782/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/11520016", "body": "please read the readme:\n\n```\nTo build twemcache from source with debug logs enabled and assertions disabled:\n\n$ git clone git@github.com:twitter/twemcache.git\n$ cd twemcache\n$ autoreconf -fvi\n$ ./configure --enable-debug=log\n$ make V=1\n$ src/twemcache -h\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/11520016/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/16709358", "body": "See twctop in scripts folder \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/16709358/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}], "commit_comments": [], "review_comments": []}, "tsuna": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6914997", "body": "Is there a performance comparison of the latest Memcached against Twemcache?\n\n@dormando re: \"This is probably untrue with a NUMA machine\" \u2013 you don't have one to test?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6914997/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6919130", "body": "@dormando I was surprised because you said you regularly tune clusters for top 20 websites, and I can't imagine that at least 10 of them don't run on NUMA architectures.  You can get a dual-Sandy Bridge with 64GB of RAM for around $3000 these days.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/6919130/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "thanhcnn2000": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/10657272", "body": "I get the same problem that I set -m to 512 but the real resident memory size of the process is (viewed by ps command) increasing time by time after it passes limit 512\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/10657272/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/10714316", "body": "Thanks you for your supporting us. We are looking forward to next release. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/10714316/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "sceoy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/10691241", "body": "it is a bug brought by 2.5.2. There is a memory leak when they switch the cas_suffix strategy (bcoz initially it will only alloc memory when return_cas so as free cache, now it will alloc memory any way but the free strategy keep the same). You could use 2.5.1 for a tricky fix. Or change the mc_connection.c and mc_core.c  to make it release the suffix cache in all cases.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/10691241/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dknecht": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/11691702", "body": "Still had issue with stock memcached so closing this issue. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/11691702/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "thedrow": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/50528965", "body": "It is likely that this PR would be accepted if it had tests.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemcache/issues/comments/50528965/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}}}}