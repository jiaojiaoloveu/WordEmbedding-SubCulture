{"_default": {"1": {"anuscool": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3393", "title": "library loading error with rocksdb5.9.2", "body": "i am using fusesource rocksdbjni which uses rocksdb library. it is working fine with rocksdb fb3.0 version but i am getting could not load library when i am using with rocksdb5.9.2.\r\n\r\ni am compiling rocksdb with make static_lib command for both version which produces static library....is there any change of static library generation in both version.\r\n\r\nTestSuite$1.warning Exception in constructor: testOpen (java.lang.UnsatisfiedLinkError: Could not load library. Reasons: [no rocksdbjni64-99-master-SNAPSHOT in java.library.path, no rocksdbjni-99-master-SNAPSHOT in java.library.path, no rocksdbjni in java.library.path, /tmp/librocksdbjni-64-99-master-SNAPSHOT-8396258567915380494.so: /tmp/librocksdbjni-64-99-master-SNAPSHOT-8396258567915380494.so: undefined symbol: _ZTIN7rocksdb6LoggerE]\r\n\tat org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:187)\r\n\tat org.fusesource.hawtjni.runtime.Library.load(Library.java:143)\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3393/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3287", "title": "librocksdb.a error", "body": "librocksdb.a(db_impl.o): relocation R_X86_64_32S against `_ZTVN7rocksdb12SnapshotImplE' can not be used when making a shared object; recompile with -fPIC\r\n[INFO] /home/anupam/Arm_Project/project_oss/old_rocksdb/librocksdb.a: error adding symbols: Bad value\r\n\r\ncan any one help me to resolve this issue?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3287/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nvanbenschoten": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3391", "title": "IngestExternalFile with either DeleteRange support or an \"ingest_in_front\" option", "body": "I began implementing this myself and got far enough to realize that it should be discussed in an issue beforehand.\r\n\r\nI'll start with the motivation. As described in [this comment](https://www.facebook.com/groups/rocksdb.dev/permalink/1508823992549443/?comment_id=1513125508785958), there are instances in CockroachDB where we'd like to be able to ingest a series of SST files atomically and have the ingestion clear out all overlapping data. Currently, this is not possible because `SstFileWriter` only supports `Put`, `Merge`, and `Delete` operations. Given a range of keys that we'd like to completely replace using `IngestExternalFile`, our only real option at the moment is to lock the range of keys using some external locking mechanism, iterate over the key range and clear out each key, call `IngestExternalFile`, then unlock the range. The need for external locking while we iterate over the entire range so that new keys aren't written underneath us isn't ideal. Even worse, doing this two-step process means that we're susceptible to state corruption in the presence of untimely crashes. We could avoid this second issue by creating new SST files with deletion tombstones for all existing keys and copies of all operations from the original set of SST files (keeping everything ordered, of course), but this still requires the external locking and a scan over the entire range, and now means that we're doubling the number of SST files. \r\n\r\nIdeally, we'd be able to specify the range of keys that should be subsumed by each new SST file, so that a single call to `IngestExternalFile` would atomically ingest all keys in the file and delete any overlapping keys. There are two ways I have thought about allowing this.\r\n\r\nFirst, I looked into adding `DeleteRange` support to `SstFileWriter`. While doing so, I came to the conclusion that this would only be useful if the `DeleteRange` was able to overlap other keys but given a \"lower\" precedence. If this was not the case then any user trying to do what we're doing would need to add a `DeleteRange` between ever pair of subsequent keys, which I expect would be bad for a number of reasons. Luckily, it looks like `BlockBasedTableBuilder` already stores `DeleteRange` operations in [their own meta block](https://github.com/facebook/rocksdb/blob/6f5ba0bf5bcb763c656743078c05fd3868f290dd/table/block_based_table_builder.cc#L431), so the requirement to add all keys to the `SstFileWriter` in order should not be an issue. It also looks like `RangeDelAggregator` already gives point operations with the [same sequence number](https://github.com/facebook/rocksdb/blob/6f5ba0bf5bcb763c656743078c05fd3868f290dd/db/range_del_aggregator.cc#L140) as a `DeleteRange` operation priority, so we will get the behavior we want without any extra changes (please note, I'm new to the RocksDB codebase, so it's likely some of these assumptions are misguided). With these issues out of the way, most of the work here has to do with correctly setting the `ExternalSstFileInfo`  and handling it correctly in `ExternalSstFileIngestionJob`. The biggest problem that sticks out to me is that the key range spanned by `ExternalSstFileInfo` is `[smallest_key, largest_key]` (inclusive upper bound). This doesn't work well with `DeleteRange's` `[start, end)` bounds. I'm guessing this has already been solved somewhere else, but solving it here will require some work.\r\n\r\nThis prototype got me thinking about alternative approaches. The general idea here seems useful enough operation that it might justify first-class support. For instance, I can imagine an `ingest_in_front` `IngestExternalFileOptions` option that parallels the current `ingest_behind` option. This could perform the task of ensuring that all keys that overlap an ingested SST's bounds are deleted. By handling this in `ExternalSstFileIngestionJob`, I think this could be done a lot more efficiently than by relying on `DeleteRange` alone. For instance, with this option, we could avoid flushing any parts of the memtable that overlap the SST. `ExternalSstFileIngestionJob` could also employ `DeleteFilesInRange` to make most of the deletes more efficient.\r\n\r\nI'd appreciate any input or advice people more familiar with this can give. @ajkr and @IslamAbdelRahman, it looks like you two are the experts here :)", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3391/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3392", "title": "Fix WriteBatch rep_ format for RangeDeletion records", "body": "This is a small amount of general cleanup I made while experimenting with https://github.com/facebook/rocksdb/issues/3391.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xuy98": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3388", "title": "The ordinal 43 could not be located in the dynamic link library Canibet.dll", "body": "I was able to built 64bit Windows RocksDB and tried to run test on Windows 7 Enterprise 64bit SP1 (according Windows Update it's up-to-date)\r\n\r\n### Expected behavior\r\nIt should run on Windows 7 64bit.\r\n\r\n### Actual behavior\r\nRan into error \"The ordinal 43 could not be located in the dynamic link library Canibet.dll\"\r\n\r\n### Steps to reproduce the behavior\r\nJust run any test in RocksDB.\r\n\r\nHere is the version of Cabinet.dll on my system: 6.1.7601.17514\r\n\r\nRan dumpbin on Cabinet.dll:\r\n\r\n         4    0 000057F2 DeleteExtractedFiles\r\n         2    1 00005188 DllGetVersion\r\n         3    2 00005741 Extract\r\n        11    3 00008CD4 FCIAddFile\r\n        10    4 00008E91 FCICreate\r\n        14    5 00008E46 FCIDestroy\r\n        13    6 00008DB8 FCIFlushCabinet\r\n        12    7 00008E16 FCIFlushFolder\r\n        22    8 00001849 FDICopy\r\n        20    9 00001C3F FDICreate\r\n        23    A 00001693 FDIDestroy\r\n        21    B 000059BD FDIIsCabinet\r\n        24    C 00005A60 FDITruncateCabinet\r\n         1    D 000046D5 GetDllVersion\r\n\r\nNo 43. What's function 43?\r\n\r\nWhere can I get Cabinet.dll used by RocksDB?\r\n\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3388/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "egor-agafonov": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3367", "title": "windows 10, vs 2017: error C2220: warning treated as error - no 'object' file generated", "body": "Hi, I followed installation instructions from CMakeLists.txt:\r\n- installed GFLAGS, SNAPPY, LZ4 and ZLIB\r\n- edited file thirdparty.inc accordingly \r\ncmake -G \"Visual Studio 15 Win64\" -DWITH_GFLAGS=1 -DWITH_SNAPPY=1 -DWITH_LZ4=1 -DWITH_ZLIB=1 ..\r\nmsbuild rocksdb.sln /p:Configuration=Release\r\nGot problem: \r\n....\\util\\compression.h(61): error C2220: warning treated as error - no 'object' file generated [...\\build\\rocksdb.vcxproj]\r\n\r\nCan anybody help me please?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3367/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lwjyqjykn": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3355", "title": "RocksDB do not well in parallel with write/read and close", "body": "We use rocksdbjni for write/read and close in parallel, then coredump, follow are the coredump infos, is anyone meet this problem, how do you solve it.\r\n\r\n```\r\n A fatal error has been detected by the Java Runtime Environment:\r\n\r\n EXCEPTION_ACCESS_VIOLATION (0xc0000005) at pc=0x000007fedcc553cd, pid=4904, tid=4416\r\n\r\n JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27)\r\n Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode windows-amd64 compressed oops)\r\n C  [librocksdbjni6164768070392737012.dll+0x2f53cd]  rocksdb::InlineSkipList<rocksdb::MemTableRep::KeyComparator const & __ptr64>::KeyIsAfterNode+0x7d\r\n\r\n Failed to write core dump. Minidumps are not enabled by default on client versions of Windows\r\n\r\n If you would like to submit a bug report, please visit:\r\n   http://bugreport.java.com/bugreport/crash.jsp\r\n The crash happened outside the Java Virtual Machine in native code.\r\n See problematic frame for where to report the bug.\r\n```\r\n\r\n\r\n```\r\n A fatal error has been detected by the Java Runtime Environment:\r\n\r\n  EXCEPTION_ACCESS_VIOLATION (0xc0000005) at pc=0x000007fedcf9b1ea, pid=5284, tid=5112\r\n\r\n JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27)\r\n Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode windows-amd64 compressed oops)\r\n Problematic frame:\r\n C  [librocksdbjni1393172717064025021.dll+0x2fb1ea]  rocksdb::InlineSkipList<rocksdb::MemTableRep::KeyComparator const & __ptr64>::Insert<0>+0xb4a\r\n\r\n Failed to write core dump. Minidumps are not enabled by default on client versions of Windows\r\n\r\n If you would like to submit a bug report, please visit:\r\n   http://bugreport.java.com/bugreport/crash.jsp\r\n The crash happened outside the Java Virtual Machine in native code.\r\n See problematic frame for where to report the bug.\r\n```\r\n\r\n\r\n```\r\n A fatal error has been detected by the Java Runtime Environment:\r\n\r\n  EXCEPTION_ACCESS_VIOLATION (0xc0000005) at pc=0x000007feeb70da64, pid=5728, tid=3372\r\n\r\n JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27)\r\n Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode windows-amd64 compressed oops)\r\n Problematic frame:\r\n C  [librocksdbjni4086279910858052465.dll+0x4fda64]  rocksdb::MemTable::Add+0x374\r\n\r\n Failed to write core dump. Minidumps are not enabled by default on client versions of Windows\r\n\r\n If you would like to submit a bug report, please visit:\r\n   http://bugreport.java.com/bugreport/crash.jsp\r\n The crash happened outside the Java Virtual Machine in native code.\r\n See problematic frame for where to report the bug.\r\n```\r\n\r\n\r\n```\r\n A fatal error has been detected by the Java Runtime Environment:\r\n\r\n  EXCEPTION_ACCESS_VIOLATION (0xc0000005) at pc=0x000007feeb60aa38, pid=5976, tid=4400\r\n\r\n Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode windows-amd64 compressed oops)\r\n Problematic frame:\r\n C  [librocksdbjni3909728769303213122.dll+0x3faa38]  std::unique_ptr<rocksdb::ColumnFamilySet,std::default_delete<rocksdb::ColumnFamilySet> >::get+0x28\r\n\r\n Failed to write core dump. Minidumps are not enabled by default on client versions of Windows\r\n\r\n If you would like to submit a bug report, please visit:\r\n   http://bugreport.java.com/bugreport/crash.jsp\r\n The crash happened outside the Java Virtual Machine in native code.\r\n See problematic frame for where to report the bug.\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3355/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3320", "title": "OS is corrput for stoping rocksdb when compaction", "body": "We do a massive of read-modify-write with rokcsdbjava, after serveral hours, invoke another thread to stop the rocksdb instance without stoping read-modify-write progress, then OS is corrput. And there is a disk full of writing, we cann't do anything in this server for serveral hours. We check rocksdb LOG, only discover \"Shutdown: canceling all background work\", then some compaction log infos, but we don't find \"Shutdown complete\" info, is it means rocksdb doesn't stop successfully, and if this will trigger brush write and OS corrupt.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3320/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sagar0": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3347", "title": "Test failures on PowerPC arch", "body": "@adamretter alerted us that `make check` is failing on PPC64le (powerpc 64-bit little-endian) architecture on RocksDB master HEAD. On investigating further, I noticed a couple of tests have been failing since RocksDB 5.7 release. \r\nThe two specific tests that are failing are: \r\n- FullBloomTest.FullVaryingLengths\r\n- DBPropertiesTest.AggregatedTablePropertiesAtLevel\r\n\r\nI narrowed the issue down to be due to ccf5f08f882038e8b481fafa0a0c0b1a04f6a390 introduced in #2579 . The tests work without this commit and fail with this commit included. \r\n\r\n### Steps to reproduce the behavior\r\nEither\r\n`make check`\r\nOR\r\n```\r\nmake bloom_test db_properties_test\r\nTMP_DIR=/dev/shm ./bloom_test --gtest_filter=FullBloomTest.FullVaryingLengths\r\nTMP_DIR=/dev/shm ./db_properties_test --gtest_filter=DBPropertiesTest.AggregatedTablePropertiesAtLevel\r\n```\r\nHere's the output:\r\n\r\n```\r\nubuntu@hortonworks-2:/tmp/rdb/rocksdb$ make bloom_test db_properties_test -j8\r\n...\r\n...\r\nubuntu@hortonworks-2:/tmp/rdb/rocksdb$ TMP_DIR=/dev/shm ./bloom_test --gtest_filter=FullBloomTest.FullVaryingLengths\r\nNote: Google Test filter = FullBloomTest.FullVaryingLengths\r\n[==========] Running 1 test from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from FullBloomTest\r\n[ RUN      ] FullBloomTest.FullVaryingLengths\r\nFalse positives:  0.00% @ length =      1 ; bytes =    133\r\nFalse positives:  0.00% @ length =      2 ; bytes =    133\r\nFalse positives:  0.00% @ length =      3 ; bytes =    133\r\nFalse positives:  0.00% @ length =      4 ; bytes =    133\r\nFalse positives:  0.00% @ length =      5 ; bytes =    133\r\nFalse positives:  0.00% @ length =      6 ; bytes =    133\r\nFalse positives:  0.00% @ length =      7 ; bytes =    133\r\nFalse positives:  0.00% @ length =      8 ; bytes =    133\r\nFalse positives:  0.00% @ length =      9 ; bytes =    133\r\nFalse positives:  0.00% @ length =     10 ; bytes =    133\r\nFalse positives:  0.00% @ length =     20 ; bytes =    133\r\nFalse positives:  0.00% @ length =     30 ; bytes =    133\r\nFalse positives:  0.02% @ length =     40 ; bytes =    133\r\nFalse positives:  0.04% @ length =     50 ; bytes =    133\r\nFalse positives:  0.15% @ length =     60 ; bytes =    133\r\nFalse positives:  0.27% @ length =     70 ; bytes =    133\r\nFalse positives:  0.59% @ length =     80 ; bytes =    133\r\nFalse positives:  0.83% @ length =     90 ; bytes =    133\r\nFalse positives:  1.17% @ length =    100 ; bytes =    133\r\nutil/bloom_test.cc:287: Failure\r\nExpected: (FilterSize()) <= ((size_t)((length * 10 / 8) + 128 + 5)), actual: 389 vs 383\r\n200\r\n[  FAILED  ] FullBloomTest.FullVaryingLengths (8 ms)\r\n[----------] 1 test from FullBloomTest (8 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 1 test from 1 test case ran. (8 ms total)\r\n[  PASSED  ] 0 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] FullBloomTest.FullVaryingLengths\r\n\r\n 1 FAILED TEST\r\n\r\n\r\nubuntu@hortonworks-2:/tmp/rdb/rocksdb$ TMP_DIR=/dev/shm ./db_properties_test --gtest_filter=DBPropertiesTest.AggregatedTablePropertiesAtLevel\r\nNote: Google Test filter = DBPropertiesTest.AggregatedTablePropertiesAtLevel\r\n[==========] Running 1 test from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from DBPropertiesTest\r\n[ RUN      ] DBPropertiesTest.AggregatedTablePropertiesAtLevel\r\ndb/db_properties_test.cc:200: Failure\r\nExpected: (static_cast<double>(dbl_a - dbl_b) / (dbl_a + dbl_b)) < (bias), actual: 0.599198 vs 0.5\r\n...\r\n```\r\n\r\nArch and OS details on the machine I have access to shows:\r\n```\r\nubuntu@hortonworks-2:/tmp/rdb/rocksdb$ uname -a\r\nLinux hortonworks-2 4.4.0-101-generic #124-Ubuntu SMP Fri Nov 10 18:29:11 UTC 2017 ppc64le ppc64le ppc64le GNU/Linux\r\nubuntu@hortonworks-2:/tmp/rdb/rocksdb$\r\nubuntu@hortonworks-2:/tmp/rdb/rocksdb$ cat /proc/cpuinfo\r\nprocessor       : 0\r\ncpu             : POWER8E (raw), altivec supported\r\nclock           : 3425.000000MHz\r\nrevision        : 2.1 (pvr 004b 0201)\r\n...\r\ntimebase        : 512000000\r\nplatform        : pSeries\r\nmodel           : IBM pSeries (emulated by qemu)\r\nmachine         : CHRP IBM pSeries (emulated by qemu)\r\n```\r\n  ", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3347/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2915", "title": "[RocksJava] Support SstFileManager API in Java to control sst file deletion rate", "body": "We should expose `SstFileManager` API in Java to enable controlled deletion of sst files. ", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2915/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2874", "title": "Improve readahead in RocksDB", "body": "Readahead in RocksDB can be configured by setting `DBOptions.advise_random_on_open = false` and by providing a reasonable value to `ReadOptions.readahead_size` as mentioned here: https://github.com/facebook/rocksdb/wiki/Iterator#read-ahead . But RocksDB's read-ahead implementation incurs considerable overhead and is suitable only for a select few use-cases. RocksDB should optimize its read-ahead implementation, especially by making prefetch more lightweight in BlockBasedTableReader.\r\n\r\nThis could improve MyRocks performance too in comparison to other storage engines. See https://github.com/facebook/mysql-5.6/issues/705 . ", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2874/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2282", "title": "Support MergeOperator in Java", "body": "[`MergeOperator`](https://github.com/facebook/rocksdb/blob/master/include/rocksdb/merge_operator.h) functionality is not currently available in the [Java API](https://github.com/facebook/rocksdb/blob/master/java/src/main/java/org/rocksdb/MergeOperator.java). [`StringAppendOperator`](https://github.com/facebook/rocksdb/blob/master/java/src/main/java/org/rocksdb/StringAppendOperator.java) is the only implementation that is exposed as part of Java API today. There has been considerable interest from the community recently to expose full functionality of MergeOperator so that they can take advantage of it in Java as well (Ex: https://github.com/facebook/rocksdb/issues/1988 , https://github.com/facebook/rocksdb/pull/2289)\r\n\r\ncc: @adamretter ", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2282/reactions", "total_count": 10, "+1": 10, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/68829ed89cec64186557dc0860fc693c118ff1c6", "message": "Revert Snappy version upgrade\n\nSummary:\nJava static builds are again broken, this time due Snappy version upgrade introduced in 90c1d81975a03b0f8b352ddc614fbc99c2496ddd (#3331).\n\nThis is due to two reasons:\n1. The new Snappy packages should now be downloaded from https://github.com/google/snappy/archive/<pkg.tar.gz> instead of https://github.com/google/snappy/releases/download/<pkg.tar.gz> which we are using now.\n1. In addition to the the above URL change, Snappy changed its build from using autotools to CMake based : https://github.com/google/snappy/blame/e69d9f880677f2aa3488c80b953ec4309f0dfa2e/README.md#L65-L72\n\nSo more changes are needed if we are going to upgrade to 1.1.7. Hence reverting the version upgrade until we figure them out.\nCloses https://github.com/facebook/rocksdb/pull/3363\n\nDifferential Revision: D6716983\n\nPulled By: sagar0\n\nfbshipit-source-id: f451a1bc5eb0bb090f4da07bc3e5ba72cf89aefa"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/e446d14093283f5eec4057901a08669cad46865c", "message": "Fix PowerPC dynamic java build\n\nSummary:\nJava build on PPC64le has been broken since a few months, due to #2716. Fixing it with the least amount of changes.\n(We should cleanup a little around this code when time permits).\n\nThis should fix the build failures seen in http://140.211.168.68:8080/job/Rocksdb/ .\nCloses https://github.com/facebook/rocksdb/pull/3359\n\nDifferential Revision: D6712938\n\nPulled By: sagar0\n\nfbshipit-source-id: 3046e8f072180693de2af4762934ec1ace309ca4"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/677f249d6d388780658a79732545b77a276f8e07", "message": "Fix Travis build failures in CMake RocksJava\n\nSummary:\nFixed RocksJava travis build failure due to a missing file in java/CMakeLists.txt. (from #3332).\nCloses https://github.com/facebook/rocksdb/pull/3344\n\nDifferential Revision: D6686472\n\nPulled By: sagar0\n\nfbshipit-source-id: dd3281dff1342c3a7235c402890420aa56db0fe3"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/3e955fad09840397c4e010f0daa21be5a48873cd", "message": "Fix zstd/zdict include path for java static build\n\nSummary:\nWith the ZSTD dictionary generator support added in #3057\n`PORTABLE=1 ROCKSDB_NO_FBCODE=1 make rocksdbjavastatic` fails as it can't find zdict.h. Specifically due to:\nhttps://github.com/facebook/rocksdb/blob/e3a06f12d27fd50af7b6c5941973f529601f9a3e/util/compression.h#L39\nIn java static builds zstd code gets directly downloaded from https://github.com/facebook/zstd , and in there zdict.h is under dictBuilder directory. So, I modified libzstd.a target to use `make install` to collect all the header files into a single location and used that as the zstd's include path.\nCloses https://github.com/facebook/rocksdb/pull/3260\n\nDifferential Revision: D6669850\n\nPulled By: sagar0\n\nfbshipit-source-id: f8a7562a670e5aed4c4fb6034a921697590d7285"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/bbef8c3884d273b82d20fcc48477ae8b524bee7b", "message": "Log GetCurrentTime failures during Flush and Compaction\n\nSummary:\n`GetCurrentTime()` is used to populate `creation_time` table property during flushes and compactions. It is safe to ignore `GetCurrentTime()` failures here but they should be logged.\n\n(Note that `creation_time` property was introduced as part of TTL-based FIFO compaction in #2480.)\n\nTes Plan:\n`make check`\nCloses https://github.com/facebook/rocksdb/pull/3231\n\nDifferential Revision: D6501935\n\nPulled By: sagar0\n\nfbshipit-source-id: 376adcf4ab801d3a43ec4453894b9a10909c8eb6"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/d51fcb21f4e0c5b46cb93c16b1a799c04147a5e4", "message": "Blob DB: Add db_bench options\n\nSummary:\nAdding more BlobDB db_bench options which are needed for benchmarking.\nCloses https://github.com/facebook/rocksdb/pull/3230\n\nDifferential Revision: D6500711\n\nPulled By: sagar0\n\nfbshipit-source-id: 91d63122905854ef7c9148a0235568719146e6c5"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/8954f830a0a870a8a79c1ce2025d796b962bbf26", "message": "Blob DB: db_bench flag to control BlobDB's garbage collection\n\nSummary:\nflag: blob_db_enable_gc, to control BlobDb's enable_garbage_collection.\nCloses https://github.com/facebook/rocksdb/pull/3190\n\nDifferential Revision: D6383395\n\nPulled By: sagar0\n\nfbshipit-source-id: 4134e835150748c425b8187264273a54c6d8381c"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/578f36e431a1e7453ef3a84cde48864a0cf4c288", "message": "Blob DB: Remove some redundant log lines\n\nSummary:\nSaw some redundant log lines when trying to benchmark blob db. So, removed the lines from blob_file.cc, and let the lines in blob_db_impl.cc take the lead.\nCloses https://github.com/facebook/rocksdb/pull/3189\n\nDifferential Revision: D6381726\n\nPulled By: sagar0\n\nfbshipit-source-id: 5f0b1e56fe4bc3b715d89ea9b5749bd935cd0606"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/a6d8e30c05ac1a2cd4bfa9aa7d95a498b02b354c", "message": "Remove unnecessary status check in TableCache::NewIterator\n\nSummary:\nWhile investigating the usage of `new_table_iterator_nanos` perf counter, I saw some code was wrapper around with unnecessary status check ... so removed it.\nCloses https://github.com/facebook/rocksdb/pull/3120\n\nDifferential Revision: D6229181\n\nPulled By: sagar0\n\nfbshipit-source-id: f8a44fe67f5a05df94553fdb233b21e54e88cc34"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/76c3fbd6510374e8eff83259dc51e094015d4b31", "message": "Add Memtable Read Tier to RocksJava\n\nSummary:\nThis options was introduced in the C++ API in #1953 .\nCloses https://github.com/facebook/rocksdb/pull/3064\n\nDifferential Revision: D6139010\n\nPulled By: sagar0\n\nfbshipit-source-id: 164de11d539d174cf3afe7cd40e667049f44b0bc"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/25ac1697b4c2197b84e9a0acd6d652cb0e617f9e", "message": "Blob DB: Evict oldest blob file when close to blob db size limit\n\nSummary:\nEvict oldest blob file and put it in obsolete_files list when close to blob db size limit. The file will be delete when the `DeleteObsoleteFiles` background job runs next time.\nFor now I set `kEvictOldestFileAtSize` constant, which controls when to evict the oldest file, at 90%. It could be tweaked or made into an option if really needed; I didn't want to expose it as an option pre-maturely as there are already too many :) .\nCloses https://github.com/facebook/rocksdb/pull/3094\n\nDifferential Revision: D6187340\n\nPulled By: sagar0\n\nfbshipit-source-id: 687f8262101b9301bf964b94025a2fe9d8573421"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/f5078dde2d46c09a3d81f114b658a5a1926d517e", "message": "Blob DB: Initialize all fields in Blob Header, Footer and Record structs\n\nSummary:\nFixing un-itializations caught by valgrind.\nCloses https://github.com/facebook/rocksdb/pull/3103\n\nDifferential Revision: D6200195\n\nPulled By: sagar0\n\nfbshipit-source-id: bf35a3fb03eb1d308e4c5ce30dee1e345d7b03b3"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/96e3a600ba28fedc6cb0be7f65d6aa1a42b7a30a", "message": "Return write error on reaching blob dir size limit\n\nSummary:\nI found that we continue accepting writes even when the blob db goes beyond the configured blob directory size limit. Now, we return an error for writes on reaching `blob_dir_size` limit and if `is_fifo` is set to false. (We cannot just drop any file when `is_fifo` is true.)\n\nDeleting the oldest file when `is_fifo` is true will be handled in a later PR.\nCloses https://github.com/facebook/rocksdb/pull/3060\n\nDifferential Revision: D6136156\n\nPulled By: sagar0\n\nfbshipit-source-id: 2f11cb3f2eedfa94524fbfa2613dd64bfad7a23c"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/a02ed12638e0a68de6e5cb4e440c10eaafe93367", "message": "Exclude DBTest.DynamicFIFOCompactionOptions test under RocksDB Lite\n\nSummary:\nThis test shouldn't be enabled under the lite version; and this fixes the failing contrun test due to #3006.\nCloses https://github.com/facebook/rocksdb/pull/3056\n\nDifferential Revision: D6114681\n\nPulled By: sagar0\n\nfbshipit-source-id: dc5243549ae6b1353cec7edb820c771d95f66dda"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/f0804db7f722b0a3bb03f7392d96c5118e6235da", "message": "Make FIFO compaction options dynamically configurable\n\nSummary:\nColumnFamilyOptions::compaction_options_fifo and all its sub-fields can be set dynamically now.\n\nSome of the ways in which the fifo compaction options can be set are:\n- `SetOptions({{\"compaction_options_fifo\", \"{max_table_files_size=1024}\"}})`\n- `SetOptions({{\"compaction_options_fifo\", \"{ttl=600;}\"}})`\n- `SetOptions({{\"compaction_options_fifo\", \"{max_table_files_size=1024;ttl=600;}\"}})`\n- `SetOptions({{\"compaction_options_fifo\", \"{max_table_files_size=51;ttl=49;allow_compaction=true;}\"}})`\n\nMost of the code has been made generic enough so that it could be reused later to make universal options (and other such nested defined-types) dynamic with very few lines of parsing/serializing code changes.\nIntroduced a few new functions like `ParseStruct`, `SerializeStruct` and `GetStringFromStruct`.\nThe duplicate code in `GetStringFromDBOptions` and `GetStringFromColumnFamilyOptions` has been moved into `GetStringFromStruct`. So they become just simple wrappers now.\nCloses https://github.com/facebook/rocksdb/pull/3006\n\nDifferential Revision: D6058619\n\nPulled By: sagar0\n\nfbshipit-source-id: 1e8f78b3374ca5249bb4f3be8a6d3bb4cbc52f92"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/b74999458f1f2a974435f748c8c51628dd8134d8", "message": "Update RocksDB Authors File\n\nSummary: Update RocksDB Authors File.\n\nReviewed By: yiwu-arbug\n\nDifferential Revision: D6075453\n\nfbshipit-source-id: dff52f483aab33c41de391f145a8273acfd6cbde"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/cc67b22d6e231b1018c881a662801d2001848fec", "message": "Add OptionsUtil class to java/CMakeLists.txt\n\nSummary:\nAdding OptionsUtil java class and options_util.cc to java/CMakeLists.txt, which were missed accidentally when they were introduced in #2898.\nCloses https://github.com/facebook/rocksdb/pull/2985\n\nDifferential Revision: D6015878\n\nPulled By: sagar0\n\nfbshipit-source-id: 1abbd46db4aebad1e07ea53523eacbdcb12823e1"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/5a38e18627d42bd79fedd7a7e0180799397d8469", "message": "Make some WriteOptions defaults more explicit\n\nSummary:\nSome WriteOptions defaults were not clearly documented. So, added comments to make the defaults more explicit.\nCloses https://github.com/facebook/rocksdb/pull/2984\n\nDifferential Revision: D6014500\n\nPulled By: sagar0\n\nfbshipit-source-id: a28078818e335e42b303c1fc6fbfec692ed16c7c"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/da29eba43b17b70c1b2d2bd89a6d16bfcf6bb573", "message": "Enable WAL for blob index\n\nSummary:\nEnabled WAL, during GC, for blob index which is stored on regular RocksDB.\nCloses https://github.com/facebook/rocksdb/pull/2975\n\nDifferential Revision: D5997384\n\nPulled By: sagar0\n\nfbshipit-source-id: b76c1487d8b5be0e36c55e8d77ffe3d37d63d85b"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/377e00404875d8785836af57717459505710e4dc", "message": "Fix DBOptionsTest.SetBytesPerSync test when run with no compression\n\nSummary:\nAlso made the test more easier to understand:\n- changed the value size to ~1MB.\n- switched to NoCompression. We don't anyway need compression in this test for dynamic options.\n\nThe test failures started happening starting from: #2893 .\nCloses https://github.com/facebook/rocksdb/pull/2957\n\nDifferential Revision: D5959392\n\nPulled By: sagar0\n\nfbshipit-source-id: 2d55641e429246328bc6d10fcb9ef540d6ce07da"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/bb38cd03a97718f053bcfd9dbdadd3c419e977e6", "message": "Limit number of merge operands in Cassandra merge operator\n\nSummary:\nNow that RocksDB supports conditional merging during point lookups (introduced in #2923), Cassandra value merge operator can be updated to pass in a limit. The limit needs to be passed in from the Cassandra code.\nCloses https://github.com/facebook/rocksdb/pull/2947\n\nDifferential Revision: D5938454\n\nPulled By: sagar0\n\nfbshipit-source-id: d64a72d53170d8cf202b53bd648475c3952f7d7f"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/93c2b91740afa1667963e3c020b354ae5d6705eb", "message": "Introduce conditional merge-operator invocation in point lookups\n\nSummary:\nFor every merge operand encountered for a key in the read path we now have the ability to decide whether to look further (to retrieve more merge operands for the key) or stop and invoke the merge operator to return the value. The user needs to override `ShouldMerge()` method with a condition to terminate search when true to avail this facility.\n\nThis has a couple of advantages:\n1. It helps in limiting the number of merge operands that are looked at to compute a value as part of a user Get operation.\n2. It allows to peek at a merge key-value to see if further merge operands need to look at.\n\nExample: Limiting the number of merge operands that are looked at: Lets say you have 10 merge operands for a key spread over various levels. If you only want RocksDB to look at the latest two merge operands instead of all 10 to compute the value, it is now possible with this PR. You can set the condition in `ShouldMerge()` to return true when the size of the operand list is 2. Look at the example implementation in the unit test. Without this PR, a Get might look at all the 10 merge operands in different levels before invoking the merge-operator.\n\nAdded a new unit test.\nMade sure that there is no perf regression by running benchmarks.\n\nCommand line to Load data:\n```\nTEST_TMPDIR=/dev/shm ./db_bench --benchmarks=\"mergerandom\" --merge_operator=\"uint64add\" --num=10000000\n...\nmergerandom  :      12.861 micros/op 77757 ops/sec;    8.6 MB/s ( updates:10000000)\n```\n\n**ReadRandomMergeRandom bechmark results:**\nCommand line:\n```\nTEST_TMPDIR=/dev/shm ./db_bench --benchmarks=\"readrandommergerandom\" --merge_operator=\"uint64add\" --num=10000000\n```\n\nBase -- Without this code change (on commit fc7476b):\n```\nreadrandommergerandom :      38.586 micros/op 25916 ops/sec; (reads:3001599 merges:6998401 total:10000000 hits:842235 maxlength:8)\n```\n\nWith this code change:\n```\nreadrandommergerandom :      38.653 micros/op 25870 ops/sec; (reads:3001599 merges:6998401 total:10000000 hits:842235 maxlength:8)\n```\nCloses https://github.com/facebook/rocksdb/pull/2923\n\nDifferential Revision: D5898239\n\nPulled By: sagar0\n\nfbshipit-source-id: daefa325019f77968639a75c851d46352c2303ef"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/0806801dc848a219dba3ea21ed1442348abf12fd", "message": "DestroyDB API\n\nSummary:\nExpose DestroyDB API in RocksJava.\nCloses https://github.com/facebook/rocksdb/pull/2934\n\nDifferential Revision: D5914775\n\nPulled By: sagar0\n\nfbshipit-source-id: 84af6ea0d2bccdcfb9fe8c07b2f87373f0d5bab6"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/c8f3606731dcb8ddc3b6e2b1c8f33c62a5aad1ce", "message": "Expose LoadLatestOptions, LoadOptionsFromFile and GetLatestOptionsFileName APIs in RocksJava\n\nSummary:\nJNI wrappers for LoadLatestOptions, LoadOptionsFromFile and GetLatestOptionsFileName APIs.\nCloses https://github.com/facebook/rocksdb/pull/2898\n\nDifferential Revision: D5857934\n\nPulled By: sagar0\n\nfbshipit-source-id: 68b79e83eab8de9416e3f1fef73e11cf7947e90a"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/96a13b4f4b67333fbb0db6814c885486064437f3", "message": "Use jemalloc in rocksdbjni library built via vagrant\n\nSummary:\nProblem:\nDuring RocksJava performance testing we found that the rocksdb jni library is not built with jemalloc; instead it was getting built with the default glibc malloc. We saw quite a bit of memory bloat due to this.\n\nAddressed this by installing jemalloc-devel package in the vm that we use to build release jars.\nCloses https://github.com/facebook/rocksdb/pull/2916\n\nDifferential Revision: D5887018\n\nPulled By: sagar0\n\nfbshipit-source-id: ace0b5d60234b3a30dcd5d39633e7827a5982a50"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/3fc08fa88e8608078cc013251509c7d79a14dd79", "message": "Expose max_background_jobs option in RocksJava\n\nSummary:\nThis option was introduced in the C++ API in RocksDB 5.6 in bb01c1880c0c89a6cf338e22fd54e7e25b7d12ba . Now, exposing it through RocksJava API.\nCloses https://github.com/facebook/rocksdb/pull/2908\n\nDifferential Revision: D5864224\n\nPulled By: sagar0\n\nfbshipit-source-id: 140aa55dcf74b14e4d11219d996735c7fdddf513"}], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3282", "title": "Improve performance of long range scans with readahead", "body": "This change improves the performance of iterators doing long range scans (e.g. big/full table scans in MyRocks) by using readahead and prefetching additional data on each disk IO. This prefetching is automatically enabled on noticing more than 2 IOs for the same table file during iteration. The readahead size starts with 8KB and is exponentially increased on each additional sequential IO, up to a max of 256 KB. This helps in cutting down the number of IOs needed to complete the range scan. \r\n\r\nConstraints:\r\n- The prefetched data is stored by the OS in page cache. So this currently works only for non direct-reads use-cases i.e applications which use page cache. (Direct-I/O support will be enabled in a later PR). \r\n- This gets currently enabled only when ReadOptions.readahead_size = 0 (which is the default value).\r\n\r\nThanks to @siying for the original idea and implementation.\r\n\r\n**Benchmarks:**\r\nData fill:\r\n```\r\nTEST_TMPDIR=/data/users/$USER/benchmarks/iter ./db_bench -benchmarks=fillrandom -num=1000000000 -compression_type=\"none\" -level_compaction_dynamic_level_bytes\r\n```\r\nDo a long range scan: Seekrandom with large number of nexts\r\n```\r\nTEST_TMPDIR=/data/users/$USER/benchmarks/iter ./db_bench -benchmarks=seekrandom -duration=60 -num=1000000000 -use_existing_db -seek_nexts=10000 -statistics -histogram\r\n```\r\n\r\nPage cache was cleared before each experiment with the command:\r\n```\r\nsudo sh -c \"echo 3 > /proc/sys/vm/drop_caches\"\r\n```\r\n```\r\nBefore:\r\nseekrandom   :   34020.945 micros/op 29 ops/sec;   32.5 MB/s (1636 of 1999 found)\r\nWith this change:\r\nseekrandom   :    8726.912 micros/op 114 ops/sec;  126.8 MB/s (5702 of 6999 found)\r\n```\r\n~3.9X performance improvement.\r\n\r\nAlso verified with strace and gdb that the readahead size is increasing as expected. \r\n```\r\nstrace -e readahead -f -T -t -p <db_bench process pid>\r\n```", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/2782", "title": "Compaction Filter counters", "body": "Add counters to track the usage of compaction filter's various decisions.\r\n\r\nTest Plan:\r\nupdated compaction filter unit tests.\r\nmake check\r\nmake jtest", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/21680190", "body": "Fixed in #2109 .", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21680190/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "javeme": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3336", "title": "support StringAppendOperator(delim_char) constructor in java-api", "body": "### Expected behavior\r\nwe should support `new StringAppendOperator(delim_char)` constructor in java-api, which is already supported in [c++ implementation](https://github.com/facebook/rocksdb/blob/72502cf2270db7323d447cc7a504dbea251d432a/utilities/merge_operators/string_append/stringappend.cc#L19).\r\n\r\n### Actual behavior\r\nwe can only use [`new StringAppendOperator()`](https://github.com/facebook/rocksdb/blob/3c327ac2d0fd50bbd82fe1f1af5de909dad769e6/java/src/main/java/org/rocksdb/StringAppendOperator.java#L14) constructor without parameter.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3336/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3337", "title": "Support StringAppendOperator(delimiter_char) constructor in java-api", "body": "Fixes #3336", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xiaoweizw": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3326", "title": "rocksdbjni coredump at rocksdb::WriteThread::EnterAsBatchGroupLeader()", "body": "we have used rocksdbjni, but when process batch write, generator coredump due to abort signal. \r\nHas anyone meet this problem and how should this problem be solved? Thank you very much.\r\ncoredump detail information:\r\nProgram terminated with signal 6, Aborted.\r\n(gdb) bt\r\n#0  0x00002b17cbbf8625 in raise () from /lib64/libc.so.6\r\n#1  0x00002b17cbbf9e05 in abort () from /lib64/libc.so.6\r\n#2  0x00002b17cc8810b5 in os::abort(bool) ()\r\n   from /usr/java/jdk1.8.0_121/jre/lib/amd64/server/libjvm.so\r\n#3  0x00002b17cca23443 in VMError::report_and_die() ()\r\n   from /usr/java/jdk1.8.0_121/jre/lib/amd64/server/libjvm.so\r\n#4  0x00002b17cc8865bf in JVM_handle_linux_signal ()\r\n   from /usr/java/jdk1.8.0_121/jre/lib/amd64/server/libjvm.so\r\n#5  0x00002b17cc87cb03 in signalHandler(int, siginfo*, void*) ()\r\n   from /usr/java/jdk1.8.0_121/jre/lib/amd64/server/libjvm.so\r\n#6  <signal handler called>\r\n#7  0x00002b1806881a95 in rocksdb::WriteThread::EnterAsBatchGroupLeader(rocksdb::WriteThread::Writer*, rocksdb::WriteThread::Writer**, rocksdb::autovector<rocksdb::WriteThread::Writer*, 8ul>*) ()\r\n   from /tmp/librocksdbjni6424814419910142735.so\r\n#8  0x00002b18067de00f in rocksdb::DBImpl::WriteImpl(rocksdb::WriteOptions const&, rocksdb::WriteBatch*, rocksdb::WriteCallback*, unsigned long*, unsigned long, bool) ()\r\n   from /tmp/librocksdbjni6424814419910142735.so\r\n#9  0x00002b18067df9eb in rocksdb::DBImpl::Write(rocksdb::WriteOptions const&, rocksdb::WriteBatch*) () from /tmp/librocksdbjni6424814419910142735.so\r\n#10 0x00002b1806758a3f in Java_org_rocksdb_RocksDB_write0 ()\r\n   from /tmp/librocksdbjni6424814419910142735.so\r\n \r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3326/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wolfkdy": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3318", "title": "MemoryLeak&Potential Deadlock", "body": "https://github.com/facebook/rocksdb/blob/b5c99cc908a7e34fa65d588b2706c33000786935/utilities/transactions/pessimistic_transaction_db.cc#L115\r\n\r\nIt should be wrapped by a unique_ptr<>\r\nDestructor is not called, so potential row-lock ( or key-lock\uff09may be leaked", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3318/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gcsideal": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3315", "title": "PPC 64 bit BigEndian ASM compilation fault", "body": "### Expected behavior\r\nSuccessful compilation on ppc64 as happened with the 5.8.8 version.\r\n\r\n### Actual behavior\r\nAs of RocksDB 5.9.2 and with the previously used GCC 7.2 compilation fails with:\r\n  CC       util/crc32c_ppc_asm.o\r\nutil/crc32c_ppc_asm.S: Assembler messages:\r\nutil/crc32c_ppc_asm.S:109: Error: invalid operands (*UND* and *ABS* sections) for `&'\r\nutil/crc32c_ppc_asm.S:109: Error: invalid operands (*UND* and *ABS* sections) for `&'\r\n[...]\r\nutil/crc32c_ppc_asm.S:88: Error: unsupported relocation against v20\r\nutil/crc32c_ppc_asm.S:89: Error: unsupported relocation against v21\r\n[...]\r\nutil/crc32c_ppc_asm.S:745: Error: unsupported relocation against v20\r\nMakefile:1812: recipe for target 'util/crc32c_ppc_asm.o' failed\r\n\r\n### Steps to reproduce the behavior\r\nTry to compile on a ppc64 machine.\r\nFull build log: https://buildd.debian.org/status/fetch.php?pkg=rocksdb&arch=ppc64&ver=5.9.2-1&stamp=1514504777&raw=0", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3315/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kl3eo": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3314", "title": "Slowdown in read after ~ 15 million records in DB", "body": "The manner we did the test is following: writing 1 mln. (key:value) pairs (time checked) - each key is bigint, value is ~200bytes, - then reading them back to csv format (time checked). Then adding other 1 million records, reading 2 million records, etc. The fact is until we reached 15 million records, the time necessary for reading grew proportionally with the growing of the number of records to read. After that, there was a serious non-linearity, so the time grew much more with the number of millions of records to read. I would not be much surprised if the linearity stopped at 100 mln. records, but 10-15 million seems rather low. Am I missing something important in setup, or this is normal and well-known behaviour of RocksDb?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3314/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "publicocean0": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3311", "title": "error compiling last rocksdb source in ubuntu ", "body": "\r\n\r\n\r\n\r\n### Actual behavior\r\n```\r\nIn file included from /usr/include/c++/6/memory:81:0,\r\n                 from ./java/./rocksjni/comparatorjnicallback.h:13,\r\n                 from java/rocksjni/comparatorjnicallback.cc:9:\r\n/usr/include/c++/6/bits/unique_ptr.h: In instantiation of \u2018void std::unique_ptr<_Tp [], _Dp>::reset(_Up) [with _Up = char*; <template-parameter-2-2> = void; _Tp = const char; _Dp = std::default_delete<const char []>]\u2019:\r\n/usr/include/c++/6/bits/unique_ptr.h:539:9:   required from \u2018typename std::enable_if<std::__and_<std::__and_<std::is_array<_Up>, std::is_same<typename std::unique_ptr<_Tp [], _Dp>::_Pointer::type, _Tp*>, std::is_same<typename std::unique_ptr<_Up, _Ep>::pointer, typename std::unique_ptr<_Up, _Ep>::element_type*>, std::is_convertible<typename std::unique_ptr<_Up, _Ep>::element_type (*)[], _Tp (*)[]>, std::__or_<std::__and_<std::is_reference<_Dp>, std::is_same<_T2, _U2> >, std::__and_<std::__not_<std::is_reference<_Dp> >, std::is_convertible<_Ep, _Dp> > > >, std::is_assignable<_T2&, _U2&&> >::value, std::unique_ptr<_Tp [], _Dp>&>::type std::unique_ptr<_Tp [], _Dp>::operator=(std::unique_ptr<_Up, _Ep>&&) [with _Up = char []; _Ep = std::default_delete<char []>; _Tp = const char; _Dp = std::default_delete<const char []>; typename std::enable_if<std::__and_<std::__and_<std::is_array<_Up>, std::is_same<typename std::unique_ptr<_Tp [], _Dp>::_Pointer::type, _Tp*>, std::is_same<typename std::unique_ptr<_Up, _Ep>::pointer, typename std::unique_ptr<_Up, _Ep>::element_type*>, std::is_convertible<typename std::unique_ptr<_Up, _Ep>::element_type (*)[], _Tp (*)[]>, std::__or_<std::__and_<std::is_reference<_Dp>, std::is_same<_T2, _U2> >, std::__and_<std::__not_<std::is_reference<_Dp> >, std::is_convertible<_Ep, _Dp> > > >, std::is_assignable<_T2&, _U2&&> >::value, std::unique_ptr<_Tp [], _Dp>&>::type = std::unique_ptr<const char []>&]\u2019\r\njava/rocksjni/comparatorjnicallback.cc:34:21:   required from here\r\n/usr/include/c++/6/bits/unique_ptr.h:614:6: error: no matching function for call to \u2018swap(const char*&, char*&)\u2019\r\n  swap(std::get<0>(_M_t), __p);\r\n\r\n```\r\n### Steps to reproduce the behavior\r\nmake -j8 rocksdbjava\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3311/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3369", "title": "JNI transactions  is working.", "body": "hi, \r\nI merged and fixed compilation problems for extending transations in JNI starting from adamretter code. Actually it seams to work correctly ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zengqingfu1442": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3310", "title": "LZ4 not supported or corrupted LZ4 compressed block contents", "body": "The lz4 compression can't work in my rocksbd now.\r\n[dzeng@dzeng rocksdb-33efdde8316759f1efc6a2a228c5efcf01f09cbc]$ ./db_sanity_test /mnt/sfx-card-root/dfs/dn/disk/rocksdb573/ create\r\nCreating...\r\nBasic -- OK\r\nSpecialComparator -- OK\r\nZlibCompression -- OK\r\nZlibCompressionVersion2 -- OK\r\nLZ4Compression -- Corruption: LZ4 not supported or corrupted LZ4 compressed block contents\r\nFAIL\r\nLZ4HCCompression -- Corruption: LZ4HC not supported or corrupted LZ4HC compressed block contents\r\nFAIL\r\nZSTDCompression -- OK\r\nCSSZlibCompression -- mypid = 7029, dev_name = /dev/sfx0, dev_num = 0\r\nOK\r\nCSSZlibCompressionVersion2 -- OK\r\nPlainTable -- OK\r\nBloomFilter -- OK\r\nThe sst file of lz4 can't be generated under LZ4Compression dir.\r\nFrom the LOG under LZ4Compression, I can see such error:\r\n 2017/12/27-17:29:08.114337 7fa3c1ccfa00 [db/version_set.cc:2969] Recovered from manifest file:/mnt/sfx-card-root/dfs/dn/disk/rocksdb573/LZ4Compression/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0\r\n 2017/12/27-17:29:08.114344 7fa3c1ccfa00 [db/version_set.cc:2977] Column family [default] (ID 0), log number is 0\r\n 2017/12/27-17:29:08.115443 7fa3c1ccfa00 [db/db_impl_open.cc:1111] DB pointer 0x220adb0\r\n 2017/12/27-17:29:13.279864 7fa3c1ccfa00 [db/db_impl_write.cc:1128] [default] New memtable created with log file: #6. Immutable memtables: 0.\r\n 2017/12/27-17:29:13.279921 7fa3b5ef5700 [db/db_impl_compaction_flush.cc:50] [JOB 2] Syncing log #3\r\n 2017/12/27-17:29:13.304067 7fa3b5ef5700 (Original Log Time 2017/12/27-17:29:13.279910) [db/db_impl_compaction_flush.cc:1216] Calling FlushMemTableToOutputFile with column family [default], flush slots available 1, compaction slots available 1, flush slots scheduled 1, compaction slots scheduled 0\r\n 2017/12/27-17:29:13.304075 7fa3b5ef5700 [db/flush_job.cc:264] [default] [JOB 2] Flushing memtable with next log file: 6\r\n 2017/12/27-17:29:13.304106 7fa3b5ef5700 EVENT_LOG_v1 {\"time_micros\": 1514366953304093, \"job\": 2, \"event\": \"flush_started\", \"num_memtables\": 1, \"num_entries\": 1000000, \"num_deletes\": 0, \"memory_usage\": 42651544}\r\n 2017/12/27-17:29:13.304113 7fa3b5ef5700 [db/flush_job.cc:293] [default] [JOB 2] Level-0 flush table #7: started\r\n 2017/12/27-17:29:13.702675 7fa3b5ef5700 [db/flush_job.cc:324] [default] [JOB 2] Level-0 flush table #7: 13976059 bytes Corruption: LZ4 not supported or corrupted LZ4 compressed block contents\r\n**2017/12/27-17:29:13.702897 7fa3b5ef5700 [ERROR] [db/db_impl_compaction_flush.cc:1255] Waiting after background flush error: Corruption: LZ4 not supported or corrupted LZ4 compressed block contentsAccumulated background error counts: 1**\r\n2017/12/27-17:29:13.702903 7fa3b5ef5700 (Original Log Time 2017/12/27-17:29:13.702885) EVENT_LOG_v1 {\"time_micros\": 1514366953702877, \"job\": 2, \"event\": \"flush_finished\", \"lsm_state\": [0, 0, 0, 0, 0, 0, 0], \"immutable_memtables\": 1}\r\n2017/12/27-17:29:13.702934 7fa3c1ccfa00 [db/db_impl.cc:222] Shutdown: canceling all background work\r\n2017/12/27-17:29:14.705500 7fa3c1ccfa00 [db/db_impl.cc:348] Shutdown complete\r\n\r\n\r\nBut I can scan the key-values of lz4 by ldb tool.\r\n[dzeng@dzeng rocksdb-33efdde8316759f1efc6a2a228c5efcf01f09cbc]$ ./ldb --db=/mnt/sfx-card-root/dfs/dn/disk/rocksdb573/LZ4Compression scan | less\r\nkey0 : value0\r\nkey1 : value1\r\nkey10 : value10\r\nkey100 : value100\r\nkey1000 : value1000\r\nkey10000 : value10000\r\nkey100000 : value100000\r\n..\r\n...", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3310/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3306", "title": "How to understand the compaction levels?", "body": "How to understand the compaction levels?https://github.com/facebook/rocksdb/wiki/Leveled-Compaction\r\nIf level_compaction_dynamic_level_bytes=false\r\nFor example, if max_bytes_for_level_base = 123456, max_bytes_for_level_multiplier = 10 and max_bytes_for_level_multiplier_additional is not set, then size of L1, L2, L3 and L4 will be 16384, 163840, 1638400, and 16384000, respectively.\r\n\r\nBut L1's target will be max_bytes_for_level_base. And then Target_Size(Ln+1) = Target_Size(Ln) * max_bytes_for_level_multiplier * max_bytes_for_level_multiplier_additional[n]. max_bytes_for_level_multiplier_additional is by default all 1.\r\n\r\nWhy is the size of L1 not 123456?\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3306/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3221", "title": "Rocksdb can't support LZ4 compression", "body": "I have installed these on my system with yum,\r\n[dzeng@dzeng rocksdb-master]$ rpm -qa | grep lz4\r\nlz4-1.7.3-1.el7.x86_64\r\nlz4-devel-1.7.3-1.el7.x86_64\r\n[dzeng@dzeng rocksdb-master]$ rpm -qa | grep zstd\r\nlibzstd-devel-1.3.1-1.el7.x86_64\r\nlibzstd-1.3.1-1.el7.x86_64\r\nzstd-1.3.1-1.el7.x86_64\r\n[dzeng@dzeng rocksdb-master]$ rpm -qa | grep snappy\r\nsnappy-devel-1.1.0-3.el7.x86_64\r\nsnappy-1.1.0-3.el7.x86_64\r\n[dzeng@dzeng rocksdb-master]$ rpm -qa | grep zlib\r\nzlib-1.2.7-17.el7.x86_64\r\nzlib-1.2.7-17.el7.i686\r\nzlib-devel-1.2.7-17.el7.x86_64\r\n\r\nThen I compile rocksdb using \"make release\", the tool db_sanity_test can also link to the so file,\r\n[dzeng@dzeng rocksdb-master]$ ldd db_sanity_test\r\nlinux-vdso.so.1 => (0x00007ffe8ffe8000)\r\nlibpthread.so.0 => /usr/lib64/libpthread.so.0 (0x00007f8107418000)\r\nlibrt.so.1 => /usr/lib64/librt.so.1 (0x00007f810720f000)\r\nlibsnappy.so.1 => /usr/lib64/libsnappy.so.1 (0x00007f8107009000)\r\nlibgflags.so.2.1 => /usr/lib64/libgflags.so.2.1 (0x00007f8106de8000)\r\nlibz.so.1 => /usr/lib64/libz.so.1 (0x00007f8106bd1000)\r\nlibcssz.so => /usr/lib64/libcssz.so (0x00007f8106929000)\r\nlibbz2.so.1 => /usr/lib64/libbz2.so.1 (0x00007f8106719000)\r\nliblz4.so.1 => /usr/lib64/liblz4.so.1 (0x00007f8106505000)\r\nlibzstd.so.1 => /usr/lib64/libzstd.so.1 (0x00007f81062a6000)\r\nlibnuma.so.1 => /usr/lib64/libnuma.so.1 (0x00007f810609a000)\r\nlibtbb.so.2 => /usr/lib64/libtbb.so.2 (0x00007f8105e64000)\r\nlibstdc++.so.6 => /usr/lib64/libstdc++.so.6 (0x00007f8105b5c000)\r\nlibm.so.6 => /usr/lib64/libm.so.6 (0x00007f810585a000)\r\nlibgcc_s.so.1 => /usr/lib64/libgcc_s.so.1 (0x00007f8105643000)\r\nlibc.so.6 => /usr/lib64/libc.so.6 (0x00007f8105280000)\r\n/lib64/ld-linux-x86-64.so.2 (0x0000561a0cb8c000)\r\nlibdl.so.2 => /usr/lib64/libdl.so.2 (0x00007f810507b000)\r\n\r\nBut it can't support lz4 compression,\r\n[dzeng@dzeng rocksdb-master]$ ./db_sanity_test /mnt/sfx-card-root/rocksdb570/ create\r\nCreating...\r\nBasic -- OK\r\nSpecialComparator -- OK\r\nZlibCompression -- OK\r\nZlibCompressionVersion2 -- OK\r\nCSSZlibCompression -- OK\r\nCSSZlibCompressionVersion2 -- OK\r\nLZ4Compression -- Corruption: LZ4 not supported or corrupted LZ4 compressed block contents\r\nFAIL\r\nLZ4HCCompression -- Corruption: LZ4HC not supported or corrupted LZ4HC compressed block contents\r\nFAIL\r\nZSTDCompression -- OK\r\nPlainTable -- OK\r\nBloomFilter -- OK\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3221/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sjmind": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3307", "title": "UnsatisfiedLinkError when RocksDB.loadLibrary()", "body": "```\r\npackage a;\r\n\r\nimport org.rocksdb.RocksDB;\r\n\r\n/**\r\n * @author z72069\r\n */\r\npublic class Main {\r\n    public static void main(String[] args) {\r\n        RocksDB.loadLibrary();\r\n    }\r\n}\r\n```\r\n\r\n```\r\nException in thread \"main\" java.lang.UnsatisfiedLinkError: C:\\Users\\z72069\\AppData\\Local\\Temp\\librocksdbjni1453993472684985674.dll: P\u52cb\u0002\r\n\tat java.lang.ClassLoader$NativeLibrary.load(Native Method)\r\n\tat java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941)\r\n\tat java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824)\r\n\tat java.lang.Runtime.load0(Runtime.java:809)\r\n\tat java.lang.System.load(System.java:1086)\r\n\tat org.rocksdb.NativeLibraryLoader.loadLibraryFromJar(NativeLibraryLoader.java:78)\r\n\tat org.rocksdb.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:56)\r\n\tat org.rocksdb.RocksDB.loadLibrary(RocksDB.java:64)\r\n\tat org.rocksdb.RocksDB.<clinit>(RocksDB.java:35)\r\n\tat a.Main.main(Main.java:20)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n```\r\n\r\nIt's different from #1715 , VC Redist 2015 x64 had been installed.\r\n![ss171225](https://user-images.githubusercontent.com/11039107/34337771-4b893634-e99d-11e7-89f4-6ca7d72eee78.png)\r\n\r\nVersion 5.5.1, 5.7.3, 5.8.6 have the problem. Version 5.0.1 is ok.\r\n\r\nOS: Windows 7(6.1.7601)\r\nJDK: jdk7u80, jdk8u121\r\nVC redist 2015 x64, VC redist 2008, .Net Framework 4.0,4.5 installed.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3307/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zilti": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3305", "title": "Making it work for those poor NetBSD users out there: max_align_t", "body": "Hi! This is mostly a heads-up. I tried to compile it on NetBSD and stumbled over `max_align_t`. used in util/arena.h. The solution is to add this to the util/arena.h file:\r\n\r\n```c\r\n/* Some platforms lack max_align_t.  The check for _GCC_MAX_ALIGN_T is\r\n   a hack in case the configure-time test was done with g++ even though\r\n   we are currently compiling with gcc.  */\r\n#if ! (0 || defined _GCC_MAX_ALIGN_T)\r\n/* On the x86, the maximum storage alignment of double, long, etc. is 4,\r\n   but GCC's C11 ABI for x86 says that max_align_t has an alignment of 8,\r\n   and the C11 standard allows this.  Work around this problem by\r\n   using __alignof__ (which returns 8 for double) rather than _Alignof\r\n   (which returns 4), and align each union member accordingly.  */\r\n# ifdef __GNUC__\r\n#  define _GL_STDDEF_ALIGNAS(type) \\\r\n     __attribute__ ((__aligned__ (__alignof__ (type))))\r\n# else\r\n#  define _GL_STDDEF_ALIGNAS(type) /* */\r\n# endif\r\ntypedef union\r\n{\r\n  char *__p _GL_STDDEF_ALIGNAS (char *);\r\n  double __d _GL_STDDEF_ALIGNAS (double);\r\n  long double __ld _GL_STDDEF_ALIGNAS (long double);\r\n  long int __i _GL_STDDEF_ALIGNAS (long int);\r\n} max_align_t;\r\n#endif\r\n```\r\n\r\nsomewhere before the reference to `max_align_t`. Feel free to copy this snippet into your code and use it under your license; no need to mention me anywhere. I don't claim any rights on it.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3305/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zhangjinpeng1987": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3291", "title": "calculate scores failed", "body": "When i set level_compaction_dynamic_level_bytes on for an existed DB, and then turn off it again, this will cause calculating scores failed. Look at the statistics in log, the scores for L3 L4 L5 are inf.\r\n```\r\n** Compaction Stats [write] **\r\nLevel    Files   Size     Score \r\n----------------------------\r\n  L0      3/0    6.92 MB   0.8   \r\n  L1     65/0   510.06 MB   1.0 \r\n  L2     98/0   509.38 MB   inf  \r\n  L3    695/0    4.89 GB   inf      \r\n  L4   6727/0   49.07 GB   inf    \r\n  L5  12541/0   57.89 GB   inf    \r\n  L6    224/0   804.98 MB   0.0     \r\n```", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3291/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/ffacaaa3ea4c9ab62114ee72eaa300ed61ae65dd", "message": "fix Seek with lower_bound\n\nSummary:\nWhen Seek a key less than `lower_bound`, should return `lower_bound`.\najkr PTAL\nCloses https://github.com/facebook/rocksdb/pull/3199\n\nDifferential Revision: D6421126\n\nPulled By: ajkr\n\nfbshipit-source-id: a06c825830573e0040630704f6bcb3f7f48626f7"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/966b32b57c673774ea677fe3a40d255e6c8916ba", "message": "fix delete range bug\n\nSummary:\nFix this [issue](https://github.com/facebook/rocksdb/issues/2989).\najkr PTAL\n\nClose #2989\nCloses https://github.com/facebook/rocksdb/pull/3017\n\nDifferential Revision: D6078541\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: ef3db87b37b9156f83ca468aa39dea1f6dbde49d"}], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3139", "title": "expose SstFileReader", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3068", "title": "support set global seqno for external file", "body": "Before ingesting external file, we usually need to verify the checksum of the external file to guarantee the file is not corrupted. \r\nWhen ingesting fail before version change has been added to MANIFEST, but after global_seqno has been set with `move_files = true and allow_global_seqno = true` mode(actually use hard link instead of move), if we restart the ingesting process, checksum verify will fail because of the global_seqno has been changed. So we need to reset the global sequence number to zero before verify.\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/2534", "title": "Improve L0->L0 compaction", "body": "In my opinion, L0->L0 compaction is used to reduce small sst files count in L0 that making unreasonable stall harder to reach. But current L0->L0 compaction mechanism:\r\n1) May generate a [big sst file](https://github.com/facebook/rocksdb/issues/2530). This will cause a very big L0->L1 compaction job which need a long time to finish.\r\n2) Simply compact [0,n) sst files of L0.\r\n\r\nThis pr:\r\n1) Only compact small sst files;\r\n2) Add a size limitation for L0->L0 compaction;\r\n3) Can compact [n, m) sst files of L0.\r\n\r\n@ajkr PTAL\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/23241976", "body": "@ajkr I see the pr has merged about 2 months, but not release yet. When this pr will be release? Is there any problem?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23241976/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "rockeet": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3278", "title": "global const object definitions in header files", "body": "We should **declare** global const objects in header files, and **define** them in source files.\r\n\r\nExamples are(master branch at 5a7e08468ad2f03b957e255aefdfa81b95a6810a(date 2017-12-15)):\r\n[const std::vector&lt;std::pair&lt;Tickers, std::string&gt;&gt; TickersNameMap](https://github.com/facebook/rocksdb/blob/5a7e08468ad2f03b957e255aefdfa81b95a6810a/include/rocksdb/statistics.h#L301)<br/>[const std::vector&lt;std::pair&lt;Histograms, std::string&gt;&gt; HistogramsNameMap](https://github.com/facebook/rocksdb/blob/5a7e08468ad2f03b957e255aefdfa81b95a6810a/include/rocksdb/statistics.h#L517)\r\n\r\n### Expected behavior\r\nOne definition rule: one global const object has one only one definition.\r\n\r\n### Actual behavior\r\nglobal const object definitions in header file will produce multiple objects, in each source file which included the header.\r\n\r\n### Steps to reproduce the behavior\r\n```c++\r\n// common.h\r\n#include <stdio.h>\r\nstruct AutoInit {\r\n        int m;\r\n        AutoInit()  { static int n = 0; m = ++n; printf(\"%s: %3d\\n\", __PRETTY_FUNCTION__, m); }\r\n        ~AutoInit() { printf(\"%s: %3d\\n\", __PRETTY_FUNCTION__, m); }\r\n};\r\nconst AutoInit aa;\r\n\r\n// a.cc\r\n#include \"common.h\"\r\n\r\n// b.cc\r\n#include \"common.h\"\r\nint main() {\r\n        return 0;\r\n}\r\n```\r\n\r\noutput:\r\n```\r\nAutoInit::AutoInit():   1\r\nAutoInit::AutoInit():   2\r\nAutoInit::~AutoInit():   2\r\nAutoInit::~AutoInit():   1\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3278/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "matthewvon": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3271", "title": "RFC:  Pluggable WriteController", "body": "I presented an alternate write throttle for slower class servers at the annual RocksDB Meetup December 4, 2017.  The current implementation is completely external to RocksDB.  During the Q&A session, I was asked how this could be better/cleanly integrated with RocksDB.  My answer was to have the WriteController object pluggable, like the bloom filters and rate limiters.  It was suggested that I post an RFC here first, then potentially submit a PR with said change and incorporating comments posted against the issue.\r\n\r\nMy suggested change is:\r\n\r\n1.  add a shared_ptr object to the Options structure where a user can supply a custom WriteController.  This is a shared_ptr, not a unique_ptr, in case the user has multiple database instances that wish to share the same WriteController so as to have a \"system-wide\" view.\r\n\r\n2.  the shared_ptr from the Options structure is assigned to the DB_Impl object in place of the current WriteController member object.  If the user does not supply a custom WriteController, an instance of the existing WriteController is automatically added.\r\n\r\n3.  write_controller.h moves to the include/rocksdb directory.\r\n\r\n4.  add \"virtual\" to most of the existing WriteController methods.  This simplifies creating object derivatives.\r\n\r\n5.  optional:  I have considered making some of the existing WriteController routines thread safe.  They currently assume the caller holds the database mutex.  This could be awkward if a derived instance is actually shared across multiple database instances.  Otherwise, any user derived object will have to include the necessary protections.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3271/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ktnowak": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3266", "title": "createNewBackup() intermittently throws \"checksum mismatch\" with writes in parallel", "body": "### Problem\r\nWe see following exception intermittently thrown from `BackupEngine.createNewBackup()` when there is high volume of batch-writes running in parallel.\r\n\r\n```\r\norg.rocksdb.RocksDBException: checksum mismatch\r\nat org.rocksdb.BackupEngine.createNewBackup(Native Method)\r\nat org.rocksdb.BackupEngine.createNewBackup(BackupEngine.java:81)\r\nat org.rocksdb.BackupEngine.createNewBackup(BackupEngine.java:53)\r\nat org.rocksdb.test.RocksDbChecksumError.testBackupCausesChecksumError(RocksDbChecksumError:44)\r\n```\r\n\r\nRocksDB 5.7.3 LOG excerpt\r\n```\r\n2017/12/3-22:27:36.148268 7fc9c410e700 [db/db_impl_write.cc:1127] [testCf] New memtable create with log file: #811. Immutable memtables 0.\r\n(...)\r\n2017/12/3-22:27:36.153696 7fc9c420e700 [db/filesnapshot.cc:34] File Deletions Disabled\r\n2017/12/3-22:27:36.160151 7fc9c420e700 [WARN] [db/wal_manager.cc:425] /path/to/rocks/data/000811.log: dropping 32768 bytes; Corruption: checksum mismatch\r\n2017/12/3-22:27:36.160161 7fc9c420e700 [WARN] [db/wal_manager.cc:425] /path/to/rocks/data/000811.log: dropping 2555358 bytes; Corruption: error in middle of record\r\n2017/12/3-22:27:36.160199 7fc9c420e700 [WARN] [db/wal_manager.cc:425] /path/to/rocks/data/000811.log: dropping 32761 bytes; Corruption: missing start of fragmented record(1)\r\n(...)\r\n2017/12/3-22:27:36.169905 7fc9c420e700 [WARN] [db/wal_manager.cc:425] /path/to/rocks/data/000811.log: dropping 5988 bytes; Corruption: missing start of fragmented record(2)\r\n2017/12/3-22:27:36.170401 7fc9c420e700 [db/filesnapshot.cc:57] File Deletions Enabled\r\n(...)\r\n2017/12/3-22:27:36.182490 7fc96c3f9700 [db/db_impl_compation_flush.cc:50] [JOB 1259] Syncing log #811\r\n(...)\r\n2017/12/3-22:27:36.204967 7fc96c3f9700 [db/db_impl_files.cc:397] [JOB 1259] Try to delete WAL files size 6034012, prev total WAL file size 12068024, number of live WAL files 2.\r\n2017/12/3-22:27:36.205022 7fc96c3f9700 [db/wal_manager.cc:269] [JOB 1259] Move log file /path/to/rocks/data/000811.log to /path/to/rocks/data/archive/000811.log -- OK\r\n```\r\n\r\nAfter exception occurs, all subsequent attempts to create backup will consitently fail with the same error complaining about corrupted WAL log.\r\n[](url)\r\nIt is also intermittently raised when trying to create checkpoint during similar circumstances (what is done by BackupEngine under the hood).\r\n\r\n### Environment\r\nRHEL 6.7 x86_64 4xCPU\r\nRocksDB 5.7.3, 5.7.5, 5.8.7\r\n\r\n### Steps to reproduce the behavior\r\nExecute Junit tests on Linux env from the package attached:\r\n`RocksDbChecksumError.testBackupCausesChecksumError()`\r\n`RocksDbChecksumError.testChecpointCausesChecksumError()`\r\n\r\n[rocksdb-checksumerror.zip](https://github.com/facebook/rocksdb/files/1554021/rocksdb-checksumerror.zip)\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3266/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "SunguckLee": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3265", "title": "Allocate huge disk space for SST, but use only small part of that file.", "body": "I am testing \"target_file_size_multiplier\" option of MongoRocks.\r\nI found that bigger target_file_size_multiplier makes mongodb use more total disk space.\r\n\r\nThis case was happened when I have added \"configString: 'max_background_compactions=20;target_file_size_multiplier=4;target_file_size_base=134217728;write_buffer_size=134217728;level0_file_num_compaction_trigger=3;level0_slowdown_writes_trigger=10'\" option to MongoDB.\r\n\r\n### Expected behavior\r\nTotal data size is less than 20GB, But sometimes single SST file allocated over 600GB.\r\nI am not sure what size is proper but not 600GB for small data.\r\n\r\n### Actual behavior\r\nSome SST file is allocated 685GB. \r\n```\r\n# du -sh /data/mongodb/db/*\r\n2.8G    /data/mongodb/db/000333.sst\r\n3.2G    /data/mongodb/db/000499.sst\r\n3.2G    /data/mongodb/db/000621.sst\r\n2.4G    /data/mongodb/db/000708.sst\r\n513M    /data/mongodb/db/000727.sst\r\n301M    /data/mongodb/db/000732.sst\r\n790M    /data/mongodb/db/000736.sst\r\n849M    /data/mongodb/db/000741.sst\r\n516M    /data/mongodb/db/000742.sst\r\n122M    /data/mongodb/db/000744.sst\r\n122M    /data/mongodb/db/000746.sst\r\n301M    /data/mongodb/db/000747.sst\r\n122M    /data/mongodb/db/000749.sst\r\n685G    /data/mongodb/db/000750.sst  ==> WHY ?\r\n122M    /data/mongodb/db/000752.sst\r\n122M    /data/mongodb/db/000754.sst\r\n607M    /data/mongodb/db/000755.sst\r\n122M    /data/mongodb/db/000757.sst\r\n4.0K    /data/mongodb/db/CURRENT\r\n4.0K    /data/mongodb/db/IDENTITY\r\n141M    /data/mongodb/db/journal\r\n0       /data/mongodb/db/LOCK\r\n1.4M    /data/mongodb/db/LOG\r\n4.0M    /data/mongodb/db/MANIFEST-000005\r\n8.0K    /data/mongodb/db/OPTIONS-000005\r\n8.0K    /data/mongodb/db/OPTIONS-000008\r\n```\r\nBut data is only resident about 1GB.\r\n```\r\n# ls -alh /data/mongodb/db/\r\ntotal 700G\r\ndrwxr-xr-x 3 mongod mongod 4.0K Dec 12 16:15 .\r\ndrwxr-xr-x 5 mongod mongod  159 Dec 12 16:01 ..\r\n-rw-r--r-- 1 mongod mongod 2.8G Dec 12 16:05 000333.sst\r\n-rw-r--r-- 1 mongod mongod 3.2G Dec 12 16:09 000499.sst\r\n-rw-r--r-- 1 mongod mongod 3.2G Dec 12 16:12 000621.sst\r\n-rw-r--r-- 1 mongod mongod 2.4G Dec 12 16:14 000708.sst\r\n-rw-r--r-- 1 mongod mongod 513M Dec 12 16:14 000727.sst\r\n-rw-r--r-- 1 mongod mongod 301M Dec 12 16:14 000732.sst\r\n-rw-r--r-- 1 mongod mongod 790M Dec 12 16:15 000736.sst\r\n-rw-r--r-- 1 mongod mongod 849M Dec 12 16:15 000741.sst\r\n-rw-r--r-- 1 mongod mongod 516M Dec 12 16:15 000742.sst\r\n-rw-r--r-- 1 mongod mongod 122M Dec 12 16:15 000744.sst\r\n-rw-r--r-- 1 mongod mongod 122M Dec 12 16:15 000746.sst\r\n-rw-r--r-- 1 mongod mongod 301M Dec 12 16:15 000747.sst\r\n-rw-r--r-- 1 mongod mongod 122M Dec 12 16:15 000749.sst\r\n-rw-r--r-- 1 mongod mongod 927M Dec 12 16:15 000750.sst ==> Only use 927MB\r\n-rw-r--r-- 1 mongod mongod 122M Dec 12 16:15 000752.sst\r\n-rw-r--r-- 1 mongod mongod 122M Dec 12 16:15 000754.sst\r\n-rw-r--r-- 1 mongod mongod 607M Dec 12 16:15 000755.sst\r\n-rw-r--r-- 1 mongod mongod 122M Dec 12 16:15 000757.sst\r\n-rw-r--r-- 1 mongod mongod   16 Dec 12 15:57 CURRENT\r\n-rw-r--r-- 1 mongod mongod   37 Dec 12 15:56 IDENTITY\r\ndrwxr-xr-x 2 mongod mongod   24 Dec 12 16:15 journal\r\n-rw-r--r-- 1 mongod mongod    0 Dec 12 15:56 LOCK\r\n-rw-r--r-- 1 mongod mongod 1.3M Dec 12 16:15 LOG\r\n-rw-r--r-- 1 mongod mongod  47K Dec 12 16:15 MANIFEST-000005\r\n-rw-r--r-- 1 mongod mongod 4.4K Dec 12 15:56 OPTIONS-000005\r\n-rw-r--r-- 1 mongod mongod 4.4K Dec 12 15:57 OPTIONS-000008\r\n```\r\n\r\n### Steps to reproduce the behavior\r\nMongoDB configuration file\r\n```\r\n....\r\nstorage:\r\n    syncPeriodSecs: 60\r\n    dbPath: /data/mongodb\r\n    journal:\r\n        enabled: true\r\n        commitIntervalMs: 100\r\n\r\n    engine: rocksdb\r\n    rocksdb:\r\n        cacheSizeGB: 30\r\n        compression: lz4\r\n        maxWriteMBPerSec : 1024\r\n        configString: \"max_background_compactions=20;target_file_size_multiplier=4;target_file_size_base=134217728;write_buffer_size=134217728;level0_file_num_compaction_trigger=3;level0_slowdown_writes_trigger=10\"\r\n```\r\n\r\nJoin member of some replica-set (which has over 30GB disk data), then this case might happen.\r\n\r\nAnd is this case something to do with \"allow_fallocate\" options ? Then could you share why do we need \"fallocate\" operation for SST file.\r\n```\r\n[Version]\r\n  rocksdb_version=5.9.0\r\n  options_file_version=1.1\r\n\r\n[DBOptions]\r\n  advise_random_on_open=true\r\n  use_adaptive_mutex=false\r\n  allow_mmap_reads=false\r\n  allow_fallocate=true\r\n  ...\r\n```", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3265/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3198", "title": "Occasional performance drop on MongoRocks", "body": "### Expected behavior\r\nI have prepared mongodb 3.4.10 + RocksDB 5.9.\r\nAnd ran sysbench(https://github.com/Percona-Lab/sysbench/tree/dev-mongodb-support-1.0) prepare for test with this options.\r\n```\r\n./sysbench/sysbench \\\r\n    --mongo-write-concern=1 \\\r\n    --mongo-url=\"mongodb://test-mongodb-server\" \\\r\n    --mongo-database-name=sbtest \\\r\n    --test=sysbench/tests/mongodb/oltp.lua \\\r\n    --oltp_table_size=500000000 \\\r\n    --oltp_tables_count=16 \\\r\n    --num-threads=16 \\\r\n    --rand-type=pareto \\\r\n    --report-interval=10 \\\r\n    --max-requests=0 \\\r\n    --max-time=600 \\\r\n    --oltp-point-selects=10 \\\r\n    --oltp-simple-ranges=1 \\\r\n    --oltp-sum-ranges=1 \\\r\n    --oltp-order-ranges=1 \\\r\n    --oltp-distinct-ranges=1 \\\r\n    --oltp-index-updates=1 \\\r\n    --oltp-non-index-updates=1 \\\r\n    --oltp-inserts=1 prepare\r\n```\r\n\r\nI have expected stable insert throughput. because there's no query(Find). only single thread inserts document to test collection (sbtest).\r\n\r\n### Actual behavior\r\nFirst  a few hours, everything is peaceful (thanks to L0 -> L0 compaction).\r\nBut after about 0.4 billion documents are insert sbtest1 (the first collection), insert performance is dropped drastically. At normal status, 30K insert / second, but it's dropped to 2K/sec. And once this case happens, this low throughput continue about 1~2 minutes.\r\n```\r\ninsert query update delete .. vsize   res qrw arw net_in net_out conn ..\r\n 29877    *0     *0     *0 .. 8.35G 6.90G 0|0 0|2  6.70m   57.7k    3 ..\r\n 30081    *0     *0     *0 .. 8.35G 6.90G 0|0 0|1  6.73m   58.2k    3 ..\r\n 30023    *0     *0     *0 .. 8.35G 6.90G 0|0 0|1  6.73m   58.0k    3 ..\r\n 27982    *0     *0     *0 .. 8.35G 6.90G 0|0 0|0  6.27m   57.1k    3 ..\r\n 30020    *0     *0     *0 .. 8.35G 6.90G 0|0 0|0  6.73m   57.5k    3 ..\r\n 29394    *0     *0     *0 .. 8.35G 6.90G 0|0 0|1  6.71m   57.3k    3 ..\r\n  5543    *0     *0     *0 .. 8.35G 6.90G 0|0 0|2  1.23m   49.0k    3 ..\r\n  1594    *0     *0     *0 .. 8.35G 6.89G 0|0 0|1   461k   54.1k    3 ..\r\n  1532    *0     *0     *0 .. 8.35G 6.89G 0|0 0|1   218k   51.1k    3 ..\r\n  1459    *0     *0     *0 .. 8.35G 6.89G 0|0 0|1   459k   54.0k    3 ..\r\n```\r\n\r\nDuring this performance drop, I ran linux profile and pstack for mongod process.\r\nAnd I have found something interesting. During performance drop, \"rocksdb::Version::AddLiveFiles\" took a lot of cpu time. I have not checked source code yet, but this is not expected (might be even you). But I am worried about 30K/sec performance drop to 2K/sec.\r\n\r\nProfile report(During performance drop) : https://gist.github.com/SunguckLee/e68a02981f6edece6214b32db8b37513#file-perf_report_slow-txt\r\nProfile report(During normal 30K insert) : https://gist.github.com/SunguckLee/9fd03e1397eb2446e858e024200cc2ec#file-perf_report_normal-txt\r\nStacktrace : https://gist.github.com/SunguckLee/a0fb86b182c259360450b8528433a8f0#file-stacetace-txt\r\n\r\n### Steps to reproduce the behavior\r\nBuild rocksdb with \"USE_RTTI=1 CFLAGS=-fPIC make static_lib\".\r\nBuild mongodb with \"scons CPPPATH=\"/mongodb-rocksdb-3.4/rocksdb/include\" LIBPATH=\"/mongodb-rocksdb-3.4/rocksdb\" LIBS=\"lz4 zstd\" -j5 mongod mongo\".\r\n\r\nAnd run mongodb with below mongod configuration.\r\n```\r\n...\r\nstorage:\r\n    syncPeriodSecs: 60\r\n    dbPath: /data/mongodb\r\n    journal:\r\n        enabled: true\r\n        commitIntervalMs: 100\r\n\r\n    engine: rocksdb\r\n    rocksdb:\r\n        cacheSizeGB: 31\r\n        compression: lz4\r\n```\r\n\r\nAnd run sysbench prepare command.\r\n\r\nI have built mogorocks with devtoolset-6 and tested it on ...\r\n```\r\nLinux mongo-test-server 3.10.0-327.36.1.el7.x86_64 #1 SMP Sun Sep 18 13:04:29 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n32 core, 64GB Memory, NVME SSD\r\n```", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3198/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/af92d4ad112f192693f6017f24f9ae1b00e1f053", "message": "Avoid too frequent MaybeScheduleFlushOrCompaction() call\n\nSummary:\nIf there's manual compaction in the queue, then \"HaveManualCompaction(compaction_queue_.front())\" will return true, and this cause too frequent MaybeScheduleFlushOrCompaction().\n\nhttps://github.com/facebook/rocksdb/issues/3198\nCloses https://github.com/facebook/rocksdb/pull/3366\n\nDifferential Revision: D6729575\n\nPulled By: ajkr\n\nfbshipit-source-id: 96da04f8fd33297b1ccaec3badd9090403da29b0"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "siying": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3254", "title": "Design Discussion of Async Support", "body": "We decided to have the design discussion of async support here. Let's see how it works.\r\n\r\nThe initial discussion started at the prototyping work by @yuslepukhin \r\nhttps://github.com/facebook/rocksdb/pull/3063\r\nand\r\nhttps://github.com/facebook/rocksdb/pull/2727", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3254/reactions", "total_count": 4, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 2}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/47ad6b81ff79c9f3320599318562d9ceefbc56a3", "message": "Add 5.10.fb to tools/check_format_compatible.sh\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/3383\n\nDifferential Revision: D6762375\n\nPulled By: siying\n\nfbshipit-source-id: dc1e0dc9718ffb59ffe42e2a2c844b67f935a5fb"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/a478e8569778266abf78fa67a860e805ac7fb86d", "message": "Remove GCC parameter \"-march=native\" for ARM\n\nSummary:\nMost popular versions of GCC can't identify platform on ARM if \"-march=native\" is specified. Remove it to unblock most people.\nCloses https://github.com/facebook/rocksdb/pull/3346\n\nDifferential Revision: D6690544\n\nPulled By: siying\n\nfbshipit-source-id: bbaba9fe2645b6b37144b36ea75beeff88992b49"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/6aa95f4d0fc904d2865c13f59051576619ca9811", "message": "Fix a wrong log formatting\n\nSummary:\nI experienced weird segfault because of this mismatch of type in log formatting. Fix it.\nCloses https://github.com/facebook/rocksdb/pull/3345\n\nDifferential Revision: D6687224\n\nPulled By: siying\n\nfbshipit-source-id: c51fb1c008b7ebc3efdc353a4adad3e8f5b3e9de"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/ccc095a016fc2c1b0deb1ab6dbac4671d1a77dc5", "message": "Speed up BlockTest.BlockReadAmpBitmap\n\nSummary:\nBlockTest.BlockReadAmpBitmap is too slow and times out in some environments. Speed it up by:\n(1) improve the way the verification is done. With this it is 5 times faster\n(2) run fewer tests for large blocks. This cut it down by another 10 times.\nNow it can finish in similar time as other tests.\nCloses https://github.com/facebook/rocksdb/pull/3313\n\nDifferential Revision: D6643711\n\nPulled By: siying\n\nfbshipit-source-id: c2397d666eab5421a78ca87e1e45491e0f832a6d"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/def6a00740fc4d15ca4e3e7cae003d4d2b8b8a98", "message": "Print out compression type of new SST files in logging\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/3264\n\nDifferential Revision: D6552768\n\nPulled By: siying\n\nfbshipit-source-id: 6303110aff22f341d5cff41f8d2d4f138a53652d"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/6b77c0737902f1f1a1982393eef1addd1fe61ab1", "message": "NUMBER_BLOCK_COMPRESSED, etc, shouldn't be treated as timer counter\n\nSummary:\nNUMBER_BLOCK_DECOMPRESSED and NUMBER_BLOCK_COMPRESSED are not reported unless the stats level contain detailed timers, which is wrong. They are normal counters. Fix it.\nCloses https://github.com/facebook/rocksdb/pull/3263\n\nDifferential Revision: D6552519\n\nPulled By: siying\n\nfbshipit-source-id: 40899ccea7b2856bb39752616657c0bfd432f6f9"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/a9c8d4ef15ed22196260ecf6326a5d3b5a5cbbe8", "message": "Fix memory issue introduced by 2f1a3a4d748ea92c282a1302b1523adc6d67ce81\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/3256\n\nDifferential Revision: D6541714\n\nPulled By: siying\n\nfbshipit-source-id: 40efd89b68587a9d58cfe6f4eebd771c2d9f1542"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/0d5692e02b4eed135057cb39b1a941d034955b42", "message": "Switch version to 5.10\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/3252\n\nDifferential Revision: D6539373\n\nPulled By: siying\n\nfbshipit-source-id: ce7c3d3fe625852179055295da9cf7bc80755025"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/2f1a3a4d748ea92c282a1302b1523adc6d67ce81", "message": "Refactor ReadBlockContents()\n\nSummary:\nDivide ReadBlockContents() to multiple sub-functions. Maintaining the input and intermediate data in a new class BlockFetcher.\nI hope in general it makes the code easier to maintain.\nAnother motivation to do it is to clearly divide the logic before file reading and after file reading. The refactor will help us evaluate how can we make I/O async in the future.\nCloses https://github.com/facebook/rocksdb/pull/3244\n\nDifferential Revision: D6520983\n\nPulled By: siying\n\nfbshipit-source-id: 338d90bc0338472d46be7a7682028dc9114b12e9"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/2a3363d52eab5d1f83fa45fbd6b295c9a66061a4", "message": "ldb dump can print histogram of value size\n\nSummary:\nMake \"ldb dump --count_only\" print histogram of value size. Also, fix a bug that \"ldb dump --path=<db_path>\" doesn't work.\nCloses https://github.com/facebook/rocksdb/pull/2944\n\nDifferential Revision: D5954527\n\nPulled By: siying\n\nfbshipit-source-id: c620a444ec544258b8d113f5f663c375dd53d6be"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/4748911357ac1e476b6f1ecb9fb694fa9bf2a7b7", "message": "Add LogDevice to USERS.md\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/2927\n\nDifferential Revision: D5906613\n\nPulled By: siying\n\nfbshipit-source-id: 607401e05b27508c816c700864fe81514606e4ef"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/c319792059e13969e1538722ba96d75c09f321a0", "message": "Directly refernce perf_context internally.\n\nSummary:\nAfter 7f6c02dda16471c2ed3318e9e7156f2b8d13bf46, the same get_perf_context() is called both of internally and externally. However, I found internally this is not got inlined. I don't know why this is the case, but directly referencing perf_context is the logical way to do.\nCloses https://github.com/facebook/rocksdb/pull/2892\n\nDifferential Revision: D5843789\n\nPulled By: siying\n\nfbshipit-source-id: b49777d8809f35847699291bb7f8ea2754c3af49"}], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3398", "title": "BlockBasedTable::NewDataBlockIterator to always return BlockIter", "body": "Summary: This is a pre-cleaning up before a major block based table iterator refactoring. BlockBasedTable::NewDataBlockIterator() will always return BlockIter. This simplifies the logic and code and enable further refactoring and optimization.\r\n\r\nTest Plan: Run all existing tests", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3385", "title": "Improve fallocate size in compaction output", "body": "Summary: Now in leveled compaction, we allocate solely based on output target file size. If the total input size is smaller than the number, we should use the total input size instead. Also, cap the allocate size to 1GB.\r\n\r\nTest Plan: Add some verification in compaction_picker_test", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3340", "title": "Preload some files even if options.max_open_files", "body": "Summary:\r\nChoose to preload some files if options.max_open_files != -1. This can slightly narrow the gap of performance between options.max_open_files is -1 and a large number. To avoid a significant regression to DB reopen speed if options.max_open_files != -1. Limit the files to preload in DB open time to 16.\r\n\r\nTest Plan: Run all existing tests.", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35950313", "body": "@slachowsky I don't think it is desirable to have checksum type per block anyway. I should try to find a way to encode it in footer. No matter it is in footer or now, it seems to be desirable to put a version field as a part of footer so that in the future if there is another format change for footer/trailer, we can make it backward compatible. How about this: now there is a magic number in the end of the file. Let's make a new format, which has [version, magic_number] in the end. To bootstrap this meta change, we can use another magic number.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35950313/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/6049155", "body": "@liukai , when you mention \"another team encountered an error\", which error do you mean?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/6049155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/12326247", "body": "Thanks! If you are interested, you can directly send a pull request to fix it:)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/12326247/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/14423538", "body": "@yuslepukhin how about blocking it from running in Windows? This benchmark is not important.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/14423538/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/14423731", "body": "Whatever works for you. Appveyor didn't fail because gflag was not available there so it was blocked. We didn't have an environment that tests your case:)\n\nOn Nov 16, 2015, at 11:01 AM, Dmitri Smirnov <notifications@github.com<mailto:notifications@github.com>> wrote:\n\n@siyinghttps://github.com/siying I can either #ifdef it or remove entirely from Windows build. Will check either way soon.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/facebook/rocksdb/commit/d06b63e99f7231c5b7df34b72a470beed6005f03#commitcomment-14423558.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/14423731/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17111952", "body": "@yuslepukhin does IsSyncThreadSafe() return true for Windows Env at all? If not DB::SyncWAL() is not supported so it returns in line 2175. In that case, it is a unit test that should be disabled in Windows.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17111952/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17937186", "body": "@IvRRimum sorry for the inconvenience. We removed it because it is a infrequently used feature and we want to reduce number of options to make it easier for users to tune RocksDB.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17937186/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17956161", "body": "I can add back the C API just doing nothing if you feel it will be helpful to you.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17956161/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/18454045", "body": "Maybe it's not the merge error but the code bug. Here `env` is not defined.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/18454045/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/18454048", "body": "Same here. `env` is not defined anymore.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/18454048/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/18784091", "body": "The side effect of this line change caused a deadlock condition when max_successive_merges is triggered during the recovery.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/18784091/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19780107", "body": "I'm glad to see we've made so much progress on blob storage. Good job!\r\n\r\nI didn't finish reading all the code. I have some preliminary comments:\r\n1. We follow Google C++ Style: https://google.github.io/styleguide/cppguide.html . For example \"The open curly brace is always on the end of the last line of the function declaration, not the start of the next line\", and \"Ordinarily, functions should start with a capital letter and have a capital letter for each new word (a.k.a. \"upper camel case\" or \"Pascal case\")\".\r\n\r\n2. All the code related to blob should be under utilities/. Don't place it under db/. utilities/ is the addon components so it can be removed without any problem. We want to make the core part (anything outside utilities/) minimum for better maintenance.\r\n\r\nI'll try to add more comments near the lines to be commented (may expand to more commits).", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19780107/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19780180", "body": "It feels very expensive to reopen the file every time a Get() is issued. Can we always keep all the log files open?\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19780180/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19780231", "body": "If a file is compacted just after Get() is called, we'll return not found, although it is still there. Nothing can hold the data after Get() is called.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19780231/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19780245", "body": "If this is the last record of an incomplete log file, we can still read without the footer, do I understand correctly?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19780245/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19780356", "body": "Can we put magic number in the very end of the file?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19780356/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19780357", "body": "I suggest we have a version number here.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19780357/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19780415", "body": "The CRC here only covers the data itself. It doesn't cover the metadata, like sequence ID, TTL, etc.\r\n\r\nMy suggestion is to have a CRC covering everything and put it in the footer of the frame, so that even a bit flip will be caught.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19780415/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19780442", "body": "There can be many incomplete files if the DB is restarted multiple times, and the recovery is going to be slow. I suggest here we rewrite to a fresh file and generate the footer. In this case, every recovery will at most see one incomplete file.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19780442/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21809261", "body": "It's better to leave it kDeprecated instead of removing it.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21809261/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23169779", "body": "It was a mistake and we fixed it: https://github.com/facebook/rocksdb/commit/4a2e4891fe4c6f66fb9e8e2d29b04f46ee702b52", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23169779/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23424910", "body": "I believe this is more expensive than doing a non-relaxed store to ts_tc_map_[seq % 1000000], as there will be cache line back-and-force for every thread, not just the one conflicting the cache line of ts_tc_map_[seq % 1000000].", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23424910/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23424947", "body": "Please make sure the line is no optimized out in your benchmark.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23424947/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23425334", "body": "Is the function called?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23425334/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23425421", "body": "Great!", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23425421/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/10631405", "body": "Please follow the coding convention. You can do \"make format\" and it will fix some code convention problems for you.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/10631405/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/10631422", "body": "nit: \"else if\" is more intuitive to read to me.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/10631422/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/12495910", "body": "Can we just replace line 79?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/12495910/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/12495983", "body": "I mean guard MAP_HUGETLB in the same way as OS_LINUX in line 79.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/12495983/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17066615", "body": "Is it a merge mistake?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17066615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17066753", "body": "If thisimplementation is a full copy of codes from line 306 to line 314. Can we make a helper function to share the codes?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17066753/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17067392", "body": "Can you explain why we still leave data_, size_, cachable_ and compression_type_ in class Block? It seems to me that in the case of not owned, we should fill the respective variables in contents_ and remove data_, size_, restart_offset_, cachable_ and compression_type_ from class Block.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17067392/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17068155", "body": "The original one follows our current coding convention. What's the reason for the change?\n\nAlso, you can run \"make format\" to fix the indenting.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17068155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20336530", "body": "Is the logic changed at all?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20336530/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20336555", "body": "Should it be\n\n(f_stat.st_size + kPageSize - 1) / kPageSize \\* (kPageSize / kBlockSize) ?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20336555/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20337334", "body": "OK, then should it be\n\n(f_stat.st_size + kPageSize + kBlockSize - 1) / kPageSize \\* (kPageSize / kBlockSize) ?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20337334/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20338502", "body": "I just wonder which one is correct. For example, f_stat.st_size=1, my equation returns 4, yours returns 5. Which one is correct?\n\nBy the way, this test doesn't fail on my machine without any change.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20338502/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33834016", "body": "I noticed quite some functions were deleted. Are they repository sync and merge issue?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33834016/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33964427", "body": "Can you create a const in port.h and still use std::numeric_limits<int32_t>::max() for other platforms? I'm concerned some platform might not have INT32_MAX.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33964427/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33964549", "body": "Not getting this change.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33964549/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33965039", "body": "Instead, can we move implementation of ToString() to util/slice.cc?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33965039/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33968113", "body": "Maybe replace existing codes this template based one.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33968113/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33973107", "body": "We follow Google C++ style: https://google-styleguide.googlecode.com/svn/trunk/cppguide.html and we use 80-char wrapping.\nWe have a tool build_tools/format-diff.sh to auto reformatting. Not sure whether you can run it on Windows. Here are the auto-formatting patch: https://reviews.facebook.net/F515421\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33973107/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33988220", "body": "@yuslepukhin It's better to be consistent but I didn't yet see another one yet.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33988220/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33988271", "body": "Oh it is a bug then. Great.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33988271/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33990744", "body": "We use 2-char space indenting.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33990744/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33991164", "body": "I like the change of passing through PlainTableOptions all the way down. But is it related to the port?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33991164/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33991390", "body": "#ifdef MAP_HUGETLB\nshould be enough.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33991390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33991575", "body": "Is there a reason changing from \"..\" to <..>?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33991575/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33991806", "body": "Is it a merge error?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33991806/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33991999", "body": "Are these changes related? Can we separate them if they are not related.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33991999/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33992588", "body": "After removing it, it does not build under gcc anymore. Can you add it back if it is not Windows?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33992588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33992785", "body": "2-char indenting.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33992785/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33998508", "body": "It's fine. Or you can first check it in and I fix it in master quickly in another patch.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33998508/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396170", "body": "@igorcanadi I did ran valgrind for their previous version and didn't see any issue. I think this is not new codes.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396170/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396964", "body": "I would hold for this for now. There is a possibility that Natham wants to implement our own mutex to improve write queue sychronization.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396964/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34397876", "body": "I reran valgrind for DBTest.IdentityAcrossRestarts and it passed.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34397876/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34528738", "body": "Which public .h file depends on them? Can we simply move it to .cc files?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34528738/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34529150", "body": "I only find Logger::Logv, which should be able to move to util/env.cc. Any other places I missed?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34529150/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34530079", "body": "Can you give examples?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34530079/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34531243", "body": "Can we avoid snprintf and ROCKSDB_PRIszt then? They are already removed from public headers. Also, can we guard others with OS_WIN?\n\nI'll merge it if you make that change.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34531243/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34532319", "body": "You only need one giant one, right?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34532319/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34532371", "body": "Also it's better to avoid strcasecmp too. These are common names and might conflict with users' define (notice users need to include public APIs).\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34532371/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34598355", "body": "@yuslepukhin sorry I gave a bad suggestion yesterday. Guarding OS_WIN in public header is a bad idea as it's not guarantee users set those flags when building application on top of RocksDB. Let me think about it more and get back to you.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34598355/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34599211", "body": "Can you include RocksDB header, instead of LevelDB one?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34599211/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34817602", "body": "You need to sync and rebase the codes for those changes.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34817602/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34817794", "body": "Duplicated?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34817794/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34819469", "body": "Oh thanks.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34819469/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/37451760", "body": "I would simply update it after line 131 for sequence ID of every row.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/37451760/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/37579392", "body": "Can you fix the indenting?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/37579392/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/37607403", "body": "Wow that's complicated. We can merge it then.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/37607403/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/38005391", "body": "virtual?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/38005391/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/38005550", "body": "Maybe call it UserDirectIO().\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/38005550/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/38018229", "body": "I mean UseDirectIO()...\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/38018229/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/38018371", "body": "A virtual function can also have default value. See IsSyncThreadSafe() for the same class. You should do the same. Even move the new function near it.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/38018371/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39294390", "body": "Maybe just call it HandleReadAhead() or something like that.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39294390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39295925", "body": "The practice we try to follow now is try to not add \"inline\" annotation and let the compiler make decisions. Not a strong policy though. You can make the call.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39295925/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39296714", "body": "Can you leave a default to return OK?\nI want users who implement their own Env outside RocksDB to avoid code changes to compile.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39296714/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39296871", "body": "1 << 20 is used twice, can declare a const above the if.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39296871/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39303560", "body": "Can we do it as a follow-up? I don't want to mix behavior change with this restructuring.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39303560/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39304359", "body": "```\n+  Status Append(const Slice& data, uint64_t offset) override {\n+    return target_->Append(data, offset);\n+  }\n```\n\nis needed on my host to build on Linux.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39304359/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39304522", "body": "Can you move it to it and its callers to .cc file and hide it in anonymous namespace?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39304522/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39304715", "body": "Our GCC doesn't have make_unique(). Can we go with the old way?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39304715/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39304800", "body": "See:\n\nmake: **\\* [util/file_reader_writer.o] Error 1\nmake: **\\* Waiting for unfinished jobs....\nIn file included from util/env_posix.cc:35:0:\n./include/rocksdb/env.h:462:18: error: \u00e2\u20ac\u02dcvirtual rocksdb::Status rocksdb::WritableFile::Append(const rocksdb::Slice&, uint64_t)\u00e2\u20ac\u2122 was hidden [-Werror=overloaded-virtual]\n   virtual Status Append(const Slice& /\\* data _/, uint64_t /_ offset _/) {\n                  ^\nutil/env_posix.cc:469:18: error:   by \u00e2\u20ac\u02dcvirtual rocksdb::Status rocksdb::{anonymous}::PosixMmapFile::Append(const rocksdb::Slice&)\u00e2\u20ac\u2122 [-Werror=overloaded-virtual]\n   virtual Status Append(const Slice& data) override {\n                  ^\nIn file included from util/env_posix.cc:35:0:\n./include/rocksdb/env.h:462:18: error: \u00e2\u20ac\u02dcvirtual rocksdb::Status rocksdb::WritableFile::Append(const rocksdb::Slice&, uint64_t)\u00e2\u20ac\u2122 was hidden [-Werror=overloaded-virtual]\n   virtual Status Append(const Slice& /_ data _/, uint64_t /_ offset _/) {\n                  ^\nutil/env_posix.cc:614:18: error:   by \u00e2\u20ac\u02dcvirtual rocksdb::Status rocksdb::{anonymous}::PosixWritableFile::Append(const rocksdb::Slice&)\u00e2\u20ac\u2122 [-Werror=overloaded-virtual]\n   virtual Status Append(const Slice& data) override {\n                  ^\ncc1plus: all warnings being treated as errors\nmake: *_\\* [util/env_posix.o] Error 1\n\ndon't know why. How about change the name Append() to something like PositionedAppend()?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39304800/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39307139", "body": "We follow Google C++ Style: https://google-styleguide.googlecode.com/svn/trunk/cppguide.html#Function_Names\n\nIn this guideline, accessors should be called something like `alignment()` and `set_alignment()`.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39307139/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39309792", "body": "Now we always align in the internal buffer. I'm fine with it. Maybe one another team member can take a look and see whether he's comfortable with the chagne. @igorcanadi @yhchiang @kradhakrishnan @rven1 @agiardullo \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39309792/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/40040422", "body": "port::kMaxUint64?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/40040422/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/40045384", "body": "Look at it again. Is there way to support 32-bit? Is http://stackoverflow.com/questions/27442885/syntax-error-with-stdnumeric-limitsmax related? Can we do something like that?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/40045384/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41418496", "body": "We follow Google C++ style in this code space, so it would be\n  if (!s.ok()) {\n    return s;\n  }\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41418496/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41418879", "body": "nit: can move the implementation to util/env.cc\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41418879/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41419026", "body": "Did you turn those indenting to tab-based instead? We use spaces.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41419026/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41419849", "body": "Do we recycle them infinitely, or can we cap the total size of files we can recycle? Just a suggestion. Not big deal if we don't cap it for now.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41419849/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41420382", "body": "Looks like you changed the indenting. If you run \"make format\" in command line, it will fix most of the indenting problems like this.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41420382/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41421728", "body": "Can we calculate stored_crc  and actual_crc together with the !recycled_ circle? I mean reuse line 352, 353, 359 and 361 go together after line 349. I'm thinking of the case that in the future if soneone modifies it, they won't miss one of the two.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41421728/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41421788", "body": "{ } for our code style.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41421788/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41422093", "body": "I feel we should flush the info log here.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41422093/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41423903", "body": "The change is very good. I know there are some coverage of multiple logs done in column_family_test. @igorcanadi , what tests in column_family_test do you think we should verify for recycled logs too?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41423903/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41424329", "body": "If it is the only place db_options_ is used, I would suggest we directly pass recycle_log_files as a parameter. It will make write unit tests in single modules easier.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41424329/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41424565", "body": "Is it the only place we use db_options_? If that is the case, I suggest we pass db_options_->info_log directly.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41424565/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41466262", "body": "If you feel it should be the way to go, then leave it. Not big deal. I still feel if we the giant object DBOptions, it is harder for us to reason about the module dependencies.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41466262/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41545367", "body": "What's the implication of calling ftruncate() in the end? I guess it is safer to do it because then all log files except the current one will not contain any data from previous log run, but just curious, can we go back to append mode in the last part of the log files, which we don't have to?\n\nSaying that, I'm fine with the current approach. We can always optimize later even if there is a chance.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41545367/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "cv711": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3245", "title": "sst_dump throws a huge panic", "body": "> Note: Please use Issues only for bug reports. For questions, discussions, feature requests, etc. post to dev group: https://www.facebook.com/groups/rocksdb.dev\r\n\r\n### Expected behavior\r\n`$> sst_dump --show_properties --file=138918.sst` should just print the table of properties\r\n\r\n### Actual behavior\r\n\r\nIt starts fine with some of the properties and then this:\r\n\r\n```\r\nsst_dump --show_properties --file=138918.sst \r\nfrom [] to []\r\nProcess 138918.sst\r\nSst file format: block-based\r\nTable Properties:\r\n------------------------------\r\n  # data blocks: 2\r\n  # entries: 63\r\n  raw key size: 4514\r\n  raw average key size: 71.650794\r\n  raw value size: 876\r\n  raw average value size: 13.904762\r\n  data block size: 1455\r\n  index block size: 51\r\n  filter block size: 0\r\n  (estimated) table size: 1506\r\n  filter policy name: N/A\r\n  column family ID: 0\r\n  column family name: default\r\n  comparator name: leveldb.BytewiseComparator\r\n  merge operator name: upsideDownMergevalue too largewrite error: %v already; errno= but memory size has nil element mheap.sweepgen= untyped locals #([a-zA-Z0-9_])+%s %s HTTP/1.1\r\n, not a function-- Name servers\r\n/etc/resolv.conf/index_meta.json0123456789ABCDEF0123456789abcdef2384185791015625: value of type :VerifyDNSLengthAlready ReportedContent-EncodingContent-LanguageContent-Length: FRAME_SIZE_ERRORGC worker (idle)Imperial_AramaicInstRuneAnyNotNLMSpanList_InsertMSpanList_RemoveMeroitic_CursiveMultiple ChoicesOther_AlphabeticPayment RequiredProxy-ConnectionSETTINGS_TIMEOUTSIGNONE: no trapUpgrade RequiredUser-Agent: %s\r\nWww-Authenticate\r\nruntime stack:\r\n^\\x00-\\x{10FFFF}after object keyallow_mmap_readsapplication/jsonarena_block_sizebad SAN sequencebad frame layoutbad special kindbad symbol tablebinary.BigEndianbolt.DB{path:%q}bucket not foundcastogscanstatuscompaction_stylecontent-encodingcontent-languagecontent-locationcontext canceleddateTimeOptionaldivision by zerogc: unswept spangcshrinkstackoffhost unreachablehostLookupOrder=integer overflowinvalid argumentinvalid databaseinvalid encodinginvalid exchangeinvalid spdelta invalid time: %vipc://%s/%d.socklength too largemessage too longmissing stackmapno colon on lineno renegotiationno route to hostnon-IPv4 addressnon-IPv6 addressobject is remotepacer: H_m_prev=policy_evaluatedproto: bad UTF-8receive time outreflect mismatchregexp: Compile(remote I/O errorruntime:  g:  g=runtime: gp: gp=runtime: nameOffruntime: nelems=runtime: textOffruntime: typeOfftime: bad [0-9]*to non-Go memoryunknown network value size wrongversion mismatchworkbuf is emptywww-authenticate does not extend  initialHeapLive= spinningthreads=%%!%c(big.Int=%s)0123456789ABCDEFX0123456789abcdefx060102150405Z07001192092895507812559604644775390625: missing method COMPRESSION_ERRORENHANCE_YOUR_CALMFailed DependencyGC assist markingHEADER_TABLE_SIZEHTTP_1_1_REQUIREDIf-Modified-SinceInvalid empty keyMoved PermanentlyOld_North_ArabianOld_South_ArabianOther_ID_ContinueSIGBUS: bus errorSIGCONT: continueSIGINT: interruptSentence_TerminalToo Many RequestsTransfer-EncodingUnified_IdeographWAL_size_limit_MBallow_mmap_writesbad TinySizeClassboost unspecifiedconnection closedcorrupt zip file create_if_missingdatabase not opendb file close: %sdecryption failedenable_statisticsentersyscallblockexec format errorfractional secondfutexwakeup addr=gp.waiting != nilhandshake failurehttp: nil handlerif-modified-sinceillegal parameterin string literalinteger too largeinvalid stream IDkeep_log_file_numkey align too bigmark - bad statusmarkBits overflowmax_log_file_sizemissing closing )missing closing ]must specify pathno data availablenotetsleepg on g0permission deniedrecursion_desiredreflect.Value.Capreflect.Value.Intreflect.Value.Lenreflect: New(nil)reflect: call of runtime: next_gc=runtime: pointer scanobject n == 0seeker can't seekselect (no cases)t.spanKey.npages=test.blockprofiletest.coverprofiletest.mutexprofilethread exhaustiontransfer-encodingtruncated headersunknown caller pcwrite_buffer_sizewriteoptions_syncwrong medium type because dotdotdot, locked to thread/etc/nsswitch.conf/etc/openssl/certs/etc/pki/tls/certs298023223876953125404 page not found: day out of rangeCaucasian_AlbanianExpectation FailedFLOW_CONTROL_ERRORField: %d Name: %sGo-http-client/1.1Go-http-client/2.0MAX-RECONNECT-TIMEMethod Not AllowedPermanent RedirectProxy-AuthenticateRFS specific errorTemporary RedirectUNKNOWN_SETTING_%dVariation_Selector[^\\x00-\\x{10FFFF}]assertion failed: bad Content-Lengthbad arg in InstAltbad manualFreeListbufio: buffer fullconnection refusedcontext.Backgrounddecoding error: %vexpected leaf nodefile name too longflag redefined: %sforEachP: not donegarbage collectiongcBlackenPromptly=http: no such fileidentifier removedin numeric literalincompatible valueindex out of rangeinput/output errorinvalid IP addressinvalid character invalid criteria: lru_cache_capacitymultihop attemptedno child processesno locks availablenodes map expectednon-minimal lengthoperation canceledproxy-authenticatereduce %v in:\r\n\t%v\r\nreflect.Value.Elemreflect.Value.Typereflect.Value.Uintreflect: Zero(nil)runtime: insert t=sequence truncatedserver misbehavingstarted retries...streams pipe errorsystem page size (too many redirectstruncated sequenceunexpected messageunknown index typeunknown query typeunknown time zone unterminated quoteuse of closed fileuse_adaptive_mutexvalue out of range (%d bytes omitted) already registered called using nil *(?U)spf=(.*)(\\s+|$),  g->atomicstatus=, gp->atomicstatus=/usr/lib/locale/TZ/14901161193847656252006-01-02 15:04:052006-01-02T15:04:0520060102150405Z07007450580596923828125Canadian_AboriginalConnection: close\r\nCreating new index!IDS_Binary_OperatorINADEQUATE_SECURITYINITIAL_WINDOW_SIZEMSpan_Sweep: state=No nodes to executePattern_White_SpacePrecondition FailedProxy-AuthorizationSIGTRAP: trace trapService UnavailableSwitching Protocols__vdso_gettimeofday_cgo_setenv missingafter array elementbad file descriptorbad kind in runfinqbad notifyList sizebad runtime\u00b7mstartbad value for fieldbinary.LittleEndiancgocall unavailableclient disconnectedcontent-dispositioncriterion too shortdevice not a streamdirectory not emptydisk quota exceedederr must be non-nilerror opening indexevictCount overflowexpired certificatefile already closedfile already existsfile does not existfile size too smallfile sync error: %shttp: Server closedif-unmodified-sinceinvalid Trailer keyinvalid URL escape invalid URL port %qmarking free objectmarkroot: bad indexmessage is too longmime: no media typemissing ']' in hostmmap stat error: %snetwork unreachableno node with id: %dnotesleep not on g0nwait > work.nprocsoverflow is not nilpanic during mallocpanic during panic\r\npanic holding lockspanicwrap: no ( in panicwrap: no ) in parsenetlinkmessageproxy-authorizationrecursion_availablereflect.Value.Fieldreflect.Value.Floatreflect.Value.Indexreflect.Value.IsNilreflect.Value.Slicerevoked certificateruntime: pcdata is runtime: preempt g0semaRoot rotateLeftskip this directorystopm holding lockssync.Cond is copiedtest.memprofileratetime: unknown unit too many open filesunaligned sysUnusedunexpected InstFailunknown Go type: %vunknown certificateunknown cipher typevalue align too bigwrite of Go pointerwrong type of batchx509: unknown error failed to connect:  markroot jobs done\r\n to unallocated span%d matches, took %s\r\n(?U)dkim=(.*)(\\s+|$)/usr/share/zoneinfo/37252902984619140625: leftover defer sp=CLIENT_RANDOM %x %x\r\nEgyptian_HieroglyphsFIELD - LESS THAN %fFUZZY STRING - %s %fIDS_Trinary_OperatorInsufficient StorageMAX_HEADER_LIST_SIZEMSpanList_InsertBackMeroitic_HieroglyphsRequest URI Too LongSIGALRM: alarm clockSIGTERM: terminationSeek: invalid offsetSeek: invalid whenceTerminal_PunctuationUnprocessable Entity[invalid char class]__vdso_clock_gettimeasn1: syntax error: bad defer size classbad font file formatbad system page sizebad use of bucket.bpbad use of bucket.mpbucket name requiredcan't send reply: %schan send (nil chan)close of nil channelconnection error: %sconnection forbiddenconnection timed outexpected branch nodeflag: help requestedfloating point errorforcegc: phase errorgc_trigger underflowgo of nil func valuegopark: bad g statusgzip: invalid headerheader line too longhttp2: stream closedinclude_term_vectorsinvalid DNS responseinvalid doc length 0invalid itab lockinginvalid m->locked = invalid option valueinvalid repeat countinvalid request codeis a named type fileitem already definedjson: Unmarshal(nil json: Unmarshal(nil)key has been revokedmalloc during signalmessage is too shortmust specify layoutsnon-empty swept listnotetsleep not on g0pacer: assist ratio=pad length too largepreempt off reason: received during forkreflect.Value.SetIntreflect.makeFuncStubruntime: casgstatus runtime: double waitruntime: unknown pc runtime: work.nwait=semaRoot rotateRighttime: invalid numbertrace: out of memoryunexpected IP lengthunexpected network: unknown storage typeworkbuf is not empty gp.gcscanvalid=true\r\n of unexported method previous allocCount=%s flag redefined: %s186264514923095703125931322574615478515625@([a-zA-Z0-9_]){1,15}Anatolian_HieroglyphsInscriptional_PahlaviInternal Server ErrorOther_Grapheme_ExtendPrecondition RequiredRoundTrip failure: %vUNKNOWN_FRAME_TYPE_%dUnhandled Setting: %v\tSET INTERNAL - '%s'\r\n_cgo_unsetenv missingadvise_random_on_openafter top-level valueblock device requiredbucket already existsbufio: negative countcheckdead: runnable gcommand not supportedconcurrent map writesdatabase already opendecompression failuredefer on system stackerror while searchingfile resize error: %sfindrunnable: wrong pgcprocs inconsistencyhttp: Handler timeouthttp: nil Request.URLin string escape codeinvalid ALPN protocolinvalid named captureinvalid property namekey is not comparablelink has been severedlocalhost.localdomainlog_file_time_to_rollmin_level_to_compresspackage not installedpage %d already freedpanic on system stackparsenetlinkrouteattrprepare_for_bulk_loadproto: no encoder forread-only file systemreadoptions_read_tierreflect.Value.Complexreflect.Value.Pointerreflect.Value.SetUintreleasep: invalid argruntime: confused by runtime: newstack sp=runtime: work.nwait =sequence tag mismatchstale NFS file handlestartlockedm: m has pstartm: m is spinningstate not recoverablestats_dump_period_secstopg: invalid statustarget_file_size_basetest.blockprofileratetf(termFreq(%s:%s)=%dunhandled operator %dunknown empty Context from SOCKS5 proxy at  into Go struct field %v.WithValue(%#v, %#v)+-=&|><!(){}[]^\"~*?:\\/.localhost.localdomain/etc/apache/mime.types/etc/ssl/ca-bundle.pem/lib/time/zoneinfo.zip/usr/local/share/certs4656612873077392578125DEBUG_HTTP2_GOROUTINESFIELD - %s PHRASE - %sFIELD - %s STRING - %sInscriptional_ParthianMAX_CONCURRENT_STREAMSRunning node: %s : %s\r\nSIGSTKFLT: stack faultSIGTSTP: keyboard stopUnsupported Media TypeX-Content-Type-OptionsXXX_InternalExtensionsaddress already in useargument list too longassembly checks failedauthentication-resultsbad arg in InstCapturebad g->status in readybody closed by handlercannot allocate memoryerror decoding messageerror marshaling statserror parsing regexp: freeIndex is not validgetenv before env initgzip: invalid checksumheader field %q = %q%shpack: string too longhttp2: frame too largehttp: invalid pattern idna: invalid label %qillegal number base %dinappropriate fallbackinplace_update_supportinteger divide by zerointerface conversion: internal inconsistencyinvalid address familymalformed HTTP versionmax_background_flushesmax_manifest_file_sizeminpc or maxpc invalidmissing ']' in addressmust specify formatternetwork is unreachablenon-Go function at pc=openToWriteOrCreate...operation was canceledpage %d: already freedpanic during softfloatproto: bad hexadecimalprotocol not availableprotocol not supportedreadoptions_fill_cachereflect.Value.MapIndexreflect.Value.SetFloatremote address changedruntime.main not on m0runtime: invalid type runtime: out of memoryruntime: s.allocCount=runtime:scanstack: gp=s.freeindex > s.nelemsscanstack - bad statussend on closed channelspan has no free spacestack not a power of 2term_searchers_startedtimer goroutine (idle)trace: alloc too largeunexpected length codewait count dropped < 0write on closed bufferzero length BIT STRING into Go value of type  is not in the Go heap\r\n) must be a power of 2\r\n+-=&|><!(){}[]^\"~*?:\\/ /* %d unknown bytes */\r\n/etc/apache2/mime.types/etc/pki/tls/cacert.pem23283064365386962890625<invalid reflect.Value>FIELD - GREATER THAN %fLogical_Order_ExceptionMB during sweep; swept Noncharacter_Code_PointSIGIO: i/o now possibleSIGSYS: bad system callUnknown field type '%s'Variant Also Negotiatesacquirep: already in goasn1: structure error: bytes.Buffer: too largechan receive (nil chan)close of closed channelcould not build date %vdefault_datetime_parserdevice or resource busyfatal: morestack on g0\r\nflate: internal error: garbage collection scangcDrain phase incorrecthttp2: handler panickedhttp2: invalid trailershttp: request too largeinterrupted system callinvalid URI for requestinvalid boost value: %vinvalid escape sequenceinvalid header receivedjson: cannot unmarshal left over markroot jobsmakechan: bad alignmentmalformed HTTP responsemax_write_buffer_numbermissing port in addressmissing protocol schememissing type in runfinqmisuse of profBuf.writemmap allocate error: %smust specify fragmenternanotime returning zeronet/http: abort Handlerno application protocolno space left on deviceoperation not permittedoperation not supportedpanic during preemptoffprocresize: invalid argproto: integer overflowproxy: SOCKS5 proxy at reflect.Value.Interfacereflect.Value.NumMethodreflect.methodValueCallruntime: internal errorruntime: netpoll failedruntime: sizeof(hmap) =s.allocCount > s.nelemsschedule: holding locksspan has no free stacksstack growth after forksyntax error in patternterm_searchers_finishedtext/css; charset=utf-8text/xml; charset=utf-8time: invalid duration truncated tag or lengthunexpected address typeunexpected map key typeunexpected signal valueunknown error code 0x%xunlock of unlocked lockunsupported certificatevarint integer overflowwork.nwait > work.nprocwrite profiles to `dir` has unexpected version  requires authentication%v.WithDeadline(%s [%s])/usr/share/lib/zoneinfo/116415321826934814453125582076609134674072265625RawSockaddrAny too smallRequest Entity Too Large\tDELETE INTERNAL - '%s'\r\napplication/x-javascriptbad defer entry in panicbad defer size class: i=block index out of rangecan't decode message: %scan't scan our own stackcaught a node panic: %s\r\nconnection reset by peerdisable_auto_compactionsdouble traceGCSweepStarterror creating index: %serror decrypting messageerror parsing number: %vflate: maxBits too largefunction not implementedgcDrainN phase incorrecthash of unhashable type http2: canceling requesthttp: nil Request.Headeridna: disallowed rune %Uincorrect protocol stateinplace_update_num_locksinvalid protocol versioninvalid pseudo-header %qinvalid request :path %qjson: unsupported type: level 2 not synchronizedlink number out of rangemax_bytes_for_level_basemax_mem_compaction_levelmissing TLS certificatesout of streams resourcespage %d: reachable freedproto: missing extensionproto: no coders for %v\r\nput: zero-length new keyput: zero-length old keyqueuefinalizer during GCreflect.Value.SetComplexreflect.Value.UnsafeAddrrunqsteal: runq overflowruntime:      t.spanKey=runtime: C malloc failedruntime: epollwait on fdruntime: found obj at *(runtime: misaligned funcslow search took %s - %vspan has no free objectsstack trace unavailable\r\nstreamSafe was not resetstructure needs cleaningtable_cache_numshardbitstext/html; charset=utf-8unexpected buffer len=%vunknown field type: '%s'weight(^%f), product of:writeoptions_disable_WAL to unused region of span(?U)header\\.d=(.*)(\\s+|$)/proc/sys/kernel/hostname2006-01-02T15:04:05Z07:002910383045673370361328125Content-Transfer-EncodingFIELD - LESS THAN DATE %sIO error: While lock file_cgo_thread_start missingacquirep: invalid p stateallgadd: bad status Gidlebad status in shrinkstackbloom_filter_bits_per_keycan't scan gchelper stackcgo result has Go pointerchansend: spurious wakeupcheckdead: no m for timercheckdead: no p for timercontext deadline exceededexplicit tag has no childhttp2: Framer %p: read %vhttp2: invalid header: %vhttp2: unsupported schemeinconsistent poll.fdMutexinvalid cross-device linkinvalid network interfaceinvalid page type: %d: %xinvalid proxy URL port %qjson: Unexpected key typejson: unsupported value: missing TLS configurationmissing stack in newstackmissing traceGCSweepStartmust specify start or endno answer from DNS serverno buffer space availableno such device or addressno such file or directoryno such network interfaceno suitable address foundoperation now in progressoptimize_for_point_lookuppage %d: invalid type: %spanicwrap: Callers failedproto: bad byte length %dreflect.Value.OverflowIntreflect.Value.SetMapIndexreflect: Bits of nil Typereleasep: invalid p stateresource deadlock avoidedrt_sigaction read failureruntime: bad select size runtime: program exceeds runtime\u00b7lock: lock countslice bounds out of rangesocket type not supportedstartm: p has runnable gsstoplockedm: not runnablestrict-transport-securitysyntax error: unexpected test.mutexprofilefractiontext/plain; charset=utf-8tls: protocol is shutdownunexpected '[' in addressunexpected ']' in addressunexpected fault address unknown Go type for sliceunknown distance unit: %s using unaddressable value/* unknown wire type %d */1455191522836685180664062572759576141834259033203125HTTP Version Not SupportedIndex opened in %s (read)\r\nSIGSTOP: stop, unblockableaddress type not supportedasn1: invalid UTF-8 stringbase 128 integer too largebidirule: failed Bidi Rulebinary.Read: invalid type boolean field has %d bytescannot marshal DNS messagecorrupted semaphore ticketcriterion lacks equal signduplicate pseudo-header %qentersyscall inconsistent forEachP: P did not run fnfreeSpan given cached spanfreedefer with d.fn != nilhttp2: Framer %p: wrote %vid (%v) <= evictCount (%v)initSpan: unaligned lengthinvalid doc length 0 - % xinvalid port %q after hostinvalid prefix coded valueinvalid request descriptorlevel0_stop_writes_triggermalformed HTTP status codemalformed chunked encodingmax_background_compactionsname not unique on networknet/http: request canceledno CSI structure availableno message of desired typenotewakeup - double wakeupout of memory (stackalloc)page %d: out of bounds: %dpersistentalloc: size == 0proto: bad extended type; proto: bad map data tag %dproto: descriptor conflictproto: illegal wireType %dput: zero-length inode keyread from empty dataBufferread: zero-length node keyreadLoopPeekFailLocked: %vreflect.Value.CanInterfacereflect.Value.OverflowUintrequired key not availableruntime: bad span s.state=runtime: writebarrierptr *selectrecv: too many casesselectsend: too many casesskip_log_error_on_recoverystartlockedm: locked to metoo many colons in addresstruncated base 128 integerunclosed criterion bracketunknown search sort by: %s is not assignable to type  rejected username/password!#$%&()*+-./:<=>?@[]^_{|}~ 363797880709171295166015625DATA frame with stream ID 0G waiting list is corruptedIndex opened in %s (write)\r\nIndexed %d lines in %0.3fs\r\nMSpan_Sweep: bad span stateReuse of exported var name:SIGILL: illegal instructionSIGXCPU: cpu limit exceeded\"2006-01-02T15:04:05Z07:00\"access-control-allow-originafter object key:value pairbinary.Write: invalid type cannot free page 0 or 1: %dcaught a sandbox panic: %s\r\ncgoUse should not be calledchannel number out of rangecipher: incorrect length IVcommunication error on senddocument ID cannot be emptyerror building analyzer: %verror recovery discards %s\r\nfieldNorm(field=%s, doc=%s)gcstopm: not waiting for gcgrowslice: cap out of rangehttp chunk length too largehttp2: response body closedidf(docFreq=%d, maxDocs=%d)insufficient security levelinternal lockOSThread errorinvalid HTTP header name %qinvalid dependent stream IDinvalid fuzziness value: %vinvalid page allocation: %dinvalid profile bucket typekey was rejected by servicelarge span treap rotateLeftmakechan: size out of rangemakeslice: cap out of rangemakeslice: len out of rangemanifest_preallocation_sizemust specify stop_token_mapnet/http: invalid method %qnet/http: use last responsenot a XENIX named type fileprogToPointerMask: overflowread: zero-length inode keyreadoptions_verify_checksumreflect.Value.OverflowFloatrestartg: unexpected statusrunlock of unlocked rwmutexruntime: checkdead: find g runtime: checkdead: nmidle=runtime: corrupted polldescruntime: netpollinit failedruntime\u00b7unlock: lock countscanframe: bad symbol tablesearch sort must specify bysignal received during forksigsend: inconsistent statespill: zero-length node keystack size not a power of 2startm: negative nmspinningstopTheWorld: holding lockstarget_file_size_multipliertime: invalid location nametls: short read from Rand: too many transfer encodingsunknown sort field mode: %sunknown sort field type: %sunsupported protocol schemework.nwait was > work.nproc args stack map entries for /proc/sys/net/core/somaxconn/system/etc/security/cacerts18189894035458564758300781259094947017729282379150390625Environment IPC_ROOT not setFIELD - GREATER THAN DATE %sFixedStack is not power-of-2MHeap_FreeSpanLocked - span MSpan_Sweep: m is not lockedPrepended_Concatenation_MarkSIGHUP: terminal line hangupSIGWINCH: window size changeTransfer-Encoding: chunked\r\n_mheap_alloc not on g0 stackbad pointer in write barriercan't dial on rep socket: %scan't get new rep socket: %scan't preserve unlinked spancannot unmarshal DNS messagecomparing uncomparable type crypto/rsa: decryption errordestination address requirederror building token map: %verror building tokenizer: %vfailed getting extension: %vfatal: morestack on gsignal\r\nfile descriptor in bad statefindrunnable: netpoll with pgchelperstart: bad m->helpgcgcstopm: negative nmspinninghttp2: Transport received %shttp2: client conn is closedhttp: no Host in request URLinode overflow: %d (pgid=%d)invalid byte in chunk lengthinvalid header field name %qinvalid proxy address %q: %vinvalid runtime symbol tablejson: Unmarshal(non-pointer json: invalid use of scannerlarge span treap rotateRightmalformed MIME header line: missing stack in shrinkstackmissing status pseudo headermultipart: message too largeneed padding in bucket (key)net: dns: unknown string tagnewproc1: new g is not Gdeadnewproc1: newg missing stacknotewakeup - double wakeup (num_plain_text_bytes_indexedos: process already finishedpage %d: multiple referencespage %d: unreachable unfreedproto: misuse of isMarshalerproto: unhandled def kind %vprotocol driver not attachedreflect.MakeSlice: len > capreflect: In of non-func typereflect: Key of non-map typeruntime: casgstatus: oldval=runtime: no module data for runtime: p.gcMarkWorkerMode=runtime: stat overflow: val strconv: \u03b5 > (den<<shift)/2syntax error scanning numbertls: bad X25519 public valueunexpected end of JSON inputunsupported compression for write: zero-length inode key cannot be converted to type 45474735088646411895751953125Environment SIFT_JSON not setEnvironment SIFT_ROOT not setFIELD - LESS THAN OR EQUAL %fNon-Authoritative InformationProxy Authentication RequiredSIGPIPE: write to broken pipeSIGPWR: power failure restartUnavailable For Legal Reasons_LARGE_STORAGE_rocksdb_store_abi mismatch detected betweenaddspecial on invalid pointerbufio.Scanner: token too longcrypto/aes: invalid key size crypto/des: invalid key size crypto/rc4: invalid key size database is in read-only modedup idle pconn %p in freelisterror building fragmenter: %verror opening index reader %verror recovery pops state %d\r\ngc done but gcphase != _GCoffgfput: bad status (not Gdead)http2: client conn not usablehttp: idle connection timeoutinteger not minimally-encodedinternal error: took too muchinvalid character class rangeinvalid header field value %qinvalid length of trace eventinvalid or unsupported optionio: read/write on closed pipeissue encoding your error: %smachine is not on the networkmanaged tx commit not allowedmime: invalid media parametermismatched local address typeno XENIX semaphores availablenumerical result out of rangeoperation already in progresspadding contained in alphabetprotocol family not supportedreflect: Elem of invalid typereflect: Out of non-func typeruntime: bad g in cgocallbackruntime: impossible type kindruntime: marking free object runtime: mmap: access denied\r\nruntime: split stack overflowruntime: stat underflow: val runtime: sudog with non-nil cruntime: unknown pc in defer selectdefault: too many casesselectgo: case count mismatchsemacquire not on the G stackstring concatenation too longsyntax error scanning booleantls: DialWithDialer timed outtls: invalid NextProtos valuetls: use of closed connectiontoo many open files in systemtraceback has leftover defersunknown IP protocol specifiedunknown certificate authorityunsupported transfer encodingwrite a cpu profile to `file`zero length OBJECT IDENTIFIER locals stack map entries for 'type' property is not defined227373675443232059478759765625HEADERS frame with stream ID 0MHeap_AllocLocked - bad npagesSIGPROF: profiling alarm clockSIGUSR1: user-defined signal 1SIGUSR2: user-defined signal 2SIGVTALRM: virtual alarm clockassignment to entry in nil mapcheckdead: inconsistent countscrypto/dsa: invalid public keycrypto/rsa: verification errorerror building char filter: %verror building highlighter: %verror opening store reader: %verror using analyzer named: %sfailed to get system page sizefreedefer with d._panic != nilhttp2: decoded hpack field %+vhttp: named cookie not presentillegal window increment valuein exponent of numeric literalinappropriate ioctl for deviceinvalid function symbol table\r\ninvalid network interface nameinvalid operation for protocolinvalid pointer found on stacklevel0_slowdown_writes_triggerlooking for beginning of valuemax_bytes_for_level_multipliermime: duplicate parameter nameneed padding in bucket (value)net: dns: unknown packing typepersistConn was already in LRUproto: Marshal called with nilproto: bad default bool %q: %vproto: misuse of isUnmarshalerproto: not an extendable protoprotocol version not supportedprotocol wrong type for socketreflect: Len of non-array typerunqputslow: queue is not fullruntime: bad pointer in frame runtime: found in object at *(socket operation on non-socketstream error: stream ID %d; %vstrings: negative Repeat countsync: inconsistent mutex statesync: unlock of unlocked mutextransform: short source bufferunknown sort field missing: %sx509: DSA verification failure...additional frames elided...\r\n.lib section in a.out corrupted11368683772161602973937988281255684341886080801486968994140625ConstantScore()^%f, product of:FIELD - %s FUZZY STRING - %s %fNetwork Authentication RequiredPRIORITY frame with stream ID 0Request Header Fields Too LargeRequested Range Not SatisfiableSIGSEGV: segmentation violationTLS: sequence number wraparoundaccess_hint_on_compaction_startapplication/json; charset=utf-8bolt.Close(): funlock error: %sbufio.Segmenter: token too longcannot assign requested addresscasgstatus: bad incoming valuescheckmark found unmarked objectentersyscallblock inconsistent error building token filter: %verror receiving from socket: %sfmt: unknown base; can't happenhttp2: connection error: %v: %vin literal null (expecting 'l')in literal null (expecting 'u')in literal true (expecting 'e')in literal true (expecting 'r')in literal true (expecting 'u')inserting span already in treapinternal error - misuse of itabinvalid network interface indexinvalid or unsupported protocoljson: invalid number literal %qmalformed time zone informationmanaged tx rollback not allowedmerge operator returned failuremergeRuneSets odd length []runemergepgids bad len %d < %d + %dmust clause must be conjunctionnet/http: TLS handshake timeoutnon in-use span in unswept listoptimize_level_style_compactionpacer: sweep done at heap size proto: bad default int32 %q: %vproto: bad default int64 %q: %vpurge_redundant_kvs_while_flushreflect.MakeSlice: negative capreflect.MakeSlice: negative lenreflect: NumIn of non-func typeresetspinning: not a spinning mruntime: address space conflictruntime: cannot allocate memoryruntime: split stack overflow: sotypeToNet unknown socket typetime: missing unit in duration tls: ECDSA verification failuretls: no certificates configuredtls: unsupported hash algorithmx509: certificate is valid for /etc/pki/tls/certs/ca-bundle.crt28421709430404007434844970703125Batch (%d ops, %d internal ops)\r\nFIELD - GREATER THAN OR EQUAL %fSIGFPE: floating-point exceptionSIGTTOU: background write to tty^(([a-zA-Z0-9]+\\.)+[a-zA-Z0-9]+)bufio: invalid use of UnreadBytebufio: invalid use of UnreadRunebufio: tried to fill full buffercannot represent %s in this typechacha20poly1305: bad key lengthcrypto/aes: input not full blockgchelper not running on g0 stackgo package net: hostLookupOrder(http2: invalid header field namein literal false (expecting 'a')in literal false (expecting 'e')in literal false (expecting 'l')in literal false (expecting 's')invalid or unsupported transportinvalid range: failed to overlapissue encoding your response: %smime: expected token after slashmin_write_buffer_number_to_mergenon-Go code disabled sigaltstacknumerical argument out of domainpanic while printing panic valueproto: bad default uint32 %q: %vproto: bad default uint64 %q: %vproto: oneof field has nil valueproto: required field %q not setproxy: got unknown address type rate_limit_delay_max_millisecondread limit of %d bytes exhaustedreflect.nameFrom: tag too long: reflect: NumOut of non-func typeremovespecial on invalid pointerresource temporarily unavailableruntime: epollcreate failed withruntime: failed MSpanList_Insertruntime: mcall function returnedruntime: newstack called from g=runtime: stack split at bad timeruntime: sudog with non-nil elemruntime: sudog with non-nil nextruntime: sudog with non-nil prevscanstack: goroutine not stoppedsender IP is ([a-fA-F0-9.:]+)\\)?software caused connection abortstackmapdata: index out of rangesweep increased allocation countsync: Unlock of unlocked RWMutexsync: negative WaitGroup countertls: NextProtos values too largetls: unknown Renegotiation valuetransform: short internal bufferuse of closed network connectionverbose: print additional outputwrite a memory profile to `file`x509: ECDSA verification failurex509: unsupported elliptic curve of method on nil interface value142108547152020037174224853515625710542735760100185871124267578125SIGCHLD: child status has changedSIGTTIN: background read from ttySIGXFSZ: file size limit exceededbacktrack called for a RuneReadercipher: message too large for GCMconcurrent map read and map writecrypto/aes: output not full blockcrypto: requested hash function #findrunnable: negative nmspinningfreeing stack not in a stack spanheapBitsSetType: unexpected shifthttp2: invalid header field valuehttp2: invalid pseudo headers: %vhttp2: recursive push not allowedhttp: CloseIdleConnections calledhttp: invalid Read on closed Bodyhttp: multiple registrations for indefinite length found (not DER)index read inconsistency detectedleafCounts[maxBits][maxBits] != nmax_sequential_skip_in_iterationsmisplaced bucket header: %x -> %xnet/http: skip alternate protocolno analyzer named '%s' registeredpad size larger than data payloadproto: bad default float32 %q: %vproto: bad default float64 %q: %vproto: type must have kind structproxy: port number out of range: pseudo header field after regularreflect.nameFrom: name too long: reflect: ChanDir of non-chan typereflect: Field index out of rangereflect: Field of non-struct typereflect: array index out of rangereflect: slice index out of rangeregexp: unhandled case in compilerun at most `n` tests in parallelruntime: castogscanstatus oldval=runtime: goroutine stack exceeds runtime: name offset out of rangeruntime: text offset out of rangeruntime: type offset out of rangeshould clause must be disjunctionstackalloc not on scheduler stackstoplockedm: inconsistent lockingstruct contains unexported fieldssync: RUnlock of unlocked RWMutextls: failed to write to key log: tls: unexpected ServerKeyExchangetoo many levels of symbolic linkstreap insert finds a broken treaptreap node with nil spanKey foundunexpected bucket header flag: %xwaiting for unsupported file type/etc/ssl/certs/ca-certificates.crt3552713678800500929355621337890625FIELD - LESS THAN OR EQUAL DATE %sMHeap_AllocLocked - MSpan not freeMSpan_EnsureSwept: m is not lockedOther_Default_Ignorable_Code_PointSIGURG: urgent condition on socketadding nil Certificate to CertPoolbad wiretype for oneof field in %Tbolt.DB.meta(): invalid meta pagescrypto/rsa: missing public modulusdereference: zero-length inode keyforEachP: sched.safePointWait != 0geo location not in a valid formatheapBitsForSpan: base out of rangehttp2: aborting request body writehttp: connection has been hijackedhttp: persistConn.readLoop exitinghttp: read on closed response bodyillegal base64 data at input byte in \\u hexadecimal character escapeinvalid childAt(%d) on a leaf nodeinvalid nested repetition operatorinvalid or unsupported Perl syntaxinvalid padding bits in BIT STRINGlevel0_file_num_compaction_triggeropenToWriteOrCreate failed becauseproto: tag has too few fields: %q\r\nqueryWeight(%s:%s^%f), product of:reflect.FuncOf: too many argumentsreflect: Field index out of boundsreflect: Method index out of rangereflect: string index out of rangerun tests and benchmarks `n` timesruntime.SetFinalizer: cannot pass runtime: g is running but p is notruntime: unexpected return pc for schedule: spinning with local worksearchResults for domain: %s (%d)\r\nspan and treap sizes do not match?stream error: stream ID %d; %v; %vtls: unknown record type requestedtoo many references: cannot splicewrite a coverage profile to `file`write an execution trace to `file`x509: unhandled critical extension177635683940025046467781066894531252006-01-02T15:04:05.999999999Z07:0088817841970012523233890533447265625CONTINUATION frame with stream ID 0MHeap_FreeSpanLocked - invalid freeanalyzer named '%s' already definedcannot open index, metadata corruptcannot open index, metadata missingdelete_obsolete_files_period_microsdynamic table size update too largeencoding/hex: odd length hex stringerror building date time parser: %vfindfunc: bad findfunctab entry idxfindrunnable: netpoll with spinningflate: corrupt input before offset greyobject: obj not pointer-alignedhandshake should have had a result.hpack: invalid Huffman-encoded datahttp: server closed idle connectionmime: bogus characters after %%: %qmime: invalid RFC 2047 encoded-wordmisrounded allocation in mallocinitmust not clause must be disjunctionnetwork dropped connection on resetno such multicast network interfaceoptimize_universal_style_compactionpersistentalloc: align is too largepidleput: P has non-empty run queueproto: message encodes to over 2 GBproto: no slice oenc for %T = []%T\r\nreflect.MakeSlice of non-slice typerun each benchmark for duration `d`run smaller test suite to save timeruntime: close polldesc w/o unblockruntime: inconsistent read deadlinesuperfluous leading zeros in lengthtable_cache_remove_scan_count_limittraceback did not unwind completelytransform: short destination buffertransport endpoint is not connectedunmarshaling the rpc request failedweight(%s:%s^%f in %s), product of:write: circular dependency occurredx509: decryption password incorrect LastStreamID=%v ErrCode=%v Debug=%q0123456789abcdefghijklmnopqrstuvwxyz444089209850062616169452667236328125Go pointer stored into non-Go memoryIA5String contains invalid characterMStats vs MemStatsType size mismatchaccessing a corrupted shared librarycrypto/cipher: input not full blockserror building stop words filter: %verror creating index, retries failedfile type does not support deadlineshttp: no Location header in responsehttp: unexpected EOF reading trailerkey size not a multiple of key alignmarkrootSpans during second markrootno highlighter named `%s` registeredparent must have at least 2 childrenpgid (%d) above high water mark (%d)proto: textWriter unindented too farproxy: failed to parse port number: reflect: IsVariadic of non-func typereflect: NumField of non-struct typereflect: funcLayout of non-func typeruntime: bad notifyList size - sync=runtime: failed MSpanList_InsertBackruntime: inconsistent write deadlineruntime: invalid pc-encoded table f=runtime: invalid typeBitsBulkBarrierruntime: mcall called on m->g0 stackruntime: sudog with non-nil waitlinkruntime: unblock on closing polldescruntime: wrong goroutine in newstacksift.json does not contain any nodessignal arrived during cgo execution\r\nsyntax error scanning complex numbertesting: unexpected use of func Maintoken map named '%s' already definedtokenizer named '%s' already definedunable to build regexp tokenizer: %vuncaching span but s.allocCount == 0unsupported SSLv2 handshake receivedx509: zero or negative DSA parameter) is smaller than minimum page size (2220446049250313080847263336181640625FIELD - GREATER THAN OR EQUAL DATE %s_cgo_notify_runtime_init_done missingall goroutines are asleep - deadlock!bytes.Buffer: truncation out of rangecannot exec a shared library directlychacha20poly1305: plaintext too largecipher: message authentication failedcrypto/rsa: public exponent too largecrypto/rsa: public exponent too smallcrypto/rsa: unsupported hash functioncrypto: Size of unknown hash functionerror building fragment formatter: %verror using datetime parser named: %sexplicitly tagged member didn't matchfieldWeight(%s:%s in %s), product of:incompatible version, %d is supportedinvalid term frequency value, no normnode has no large storage with name: operation not possible due to RF-killproto does not have a message type IDproto: no protobuf decoder for %s.%s\r\nproto: repeated field has nil elementproto: tag has unknown wire type: %q\r\nreflect.Value.Bytes of non-byte slicereflect.Value.Bytes of non-rune slicereflect.Value.Convert: value of type reflect: Bits of non-arithmetic Type run only benchmarks matching `regexp`runtime: address space conflict: map(runtime: allocation size out of rangesetprofilebucket: profile already setstartTheWorld: inconsistent mp->nextpstrings: Repeat count causes overflowunable to parse geo_distance locationunexpected CONTINUATION for stream %dunicode word segmentation parse errorvalue too large for defined data typex509: RSA key missing NULL parameters1110223024625156540423631668090820312555511151231257827021181583404541015625after decimal point in numeric literalarg size to reflect.call more than 1GBcan not access a needed shared librarycannot open index, path does not existcannot resolve '%s' tokenizer type: %schacha20poly1305: ciphertext too largechar filter named '%s' already definedconcurrent map iteration and map writeencoding alphabet is not 64-bytes longgcBgMarkWorker: blackening not enabledgo package net: using cgo DNS resolverinternal error: unknown string type %dmakechan: invalid channel element typemime: expected slash after first tokennet/http: invalid header field name %qproto: no ptr oenc for %T -> %T -> %T\r\nproxy: destination hostname too long: runtime: blocked read on free polldescruntime: function symbol table header:runtime: sudog with non-nil selectdonesyscall: readInt with unsupported sizetime: missing Location in call to Datetls: failed to sign ECDHE parameters: tls: invalid ClientKeyExchange messagetls: invalid ServerKeyExchange messagetls: missing ServerKeyExchange messagetls: server selected unsupported curvetls: unknown ECDHE signature algorithmtls: unsupported signing key type (%T)x509: cannot validate certificate for x509: trailing data after X.509 key-id because it doesn't contain any IP SANs2006-01-02 15:04:05.999999999 -0700 MST277555756156289135105907917022705078125MSpan_Sweep: bad span state after sweepUnmarshal the entry to index failed: %scannot perform operation on empty aliasfield mapping contains invalid keys: %vheapBitsSetTypeGCProg: small allocationhttp: putIdleConn: keep alives disabledindex mapping contains invalid keys: %vinvalid indexed representation index %dinvalid term frequency key, empty docidmisrounded allocation in MHeap_SysAllocmissing argument to repetition operatorprint memory allocations for benchmarksreceived on thread with no signal stackreflect.MakeMapWithSize of non-map typereflect: FieldByName of non-struct typeruntime: blocked write on free polldescruntime: casfrom_Gscanstatus failed gp=runtime: typeBitsBulkBarrier with type span and treap node npages do not matchstack growth not allowed in system callstrings.NewReplacer: odd argument counttags don't match (%d vs %+v) %+v %s @%dtoken filter named '%s' already definedtrailing backslash at end of expressiontransport endpoint is already connectedx509: trailing data after DSA signaturex509: trailing data after X.509 subject'type' property must be a string, not %T13877787807814456755295395851135253906256938893903907228377647697925567626953125Frame accessor called on non-owned Frameaddress family not supported by protocolbulkBarrierPreWrite: unaligned argumentscannot free workbufs when work.full != 0cannot marshal match operator %d to JSONcrypto/cipher: output smaller than inputcrypto/rsa: input must be hashed messagedate ranges contains duplicate name '%s'deferproc: d.panic != nil after newdeferevictOldest(%v) on table with %v entrieshttp2: Transport encoding header %q = %qhttp2: invalid pseudo header in trailershttp2: timeout awaiting response headersno pattern found in 'exception' propertyoversized record received with length %dproto: %s: illegal tag %d (wire type %d)proto: no encoder function for %v -> %v\r\nreceived but handler not on signal stackrefill of span with free space remainingreflect.Value.SetBytes of non-byte slicereflect.Value.setRunes of non-rune slicereflect: FieldByIndex of non-struct typeruntime.SetFinalizer: first argument is runtime: out of memory: cannot allocate term range query must specify min or maxtime: Stop called on uninitialized Timertls: ECDHE RSA requires a RSA server keytls: client didn't provide a certificatevalue size not a multiple of value alignx509: trailing data after DSA parametersx509: trailing data after DSA public keyx509: trailing data after RSA public keyx509: trailing data after X.509 KeyUsage34694469519536141888238489627838134765625MHeap_FreeSpanLocked - invalid span stateMHeap_FreeSpanLocked - invalid stack freeasn1: internal error in parseTagAndLengthbinary: varint overflows a 64-bit integerbytes.Buffer.WriteTo: invalid Write countbytes.Reader.WriteTo: invalid Write countcan't call pointer on a non-pointer Valuecannot shift %d, must be between 0 and 63cgo argument has Go pointer to Go pointererror serializing explanation to json: %vgcSweep being done but phase is not GCoffhttp2: invalid Upgrade request header: %qhttp2: no cached connection was availableidna: internal error in punycode encodingjson: error calling MarshalJSON for type reflect.Value.Addr of unaddressable valueruntime.SetFinalizer: second argument is runtime.makemap: unsupported map key typeruntime: blocked read on closing polldescruntime: typeBitsBulkBarrier without typesearch sort mode field must specify fieldsetCheckmarked and isCheckmarked disagreestopTheWorld: not stopped (stopwait != 0)strconv: illegal AppendInt/FormatInt basetime: Reset called on uninitialized Timertls: failed to parse client certificate #tls: failed to parse client certificate: tls: handshake has not yet been performedtls: no supported elliptic curves offeredtls: unsupported decryption key type (%T)transport got GOAWAY with error code = %vx509: trailing data after ECDSA signaturex509: trailing data after X.509 extension173472347597680709441192448139190673828125867361737988403547205962240695953369140625Dictionary Term: `%s` Field: %d Count: %d PRIORITY frame payload size was %d; want 5PrintableString contains invalid characteracquireSudog: found s.elem != nil in cachedocument mapping contains invalid keys: %vhttp: ContentLength=%d with Body length %dinvalid HTTP header value %q for header %qinvalid term frequency key, no valid fieldlooking for beginning of object key stringmalformed non-numeric status pseudo headermix of request and response pseudo headersmust specify tokenizer for remaining inputpersistentalloc: align is not a power of 2proto: invalid google.protobuf.Any messageproto: wrong wireType = %d for field Fieldproto: wrong wireType = %d for field Termsruntime: blocked write on closing polldescruntime: heapBitsSetTypeGCProg: total bitssync/atomic: store of nil value into Valueunexpected signal during runtime executionx509: RSA modulus is not a positive numberx509: trailing data after ECDSA parameters%d matches, showing %d through %d, took %s\r\nError enabling Transport HTTP/2 support: %vInternalStore - Key: %s (% x) Val: %s (% x)Requested to run a non-Go node at index %d\r\nTooManyClauses[maxClauseCount is set to %d]Transport: unhandled response frame type %Tbufio: tried to rewind past start of buffercipher: incorrect nonce length given to GCMdate time parser named '%s' already definedgcBgMarkWorker: unexpected gcMarkWorkerModegentraceback before goexitPC initializationgeo location top_left not in a valid formatheapBitsSetTypeGCProg: unexpected bit counthttp2: unexpected ALPN protocol %q; want %qinterrupted system call should be restartedmultiple Read calls return no data or errornet/http: timeout awaiting response headersnumeric range query must specify min or maxnumeric ranges contains duplicate name '%s'phrase query must contain at least one termproto: %s: wiretype end group for non-groupreflect: FieldByNameFunc of non-struct typereflect: nil type passed to Type.Implementsruntime.SetFinalizer: finalizer already setruntime.SetFinalizer: first argument is nilruntime: casfrom_Gscanstatus bad oldval gp=runtime: releaseSudog with non-nil gp.paramtls: client's Finished message is incorrecttls: unsupported hash function used by peertransform: inconsistent byte count returnedunknown runnable goroutine during bootstrap using value obtained using unexported fieldDictionaryRow parse Uvarint error, nread: %dbleve not configured for file based indexingcannot create new index, path already existscipher: NewGCM requires 128-bit block cipherencoding alphabet contains newline charactergcmarknewobject called while doing checkmarkhttp2: could not negotiate protocol mutuallyhttp2: invalid Connection request header: %qhttp: Request.ContentLength=%d with nil Bodyhttp: putIdleConn: too many idle connectionsmime: unexpected content after media subtypereflect: funcLayout with interface receiver reflect: slice length out of range in SetLenruntime: lfstack.push invalid packing: node=something went wrong with parsing RequestURIsystemstack called from unexpected goroutinetls: failed to verify client's certificate: tls: server's Finished message was incorrectuse of WriteTo with pre-connected connectionwrite a goroutine blocking profile to `file`cannot send after transport endpoint shutdowncontext: internal error: missing cancel errorcrypto: RegisterHash of unknown hash functionexitsyscall: syscall frame is no longer validfreelist pgid (%d) above high water mark (%d)heapBitsSetType: called with non-pointer typehttp: putIdleConn: connection is in bad stateinvalid request :path %q from URL.Opaque = %qjson.RawMessage: UnmarshalJSON on nil pointermath/big: cannot unmarshal %q into a *big.Intno analyzer with name or type '%s' registeredproto: can't skip unknown wire type %d for %sproto: internal error: bad wiretype for oneofproto: no slice elem oenc for %T -> %T -> %T\r\nreflect: internal error: invalid method indexreflect: nil type passed to Type.AssignableTorun only tests and examples matching `regexp`runtime: failed MSpanList_Remove span.npages=tls: ECDHE ECDSA requires an ECDSA server keytransform: input and output are not identicalx509: certificate signed by unknown authorityzero length explicit tag was not an asn1.Flagbytes.Reader.UnreadByte: at beginning of slicecannot unmarshal match operator '%v' from JSONcipher.NewCTR: IV length must equal block sizefirst path segment in URL cannot contain colonhttp2: Transport creating client conn %p to %vinline bucket non-zero page access(2): %d != 0invalid latitude %f; must be between %f and %fmath/big: mismatched montgomery number lengthsno token map with name or type '%s' registeredno tokenizer with name or type '%s' registeredpanicwrap: unexpected string after type name: reflect.Value.Slice: slice index out of boundsreflect: nil type passed to Type.ConvertibleToruntime: failed to create new OS thread (have runtime: name offset base pointer out of rangeruntime: panic before malloc heap initialized\r\nruntime: text offset base pointer out of rangeruntime: type offset base pointer out of rangestopTheWorld: not stopped (status != _Pgcstop)tls: failed to parse certificate from server: tls: server chose an unconfigured cipher suitex509: failed to unmarshal elliptic curve pointP has cached GC work at end of mark terminationattempting to link in too many shared librariesbufio: reader returned negative count from Readchacha20poly1305: message authentication failedfirst record does not look like a TLS handshakegeo location bottom_right not in a valid formatinvalid longitude %f; must be between %f and %finvalid term frequency value, invalid frequencynet/http: Transport.DialTLS returned (nil, nil)no fragmenter with name or type '%s' registeredreflect.Value.UnsafeAddr of unaddressable valuetls: ECDHE RSA requires a RSA server public keytls: handshake did not verify certificate chaintls: incorrect renegotiation extension contentstls: preferredCurves includes unsupported curveunexpected error parsing dictionary row val: %vx509: trailing data after X.509 NameConstraintsP has unflushed stats at end of mark terminationTime.MarshalJSON: year outside of range [0,9999]Time.MarshalText: year outside of range [0,9999]accessing a node with a zero-length cursor stackattempted to register duplicate store named '%s'bufio.Scan: 100 empty tokens without progressingbufio: writer returned negative count from Writeif >= 0, calls runtime.SetMutexProfileFraction()no char filter with name or type '%s' registeredno highlighter with name or type '%s' registeredphrase searcher error building term searcher: %vproto: negative length found during unmarshalingproxy: failed to read port from SOCKS5 proxy at root bucket pgid (%d) above high water mark (%d)runtime: cannot map pages in arena address spacesearch sort mode geo_distance must specify fieldstrconv: illegal AppendFloat/FormatFloat bitSizetls: CloseWrite called before handshake completetls: server advertised unrequested NPN extensiontrailer header without chunked transfer encodingunable to parse datetime with any of the layoutsx509: trailing data after X.509 BasicConstraintsx509: trailing data after X.509 ExtendedKeyUsagex509: trailing data after X.509 authority key-id/etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pemcasgstatus: waiting for Gwaiting but is Grunnablechacha20poly1305: bad nonce length passed to Openchacha20poly1305: bad nonce length passed to Sealgo package net: dynamic selection of DNS resolverhttp2: request body closed due to handler exitinghttp: wrote more than the declared Content-Lengthinvalid memory address or nil pointer dereferenceinvalid or incomplete multibyte or wide characternet/http: Transport.Dial hook returned (nil, nil)no token filter with name or type '%s' registeredpanicwrap: unexpected string after package name: proto: wrong wireType = %d for field TermsEntriesreflect.Value.Slice: slice of unaddressable arrays.allocCount != s.nelems && freeIndex == s.nelemsstrconv: internal error, rest != 0 but needed > 0strconv: num > den<<shift in adjustLastDigitFixedstrings.Reader.UnreadByte: at beginning of stringstrings.Reader.WriteTo: invalid WriteString counttls: client using inappropriate protocol fallbacktls: server advertised unrequested ALPN extensionwritebarrierptr_prewrite1 called with mp.p == nilx509: certificate contained IP address of length x509: certificate has expired or is not yet validattempt to execute system stack code on user stackdereference: zero-length node key on existing nodegot CONTINUATION for stream %d; expected stream %dhttp: not caching alternate protocol's connectionshttp: putIdleConn: CloseIdleConnections was calledhttp: suspiciously long trailer after chunked bodyinvalid term frequency value, vector field invalidmallocgc called with gcphase == _GCmarkterminationnet/http: HTTP/1.x transport connection broken: %vnet/http: Transport failed to read from server: %vnet/http: invalid header field value %q for key %vproto: wrong wireType = %d for field StoredEntriesrecursive call during initialization - linker skewx509: RSA public exponent is not a positive numberx509: missing ASN.1 contents; use ParseCertificateJSON decoder out of sync - data changing underfoot?attempted to register duplicate analyzer named '%s'error parsing mapping JSON: %v\r\nmapping contents:\r\n%shttp2: invalid Transfer-Encoding request header: %qproto: bad extension number; not in declared rangesproto: wrong wireType = %d for field ArrayPositionsprotocol error: received %T before a SETTINGS frameproxy: failed to read address from SOCKS5 proxy at proxy: failed to write greeting to SOCKS5 proxy at runtime: cannot reserve arena virtual address spacetls: ECDHE ECDSA requires a ECDSA server public keytls: VerifyHostname called on TLS server connectiontls: server advertised both NPN and ALPN extensionstls: server selected unsupported compression formattls: server's identity changed during renegotiation&document.DateField{Name:%s, Options: %s, Value: %s}attempted to register duplicate token map named '%s'attempted to register duplicate tokenizer named '%s'casfrom_Gscanstatus: gp->status is not in scan statecrypto/rsa: message too long for RSA public key sizefunction symbol table not sorted by program counter:http2: Transport readFrame error on conn %p: (%T) %vhttp: method cannot contain a Content-Length; got %qinvalid term frequency value, vector contains no endproxy: failed to read greeting from SOCKS5 proxy at runtime.SetFinalizer: pointer not in allocated blockruntime: use of FixAlloc_Alloc before FixAlloc_Init\r\ntls: bad signature type for client's RSA certificatetls: server selected unsupported protocol version %xx509: cannot verify signature: insecure algorithm %vx509: trailing data after X.509 certificate policiesStart: %d  End: %d  Position: %d  Token: %s  Type: %dattempted to register duplicate fragmenter named '%s'http2: Framer %p: failed to decode just-written framehttp2: Transport failed to get client conn for %s: %vhttp: putIdleConn: too many idle connections for hostillegal use of AllowIllegalReads with ReadMetaHeadersno date time parser with name or type '%s' registeredreflect.Value.Slice: string slice index out of boundsreflect: non-interface type passed to Type.Implementstls: client does not support uncompressed connectionstls: client doesn't support any common hash functionstls: unsupported hash function for client certificatex509: DSA signature contained zero or negative valuesx509: certificate specifies an incompatible key usagex509: trailing data after X.509 authority informationField: %d Pos: %d Start: %d End %d ArrayPositions: %#v\tgoroutine running on other thread; stack unavailable\r\nattempted to register duplicate highlighter named '%s'gcControllerState.findRunnable: blackening not enabledhttp: Request.Write on Request with no Host or URL setinvalid term frequency value, vector contains no startno goroutines (main called runtime.Goexit) - deadlock!reflect.FuncOf does not support more than 50 argumentstls: ECDSA signature contained zero or negative valuestls: bad signature type for client's ECDSA certificatetls: failed to create cipher while encrypting ticket: tls: server resumed a session with a different versiontls: unsupported signature type for client certificatex509: cannot verify signature: algorithm unimplementedx509: trailing data after X.509 CRL distribution point&document.BooleanField{Name:%s, Options: %s, Value: %s}&document.NumericField{Name:%s, Options: %s, Value: %s}attempted to register duplicate token filter named '%s'bufio.Scanner: SplitFunc returns negative advance countcasfrom_Gscanstatus:top gp->status is not in scan statecipher.NewCBCDecrypter: IV length must equal block sizecipher.NewCBCEncrypter: IV length must equal block sizegentraceback callback cannot be used with non-zero skipnet/http: request canceled while waiting for connectionnewproc: function arguments too large for new goroutineno fragment formatter with name or type '%s' registeredphrase searcher error building conjunction searcher: %vproxy: no support for SOCKS5 proxy connections of type reflect.FuncOf: last arg of variadic func must be slicereflect: internal error: invalid use of makeMethodValuex509: ECDSA signature contained zero or negative valuesx509: failed to load system roots and no roots providedx509: too many intermediates for path length constraint&document.GeoPointField{Name:%s, Options: %s, Value: %s}b4050a850c04b3abf54132565044b0b7d7bfd8ba270b39432355ffb4b70e0cbd6bb4bf7f321390b94a03c1d356c21122343280d6115c1d21bd376388b5f723fb4c22dfe6cd4375a05a07476444d5819985007e34http2: response header list larger than advertised limitin gcMark expecting to see gcphase as _GCmarkterminationpanic test binary after duration `d` (0 means unlimited)set memory profiling `rate` (see runtime.MemProfileRate)tls: could not validate signature of connection nonces: tls: no cipher suite supported by both client and serverattempted to register duplicate index encoding named '%s'bufio.Segmenter: SplitFunc returns negative advance countgentraceback cannot trace user goroutine on its own stackinvalid term frequency value, vector contains no positionnon-Go code set up signal handler without SA_ONSTACK flagproxy: failed to read connect reply from SOCKS5 proxy at proxy: failed to read domain length from SOCKS5 proxy at received record with version %x when expecting version %xruntime:stoplockedm: g is not Grunnable or Gscanrunnable\r\nsync: WaitGroup misuse: Add called concurrently with Waittls: application data record requested while in handshake&document.Document{ID:%s, Fields: %s, CompositeFields: %s}batch function returned an error and should be re-run solocomma-separated `list` of cpu counts to run each test withproxy: failed to write connect request to SOCKS5 proxy at Backindex DocId: `%s` Terms Entries: %v, Stored Entries: %vattempted to register duplicate date time parser named '%s'bufio.Scanner: SplitFunc returns advance count beyond inputhttp2: Transport received Server's graceful shutdown GOAWAYreflect: indirection through nil pointer to embedded structruntime: mmap: too much locked memory (check 'ulimit -l').\r\nsync/atomic: store of inconsistently typed value into Valuesync: WaitGroup is reused before previous Wait has returnedtls: server resumed a session with a different cipher suitego package net: GODEBUG setting forcing use of Go's resolvernet/http: server response headers exceeded %d bytes; abortedruntime: may need to increase max user processes (ulimit -u)tls: initial handshake had non-empty renegotiation extensionattempted to register duplicate fragment formatter named '%s'bufio.Segmenter: SplitFunc returns advance count beyond inputcannot perform single index operation on multiple index aliasfacet can only conain numeric ranges or date ranges, not bothproto: bad wiretype for field %s.%s: got wiretype %d, want %dreflect: creating a name with a package path is not supportedset blocking profile `rate` (see runtime.SetBlockProfileRate)tls: certificate private key does not implement crypto.SignerDocument: %s Field %d, Array Positions: %v, Type: %s Value: %sfound bad pointer in Go heap (incorrect use of unsafe or cgo?)http2: ConfigureTransport is only supported starting at Go 1.6invalid term frequency key, no byte separator terminating termruntime: internal error: misuse of lockOSThread/unlockOSThreadx509: certificate is not authorized to sign other certificateshttp2: push would exceed peer's SETTINGS_MAX_CONCURRENT_STREAMSinternal error: exactly one of res or err should be set; nil=%vlist tests, examples, and benchmarch maching `regexp` then exit4fe342e2fe1a7f9b8ee7eb4a7c0f9e162bce33576b315ececbb6406837bf51f55ac635d8aa3a93e7b3ebbd55769886bc651d06b0cc53b0f63bce3c3e27d2604b6b17d1f2e12c4247f8bce6e563a440f277037d812deb33a0f4a13945d898c296ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_http: request method or response status code does not allow bodyjson: invalid number literal, trying to unmarshal %q into Numberproxy: failed to read authentication reply from SOCKS5 proxy at tls: certificate private key does not implement crypto.Decrypterinvalid term frequency value, vector contains no arrayPositionLenproxy: failed to write authentication request to SOCKS5 proxy at runtime.SetFinalizer: pointer not at beginning of allocated blockstrconv: internal error: extFloat.FixedDecimal called with n == 0x509: issuer name does not match subject from issuing certificatetls: client offered an unsupported, maximum protocol version of %xwrite a mutex contention profile to the named file after executionx509: certificate is not valid for any names, but wanted to match Term: `%s` Field: %d DocId: `%s` Frequency: %d Norm: %f Vectors: %vgo package net: built with netgo build tag; using Go's DNS resolvertls: handshake or ChangeCipherSpec requested while not in handshake2695994666715063979466701508701962594045780771442439172168272236806126959946667150639794667015087019630673557916260026308143510066298881http2: Transport conn %p received error from processing frame %v: %vhttp2: Transport received unsolicited DATA frame; closing connectionhttp: message cannot contain multiple Content-Length headers; got %qpadding bytes must all be zeros unless AllowIllegalWrites is enabledruntime:greyobject: checkmarks finds unexpected unmarked object obj=http2: Transport closing idle conn %p (forSingleUse=%v, maxStream=%v)phrase searcher error building term position disjunction searcher: %vtls: handshake message of length %d bytes exceeds maximum of %d bytesbytes.Buffer: UnreadByte: previous operation was not a successful readgot %s for stream %d; expected CONTINUATION following %s for stream %djson: invalid use of ,string struct tag, trying to unmarshal %q into %vtls: client's certificate contains an unsupported public key of type %Ttls: server's certificate contains an unsupported type of public key: %Tboolean query must contain at least one must or should or not must clausedisjunction query has fewer than the minimum number of clauses to satisfytls: received unexpected handshake message of type %T when waiting for %Tinvalid term frequency value, vector contains no arrayPosition of index %dnet/http: server replied with more than declared Content-Length; truncatedtls: no supported signature algorithm found for signing client certificateUnsolicited response received on idle HTTP channel starting with %q; err=%vdate range query must specify either start, end or both for range name '%s'handshake should not have been able to complete after handshakeCond was setnumeric range query must specify either min, max or both for range name '%s'runtime: found space for saved base pointer, but no framepointer experiment\r\n115792089210356248762697446949407573529996955224135760342422259061068512044369115792089210356248762697446949407573530086143415290314195533631308867097853951tls: client certificate private key of type %T does not implement crypto.Signertls: either ServerName or InsecureSkipVerify must be specified in the tls.Configx509: invalid signature: parent certificate cannot sign this kind of certificatex509: a root or intermediate certificate is not authorized to sign in this domain (possibly because of %q while trying to verify candidate authority certificate %q)json: invalid use of ,string struct tag, trying to unmarshal unquoted value into %vreflect.Value.Interface: cannot return value obtained from unexported field or method&document.TextField{Name:%s, Options: %s, Analyzer: %v, Value: %s, ArrayPositions: %v}http2: server sent GOAWAY and closed the connection; LastStreamID=%v, ErrCode=%v, debug=%qnet/http: refusing to use HTTP_PROXY value in CGI environment; see golang.org/s/cgihttpproxya handshake hash for a client-certificate was requested after discarding the handshake bufferhttp2: Transport received Server's graceful shutdown GOAWAY; some request body already writtentls: failed to sign handshake with client certificate: unknown client certificate key type: %T3617de4a96262c6f5d9e98bf9292dc29f8f41dbd289a147ce9da3113b5f0b8c00a60b1ce1d7e819d7a431d7c90ea0e5faa87ca22be8b05378eb1c71ef320ad746e1d3b628ba79b9859f741e082542a385502f25dbf55296c3a545e3872760ab7b3312fa7e23ee7e4988e056be3f82d19181d9c6efe8141120314088f5013875ac656398d8a2ed19d2a85c8edd3ec2aefasn1: time did not serialize back to the original value and may be invalid: given %q, but serialized as %q3940200619639447921227904010014361380507973927046544666794829340424572177149687032904726608825893800186160697311231939402006196394479212279040100143613805079739270465446667946905279627659399113263569398956308152294913554433653942643c6858e06b70404e9cd9e3ecb662395b4429c648139053fb521f828af606b4d3dbaa14b5e77efe75928fe1dc127a2ffa8de3348b3c1856a429bf97e7e31c2e5bd66051953eb9618e1c9a1f929a21a0b68540eea2da725b99b315f3b8b489918ef109e156193951ec7e937b1652c0bd3bb1bf073573df883d2c34f1ef451fd46b503f0011839296a789a3bc0045c8a5fb42c7d1bd998f54449579b446817afbd17273e662c97ee72995ef42640c550b9013fad0761353c7086a272c24088be94769fd16650http2: Transport: peer server initiated graceful shutdown after some of Request.Body was written; define Request.GetBody to avoid this error6864797660130609714981900799081393217269435300143305409394463459185543183397656052122559640661454554977296311391480858037121987999716643812574028291115057151686479766013060971498190079908139321726943530014330540939446345918554318339765539424505774633321719753296399637136332111386476861244038034037280889270700544900010203040506070809101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\r\n                                                                                                                                                                                \t??????\r\n                                                                                                                                                                                              37?/*\r\n                                                                                                                                                                                                   &\r\n                                                                                                                                                                                                    *\r\n                                                                                                                                                                                                     ;\r\n                                                                                                                                                                                                      9\r\n        1                                                                                                                                                                                              %??????????????????>'\r\n  property collectors names: []\r\n  SST file compression algo: Snappy\r\n  creation time: 1512634582\r\n  # deleted keys: 0\r\n  # merge operands: 30\r\nRaw user collected properties\r\n------------------------------\r\n  # rocksdb.block.based.table.index.type: 0x00000000\r\n  # rocksdb.block.based.table.prefix.filtering: 0x30\r\n  # rocksdb.block.based.table.whole.key.filtering: 0x31\r\n  # rocksdb.deleted.keys: 0x00\r\n  # rocksdb.merge.operands: 0x1E\r\n```\r\n\r\n### Steps to reproduce the behavior\r\n\r\nThis happens on RocksDBv5.8 used as storage for a [Bleve](http://www.blevesearch.com/) Index\r\nand I don't think the file is corrupted.\r\n\r\n```\r\n$> sst_dump --verify_checksum --command=check --file=138918.sst\r\nfrom [] to []\r\nProcess 138918.sst\r\nSst file format: block-based\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3245/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thatsafunnyname": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3229", "title": "Write (and read) amplification when using level_compaction_dynamic_level_bytes=true and target_file_size_multiplier!=1 (and a delete workload reproduced with db_bench ).", "body": "Hello and thanks for RocksDB.\r\n\r\nWhile testing `level_compaction_dynamic_level_bytes=true` on a workload with deletes and where `target_file_size_multiplier!=1` I noticed a significant increase in write (and read) amplification.\r\nI have included runs of db_bench (from rocksdb-5.8.7) that reproduce similar behaviour to that observed.\r\n\r\nSummary of runs:\r\n\r\n    level_compaction_dynamic_level_bytes\r\n            target_file_size_multiplier \r\n               Cumulative compaction\r\n    true    2  107.84 GB write, 104.67 GB read\r\n    true    1   59.35 GB write,  57.22 GB read\r\n    false   1   53.75 GB write,  51.71 GB read\r\n    false   2   61.73 GB write,  59.60 GB read \r\n\r\nI am wondering if this behaviour is expected or not, and worth mentioning in the documentation:\r\n\r\n  https://github.com/facebook/rocksdb/wiki/RocksDB-Tuning-Guide#level-style-compaction - where `target_file_size_multiplier` is mentioned maybe mention that setting it to a value other than 1 can impact write and read amplification when `level_compaction_dynamic_level_bytes=true` \r\n\r\n  https://github.com/facebook/rocksdb/wiki/Leveled-Compaction#level_compaction_dynamic_level_bytes-is-true - could also mention the potential for an increase in write and read amplification when used with `target_file_size_multiplier` set to a value other than 1. \r\n\r\n  http://rocksdb.org/blog/2015/07/23/dynamic-level.html\r\n  https://github.com/facebook/rocksdb/blob/master/docs/_posts/2015-07-23-dynamic-level.markdown\r\n\r\nI was also wondering if the target SST size per level when using `level_compaction_dynamic_level_bytes=true` could behave similar to how the compression per level behaves when `level_compaction_dynamic_level_bytes=true` :\r\n\r\n  https://github.com/facebook/rocksdb/blob/master/include/rocksdb/advanced_options.h#L322 - the comment above `compression_per_level`.\r\n  https://github.com/facebook/rocksdb/blob/master/db/compaction_picker.cc#L79 - `GetCompressionType`\r\n\r\nSo that the SST sizes are adjusted for the dynamic level.\r\n\r\nThanks.\r\n\r\nRuns:\r\n```\r\n> rm tmp/* ; ./db_bench --benchmarks=\"fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,levelstats,stats\" --statistics -deletes=5000 -db=tmp -compression_type=none --num=20000 -value_size=100000 -level_compaction_dynamic_level_bytes=true -target_file_size_multiplier=2\r\n\r\nLevel Files Size(MB)\r\n--------------------\r\n  0       12      738\r\n  1        0        0\r\n  2        0        0\r\n  3        0        0\r\n  4        0        0\r\n  5       16      861 - mean size is  54 MB\r\n  6        3     1645 - mean size is 548 MB <- one might expect the mean size to be closer to 2X the L-1 mean size, so ~ 108 MB\r\n\r\n** Compaction Stats [default] **\r\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\r\n----------------------------------------------------------------------------------------------------------------------------------------------------------\r\n  L0     12/0  737.54 MB   3.0      0.0     0.0      0.0      18.6     18.6       0.0   1.0      0.0    705.0        27       310    0.087       0      0\r\n  L5     16/1  861.23 MB   3.1     18.4    12.0      6.4      15.5      9.1       5.6   1.3    894.6    753.4        21        87    0.242    238K    39K\r\n  L6      3/1    1.61 GB   0.0     86.3    12.7     73.6      73.8      0.2       1.4   5.8    924.7    790.4        96        51    1.874    957K   166K\r\n Sum     31/2    3.17 GB   0.0    104.7    24.7     80.0     107.8     27.9       7.0   5.8    746.3    768.9       144       448    0.321   1196K   206K\r\nFlush(GB): cumulative 18.606, interval 18.366\r\nCumulative compaction: 107.84 GB write, 104.67 GB read\r\n...\r\nrocksdb.compact.read.bytes COUNT : 113621923639\r\nrocksdb.compact.write.bytes COUNT : 96950247376\r\n\r\nrchar: 208378047204 wchar: 136999657163 syscr: 2086368 syscw: 975794\r\n \r\n \r\n> rm tmp/* ; ./db_bench --benchmarks=\"fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,levelstats,stats\" --statistics -deletes=5000 -db=tmp -compression_type=none --num=20000 -value_size=100000 -level_compaction_dynamic_level_bytes=true -target_file_size_multiplier=1\r\n\r\nLevel Files Size(MB)\r\n--------------------\r\n  0        3      184\r\n  1        0        0\r\n  2        0        0\r\n  3        0        0\r\n  4        0        0\r\n  5        4      211 - mean size is 53 MB\r\n  6       30     1785 - mean size is 60 MB\r\n\r\n** Compaction Stats [default] **\r\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\r\n----------------------------------------------------------------------------------------------------------------------------------------------------------\r\n  L0      3/0  184.39 MB   0.8      0.0     0.0      0.0      18.6     18.6       0.0   1.0      0.0    909.4        21       310    0.068       0      0\r\n  L5      4/0  211.49 MB   0.8      6.0     3.5      2.5       4.4      1.9      14.6   1.3    907.9    661.8         7        24    0.282    105K    19K\r\n  L6     30/0    1.74 GB   0.0     51.2    15.2     36.1      36.4      0.3       1.4   2.4    921.7    654.5        57       217    0.262    588K   198K\r\n Sum     37/0    2.13 GB   0.0     57.2    18.6     38.6      59.3     20.8      16.1   3.2    692.4    718.2        85       551    0.154    693K   217K\r\nFlush(GB): cumulative 18.606, interval 18.366\r\nCumulative compaction: 59.35 GB write, 57.22 GB read\r\n...\r\nrocksdb.compact.read.bytes COUNT :  61450755309\r\nrocksdb.compact.write.bytes COUNT : 43745999431\r\n\r\nrchar: 90668564248 wchar: 83740004541 syscr: 914223 syscw: 924204\r\n\r\n\r\n\r\n> rm tmp/* ; ./db_bench --benchmarks=\"fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,levelstats,stats\" --statistics -deletes=5000 -db=tmp -compression_type=none --num=20000 -value_size=100000 -level_compaction_dynamic_level_bytes=false -target_file_size_multiplier=1\r\n \r\nLevel Files Size(MB)\r\n--------------------\r\n  0        3      184\r\n  1        4      198 - mean size is 50 MB\r\n  2       28     1706 - mean size is 61 MB\r\n  3        0        0\r\n  4        0        0\r\n  5        0        0\r\n  6        0        0\r\n\r\n** Compaction Stats [default] **\r\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\r\n----------------------------------------------------------------------------------------------------------------------------------------------------------\r\n  L0      3/0  184.39 MB   0.8      0.0     0.0      0.0      18.6     18.6       0.0   1.0      0.0    918.5        21       310    0.067       0      0\r\n  L1      4/0  197.75 MB   0.8      4.8     2.7      2.1       3.2      1.1      15.7   1.2    901.8    607.7         5        15    0.362     92K    19K\r\n  L2     28/0    1.67 GB   0.7     46.9    15.2     31.7      31.9      0.2       1.4   2.1    897.6    610.6        54       217    0.247    541K   199K\r\n Sum     35/0    2.04 GB   0.0     51.7    17.9     33.8      53.7     20.0      17.2   2.9    664.3    690.5        80       542    0.147    634K   218K\r\nFlush(GB): cumulative 18.606, interval 18.366\r\nCumulative compaction: 53.75 GB write, 51.71 GB read\r\n...\r\nrocksdb.compact.read.bytes COUNT :  55531021778\r\nrocksdb.compact.write.bytes COUNT : 37731278067\r\n\r\nrchar: 82170248183 wchar: 77725185485 syscr: 828601 syscw: 918067\r\n\r\n\r\n\r\n> rm tmp/* ; ./db_bench --benchmarks=\"fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,deleteseq,fillseekseq,levelstats,stats\" --statistics -deletes=5000 -db=tmp -compression_type=none --num=20000 -value_size=100000 -level_compaction_dynamic_level_bytes=false -target_file_size_multiplier=2\r\n \r\nLevel Files Size(MB)\r\n--------------------\r\n  0        2      123\r\n  1        4      211 - mean size is  53 MB\r\n  2       17     1847 - mean size is 109 MB\r\n  3        0        0\r\n  4        0        0\r\n  5        0        0\r\n  6        0        0\r\n\r\n** Compaction Stats [default] **\r\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\r\n----------------------------------------------------------------------------------------------------------------------------------------------------------\r\n  L0      2/0  122.92 MB   0.5      0.0     0.0      0.0      18.6     18.6       0.0   1.0      0.0    835.0        23       310    0.074       0      0\r\n  L1      4/0  211.49 MB   0.8      5.2     2.9      2.3       3.5      1.2      15.5   1.2    955.4    633.9         6        18    0.312     97K    21K\r\n  L2     17/0    1.80 GB   0.7     54.4    15.1     39.3      39.6      0.4       1.4   2.6    964.1    703.2        58       177    0.326    621K   196K\r\n Sum     23/0    2.13 GB   0.0     59.6    18.0     41.6      61.7     20.1      17.0   3.3    708.3    733.6        86       505    0.171    718K   217K\r\nFlush(GB): cumulative 18.606, interval 18.366\r\nCumulative compaction: 61.73 GB write, 59.60 GB read \r\n```", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3229/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/1780", "title": "More than one RocksDB process can access a single database.  fcntl lock released on close.", "body": "Hello and thanks for RocksDB,\r\n\r\nWhen a process attempts to open a RocksDB database it already has open, it opens the LOCK file and then closes it, this releases the fcntl lock the first RocksDB instance thought it had, and means that another process can also open the same RocksDB database.\r\n\r\nTest code:\r\n\r\n```\r\n#include \"db/db_test_util.h\"\r\n#include \"port/stack_trace.h\"\r\n\r\nnamespace { long test_num = -1; }\r\n\r\nTEST(RocksDBLocked,ExpectToOpenAndLock) {\r\n  rocksdb::DB* db;\r\n  rocksdb::Options options;\r\n  options.create_if_missing = true;\r\n  rocksdb::Status status = rocksdb::DB::Open(options, \"/tmp/testdb\", &db);\r\n  if( 0 == test_num ){\r\n    ASSERT_TRUE( status.ok() );\r\n  }\r\n  if ( 2 == test_num ) {\r\n    ASSERT_TRUE( ! status.ok() );\r\n  }\r\n}\r\n\r\nTEST(RocksDBLocked,Sleep4s){ usleep( 4 * 1000000 ); }\r\n\r\nTEST(RocksDBLocked,ExpectToFailAsLocked) {\r\n  rocksdb::DB* db;\r\n  rocksdb::Options options;\r\n  options.create_if_missing = true;\r\n  rocksdb::Status status = rocksdb::DB::Open(options, \"/tmp/testdb\", &db);\r\n  ASSERT_TRUE(!status.ok());\r\n}\r\n\r\nint main(int argc, char** argv) {\r\n  if( argc > 1 ){ test_num = strtol(argv[1], NULL, 0); }\r\n  printf(\"test_num = %ld\\n\", test_num);\r\n  ::testing::InitGoogleTest(&argc, argv);\r\n  return RUN_ALL_TESTS();\r\n}\r\n```\r\n\r\nRun 2 processes, with 2s staggered start, passing 0 and then 2 as argv[1]:\r\n\r\n```\r\n> echo -en \"0\\n2\" | xargs -t -I {} -P 2 bash -c \"sleep {} ; ./db_lock_test {} \"\r\nbash -c sleep 0 ; ./db_lock_test 0\r\nbash -c sleep 2 ; ./db_lock_test 2\r\ntest_num = 0\r\n[==========] Running 3 tests from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 3 tests from RocksDBLocked\r\n[ RUN      ] RocksDBLocked.ExpectToOpenAndLock\r\n[       OK ] RocksDBLocked.ExpectToOpenAndLock (3 ms)\r\n[ RUN      ] RocksDBLocked.Sleep4s\r\ntest_num = 2\r\n[==========] Running 3 tests from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 3 tests from RocksDBLocked\r\n[ RUN      ] RocksDBLocked.ExpectToOpenAndLock\r\n[       OK ] RocksDBLocked.ExpectToOpenAndLock (0 ms)\r\n[ RUN      ] RocksDBLocked.Sleep4s\r\n[       OK ] RocksDBLocked.Sleep4s (4000 ms)\r\n[ RUN      ] RocksDBLocked.ExpectToFailAsLocked\r\n[       OK ] RocksDBLocked.ExpectToFailAsLocked (0 ms)\r\n[----------] 3 tests from RocksDBLocked (4003 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 3 tests from 1 test case ran. (4003 ms total)\r\n[  PASSED  ] 3 tests.\r\n[       OK ] RocksDBLocked.Sleep4s (4001 ms)\r\n[ RUN      ] RocksDBLocked.ExpectToFailAsLocked\r\ndb/db_lock_test.cc:26: Failure\r\nValue of: !status.ok()\r\n  Actual: false\r\nExpected: true\r\n[  FAILED  ] RocksDBLocked.ExpectToFailAsLocked (3 ms)\r\n[----------] 3 tests from RocksDBLocked (4004 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 3 tests from 1 test case ran. (4004 ms total)\r\n[  PASSED  ] 2 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] RocksDBLocked.ExpectToFailAsLocked\r\n\r\n 1 FAILED TEST\r\n```\r\n\r\nThe first process sets the fcntl lock first,\r\nthen second process attempts to set the fcntl lock but fcntl correctly returns -1 as it is locked by the first,\r\nthen first process opens the file in LockFile ( https://github.com/facebook/rocksdb/blob/master/util/env_posix.cc#L554 ) , but the filename is present in the set of locked files ( https://github.com/facebook/rocksdb/blob/master/util/env_posix.cc#L89 ) and -1 is returned from LockOrUnlock, so the file is closed ( https://github.com/facebook/rocksdb/blob/master/util/env_posix.cc#L560 ) , when the file is closed the posix advisory lock it had is released and this allows the second process to also\r\nopen the RocksDB database when we would expect it to be prevented from doing so (because of the lock).\r\n\r\nFrom the man page for fcntl:\r\n\r\n       The record locks described above are associated with the process\r\n       (unlike the open file description locks described below).  This has\r\n       some unfortunate consequences:\r\n\r\n       *  If a process closes any file descriptor referring to a file, then\r\n          all of the process's locks on that file are released, regardless\r\n          of the file descriptor(s) on which the locks were obtained.  This\r\n          is bad: it means that a process can lose its locks on a file such\r\n          as /etc/passwd or /etc/mtab when for some reason a library\r\n          function decides to open, read, and close the same file.\r\n\r\nAn strace with timestamps, run at the same time as the bash below it which is capturing the locks for the LOCK file:\r\n\r\n```\r\n> echo -en \"0\\n2\" | xargs -t -I {} -P 2 bash -c \"sleep {} ; strace -ttt -y -f -e trace=open,close,fcntl ./db_lock_test {} \" 2>&1 | grep LOCK\r\n1484694794.734141 open(\"/tmp/testdb/LOCK\", O_RDWR|O_CREAT, 0644) = 5\r\n1484694794.734161 fcntl(5</tmp/testdb/LOCK>, F_SETLK, {type=F_WRLCK, whence=SEEK_SET, start=0, len=0}) = 0\r\n1484694794.734185 fcntl(5</tmp/testdb/LOCK>, F_GETFD) = 0\r\n1484694794.734202 fcntl(5</tmp/testdb/LOCK>, F_SETFD, FD_CLOEXEC) = 0\r\n \r\n1484694796.734743 open(\"/tmp/testdb/LOCK\", O_RDWR|O_CREAT, 0644) = 5\r\n1484694796.734763 fcntl(5</tmp/testdb/LOCK>, F_SETLK, {type=F_WRLCK, whence=SEEK_SET, start=0, len=0}) = -1 EAGAIN (Resource temporarily unavailable)\r\n1484694796.734809 close(5</tmp/testdb/LOCK>) = 0\r\n \r\n1484694798.740424 open(\"/tmp/testdb/LOCK\", O_RDWR|O_CREAT, 0644) = 10\r\n1484694798.740453 close(10</tmp/testdb/LOCK>) = 0\r\n \r\n1484694800.736141 open(\"/tmp/testdb/LOCK\", O_RDWR|O_CREAT, 0644) = 5\r\n1484694800.736160 fcntl(5</tmp/testdb/LOCK>, F_SETLK, {type=F_WRLCK, whence=SEEK_SET, start=0, len=0}) = 0\r\n1484694800.736184 fcntl(5</tmp/testdb/LOCK>, F_GETFD) = 0\r\n1484694800.736201 fcntl(5</tmp/testdb/LOCK>, F_SETFD, FD_CLOEXEC) = 0\r\n ```\r\n\r\n```\r\n> for i in {1..60}; do sleep 0.1;date +%s.%N;grep `ls -i /tmp/testdb/LOCK | awk '{print $1}'` /proc/locks ; done\r\n1484694794.601676035\r\n1484694794.707443348\r\n1484694794.812417016\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694794.917349009\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694795.022629478\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694795.127793504\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694795.232797370\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694795.337953964\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694795.443171801\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694795.548403582\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694795.653550237\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694795.759104030\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694795.864437396\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694795.969722451\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694796.075128729\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694796.181272834\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694796.287072797\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694796.392213267\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694796.497987835\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694796.603113756\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694796.709120783\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694796.814712184\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694796.920128990\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694797.025313364\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694797.130242746\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694797.235746114\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694797.341186226\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694797.446314883\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694797.551756430\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694797.657506503\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694797.762840443\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694797.868171087\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694797.973480076\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694798.078596669\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694798.183786058\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694798.289064968\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694798.394004472\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694798.499266611\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694798.604694566\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694798.709896097\r\n976: POSIX  ADVISORY  WRITE 194540 fd:19:25167883 0 EOF\r\n1484694798.815175839\r\n1484694798.920411649\r\n1484694799.025445454\r\n```\r\n\r\nI think it should be possible to avoid this by checking if the file to be locked is already in lockedFiles earlier, and avoiding the open and *close* if it is already present.\r\n\r\nThanks.\r\n\r\nI am testing on kernel 3.10.0-327.22.2.el7.x86_64 with glibc-2.17-106.el7_2.4, /tmp is xfs.\r\n\r\nXref:\r\nhttps://github.com/facebook/rocksdb/commit/e56b2c5a31eee7131dd36792a072bf5d40c05e67#diff-5b717965879a622d11268aaf03fb19c8 - \"Prevent concurrent multiple opens of leveldb database.\"\r\n ", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/1780/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/45828c721536e2145be6f825e563955d39948f02", "message": "Consider an increase to buffer size when reading option file, from 4K to 8K.\n\nSummary:\nHello and thank you for RocksDB,\n\nWhile looking into the buffered io used when an `OPTIONS` file is read I noticed the `OPTIONS` files produced by RocksDB 5.8.8 (and head of master) were just over 4096 bytes in size, resulting in the version of glibc I am using (glibc-2.17-196.el7) (on the filesystem used) being passed a 4K buffer for the `fread_unlocked` call and 2 system call reads using a 4096 buffer being used to read the contents of the `OPTIONS` file.\n\n  If the buffer size is increased to 8192 then 1 system call read is used to read the contents.\n\n  As I think the buffer size is just used for reading `OPTIONS` files, and I thought it likely that `OPTIONS` files have increased in size (as more options are added), I thought I would suggest an increase.\n\n[  If the comments from the top of the `OPTIONS` file are removed, and white space from the start of lines is removed then the size can be reduced to be under 4K, but as more options are added the size seems likely to grow again. ]\n\nCreate a new database:\n\n```\n> ./ldb --create_if_missing --db=/tmp/rdb_tmp put 1 1\nOK\n```\n\nThe OPTIONS file is 4252 bytes:\n\n```\n> stat /tmp/rdb_tmp/OPTIONS* | head -n 2\n  File: \u2018/tmp/rdb_tmp/OPTIONS-000005\u2019\n  Size: 4252            Blocks: 16         IO Block: 4096   regular file\n```\n\nBefore, the 4096 byte buffer is used from 2 system read calls:\n\n```\n> strace -f ./ldb --try_load_options --db=/tmp/rdb_tmp get DOES_NOT_EXIST 2>&1 |\n    grep -A 1 'RocksDB option file'\nread(3, \"# This is a RocksDB option file.\"..., 4096) = 4096\nread(3, \"e\\n  metadata_block_size=4096\\n  c\"..., 4096) = 156\n```\n\nltrace shows 4096 passed to fread_unlocked\n\n```\n> ltrace -S -f ./ldb --try_load_options --db=/tmp/rdb_tmp get DOES_NOT_EXIST 2>&1 |\n    grep -C 3 'RocksDB option file'\n[pid 51013] fread_unlocked(0x7ffd5fbf2d50, 1, 4096, 0x7fd2e084e780 <unfinished ...>\n[pid 51013] fstat@SYS(3, 0x7ffd5fbf28f0)         = 0\n[pid 51013] mmap@SYS(nil, 4096, 3, 34, -1, 0)    = 0x7fd2e318c000\n[pid 51013] read@SYS(3, \"# This is a RocksDB option file.\"..., 4096) = 4096\n[pid 51013] <... fread_unlocked resumed> )       = 4096\n...\n```\n\nAfter, the 8192 byte buffer is used from 1 system read call:\n\n```\n> strace -f ./ldb --try_load_options --db=/tmp/rdb_tmp get DOES_NOT_EXIST 2>&1 | grep -A 1 'RocksDB option file'\nread(3, \"# This is a RocksDB option file.\"..., 8192) = 4252\nread(3, \"\", 4096)                       = 0\n```\n\nltrace shows 8192 passed to fread_unlocked\n\n```\n> ltrace -S -f ./ldb --try_load_options --db=/tmp/rdb_tmp get DOES_NOT_EXIST 2>&1 | grep -C 3 'RocksDB option file'\n[pid 146611] fread_unlocked(0x7ffcfba382f0, 1, 8192, 0x7fc4e844e780 <unfinished ...>\n[pid 146611] fstat@SYS(3, 0x7ffcfba380f0)        = 0\n[pid 146611] mmap@SYS(nil, 4096, 3, 34, -1, 0)   = 0x7fc4eaee0000\n[pid 146611] read@SYS(3, \"# This is a RocksDB option file.\"..., 8192) = 4252\n[pid 146611] read@SYS(3, \"\", 4096)               = 0\n[pid 146611] <... fread_unlocked resumed> )      = 4252\n[pid 146611] feof(0x7fc4e844e780)                = 1\n```\nCloses https://github.com/facebook/rocksdb/pull/3294\n\nDifferential Revision: D6653684\n\nPulled By: ajkr\n\nfbshipit-source-id: 222f25f5442fefe1dcec18c700bd9e235bb63491"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "knutj42": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3228", "title": "buffer overrun in BackupEngineImpl::BackupMeta::StoreToFile", "body": "The \"BackupEngineImpl::BackupMeta::StoreToFile\" method in https://github.com/facebook/rocksdb/blob/master/utilities/backupable/backupable_db.cc loops over all the files in the database to produce a backup metadata file containing the crc-code of each file. The problem is that this information is first written to an in-memory buffer of a fixed size, without checking that there is room in the buffer.\r\n\r\nThe problematic piece of the code is this loop:\r\n\r\n    for (const auto& file : files_) {\r\n       // use crc32 for now, switch to something else if needed\r\n       len += snprintf(buf.get() + len, buf_size - len, \"%s crc32 %u\\n\",\r\n                       file->filename.c_str(), file->checksum_value);\r\n    }\r\n\r\nIf the number of files is large enough (in the range 200000-300000), this loop will cause a buffer overrun, since it constantly adds to the \"buf\" buffer. The size of the buffer is hardcoded to 10MB.\r\n\r\nIn our case this overrun usually causes a segfault, but it might theoretically cause other problems (including a corrupted database, if you are unlucky about what was in the memory-location the overrun overwrites).\r\n\r\nIdeas on how to fix the problem:\r\n 1. To dynamically grow the in-memory buffer when needed.\r\n 2. To write directly to the target file with no in-memory buffer (or to a temporary file that can be renamed to the target filename when it has been completely written)\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3228/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mperham": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3227", "title": "Build failure in db/write_callback_test", "body": "Trying to compile master on OSX Sierra.  It's in code that changed 5 days ago so I assume it is a recent break.\r\n\r\n```\r\n  CC       db/write_callback_test.o\r\ndb/write_callback_test.cc:297:48: error: declaration shadows a local variable [-Werror,-Wshadow]\r\n                    PublishSeqCallback(DBImpl* db_impl) : db_impl_(db_impl) {}\r\n                                               ^\r\ndb/write_callback_test.cc:152:23: note: previous declaration is here\r\n              DBImpl* db_impl;\r\n                      ^\r\n1 error generated.\r\n```", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3227/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/331488470", "body": "@maysamyabandeh Thank you for your advice, I appreciate your time here.  I don't know C++ and am using the tecbot/gorocksdb bindings.  None of that appears to be in the Go binding.  Any chance you can show the actual C++ code necessary and I can adapt to the Go bindings?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/331488470/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/332320818", "body": "Yep, diving into LOG showed me that the system appears corrupt, due to an invalid merge operator.  Thank you!\r\n\r\nIs it best practice to keep the backups (i.e. meta/ share/ private/) in the same directory as the hot store?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/332320818/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/332324766", "body": "This was the root cause:\r\n\r\nhttps://github.com/tecbot/gorocksdb/issues/79", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/332324766/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/332609187", "body": "Cool, it works when I close the DB and reopen.  Note the wiki page says \"use these APIs\" but doesn't say anything higher-level about needing to reopen the database.  I added a note that restored databases must be reopened.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/332609187/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "vitalyisaev2": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3223", "title": "SIGABRT in malloc", "body": "### Problem description\r\n\r\nOur Rocksdb use-case is an embedded storage for serialized Protobuf messages. The application is written in Go and uses https://github.com/tecbot/gorocksdb to access RocksDB API.\r\n\r\nUnfortunately, from time to time RocksDB code crashes during writing (it can be etiher `Put` or `Merge` method), for example:\r\n\r\n```gdb\r\nProgram terminated with signal SIGABRT, Aborted.\r\n#0  runtime.raise () at /usr/local/go/src/runtime/sys_linux_amd64.s:113\r\n113        RET\r\n[Current thread is 1 (Thread 0x7f96b0c5b700 (LWP 28684))]\r\nLoading Go Runtime support.\r\n(gdb) bt\r\n#0  runtime.raise () at /usr/local/go/src/runtime/sys_linux_amd64.s:113\r\n#1  0x000000000044d29b in runtime.dieFromSignal (sig=6) at /usr/local/go/src/runtime/signal_unix.go:400\r\n#2  0x000000000044d429 in runtime.crash () at /usr/local/go/src/runtime/signal_unix.go:482\r\n#3  0x0000000000437b72 in runtime.dopanic_m (gp=0xc420072180, pc=4420981, sp=140285187565520) at /usr/local/go/src/runtime/panic.go:732\r\n#4  0x000000000046140c in runtime.dopanic.func1 () at /usr/local/go/src/runtime/panic.go:587\r\n#5  0x000000000046258b in runtime.systemstack () at /usr/local/go/src/runtime/asm_amd64.s:360\r\n#6  0x0000000000437488 in runtime.dopanic (unused=0) at /usr/local/go/src/runtime/panic.go:586\r\n#7  0x0000000000437575 in runtime.throw (s=\"unexpected signal during runtime execution\") at /usr/local/go/src/runtime/panic.go:605\r\n#8  0x000000000044d248 in runtime.sigpanic () at /usr/local/go/src/runtime/signal_unix.go:351\r\n#9  0x00007f96b3c41e10 in _int_malloc (av=0x20, av@entry=0x7f9680000020, bytes=bytes@entry=395) at malloc.c:3515\r\n#10 0x00007f96b3c44184 in __GI___libc_malloc (bytes=395) at malloc.c:2913\r\n#11 0x00007f96b2a0be78 in operator new(unsigned long) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#12 0x00007f96b2a9d87d in std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::reserve(unsigned long) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#13 0x00007f96b44e3584 in rocksdb::WriteBatch::WriteBatch (this=0x7f96b0c5a9d0, reserved_bytes=<optimized out>, max_bytes=<optimized out>) at db/write_batch.cc:136\r\n#14 0x00007f96b443b2de in rocksdb::DB::Put (this=0x1c70480, opt=..., column_family=0x1ca2e90, key=..., value=...) at db/db_impl_write.cc:1230\r\n#15 0x00007f96b443b390 in rocksdb::DBImpl::Put (this=<optimized out>, o=..., column_family=<optimized out>, key=..., val=...) at db/db_impl_write.cc:24\r\n#16 0x00007f96b43e3a85 in rocksdb::DB::Put (this=0x1c70480, options=..., key=..., value=...) at ./include/rocksdb/db.h:228\r\n#17 0x00007f96b43d3e9f in rocksdb_put (db=<optimized out>, options=<optimized out>, key=<optimized out>, keylen=<optimized out>, val=<optimized out>, vallen=<optimized out>, errptr=0xc420092198)\r\n    at db/c.cc:734\r\n#18 0x0000000000957c03 in _cgo_e8f4e72abb2a_Cfunc_rocksdb_put (v=<optimized out>) at cgo-gcc-prolog:755\r\n#19 0x0000000000463d30 in runtime.asmcgocall () at /usr/local/go/src/runtime/asm_amd64.s:624\r\n#20 0x000000c420072180 in ?? ()\r\n#21 0x00007f96b0c5abb0 in ?? ()\r\n#22 0x000000000045f772 in runtime.(*mcache).nextFree.func1 () at /usr/local/go/src/runtime/malloc.go:557\r\n#23 0x0000000000462559 in runtime.systemstack () at /usr/local/go/src/runtime/asm_amd64.s:344\r\n#24 0x000000000043bd70 in ?? () at /usr/local/go/src/runtime/proc.go:1070\r\n#25 0x000000c42002a600 in ?? ()\r\n#26 0x00007f96b1c5c81f in ?? ()\r\n#27 0x000000c420072180 in ?? ()\r\n#28 0x00007f96b0c5abf8 in ?? ()\r\n#29 0x000000000043bdd4 in runtime.mstart () at /usr/local/go/src/runtime/proc.go:1152\r\n#30 0x00000000009597a3 in crosscall_amd64 () at gcc_amd64.S:35\r\n#31 0x00007f96b1c5c8a0 in ?? ()\r\n#32 0x00007f96b0c5b9c0 in ?? ()\r\n#33 0x00007f96b1c5c81f in ?? ()\r\n#34 0x0000000000000000 in ?? ()\r\n```\r\n```gdb\r\nProgram terminated with signal SIGABRT, Aborted.\r\n#0  runtime.raise () at /usr/local/go/src/runtime/sys_linux_amd64.s:113\r\n113        RET\r\n[Current thread is 1 (Thread 0x7fa4a1586a00 (LWP 21405))]\r\nLoading Go Runtime support.\r\n(gdb) bt\r\n#0  runtime.raise () at /usr/local/go/src/runtime/sys_linux_amd64.s:113\r\n#1  0x000000000044d29b in runtime.dieFromSignal (sig=6) at /usr/local/go/src/runtime/signal_unix.go:400\r\n#2  0x000000000044d429 in runtime.crash () at /usr/local/go/src/runtime/signal_unix.go:482\r\n#3  0x0000000000437b72 in runtime.dopanic_m (gp=0x10fe020 <runtime.g0>, pc=4420981, sp=140720322159072) at /usr/local/go/src/runtime/panic.go:732\r\n#4  0x000000000046140c in runtime.dopanic.func1 () at /usr/local/go/src/runtime/panic.go:587\r\n#5  0x000000000046258b in runtime.systemstack () at /usr/local/go/src/runtime/asm_amd64.s:360\r\n#6  0x0000000000437488 in runtime.dopanic (unused=0) at /usr/local/go/src/runtime/panic.go:586\r\n#7  0x0000000000437575 in runtime.throw (s=\"unexpected signal during runtime execution\") at /usr/local/go/src/runtime/panic.go:605\r\n#8  0x000000000044d248 in runtime.sigpanic () at /usr/local/go/src/runtime/signal_unix.go:351\r\n#9  0x00007fa4a064de10 in _int_malloc (av=0x20, av@entry=0x7fa4a0990b20 <main_arena>, bytes=bytes@entry=265) at malloc.c:3515\r\n#10 0x00007fa4a0650184 in __GI___libc_malloc (bytes=265) at malloc.c:2913\r\n#11 0x00007fa49f417e78 in operator new(unsigned long) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#12 0x00007fa49f4a9499 in std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_mutate(unsigned long, unsigned long, char const*, unsigned long) ()\r\n   from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#13 0x00007fa49f4aa833 in std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_append(char const*, unsigned long) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#14 0x00007fa4a0ef32db in std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::append (__n=<optimized out>, __s=<optimized out>, this=0x7ffc00d0a520)\r\n    at /usr/include/c++/5/bits/basic_string.h:1015\r\n#15 rocksdb::PutLengthPrefixedSlice (value=..., dst=0x7ffc00d0a520) at ./util/coding.h:247\r\n#16 rocksdb::WriteBatchInternal::Merge (b=b@entry=0x7ffc00d0a4f0, column_family_id=<optimized out>, key=..., value=...) at db/write_batch.cc:748\r\n#17 0x00007fa4a0ef3475 in rocksdb::WriteBatch::Merge (this=this@entry=0x7ffc00d0a4f0, column_family=column_family@entry=0x280b300, key=..., value=...) at db/write_batch.cc:758\r\n#18 0x00007fa4a0e47740 in rocksdb::DB::Merge (this=this@entry=0x2809d70, opt=..., column_family=column_family@entry=0x280b300, key=..., value=...) at db/db_impl_write.cc:1260\r\n#19 0x00007fa4a0e4780e in rocksdb::DBImpl::Merge (this=0x2809d70, o=..., column_family=0x280b300, key=..., val=...) at db/db_impl_write.cc:33\r\n#20 0x00007fa4a0def31d in rocksdb::DB::Merge (this=0x2809d70, options=..., key=..., value=...) at ./include/rocksdb/db.h:293\r\n#21 0x00007fa4a0de01d2 in rocksdb_merge (db=<optimized out>, options=<optimized out>, key=<optimized out>, keylen=<optimized out>, val=<optimized out>, vallen=<optimized out>, errptr=0xc420b3c1a8)\r\n    at db/c.cc:774\r\n#22 0x0000000000957a13 in _cgo_e8f4e72abb2a_Cfunc_rocksdb_merge (v=<optimized out>) at cgo-gcc-prolog:591\r\n#23 0x0000000000463d30 in runtime.asmcgocall () at /usr/local/go/src/runtime/asm_amd64.s:624\r\n#24 0x0000000000000000 in ?? ()\r\n```\r\n\r\n### Environment\r\nPlatform: `Ubuntu 16.04 x86_64`\r\nRocksdb: `v5.8.7`\r\nRocksdb dependencies:\r\n```\r\nlibbz2-dev:amd64                            1.0.6-8\r\nlibgflags-dev                               2.1.2-3\r\nliblz4-dev:amd64                            0.0~r131-2ubuntu2\r\nlibzstd-dev                                 0.5.1-1        \r\ngcc-5                                       5.4.0-6ubuntu1~16.04.5\r\n```\r\n\r\n### Question\r\n\r\nBut the interesting thing we have found out is that if we store hex-encoded string from protobuf serialized message instead of pure protobuf, the crashes are no longer observed.\r\n\r\nSo the question is: are there any limitations for symbols that can be used as a values in Rocksdb?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3223/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "anuthekool": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3218", "title": "cross compiling error for aarch64", "body": "i am getting following error when i am cross comiling it for aarch64\r\n\r\n-- Found Threads: TRUE  \r\nCMake Warning (dev) at CMakeLists.txt:603 (add_library):\r\n  ADD_LIBRARY called with SHARED option but the target platform does not\r\n  support dynamic linking.  Building a STATIC library instead.  This may lead\r\n  to problems.\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- JNI library is disabled\r\nCMake Error at CMakeLists.txt:685 (install):\r\n  install TARGETS given no ARCHIVE DESTINATION for static library target\r\n  \"rocksdb-shared\".\r\n-- Configuring incomplete, errors occurred!\r\n\r\nDone changes for cross compiling is cmake file is  = \r\n\r\n# this one is important\r\nSET(CMAKE_SYSTEM_NAME Generic)\r\n#this one not so much\r\nSET(CMAKE_SYSTEM_VERSION 1)\r\nSET(CMAKE_SYSTEM_PROCESSOR aarch64)\r\n# specify the cross compiler\r\nSET(CMAKE_C_COMPILER   /opt/gcc-linaro-5.5.0-2017.10-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc)\r\nSET(CMAKE_CXX_COMPILER /opt/gcc-linaro-5.5.0-2017.10-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-g++)\r\n\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3218/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "weijietong": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3207", "title": "SIGSEGV occur on Rocksdb 5.7.3", "body": "* We got the following core dump stack:\r\n```\r\nStack: [0x00007fd7bcc67000,0x00007fd7bcd68000],  sp=0x00007fd7bcd65b38,  free space=1018k\r\nNative frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)\r\nRegister to memory mapping:\r\n\r\nRAX=0x00007fd6ed800018 is an unknown value\r\nRBX=0x000000000075072e is an unknown value\r\nRCX=0x0000000000010000 is an unknown value\r\nRDX=0x000000000075072e is an unknown value\r\nRSP=0x00007fd7bcd65b38 is pointing into the stack for thread: 0x00007fd803061000\r\nRBP=0x00007fd7bcd65ca0 is pointing into the stack for thread: 0x00007fd803061000\r\nRSI=0x00007fd660c871e6 is an unknown value\r\nRDI=0x00007fd6edf50746 is an unknown value\r\nR8 =0x00007fd6ed800018 is an unknown value\r\nR9 =0x000000000001bf79 is an unknown value\r\nR10=0x00007fd7e8ff7480 is an unknown value\r\nR11=0x00007fd7e8ff7480 is an unknown value\r\nR12=0x0000000000000000 is an unknown value\r\nR13=0x00007fd660536ab8 is an unknown value\r\nR14=0x00007fd7bcd65d50 is pointing into the stack for thread: 0x00007fd803061000\r\nR15=0x00007fd7f407a540 is an unknown value\r\n\r\nC  [libc.so.6+0x14a1a6]  __memcpy_ssse3_back+0x1a36\r\nC  0x00007fd7bcd65d20\r\n\r\nJava frames: (J=compiled Java code, j=interpreted, Vv=VM code)\r\nj  org.rocksdb.RocksDB.get(J[BII)[B+0\r\nj  org.rocksdb.RocksDB.get([B)[B+9\r\nj  com.alibaba.client.cache.LocalRocksDBCache.get(Lio/druid/client/cache/Cache$NamedKey;)[B+21\r\n```\r\n\r\n* our version is:\r\n```\r\n  <dependency>\r\n            <groupId>org.rocksdb</groupId>\r\n            <artifactId>rocksdbjni</artifactId>\r\n            <version>5.7.3</version>\r\n        </dependency>\r\n```\r\n\r\n* our options are:\r\n```\r\n      try (Options options = new Options().setAllowMmapReads(true).setCreateIfMissing(true)) {\r\n        rocksDB = RocksDB.open(options, localDataDir);\r\n      } catch (RocksDBException r) {\r\n        logger.error(\"error to create a local rocksdb cache !\", r);\r\n        initialized = false;\r\n      }\r\n```\r\n* Steps to reproduce the behavior\r\nwe just use the methods:\r\n```\r\npublic byte[] get(byte[] var1) throws RocksDBException\r\npublic void put(byte[] var1, byte[] var2) throws RocksDBException \r\n```\r\nIt will cause the jvm core dump after processing some data ,running some time.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3207/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hermanlee": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3200", "title": "SyncWAL() triggers a failure on Windows", "body": "Testing of MyRocks being ported into MariaDB of the latest SyncWAL() changes have triggered an error on Windows builds:\r\n\r\nhttps://jira.mariadb.org/browse/MDEV-13852\r\n\r\nThe check !it->writer->file()->writable_file()->IsSyncThreadSafe() fails for Windows. What is the expected behavior here. Is SyncWAL() not supposed to be called on Windows builds?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3200/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Andymic": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3191", "title": "Write_stress cored dumped", "body": "### Expected behavior\r\nExpected to see the results of the stress test\r\n\r\n### Actual behavior\r\nWrite to DB failed: IO error: While open a file for appending: /tmp/rocksdbtest-1000/write_stress/004853.sst: Too many open files Aborted (core dumped)\r\n\r\n### Steps to reproduce the behavior\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 16.04.3 LTS\r\nRelease:\t16.04\r\nCodename:\txenial\r\nCompilers: G++ and GCC 5\r\nBuild With: make all\r\nExecute: write_stress", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xoxoj": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3181", "title": "win7 vs2015 x86 NewIterator prefix seek crush..", "body": "win7\r\nvs2015\r\nx86\r\nrocksdb\r\n\r\nremove the XPRESS depend\r\n\r\n==========================================================\r\n\r\n![rocksdb_error](https://user-images.githubusercontent.com/1118723/32987811-406300ac-cd30-11e7-90af-04dfdb2681a1.png)\r\n\r\ncode:\r\n\r\n    ```\r\n    rocksdb::DB* db;\r\n    rocksdb::Options options;\r\n    options.IncreaseParallelism();\r\n    options.OptimizeLevelStyleCompaction();\r\n    options.create_if_missing = true;\r\n    options.prefix_extractor.reset(rocksdb::NewFixedPrefixTransform(3));\r\n\r\n\r\n    rocksdb::Status s = rocksdb::DB::Open(options, \"./testdb\", &db);\r\n    assert(s.ok());\r\n\r\n    s = db->Put(WriteOptions(), \"key1\", \"value1\");\r\n    assert(s.ok());\r\n    s = db->Put(WriteOptions(), \"key2\", \"value2\");\r\n    assert(s.ok());\r\n    s = db->Put(WriteOptions(), \"key3\", \"value3\");\r\n    assert(s.ok());\r\n    s = db->Put(WriteOptions(), \"key4\", \"value4\");\r\n    assert(s.ok());\r\n    s = db->Put(WriteOptions(), \"otherPrefix1\", \"otherValue1\");\r\n    assert(s.ok());\r\n    s = db->Put(WriteOptions(), \"abc\", \"abcvalue1\");\r\n    assert(s.ok());\r\n\r\n\r\n\r\n    auto iter = db->NewIterator(ReadOptions());\r\n    Slice prefix = options.prefix_extractor->Transform(\"key0\");\r\n    for (iter->Seek(\"key0\"); iter->Valid() && iter->key().starts_with(prefix); iter->Next())\r\n    {\r\n     std::cout << iter->key().ToString() << \": \" << iter->value().ToString() << std::endl;\r\n    }\r\n    \r\n    delete db;\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3181/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yuslepukhin": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3180", "title": "Minor FilePrefetch issue", "body": "> Note: Please use Issues only for bug reports. For questions, discussions, feature requests, etc. post to dev group: https://www.facebook.com/groups/rocksdb.dev\r\n\r\nhttps://github.com/facebook/rocksdb/blob/master/util/file_reader_writer.cc#L609\r\n### Expected behavior\r\nWe want to read the data at least as early as the specified read offset and thus the offset needs to be rounded down.\r\n### Actual behavior\r\nBy rounding up the read offset when pref-etching data we forgo up to the alignment length of the data that we may be interested in.\r\n### Steps to reproduce the behavior\r\nCode inspection", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3180/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3013", "title": "RecordTick in DbIter __dtor potentially wrong", "body": "> Note: Please use Issues only for bug reports. For questions, discussions, feature requests, etc. post to dev group: https://www.facebook.com/groups/rocksdb.dev\r\n\r\nhttps://github.com/facebook/rocksdb/blob/master/db/db_iter.cc#L142 potentially make the stats wrong.\r\n\r\nThe argument of -1 for uint64_t results in a max value for the type. \r\n\r\n### Expected behavior\r\n\r\nThe number of ticks decreased\r\n\r\n### Actual behavior\r\n\r\nDepending on the impl we have a potential overflow.\r\n\r\n### Steps to reproduce the behavior\r\n\r\nCode review", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3013/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/b010116d8263db1ba3f6ea5b30f9886243320a59", "message": "Eliminate some redundant block reads.\n\nSummary:\nRe-use metadata for reading Compression Dictionary on BlockBased\n  table open, this saves two reads from disk.\n  This helps to our 999 percentile in 5.6.1 where prefetch buffer is  not present.\nCloses https://github.com/facebook/rocksdb/pull/3354\n\nDifferential Revision: D6695753\n\nPulled By: ajkr\n\nfbshipit-source-id: bb8acd9e9e66e65b89c548ab8940570ae360333c"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/84ddbd186a6f52b1a26e4ad547ac87c2fa172e16", "message": "Make Windows dep switches compatible with other builds\n\nSummary:\nMake dependacies switches compatible with other OS builds\n  TODO: Make find_package work for Windows.\nCloses https://github.com/facebook/rocksdb/pull/3322\n\nDifferential Revision: D6667637\n\nPulled By: sagar0\n\nfbshipit-source-id: 5afcd7bbfe69465310a4fbc8e589f01e506b95f5"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/fe608e32abc166de10671af71ed95e701cd103a8", "message": "Fix a race condition in WindowsThread (port::Thread)\n\nSummary:\nFix a race condition when we create a thread and immediately destroy\n This case should be supported.\n  What happens is that the thread function needs the Data instance\n  to actually run but has no shared ownership and must rely on the\n  WindowsThread instance to continue existing.\n  To address this we change unique_ptr to shared_ptr and then\n  acquire an additional refcount for the threadproc which destroys it\n  just before the thread exit.\n  We choose to allocate shared_ptr instance on the heap as this allows\n  the original thread to continue w/o waiting for the new thread to start\n  running.\nCloses https://github.com/facebook/rocksdb/pull/3240\n\nDifferential Revision: D6511324\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 4633ff7996daf4d287a9fe34f60c1dd28cf4ff36"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/f8e2db071733980ee8aa994ec7ae2a4adb13d2ba", "message": "Fix crashes, address test issues and adjust windows test script\n\nSummary:\nAdd per-exe execution capability\n  Add fix parsing of groups/tests\n  Add timer test exclusion\n\n Fix unit tests\n  Ifdef threadpool specific tests that do not pass on Vista threadpool.\n  Remove spurious outout from prefix_test so test case listing works\n  properly.\n  Fix not using standard test directories results in file creation errors\n  in sst_dump_test.\n\n  BlobDb fixes:\n    In C++ end() iterators can not be dereferenced. They are not valid.\n\tWhen deleting blob_db_ set it to nullptr before any other code executes.\n\tNot fixed:. On Windows you can not delete a file while it is open.\n\t[ RUN      ] BlobDBTest.ReadWhileGC\n\td:\\dev\\rocksdb\\rocksdb\\utilities\\blob_db\\blob_db_test.cc(75): error: DestroyBlobDB(dbname_, options, bdb_options)\n\tIO error: Failed to delete: d:/mnt/db\\testrocksdb-17444/blob_db_test/blob_dir/000001.blob: Permission denied\n\td:\\dev\\rocksdb\\rocksdb\\utilities\\blob_db\\blob_db_test.cc(75): error: DestroyBlobDB(dbname_, options, bdb_options)\n\tIO error: Failed to delete: d:/mnt/db\\testrocksdb-17444/blob_db_test/blob_dir/000001.blob: Permission denied\n\n  write_batch\n    Should not call front() if there is a chance the container is empty\nCloses https://github.com/facebook/rocksdb/pull/3152\n\nDifferential Revision: D6293274\n\nPulled By: sagar0\n\nfbshipit-source-id: 318c3717c22087fae13b18715dffb24565dbd956"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/682db81385cc8a80973bfb3d87e4f8d290bf6f26", "message": "Enable cacheline_aligned_alloc() to allocate from jemalloc if enabled.\n\nSummary:\nReuse WITH_JEMALLOC option in preparation for module search unification.\n  Move jemalloc overrides into a separate .cc\n  Remote obsolete JEMALLOC_NOINIT option.\nCloses https://github.com/facebook/rocksdb/pull/3078\n\nDifferential Revision: D6174826\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 9970a0289b4490272d15853920d9d7531af91140"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/d2a65c59e1ae270822c4c56176a7d1dcfb8e2033", "message": "Fix unused var warnings in Release mode\n\nSummary:\nMSVC does not support unused attribute at this time. A separate assignment line fixes the issue probably by being counted as usage for MSVC and it no longer complains about unused var.\nCloses https://github.com/facebook/rocksdb/pull/3048\n\nDifferential Revision: D6126272\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 4907865db45fd75a39a15725c0695aaa17509c1f"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/ebab2e2d42bd13a85b355264603d63f9f988572f", "message": "Enable MSVC W4 with a few exceptions. Fix warnings and bugs\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/3018\n\nDifferential Revision: D6079011\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 988a721e7e7617967859dba71d660fc69f4dff57"}], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3325", "title": "Demonstrate async IO with coroutines", "body": "  with async/coroutine_random_access_test.cc windows only test.\r\n  RandomAccessFileReader::RequestRead() is a clone of Read() for test purposes.\r\n  demonstrates minimal would be changes to Read() with the insertion of 3 keywords\r\n  and keeping the source code intact.\r\n  Some scaffolding is required but it is all on the side and does not affect the code flow.\r\n  The downside that we would need to keep track that all functions on the way\r\n  down to I/O become coroutines and make use of co_await and co_return if they returning\r\n  anything (usually status). A separate AppVeyor build would be enough to make sure we are OK.\r\n  Use VS 2017 to build. It might work on 2015, I have not tried yet.\r\n  Use -DWITH_COROUTINES=1 to enable coroutines.\r\n  The output on my box is:\r\n  [==========] Running 2 tests from 1 test case.\r\n  [----------] Global test environment set-up.\r\n  [----------] 2 tests from TestAsyncRead/RandomAccessCoroutineTest\r\n  [ RUN      ] TestAsyncRead/RandomAccessCoroutineTest.TestAsyncRead/0\r\n  Thread ID before co_await: 240 after co_await: 27328\r\n  Read:7936 bytes, status: OK\r\n  Wait completed\r\n  Thread ID before co_await: 2600 after co_await: 27328\r\n  Thread ID before co_await: 17928 after co_await: 9420\r\n  Read:7935 bytes, status: OK\r\n  Thread ID before co_await: 28480 after co_await: 30912\r\n  Thread ID before co_await: 25000 after co_await: 15280\r\n  Thread ID before co_await: 14312 after co_await: 21532\r\n  Read:7935 bytes, status: OK\r\n  Thread ID before co_await: 19940 after co_await: 14912\r\n  Thread ID before co_await: 7188 after co_await: 9420\r\n  Thread ID before co_await: 360 after co_await: 7012\r\n  Thread ID before co_await: 28892 after co_await: 27328\r\n  Thread ID before co_await: 14744 after co_await: 26552\r\n  Read:7935 bytes, status: OK\r\n  Read:7935 bytes, status: OK\r\n  Thread ID before co_await: 13904 after co_await: 12556\r\n  Thread ID before co_await: 15712 after co_await: 28904\r\n  Read:7935 bytes, status: OK\r\n  Read:7935 bytes, status: OK\r\n  Read:7935 bytes, status: OK\r\n  Read:7935 bytes, status: OK\r\n  Read:7935 bytes, status: OK\r\n  Read:7935 bytes, status: OK\r\n  Read:7935 bytes, status: OK\r\n  Read:7935 bytes, status: OK\r\n  [       OK ] TestAsyncRead/RandomAccessCoroutineTest.TestAsyncRead/0 (235 ms)\r\n  [ RUN      ] TestAsyncRead/RandomAccessCoroutineTest.TestAsyncRead/1\r\n  Thread ID before co_await: 240 after co_await: 13428\r\n  Read:7936 bytes, status: OK\r\n  Wait completed\r\n  Thread ID before co_await: 26040 after co_await: 13428\r\n  Thread ID before co_await: 10276 after co_await: 14276\r\n  Read:7935 bytes, status: OK\r\n  Read:Thread ID before co_await: 7396Thread ID before co_await: 20800 after co_await: 18884\r\n  Thread ID before co_await: 20684 after co_await: 26680\r\n  Thread ID before co_await: 16728 after co_await: 18724\r\n  Thread ID before co_await: 27968 after co_await: 7148\r\n  after co_await: 13428\r\n  Thread ID before co_await: 27496 after co_await: 22984\r\n  Read:Read:7935 bytes, status: OK\r\n  Read:7935 bytes, status: OK\r\n  7935 bytes, status: OK\r\n  Read:7935 bytes, status: OK\r\n  Thread ID before co_await: 21056 after co_await: 2012\r\n  Read:7935 bytes, status: OK\r\n  Thread ID before co_await: 29944 after co_await: 2896\r\n  7935Read:7935 bytes, status: OK\r\n  bytes, status: OK\r\n  Thread ID before co_await: 10464 after co_await: 31984\r\n  Read:7935 bytes, status: OK\r\n  Thread ID before co_await: 15656 after co_await: 22984\r\n  Read:7935 bytes, status: OK\r\n  Read:7935 bytes, status: OK\r\n  Read:7935 bytes, status: OK\r\n  [       OK ] TestAsyncRead/RandomAccessCoroutineTest.TestAsyncRead/1 (230 ms)\r\n  [----------] 2 tests from TestAsyncRead/RandomAccessCoroutineTest (467 ms total)\r\n\r\n  [----------] Global test environment tear-down\r\n  [==========] 2 tests from 1 test case ran. (468 ms total)\r\n  [  PASSED  ] 2 tests.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3063", "title": "Async 5.6.1", "body": "This is the second iteration of async. This includes two major things: 1) Get code refactored for more inlining. 2) Async iterators implementation.\r\n\r\nIn my perf runs I no longer see the difference on windows while running single thread against RAM drive.\r\n\r\nHowever, if this is still no acceptable I am prepared to restore sync interfaces and leave async path on the side. Async iteration is implemented on the side.\r\n\r\nWe may need another meeting with better preparation to explain how things work.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/2727", "title": "Async Get() experimental", "body": "This is an informational PR seeking input for an async\r\nmode implementation of Get(). It is trivial to implement MultiGet()\r\nwhich is what reworked db_bench essentially is doing.\r\n\r\nRangeQuery code was not addressed at this time as it might\r\ninvolve building async iteration which I plan to address shortly.\r\n\r\nThe main gist of the PR is to show positive dynamics in throughput\r\nwhile keeping the number of threads to a minimum, possibly close\r\nto the number of the cores on the machine. This is usually\r\nachieved at the expense of an individual request latency (<5%) while\r\nrunning in async mode only but increasing the overall X number of times and\r\nallow to fully utilize the media bandwidth.\r\n\r\nAttached results are ad-hoc testing and should not be interpreted\r\nliterally but demonstrate positive trend given HDD example. Results for\r\nSSD are very similar.\r\n\r\nThis is achieved by making use  one of the well known async patterns.\r\nTo achieve that we use generic Callable<> class to build ad-hoc a necessary\r\nchain of callbacks between the pariticipating dynamically allocated\r\ncontexts.\r\n\r\nOn the OS level Windows makes use kernel implemented Vista threadpool\r\nwith I/O completion functionality. As the async read completes the threadpool\r\ninvokes a supplied callback whcih resumes request processing where it was left off.\r\n\r\nOther OS utilize similar facilities as long as they invoke a callback such as epoll,\r\naio_*() and other suitable facilities.\r\n\r\nIt is possible to implement a lock-free threadpool for more efficiency.\r\n\r\nThe nice property of the code is that is can run both sync and async.\r\nIndeed, there is not another way since rocksdb can complete many\r\nrequests sync.\r\n\r\nThus we can switch the running mode simply by switching an option.\r\nThis also allows to re-use the same unit tests and obtain the same\r\ncoverage. There is very little of generic code that is purely async\r\nspecific. I have added a few unit tests that address specifically async mode\r\nof operation. db_bench seems to be the most indicative of that.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/12938457", "body": "There might be a problem with Windows WritableFile as it enforces rate limiting internally as originally. Reason being is that in buffered mode we must calculate the requested write number of bytes aligned to sector size.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/12938457/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/14369525", "body": "@yhchiang  CMake Error at CMakeLists.txt:421 (add_executable):\n  Cannot find source file:\n    `utilities/options_util_test.cc`\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/14369525/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/14423439", "body": "@rven1  `<semaphore.h>` breaks windows build\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/14423439/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/14423558", "body": "@siying I can either #ifdef it  or remove entirely from Windows build. Will check either way soon.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/14423558/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/14423826", "body": "@siying Re: env_test is a pinch :+1:  \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/14423826/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/15000730", "body": "@igorcanadi @gunnarku  ROCKSDB_PRIszt requires % before it just like any other PR formatting\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/15000730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/15022945", "body": "@gunnarku Your are right. It shows white so I could not see it. Pls, disregard my comment.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/15022945/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17111842", "body": "@siying This test hangs on Windows waiting for the first SyncPoint which is never signalled. Any pointers?\n\n```\ncolumn_family_test_je.exe!rocksdb::SyncPoint::Process(const std::basic_string<char,std::char_traits<char>,std::allocator<char> > & point, void * cb_arg) Line 109   C++\n```\n\n>   column_family_test_je.exe!rocksdb::ColumnFamilyTest_LogSyncConflictFlush_Test::TestBody() Line 2577 C++\n>     column_family_test_je.exe!testing::internal::HandleSehExceptionsInMethodIfSupportedtesting::Test,void(testing::Test \\* object, void (void) \\* method, const char \\* location) Line 3807  C++\n>     column_family_test_je.exe!testing::internal::HandleExceptionsInMethodIfSupportedtesting::Test,void(testing::Test \\* object, void (void) \\* method, const char \\* location) Line 3858 C++\n>     column_family_test_je.exe!testing::Test::Run() Line 3901    C++\n>     column_family_test_je.exe!testing::TestInfo::Run() Line 4074    C++\n>     column_family_test_je.exe!testing::TestCase::Run() Line 4189    C++\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17111842/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17112054", "body": "@siying Found it. Yes, it returns false. Will disable.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17112054/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17112081", "body": "For the WAL and others it is actually safe but not safe in unbuffered mode.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17112081/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17438331", "body": "@ajkr This breaks windows build because NewChrootEnv is ifdefed on windows. This wastes a lot of time on PRs, please, observe common rules.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17438331/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17440615", "body": "@ajkr I agree. I could have been more specific. What I meant is to find a way and observe the CI build and tests completions. Thank you for fixing it.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17440615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17454916", "body": "This is not a good solution since the feature is present and we want to use it on Windows\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17454916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17457912", "body": "I will take this action point. There are couple of options. 1) Refactor the test not to use ChrootEnv on Windows and keep Chroot logic on Posix 2) Make Chroot work on windows\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17457912/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17474269", "body": "@ajkr Just to make sure we are on the same page. I can implement realpath() but I do not see that backup engine makes use of symbolic links though. If you could, please, better explain to me the purpose of ChrootEnv in connection with the backup. Right now it seems that it is more related to whether one creates symlinks to run tests. Thanks.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17474269/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17491643", "body": "@kradhakrishnan I think this line breaks the build\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17491643/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17494651", "body": "@kradhakrishnan It is easy to spot. The new file is under table folder. So just as you added it under table in src.mk you should do the same in CMakeLists.txt.\nOn a related not. Would it possible for you to create PRs for your internal check ins before merging? This would afford an oppty to observe integration builds including Windows and address simple issues like the above.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17494651/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17635039", "body": "Would a util/aligned_buffer.h be a good alternative so it continues to build on all platforms?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17635039/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17638218", "body": "@kradhakrishnan Also, be advised that File Readers/Writers wrappers already make use of the aligned buffer, so downstream classes at the environment can directly read/write from them.\n\nWindows imlpements unbuffered operations making use of use_os_buffer option. So we have a discrepancy in that dept.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17638218/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/18328883", "body": "Should include port/port.h to handle snprintf\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/18328883/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/20367271", "body": "@adamretter   This should have been strstr() and it now fails the test on Windows", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/20367271/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/20380907", "body": "@adamretter That's because it relies on the message to start with \"lock \" but the messages are different slightly on windows because it comes from a separate piece of code. However, it does contain \"lock \".", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/20380907/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/22074655", "body": "@IslamAbdelRahman I think the comments for PrefixMayMatch are now somewhat misleading.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/22074655/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/22199158", "body": "@boolean5 Line 506 now fails with error \"Directory already exists\" because it is created at line 503", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/22199158/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33834442", "body": "Yes, this diff is deficient. There are other omissions that I noticed.We have a new one coming.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33834442/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33835844", "body": "This will will go away\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33835844/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33835997", "body": "Comment is no longer true.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33835997/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33965773", "body": "Debug diagnostics was (correctly) complaining that the mutex is being destroyed while owned because it is acquired at the start of the __dtor. The proper fix would be to remove the Lock() line since the object must not be ordinarily a subject of MT access at the time of destruction but we apparently decided to play along in this case and simply unlocked it.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33965773/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33981806", "body": "@siying Let's chat quickly after you done with the review. I sent you personal email.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33981806/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33987807", "body": "@siying INT32_MAX is not in limits.h and may not be in stdint.h so it is fare. Do you want a similar approach for other constants or just this one?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33987807/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33991435", "body": "This is a new feature that we needed for integrating this into our database.  Allows to pass a config string and get options instance back. I think this is the only one here.\n\nFrom: Siying Dong [mailto:notifications@github.com]\nSent: Monday, July 6, 2015 3:45 PM\nTo: facebook/rocksdb\nCc: Dmitri Smirnov\nSubject: Re: [rocksdb] Windows Port from Microsoft (#646)\n\nIn table/plain_table_reader.cchttps://github.com/facebook/rocksdb/pull/646#discussion_r33991164:\n\n> ```\n>                            uint64_t file_size,\n> ```\n> - ```\n>                            unique_ptr<TableReader>* table_reader,\n>   ```\n> - ```\n>                            const int bloom_bits_per_key,\n>   ```\n> - ```\n>                            double hash_table_ratio, size_t index_sparseness,\n>   ```\n> - ```\n>                            size_t huge_page_tlb_size, bool full_scan_mode) {\n>   ```\n\nI like the change of passing through PlainTableOptions all the way down. But is it related to the port?\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/facebook/rocksdb/pull/646/files#r33991164.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33991435/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33996972", "body": "@siying Cygwin bash refuses to run format-diff for some bizarre reason. \n\n$ build_tools/format-diff.sh\nbuild_tools/format-diff.sh: line 108: syntax error: unexpected end of file\n\nWould you be fine if I applied this patch on top on my most recent internal code review changes? I am asking because the patch includes some posix only files and others we did not change and they are not related to this port.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/33996972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34395243", "body": "@igorcanadi sure. Pls, let me know when you are done with the review.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34395243/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34395377", "body": "@igorcanadi Understood. However, just FYI, on Windows Release build perf diff is about 10x with Debug and that affects perf related tests. Also on one occasion we had test pass in Debug but not in Release.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34395377/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396329", "body": "@siying After fixing the line endings and installing python scripts and modules, this thing starts but python dumps core. I will rely on you to fix the formatting while I will see to produce a properly formatted code to begin with. GCC, however, runs fine is valuable.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396329/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396634", "body": "@igorcanadi. Yes, this was in previous diff. This will not leak as this is a rough equivalent of reserving a stack space in a dynamic manner. For this reason, if valgrind does not complain, I'd advocate leaving this as is because it would be close to the original code perf wise.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396634/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396799", "body": "@igorcanadi Will add a destructor\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396799/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396955", "body": "@igorcanadi This is an oversight, probably a copy paste from the original porting effort months ago.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396955/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34397617", "body": "@igorcanadi Not really, the semantics of the object and how it was and being used does not suggest that we need two functional copies of it due to the fact that the std::unique_ptr also has copy and assignment deleted. So I chose to replicate those because it yields fast return from functions and container insertion automatically w/o std::move.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34397617/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34401201", "body": "@igorcanadi I will have to address this separately.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34401201/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34528874", "body": "Db.h, env.h, perf_context.h and some others. With .cc files it is easy to include port/port.h\n\nDmitri\n\nFrom: Siying Dong [mailto:notifications@github.com]\nSent: Monday, July 13, 2015 5:53 PM\nTo: facebook/rocksdb\nCc: Dmitri Smirnov\nSubject: Re: [rocksdb] Ensure Windows build w/o port/port.h in public headers (#652)\n\nIn include/rocksdb/public_port.hhttps://github.com/facebook/rocksdb/pull/652#discussion_r34528738:\n\n> +#undef min\n> \n> +#undef max\n> \n> +#undef DeleteFile\n> \n> +#undef GetCurrentTime\n> \n> +\n> \n> +#ifndef strcasecmp\n> \n> +#define strcasecmp _stricmp\n> \n> +#endif\n> \n> +\n> \n> +#ifndef snprintf\n> \n> +#define snprintf _snprintf\n> \n> +#endif\n> \n> +\n> \n> +#ifndef ROCKSDB_PRIszt\n> \n> +#define ROCKSDB_PRIszt \"Iu\"\n> \n> +#endif\n\nWhich public .h file depends on them? Can we simply move it to .cc files?\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/facebook/rocksdb/pull/652/files#r34528738.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34528874/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34529975", "body": "Logv I already moved to env.cc, however, there are many places in env.h that depend on the public_port.h to build\n\nFrom: Siying Dong [mailto:notifications@github.com]\nSent: Monday, July 13, 2015 6:02 PM\nTo: facebook/rocksdb\nCc: Dmitri Smirnov\nSubject: Re: [rocksdb] Ensure Windows build w/o port/port.h in public headers (#652)\n\nIn include/rocksdb/public_port.hhttps://github.com/facebook/rocksdb/pull/652#discussion_r34529150:\n\n> +#undef min\n> \n> +#undef max\n> \n> +#undef DeleteFile\n> \n> +#undef GetCurrentTime\n> \n> +\n> \n> +#ifndef strcasecmp\n> \n> +#define strcasecmp _stricmp\n> \n> +#endif\n> \n> +\n> \n> +#ifndef snprintf\n> \n> +#define snprintf _snprintf\n> \n> +#endif\n> \n> +\n> \n> +#ifndef ROCKSDB_PRIszt\n> \n> +#define ROCKSDB_PRIszt \"Iu\"\n> \n> +#endif\n\nI only find Logger::Logv, which should be able to move to util/env.cc. Any other places I missed?\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/facebook/rocksdb/pull/652/files#r34529150.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34529975/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34530352", "body": "The recurring theme both in db.h and env.h are macros that redefine DeleteFile, GetCurrentTime.\n\nAlthough there are still #undefs remaining from previous porting efforts within the headers which we should remove because having it in one file is much cleaner.\n\nPerf_context.h requires __thread (tls) keyword.\n\nSnprintf is also a function that we hit in the past and sure enough will hit in the future.\n\nThanks,\nDmitri\n\nFrom: Siying Dong [mailto:notifications@github.com]\nSent: Monday, July 13, 2015 6:24 PM\nTo: facebook/rocksdb\nCc: Dmitri Smirnov\nSubject: Re: [rocksdb] Ensure Windows build w/o port/port.h in public headers (#652)\n\nIn include/rocksdb/public_port.hhttps://github.com/facebook/rocksdb/pull/652#discussion_r34530079:\n\n> +#undef min\n> \n> +#undef max\n> \n> +#undef DeleteFile\n> \n> +#undef GetCurrentTime\n> \n> +\n> \n> +#ifndef strcasecmp\n> \n> +#define strcasecmp _stricmp\n> \n> +#endif\n> \n> +\n> \n> +#ifndef snprintf\n> \n> +#define snprintf _snprintf\n> \n> +#endif\n> \n> +\n> \n> +#ifndef ROCKSDB_PRIszt\n> \n> +#define ROCKSDB_PRIszt \"Iu\"\n> \n> +#endif\n\nCan you give examples?\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/facebook/rocksdb/pull/652/files#r34530079.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34530352/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34530942", "body": "@siying I'd also suggest to include that public header into_port_win.h so we do not have duplicates.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34530942/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34531904", "body": "@siying  Sure I can do that. However, more ifdefs means more ifdefs in the future because people will keep adding them.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34531904/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34591430", "body": "So we have on choice then is Win32 and/or _MSVER which as predefined by the compiler?\n\nDmitri\n\nFrom: Igor Canadi [mailto:notifications@github.com]\nSent: Monday, July 13, 2015 9:43 PM\nTo: facebook/rocksdb\nCc: Dmitri Smirnov\nSubject: Re: [rocksdb] Ensure Windows build w/o port/port.h in public headers (#652)\n\nIn include/rocksdb/public_port.hhttps://github.com/facebook/rocksdb/pull/652#discussion_r34536787:\n\n> @@ -0,0 +1,42 @@\n> \n> +// Copyright (c) 2011 The LevelDB Authors. All rights reserved.\n> \n> +// Use of this source code is governed by a BSD-style license that can be\n> \n> +// found in the LICENSE file. See the AUTHORS file for names of contributors.\n> \n> +#pragma once\n> \n> +\n> \n> +// Best used after all the headers in case windows #defines give you trouble\n> \n> +// Minimum subset\n> \n> +\n> \n> +#ifndef ROCKSDB_PUBLIC_PORT_H__\n> \n> +#define ROCKSDB_PUBLIC_PORT_H__\n> \n> +\n> \n> +#ifdef OS_WIN\n\nThis will not work in public header files since public header files are not compiled. We don't pass the defines to our customers.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/facebook/rocksdb/pull/652/files#r34536787.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34591430/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34815038", "body": "@qinzuoyan snprintf will not zero terminate if there is not enough space.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34815038/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34819337", "body": "@siying I believe this is the result of it.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34819337/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34819362", "body": "@siying seems like it\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34819362/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39301014", "body": "@siying I had a constant in env_win then decided to play along. While on the topic, I do believe we should increase the buffer 1.5 times and not twice.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39301014/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39301135", "body": "@siying I do not mind but some compilers/linkers would produce a duplicate definition error.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39301135/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39301349", "body": "@siying  although in unbuffered mode I will still have to make it twice the size but at least the rest of the code might get some perf from x1.5\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39301349/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39317431", "body": "@siying Yes, cygwin says the same\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39317431/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39317460", "body": "@siying yes, I noticed\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39317460/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39317664", "body": "It is simpler this way.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39317664/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "christian-esken": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3167", "title": "SIGSEGV in librocksdbjni when using options.setMaxOpenFiles()", "body": "I want to reduce the memory footprint of RocksDB. As the DB got quite large, i changed options:\r\n`options.setMaxOpenFiles(-1);  // original code. works properly if DB is small enough`\r\n`options.setMaxOpenFiles(20);  // new code. Leads to SIGSEGV`\r\n\r\nI am using RocksDB in Java 8, and tested with rocksdbjni 5.7.2, 5.7.3 and 5.8.0.\r\n\r\n### Expected behavior\r\nRocksDB should run properly and use less memory when using options.setMaxOpenFiles(maxFiles), with maxFiles being a positive number. options are DBoptions. \r\n\r\n\r\n### Actual behavior\r\nRocksDB crashes after some time with SIGSEGV. It does not immediately crash, but after some time, e.g. some seconds or minutes. The smaller I configure the maxOpenFiles, the faster is the crash. I tested the following values: 20, 60, 512. The DB has 1420 *.sst files, with overall size of 352GB.\r\n\r\nRocksDB LOG: [LOG.gz](https://github.com/facebook/rocksdb/files/1471601/LOG.gz)\r\nJava error log: [hs_err_pid24729.log](https://github.com/facebook/rocksdb/files/1471588/hs_err_pid24729.log)\r\n\r\n\r\n### Steps to reproduce the behavior\r\nThis is likely hard to reproduce. It seems to require a large DB, frequent reads, writes and compactions being triggered.  I could not reproduce it on a local test environment. The most important thing is to call options.setMaxOpenFiles(maxFiles), with maxFiles being a positive number. The application in question runs properly for weeks with options.setMaxOpenFiles(-1), but if the database gets too big  the server runs out of memory.\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3167/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "killme2008": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3154", "title": "RocksDBJava: Consider using PhantomReference-based cleaner to replace finalize?", "body": "A idea came from http://mail.openjdk.java.net/pipermail/core-libs-dev/2015-June/034425.htmlph\r\n\r\n`finalize` in RocksDB objects is not a good practice and it will affect GC, make young GC takes more time even when you close the resource explicitly.\r\n\r\nIn our system ,we replace the finalize with  PhantomReference-based cleaner, the young GC cost time drop to almost half of the original.\r\n\r\nGoogle guava lib provides [FinalizablePhantomReference](https://github.com/google/guava/blob/master/guava/src/com/google/common/base/FinalizablePhantomReference.java), and i think it can be introduced into RocksDB Java. If you think this is valuable, i can create a patch for it.\r\n\r\nThanks.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3154/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/343165207", "body": "hello ,is it released?  @zawlazaw ", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/343165207/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "LinuxGit": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3150", "title": "rocksdb open `/sys/devices/system/cpu/online` frequently", "body": "> Note: Please use Issues only for bug reports. For questions, discussions, feature requests, etc. post to dev group: https://www.facebook.com/groups/rocksdb.dev\r\n\r\n### Expected behavior\r\nrocksdb open `/sys/devices/system/cpu/online` once\r\n\r\n### Actual behavior\r\nrocksdb open `/sys/devices/system/cpu/online` frequently\r\n\r\n### Steps to reproduce the behavior\r\nWe use systemtap to trace vfs.open\u3001vfs.read and vfs.write syscalls of [TiKV](https://github.com/pingcap/tikv) process . (TiKV use RocksDB as storage engine.)\r\nAnd find that it opens `/sys/devices/system/cpu/online` frequently.\r\n```\r\nraftstore-2[61275] - open /sys/devices/system/cpu/online\r\nraftstore-2[61275] - read 8192 from devices/system/cpu/online\r\nraftstore-2[61275] - close /sys/devices/system/cpu/online\r\napply worker[61311] - open /sys/devices/system/cpu/online\r\napply worker[61311] - read 8192 from devices/system/cpu/online\r\napply worker[61311] - close /sys/devices/system/cpu/online\r\nraftstore-2[61275] - open /sys/devices/system/cpu/online\r\nraftstore-2[61275] - read 8192 from devices/system/cpu/online\r\nraftstore-2[61275] - close /sys/devices/system/cpu/online\r\n```\r\nThen we print kernel and user space stack backtraces, and find that the syscalls come from rocksdb's [std::thread::hardware_concurrency()](http://en.cppreference.com/w/cpp/thread/thread/hardware_concurrency).\r\nThe function returns the number of concurrent threads supported by calling Linux [get_nprocs](https://www.gnu.org/software/libc/manual/html_node/Processor-Resources.html) library function which opens `/sys/devices/system/cpu/online`, and read 8k data from it.\r\nHere's our systemtap vfs.read probe.\r\n```\r\nprobe vfs.read\r\n{\r\n\tif (target() == pid()) {\r\n\t\tprintf(\"%s[%ld] - read %d from %s\\n\", execname(), tid(), bytes_to_read, reverse_path_walk($file->f_path->dentry))\r\n\t\tprint_ubacktrace()\r\n\t\tprint_backtrace()\r\n\t}\r\n}\r\n```\r\n\r\n Here's our stack traces.\r\n```\r\napply worker[61311] - open /sys/devices/system/cpu/online\r\n 0x7f2755343a40 : __open_nocancel+0x7/0x57 [/usr/lib64/libc-2.17.so]\r\n 0x7f2755350c4e : get_nprocs+0x7e/0x2a0 [/usr/lib64/libc-2.17.so]\r\n 0x7f27568fa769 : _ZNSt6thread20hardware_concurrencyEv+0x9/0x20 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f27568afd63 : _ZN7rocksdb15ConcurrentArenaC1EmPNS_12AllocTrackerEm+0x43/0x180 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f27567fb915 : _ZN7rocksdb8MemTableC2ERKNS_21InternalKeyComparatorERKNS_18ImmutableCFOptionsERKNS_16MutableCFOptionsEPNS_18WriteBufferManagerEmj+0xc5/0x650 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f27568fa769 : _ZNSt6thread20hardware_concurrencyEv+0x9/0x20 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f27568afd63 : _ZN7rocksdb15ConcurrentArenaC1EmPNS_12AllocTrackerEm+0x43/0x180 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f27567fb915 : _ZN7rocksdb8MemTableC2ERKNS_21InternalKeyComparatorERKNS_18ImmutableCFOptionsERKNS_16MutableCFOptionsEPNS_18WriteBufferManagerEmj+0xc5/0x650\r\n 0x7f2755350c4e : get_nprocs+0x7e/0x2a0 [/usr/lib64/libc-2.17.so]\r\n 0x7f27568fa769 : _ZNSt6thread20hardware_concurrencyEv+0x9/0x20 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f27568afd63 : _ZN7rocksdb15ConcurrentArenaC1EmPNS_12AllocTrackerEm+0x43/0x180 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f27567fb915 : _ZN7rocksdb8MemTableC2ERKNS_21InternalKeyComparatorERKNS_18ImmutableCFOptionsERKNS_16MutableCFOptionsEPNS_18WriteBufferManagerEmj+0xc5/0x650\r\n [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f2756900b1a : _ZN7rocksdb16ColumnFamilyData20ConstructNewMemtableERKNS_16MutableCFOptionsEm+0x4a/0x70 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f27567a4d6e : _ZN7rocksdb6DBImpl14SwitchMemtableEPNS_16ColumnFamilyDataEPNS0_12WriteContextE+0x4ce/0x1970 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f27567a65f7 : _ZN7rocksdb6DBImpl15ScheduleFlushesEPNS0_12WriteContextE+0x47/0x110 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f27567a6959 : _ZN7rocksdb6DBImpl15PreprocessWriteERKNS_12WriteOptionsEPbPNS0_12WriteContextE+0x299/0x400 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f27567a7627 : _ZN7rocksdb6DBImpl18PipelinedWriteImplERKNS_12WriteOptionsEPNS_10WriteBatchEPNS_13WriteCallbackEPmmb+0x677/0x11c0 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f27567a920e : _ZN7rocksdb6DBImpl9WriteImplERKNS_12WriteOptionsEPNS_10WriteBatchEPNS_13WriteCallbackEPmmb+0xce/0x1c60 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f27567aadc7 : _ZN7rocksdb6DBImpl5WriteERKNS_12WriteOptionsEPNS_10WriteBatchE+0x27/0x40 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f275677a2e1 : crocksdb_write+0x21/0x60 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f275677424f : _ZN7rocksdb7rocksdb2DB5write17h8d75b741c2257b10E+0x3f/0xd0 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f2756645bf4 : _ZN145_$LT$tikv..raftstore..store..worker..apply..Runner$u20$as$u20$tikv..util..worker..Runnable$LT$tikv..raftstore..store..worker..apply..Task$GT$$GT$3run17h7303c7661b132e04E+0x2114/0x5280 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f27562c2036 : _ZN3std10sys_common9backtrace28__rust_begin_short_backtrace17h88131a93d4f1779fE+0x10a6/0x1980 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f27562c9ba3 : _ZN3std9panicking3try7do_call17hfc6aacd3560e102dE+0x53/0x60 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f2756db7c2d : __rust_maybe_catch_panic+0x1d/0x90 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f2756376b83 : _ZN50_$LT$F$u20$as$u20$alloc..boxed..FnBox$LT$A$GT$$GT$8call_box17h85f9f2bff29f82feE+0x103/0x220 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\n 0x7f2756daf6fc : _ZN3std3sys3imp6thread6Thread3new12thread_start17h0aeebbbbd5e5d9cdE+0x8c/0x120 [/data3/tangliu/deploy/tikv1/bin/tikv-server]\r\nReturning from:  0xffffffff811fbf30 : vfs_open+0x0/0xe0 [kernel]\r\nReturning to  :  0xffffffff8120cafd : do_last+0x1ed/0x12a0 [kernel]\r\n 0xffffffff8120dc72 : path_openat+0xc2/0x490 [kernel]\r\n 0xffffffff8120fdeb : do_filp_open+0x4b/0xb0 [kernel]\r\n 0xffffffff811fd2f3 : do_sys_open+0xf3/0x1f0 [kernel]\r\n 0xffffffff811fd40e : sys_open+0x1e/0x20 [kernel]\r\n 0xffffffff816967d2 : tracesys+0xdd/0xe2 [kernel]\r\n````\r\nHere's some snippets in stack traces.\r\nhttps://github.com/facebook/rocksdb/blob/3c327ac2d0fd50bbd82fe1f1af5de909dad769e6/util/core_local.h#L45\r\n\r\nhttps://github.com/facebook/rocksdb/blob/3c327ac2d0fd50bbd82fe1f1af5de909dad769e6/util/concurrent_arena.h#L40", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3150/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mdcallag": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3147", "title": "bottommost_compression doesn't reject unsupported compression types", "body": "I think this explains a user problem. They set bottommost_compression to kZSTD, but that is not supported, so they don't get compression and use 10X more space than expected.\r\nhttps://groups.google.com/forum/#!topic/myrocks-dev/N95FCR2UGoA\r\n\r\nMy server supports kZSTDNotFinalCompression but not kZSTD.\r\n\r\nWhen I try to set compression for a level to kZSTD the server doesn't start as expected:\r\n```\r\n[ERROR] RocksDB: Error opening instance, Status Code: 4, Status: Invalid argument: Compression type ZSTD is not linked wit\r\nh the binary.\r\n```\r\nWhen I set bottommost_compression to kZSTD the server starts.\r\n\r\nThis is rejected, as expected:\r\n```\r\nrocksdb_default_cf_options=compression_per_level=kLZ4Compression:kZSTD;bottommost_compression=kLZ4Compression\r\n```\r\n\r\nThis not rejected, as expected, because the compression types are supported:\r\n```\r\nrocksdb_default_cf_options=compression_per_level=kLZ4Compression:kLZ4Compression;bottommost_compression=kZSTDNotFinalCompression\r\n```\r\n\r\nThis is accepted but should be rejected because kZSTD is not supported:\r\n```\r\nrocksdb_default_cf_options=compression_per_level=kLZ4Compression:kLZ4Compression;bottommost_compression=kZSTD\r\n```\r\n\r\nFrom RocksDB LOG\r\n```\r\n2017/11/08-10:42:07.701440 7f7a7fc8da40 Compression algorithms supported:\r\n2017/11/08-10:42:07.701441 7f7a7fc8da40         Snappy supported: 1\r\n2017/11/08-10:42:07.701442 7f7a7fc8da40         Zlib supported: 1\r\n2017/11/08-10:42:07.701443 7f7a7fc8da40         Bzip supported: 0\r\n2017/11/08-10:42:07.701444 7f7a7fc8da40         LZ4 supported: 1\r\n2017/11/08-10:42:07.701445 7f7a7fc8da40         ZSTDNotFinal supported: 1\r\n2017/11/08-10:42:07.701451 7f7a7fc8da40         ZSTD supported: 0\r\n...\r\n2017/11/08-10:42:07.701662 7f7a7fc8da40        Options.compression[0]: LZ4\r\n2017/11/08-10:42:07.701664 7f7a7fc8da40        Options.compression[1]: LZ4\r\n2017/11/08-10:42:07.701665 7f7a7fc8da40                  Options.bottommost_compression: ZSTD\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3147/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3146", "title": "DumpSupportInfo doesn't log all supported compression types", "body": "read DumpSupportInfo in https://github.com/facebook/rocksdb/blob/master/util/compression.h#L131\r\n\r\nIt is doesn't print out for all supported compression types from:\r\n\r\n```\r\ninline bool CompressionTypeSupported(CompressionType compression_type) {\r\n  switch (compression_type) {\r\n    case kNoCompression:\r\n      return true;\r\n    case kSnappyCompression:\r\n      return Snappy_Supported();\r\n    case kZlibCompression:\r\n      return Zlib_Supported();\r\n    case kBZip2Compression:\r\n      return BZip2_Supported();\r\n    case kLZ4Compression:\r\n      return LZ4_Supported();\r\n    case kLZ4HCCompression:\r\n      return LZ4_Supported();\r\n    case kXpressCompression:\r\n      return XPRESS_Supported();\r\n    case kZSTDNotFinalCompression:\r\n      return ZSTDNotFinal_Supported();\r\n    case kZSTD:\r\n      return ZSTD_Supported();\r\n    default:\r\n      assert(false);\r\n      return false;\r\n  }\r\n}\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3146/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/1809", "title": "perf_context timers enabled for DBImpl::mutex_", "body": "This is a request to clean up InstrumentedMutex:\r\n1)  DBImpl::mutex_ enabled the perf_context timer by default\r\n2) When both the perf_context timer and the stats timer are requested, can we only start the timer once in InstrumentedMutex::Lock?\r\n3) Can the check for the stats timer be simpler? ShouldReportToStats checks 3 conditions today and in the common case all 3 must be evaluated. I want mutex entry & exit to be as fast as possible to reduce contention.\r\n\r\n-- More detail for problem #1 \r\n\r\nDBImpl::mutex_ is a source of mutex contention and I think the perf_context timer is enabled by default for it which means that InstrumentedMutex::Lock will be slower. It is enabled because DB_MUTEX_WAIT_MICROS is used in ctor for mutex_\r\nhttps://github.com/facebook/rocksdb/blob/master/db/db_impl.cc#L318\r\n\r\nAnd then it is used by the macro:\r\nhttps://github.com/facebook/rocksdb/blob/master/util/instrumented_mutex.cc#L20\r\n\r\nTo reproduce do \"mark release\" after applying this diff\r\n```\r\ndiff --git a/util/perf_context_imp.h b/util/perf_context_imp.h\r\nindex ee1a7c3..c0b00d6 100644\r\n--- a/util/perf_context_imp.h\r\n+++ b/util/perf_context_imp.h\r\n@@ -36,6 +36,7 @@ namespace rocksdb {\r\n#define PERF_CONDITIONAL_TIMER_FOR_MUTEX_GUARD(metric, condition) \\\r\nPerfStepTimer perf_step_timer_##metric(&(perf_context.metric), true); \\\r\nif ((condition)) { \\\r\n+fprintf(stderr, \"perf timer\\n\"); \\\r\nperf_step_timer_##metric.Start(); \\\r\n}\r\n```\r\n\r\nRun: ./db_bench --db=/s/bld/rdb --benchmarks=overwrite\r\nSee lots of printing from that fprintf I added.\r\n\r\ndb_bench uses this for the perf level:\r\nDEFINE_int32(perf_level, rocksdb::PerfLevel::kDisable, \"Level of perf collection\");", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/1809/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28651273", "body": "Do you need to install gflags? Is that a dependency we need to document?\n\nOn a recent ubuntu 12.10 install I had to do:\nsudo apt-get install libgflags-dev\n\n---\n\nAnd I also had to add this to db/db_test.cc for the default (debug?) build\nto define \"truncate\".\n\ndiff --git a/db/db_test.cc b/db/db_test.cc\nindex e246798..2f1a1e3 100644\n--- a/db/db_test.cc\n+++ b/db/db_test.cc\n@@ -27,6 +27,7 @@\n #include \"util/testharness.h\"\n #include \"util/testutil.h\"\n #include \"utilities/merge_operators.h\"\n+#include <unistd.h>\n\n namespace rocksdb {\n\nOn Sun, Nov 17, 2013 at 3:03 AM, David Yu notifications@github.com wrote:\n\n> I'm getting these erros:\n> db/db_bench.cc:14:27: fatal error: gflags/gflags.h: No such file or\n> directory\n> compilation terminated.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/facebook/rocksdb/issues/4#issuecomment-28646477\n> .\n\n## \n\nMark Callaghan\nmdcallag@gmail.com\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28651273/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28652050", "body": "RocksDB includes snappy and libs were built during make. However, I don't see how snappy or zlib or bz2 were linked. They aren't mentioned in make output and bzip2 is installed for sure. I think the makefile needs help. Will start a separate issue for that.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28652050/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28657152", "body": "The point of having build_tools/build_detect_platform is for things to run\nfine when everything isn't there. That isn't the case right now.\n\nOn Sun, Nov 17, 2013 at 8:50 AM, jkeys089 notifications@github.com wrote:\n\n> I just pulled in everything in build_tools/build_detect_platform (\n> https://github.com/facebook/rocksdb/blob/master/build_tools/build_detect_platform#L186)\n> and the tests now run fine for me.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/facebook/rocksdb/issues/6#issuecomment-28652273\n> .\n\n## \n\nMark Callaghan\nmdcallag@gmail.com\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28657152/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28657832", "body": "And BZip2_Uncompress uses Z_OK and Z_BUF_ERROR which also come from zlib. Those references should be changed to BZ_\\* equivalents.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28657832/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28658846", "body": "Does this test need checks to avoid compression tests when -DSNAPPY and other compression options were not compiled? \"make check\" passes when snappy, bzip2 and zlib have been installed.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28658846/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28658891", "body": "Yes, need to add \"include <unistd.h>\" to fix he compile failure\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28658891/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28705025", "body": "I don't think gcc 4.8 is required. I compile with 4.7.2 using Ubuntu 12.10\n\nOn Mon, Nov 18, 2013 at 6:52 AM, vinoth chandar notifications@github.comwrote:\n\n> Works now.. Thanks guys!\n> \n> For others: On a fresh 12.04 LTS, the process is as roughly follows\n> 1. sudo apt-get install libsnappy-dev\n> 2. Upgrade gcc/g++ to 4.8\n>    See http://askubuntu.com/questions/312620/how-do-i-install-gcc-4-8-1-on-ubuntu-13-04\n>    (it is for 13.04. but the process is common I think)\n> 3. Install gflags\n>    (its not 1-command if you are < 12.10)\n>    see http://askubuntu.com/questions/312173/installing-gflags-12-04?rq=1\n>    \n>    then make, make install in rocksdb and done.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/facebook/rocksdb/issues/4#issuecomment-28703440\n> .\n\n## \n\nMark Callaghan\nmdcallag@gmail.com\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28705025/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29532261", "body": "Your suggestions for reducing write-amp are valid. However getting more data into memory isn't trival. If you make the memtable huge then queries of it and inserts into it can be slow. An alternative is to keep old memtables in memory and then merge several of them to create a larger L0 file. RocksDB also has alternate memtable implementations where you sacrifice some features provided by the skip list to get better performance.\n\nHaving spent a lot of time thinking about write-amplification this year, I am happy to read what others write about it. With RocksDB we now have LevelDB-style (leveled) and HBase-style compaction (universal). I think you are describing write-amp for leveled compaction above and write-amp for it is usually much worse than for universal. However, I don't understand why you include the \"2*\" term in the cost formula above. I think that might be doubling the write-amp that is likely to occur. The 2X will account for the reads & writes that can be done during compaction, but I call that compaction amplification, not write amplification.\n\nI use the following estimate for write-amp with leveled compaction when there are N levels -- L0, L1, L2, ... LN-1.\n- +1 for the redo log\n- +1 for the write to an L0 file\n- +1 for compaction to L1 assuming size(L0 files in compaction) == size(L1)\n- +10 for each level that follows assuming default growth ratio of 10 between levels was used\n\nSo my estimate is 3 + 10*X where X is the number of levels excluding L0 and L1. If you use the --stats_per_interval and --stats_interval options with db_bench when doing a benchmark that writes data it will show the amount of reads and writes per level and the total write-amp and the value printed there tends to match the estimated cost I describe here.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29532261/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29544600", "body": "My guess is that these are the key values as they prevent compaction from\nL0->L1 during the load...\nctrig=10000000; delay=10000000; stop=10000000\n\nOn Fri, Nov 29, 2013 at 6:07 PM, dhruba borthakur\nnotifications@github.comwrote:\n\n> If you look at\n> https://github.com/facebook/rocksdb/wiki/Performance-Benchmarks#2-bulk-load-of-keys-in-random-order,\n> you will find the command line options to load data in bulk-load fashion.\n> \n> echo \"Bulk load database into L0....\"\n> bpl=10485760;overlap=10;mcz=2;del=300000000;levels=2;ctrig=10000000;\n> delay=10000000; stop=10000000; wbn=30; mbc=20; mb=1073741824;wbs=268435456;\n> dds=1; sync=0; r=1000000000; t=1; vs=800; bs=65536; cs=1048576; of=500000;\n> si=1000000; ./db_bench --benchmarks=fillrandom --disable_seek_compaction=1\n> --mmap_read=0 --statistics=1 --histogram=1 --num=$r --threads=$t\n> --value_size=$vs --block_size=$bs --cache_size=$cs --bloom_bits=10\n> --cache_numshardbits=4 --open_files=$of --verify_checksum=1\n> --db=/data/mysql/leveldb/test --sync=$sync --disable_wal=1\n> --compression_type=zlib --stats_interval=$si --compression_ratio=50\n> --disable_data_sync=$dds --write_buffer_size=$wbs\n> --target_file_size_base=$mb --max_write_buffer_number=$wbn\n> --max_background_compactions=$mbc\n> --level0_file_num_compaction_trigger=$ctrig\n> --level0_slowdown_writes_trigger=$delay --level0_stop_writes_trigger=$stop\n> --num_levels=$levels --delete_obsolete_files_period_micros=$del\n> --min_level_to_compress=$mcz --max_grandparent_overlap_fac tor=$overlap\n> --stats_per_interval=1 --max_bytes_for_level_base=$bpl --memtablerep=vector\n> --use_existing_db=0 --disable_auto_compactions=1\n> --source_compaction_factor=10000000\n> echo \"Running manual compaction to do a global sort map-reduce style....\"\n> bpl=10485760;overlap=10;mcz=2;del=300000000;levels=2;ctrig=10000000;\n> delay=10000000; stop=10000000; wbn=30; mbc=20; mb=1073741824;wbs=268435456;\n> dds=1; sync=0; r=1000000000; t=1; vs=800; bs=65536; cs=1048576; of=500000;\n> si=1000000; ./db_bench --benchmarks=compact --disable_seek_compaction=1\n> --mmap_read=0 --statistics=1 --histogram=1 --num=$r --threads=$t\n> --value_size=$vs --block_size=$bs --cache_size=$cs --bloom_bits=10\n> --cache_numshardbits=4 --open_files=$of --verify_checksum=1\n> --db=/data/mysql/leveldb/test --sync=$sync --disable_wal=1\n> --compression_type=zlib --stats_interval=$si --compression_ratio=50\n> --disable_data_sync=$dds --write_buffer_size=$wbs\n> --target_file_size_base=$mb --max_write_buffer_number=$wbn\n> --max_background_compactions=$mbc\n> --level0_file_num_compaction_trigger=$ctrig\n> --level0_slowdown_writes_trigger=$delay --level0_stop_writes_trigger=$stop\n> --num_levels=$levels --delete_obsolete_files_period_micros=$del\n> --min_level_to_compress=$mcz --max_grandparent_overlap_factor =$overlap\n> --stats_per_interval=1 --max_bytes_for_level_base=$bpl --memtablerep=vector\n> --use_existing_db=1 --disable_auto_compactions=1\n> --source_compaction_factor=10000000\n> du -s -k test\n> 504730832 test\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/facebook/rocksdb/issues/21#issuecomment-29544041\n> .\n\n## \n\nMark Callaghan\nmdcallag@gmail.com\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29544600/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29595110", "body": "RocksDB has two compaction algorithms -- leveled and universal. leveled is what you we discussed earlier on this thread. universal is more like hbase and write amplification is much lower with it. Try db_bench --compaction_type=1. But we really need to provide more docs for it.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29595110/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31474605", "body": "Is there an option to use madvise for those reads?\n\nOn Thu, Jan 2, 2014 at 10:50 AM, Kai Liu notifications@github.com wrote:\n\n> Can you elaborate this problem a little bit more? What are the \"useless\n> values\"?\n> \n> Right now when compacting sst files, we'll not load the whole file;\n> instead, we'll read 1 block (by default it is 4k) to the memory.\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/facebook/rocksdb/issues/49#issuecomment-31474480\n> .\n\n## \n\nMark Callaghan\nmdcallag@gmail.com\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31474605/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34389291", "body": "If you don't mind the risk of hanging/crashing your server then thread\nstack traces during the stall would be useful for debugging --\nhttp://poormansprofiler.org/\n\nOn Thu, Feb 6, 2014 at 3:42 PM, Albert Strasheim\nnotifications@github.comwrote:\n\n> Thanks. Here's another log:\n> \n> https://drive.google.com/file/d/0B0b-iwt-kSG2SE40NGxZZUpLRlk/edit?usp=sharing\n> \n> I still hit a 10 minute stall at around 23:19.\n> \n> Logs from around that time included. Looking forward to more debug logging.\n> \n> 2014/02/06-22:56:42.837503 7f14dcff9700 compacted to: files[28 474 870 0 0\n> 0 ], 99.1 MB/sec, level 2, files in(1, 2) out(4) MB in(64.1, 128.2)\n> out(192.2), read-write-amplify(6.0) write-amplify(3.0) OK\n> 2014/02/06-22:56:43.252372 7f14f57fa700 compacted to: files[28 472 873 0 0\n> 0 ], 121.2 MB/sec, level 2, files in(2, 2) out(5) MB in(128.2, 128.1)\n> out(256.2), read-write-amplify(4.0) write-amplify(2.0) OK\n> 2014/02/06-22:56:43.256281 7f14b0996700 compacted to: files[28 469 876 0 0\n> 0 ], 78.8 MB/sec, level 2, files in(3, 2) out(5) MB in(179.8, 128.1)\n> out(307.9), read-write-amplify(3.4) write-amplify(1.7) OK\n> 2014/02/06-23:09:52.407980 7f14f57fa700 compacted to: files[36 1353 876 0\n> 0 0 ], 313.8 MB/sec, level 1, files in(28, 469) out(1353) MB in(55573.6,\n> 28631.1) out(84127.6), read-write-amplify(3.0) write-amplify(1.5) OK\n> 2014/02/06-23:28:34.589486 7f14bccca700 compacted to: files[28 2466 876 0\n> 0 0 ], 327.8 MB/sec, level 1, files in(36, 1353) out(2466) MB in(71452.9,\n> 84127.6) out(155472.7), read-write-amplify(4.4) write-amplify(2.2) OK\n> 2014/02/06-23:28:48.287452 7f14dd7fa700 compacted to: files[28 2465 877 0\n> 0 0 ], 74.6 MB/sec, level 2, files in(1, 1) out(2) MB in(64.1, 64.1)\n> out(128.2), read-write-amplify(4.0) write-amplify(2.0) OK\n> 2014/02/06-23:28:48.289521 7f14f4ff9700 compacted to: files[28 2462 880 0\n> 0 0 ], 74.1 MB/sec, level 2, files in(1, 1) out(2) MB in(64.1, 62.0)\n> out(126.1), read-write-amplify(3.9) write-amplify(2.0) OK\n> 2014/02/06-23:28:48.289651 7f14f57fa700 compacted to: files[28 2462 880 0\n> 0 0 ], 73.2 MB/sec, level 2, files in(1, 1) out(2) MB in(64.1, 64.1)\n> out(128.2), read-write-amplify(4.0) write-amplify(2.0) OK\n> \n> ## \n> \n> Reply to this email directly or view it on GitHubhttps://github.com/facebook/rocksdb/issues/54#issuecomment-34389080\n> .\n\n## \n\nMark Callaghan\nmdcallag@gmail.com\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34389291/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34490055", "body": "Vague advice from someone who has spent a lot of time (weeks, months maybe)\ndoing perf tests on fast HW with leveled compaction. If you need to sustain\na high write rate then you have two choices:\n1) leveled compaction and expect stalls\n2) universal compaction\n\nOne problem with leveled compaction is that either L0->L1 compaction can be\ndone or L1->L2 compaction can be done. They can't be concurrent because\nL0->L1 needs exclusive access to all/most L1 files and that blocks L1->L2\ncompaction.\n\nA workaround is to make L0->L1 compaction as fast as possible and that is\ndone by making sizeof(L0) similar to sizeof(L1) and making sure that both\nlevels don't get too big. I usually trigger compaction when the L0 is less\nthan 1GB (maybe 256MB or 512MB).\n\nOn Fri, Feb 7, 2014 at 11:09 AM, Igor Canadi notifications@github.comwrote:\n\n> Here's an insight. In the stable state, the level0 compaction with 32\n> files takes longer than it takes for your writer to generate 32 new level0\n> files.\n> 1. Compaction takes 32 files in level0 and starts compaction (no other\n>    compactions can happen on level0)\n> 2. while compaction is happening, writer dumps 32 new memtables (total\n>    is 64 now), causing hard stall.\n> 3. Compaction finishes, clears 32 files from level0, 32 files are left.\n> 4. Goto 1.\n> \n> Did you try with bigger target_file_size_base?\n> \n> The universal style compactions lowers write amplification and can sustain\n> higher write throughput. It's a good thing to try if your goal is to\n> optimize write throughput.\n> \n> ## \n> \n> Reply to this email directly or view it on GitHubhttps://github.com/facebook/rocksdb/issues/54#issuecomment-34489164\n> .\n\n## \n\nMark Callaghan\nmdcallag@gmail.com\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34490055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34491087", "body": "My perf model is that there are two things that have to be fast enough to\nkeep up with the production of new L0 files, and both of these things can't\nrun concurrently. The things are L0->L1 compaction and L1->L2 compaction.\nIf you generate X L0 files per minute then you must do both of these at the\nsame rate.\n\nYou can be clever and try to reduce stalls with proper tuning for leveled\ncompaction. But the simple solution is universal compaction.\n\nOn Fri, Feb 7, 2014 at 11:24 AM, Albert Strasheim\nnotifications@github.comwrote:\n\n> Hi. Makes sense. I haven't experimented with bigger target_file_size_base\n> yet. What's the reasoning behind why it might help?\n> \n> 2677091118 records in 3h1m21.811418742s: 246015 records/sec 135\n> MiB/sec... wrote 1431 GiB to the FIO drive before compaction hit first\n> ENOSPC\n> \n> This is the total throughput measured on the put side including all the\n> stall time. So my understanding is that if I write to this database at a\n> bit less than 135 MiB/sec, I should be able to avoid the stalls.\n> \n> I am repeating the test with Snappy compression enabled now, and then I'll\n> try universal compaction.\n> \n> Feature request: it would be _very_ useful to have an API to Write a\n> batch with a timeout, so that if my RocksDB is stalling under me, I can at\n> least easily signal an error and/or arrange to send the data I'm trying to\n> write somewhere else (another DB, or /dev/null).\n> \n> It seems that any way to optimize or parallelize the compaction that is\n> stalling might help. I don't think I'm I/O bound on this process (I'll make\n> some flame graphs soon). Low hanging fruit: don't checksum so much, or\n> checksum faster. Anything else?\n> \n> I am basically building a big circular buffer to retain some high volume\n> time series data in order for a few hours, with a few additional indexes\n> (please finish column families! :)).\n> \n> @mdcallag https://github.com/mdcallag thanks for the advice. I've been\n> reaching the same conclusions.\n> \n> ## \n> \n> Reply to this email directly or view it on GitHubhttps://github.com/facebook/rocksdb/issues/54#issuecomment-34490669\n> .\n\n## \n\nMark Callaghan\nmdcallag@gmail.com\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34491087/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36088120", "body": "Can you explain more about your test context? Too slow is a strong claim.\nToo slow for one workload is very likely. I assume your test database was\nnot cached in the RocksDB/LevelDB block cache but did fit in the OS\nfilesystem cache. In that case mmap reads are faster than doing pread.\n\nOn Tue, Feb 25, 2014 at 7:25 PM, cnstar9988 notifications@github.comwrote:\n\n> 2.4GHZ 4 CPU(24 cores), 512G RAM.\n> I tested on another SSD with write and read. rocksdb is too slow compare\n> to leveldb.\n> I use nmon and pstack, nmon indicates that there is too many cpu +sys,\n> pstack indiate on pread64.\n> \n> I think default options->allow_mmap_reads = false is the problem.\n> I change options->allow_mmap_reads = true, run well.\n> \n> ## \n> \n> Reply to this email directly or view it on GitHubhttps://github.com/facebook/rocksdb/issues/88#issuecomment-36087190\n> .\n\n## \n\nMark Callaghan\nmdcallag@gmail.com\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36088120/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36093846", "body": "RocksDB has a lot of options, perhaps too many. mmap reads are disabled by\ndefault. If you want to use RocksDB then enabling mmap reads might fix this\nfor you. Increasing the size of the block cache so the database is cached\nby it might also help.\n\nOn Tue, Feb 25, 2014 at 9:54 PM, cnstar9988 notifications@github.comwrote:\n\n> My app, more than 10 threads.\n> each threads: put key-value into db is key not exist; Get old value if key\n> is exist and then overwrite with newValue.\n> \n> maybe 10x slower, (key=20bytes, value=30bytes, reccount=1000 0000).\n> \n>  I assume your test database was not cached in the RocksDB/LevelDB block\n> cache but did fit in the OS filesystem cache.\n> \n>  I use default options.\n> \n> ## \n> \n> Reply to this email directly or view it on GitHubhttps://github.com/facebook/rocksdb/issues/88#issuecomment-36093361\n> .\n\n## \n\nMark Callaghan\nmdcallag@gmail.com\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36093846/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/4643282", "body": "Do we make it clear anywhere that is the case? At some point in time InnoDB\nhad to do special things for OSX to make sure that fsync really forced data\nto disk. See\nhttp://dev.mysql.com/doc/refman/5.6/en/innodb-configuration.html or google\nfor more details.\n# if defined(HAVE_DARWIN_THREADS)\n# ifndef F_FULLFSYNC\n\n```\n    /* The following definition is from the Mac OS X 10.3 <sys/fcntl.h>\n```\n\n*/\n# define F_FULLFSYNC 51 /\\* fsync + ask the drive to flush to the media */\n# elif F_FULLFSYNC != 51\n# error \"F_FULLFSYNC != 51: ABI incompatibility with Mac OS X 10.3\"\n# endif\n\n```\n    /* Apple has disabled fsync() for internal disk drives in OS X. That\n    caused corruption for a user when he tested a power outage. Let us\n```\n\nin\n        OS X use a nonstandard flush method recommended by an Apple\n        engineer. */\n\n```\n    if (!srv_have_fullfsync) {\n            /* If we are not on an operating system that supports this,\n            then fall back to a plain fsync. */\n\n            ret = os_file_fsync(file);\n    } else {\n            ret = fcntl(file, F_FULLFSYNC, NULL);\n\n            if (ret) {\n                    /* If we are not on a file system that supports\n```\n\nthis,\n                        then fall back to a plain fsync. */\n                        ret = os_file_fsync(file);\n                }\n        }\n# else\n\nOn Tue, Nov 19, 2013 at 10:24 AM, Kai Liu notifications@github.com wrote:\n\n> As our primary production platform is Linux, We do not (or plan to) run\n> any production workload on mac.\n> We made sure rocksdb can be compiled on OSX just for convenience :-)\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/facebook/rocksdb/commit/97d8e573a6f97cbf40b55744110c8268689d510d#commitcomment-4643173\n> .\n## \n\nMark Callaghan\nmdcallag@gmail.com\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4643282/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28323868", "body": "I recently added --stats_interval_seconds and now use that for most benchmarks so I don't have to figure out reasonable values for --stats_interval\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28323868/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28323927", "body": "Why are max_background_{compactions,flushes} not greater than 1?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28323927/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28324051", "body": "I think max_background_flushes should be larger than one so that all 5 of the non-active write buffers can be flushed concurrently (set it to between 3 and 5).\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28324051/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28366441", "body": "I have done a lot of work to make tools/run_flash_bench.sh configurable. It\nruns many tests via tools/benchmark.sh and now via environment variables\nyou can set things like memory (write_buffer_size), number of background\nthreads and number of \"user\" threads. Without looking at your diff again it\nmight be good to do that here. However, if you statically configure this\nfor a small server (low memory, low threads) then that should be fine.\n\nOn Wed, Apr 15, 2015 at 5:12 AM, Pooya Shareghi notifications@github.com\nwrote:\n\n> In benchmark/cpp_xorUpdateRandom_readRandom.sh\n> https://github.com/facebook/rocksdb/pull/575#discussion_r28365462:\n> \n> > +####   Statistics                       ####\n> > +############################################\n> > +# Report database statistics (true/false)\n> > +statistics=1\n> > +\n> > +# Print histogram of operation timings (true/false)\n> > +histogram=1\n> > +\n> > +# a temp varibale that estimates the total size of an entry\n> > +entry_size=$(($key_size + $value_size))\n> > +\n> > +# Stats are reported every N operations when this is greater than zero.\n> > +# When 0 the interval grows over time. The formula below comes out of an\n> > +# experiment where we had an 100K interval with 16B keys and 8B values.\n> > +# interval_entry_size=100K_(16+8), hence interval = 2400K/entry_size\n> > +stats_interval=$((240000000 / $entry_size))\n> \n> Great work! I will add that as well. Since stats_interval_seconds\n> overrides stats_interval, I will keep both.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/facebook/rocksdb/pull/575/files#r28365462.\n\n## \n\nMark Callaghan\nmdcallag@gmail.com\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28366441/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "saurabhkadekodi": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3135", "title": "Segmentation fault when using rocksdb JNI and the Java wrapper", "body": "> Note: Please use Issues only for bug reports. For questions, discussions, feature requests, etc. post to dev group: https://www.facebook.com/groups/rocksdb.dev\r\n\r\n### Expected behavior\r\nRocksDB does not crash on key-value insertions.\r\n\r\n### Actual behavior\r\nAny insertion into the DB results in the following crash:\r\n`SIGSEGV (0xb) at pc=0x000000012a774ec5, pid=48117, tid=0x0000000000006903`\r\n\r\n### Steps to reproduce the behavior\r\nConfiguration:\r\n```\r\nRocksDB.loadLibrary();\r\nOptions options = new Options();\r\noptions.setCreateIfMissing(true);\r\noptions.setWriteBufferSize(268435456); // 256MB\r\noptions.prepareForBulkLoad();\r\noptions.setMaxOpenFiles(-1); // all files open\r\n```\r\n\r\nThe stacktrace of the crash is:\r\n```\r\nCurrent thread (0x00007fcf252a0000):  JavaThread \"FilePacketWriterExecutor-0\" daemon [_thread_in_native, id=27907, stack(0x000070000fdf3000,0x000070000fef3000)]\r\n\r\nsiginfo: si_signo: 11 (SIGSEGV), si_code: 1 (SEGV_MAPERR), si_addr: 0x0000000000000048\r\n\r\nRegisters:\r\nRAX=0x0000000000000000, RBX=0x00007fcf25ce2800, RCX=0x000070000fef21b0, RDX=0x000000012fcd6e30\r\nRSP=0x000070000fef21b0, RBP=0x000070000fef2240, RSI=0x00007fcf251efc00, RDI=0x000070000fef21f0\r\nR8 =0x000070000fef21c0, R9 =0x00007fcf25ce2800, R10=0x00007fcf25800000, R11=0x00007fc7ba088a00\r\nR12=0x0000000000000029, R13=0x00000000000068df, R14=0x00007fcf252a01f8, R15=0x00007fcf26d416c0\r\nRIP=0x000000012f505ec5, EFLAGS=0x0000000000010246, ERR=0x0000000000000004\r\n  TRAPNO=0x000000000000000e\r\n\r\nTop of Stack: (sp=0x000070000fef21b0)\r\n0x000070000fef21b0:   00007fcf26d416c0 0000000000000029\r\n0x000070000fef21c0:   00007fcf25ce2800 00000000000068df\r\n0x000070000fef21d0:   0000000000000000 000000012fcd6e30\r\n0x000070000fef21e0:   00007fcf251efc00 00007fcf26d416c0\r\n0x000070000fef21f0:   000070000fef2240 00007fffbcc55148\r\n0x000070000fef2200:   0000000000000000 0000000000000000\r\n0x000070000fef2210:   0000000100000001 0000000000000029\r\n0x000070000fef2220:   000070000fef2388 00007fcf251efc00\r\n0x000070000fef2230:   00007fcf252a01f8 0000000000000000\r\n0x000070000fef2240:   000070000fef22a0 000000012f506002\r\n0x000070000fef2250:   0000000000000029 000070000fef2370\r\n0x000070000fef2260:   0000000000000000 00000000000068df\r\n0x000070000fef2270:   000000012f505f70 00000001158f0340\r\n0x000070000fef2280:   0000000000000000 00000001158f0338\r\n0x000070000fef2290:   000070000fef23a0 00007fcf252a0000\r\n0x000070000fef22a0:   000070000fef2340 0000000115a489f4\r\n0x000070000fef22b0:   000070000fef2370 0000000100000000\r\n0x000070000fef22c0:   00007fcf000068df 00007fcf252a0000\r\n0x000070000fef22d0:   00000001158f0340 000070000fef23a0\r\n0x000070000fef22e0:   000070000fef2340 0000000115a48779\r\n0x000070000fef22f0:   0000000115a54223 0000000115a48742\r\n0x000070000fef2300:   000070000fef2300 00000001158f0338\r\n0x000070000fef2310:   000070000fef23a0 00000001158f9028\r\n0x000070000fef2320:   0000000000000000 00000001158f0340\r\n0x000070000fef2330:   0000000000000000 000070000fef2360\r\n0x000070000fef2340:   000070000fef23e8 0000000115a392bd\r\n0x000070000fef2350:   0000000000000000 0000000115a42538\r\n0x000070000fef2360:   00000000000068df 0000000000000000\r\n0x000070000fef2370:   000000076bc59df0 0000000000000029\r\n0x000070000fef2380:   0000000000000000 000000076bc607a8\r\n0x000070000fef2390:   00007fcf251efc00 000000076bc00888\r\n0x000070000fef23a0:   000000076bc00888 000070000fef23a8 \r\n\r\nInstructions: (pc=0x000000012f505ec5)\r\n0x000000012f505ea5:   98 ff 50 40 eb 1d 48 8b 75 a0 48 8b 06 48 8d 7d\r\n0x000000012f505eb5:   b0 48 8d 8d 70 ff ff ff 4c 8d 45 80 48 8b 55 98\r\n0x000000012f505ec5:   ff 50 48 4d 89 fc 8b 45 b0 89 45 c0 c7 45 b0 00\r\n0x000000012f505ed5:   00 00 00 8b 45 b4 89 45 c4 c7 45 b4 00 00 00 00 \r\n\r\nRegister to memory mapping:\r\n\r\nRAX=0x0000000000000000 is an unknown value\r\nRBX=0x00007fcf25ce2800 is an unknown value\r\nRCX=0x000070000fef21b0 is pointing into the stack for thread: 0x00007fcf252a0000\r\nRDX=0x000000012fcd6e30: _ZN7rocksdb14ThreadLocalPtr10StaticMeta4tls_E+0x658 in /private/var/folders/pn/c59lvsqj08z30l4b51ymk3r00000gn/T/librocksdbjni5520922190068836742.jnilib at 0x000000012f4b3000\r\nRSP=0x000070000fef21b0 is pointing into the stack for thread: 0x00007fcf252a0000\r\nRBP=0x000070000fef2240 is pointing into the stack for thread: 0x00007fcf252a0000\r\nRSI=0x00007fcf251efc00 is an unknown value\r\nRDI=0x000070000fef21f0 is pointing into the stack for thread: 0x00007fcf252a0000\r\nR8 =0x000070000fef21c0 is pointing into the stack for thread: 0x00007fcf252a0000\r\nR9 =0x00007fcf25ce2800 is an unknown value\r\nR10=0x00007fcf25800000 is an unknown value\r\nR11=0x00007fc7ba088a00 is an unknown value\r\nR12=0x0000000000000029 is an unknown value\r\nR13=0x00000000000068df is an unknown value\r\nR14=0x00007fcf252a01f8 is an unknown value\r\nR15=0x00007fcf26d416c0 is an unknown value\r\n\r\n\r\nStack: [0x000070000fdf3000,0x000070000fef3000],  sp=0x000070000fef21b0,  free space=1020k\r\nNative frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)\r\nC  [librocksdbjni5520922190068836742.jnilib+0x52ec5]  rocksdb_put_helper(JNIEnv_*, rocksdb::DB*, rocksdb::WriteOptions const&, rocksdb::ColumnFamilyHandle*, _jbyteArray*, int, int, _jbyteArray*, int, int)+0x145\r\nC  [librocksdbjni5520922190068836742.jnilib+0x53002]  Java_org_rocksdb_RocksDB_put__J_3BII_3BII+0x92\r\nj  org.rocksdb.RocksDB.put(J[BII[BII)V+0\r\nj  org.rocksdb.RocksDB.put([B[B)V+13\r\n```", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3135/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "huachaohuang": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3133", "title": "SeekForPrev with prefix_same_as_start does not work", "body": "``` c++\r\n#include <rocksdb/db.h>\r\n#include <rocksdb/slice_transform.h>\r\n\r\nusing namespace rocksdb;\r\n\r\nint main(int argc, char* argv[]) {\r\n  Options opts;\r\n  opts.create_if_missing = true;\r\n  opts.prefix_extractor.reset(NewFixedPrefixTransform(1));\r\n\r\n  DB* db = nullptr;\r\n  assert(DB::Open(opts, \"/tmp/example\", &db).ok());\r\n\r\n  WriteOptions wopts;\r\n  assert(db->Put(wopts, \"a\", \"\").ok());\r\n  // Adding one more key before \"c\" will pass the assertion.\r\n  // assert(db->Put(wopts, \"b\", \"\").ok());\r\n  assert(db->Put(wopts, \"d\", \"\").ok());\r\n\r\n  ReadOptions ropts;\r\n  ropts.prefix_same_as_start = true;\r\n  auto iter = db->NewIterator(ropts);\r\n  iter->SeekForPrev(\"c\");\r\n  // This assertion will fail.\r\n  assert(!iter->Valid());\r\n  delete iter;\r\n\r\n  delete db;\r\n}\r\n```\r\n\r\nSeems something is wrong with `DBIter::PrevInternal()` if the previous key of the seek key is the first key in the database, hope that helps.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3133/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2951", "title": "Speed up `DeleteFilesInRange()`", "body": "In some of our tests, thousands of `DeleteFilesInRange()` may take hundreds of seconds to complete. \r\nIs it reasonable to let `DeleteFilesInrange()` delete multiple ranges at a time to speed up continuous delete files operation?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2951/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2871", "title": "WriteBatch corruption when using manual_wal_flush", "body": "```\r\n2017/09/11-10:31:22.437701 7f75c336ae40 [db/db_impl_open.cc:506] Recovering log #757 mode 2\r\n2017/09/11-10:31:22.463380 7f75c336ae40 [WARN] [db/db_impl_open.cc:448] /data1/kv2/data/db/000757.log: dropping 422773 bytes; Corruption: unknown WriteBatch tag\r\n2017/09/11-10:31:22.463405 7f75c336ae40 [db/db_impl_open.cc:726] Point in time recovered to log #757 seq #24603006\r\n```\r\n\r\nHere is the LOG file [LOG.old.1505097155505139.txt](https://github.com/facebook/rocksdb/files/1294603/LOG.old.1505097155505139.txt).\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2871/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lght": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3129", "title": "[BUG] RocksDB panics on rocksdb::InternalKeyComparator::Compare", "body": "This bug is a cross-post from [rust-rocksdb issue #148](https://github.com/spacejam/rust-rocksdb/issues/148)\r\n\r\nI was working on a project that relies on rust-rocksdb, and got a core dump while the program was compacting the internal rocksdb.\r\n\r\nBelow is a core dump, and some basic analysis of the bug. I'm still working on figuring this bug out, but the bug seems to be coming from this C++ library.\r\n\r\nLine #6 in the core dump is the relevant call before the crash.\r\n\r\nCore Dump:\r\n```\r\n#0  0x00007f189355d8a0 in raise () from /usr/lib/libc.so.6\r\n#1  0x00007f189355ef09 in abort () from /usr/lib/libc.so.6\r\n#2  0x00007f1894ab7d77 in __gnu_cxx::__verbose_terminate_handler () at /build/gcc/src/gcc/libstdc++-v3/libsupc++/vterminate.cc:95\r\n#3  0x00007f1894ab58e6 in __cxxabiv1::__terminate (handler=<optimized out>) at /build/gcc/src/gcc/libstdc++-v3/libsupc++/eh_terminate.cc:47\r\n#4  0x00007f1894ab5933 in std::terminate () at /build/gcc/src/gcc/libstdc++-v3/libsupc++/eh_terminate.cc:57\r\n#5  0x00007f1894ab6741 in __cxxabiv1::__cxa_pure_virtual () at /build/gcc/src/gcc/libstdc++-v3/libsupc++/pure.cc:50\r\n#6  0x000055c0a7d7dfe8 in rocksdb::InternalKeyComparator::Compare (this=0x7f188a441840, akey=..., bkey=...) at rocksdb/db/dbformat.cc:73\r\n#7  0x000055c0a7cab41a in rocksdb::MinIteratorComparator::operator() (this=0x7f1885216868, a=0x7f1885216820, b=0x7f18852167e0) at rocksdb/table/iter_heap.h:36\r\n#8  0x000055c0a7caded1 in rocksdb::BinaryHeap<rocksdb::IteratorWrapper*, rocksdb::MinIteratorComparator>::downheap (this=0x7f1885216868, index=0) at rocksdb/util/heap.h:123\r\n#9  0x000055c0a7cad240 in rocksdb::BinaryHeap<rocksdb::IteratorWrapper*, rocksdb::MinIteratorComparator>::replace_top (this=0x7f1885216868,\r\n    value=@0x7f1885216858: 0x7f18852167c0) at rocksdb/util/heap.h:63\r\n#10 0x000055c0a7cac20a in rocksdb::MergingIterator::Next (this=0x7f1885216780) at rocksdb/table/merger.cc:164\r\n#11 0x000055c0a7d55114 in rocksdb::CompactionIterator::Next (this=0x7f1885852000) at rocksdb/db/compaction_iterator.cc:103\r\n#12 0x000055c0a7d5be33 in rocksdb::CompactionJob::ProcessKeyValueCompaction (this=0x7f18873fc690, sub_compact=0x7f1885239700) at rocksdb/db/compaction_job.cc:845\r\n#13 0x000055c0a7d5a0fe in rocksdb::CompactionJob::Run (this=0x7f18873fc690) at rocksdb/db/compaction_job.cc:530\r\n#14 0x000055c0a7b7bfff in rocksdb::DBImpl::BackgroundCompaction (this=0x7f188a4cf000, made_progress=0x7f18873fcd3f, job_context=0x7f18873fcda0, log_buffer=0x7f18873fcf90,\r\n    arg=0x0) at rocksdb/db/db_impl.cc:3502\r\n#15 0x000055c0a7b7a6c1 in rocksdb::DBImpl::BackgroundCallCompaction (this=0x7f188a4cf000, arg=0x0) at rocksdb/db/db_impl.cc:3196\r\n#16 0x000055c0a7b79957 in rocksdb::DBImpl::BGWorkCompaction (arg=0x7f1885414090) at rocksdb/db/db_impl.cc:3002\r\n#17 0x000055c0a7da553d in rocksdb::ThreadPool::BGThread (this=0x7f1892d28dc0, thread_id=1) at rocksdb/util/threadpool.cc:230\r\n#18 0x000055c0a7da55f2 in rocksdb::BGThreadWrapper (arg=0x7f18922cf360) at rocksdb/util/threadpool.cc:254\r\n#19 0x00007f1893afe08a in start_thread () from /usr/lib/libpthread.so.0\r\n#20 0x00007f189361f24f in clone () from /usr/lib/libc.so.6\r\n```\r\n\r\nKey line from the core dump excerpt above: \r\n- #6 0x000055c0a7d7dfe8 in rocksdb::InternalKeyComparator::Compare (this=0x7f188a441840, akey=..., bkey=...) at rocksdb/db/dbformat.cc:73\r\n\r\nThe line numbers changed in the github repo, but here's the relevant function definition for rocksdb::InternalKeyComparator::Compare from the C++ library:\r\n\r\n```\r\nint InternalKeyComparator::Compare(const Slice& akey, const Slice& bkey) const {\r\n  // Order by:\r\n  //    increasing user key (according to user-supplied comparator)\r\n  //    decreasing sequence number\r\n  //    decreasing type (though sequence# should be enough to disambiguate)\r\n  int r = user_comparator_->Compare(ExtractUserKey(akey), ExtractUserKey(bkey));\r\n  PERF_COUNTER_ADD(user_key_comparison_count, 1);\r\n  if (r == 0) {\r\n    const uint64_t anum = DecodeFixed64(akey.data() + akey.size() - 8);\r\n    const uint64_t bnum = DecodeFixed64(bkey.data() + bkey.size() - 8);\r\n    if (anum > bnum) {\r\n      r = -1;\r\n    } else if (anum < bnum) {\r\n      r = +1;\r\n    }\r\n  }\r\n  return r;\r\n}\r\n```\r\n\r\nand the relevant github link [facebook/rocksdb/db/dbformat.cc:109](https://github.com/facebook/rocksdb/blob/master/db/dbformat.cc#L109) (w/ adjusted line #).\r\n\r\nPERF_COUNTER_ADD looks like may be the culprit, defined here: \r\n```\r\nmonitoring/perf_context_imp.h\r\n\r\n30: #define PERF_COUNTER_ADD(metric, value)\r\n...\r\n...\r\n55: #define PERF_COUNTER_ADD(metric, value)        \\\r\n56:   if (perf_level >= PerfLevel::kEnableCount) { \\\r\n57:     perf_context.metric += value;              \\\r\n58:    }\r\n59:\r\n60: #endif\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3129/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jerousrb": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3112", "title": "may be a code bug", "body": "in the file db/column_family.cc ,the destructor of ColumnFamilySet,  deleting cfd directly after dereferencing by call  cfd->Unref();\r\n\r\nI think we can delete cfd  only when the  cfd->Unref()== true, others we no need to delete!\r\n\r\nColumnFamilySet::~ColumnFamilySet() {\r\n  while (column_family_data_.size() > 0) {\r\n    // cfd destructor will delete itself from column_family_data_\r\n    auto cfd = column_family_data_.begin()->second;\r\n    cfd->Unref();\r\n    delete cfd; // this should  do it when cfd->Unref()==true ?\r\n  }\r\n  dummy_cfd_->Unref();\r\n  delete dummy_cfd_;\r\n}\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3112/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jianqingdu": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3109", "title": "How to improve the performance of multiple Seek and Delete pattern", "body": "### Expected behavior\r\n10000 operation of Put should cost almost the same time with Seek and Delete\r\n\r\n### Actual behavior\r\nthe Seek and Delete are hundreds time slower than the Put .\r\nIf comment the Delete operation, Put and Seek almost cost the same time.\r\nSo I want to know if this is an issue, or how to improve the performance if I want to use\r\nthis Seek and Delete pattern multiple times\r\n\r\n### Steps to reproduce the behavior\r\n\r\n```\r\n#include <string>\r\n#include <sys/time.h>\r\n#include <stdint.h>\r\n#include <assert.h>\r\n#include \"rocksdb/db.h\"\r\nusing namespace std;\r\n\r\nuint64_t get_tick_count()\r\n{\r\n    struct timeval tval;\r\n    gettimeofday(&tval, NULL);\r\n    uint64_t current_tick = tval.tv_sec * 1000L + tval.tv_usec / 1000L;\r\n    return current_tick;\r\n}\r\n\r\nint main(int argc, char* argv[])\r\n{\r\n    int test_num = 10000;\r\n    if (argc == 2) {\r\n        test_num = atoi(argv[1]);\r\n    }\r\n    \r\n    rocksdb::DB* db;\r\n    rocksdb::Options options;\r\n    rocksdb::ReadOptions read_options;\r\n    rocksdb::WriteOptions write_options;\r\n    options.create_if_missing = true;\r\n    rocksdb::Status status = rocksdb::DB::Open(options, \"./dbfile\", &db);\r\n    assert(status.ok());\r\n    \r\n    uint64_t start_time = get_tick_count();\r\n    for (int i = 0; i < test_num; i++) {\r\n        string key = \"prefix_\" + to_string(i);\r\n        string value = \"value\";\r\n        \r\n        db->Put(write_options, key, value);\r\n    }\r\n    printf(\"put cost %llu ms\\n\", get_tick_count() - start_time);\r\n    \r\n    start_time = get_tick_count();\r\n    for (int i = 0; i < test_num; i++) {\r\n        rocksdb::Iterator* it = db->NewIterator(read_options);\r\n        it->Seek(\"prefix_\");\r\n        if (it->Valid()) {\r\n            db->Delete(write_options, it->key());\r\n        }\r\n        delete it;\r\n    }\r\n    printf(\"seek and delete cost %llu ms\\n\", get_tick_count() - start_time);\r\n    \r\n    delete db;\r\n    return 0;\r\n}\r\n```", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3109/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yiwu-arbug": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3096", "title": "Concurrent writes may get Status::OK while it actually gets IOError on WAL write", "body": "Concurrent writes may get Status::OK while it actually gets IOError on WAL write. This happens when multiple writes form a write batch group, and the leader get an IOError while writing to WAL. The leader failed to pass the error to followers in the group, and the followers end up return Status::OK() while actually writing nothing. The bug only affect writes in a batch group. Future writes after the batch group will correctly return immediately with the IOError.\r\n\r\nAffects RocksDB >= 4.6.1 and <= 5.8.x since this commit: 6f71d3b68b011569dfaf5c406a15319e4d66ca76\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3096/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/f2f034ef3be52ea48b3b94d50a2d1fee2bf0e2f9", "message": "Blob DB: fix crash when DB full but no candidate file to evict\n\nSummary:\nWhen blob_files is empty, std::min_element will return blobfiles.end(), which cannot be dereference. Fixing it.\nCloses https://github.com/facebook/rocksdb/pull/3387\n\nDifferential Revision: D6764927\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 86f78700132be95760d35ac63480dfd3a8bbe17a"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/5568aec421eff60944f1e7a88677c97c52e4ef19", "message": "Fix DBTest::SoftLimit TSAN failure\n\nSummary:\nFix data race found by TSAN around WriteStallListener: https://gist.github.com/yiwu-arbug/027d2448b903648f2f0f40b05258d80f\nCloses https://github.com/facebook/rocksdb/pull/3384\n\nDifferential Revision: D6762167\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: cd3a5c9f806de390bd1af6077ea6dbbc8bcaec09"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/f1cb83fcf408b2870e3c4372fb13e58255a2cadf", "message": "Fix Flush() keep waiting after flush finish\n\nSummary:\nFlush() call could be waiting indefinitely if min_write_buffer_number_to_merge is used. Consider the sequence:\n1. User call Flush() with flush_options.wait = true\n2. The manual flush started in the background\n3. New memtable become immutable because of writes. The new memtable will not trigger flush if min_write_buffer_number_to_merge is not reached.\n4. The manual flush finish.\n\nBecause of the new memtable created at step 3 not being flush, previous logic of WaitForFlushMemTable() keep waiting, despite the memtables it intent to flush has been flushed.\n\nHere instead of checking if there are any more memtables to flush, WaitForFlushMemTable() also check the id of the earliest memtable. If the id is larger than that of latest memtable at the time flush was initiated, it means all the memtable at the time of flush start has all been flush.\nCloses https://github.com/facebook/rocksdb/pull/3378\n\nDifferential Revision: D6746789\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 35e698f71c7f90b06337a93e6825f4ea3b619bfa"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/dc360df81ec48e56a5d9cee4adb7f11ef0ca82ac", "message": "Fix multiple build failures\n\nSummary:\n* Fix DBTest.CompactRangeWithEmptyBottomLevel lite build failure\n* Fix DBTest.AutomaticConflictsWithManualCompaction failure introduce by #3366\n* Fix BlockBasedTableTest::IndexUncompressed should be disabled if snappy is disabled\n* Fix ASAN failure with DBBasicTest::DBClose test\nCloses https://github.com/facebook/rocksdb/pull/3373\n\nDifferential Revision: D6732313\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 1eb9b9d9a8d795f56188fa9770db9353f6fdedc5"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/46ec52499eb0bab57f9b649b11ca9161f93ba06b", "message": "Fix db_bench write being disabled in lite build\n\nSummary:\nThe macro was added by mistake in #2372\nCloses https://github.com/facebook/rocksdb/pull/3343\n\nDifferential Revision: D6681356\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 4180172fb0eaef4189c07f219241e0c261c03461"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/30a017fecae60aa7b87c4a1e283b6ac027724a92", "message": "Blob DB: avoid having a separate read of checksum\n\nSummary:\nPreviously on a blob db read, we are making a read of the blob value, and then make another read to get CRC checksum. I'm combining the two read into one.\n\nreadrandom db_bench with 1G database with base db size of 13M, value size 1k:\n`./db_bench --db=/home/yiwu/tmp/db_bench --use_blob_db --value_size=1024 --num=1000000 --benchmarks=readrandom --use_existing_db --cache_size=32000000`\nmaster: throughput 234MB/s, get micros p50 5.984 p95 9.998 p99 20.817 p100 787\nthis PR: throughput 261MB/s, get micros p50 5.157 p95 9.928 p99 20.724 p100 190\nCloses https://github.com/facebook/rocksdb/pull/3301\n\nDifferential Revision: D6615950\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 052410c6d8539ec0cc305d53793bbc8f3616baa3"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/e763e1b6239ae498e730ed1acd3a5f75e2c4116c", "message": "BlobDB: dump blob db options on open\n\nSummary:\nWe dump blob db options on blob db open, but it was removed by mistake in #3246. Adding it back.\nCloses https://github.com/facebook/rocksdb/pull/3298\n\nDifferential Revision: D6607177\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 2a4aacbfa52fd8f1878dc9e1fbb95fe48faf80c0"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/48cf8da2bbda3c75e50e35b893e40ac757ee700d", "message": "BlobDB: update blob_db_options.bytes_per_sync behavior\n\nSummary:\nPreviously, if blob_db_options.bytes_per_sync, there is a background job to call fsync() for every bytes_per_sync bytes written to a blob file. With the change we simply pass bytes_per_sync as env_options_ to blob files so that sync_file_range() will be used instead.\nCloses https://github.com/facebook/rocksdb/pull/3297\n\nDifferential Revision: D6606994\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 452424be52e32ba92f5ea603b564e9b88929af47"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/06149429d91c7e766675c43ae41da695bbd2ced9", "message": "WritePrepared Txn: Return NotSupported on iterator refresh\n\nSummary:\nA proper implementation of Iterator::Refresh() for WritePreparedTxnDB would require release and acquire another snapshot. Since MyRocks don't make use of Iterator::Refresh(), we just simply mark it as not supported.\nCloses https://github.com/facebook/rocksdb/pull/3290\n\nDifferential Revision: D6599931\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 4e1632d967316431424f6e458254ecf9a97567cf"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/2190e967272ad825b45959a62d3f54911b11b798", "message": "Remove incorrect comment\n\nSummary:\nWe actually create individual compaction filter from compaction filter factory per sub-compaction in `CompactionJob::ProcessKeyValueCompaction`: https://github.com/facebook/rocksdb/blob/master/db/compaction_job.cc#L742\nThe comment seems incorrect.\nCloses https://github.com/facebook/rocksdb/pull/3288\n\nDifferential Revision: D6598455\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: a6bc059a9103b87a73ae6ec4bb01ca33f5d48cf5"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/237b2925157fee07569b01a4eae8164afdf29bce", "message": "BlobDB: Remove the need to get sequence number per write\n\nSummary:\nPreviously we store sequence number range of each blob files, and use the sequence number range to check if the file can be possibly visible by a snapshot. But it adds complexity to the code, since the sequence number is only available after a write. (The current implementation get sequence number by calling GetLatestSequenceNumber(), which is wrong.) With the patch, we are not storing sequence number range, and check if snapshot_sequence < obsolete_sequence to decide if the file is visible by a snapshot (previously we check if first_sequence <= snapshot_sequence < obsolete_sequence).\nCloses https://github.com/facebook/rocksdb/pull/3274\n\nDifferential Revision: D6571497\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: ca06479dc1fcd8782f6525b62b7762cd47d61909"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/7393ef779c2d430d93fabb8c82e0c375f24bc46e", "message": "Fix BlockFetcher ASAN error\n\nSummary:\nSome call sites of BlockFetcher create temporary ReadOptions and pass to BlockFetcher. The temporary object will be gone after BlockFetcher construction but BlockFetcher keep its reference, causing stack-use-after-scope. Fixing it.\nCloses https://github.com/facebook/rocksdb/pull/3258\n\nDifferential Revision: D6547152\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 6b49e9dd46bb72307f5d8f88ea15faacff35b9bc"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/e3a06f12d27fd50af7b6c5941973f529601f9a3e", "message": "WritePrepared Txn: fix compaction filter snapshot checks\n\nSummary:\nAdd snapshot_checker check whenever we need to check sequence against snapshots and decide what to do with an input key. The changes are related to one of:\n* compaction filter\n* single delete\n* delete at bottom level\n* merge\nCloses https://github.com/facebook/rocksdb/pull/3251\n\nDifferential Revision: D6537850\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 3faba40ed5e37779f4a0cb7ae78af9546659c7f2"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/9a27ac5d893d588354963f6c4b37fa138343f0b8", "message": "Fix drop column family data race\n\nSummary:\nA data race is caught by tsan_crash test between compaction and DropColumnFamily:\nhttps://gist.github.com/yiwu-arbug/5a2b4baae05eeb99ae1719b650f30a44 Compaction checks if the column family has been dropped on each key input, while user can issue DropColumnFamily which updates cfd->dropped_, causing the data race. Fixing it by making cfd->dropped_ an atomic.\nCloses https://github.com/facebook/rocksdb/pull/3250\n\nDifferential Revision: D6535991\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 5571df020beae7fa7db6fff5ad0d598f49962895"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/250a51a3f9a1d99b2d0ce4d0053083366bbcc646", "message": "BlobDB: refactor DB open logic\n\nSummary:\nRefactor BlobDB open logic. List of changes:\n\nMajor:\n* On reopen, mark blob files found as immutable, do not use them for writing new keys.\n* Not to scan the whole file to find file footer. Instead just seek to the end of the file and try to read footer.\n\nMinor:\n* Move most of the real logic from blob_db.cc to blob_db_impl.cc.\n* Not to hold shared_ptr of event listeners in global maps in blob_db.cc\n* Some changes to BlobFile interface.\n* Improve logging and error handling.\nCloses https://github.com/facebook/rocksdb/pull/3246\n\nDifferential Revision: D6526147\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 9dc4cdd63359a2f9b696af817086949da8d06952"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/e1c569c324e48fb48a4091ba2209330f05e13f20", "message": "Fix clang-analyzer false-positive on ldb_cmd.cc\n\nSummary:\nclang-analyzer complaint about db_ being nullptr, but it couldn't be because it checks exec_stats before proceed. Add an assert to get around the false-positive.\n\nTest Plan\n`make analyze`\nCloses https://github.com/facebook/rocksdb/pull/3236\n\nDifferential Revision: D6505417\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: e5b65764ea994dd9e4bab3e697b97dc70dc22cab"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/a7d32776f08f4e04d8cb9837491d0da359459343", "message": "Fix write_callback_test compile error\n\nSummary:\nRename shadow variable name db_impl.\n\nFixing #3227\nCloses https://github.com/facebook/rocksdb/pull/3235\n\nDifferential Revision: D6504051\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 186c9378dabb11f8d6db56f45c95cc3b029fcb88"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/7f04af32a5cff78a9aa773c010b344616a4c0b5a", "message": "ldb to allow db with --try_load_options and without an options file\n\nSummary:\nThis is to fix tools/check_format_compatible.sh. The tool try to open\nold versions of rocksdb with the provided options file. When options\nfile is missing (e.g. rocksdb 2.2), it should still proceed with default\noptions.\nCloses https://github.com/facebook/rocksdb/pull/3232\n\nDifferential Revision: D6503955\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: e44cfcce7ddc7d12cf83466ed3f3fe7624aa78b8"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/b5798bd3249253b5ec567f30eeaf305e5dfc4581", "message": "Add missing recent versions to format compatible test\n\nSummary:\nAdd recent versions for format compatible test. We should probably update the script to auto include available versions (by looking at include/rocksdb/versions.h and deduce branch names), but we can do it later.\nCloses https://github.com/facebook/rocksdb/pull/3233\n\nDifferential Revision: D6503631\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: e2b01d1ef6e784ff6ffa1bd75d741755e3c69a8c"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/20995c572995c31524b39c67b26aa6203fb0a80c", "message": "Make iterator invalid on Merge error\n\nSummary:\nSince #1665, on merge error, iterator will be set to corrupted status, but it doesn't invalidate the iterator. Fixing it.\nCloses https://github.com/facebook/rocksdb/pull/3226\n\nDifferential Revision: D6499094\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 80222930f949e31f90a6feaa37ddc3529b510d2c"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/54095d3389170c00f50ec8176a6c7543d414a5e1", "message": "TARGETS file not include tests in opt mode\n\nSummary:\nDo not build the tests in opt mode, since SyncPoint and other test code will not be included.\nCloses https://github.com/facebook/rocksdb/pull/3204\n\nDifferential Revision: D6431154\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: c404ef042c1a6f679e5c1dc57600b3d8cb52fc28"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/78279350aae53b601eb33935bab6de653a17f26d", "message": "Blob DB: Add statistics\n\nSummary:\nAdding a list of blob db counters.\n\nAlso remove WaStats() which doesn't expose the stats and can be substitute by (BLOB_DB_BYTES_WRITTEN / BLOB_DB_BLOB_FILE_BYTES_WRITTEN).\nCloses https://github.com/facebook/rocksdb/pull/3193\n\nDifferential Revision: D6394216\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 017508c8ff3fcd7ea7403c64d0f9834b24816803"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/3cf562be31a5876a00caff6eb638049bcad5ee9b", "message": "Fix IOError on WAL write doesn't propagate to write group follower\n\nSummary:\nThis is a simpler version of #3097 by removing all unrelated changes.\n\nFixing the bug where concurrent writes may get Status::OK while it actually gets IOError on WAL write. This happens when multiple writes form a write batch group, and the leader get an IOError while writing to WAL. The leader failed to pass the error to followers in the group, and the followers end up returning Status::OK() while actually writing nothing. The bug only affect writes in a batch group. Future writes after the batch group will correctly return immediately with the IOError.\nCloses https://github.com/facebook/rocksdb/pull/3201\n\nDifferential Revision: D6421644\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 1c2a455c5b73f6842423785eb8a9dbfbb191dc0e"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/f0dde49cda71f61ccf36f3fc19428cc9bf930e13", "message": "Blob DB: Fix GC handling for inlined blob\n\nSummary:\nGarbage collection checks if the offset in blob index matches the offset of the blob value in the file. If it is a mismatch, the value is the current version. However it failed to check if the blob index is an inlined type, which don't even have an offset. Fixing it.\nCloses https://github.com/facebook/rocksdb/pull/3194\n\nDifferential Revision: D6394270\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 7c2b9d795f1116f55f4d728086980f9b6e88ea78"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/dd49f89466521ae86b45ebcd97f7e2d7a84c03ca", "message": "Fix TARGETS lint warnings.\n\nSummary:\nFix buckifier script and regenerate TARGETS file with no lint warnings.\nCloses https://github.com/facebook/rocksdb/pull/3170\n\nDifferential Revision: D6328993\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 17d0e4ed92f676f35fed76659386611cc72b00b2"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/bbcd3b0bd2c6dbc10e1bbf73f994179031c76c27", "message": "Suppress valgrind \"unimplemented functionality\" error\n\nSummary:\nAdd ROCKSDB_VALGRIND_RUN macro and suppress false-positive \"unimplemented functionality\" throw by valgrind for steam hints.\n\nAnother approach would be add a valgrind suppress file. Valgrind is suppose to print the suppression when given \"--gen-suppressions=all\" param, which is suppose to be the content for the suppression file. But it doesn't print.\nCloses https://github.com/facebook/rocksdb/pull/3174\n\nDifferential Revision: D6338786\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 3559efa5f3b92d40d09ad6ac82bc7b59f86c75aa"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/9871ea43577bebd6dd2a748f1fc2568b67df40fa", "message": "Regression test build binaries with PORTABLE=1\n\nSummary:\nWe hit \"Illegal instruction\" error in regression test with \"shlx\" instruction. Setting PORTABLE=1 to resolve it.\nCloses https://github.com/facebook/rocksdb/pull/3165\n\nDifferential Revision: D6321972\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: cc9fe0dbd4698d1b66a750a0b062f66899862719"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/42564ada5374787b0087206194f8880950958395", "message": "Blob DB: not using PinnableSlice move assignment\n\nSummary:\nThe current implementation of PinnableSlice move assignment have an issue #3163. We are moving away from it instead of try to get the move assignment right, since it is too tricky.\nCloses https://github.com/facebook/rocksdb/pull/3164\n\nDifferential Revision: D6319201\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 8f3279021f3710da4a4caa14fd238ed2df902c48"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/5e9e5a4702d121e65c547b2e0ed1321e09462494", "message": "Blob DB: Fix race condition between flush and write\n\nSummary:\nA race condition will happen when:\n* a user thread writes a value, but it hits the write stop condition because there are too many un-flushed memtables, while holding blob_db_impl.write_mutex_.\n* Flush is triggered and call flush begin listener and try to acquire blob_db_impl.write_mutex_.\n\nFixing it.\nCloses https://github.com/facebook/rocksdb/pull/3149\n\nDifferential Revision: D6279805\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 0e3c58afb78795ebe3360a2c69e05651e3908c40"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/ca75f0a64a0d2aa0fb8517d6827b5e1d639e64f3", "message": "Blob DB: Fix release build\n\nSummary:\n`compression` shadow the method name in `BlobFile`. Rename it.\nCloses https://github.com/facebook/rocksdb/pull/3148\n\nDifferential Revision: D6274498\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 7d293596530998b23b6b8a8940f983f9b6343a98"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/4f9f1243475b729f7da63388aaa002aedc1d9559", "message": "Blob DB: use compression in file header instead of global options\n\nSummary:\nTo fix the issue of failing to decompress existing value after reopen DB with a different compression settings.\nCloses https://github.com/facebook/rocksdb/pull/3142\n\nDifferential Revision: D6267260\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: c7cf7f3e33b0cd25520abf4771cdf9180cc02a5f"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/be410dede8c02de1d430634bd719d3d8710aa5da", "message": "Fix PinnableSlice move assignment\n\nSummary:\nAfter move assignment, we need to re-initialized the moved PinnableSlice.\n\nAlso update blob_db_impl.cc to not reuse the moved PinnableSlice since it is supposed to be in an undefined state after move.\nCloses https://github.com/facebook/rocksdb/pull/3127\n\nDifferential Revision: D6238585\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: bd99f2e37406c4f7de160c7dee6a2e8126bc224e"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/d95616956355921d3535d6e4c59f27264a5f8b30", "message": "Fix clang build error\n\nSummary:\nFix cast from size_t to unsigned int.\nCloses https://github.com/facebook/rocksdb/pull/3125\n\nDifferential Revision: D6232863\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 4c6131168b1faec26f7820b2cf4a09c242d323b7"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/2581c0a5a18dda0fe15055db6f8bace4546c8483", "message": "Blob DB: Fix BlobDBTest::SnapshotAndGarbageCollection asan failure\n\nSummary:\nFix unreleased snapshot at the end of the test.\nCloses https://github.com/facebook/rocksdb/pull/3126\n\nDifferential Revision: D6232867\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 651ca3144fc573ea2ab0ab20f0a752fb4a101d26"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/62578d80c143c1e126a261479c721092b000d56e", "message": "Blob DB: Add compaction filter to remove expired blob index entries\n\nSummary:\nAfter adding expiration to blob index in #3066, we are now able to add a compaction filter to cleanup expired blob index entries.\nCloses https://github.com/facebook/rocksdb/pull/3090\n\nDifferential Revision: D6183812\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 9cb03267a9702975290e758c9c176a2c03530b83"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/7bfa88037e71137764c8a72464be5cfca903bb1b", "message": "Blob DB: fix snapshot handling\n\nSummary:\nBlob db will keep blob file if data in the file is visible to an active snapshot. Before this patch it checks whether there is an active snapshot has sequence number greater than the earliest sequence in the file. This is problematic since we take snapshot on every read, if it keep having reads, old blob files will not be cleanup. Change to check if there is an active snapshot falls in the range of [earliest_sequence, obsolete_sequence) where obsolete sequence is\n1. if data is relocated to another file by garbage collection, it is the latest sequence at the time garbage collection finish\n2. otherwise, it is the latest sequence of the file\nCloses https://github.com/facebook/rocksdb/pull/3087\n\nDifferential Revision: D6182519\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: cdf4c35281f782eb2a9ad6a87b6727bbdff27a45"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/f662f8f0b6f3fe2983478f6f95c4128d602deac1", "message": "Blob DB: option to enable garbage collection\n\nSummary:\nAdd an option to enable/disable auto garbage collection, where we keep counting how many keys have been evicted by either deletion or compaction and decide whether to garbage collect a blob file.\n\nDefault disable auto garbage collection for now since the whole logic is not fully tested and we plan to make major change to it.\nCloses https://github.com/facebook/rocksdb/pull/3117\n\nDifferential Revision: D6224756\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: cdf53bdccec96a4580a2b3a342110ad9e8864dfe"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/167ba599ec3c7e1cb678453bff05cf028ab4757e", "message": "Blob DB: Fix flaky BlobDBTest::GCExpiredKeyWhileOverwriting test\n\nSummary:\nThe test intent to wait until key being overwritten until proceed with garbage collection. It failed to wait for `PutUntil` finally finish. Fixing it.\nCloses https://github.com/facebook/rocksdb/pull/3116\n\nDifferential Revision: D6222833\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: fa9b57a772b92a66cf250b44e7975c43f62f45c5"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/f6082d194439c0fc3992ab343d63e2389f867ee5", "message": "Blob DB: cleanup unused options\n\nSummary:\n* cleanup num_concurrent_simple_blobs. We don't do concurrent writes (by taking write_mutex_) so it doesn't make sense to have multiple non TTL files open. We can revisit later when we want to improve writes.\n* cleanup eviction callback. we don't have plan to use it now.\n* rename s/open_simple_blob_files_/open_non_ttl_file_/ and s/open_blob_files_/open_ttl_files_/ to avoid confusion.\nCloses https://github.com/facebook/rocksdb/pull/3088\n\nDifferential Revision: D6182598\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 99e6f5e01fa66d31309cdb06ce48502464bac6ad"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/792ef10ca85e7525869838065cfe4d3a1558ee7f", "message": "Return Status::InvalidArgument if user request sync write while disabling WAL\n\nSummary:\nwrite_options.sync = true and write_options.disableWAL is incompatible. When WAL is disabled, we are not able to persist the write immediately. Return an error in this case to avoid misuse of the options.\nCloses https://github.com/facebook/rocksdb/pull/3086\n\nDifferential Revision: D6176822\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 1eb10028c14fe7d7c13c8bc12c0ef659f75aa071"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/84a04af9a95ffbfcd120296d3507b6da8bafcda5", "message": "TableProperty::oldest_key_time defaults to 0\n\nSummary:\nWe don't propagate TableProperty::oldest_key_time on compaction and just write the default value to SST files. It is more natural to default the value to 0.\n\nAlso revert db_sst_test back to before #2842.\nCloses https://github.com/facebook/rocksdb/pull/3079\n\nDifferential Revision: D6165702\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: ca3ce5928d96ae79a5beb12bb7d8c640a71478a0"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/3ebb7ba7b989df8d000d4f4caebe7b607ace4a93", "message": "Blob DB: update blob file format\n\nSummary:\nChanging blob file format and some code cleanup around the change. The change with blob log format are:\n* Remove timestamp field in blob file header, blob file footer and blob records. The field is not being use and often confuse with expiration field.\n* Blob file header now come with column family id, which always equal to default column family id. It leaves room for future support of column family.\n* Compression field in blob file header now is a standalone byte (instead of compact encode with flags field)\n* Blob file footer now come with its own crc.\n* Key length now being uint64_t instead of uint32_t\n* Blob CRC now checksum both key and value (instead of value only).\n* Some reordering of the fields.\n\nThe list of cleanups:\n* Better inline comments in blob_log_format.h\n* rename ttlrange_t and snrange_t to ExpirationRange and SequenceRange respectively.\n* simplify blob_db::Reader\n* Move crc checking logic to inside blob_log_format.cc\nCloses https://github.com/facebook/rocksdb/pull/3081\n\nDifferential Revision: D6171304\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: e4373e0d39264441b7e2fbd0caba93ddd99ea2af"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/5a2a6483dc23a8854e02984ece3b8d88b783959d", "message": "Blob DB: Inline small values in base DB\n\nSummary:\nAdding the `min_blob_size` option to allow storing small values in base db (in LSM tree) together with the key. The goal is to improve performance for small values, while taking advantage of blob db's low write amplification for large values.\n\nAlso adding expiration timestamp to blob index. It will be useful to evict stale blob indexes in base db by adding a compaction filter. I'll work on the compaction filter in future patches.\n\nSee blob_index.h for the new blob index format. There are 4 cases when writing a new key:\n* small value w/o TTL: put in base db as normal value (i.e. ValueType::kTypeValue)\n* small value w/ TTL: put (type, expiration, value) to base db.\n* large value w/o TTL: write value to blob log and put (type, file, offset, size, compression) to base db.\n* large value w/TTL: write value to blob log and put (type, expiration, file, offset, size, compression) to base db.\nCloses https://github.com/facebook/rocksdb/pull/3066\n\nDifferential Revision: D6142115\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 9526e76e19f0839310a3f5f2a43772a4ad182cd0"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/66a2c44ef4b6ecaacc1d9d505cde56b8f5da565e", "message": "Add DB::Properties::kEstimateOldestKeyTime\n\nSummary:\nWith FIFO compaction we would like to get the oldest data time for monitoring. The problem is we don't have timestamp for each key in the DB. As an approximation, we expose the earliest of sst file \"creation_time\" property.\n\nMy plan is to override the property with a more accurate value with blob db, where we actually have timestamp.\nCloses https://github.com/facebook/rocksdb/pull/2842\n\nDifferential Revision: D5770600\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 03833c8f10bbfbee62f8ea5c0d03c0cafb5d853a"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/eaaef9117820c2cc9bda1ccc8c53cc499f831055", "message": "Blob DB: Store blob index as kTypeBlobIndex in base db\n\nSummary:\nBlob db insert blob index to base db as kTypeBlobIndex type, to tell apart values written by plain rocksdb or blob db. This is to make it possible to migrate from existing rocksdb to blob db.\n\nAlso with the patch blob db garbage collection get away from OptimisticTransaction. Instead it use a custom write callback to achieve similar behavior as OptimisticTransaction. This is because we need to pass the is_blob_index flag to DBImpl::Get but OptimisticTransaction don't support it.\nCloses https://github.com/facebook/rocksdb/pull/3000\n\nDifferential Revision: D6050044\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 61dc72ab9977625e75f78cd968e7d8a3976e3632"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/0552029b5c710c332368ee05c112847f7d47c6b7", "message": "Blob DB: not writing sequence number as blob record footer\n\nSummary:\nPreviously each time we write a blob we write blog_record_header + key + value + blob_record_footer to blob log. The footer only contains a sequence and a crc for the sequence number. The sequence number was used in garbage collection to verify the value is recent. After #2703 we moved to use optimistic transaction and no longer use sequence number from the footer. Remove the footer altogether.\n\nThere's another usage of sequence number and we are keeping it: Each blob log file keep track of sequence number range of keys in it, and use it to check if it is reference by a snapshot, before being deleted.\nCloses https://github.com/facebook/rocksdb/pull/3005\n\nDifferential Revision: D6057585\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: d6da53c457a316e9723f359a1b47facfc3ffe090"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/8e63cad078503941530b1c8278dbef32d0027255", "message": "fix lite build\n\nSummary:\n* make `checksum_type_string_map` available for lite\n* comment out `FilesPerLevel` in lite mode.\n* travis and legocastle lite build also build `all` target and run tests\nCloses https://github.com/facebook/rocksdb/pull/3015\n\nDifferential Revision: D6069822\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 9fe92ac220e711e9e6ed4e921bd25ef4314796a0"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/60c09f5fbb43ede016cea16bfb8002fb56f774a8", "message": "print more table_options to info log\n\nSummary:\nprint more table_options to info log\nCloses https://github.com/facebook/rocksdb/pull/3003\n\nDifferential Revision: D6054490\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 8e6f96e08bdc906077b6c62ade419d7cb739110f"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/10ba50e9eb9a38ba3e4b8e6241b2e65f4150f2d2", "message": "Blob DB: Move BlobFile definition to a separate file\n\nSummary:\nsimply move BlobFile definition from blob_db_impl.h to blob_file.h.\nCloses https://github.com/facebook/rocksdb/pull/3002\n\nDifferential Revision: D6050143\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: a8fb6e094fe39bdeace6279569834bc65aa64a34"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/31d3e4181099bd92ca5c1be0ab8b7f51765fa558", "message": "PinnableSlice move assignment\n\nSummary:\nAllow `std::move(pinnable_slice)`.\nCloses https://github.com/facebook/rocksdb/pull/2997\n\nDifferential Revision: D6036782\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 583fb0419a97e437ff530f4305822341cd3381fa"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/fb4ae4d8108120b3381d143d77d44f656ffbc29c", "message": "fix DBImpl::NewInternalIterator super-version leak on failure\n\nSummary:\nClose #2955\nCloses https://github.com/facebook/rocksdb/pull/2960\n\nDifferential Revision: D5962872\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: a6472d5c015bea3dc476c572ff5a5c90259e6059"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/38bbc879a5e35a92a092ba20a992b626b88fd105", "message": "fix travis failure for ccache command not found\n\nSummary:\nTravis don't have ccache installed on Mac. Only run the ccache command when it exists.\nhttps://docs.travis-ci.com/user/caching/#ccache-cache\nCloses https://github.com/facebook/rocksdb/pull/2990\n\nDifferential Revision: D6028837\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 1e2d1c7f37be2c73773258c1fd5f24eebe7a06c6"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/8c392a31d7a4ab4e1f875a69e4205d94018fdde3", "message": "WritePrepared Txn: Iterator\n\nSummary:\nOn iterator create, take a snapshot, create a ReadCallback and pass the ReadCallback to the underlying DBIter to check if key is committed.\nCloses https://github.com/facebook/rocksdb/pull/2981\n\nDifferential Revision: D6001471\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 3565c4cdaf25370ba47008b0e0cb65b31dfe79fe"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/17c6325e8a6b5bc1489ff7a422a15ec81720efbb", "message": "WritePrepare Txn: Cancel flush/compaction before destruction\n\nSummary:\nOn WritePreparedTxnDB destruct there could be running compaction/flush holding a SnapshotChecker, which holds a pointer back to WritePreparedTxnDB. Make sure those jobs finished before destructing WritePreparedTxnDB.\n\nThis is caught by TransactionTest::SeqAdvanceTest.\nCloses https://github.com/facebook/rocksdb/pull/2982\n\nDifferential Revision: D6002957\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: f1e70390c9798d1bd7959f5c8e2a1c14100773c3"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/d1b74b0c82ea261436e8caf72ba427c7a7230953", "message": "WritePrepared Txn: Compaction/Flush\n\nSummary:\nUpdate Compaction/Flush to support WritePreparedTxnDB: Add SnapshotChecker which is a proxy to query WritePreparedTxnDB::IsInSnapshot. Pass SnapshotChecker to DBImpl on WritePreparedTxnDB open. CompactionIterator use it to check if a key has been committed and if it is visible to a snapshot. In CompactionIterator:\n* check if key has been committed. If not, output uncommitted keys AS-IS.\n* use SnapshotChecker to check if key is visible to a snapshot when in need.\n* do not output key with seq = 0 if the key is not committed.\nCloses https://github.com/facebook/rocksdb/pull/2926\n\nDifferential Revision: D5902907\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 945e037fdf0aa652dc5ba0ad879461040baa0320"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/cc20ec3689ff7da0df229196cad9adfca6ab4702", "message": "WritePrepared Txn: Test sequence number 0 is visible\n\nSummary:\nCompaction will output keys with sequence number 0, if it is visible to\nearliest snapshot. Adding a test to make sure IsInSnapshot() report sequence number 0 is\nvisible to any snapshot.\nCloses https://github.com/facebook/rocksdb/pull/2974\n\nDifferential Revision: D5990665\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: ef50ebc777ff8ca688771f3ab598c7a609b0b65e"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/92ccae71235d4a86e38ef7b80507bd3a548c8ef4", "message": "speedup 'make check'\n\nSummary:\nMake SnapshotConcurrentAccessTest run in the beginning of the queue.\n\nTest Plan\n`make all check -j64` on devserver\nCloses https://github.com/facebook/rocksdb/pull/2962\n\nDifferential Revision: D5965871\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 8cb5a47c2468be0fbbb929226a143ec5848bfaa9"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/d1cab2b64ea9d92fb4b27f3f31a0d45267d7fbc5", "message": "Add ValueType::kTypeBlobIndex\n\nSummary:\nAdd kTypeBlobIndex value type, which will be used by blob db only, to insert a (key, blob_offset) KV pair. The purpose is to\n1. Make it possible to open existing rocksdb instance as blob db. Existing value will be of kTypeIndex type, while value inserted by blob db will be of kTypeBlobIndex.\n2. Make rocksdb able to detect if the db contains value written by blob db, if so return error.\n3. Make it possible to have blob db optionally store value in SST file (with kTypeValue type) or as a blob value (with kTypeBlobIndex type).\n\nThe root db (DBImpl) basically pretended kTypeBlobIndex are normal value on write. On Get if is_blob is provided, return whether the value read is of kTypeBlobIndex type, or return Status::NotSupported() status if is_blob is not provided. On scan allow_blob flag is pass and if the flag is true, return wether the value is of kTypeBlobIndex type via iter->IsBlob().\n\nChanges on blob db side will be in a separate patch.\nCloses https://github.com/facebook/rocksdb/pull/2886\n\nDifferential Revision: D5838431\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 3c5306c62bc13bb11abc03422ec5cbcea1203cca"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/ec48e5c77ff40615b5201a9770cbb48214abc496", "message": "Add TransactionDB::SingleDelete()\n\nSummary:\nLooks like the API is simply missing. Adding it.\nCloses https://github.com/facebook/rocksdb/pull/2937\n\nDifferential Revision: D5919955\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 6e2e9c96c29882b0bb4113d1f8efb72bffc57878"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/be97dbb15c1bb59f54916afac62b7abf39e18d02", "message": "Fix WritePreparedTransactionTest::SeqAdvanceTest ASAN failure\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/2922\n\nDifferential Revision: D5895310\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 52c635a25d22478ec1eca49b6817551202babac2"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/1480e6f7cffdc02ffc13b3ba04d559f0c2f688fb", "message": "Fix TransactionTest::SeqAdvanceTest ASAN failure\n\nSummary:\nThe test didn't delete txn before creating a new one.\nCloses https://github.com/facebook/rocksdb/pull/2913\n\nDifferential Revision: D5880236\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 7a4fcaada3d86332292754502cd8f4341143bf4f"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/b4596c6174a5f8e050402f85c091f35a89a0263f", "message": "Fix Get does not return super version on error\n\nSummary:\nThis is caught when I was testing #2886.\nCloses https://github.com/facebook/rocksdb/pull/2907\n\nDifferential Revision: D5863153\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 8c54759ba1a0dc101f24ab50423e35731300612d"}], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3397", "title": "Assert last reference before destroy ColumnFamilyData", "body": "Summary:\r\nIn ColumnFamilySet destructor, assert it hold the last reference to cfd before destroy them.\r\n\r\nCloses #3112 \r\n\r\nTest Plan:\r\n`make all check`.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3396", "title": "DB::DumpSupportInfo should log all supported compression types", "body": "Summary:\r\nDB::DumpSupportInfo should log all supported compression types.\r\nCloses #3146 \r\n\r\nTest Plan:\r\nRun db_bench and check LOG. Sample output: https://gist.github.com/yiwu-arbug/8ca0364843e9b8648d661dfb5c2fb157", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3382", "title": "WritePrepared Txn: Fix DBIterator and add test", "body": "Summary:\r\nIn DBIter, Prev() call FindValueForCurrentKey() to search current value backward. If it find that there are too many stale value being skip, it fallback to FindValueForCurrentKeyUsingSeek(), seek directly to the key with snapshot sequence. With introduce of read_callback, however, the key it seek to might not be visible, according to read_callback. It needs to keep searching forward until the first visible value.\r\n\r\nTest Plan:\r\nAdded a new test to test DBIter with read_callback. ", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3097", "title": "Write path error handling cleanup", "body": "Summary:\r\nFixing the bug where concurrent writes may get Status::OK while it actually gets IOError on WAL write. This happens when multiple writes form a write batch group, and the leader get an IOError while writing to WAL. The leader failed to pass the error to followers in the group, and the followers end up return Status::OK() while actually writing nothing. The bug only affect writes in a batch group. Future writes after the batch group will correctly return immediately with the IOError.\r\n\r\nCloses #3096 \r\n\r\nTest Plan:\r\nSee the new test.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/23348148", "body": "Need to call TsTCSync() first? But actually loading last sequence (line 952 snapshot = version_->LastSequence()) is a fence as well so I think maybe we don't even need `ts_tc_fence_`.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23348148/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23710489", "body": "target_file_size_base is of uint64_t, casting it to int32 cause compile error under clang.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23710489/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "guozhangwang": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3072", "title": "5.8.0 has a regression error on Windows compared to 5.7.3", "body": "We found an issue running Kafka Streams with rocksDB 5.8.0 on Windows, while 5.7.3 does not have this issue. The summary as well as the proc-dump can be found on the JIRA itself:\r\n\r\nhttps://issues.apache.org/jira/browse/KAFKA-6100\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3072/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "moderndeveloperllc": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3070", "title": "Unable to start mongodb do to [RocksCompactionThread] Invalid access at address: 0x1b", "body": "### Expected behavior\r\nComplete start up. The exact data works fine with 5.7.3.\r\n\r\n### Actual behavior\r\n```\r\n2017-10-16T10:01:55.270-0500 I CONTROL  [main] ***** SERVER RESTARTED *****\r\n2017-10-16T10:01:55.277-0500 I STORAGE  [main] [RocksDB] Block Cache Size GB: 2\r\n2017-10-16T10:01:55.277-0500 I STORAGE  [main] [RocksDB] Compression: lz4\r\n2017-10-16T10:01:55.277-0500 I STORAGE  [main] [RocksDB] MaxWriteMBPerSec: 1024\r\n2017-10-16T10:01:55.277-0500 I STORAGE  [main] [RocksDB] Engine custom option: \r\n2017-10-16T10:01:55.277-0500 I STORAGE  [main] [RocksDB] Crash safe counters: 0\r\n2017-10-16T10:01:55.277-0500 I STORAGE  [main] [RocksDB] Counters: 1\r\n2017-10-16T10:01:55.277-0500 I STORAGE  [main] [RocksDB] Use SingleDelete in index: 0\r\n2017-10-16T10:01:55.282-0500 I CONTROL  [initandlisten] MongoDB starting : pid=8419 port=27017 dbpath=/usr/local/var/mongodb 64-bit host=xxxxxxxxxx.local\r\n2017-10-16T10:01:55.282-0500 I CONTROL  [initandlisten] db version v3.4.7-1.8\r\n2017-10-16T10:01:55.282-0500 I CONTROL  [initandlisten] git version: ae48d6e032e8775d6cc57699e9bf210a0b99bdc9\r\n2017-10-16T10:01:55.282-0500 I CONTROL  [initandlisten] allocator: system\r\n2017-10-16T10:01:55.282-0500 I CONTROL  [initandlisten] modules: none\r\n2017-10-16T10:01:55.282-0500 I CONTROL  [initandlisten] build environment:\r\n2017-10-16T10:01:55.283-0500 I CONTROL  [initandlisten]     distarch: x86_64\r\n2017-10-16T10:01:55.283-0500 I CONTROL  [initandlisten]     target_arch: x86_64\r\n2017-10-16T10:01:55.283-0500 I CONTROL  [initandlisten] options: { config: \"/usr/local/etc/mongod.conf\", net: { bindIp: \"127.0.0.1\", port: 27017 }, repair: true, security: { authorization: \"enabled\" }, storage: { dbPath: \"/usr/local/var/mongodb\", engine: \"rocksdb\", journal: { enabled: false }, rocksdb: { cacheSizeGB: 2, compression: \"lz4\" } }, systemLog: { destination: \"file\", logAppend: true, path: \"/usr/local/var/log/mongodb/mongo.log\" } }\r\n2017-10-16T10:01:55.292-0500 I STORAGE  [initandlisten] 2 dropped prefixes need compaction\r\n2017-10-16T10:01:55.292-0500 F -        [RocksCompactionThread] Invalid access at address: 0x1b\r\n2017-10-16T10:01:55.292-0500 I STORAGE  [initandlisten] Repairing catalog metadata\r\n2017-10-16T10:01:55.295-0500 F -        [RocksCompactionThread] Got signal: 11 (Segmentation fault: 11).\r\n\r\n 0x104cbbfd9 0x104cbb9ba 0x7fffe82f5b3a 0x107d7b508 0x107953e0a 0x10767aa94 0x1049b575c 0x1049b50b0 0x104c4b5ba 0x104c4d2be 0x7fffe82ff93b 0x7fffe82ff887 0x7fffe82ff08d\r\n----- BEGIN BACKTRACE -----\r\n{\"backtrace\":[{\"b\":\"1040FE000\",\"o\":\"BBDFD9\",\"s\":\"_ZN5mongo15printStackTraceERNSt3__113basic_ostreamIcNS0_11char_traitsIcEEEE\"},{\"b\":\"1040FE000\",\"o\":\"BBD9BA\",\"s\":\"_ZN5mongo12_GLOBAL__N_124abruptQuitWithAddrSignalEiP9__siginfoPv\"},{\"b\":\"7FFFE82F3000\",\"o\":\"2B3A\",\"s\":\"_sigtramp\"},{\"b\":\"107D49000\",\"o\":\"32508\",\"s\":\"je_tcache_arena_associate\"},{\"b\":\"1075FD000\",\"o\":\"356E0A\",\"s\":\"_ZN7rocksdb16IdentityFileNameERKNSt3__112basic_stringIcNS0_11char_traitsIcEENS0_9allocatorIcEEEE\"},{\"b\":\"1075FD000\",\"o\":\"7DA94\",\"s\":\"_ZNK7rocksdb6DBImpl13GetDbIdentityERNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEE\"},{\"b\":\"1040FE000\",\"o\":\"8B775C\",\"s\":\"_ZN5mongo23CompactionBackgroundJob7compactERKNS0_9CompactOpE\"},{\"b\":\"1040FE000\",\"o\":\"8B70B0\",\"s\":\"_ZN5mongo23CompactionBackgroundJob3runEv\"},{\"b\":\"1040FE000\",\"o\":\"B4D5BA\",\"s\":\"_ZN5mongo13BackgroundJob7jobBodyEv\"},{\"b\":\"1040FE000\",\"o\":\"B4F2BE\",\"s\":\"_ZNSt3__114__thread_proxyINS_5tupleIJNS_10unique_ptrINS_15__thread_structENS_14default_deleteIS3_EEEENS_6__bindIMN5mongo13BackgroundJobEFvvEJPS9_EEEEEEEEPvSF_\"},{\"b\":\"7FFFE82FC000\",\"o\":\"393B\",\"s\":\"_pthread_body\"},{\"b\":\"7FFFE82FC000\",\"o\":\"3887\",\"s\":\"_pthread_body\"},{\"b\":\"7FFFE82FC000\",\"o\":\"308D\",\"s\":\"thread_start\"}],\"processInfo\":{ \"mongodbVersion\" : \"3.4.7-1.8\", \"gitVersion\" : \"ae48d6e032e8775d6cc57699e9bf210a0b99bdc9\", \"compiledModules\" : [], \"uname\" : { \"sysname\" : \"Darwin\", \"release\" : \"16.7.0\", \"version\" : \"Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64\", \"machine\" : \"x86_64\" }, \"somap\" : [ { \"path\" : \"/usr/local/bin/mongod\", \"machType\" : 2, \"b\" : \"1040FE000\", \"vmaddr\" : \"100000000\", \"buildId\" : \"A2ECB036470B3C0CA211B1135388882D\" }, { \"path\" : \"/usr/local/opt/rocksdb/lib/librocksdb.5.8.dylib\", \"machType\" : 6, \"b\" : \"1075FD000\", \"vmaddr\" : \"0\", \"buildId\" : \"F1733150D1DF3D63B0A90EE263A29CDA\" }, { \"path\" : \"/usr/lib/libz.1.dylib\", \"machType\" : 6, \"b\" : \"7FFFE7FEA000\", \"vmaddr\" : \"7FFF8AA8A000\", \"buildId\" : \"46E3FFA24328327A8D34A03E20BFFB8E\" }, { \"path\" : \"/usr/lib/libbz2.1.0.dylib\", \"machType\" : 6, \"b\" : \"7FFFE6C67000\", \"vmaddr\" : \"7FFF89707000\", \"buildId\" : \"ADFA329ADCE7356D8F09A3168DFC6610\" }, { \"path\" : \"/usr/lib/libSystem.B.dylib\", \"machType\" : 6, \"b\" : \"7FFFE6B3C000\", \"vmaddr\" : \"7FFF895DC000\", \"buildId\" : \"F18AC1E7C6F134B18069BE571B3231D4\" }, { \"path\" : \"/usr/lib/libc++.1.dylib\", \"machType\" : 6, \"b\" : \"7FFFE6C76000\", \"vmaddr\" : \"7FFF89716000\", \"buildId\" : \"0B43BB5DE6EB34648DE9B41AC8ED9D1C\" }, { \"path\" : \"/usr/local/opt/snappy/lib/libsnappy.1.dylib\", \"machType\" : 6, \"b\" : \"107D05000\", \"vmaddr\" : \"0\", \"buildId\" : \"1875A745F9313F159DAED5BECC8433AD\" }, { \"path\" : \"/usr/local/opt/gflags/lib/libgflags.2.2.dylib\", \"machType\" : 6, \"b\" : \"107D14000\", \"vmaddr\" : \"0\", \"buildId\" : \"B05FC49CDD5A3727B95A7BB293BF2E40\" }, { \"path\" : \"/usr/local/opt/lz4/lib/liblz4.1.dylib\", \"machType\" : 6, \"b\" : \"107D33000\", \"vmaddr\" : \"0\", \"buildId\" : \"C62AC8446E343E0288B2C6633AF16AC0\" }, { \"path\" : \"/usr/local/opt/jemalloc/lib/libjemalloc.2.dylib\", \"machType\" : 6, \"b\" : \"107D49000\", \"vmaddr\" : \"0\", \"buildId\" : \"3277F5CF86BD3A7C9D6F416DFA306FA0\" }, { \"path\" : \"/usr/lib/libc++abi.dylib\", \"machType\" : 6, \"b\" : \"7FFFE6CCD000\", \"vmaddr\" : \"7FFF8976D000\", \"buildId\" : \"BC271AD3831B362A9DA7E8C51F285FE4\" }, { \"path\" : \"/usr/lib/system/libcache.dylib\", \"machType\" : 6, \"b\" : \"7FFFE800A000\", \"vmaddr\" : \"7FFF8AAAA000\", \"buildId\" : \"093A4DAB83853D47A350E20CB7CCF7BF\" }, { \"path\" : \"/usr/lib/system/libcommonCrypto.dylib\", \"machType\" : 6, \"b\" : \"7FFFE800F000\", \"vmaddr\" : \"7FFF8AAAF000\", \"buildId\" : \"8A64D1B0C70E385C92F0E669079FDA90\" }, { \"path\" : \"/usr/lib/system/libcompiler_rt.dylib\", \"machType\" : 6, \"b\" : \"7FFFE801A000\", \"vmaddr\" : \"7FFF8AABA000\", \"buildId\" : \"55D47421772A32ABB5291A46C2F43B4D\" }, { \"path\" : \"/usr/lib/system/libcopyfile.dylib\", \"machType\" : 6, \"b\" : \"7FFFE8022000\", \"vmaddr\" : \"7FFF8AAC2000\", \"buildId\" : \"819BEA3CDF113E3DA1A15A51C5BF1961\" }, { \"path\" : \"/usr/lib/system/libcorecrypto.dylib\", \"machType\" : 6, \"b\" : \"7FFFE802B000\", \"vmaddr\" : \"7FFF8AACB000\", \"buildId\" : \"65D7165E2E71335DA2D633F78E2DF0C1\" }, { \"path\" : \"/usr/lib/system/libdispatch.dylib\", \"machType\" : 6, \"b\" : \"7FFFE80AF000\", \"vmaddr\" : \"7FFF8AB4F000\", \"buildId\" : \"6582BAD6ED273B30B62090B1C5A4AE3C\" }, { \"path\" : \"/usr/lib/system/libdyld.dylib\", \"machType\" : 6, \"b\" : \"7FFFE80E1000\", \"vmaddr\" : \"7FFF8AB81000\", \"buildId\" : \"9B2AC56D107C3541A1279094A751F2C9\" }, { \"path\" : \"/usr/lib/system/libkeymgr.dylib\", \"machType\" : 6, \"b\" : \"7FFFE80E7000\", \"vmaddr\" : \"7FFF8AB87000\", \"buildId\" : \"7AA011A9DC213488BF733B5B14D1FDD6\" }, { \"path\" : \"/usr/lib/system/liblaunch.dylib\", \"machType\" : 6, \"b\" : \"7FFFE80F5000\", \"vmaddr\" : \"7FFF8AB95000\", \"buildId\" : \"B856ABD2896E3DE0B2C8146A6AF8E2A7\" }, { \"path\" : \"/usr/lib/system/libmacho.dylib\", \"machType\" : 6, \"b\" : \"7FFFE80F6000\", \"vmaddr\" : \"7FFF8AB96000\", \"buildId\" : \"17D5D855F6C33B04B680E9BF02EF8AED\" }, { \"path\" : \"/usr/lib/system/libquarantine.dylib\", \"machType\" : 6, \"b\" : \"7FFFE80FC000\", \"vmaddr\" : \"7FFF8AB9C000\", \"buildId\" : \"12448CC2378E35F3BE339DC395A5B970\" }, { \"path\" : \"/usr/lib/system/libremovefile.dylib\", \"machType\" : 6, \"b\" : \"7FFFE80FF000\", \"vmaddr\" : \"7FFF8AB9F000\", \"buildId\" : \"38D4CB9C10CD30D38B7BA515EC75FE85\" }, { \"path\" : \"/usr/lib/system/libsystem_asl.dylib\", \"machType\" : 6, \"b\" : \"7FFFE8101000\", \"vmaddr\" : \"7FFF8ABA1000\", \"buildId\" : \"096E42283B7C30A68B13EC909A64499A\" }, { \"path\" : \"/usr/lib/system/libsystem_blocks.dylib\", \"machType\" : 6, \"b\" : \"7FFFE811A000\", \"vmaddr\" : \"7FFF8ABBA000\", \"buildId\" : \"10DC540473AB35B3A277A8AFECB476EB\" }, { \"path\" : \"/usr/lib/system/libsystem_c.dylib\", \"machType\" : 6, \"b\" : \"7FFFE811B000\", \"vmaddr\" : \"7FFF8ABBB000\", \"buildId\" : \"E5AE52447D0C36AC8BB6C7AE7EA52A4B\" }, { \"path\" : \"/usr/lib/system/libsystem_configuration.dylib\", \"machType\" : 6, \"b\" : \"7FFFE81A9000\", \"vmaddr\" : \"7FFF8AC49000\", \"buildId\" : \"BECC01A2CA8D31E6BCDFD452965FA976\" }, { \"path\" : \"/usr/lib/system/libsystem_coreservices.dylib\", \"machType\" : 6, \"b\" : \"7FFFE81AD000\", \"vmaddr\" : \"7FFF8AC4D000\", \"buildId\" : \"7D26DE79B424345085E1F7FAB32714AB\" }, { \"path\" : \"/usr/lib/system/libsystem_coretls.dylib\", \"machType\" : 6, \"b\" : \"7FFFE81B1000\", \"vmaddr\" : \"7FFF8AC51000\", \"buildId\" : \"EC6FCF07DCFB3A039CC96DD3709974C6\" }, { \"path\" : \"/usr/lib/system/libsystem_dnssd.dylib\", \"machType\" : 6, \"b\" : \"7FFFE81CA000\", \"vmaddr\" : \"7FFF8AC6A000\", \"buildId\" : \"CC9602150B1B3822A13A3DDE96FA796F\" }, { \"path\" : \"/usr/lib/system/libsystem_info.dylib\", \"machType\" : 6, \"b\" : \"7FFFE81D1000\", \"vmaddr\" : \"7FFF8AC71000\", \"buildId\" : \"611DB84CBF703F928702B9F28A900920\" }, { \"path\" : \"/usr/lib/system/libsystem_kernel.dylib\", \"machType\" : 6, \"b\" : \"7FFFE81FB000\", \"vmaddr\" : \"7FFF8AC9B000\", \"buildId\" : \"34B1F16CBC9C3C5F90450CAE91CB5914\" }, { \"path\" : \"/usr/lib/system/libsystem_m.dylib\", \"machType\" : 6, \"b\" : \"7FFFE821E000\", \"vmaddr\" : \"7FFF8ACBE000\", \"buildId\" : \"86D499B5BBDC3D3B8A4E97AE8E6672A4\" }, { \"path\" : \"/usr/lib/system/libsystem_malloc.dylib\", \"machType\" : 6, \"b\" : \"7FFFE8266000\", \"vmaddr\" : \"7FFF8AD06000\", \"buildId\" : \"A3D15F1799A633678C7E4280E8619C95\" }, { \"path\" : \"/usr/lib/system/libsystem_network.dylib\", \"machType\" : 6, \"b\" : \"7FFFE8285000\", \"vmaddr\" : \"7FFF8AD25000\", \"buildId\" : \"369D022156CA3C3E9EDE94B41CAE77B7\" }, { \"path\" : \"/usr/lib/system/libsystem_networkextension.dylib\", \"machType\" : 6, \"b\" : \"7FFFE82DF000\", \"vmaddr\" : \"7FFF8AD7F000\", \"buildId\" : \"B021F2B38A753633ABB0FC012B8E9B0C\" }, { \"path\" : \"/usr/lib/system/libsystem_notify.dylib\", \"machType\" : 6, \"b\" : \"7FFFE82E9000\", \"vmaddr\" : \"7FFF8AD89000\", \"buildId\" : \"B8160190A0693B3ABDF62AA408221FAE\" }, { \"path\" : \"/usr/lib/system/libsystem_platform.dylib\", \"machType\" : 6, \"b\" : \"7FFFE82F3000\", \"vmaddr\" : \"7FFF8AD93000\", \"buildId\" : \"897462FDB318321BA554E61982630F7E\" }, { \"path\" : \"/usr/lib/system/libsystem_pthread.dylib\", \"machType\" : 6, \"b\" : \"7FFFE82FC000\", \"vmaddr\" : \"7FFF8AD9C000\", \"buildId\" : \"B8FB5E20329539E2B5EBB464D1D4B104\" }, { \"path\" : \"/usr/lib/system/libsystem_sandbox.dylib\", \"machType\" : 6, \"b\" : \"7FFFE8307000\", \"vmaddr\" : \"7FFF8ADA7000\", \"buildId\" : \"4B92EC49ACD036AEB07AA2B8152EAF9D\" }, { \"path\" : \"/usr/lib/system/libsystem_secinit.dylib\", \"machType\" : 6, \"b\" : \"7FFFE830B000\", \"vmaddr\" : \"7FFF8ADAB000\", \"buildId\" : \"F78B847B35653E4B98A6F7AD40392E2D\" }, { \"path\" : \"/usr/lib/system/libsystem_symptoms.dylib\", \"machType\" : 6, \"b\" : \"7FFFE830D000\", \"vmaddr\" : \"7FFF8ADAD000\", \"buildId\" : \"3390E07CC1CE348FADBD2C5440B45EAA\" }, { \"path\" : \"/usr/lib/system/libsystem_trace.dylib\", \"machType\" : 6, \"b\" : \"7FFFE8315000\", \"vmaddr\" : \"7FFF8ADB5000\", \"buildId\" : \"AC63A7FE50D93A3096E6F6B7FF16E465\" }, { \"path\" : \"/usr/lib/system/libunwind.dylib\", \"machType\" : 6, \"b\" : \"7FFFE8329000\", \"vmaddr\" : \"7FFF8ADC9000\", \"buildId\" : \"3D50D8A8C460334DA5192DA841102C6B\" }, { \"path\" : \"/usr/lib/system/libxpc.dylib\", \"machType\" : 6, \"b\" : \"7FFFE832F000\", \"vmaddr\" : \"7FFF8ADCF000\", \"buildId\" : \"BF896DF0D8E931A8A4B301120BFEEE52\" }, { \"path\" : \"/usr/lib/libobjc.A.dylib\", \"machType\" : 6, \"b\" : \"7FFFE77EB000\", \"vmaddr\" : \"7FFF8A28B000\", \"buildId\" : \"70614861034032E285EDFE65759CDFFA\" } ] }}\r\n mongod(_ZN5mongo15printStackTraceERNSt3__113basic_ostreamIcNS0_11char_traitsIcEEEE+0x39) [0x104cbbfd9]\r\n mongod(_ZN5mongo12_GLOBAL__N_124abruptQuitWithAddrSignalEiP9__siginfoPv+0x12A) [0x104cbb9ba]\r\n libsystem_platform.dylib(_sigtramp+0x1A) [0x7fffe82f5b3a]\r\n libjemalloc.2.dylib(je_tcache_arena_associate+0x2A) [0x107d7b508]\r\n librocksdb.5.8.dylib(_ZN7rocksdb16IdentityFileNameERKNSt3__112basic_stringIcNS0_11char_traitsIcEENS0_9allocatorIcEEEE+0x15) [0x107953e0a]\r\n librocksdb.5.8.dylib(_ZNK7rocksdb6DBImpl13GetDbIdentityERNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEE+0x38) [0x10767aa94]\r\n mongod(_ZN5mongo23CompactionBackgroundJob7compactERKNS0_9CompactOpE+0x37C) [0x1049b575c]\r\n mongod(_ZN5mongo23CompactionBackgroundJob3runEv+0x3B0) [0x1049b50b0]\r\n mongod(_ZN5mongo13BackgroundJob7jobBodyEv+0x13A) [0x104c4b5ba]\r\n mongod(_ZNSt3__114__thread_proxyINS_5tupleIJNS_10unique_ptrINS_15__thread_structENS_14default_deleteIS3_EEEENS_6__bindIMN5mongo13BackgroundJobEFvvEJPS9_EEEEEEEEPvSF_+0x3E) [0x104c4d2be]\r\n libsystem_pthread.dylib(_pthread_body+0xB4) [0x7fffe82ff93b]\r\n libsystem_pthread.dylib(_pthread_body+0x0) [0x7fffe82ff887]\r\n libsystem_pthread.dylib(thread_start+0xD) [0x7fffe82ff08d]\r\n-----  END BACKTRACE  -----\r\n```\r\n### Steps to reproduce the behavior\r\nThis is using the v3.4.7-1.8 tar build from Percona. You can see the config file above.\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3070/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shanshan0309": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3049", "title": "RocksDB failed to build (Release/x64) on windows due to warning C4189: 'ret': local variable is initialized but not referenced ", "body": "RocksDB failed to build (Release/x64) on VS2017 due to warning C4189. We noticed the warning C4189 is reported after commit [ebab2e2](https://github.com/facebook/rocksdb/commits/master) \"Enable MSVC W4 with a few exceptions. Fix warnings and bugs\". Could you please help take a look about this issue? Thank you very much!\r\n\r\n**Failures:**\r\nd:\\rocksdb\\src\\db\\column_family.cc(466): warning C4189: 'deleted': local variable is initialized but not referenced [D:\\RocksDB\\build_x64\\rocksdb-shared.vcxproj]\r\nd:\\rocksdb\\src\\db\\compaction_iterator.cc(140): warning C4189: 'valid_key': local variable is initialized but not referenced [D:\\RocksDB\\build_x64\\rocksdb-shared.vcxproj]\r\nd:\\rocksdb\\src\\db\\compaction_iterator.cc(337): warning C4189: 'last_sequence': local variable is initialized but not referenced [D:\\RocksDB\\build_x64\\rocksdb-shared.vcxproj]         d:\\rocksdb\\src\\db\\compaction_iterator.cc(541): warning C4189: 'valid_key': local variable is initialized but not referenced [D:\\RocksDB\\build_x64\\rocksdb-shared.vcxproj]\r\nd:\\rocksdb\\src\\db\\db_impl_compaction_flush.cc(1681): warning C4189: 'output_level': local variable is initialized but not referenced [D:\\RocksDB\\build_x64\\rocksdb-shared.vcxproj]\r\nd:\\rocksdb\\src\\db\\version_set.cc(1914): warning C4189: 'count': local variable is initialized but not referenced [D:\\RocksDB\\build_x64\\rocksdb-shared.vcxproj]\r\nd:\\rocksdb\\src\\env\\mock_env.cc(382): warning C4189: 'ret': local variable is initialized but not referenced [D:\\RocksDB\\build_x64\\rocksdb-shared.vcxproj]\r\nd:\\rocksdb\\src\\monitoring\\thread_status_updater.cc(255): warning C4189: 'result': local variable is initialized but not referenced [D:\\RocksDB\\build_x64\\rocksdb-shared.vcxproj]\r\nd:\\rocksdb\\src\\utilities\\blob_db\\blob_db_impl.cc(1315): warning C4189: 'erased': local variable is initialized but not referenced [D:\\RocksDB\\build_x64\\rocksdb-shared.vcxproj]\r\nd:\\rocksdb\\src\\utilities\\document\\json_document.cc(51): warning C4189: 'bytesWritten': local variable is initialized but not referenced [D:\\RocksDB\\build_x64\\rocksdb-shared.vcxproj]\r\nd:\\rocksdb\\src\\utilities\\persistent_cache\\block_cache_tier_metadata.cc(66): warning C4189: 'ok': local variable is initialized but not referenced [D:\\RocksDB\\build_x64\\rocksdb-shared.vcxproj]\r\nd:\\rocksdb\\src\\utilities\\write_batch_with_index\\write_batch_with_index.cc(483): warning C4189: 'success': local variable is initialized but not referenced [D:\\RocksDB\\build_x64\\rocksdb-shared.vcxproj]\r\nd:\\rocksdb\\src\\port\\win\\io_win.cc(282): warning C4189: 'ret': local variable is initialized but not referenced [D:\\RocksDB\\build_x64\\rocksdb-shared.vcxproj]\r\nd:\\rocksdb\\src\\port\\win\\io_win.cc(1023): warning C4189: 'ret': local variable is initialized but not referenced [D:\\RocksDB\\build_x64\\rocksdb-shared.vcxproj]\r\nd:\\rocksdb\\src\\port\\win\\env_win.cc(77): warning C4189: 'ret': local variable is initialized but not referenced [D:\\RocksDB\\build_x64\\rocksdb-shared.vcxproj]\r\n\r\n**Repro steps:**\r\n1.Open VS2017 x64 Native tools command tool\r\n2.git clone https://github.com/facebook/rocksdb D:\\RocksDB\\src\r\n3.cd D:\\RocksDB\r\n4.mkdir build_x64 && pushd build_x64\r\n5.cmake -G \"Visual Studio 15 Win64\" -DCMAKE_SYSTEM_VERSION=10.0.15063.0  ..\\src\r\n6.msbuild build_x64\\rocksdb.sln /p:Configuration=Release;Platform=x64 /t:Rebuild /m /p:BuildInParallel=true\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3049/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chenqinb": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/3012", "title": "corrupt when value to long", "body": "> Note: Please use Issues only for bug reports. For questions, discussions, feature requests, etc. post to dev group: https://www.facebook.com/groups/rocksdb.dev\r\n\r\n### Expected behavior\r\nshould not corrupt\r\n### Actual behavior\r\ncorrupt\r\n### Steps to reproduce the behavior\r\nI have one Key-Value,like which only have a lots of merge operations.\r\neg:\r\nkey = 8618000000000\r\nvalue = id1,id2,id3,......\r\nthe value is growning when the id append.Then the process corrupt.\r\nHow could I spilt the value to two or more Key-Value instace, in FullMerge2 or PartialMergeMulti?\r\nlike this:\r\nkey = 8618000000000-1\r\nvalue = id1,id2,id3......id10000000\r\nKey = 8618000000000-2\r\nvalue = id10000001,id10000002....\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/3012/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "qraz": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/2968", "title": "Static library Windows build size", "body": "I was building using vcpkg on Windows 7 with VS 2015 with update 3.\r\n\r\n When I built using static library option, the size of the binary is around 1 GB. I think it could be due to the debug information being added to the library, although it happens with release builds (750 MB) as well.\r\n\r\nI've read this post replies to this https://www.facebook.com/groups/rocksdb.dev/permalink/1341600742605103/ saying that the size is normal. \r\nHas anyone else noticed this and have any solution or work around? Thanks.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2968/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "adamretter": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/2954", "title": "Unable to build with jemalloc on MacOS X", "body": "After installing jemalloc on MacOS with Homebrew:\r\n\r\n```bash\r\n$ brew install jemalloc\r\n```\r\nI am no longer able to build a release with `make rocksdbjavastatic`. I now get this error at the linker stage:\r\n\r\n```\r\n\t  libz.a libbz2.a libsnappy.a liblz4.a libzstd.a \r\nUndefined symbols for architecture x86_64:\r\n  \"_malloc_stats_print\", referenced from:\r\n      rocksdb::DumpMallocStats(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*) in malloc_stats.o\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake: *** [rocksdbjavastatic] Error 1\r\n```", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2954/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/1251", "title": "RocksDB#multiGet copies keys", "body": "The `multiGet` function in RocksJava in the `RocksDB` and and `Transaction` class make copies of the provided `key` lists, this should be avoided by either:\n1. Using Slices instead so that the `byte[]` are encapsulated and the `List`/`Map` contains just a reference to the slice.\n2. Just returning a `List` of values as opposed to a `Map`. Probably the simplest option and closest to the C++ API!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/1251/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/d4da02d147e6ab331a0450e67368839c2e22c291", "message": "Add Jenkins for PPC64le build status badge\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/3356\n\nDifferential Revision: D6706909\n\nPulled By: sagar0\n\nfbshipit-source-id: 6e4757d9eceab3e8a6c1b83c1be4108e86576cb2"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/a53c571d2d69e212922c92f275258ede328652ab", "message": "FreeBSD build support for RocksDB and RocksJava\n\nSummary:\nTested on a clean FreeBSD 11.01 x64.\n\nCloses https://github.com/facebook/rocksdb/pull/1423\nCloses https://github.com/facebook/rocksdb/pull/3357\n\nDifferential Revision: D6705868\n\nPulled By: sagar0\n\nfbshipit-source-id: cbccbbdafd4f42922512ca03619a5d5583a425fd"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/398d72fa614b809cc4df153092e0665b57b170d2", "message": "Add autotune and #getBytesPerSecond() to RocksJava RateLimiter\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/3332\n\nDifferential Revision: D6667680\n\nPulled By: ajkr\n\nfbshipit-source-id: b2bb6889257850a4eb6f6cbd7106f62df7b82730"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/90c1d81975a03b0f8b352ddc614fbc99c2496ddd", "message": "Update javastatic dependencies\n\nSummary:\n1. Snappy 1.1.7\n2. LZ4 1.8.0\n3. ZSTD  1.3.3\nCloses https://github.com/facebook/rocksdb/pull/3331\n\nDifferential Revision: D6667933\n\nPulled By: ajkr\n\nfbshipit-source-id: 21c526609df7580481195a389d31f733e2695e65"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/00e5e1ef7f0b4c47509beef562cdc6f5eadc45eb", "message": "Fixes the build on Windows\n\nSummary:\nAs discovered during v5.9.2 release, and forward-ported.\nCloses https://github.com/facebook/rocksdb/pull/3323\n\nDifferential Revision: D6657209\n\nPulled By: siying\n\nfbshipit-source-id: b560d9f8ddb89e0ffaff7c895ec80f68ddf7dab4"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/560e9849959018f6a3fe210a24e78723d190541d", "message": "Added CompactionFilterFactory support to RocksJava\n\nSummary:\nThis PR also includes some cleanup, bugfixes and refactoring of the Java API. However these are really pre-cursors on the road to CompactionFilterFactory support.\nCloses https://github.com/facebook/rocksdb/pull/1241\n\nDifferential Revision: D6012778\n\nPulled By: sagar0\n\nfbshipit-source-id: 0774465940ee99001a78906e4fed4ef57068ad5c"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/983028f09756c1b9d446cbe327317e0f9e47a199", "message": "RocksJava build target for Docker on ppc64le\n\nSummary:\nThis enables us to crossbuild pcc64le RocksJava binaries with a suitably old version of glibc (2.17) on CentOS 7.\nCloses https://github.com/facebook/rocksdb/pull/2491\n\nDifferential Revision: D5955301\n\nPulled By: sagar0\n\nfbshipit-source-id: 69ef9746f1dc30ffde4063dc764583d8c7ae937e"}], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3334", "title": "Java wrapper for Native Comparators", "body": "This is an abstraction for working with custom Comparators implemented in native C++ code from Java. Native code must directly extend `rocksdb::Comparator`. When the native code comparator is compiled into the RocksDB codebase, you can then create a Java Class, and JNI stub to wrap it.\r\n\r\nUseful if the C++/JNI barrier overhead is too much for your applications comparator performance.\r\n\r\nAn example is provided in `java/rocksjni/native_comparator_wrapper_test.cc` and `java/src/main/java/org/rocksdb/NativeComparatorWrapperTest.java`.\r\n  \r\n  ", "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3065", "title": "Added bytes XOR merge operator", "body": "Closes https://github.com/facebook/rocksdb/pull/575\r\n\r\nI fixed the merge conflicts etc.", "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/1298", "title": "Add TransactionDB and OptimisticTransactionDB to the Java API", "body": "Closes https://github.com/facebook/rocksdb/issues/697\nCloses https://github.com/facebook/rocksdb/issues/1151\n", "author_association": "COLLABORATOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/7949051", "body": "This change breaks support for Mac OSX platforms. On MacOSX the library in the JAR will not be called librocksdbjni.so. Rather it will be called librocksdbjni.jnilib. I will submit a pull-request to fix this once my existing request https://github.com/facebook/rocksdb/pull/231is closed.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/7949051/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/9065079", "body": "This no longer builds on Mac OS X with the error:\n\n```\n./java/rocksjni/slice.cc:31:19: error: implicit conversion loses integer precision: 'size_t' (aka 'unsigned long') to 'const int' [-Werror,-Wshorten-64-to-32]\n  const int len = strlen(str);\n```\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/9065079/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/9070237", "body": "No worries, I resolved it locally by changing:\n\n`const int len = strlen(str);`\n\nto `const size_t len = strlen(str);`\n\nHowever, I also found other breakages (not caused by you) in Mac OS build when running `make check`.\n\nHave a great Christmas and don't worry for now. Cheers.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/9070237/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/9076636", "body": "@igorcanadi Ah ha you are of course correct\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/9076636/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/9082483", "body": "@fyrz Resolved by https://github.com/facebook/rocksdb/pull/444\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/9082483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/18664723", "body": "@sgalles Doh! Easily done it seems. I will send a PR...\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/18664723/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/18674959", "body": "@sgalles here it is - https://github.com/facebook/rocksdb/pull/1284\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/18674959/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/20380182", "body": "@yuslepukhin I am not sure I follow. Can you tell me why it fails on Windows?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/20380182/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16171312", "body": "I have added the comment and javadocs as requested and now re-based my commit.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16171312/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16179003", "body": "Ah. Sorry that was a mistake left-over from several rounds of refactoring. `startsWith` is common to both `Slice` and `DirectSlice` classes. It should have taken an argument of `final AbstractSlice prefix` rather than `final DirectSlice prefix`. I have now fixed this and rebased.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16179003/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16179368", "body": "1) I could have it return the handle, and have the Java code set the handle.\n\n2) However `AbstractSliceJni::setHandle` and corresponding functions are still needed by *`ComparatorJniCallback` classes, as it allows us to re-use slice objects, which reduces object creation over-head and massively improves performance for frequent method callbacks such as `compare`.\n\nWould you still like me to do (1)?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16179368/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16290398", "body": "I was under the impression that `std::string(utf)` creates a copy of the data, is that not so?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16290398/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16290435", "body": "Done.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16290435/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16290440", "body": "Done.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16290440/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16290495", "body": "I have reused port::Mutex to add locking where necessary. Please can you check and comment?\n\nStrangely, since adding the locking, the performance of my test java comparator application has actually increased  by 100%. I am not sure why that would be the case? At the moment I can only imagine that my test platform has changed somehow between the test runs.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16290495/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16290504", "body": "Ah ha. Okay, I have removed the commented code.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16290504/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16292324", "body": "I have fixed this in the manner you suggested.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16292324/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16292411", "body": "Would this be done through calling `delete [] slice->data_;`?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16292411/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/18389746", "body": "The point is to be able to re-use the slice objects. For example in a comparator you will compare many many slices, if you have to create a slice object and then destroy it each time the compare callback is made, it really kills performance. I have tested this, i.e. in my first approach I did not re-use the objects but performance was terrible, so this is why I introduced being able to re-use the Java slice objects. The setHandle function allows us to take the existing Java object and change it to point at a different underlying C++ slice object.\n\nDoes that make sense now? If not, it may be better for us to discuss this in real-time over IM or Skype if you could?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/18389746/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/24111561", "body": "Yes but I would rather do that in a seperate request in future\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/24111561/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}]}, "topilski": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/2930", "title": "Disable shlx instructon", "body": "Hi, how to disable SHLX instruction? according issue https://github.com/fastogt/fastonosql/issues/45#issuecomment-332107389", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2930/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/b9873162f07ef6cb60573f356d520a85c4fdc6a4", "message": "Fixed get version on windows, moved throwing exceptions into cc file.\n\nSummary:\nFixes for msys2 and mingw, hide exceptions into cpp  file.\nCloses https://github.com/facebook/rocksdb/pull/3377\n\nDifferential Revision: D6746707\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 456b38df80bc48b8386a2cf87f669b5a4f9999a4"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "paulovap": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/2809", "title": "[Android] SIGSEGV (signal SIGSEGV: invalid address (fault address: 0xa00004)) on Android NDK", "body": "Hello Team,\r\nI manage to compile RocksDB using Android NDK with a fairly trivial change(https://github.com/paulovap/rocksdb/commit/fa4f7b22bce379fed5a3591abf064f15fc1b5ff6) on CMakeFiles.txt, but got stuck in a SIGSEGV when Options object is being destroyed.\r\n\r\nThe code above is the entry point of the library. It simply creates rocksdb::Option on the stack and returns a string for the java part:\r\n```c++\r\nusing namespace rocksdb;\r\n\r\nextern \"C\"\r\nJNIEXPORT jstring\r\n\r\n\r\nJNICALL\r\nJava_com_app_android_db_engine_rocksdb_RocksDBEngine_hello_world(\r\n        JNIEnv *env,\r\n        jobject /* this */) {\r\n    Options options;\r\n    options.OptimizeForSmallDb();\r\n    std::string hello = \"hello world\";\r\n    return env->NewStringUTF(hello.c_str());\r\n}\r\n``` \r\n\r\nThe signal is thrown with the following stacktrace:\r\n\r\n![image](https://user-images.githubusercontent.com/2519883/29891858-dae78286-8dcc-11e7-98d7-1b1c350229f7.png)\r\n\r\n![image](https://user-images.githubusercontent.com/2519883/29891818-b9f66100-8dcc-11e7-8bed-7a12b0c2d84b.png)\r\n\r\nI am using the following CMakeFiles configuration on build.gradle:\r\n\r\n```gradle\r\n        externalNativeBuild {\r\n            cmake {\r\n                arguments \"-DANDROID_STL=c++_shared\" # libc++ LLVM runtime\r\n                arguments \"-DROCKSDB_LITE=ON\"\r\n                arguments \"-DWITH_TESTS=OFF\"\r\n                arguments \"-DWITH_TOOLS=OFF\"\r\n                arguments \"-DPORTABLE=ON\"\r\n            }\r\n        }\r\n```\r\nCMake variables for x86:\r\n```\r\nExecutable : /Users/ppinheiro/android/cmake/3.6.4111459/bin/cmake\r\narguments : \r\n-H/Users/ppinheiro/app\r\n-B/Users/ppinheiro/app/.externalNativeBuild/cmake/debug/x86\r\n-GAndroid Gradle - Ninja\r\n-DANDROID_ABI=x86\r\n-DANDROID_NDK=/Users/ppinheiro/android/ndk-bundle\r\n-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=/Users/ppinheiro/app/build/intermediates/cmake/debug/obj/x86\r\n-DCMAKE_BUILD_TYPE=Debug\r\n-DCMAKE_MAKE_PROGRAM=/Users/ppinheiro/android/cmake/3.6.4111459/bin/ninja\r\n-DCMAKE_TOOLCHAIN_FILE=/Users/ppinheiro/android/ndk-bundle/build/cmake/android.toolchain.cmake\r\n-DANDROID_PLATFORM=android-19\r\n-DCMAKE_CXX_FLAGS=\r\n-DANDROID_STL=c++_shared\r\njvmArgs : \r\n```\r\n\r\nAnd tested on x86 emulator and an arm device. Due to my lack of knowledge of C++, I am not sure what could be the issue. Hope you can shed some light on this matter. It would be awesome if we can use this project on Android environment.\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2809/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "magiczqw": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/2803", "title": "RocksDB.loadLibrary cause SIGSEGV", "body": "Hi all,\r\n     I downloaded release v5.7.2, merged code for java samples from \u201cFix statistics in RocksJava sample\u201d, but when I run RocksDBSample, I got SIGSEGV.\r\n     I simplified the code as this:\r\n\r\n```java\r\nimport org.rocksdb.*;\r\n\r\npublic class RocksDBSimple {\r\n    static {\r\n        RocksDB.loadLibrary();\r\n    }\r\n\r\n    public static void main(final String[] args) {\r\n        System.out.println(\"RocksDBSimple start\");\r\n        System.out.println(\"RocksDBSimple end\");\r\n    }\r\n}\r\n```\r\n\r\nStill, I got SIGSEGV. Here is the log:\r\n```\r\n#\r\n# A fatal error has been detected by the Java Runtime Environment:\r\n#\r\n#  SIGSEGV (0xb) at pc=0x00000001274f7ec5, pid=604, tid=0x0000000000001c03\r\n#\r\n# JRE version: Java(TM) SE Runtime Environment (8.0_131-b11) (build 1.8.0_131-b11)\r\n# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.131-b11 mixed mode bsd-amd64 compressed oops)\r\n# Problematic frame:\r\n# C  [libjemalloc.2.dylib+0x5ec5]  free+0xa4\r\n#\r\n# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try \"ulimit -c unlimited\" before starting Java again\r\n#\r\n# If you would like to submit a bug report, please visit:\r\n#   http://bugreport.java.com/bugreport/crash.jsp\r\n#\r\n\r\n---------------  T H R E A D  ---------------\r\n\r\nCurrent thread is native thread\r\n\r\nsiginfo: si_signo: 11 (SIGSEGV), si_code: 1 (SEGV_MAPERR), si_addr: 0x00000000000fa038\r\n\r\nRegisters:\r\nRAX=0x00000000000fa038, RBX=0x00000000ffffffdc, RCX=0x0000000000000008, RDX=0xffffffffffffffe8\r\nRSP=0x000070000c5b6bd0, RBP=0x000070000c5b6c30, RSI=0x0000000127d53028, RDI=0x0000000127d531c0\r\nR8 =0x0000000000000001, R9 =0x0000000000000000, R10=0x0000000000000000, R11=0x0000000127d531a8\r\nR12=0x0000000127d53008, R13=0x00007f9000000000, R14=0x0000000000000000, R15=0x0000000127d53028\r\nRIP=0x00000001274f7ec5, EFLAGS=0x0000000000010202, ERR=0x0000000000000004\r\n  TRAPNO=0x000000000000000e\r\n\r\nTop of Stack: (sp=0x000070000c5b6bd0)\r\n0x000070000c5b6bd0:   0000000127f2a000 0000000127d53028\r\n0x000070000c5b6be0:   00007f901f41c2b0 00007f901f706dd0\r\n0x000070000c5b6bf0:   0000000127f2a000 00007f901f706e00\r\n0x000070000c5b6c00:   00007f901f407940 0000000127d53028\r\n0x000070000c5b6c10:   00007f901f407948 00007fffc59814b0\r\n0x000070000c5b6c20:   00007f901f407940 00007fffc597a558\r\n0x000070000c5b6c30:   000070000c5b6dd0 0000000127526d50\r\n0x000070000c5b6c40:   000070000c5b6de0 0000000000000005\r\n0x000070000c5b6c50:   000070000c5b6ce0 00007fffc02a54fc\r\n0x000070000c5b6c60:   0000000127d53028 00007fffc8ef8cd0\r\n0x000070000c5b6c70:   000000010de5b000 00007f901f4fc080\r\n0x000070000c5b6c80:   00007f901f410e40 00007f901f410e90\r\n0x000070000c5b6c90:   0000000000000005 00000002c02a54fc\r\n0x000070000c5b6ca0:   0000000000000050 000000010de5fe00\r\n0x000070000c5b6cb0:   00007f901f400000 0000000000000002\r\n0x000070000c5b6cc0:   000000010de5fe00 0000000000000002\r\n0x000070000c5b6cd0:   000000010de5b000 00007f901f400000\r\n0x000070000c5b6ce0:   000070000c5b6d50 00007fffc02a60f3\r\n0x000070000c5b6cf0:   000000012773d510 00007f901f410e20\r\n0x000070000c5b6d00:   000000010de5b000 00007fffc028ff42\r\n0x000070000c5b6d10:   000000010de5ea00 00007f901f410e40\r\n0x000070000c5b6d20:   00007f901f400005 00007f901f410e20\r\n0x000070000c5b6d30:   00007f901f407948 00007fffc59814b0\r\n0x000070000c5b6d40:   0000000000000a00 00007f901f407940\r\n0x000070000c5b6d50:   00007fffc8e46e80 00007fffc8e46e88\r\n0x000070000c5b6d60:   000070000c5b6db0 00007fffbf8223d1\r\n0x000070000c5b6d70:   00007fffc8e46e80 ffff806fe0bf86c0\r\n0x000070000c5b6d80:   00007f901f71d500 00007f901f407940\r\n0x000070000c5b6d90:   00007f901f407948 00007fffc59814b0\r\n0x000070000c5b6da0:   00007fffc597bc01 00007fffc597a558\r\n0x000070000c5b6db0:   000070000c5b6dd0 00007fffbf8220f9\r\n0x000070000c5b6dc0:   00007f901f407940 00007fffc597bc30 \r\n\r\nInstructions: (pc=0x00000001274f7ec5)\r\n0x00000001274f7ea5:   e8 0f 85 f9 01 00 00 48 89 c8 48 c1 e8 09 25 f8\r\n0x00000001274f7eb5:   ff 1f 00 4b 03 44 34 28 49 8d bc 24 b8 01 00 00\r\n0x00000001274f7ec5:   48 8b 08 48 c1 e9 30 48 8b 00 48 8d 15 6a 32 03\r\n0x00000001274f7ed5:   00 48 8b 14 ca 49 01 54 24 10 a8 01 0f 84 6c 01 \r\n\r\nRegister to memory mapping:\r\n\r\nRAX=0x00000000000fa038 is an unknown value\r\nRBX=0x00000000ffffffdc is an unknown value\r\nRCX=0x0000000000000008 is an unknown value\r\nRDX=0xffffffffffffffe8 is an unknown value\r\nRSP=0x000070000c5b6bd0 is an unknown value\r\nRBP=0x000070000c5b6c30 is an unknown value\r\nRSI=0x0000000127d53028 is an unknown value\r\nRDI=0x0000000127d531c0 is an unknown value\r\nR8 =0x0000000000000001 is an unknown value\r\nR9 =0x0000000000000000 is an unknown value\r\nR10=0x0000000000000000 is an unknown value\r\nR11=0x0000000127d531a8 is an unknown value\r\nR12=0x0000000127d53008 is an unknown value\r\nR13=0x00007f9000000000 is an unknown value\r\nR14=0x0000000000000000 is an unknown value\r\nR15=0x0000000127d53028 is an unknown value\r\n\r\n\r\nStack: [0x000070000c4b7000,0x000070000c5b7000],  sp=0x000070000c5b6bd0,  free space=1022k\r\nNative frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)\r\nC  [libjemalloc.2.dylib+0x5ec5]  free+0xa4\r\nC  [libjemalloc.2.dylib+0x34d50]  zone_free+0xa8\r\nC  [CoreFoundation+0x155795]  _CFRelease+0x445\r\nC  [SystemConfiguration+0x729e]  __SCThreadSpecificDataFinalize+0x3c\r\nC  [libsystem_pthread.dylib+0x54c5]  _pthread_tsd_cleanup+0x1d6\r\nC  [libsystem_pthread.dylib+0x5249]  _pthread_exit+0x98\r\nC  [libsystem_pthread.dylib+0x3ab6]  pthread_sigmask+0x0\r\nC  [libsystem_pthread.dylib+0x39f7]  _pthread_body+0x0\r\nC  [libsystem_pthread.dylib+0x3221]  thread_start+0xd\r\n\r\n\r\n---------------  P R O C E S S  ---------------\r\n\r\nVM state:at safepoint (shutting down)\r\n\r\nVM Mutex/Monitor currently owned by a thread:  ([mutex/lock_event])\r\n[0x00007f901f704470] Threads_lock - owner thread: 0x00007f9021028000\r\n\r\nHeap:\r\n PSYoungGen      total 38400K, used 6037K [0x0000000795580000, 0x0000000798000000, 0x00000007c0000000)\r\n  eden space 33280K, 18% used [0x0000000795580000,0x0000000795b65660,0x0000000797600000)\r\n  from space 5120K, 0% used [0x0000000797b00000,0x0000000797b00000,0x0000000798000000)\r\n  to   space 5120K, 0% used [0x0000000797600000,0x0000000797600000,0x0000000797b00000)\r\n ParOldGen       total 87552K, used 0K [0x0000000740000000, 0x0000000745580000, 0x0000000795580000)\r\n  object space 87552K, 0% used [0x0000000740000000,0x0000000740000000,0x0000000745580000)\r\n Metaspace       used 4810K, capacity 5002K, committed 5248K, reserved 1056768K\r\n  class space    used 539K, capacity 562K, committed 640K, reserved 1048576K\r\n\r\nCard table byte_map: [0x000000010fb63000,0x000000010ff64000] byte_map_base: 0x000000010c163000\r\n\r\nMarking Bits: (ParMarkBitMap*) 0x000000010f0f1ea0\r\n Begin Bits: [0x0000000120579000, 0x0000000122579000)\r\n End Bits:   [0x0000000122579000, 0x0000000124579000)\r\n\r\nPolling page: 0x000000010df13000\r\n\r\nCodeCache: size=245760Kb used=1571Kb max_used=1571Kb free=244188Kb\r\n bounds [0x0000000111579000, 0x00000001117e9000, 0x0000000120579000]\r\n total_blobs=527 nmethods=213 adapters=228\r\n compilation: enabled\r\n\r\nCompilation events (10 events):\r\nEvent: 0.486 Thread 0x00007f90210cc800  208       3       java.nio.Buffer::checkIndex (22 bytes)\r\nEvent: 0.486 Thread 0x00007f90210cc800 nmethod 208 0x0000000111701210 code [0x0000000111701380, 0x00000001117015c8]\r\nEvent: 0.486 Thread 0x00007f90210cc800  209       3       java.nio.DirectLongBufferU::ix (10 bytes)\r\nEvent: 0.487 Thread 0x00007f90210cc800 nmethod 209 0x00000001116fc950 code [0x00000001116fcaa0, 0x00000001116fcc10]\r\nEvent: 0.487 Thread 0x00007f90210cc800  211       1       sun.misc.URLClassPath$Loader::getBaseURL (5 bytes)\r\nEvent: 0.487 Thread 0x00007f90210cc800 nmethod 211 0x0000000111701690 code [0x00000001117017e0, 0x00000001117018f0]\r\nEvent: 0.492 Thread 0x00007f90210cc800  212       3       java.lang.String::<init> (10 bytes)\r\nEvent: 0.492 Thread 0x00007f90210cc800 nmethod 212 0x0000000111701950 code [0x0000000111701ac0, 0x0000000111701c90]\r\nEvent: 0.493 Thread 0x00007f90210cc800  213       1       java.net.URL::getUserInfo (5 bytes)\r\nEvent: 0.493 Thread 0x00007f90210cc800 nmethod 213 0x0000000111701d50 code [0x0000000111701ea0, 0x0000000111701fb0]\r\n\r\nGC Heap History (0 events):\r\nNo events\r\n\r\nDeoptimization events (0 events):\r\nNo events\r\n\r\nInternal exceptions (6 events):\r\nEvent: 0.050 Thread 0x00007f9021008000 Exception <a 'java/lang/NoSuchMethodError': Method sun.misc.Unsafe.defineClass(Ljava/lang/String;[BII)Ljava/lang/Class; name or signature does not match> (0x0000000795587ca8) thrown at [/Users/java_re/workspace/8-2-build-macosx-x86_64/jdk8u131/8869/hotspo\r\nEvent: 0.050 Thread 0x00007f9021008000 Exception <a 'java/lang/NoSuchMethodError': Method sun.misc.Unsafe.prefetchRead(Ljava/lang/Object;J)V name or signature does not match> (0x0000000795587f90) thrown at [/Users/java_re/workspace/8-2-build-macosx-x86_64/jdk8u131/8869/hotspot/src/share/vm/pri\r\nEvent: 0.294 Thread 0x00007f9021008000 Exception <a 'java/security/PrivilegedActionException'> (0x0000000795910c50) thrown at [/Users/java_re/workspace/8-2-build-macosx-x86_64/jdk8u131/8869/hotspot/src/share/vm/prims/jvm.cpp, line 1390]\r\nEvent: 0.294 Thread 0x00007f9021008000 Exception <a 'java/security/PrivilegedActionException'> (0x0000000795911048) thrown at [/Users/java_re/workspace/8-2-build-macosx-x86_64/jdk8u131/8869/hotspot/src/share/vm/prims/jvm.cpp, line 1390]\r\nEvent: 0.294 Thread 0x00007f9021008000 Exception <a 'java/security/PrivilegedActionException'> (0x0000000795914748) thrown at [/Users/java_re/workspace/8-2-build-macosx-x86_64/jdk8u131/8869/hotspot/src/share/vm/prims/jvm.cpp, line 1390]\r\nEvent: 0.294 Thread 0x00007f9021008000 Exception <a 'java/security/PrivilegedActionException'> (0x0000000795914b40) thrown at [/Users/java_re/workspace/8-2-build-macosx-x86_64/jdk8u131/8869/hotspot/src/share/vm/prims/jvm.cpp, line 1390]\r\n\r\nEvents (10 events):\r\nEvent: 0.485 loading class org/rocksdb/MutableColumnFamilyOptionsInterface done\r\nEvent: 0.487 loading class org/rocksdb/AdvancedMutableColumnFamilyOptionsInterface\r\nEvent: 0.487 loading class org/rocksdb/AdvancedMutableColumnFamilyOptionsInterface done\r\nEvent: 0.492 loading class org/rocksdb/Env\r\nEvent: 0.492 loading class org/rocksdb/Env done\r\nEvent: 0.492 loading class org/rocksdb/RocksEnv\r\nEvent: 0.492 loading class org/rocksdb/RocksEnv done\r\nEvent: 0.502 Thread 0x00007f9021008000 Thread exited: 0x00007f9021008000\r\nEvent: 0.502 Thread 0x0000000127f2a000 Thread added: 0x0000000127f2a000\r\nEvent: 0.503 Thread 0x0000000127f2a000 Thread exited: 0x0000000127f2a000\r\n\r\n\r\nDynamic libraries:\r\n0x0000000035b70000 \t/System/Library/Frameworks/Cocoa.framework/Versions/A/Cocoa\r\n0x0000000035b70000 \t/System/Library/Frameworks/Security.framework/Versions/A/Security\r\n0x0000000035b70000 \t/System/Library/Frameworks/ApplicationServices.framework/Versions/A/ApplicationServices\r\n0x0000000035b70000 \t/usr/lib/libz.1.dylib\r\n0x0000000035b70000 \t/usr/lib/libSystem.B.dylib\r\n0x0000000035b70000 \t/usr/lib/libobjc.A.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation\r\n0x0000000035b70000 \t/System/Library/Frameworks/Foundation.framework/Versions/C/Foundation\r\n0x0000000035b70000 \t/System/Library/Frameworks/AppKit.framework/Versions/C/AppKit\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreData.framework/Versions/A/CoreData\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/RemoteViewServices.framework/Versions/A/RemoteViewServices\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/UIFoundation.framework/Versions/A/UIFoundation\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/DFRFoundation.framework/Versions/A/DFRFoundation\r\n0x0000000035b70000 \t/usr/lib/libenergytrace.dylib\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/SkyLight.framework/Versions/A/SkyLight\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreGraphics.framework/Versions/A/CoreGraphics\r\n0x0000000035b70000 \t/usr/lib/libScreenReader.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/Accelerate.framework/Versions/A/Accelerate\r\n0x0000000035b70000 \t/System/Library/Frameworks/IOSurface.framework/Versions/A/IOSurface\r\n0x0000000035b70000 \t/System/Library/Frameworks/AudioToolbox.framework/Versions/A/AudioToolbox\r\n0x0000000035b70000 \t/System/Library/Frameworks/AudioUnit.framework/Versions/A/AudioUnit\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/DataDetectorsCore.framework/Versions/A/DataDetectorsCore\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/DesktopServicesPriv.framework/Versions/A/DesktopServicesPriv\r\n0x0000000035b70000 \t/System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/HIToolbox.framework/Versions/A/HIToolbox\r\n0x0000000035b70000 \t/usr/lib/libicucore.A.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/QuartzCore.framework/Versions/A/QuartzCore\r\n0x0000000035b70000 \t/System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/SpeechRecognition.framework/Versions/A/SpeechRecognition\r\n0x0000000035b70000 \t/usr/lib/libauto.dylib\r\n0x0000000035b70000 \t/usr/lib/libxml2.2.dylib\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/CoreUI.framework/Versions/A/CoreUI\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreAudio.framework/Versions/A/CoreAudio\r\n0x0000000035b70000 \t/System/Library/Frameworks/DiskArbitration.framework/Versions/A/DiskArbitration\r\n0x0000000035b70000 \t/usr/lib/liblangid.dylib\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/MultitouchSupport.framework/Versions/A/MultitouchSupport\r\n0x0000000035b70000 \t/System/Library/Frameworks/IOKit.framework/Versions/A/IOKit\r\n0x0000000035b70000 \t/usr/lib/libDiagnosticMessagesClient.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreServices.framework/Versions/A/CoreServices\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/PerformanceAnalysis.framework/Versions/A/PerformanceAnalysis\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/GenerationalStorage.framework/Versions/A/GenerationalStorage\r\n0x0000000035b70000 \t/System/Library/Frameworks/OpenGL.framework/Versions/A/OpenGL\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreImage.framework/Versions/A/CoreImage\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreText.framework/Versions/A/CoreText\r\n0x0000000035b70000 \t/System/Library/Frameworks/ImageIO.framework/Versions/A/ImageIO\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/Backup.framework/Versions/A/Backup\r\n0x0000000035b70000 \t/usr/lib/libarchive.2.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/CFNetwork.framework/Versions/A/CFNetwork\r\n0x0000000035b70000 \t/System/Library/Frameworks/SystemConfiguration.framework/Versions/A/SystemConfiguration\r\n0x0000000035b70000 \t/usr/lib/libCRFSuite.dylib\r\n0x0000000035b70000 \t/usr/lib/libc++.1.dylib\r\n0x0000000035b70000 \t/usr/lib/libc++abi.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libcache.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libcommonCrypto.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libcompiler_rt.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libcopyfile.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libcorecrypto.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libdispatch.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libdyld.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libkeymgr.dylib\r\n0x0000000035b70000 \t/usr/lib/system/liblaunch.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libmacho.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libquarantine.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libremovefile.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_asl.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_blocks.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_c.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_configuration.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_coreservices.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_coretls.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_dnssd.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_info.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_kernel.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_m.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_malloc.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_network.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_networkextension.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_notify.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_platform.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_pthread.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_sandbox.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_secinit.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_symptoms.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libsystem_trace.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libunwind.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libxpc.dylib\r\n0x0000000035b70000 \t/usr/lib/libbsm.0.dylib\r\n0x0000000035b70000 \t/usr/lib/system/libkxld.dylib\r\n0x0000000035b70000 \t/usr/lib/libcoretls.dylib\r\n0x0000000035b70000 \t/usr/lib/libcoretls_cfhelpers.dylib\r\n0x0000000035b70000 \t/usr/lib/libxar.1.dylib\r\n0x0000000035b70000 \t/usr/lib/libsqlite3.dylib\r\n0x0000000035b70000 \t/usr/lib/libpam.2.dylib\r\n0x0000000035b70000 \t/usr/lib/libOpenScriptingUtil.dylib\r\n0x0000000035b70000 \t/usr/lib/libbz2.1.0.dylib\r\n0x0000000035b70000 \t/usr/lib/liblzma.5.dylib\r\n0x0000000035b70000 \t/usr/lib/libnetwork.dylib\r\n0x0000000035b70000 \t/usr/lib/libpcap.A.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/FSEvents.framework/Versions/A/FSEvents\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/CarbonCore.framework/Versions/A/CarbonCore\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/Metadata.framework/Versions/A/Metadata\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/OSServices.framework/Versions/A/OSServices\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/SearchKit.framework/Versions/A/SearchKit\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/AE.framework/Versions/A/AE\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/LaunchServices.framework/Versions/A/LaunchServices\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/DictionaryServices.framework/Versions/A/DictionaryServices\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/SharedFileList.framework/Versions/A/SharedFileList\r\n0x0000000035b70000 \t/System/Library/Frameworks/NetFS.framework/Versions/A/NetFS\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/NetAuth.framework/Versions/A/NetAuth\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/login.framework/Versions/A/Frameworks/loginsupport.framework/Versions/A/loginsupport\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/TCC.framework/Versions/A/TCC\r\n0x0000000035b70000 \t/usr/lib/libmecabra.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/ATS\r\n0x0000000035b70000 \t/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ColorSync.framework/Versions/A/ColorSync\r\n0x0000000035b70000 \t/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/HIServices.framework/Versions/A/HIServices\r\n0x0000000035b70000 \t/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/LangAnalysis.framework/Versions/A/LangAnalysis\r\n0x0000000035b70000 \t/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/PrintCore.framework/Versions/A/PrintCore\r\n0x0000000035b70000 \t/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/QD.framework/Versions/A/QD\r\n0x0000000035b70000 \t/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/SpeechSynthesis.framework/Versions/A/SpeechSynthesis\r\n0x0000000035b70000 \t/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vImage.framework/Versions/A/vImage\r\n0x0000000035b70000 \t/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/vecLib\r\n0x0000000035b70000 \t/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvDSP.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBNNS.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libQuadrature.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLinearAlgebra.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libSparseBLAS.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/Resources/libFontParser.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/Resources/libFontRegistry.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libJPEG.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libTIFF.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libPng.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libGIF.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libJP2.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libRadiance.dylib\r\n0x0000000035b70000 \t/usr/lib/libcompression.dylib\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/AppleJPEG.framework/Versions/A/AppleJPEG\r\n0x0000000035b70000 \t/usr/lib/libcups.2.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/Kerberos.framework/Versions/A/Kerberos\r\n0x0000000035b70000 \t/System/Library/Frameworks/GSS.framework/Versions/A/GSS\r\n0x0000000035b70000 \t/usr/lib/libresolv.9.dylib\r\n0x0000000035b70000 \t/usr/lib/libiconv.2.dylib\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/Heimdal.framework/Versions/A/Heimdal\r\n0x0000000035b70000 \t/usr/lib/libheimdal-asn1.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/OpenDirectory.framework/Versions/A/OpenDirectory\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/CommonAuth.framework/Versions/A/CommonAuth\r\n0x0000000035b70000 \t/System/Library/Frameworks/OpenDirectory.framework/Versions/A/Frameworks/CFOpenDirectory.framework/Versions/A/CFOpenDirectory\r\n0x0000000035b70000 \t/System/Library/Frameworks/SecurityFoundation.framework/Versions/A/SecurityFoundation\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/LanguageModeling.framework/Versions/A/LanguageModeling\r\n0x0000000035b70000 \t/usr/lib/libmarisa.dylib\r\n0x0000000035b70000 \t/usr/lib/libChineseTokenizer.dylib\r\n0x0000000035b70000 \t/usr/lib/libcmph.dylib\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/CoreEmoji.framework/Versions/A/CoreEmoji\r\n0x0000000035b70000 \t/System/Library/Frameworks/ServiceManagement.framework/Versions/A/ServiceManagement\r\n0x0000000035b70000 \t/usr/lib/libxslt.1.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Ink.framework/Versions/A/Ink\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/TextureIO.framework/Versions/A/TextureIO\r\n0x0000000035b70000 \t/System/Library/Frameworks/Metal.framework/Versions/A/Metal\r\n0x0000000035b70000 \t/usr/lib/libate.dylib\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/GPUCompiler.framework/libmetal_timestamp.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libCoreFSCache.dylib\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/IOAccelerator.framework/Versions/A/IOAccelerator\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreVideo.framework/Versions/A/CoreVideo\r\n0x0000000035b70000 \t/usr/lib/libFosl_dynamic.dylib\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/MetalPerformanceShaders.framework/Versions/A/MetalPerformanceShaders\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/FaceCore.framework/Versions/A/FaceCore\r\n0x0000000035b70000 \t/System/Library/Frameworks/OpenCL.framework/Versions/A/OpenCL\r\n0x0000000035b70000 \t/System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGLU.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGFXShared.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGL.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGLImage.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libCVMSPluginSupport.dylib\r\n0x0000000035b70000 \t/System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libCoreVMClient.dylib\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/CrashReporterSupport.framework/Versions/A/CrashReporterSupport\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/Sharing.framework/Versions/A/Sharing\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/IconServices.framework/Versions/A/IconServices\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/ProtocolBuffer.framework/Versions/A/ProtocolBuffer\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/Apple80211.framework/Versions/A/Apple80211\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreWLAN.framework/Versions/A/CoreWLAN\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/CoreUtils.framework/Versions/A/CoreUtils\r\n0x0000000035b70000 \t/System/Library/Frameworks/IOBluetooth.framework/Versions/A/IOBluetooth\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/CoreWiFi.framework/Versions/A/CoreWiFi\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreBluetooth.framework/Versions/A/CoreBluetooth\r\n0x0000000035b70000 \t/System/Library/Frameworks/CoreDisplay.framework/Versions/A/CoreDisplay\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/IOPresentment.framework/Versions/A/IOPresentment\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/DebugSymbols.framework/Versions/A/DebugSymbols\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/CoreSymbolication.framework/Versions/A/CoreSymbolication\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/Symbolication.framework/Versions/A/Symbolication\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/AppleFSCompression.framework/Versions/A/AppleFSCompression\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/SpeechRecognitionCore.framework/Versions/A/SpeechRecognitionCore\r\n0x0000000035b70000 \t/System/Library/PrivateFrameworks/ChunkingLibrary.framework/Versions/A/ChunkingLibrary\r\n0x000000010de8c000 \t/System/Library/CoreServices/Encodings/libSimplifiedChineseConverter.dylib\r\n0x000000010e800000 \t/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server/libjvm.dylib\r\n0x0000000035b70000 \t/usr/lib/libstdc++.6.dylib\r\n0x000000010ded0000 \t/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/libverify.dylib\r\n0x000000010dede000 \t/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/libjava.dylib\r\n0x000000010df14000 \t/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/libinstrument.dylib\r\n0x000000010df72000 \t/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/libzip.dylib\r\n0x00000001250f9000 \t/System/Library/Frameworks/JavaVM.framework/Frameworks/JavaRuntimeSupport.framework/JavaRuntimeSupport\r\n0x0000000125113000 \t/System/Library/Frameworks/JavaVM.framework/Versions/A/Frameworks/JavaNativeFoundation.framework/Versions/A/JavaNativeFoundation\r\n0x00000001114db000 \t/System/Library/Frameworks/JavaVM.framework/Versions/A/JavaVM\r\n0x0000000035b70000 \t/System/Library/Frameworks/Carbon.framework/Versions/A/Carbon\r\n0x0000000125128000 \t/System/Library/PrivateFrameworks/JavaLaunching.framework/Versions/A/JavaLaunching\r\n0x0000000035b70000 \t/System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/CommonPanels.framework/Versions/A/CommonPanels\r\n0x0000000035b70000 \t/System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Help.framework/Versions/A/Help\r\n0x0000000035b70000 \t/System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/ImageCapture.framework/Versions/A/ImageCapture\r\n0x0000000035b70000 \t/System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/OpenScripting.framework/Versions/A/OpenScripting\r\n0x0000000035b70000 \t/System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Print.framework/Versions/A/Print\r\n0x0000000035b70000 \t/System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/SecurityHI.framework/Versions/A/SecurityHI\r\n0x0000000126b8a000 \t/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/libnet.dylib\r\n0x0000000126c14000 \t/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/libnio.dylib\r\n0x0000000126c24000 \t/private/var/folders/g1/cg_lhjrs7dj2m1gxpj6nm45w0000gn/T/librocksdbjni3687672119095060507.jnilib\r\n0x00000001274d7000 \t/usr/local/opt/snappy/lib/libsnappy.1.dylib\r\n0x00000001274e0000 \t/usr/local/opt/lz4/lib/liblz4.1.dylib\r\n0x00000001274f2000 \t/usr/local/opt/jemalloc/lib/libjemalloc.2.dylib\r\n\r\nVM Arguments:\r\njvm_args: -javaagent:/Applications/IntelliJ IDEA CE.app/Contents/lib/idea_rt.jar=49404:/Applications/IntelliJ IDEA CE.app/Contents/bin -Dfile.encoding=UTF-8 \r\njava_command: RocksDBSimple /tmp/simple_zqw\r\njava_class_path (initial): /Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home\r\nLauncher Type: SUN_STANDARD\r\n\r\nEnvironment Variables:\r\nPATH=/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin\r\nSHELL=/bin/bash\r\n\r\nSignal Handlers:\r\nSIGSEGV: [libjvm.dylib+0x5b1eb1], sa_mask[0]=11111111011111110111111111111111, sa_flags=SA_ONSTACK|SA_RESTART|SA_SIGINFO\r\nSIGBUS: [libjvm.dylib+0x5b1eb1], sa_mask[0]=11111111011111110111111111111111, sa_flags=SA_RESTART|SA_SIGINFO\r\nSIGFPE: [libjvm.dylib+0x488988], sa_mask[0]=11111111011111110111111111111111, sa_flags=SA_RESTART|SA_SIGINFO\r\nSIGPIPE: [libjvm.dylib+0x488988], sa_mask[0]=11111111011111110111111111111111, sa_flags=SA_RESTART|SA_SIGINFO\r\nSIGXFSZ: [libjvm.dylib+0x488988], sa_mask[0]=11111111011111110111111111111111, sa_flags=SA_RESTART|SA_SIGINFO\r\nSIGILL: [libjvm.dylib+0x488988], sa_mask[0]=11111111011111110111111111111111, sa_flags=SA_RESTART|SA_SIGINFO\r\nSIGUSR1: SIG_DFL, sa_mask[0]=00000000000000000000000000000000, sa_flags=none\r\nSIGUSR2: [libjvm.dylib+0x4884a6], sa_mask[0]=00100000000000000000000000000000, sa_flags=SA_RESTART|SA_SIGINFO\r\nSIGHUP: [libjvm.dylib+0x486a7d], sa_mask[0]=11111111011111110111111111111111, sa_flags=SA_RESTART|SA_SIGINFO\r\nSIGINT: [libjvm.dylib+0x486a7d], sa_mask[0]=11111111011111110111111111111111, sa_flags=SA_RESTART|SA_SIGINFO\r\nSIGTERM: [libjvm.dylib+0x486a7d], sa_mask[0]=11111111011111110111111111111111, sa_flags=SA_RESTART|SA_SIGINFO\r\nSIGQUIT: [libjvm.dylib+0x486a7d], sa_mask[0]=11111111011111110111111111111111, sa_flags=SA_RESTART|SA_SIGINFO\r\n\r\n\r\n---------------  S Y S T E M  ---------------\r\n\r\nOS:Bsduname:Darwin 16.1.0 Darwin Kernel Version 16.1.0: Wed Oct 19 20:31:56 PDT 2016; root:xnu-3789.21.4~4/RELEASE_X86_64 x86_64\r\nrlimit: STACK 8192k, CORE 0k, NPROC 709, NOFILE 10240, AS infinity\r\nload average:2.76 2.25 2.23\r\n\r\nCPU:total 4 (initial active 4) (2 cores per cpu, 2 threads per core) family 6 model 61 stepping 4, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, sse4.1, sse4.2, popcnt, avx, avx2, aes, clmul, erms, 3dnowpref, lzcnt, ht, tsc, tscinvbit, bmi1, bmi2, adx\r\n\r\nMemory: 4k page, physical 8388608k(3394592k free)\r\n\r\n/proc/meminfo:\r\n\r\n\r\nvm_info: Java HotSpot(TM) 64-Bit Server VM (25.131-b11) for bsd-amd64 JRE (1.8.0_131-b11), built on Mar 15 2017 01:32:22 by \"java_re\" with gcc 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2336.11.00)\r\n\r\ntime: Wed Aug 30 07:05:33 2017\r\nelapsed time: 0 seconds (0d 0h 0m 0s)\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2803/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nastarangithub": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/2770", "title": "A performance related question", "body": "Hi,\r\nI'm using RocksDB inside a Java application and the platform is Ubuntu.\r\nThere is only one instance of the DB in whole application.\r\nThe block of code I'm using to initialize the instance is as follows:\r\n\r\n```java\r\nRocksDB.loadLibrary();\r\nBlockBasedTableConfig blockBasedTableConfig = new BlockBasedTableConfig()\r\n                    .setFilter(new BloomFilter(10, false));\r\nOptions options = new Options()\r\n                    .setCreateIfMissing(true)\r\n                    .setLevelCompactionDynamicLevelBytes(true)\r\n                    .setMaxBackgroundCompactions(4)\r\n                    .setMaxBackgroundFlushes(4)\r\n                    .setBytesPerSync(1048576)\r\n                    .setTableFormatConfig(blockBasedTableConfig)\r\n                    .setMergeOperator(new StringAppendOperator())\r\n                    .setBottommostCompressionType(CompressionType.ZSTD_COMPRESSION);\r\n\r\nRocksDB rocksDB = RocksDB.open(options,  \"path-to-the-db\");\r\n```\r\n\r\nThe problem is that when the RocksDB size is more the 6.5G, the read operation is too slow.\r\nI use `get` method to read a value by a key.\r\nThe keys are strings such as \"cardno:6037997126565777\".\r\nPlease notify me if there is a problem in the tuning or reading process.\r\nThank you", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2770/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "merlimat": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/2717", "title": "Assertion failure in GetFilter() when opening db", "body": "I am seeing assertion failures when opening a db that was not gracefully closed. \r\n\r\n```\r\nAssertion failed: (filter->size() > 0), function GetFilter, file table/block_based_table_reader.cc, line 1133.\r\nAbort trap: 6\r\n```\r\n\r\nDb Logs prior to assert failure: \r\n\r\n```\r\n2017/08/07-14:16:55.549142 700000589000 [db/version_set.cc:2882] Recovered from manifest file:data/standalone/bookkeeper0/current/locations/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0\r\n2017/08/07-14:16:55.549153 700000589000 [db/version_set.cc:2890] Column family [default] (ID 0), log number is 0\r\n2017/08/07-14:16:55.549239 700000589000 EVENT_LOG_v1 {\"time_micros\": 1502140615549232, \"job\": 1, \"event\": \"recovery_started\", \"log_files\": [3]}\r\n2017/08/07-14:16:55.549245 700000589000 [db/db_impl_open.cc:484] Recovering log #3 mode 2\r\n```\r\n\r\nStack trace:\r\n\r\n```\r\n thread #6, stop reason = signal SIGSTOP\r\n    frame #0: 0x00007fffd445cd42 libsystem_kernel.dylib`__pthread_kill + 10\r\n    frame #1: 0x00007fffd454a457 libsystem_pthread.dylib`pthread_kill + 90\r\n    frame #2: 0x00007fffd43c2420 libsystem_c.dylib`abort + 129\r\n    frame #3: 0x00007fffd4389893 libsystem_c.dylib`__assert_rtn + 320\r\n    frame #4: 0x000000013da81279 librocksdbjni1259198247081385980.jnilib`rocksdb::BlockBasedTable::GetFilter(rocksdb::BlockHandle const&, bool, bool) const + 1017\r\n    frame #5: 0x000000013da7e202 librocksdbjni1259198247081385980.jnilib`rocksdb::BlockBasedTable::Open(rocksdb::ImmutableCFOptions const&, rocksdb::EnvOptions const&, rocksdb::BlockBasedTableOptions const&, rocksdb::InternalKeyComparator const&, std::__1::unique_ptr<rocksdb::RandomAccessFileReader, std::__1::default_delete<rocksdb::RandomAccessFileReader> >&&, unsigned long long, std::__1::unique_ptr<rocksdb::TableReader, std::__1::default_delete<rocksdb::TableReader> >*, bool, bool, int) + 4338\r\n    frame #6: 0x000000013da7958a librocksdbjni1259198247081385980.jnilib`rocksdb::BlockBasedTableFactory::NewTableReader(rocksdb::TableReaderOptions const&, std::__1::unique_ptr<rocksdb::RandomAccessFileReader, std::__1::default_delete<rocksdb::RandomAccessFileReader> >&&, unsigned long long, std::__1::unique_ptr<rocksdb::TableReader, std::__1::default_delete<rocksdb::TableReader> >*, bool) const + 90\r\n    frame #7: 0x000000013d996912 librocksdbjni1259198247081385980.jnilib`rocksdb::TableCache::GetTableReader(rocksdb::EnvOptions const&, rocksdb::InternalKeyComparator const&, rocksdb::FileDescriptor const&, bool, unsigned long, bool, rocksdb::HistogramImpl*, std::__1::unique_ptr<rocksdb::TableReader, std::__1::default_delete<rocksdb::TableReader> >*, bool, int, bool) + 546\r\n    frame #8: 0x000000013d996dfa librocksdbjni1259198247081385980.jnilib`rocksdb::TableCache::FindTable(rocksdb::EnvOptions const&, rocksdb::InternalKeyComparator const&, rocksdb::FileDescriptor const&, rocksdb::Cache::Handle**, bool, bool, rocksdb::HistogramImpl*, bool, int, bool) + 602\r\n    frame #9: 0x000000013d99716a librocksdbjni1259198247081385980.jnilib`rocksdb::TableCache::NewIterator(rocksdb::ReadOptions const&, rocksdb::EnvOptions const&, rocksdb::InternalKeyComparator const&, rocksdb::FileDescriptor const&, rocksdb::RangeDelAggregator*, rocksdb::TableReader**, rocksdb::HistogramImpl*, bool, rocksdb::Arena*, bool, int) + 394\r\n    frame #10: 0x000000013d85b35d librocksdbjni1259198247081385980.jnilib`rocksdb::BuildTable(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, rocksdb::Env*, rocksdb::ImmutableCFOptions const&, rocksdb::MutableCFOptions const&, rocksdb::EnvOptions const&, rocksdb::TableCache*, rocksdb::InternalIterator*, std::__1::unique_ptr<rocksdb::InternalIterator, std::__1::default_delete<rocksdb::InternalIterator> >, rocksdb::FileMetaData*, rocksdb::InternalKeyComparator const&, std::__1::vector<std::__1::unique_ptr<rocksdb::IntTblPropCollectorFactory, std::__1::default_delete<rocksdb::IntTblPropCollectorFactory> >, std::__1::allocator<std::__1::unique_ptr<rocksdb::IntTblPropCollectorFactory, std::__1::default_delete<rocksdb::IntTblPropCollectorFactory> > > > const*, unsigned int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<unsigned long long, std::__1::allocator<unsigned long long> >, unsigned long long, rocksdb::CompressionType, rocksdb::CompressionOptions const&, bool, rocksdb::InternalStats*, rocksdb::TableFileCreationReason, rocksdb::EventLogger*, int, rocksdb::Env::IOPriority, rocksdb::TableProperties*, int) + 3981\r\n    frame #11: 0x000000013d912099 librocksdbjni1259198247081385980.jnilib`rocksdb::DBImpl::WriteLevel0TableForRecovery(int, rocksdb::ColumnFamilyData*, rocksdb::MemTable*, rocksdb::VersionEdit*) + 1337\r\n    frame #12: 0x000000013d910563 librocksdbjni1259198247081385980.jnilib`rocksdb::DBImpl::RecoverLogFiles(std::__1::vector<unsigned long long, std::__1::allocator<unsigned long long> > const&, unsigned long long*, bool) + 2355\r\n    frame #13: 0x000000013d90f3ca librocksdbjni1259198247081385980.jnilib`rocksdb::DBImpl::Recover(std::__1::vector<rocksdb::ColumnFamilyDescriptor, std::__1::allocator<rocksdb::ColumnFamilyDescriptor> > const&, bool, bool, bool) + 1370\r\n    frame #14: 0x000000013d91371e librocksdbjni1259198247081385980.jnilib`rocksdb::DB::Open(rocksdb::DBOptions const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<rocksdb::ColumnFamilyDescriptor, std::__1::allocator<rocksdb::ColumnFamilyDescriptor> > const&, std::__1::vector<rocksdb::ColumnFamilyHandle*, std::__1::allocator<rocksdb::ColumnFamilyHandle*> >*, rocksdb::DB**) + 2238\r\n    frame #15: 0x000000013d912c27 librocksdbjni1259198247081385980.jnilib`rocksdb::DB::Open(rocksdb::Options const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, rocksdb::DB**) + 1351\r\n    frame #16: 0x000000013d83610c librocksdbjni1259198247081385980.jnilib`___lldb_unnamed_symbol106$$librocksdbjni1259198247081385980.jnilib + 28\r\n    frame #17: 0x000000013d8302a3 librocksdbjni1259198247081385980.jnilib`rocksdb_open_helper(JNIEnv_*, long, _jstring*, std::__1::function<rocksdb::Status (rocksdb::Options const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, rocksdb::DB**)>) + 147\r\n    frame #18: 0x000000013d8303a9 librocksdbjni1259198247081385980.jnilib`Java_org_rocksdb_RocksDB_open__JLjava_lang_String_2 + 73\r\n\r\n```\r\n\r\nDb archive where the problem reproduces: https://www.dropbox.com/sh/kyfotpyqgvbrsce/AADxlJo8OX0ZPWIiqCVfaL_Ia?dl=0&preview=locations.tar.gz\r\n\r\nTried with RocksDB: 5.5.5 and 5.6.1 with RocksDB-JNI from MacOS.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2717/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cld378632668": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/2715", "title": "Problem compiling rocksdb (or problem excuting make comand)", "body": "When excuting make comand in rocksdb directory, it reports:\r\n```\r\n[root@myheart-2 rocksdb]# make  or make all\r\nMakefile:117: Warning: Compiling in debug mode. Don't use the resulting binary in production \uff08\u641c\u7d22\u4e0d\u5230\uff09\r\n  GEN      util/build_version.cc\r\nMakefile:117: Warning: Compiling in debug mode. Don't use the resulting binary in production\r\n  GEN      util/build_version.cc\r\n  CC       cache/clock_cache.o\r\n  CC       cache/lru_cache.o\r\n/tmp/ccZRJ2y7.s: Assembler messages:\r\n/tmp/ccZRJ2y7.s:7334: Error: no such instruction: `shlx %ebx,%r14d,%ebx'\r\nmake: *** [cache/lru_cache.o] Error 1\r\n```\r\nI'm new to c/c++ on linux,  what should I do next?\r\nThanks! \r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2715/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ananclub": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/2707", "title": "build err on freebsd11.1 clang 4.0", "body": "  CC       utilities/column_aware_encoding_util.o\r\nutilities/column_aware_encoding_util.cc:61:23: error: cannot use dynamic_cast with -fno-rtti\r\n  table_reader_.reset(dynamic_cast<BlockBasedTable*>(table_reader.release()));\r\n\r\n\r\n\r\nFreeBSD clang version 4.0.0 (tags/RELEASE_400/final 297347) (based on LLVM 4.0.0)\r\nTarget: x86_64-unknown-freebsd11.1\r\nThread model: posix\r\nInstalledDir: /usr/bin\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2707/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "grooverdan": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/2705", "title": "gcc-8.0 error=class-memaccess compile failure due to memcpy in ./memtable/inlineskiplist.h", "body": "This isn't occurring on the gcc-7-branch however the gcc master/trunk branch is enhancing its strictness\r\n\r\n<pre>\r\n${CXX} --version\r\n+ x86_64-pc-linux-gnu-g++ --version\r\nx86_64-pc-linux-gnu-g++ (GCC) 8.0.0 20170808 (experimental)\r\n\r\nx86_64-pc-linux-gnu-g++ -m64 -O3 -g -I /opt/ibm/java/include/ -mtune=native -std=c++11  -DROCKSDB_USE_RTTI -g -faligned-new -W -Wextra -Wall -Wsign-compare -Wshadow -Wno-unused-parameter -Werror -I. -I./include -std=c++11 -m64 -O3 -g -I /opt/ibm/java/include/ -mtune=native -std=c++11 -DROCKSDB_PLATFORM_POSIX -DROCKSDB_LIB_IO_POSIX -m64 -O3 -g -I /opt/ibm/java/include/ -mtune=native -DOS_LINUX -fno-builtin-memcmp -DROCKSDB_FALLOCATE_PRESENT -DSNAPPY -DGFLAGS=gflags -DZLIB -DBZIP2 -DLZ4 -DZSTD -DNUMA -DTBB -DROCKSDB_MALLOC_USABLE_SIZE -DROCKSDB_PTHREAD_ADAPTIVE_MUTEX -DROCKSDB_BACKTRACE -DROCKSDB_RANGESYNC_PRESENT -DROCKSDB_SCHED_GETCPU_PRESENT -march=native  -DHAVE_SSE42 -DROCKSDB_SUPPORT_THREAD_LOCAL -DROCKSDB_JEMALLOC -DJEMALLOC_NO_DEMANGLE  -isystem ./third-party/gtest-1.7.0/fused-src -DSNAPPY=1 -DLZ4=1 -DZLIB=1 -DJEMALLOC=1 -DZSTD=1 -DNUMA=1 -Woverloaded-virtual -Wnon-virtual-dtor -Wno-missing-field-initializers -c memtable/skiplistrep.cc -o memtable/skiplistrep.o \r\nIn file included from memtable/skiplistrep.cc:6:0:\r\n./memtable/inlineskiplist.h: In instantiation of 'void rocksdb::InlineSkipList<Comparator>::Node::StashHeight(int) [with Comparator = const rocksdb::MemTableRep::KeyComparator&]':\r\n./memtable/inlineskiplist.h:599:3:   required from 'rocksdb::InlineSkipList<Comparator>::Node* rocksdb::InlineSkipList<Comparator>::AllocateNode(size_t, int) [with Comparator = const rocksdb::MemTableRep::KeyComparator&; size_t = long unsigned int]'\r\n./memtable/inlineskiplist.h:561:25:   required from 'rocksdb::InlineSkipList<Comparator>::InlineSkipList(Comparator, rocksdb::Allocator*, int32_t, int32_t) [with Comparator = const rocksdb::MemTableRep::KeyComparator&; int32_t = int]'\r\nmemtable/skiplistrep.cc:28:28:   required from here\r\n./memtable/inlineskiplist.h:281:11: error: 'void* memcpy(void*, const void*, size_t)' writing to an object of type 'struct std::atomic<rocksdb::InlineSkipList<const rocksdb::MemTableRep::KeyComparator&>::Node*>' with no trivial copy-assignment [-Werror=class-memaccess]\r\n     memcpy(&next_[0], &height, sizeof(int));\r\n     ~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./memtable/inlineskiplist.h:47:0,\r\n                 from memtable/skiplistrep.cc:6:\r\n/compiler/include/c++/8.0.0/atomic:352:12: note: 'struct std::atomic<rocksdb::InlineSkipList<const rocksdb::MemTableRep::KeyComparator&>::Node*>' declared here\r\n     struct atomic<_Tp*>\r\n            ^~~~~~~~~~~~\r\ncc1plus: all warnings being treated as errors\r\nMakefile:1731: recipe for target 'memtable/skiplistrep.o' failed\r\nmake: *** [memtable/skiplistrep.o] Error 1\r\n</pre>", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2705/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "scottfines": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/2687", "title": "RockJava assertion error with ColumnFamilyHandle", "body": "Looks like a very simple looping problem can result in an assertion failure:\r\n\r\n```java\r\nFile tmpDir = Files.createTempDir();\r\n\r\ntry (RocksDB rocksDB = RocksDB.open(tmpDir.getAbsolutePath())) {\r\n\tColumnFamilyDescriptor cdf = new ColumnFamilyDescriptor(\"column-0\".getBytes(Charsets.UTF_8),new ColumnFamilyOptions());\r\n\tColumnFamilyHandle handle = rocksDB.createColumnFamily(cdf);\r\n\r\n\tint numKeys = 500_000;\r\n\r\n\tfor (int i = 0; i < numKeys; ++i) {\r\n\t\tbyte[] key = new byte[50];\r\n\t\tbyte[] val = new byte[] { 42 };\r\n\t\trocksDB.put(handle,key, val);\r\n\t}\r\n}\r\n\r\ntmpDir.delete();\r\n```\r\n\r\nwill produce \r\n\r\n```\r\nAssertion failed: (refs_.load(std::memory_order_relaxed) == 0), function ~ColumnFamilyData, file db/column_family.cc, line 425.\r\n```\r\n\r\nThis does not appear to occur in rocks 5.3.6, but starts occurring in 5.4.5 and carries through to 5.6.1 (I didn't try 5.5.x)\r\n\r\nEnvironment:\r\njava version: Java HotSpot(TM) 64-Bit Server VM (build 25.111-b14, mixed mode)\r\nrocksdb version: 5.6.1 AND 5.4.5\r\nOs: Mac OS X Siera\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2687/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nihathrael": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/2622", "title": "RocksDB Java requires Visual C++ Runtime for Visual Studio 2015 to be installed", "body": "Hi everyone,\r\nit seems rocksdb java requires the Visual C++ Runtime for Visual Studio 2015 to be installed on a windows machine in order to run. Otherwise an exception similar to this will be thrown:\r\n\r\n```\r\nException in thread \"main\" java.lang.UnsatisfiedLinkError: C:\\...\\librocksdbjni8946565711160985166.dll: Can't find dependent libraries\r\n        at java.lang.ClassLoader$NativeLibrary.load(Native Method)\r\n        at java.lang.ClassLoader.loadLibrary0(Unknown Source)\r\n        at java.lang.ClassLoader.loadLibrary(Unknown Source)\r\n        at java.lang.Runtime.load0(Unknown Source)\r\n        at java.lang.System.load(Unknown Source)\r\n        at org.rocksdb.NativeLibraryLoader.loadLibraryFromJar(NativeLibraryLoader.java:78)\r\n        at org.rocksdb.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:56)\r\n        at org.rocksdb.RocksDB.loadLibrary(RocksDB.java:64)\r\n        at org.rocksdb.RocksDB.<clinit>(RocksDB.java:35\r\n```\r\n\r\nAs far as I understood the included binaries should be statically linked. \r\n\r\nIs it possible to include the visual studio runtime statically, so that no additional software needs to be installed on an \"empty\" windows server when using rocksdb or is this technically not possible?\r\n\r\nBest,\r\nThomas", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2622/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "EliFinkelshteyn": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/2547", "title": "RocksDB deleted, but locks still exist", "body": "Currently, if I have a connection open to a RocksDB, even if I delete the entire RocksDB on disk, I still can't create a new RocksDB in the same location due to the `No locks available` error. Is this intended behavior, and if so, is there any way to force close all connections to a RocksDB or any other way to force the release of a lock from some connection that wasn't picked up during garbage collection?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2547/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ashkrit": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/2531", "title": "Unable to load librocksdbjni-win64.dll in windows10", "body": "I am using 5.4.5 version or rocksdb on windows 10 https://mvnrepository.com/artifact/org.rocksdb/rocksdbjni/5.4.5\r\n\r\nIt fails with below error:\r\n```\r\nException in thread \"main\" java.lang.UnsatisfiedLinkError:c:\\XXX\\librocksdbjni-win64.dll: A dynamic link library (DLL) initialization routine failed\r\n\tat java.lang.ClassLoader$NativeLibrary.load(Native Method)\r\n\tat java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941)\r\n\tat java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824)\r\n\tat java.lang.Runtime.load0(Runtime.java:809)\r\n```\r\nHas anybody faced similar issue on windows ?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2531/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "soerendd": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/2036", "title": "Windows: Threads created by RocksDB are not impersonated ", "body": "presumptions:\r\n- the thread which opens a rocksdb is impersonating a user - the process user is a different user\r\n- the process user does not have sufficient access rights for the database directory\r\n\r\nrocksdb opens the database with success. Some functions (compacting?) are failing because they try to open files or try to move (rename) files because they run in a background thread. On windows newly created threads are running as the process user (and not as a impersonated user)\r\n\r\nThis is somewhat inconsistent, because there is no error when opening the database and also when inserting values (sometimes inserts fail - because files needs to be created/moved)\r\n\r\nSolution could be that the database instance stores the impersonation (DuplicateTokenEx). Every tasks that starts for that specific database also gets that token and impersonates at the very beginning of execution of the task and reverts after the work is done.\r\n\r\nPS: Thank you all for this wonderful piece of software.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2036/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "oranagra": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/2005", "title": "heavy write load is unstable", "body": "Hi,\r\nI have a problem with heavy write load in which the throughput jumps from about 80,000 writes per seconds, down to some 300 writes per seconds and then back up.\r\nthese peaks repeat themselves about once or twice a second.\r\nlooking at the recent change log i saw `Improve Write Stalling System` (cd7c4143d795ab9a53e6eaeb4ee572b4e258313b) which was suppose to improve that situation, but i don't see that it did in my case.\r\n\r\ntested on EC2 i3.8xlarage with RAID0 of 4 NVMe drives.\r\ni should note that i'm enabling **use_direct_writes** (v5.1.5) so that the writes go the the disk and don't aggregate in the OS page cache (see note below).\r\ni saw this behavior in v4.9 and v5.0.2, too, where i used allow_os_buffer=false.\r\n\r\ni'm attaching a small sample application that reproduces the problem, and the output i got.\r\nit attempts to print the average ops/sec every 250ms.\r\n[simple_example.cc.txt](https://github.com/facebook/rocksdb/files/854896/simple_example.cc.txt)\r\n[output.txt](https://github.com/facebook/rocksdb/files/854897/output.txt)\r\n\r\ni attempted to tweak many of the parameters such as level0_slowdown_writes_trigger etc, but i didn't succeed in solving the problem.\r\nplease tell me if there's some configuration change that i can do to improve, or if you see some problem with rocksdb implementation that can be fixed.\r\n\r\nnote: one of the reasons i use direct writes (or allow_os_buffer=false) is because i'm doing my tests and experiments on a very powerful machine that is currently mostly empty, and i don't want to see false benchmark due to the fact everything goes to the RAM, and then when the real application will use all the RAM of the machine, the performance will drop (when it will really become disk-bound)", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/2005/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "igorcanadi": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/1974", "title": "SuggestCompactRange() should execute compaction even if the data exists only on the bottommost level", "body": "This is related to https://jira.percona.com/browse/PSMDB-127. When we drop an index or a collection in MongoRocks, we mark all the keys as dead through a compaction filter. We don't issue any deletes. Compaction filters will slowly get rid of those dead indexes as compactions on the dead data gets executed.\r\n\r\nHowever, we have a pathological case if a data for a dropped index only exists on the bottom-most level. In that case, compaction for the data will never be executed and the data won't be cleaned up.\r\n\r\nWe do call SuggestCompactRange() on the dropped data, but this also doesn't do anything if data is on the bottommost-level: https://github.com/mongodb-partners/mongo-rocks/blob/master/src/rocks_engine.cpp#L500\r\n\r\nWe should add an option to SuggestCompactRange() that would let us compact the data even if it exists only on the bottommost level.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/1974/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28621278", "body": "What version of g++ are you using? Can you try upgrading to 4.7 or 4.8?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28621278/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28637546", "body": "Tnx @pborreli, this is great! :)\n\nCan you please sign a Contributor License Agreement, see: https://github.com/facebook/rocksdb/blob/master/CONTRIBUTING.md\n\nWe require this from every contributor. Thanks.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28637546/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28640627", "body": "Tnx!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28640627/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28664939", "body": "Working on it ;)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28664939/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28666736", "body": "Sounds reasonable, I submitted a diff: https://reviews.facebook.net/D14133\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28666736/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28682452", "body": "Can you please try compiling on Ubuntu now? I am able to compile and run on Ubuntu 12.04.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28682452/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28728179", "body": "Tnx Mark, addressing this.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28728179/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28728267", "body": "I think we should just require all three libraries (snappy, bzip2 and zlib) to be installed. They are all standard libraries and can be easily installed on all linux platforms.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28728267/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28728345", "body": "Try `cat /proc/sys/kernel/random/uuid`\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28728345/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28730907", "body": "@dhruba done and done.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28730907/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28731040", "body": "Added INSTALL.md: https://github.com/facebook/rocksdb/blob/master/INSTALL.md\n\nI hope this is resolved. Let us know if you are still having issues.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28731040/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28731060", "body": "Done, we're using <atomic> now.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28731060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28731519", "body": "Added requirements on all three compression libraries in https://github.com/facebook/rocksdb/blob/master/INSTALL.md\n\nIt would be good for `build_detect_platform` to work even if only a subset of them is installed, but they are so easy to install it doesn't make much sense to worry about it.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28731519/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28757131", "body": "COMPILATION.md moved to https://github.com/facebook/rocksdb/blob/master/INSTALL.md\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28757131/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28764543", "body": "Tnx Mark, I fixed the problem.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28764543/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28932891", "body": "Which g++ version are you using?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28932891/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29162513", "body": "Tnx @beginnerlan. We need to have more prominent message during make when we can't find those libraries.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29162513/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29162528", "body": "@pragathacts, where is docs/impl.html linked from?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29162528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29254893", "body": "Tnx @Photonios \nI think we want to do -fPIC in any case. I added https://github.com/facebook/rocksdb/commit/793fdd6731a8df7423f2b82e4f7ad35bad5cd7c0\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29254893/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29310419", "body": "Hi @Kangmo, we actually have linked the instructions in the first bullet in INSTALL.md:\n\n  Install gflags. If you're on Ubuntu, here's a nice tutorial: (http://askubuntu.com/questions/312173/installing-gflags-12-04)\n\nOn Ubuntu 12, libgflags-dev is not available, that's why I linked the complete tutorial.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29310419/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29315272", "body": "@Photonios does https://github.com/facebook/rocksdb/commit/fd4eca73e75282cf89b6de7f6f3bb4cf2b0acbc6 fix the problem?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29315272/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29360994", "body": "Looks like we accidentally fixed your problem! :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29360994/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29581089", "body": "Hi @junyoungkim, tnx for the reported issue. Regarding your pull request, in the second line there should be no define() around __GLIBC_PREREQ.\n\nWe have fixed the issue with this commit: https://github.com/facebook/rocksdb/commit/45a2f2d8d30103f74b20ae19cd61be1f775c5ddb -- which is basically your commit, without define() in the second line.\n\nLet me know if this fixed your problem.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29581089/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29651018", "body": "@Kangmo if you rewrote INSTALL.md that would be awesome.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29651018/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29651214", "body": "Tnx for the report @PragathaM , I have removed the link to doc/impl.html from README.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29651214/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29845625", "body": "Can you please run ./db_test and send us the output? Tnx.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29845625/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30103121", "body": "Tnx @bartman . Did you sign the CLA? Please see: https://github.com/facebook/rocksdb/blob/master/CONTRIBUTING.md\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30103121/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30192305", "body": "Tnx @sepeth .\n\nSince at Facebook we don't use shared library, can you please remove it from the default target? Also, have you signed CLA? Please see our CONTRIBUTING.md file.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30192305/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30192604", "body": "You can try running `ldd ./db_test` -- it should tell you where your environment is looking for libraries\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30192604/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30192648", "body": "@sepeth Feel free to change leveldb_\\* in `c.h` to rocksdb_\\* and send us a pull request :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30192648/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30244032", "body": "Tnx @sepeth . Have you signed our CLA?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30244032/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30436337", "body": "Tnx @junyoungkim . I have pushed the portable patch https://github.com/facebook/rocksdb/commit/249e736bc5f978a58dae669154fd1eb3438f964b\n\nLet me know if you have any more problems :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30436337/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31028814", "body": "Does https://github.com/facebook/rocksdb/commit/b26dc9562801d935ceb1f4410fbb709851840c99 fix the problem?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31028814/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31441509", "body": "Already fixed by https://github.com/facebook/rocksdb/pull/48\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31441509/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31441580", "body": "Merged the fix, let me know if you still have issues.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31441580/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31441679", "body": "cc @jamesgpearce \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31441679/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31443497", "body": "@anatol what is your gcc version?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31443497/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31444095", "body": "This looks like the same problem as https://github.com/facebook/rocksdb/issues/39, fixed with https://github.com/facebook/rocksdb/commit/b26dc9562801d935ceb1f4410fbb709851840c99. Please reopen if the problem is not fixed.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31444095/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31444118", "body": "Can you please provide more details?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31444118/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31762295", "body": "Hey @jamesgolick, can you please send us the LOG file of the rocksdb instance in which you observe big outages? Compaction process does not block reads/writes (mutex is held only for a short amount of time) and should not be causing any outages.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31762295/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/32063910", "body": "Tnx @matope . Have you signed CLA? Please refer to: https://github.com/facebook/rocksdb/blob/master/CONTRIBUTING.md\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/32063910/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34361340", "body": "Just had a discussion with @dhruba \n\nFrom reading your options, your level0 size is way too low (Options.max_bytes_for_level_base). It's currently at 10MB. Your memtable size is 128MB. Every time you push your memtable to level0, it's already too big and you need to do a compaction. There can be at most one concurrent compaction at level0, so when one file is compacting, other files are arriving to level0 and are piling up. When there are too many level0 files, we start delaying writes.\n\nHere are the settings that might work better:\nOptions.write_buffer_size = 1GB (memtable size)\nOptions.min_write_buffer_number_to_merge = 2 (as soon as you have 2 memtables, start flushing. no need to keep them in memory since you're not reading)\nOptions.max_write_buffer_number = 10 (you'll probably never get to this number, but better safe than sorry)\nOptions.max_bytes_for_level_base = 5GB (this will make level1 50GB and level2 500GB)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34361340/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34378506", "body": "max_bytes_for_level_base is level0 size, let's say 5GB (total size of all files in level0 that will trigger the compaction). That 5GB from level0 will get compacted and pushed to level1 chunked up into N files of target_file_size_base each. N is 5GB / target_file_size_base.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34378506/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34394085", "body": "Can you turn on statistics (options.statistics) and send the LOG file when you run the DB with statistics enabled? Tnx.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34394085/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34395409", "body": "filter_block - false positive, deletion in line 113\ndb_stats_logger - MaybeScheduleLogDBDeployStats() is always called with mutex held.\n\nI'm fixing the rest\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34395409/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34406422", "body": "https://github.com/facebook/rocksdb/commit/d53b18822834c5596f1bb3f5201dc566dfb5bd30\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34406422/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34406890", "body": "This is interesting, I'm learning more about tuning RocksDB :)\n\nDidn't have time to dig deeper into LOG file, but would you mind trying the experiment with bigger Options.target_file_size_base (let's say 1GB).\n\nAlso, it might be the case that your disk just can't process all the data coming in. In that case, stalls are good, however, we would need to reduce the variance (delay each write by 10ms instead of accepting all the writes and then all of a sudden start delaying writes by couple of minutes). What's your disk/cpu utilization when you see the stalls?\n\nTo get less variance, good options to look at are level0_slowdown_writes_trigger and level0_stop_writes_trigger.\n\nConfiguring RocksDB is not trivial and we're just starting an effort to provide simpler configuration APIs and less confusing options.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34406890/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34411391", "body": "Do you increase number of background threads in Env object? Take a look at Env::SetBackgroundThreads()\n\n@dhruba do you think that https://github.com/facebook/rocksdb/commit/abd70ecc2b7fe02709955ef14905d1628581b306 caused the crc32c function to slow down the compaction process?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34411391/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34489164", "body": "Here's an insight. In the stable state, the level0 compaction with 32 files takes longer than it takes for your writer to generate 32 new level0 files.\n1. Compaction takes 32 files in level0 and starts compaction (no other compactions can happen on level0)\n2. while compaction is happening, writer dumps 32 new memtables (total is 64 now), causing hard stall.\n3. Compaction finishes, clears 32 files from level0, 32 files are left.\n4. Goto 1.\n\nDid you try with bigger target_file_size_base?\n\nThe universal style compactions lowers write amplification and can sustain higher write throughput. It's a good thing to try if your goal is to optimize write throughput.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34489164/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34492020", "body": "Great PR, tnx! I have some comments. Can you also try running `arc lint`?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34492020/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34515267", "body": "cc @liukai on `make format`\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34515267/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34667189", "body": "Fixed with https://github.com/facebook/rocksdb/commit/bc2ff597b8ef10148827c4ca9c72a7343b33e81b\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34667189/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34668499", "body": "(1) is fixed\n(2) should already be fixed in the master branch. can you confirm?\n(3) cc @liukai \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34668499/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35103390", "body": "Also, @alberts, can you please review this PR: https://github.com/facebook/rocksdb/pull/82 (not sure why I can't mention you there)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35103390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35141378", "body": "Tnx. Should be fixed with https://github.com/facebook/rocksdb/commit/be7e273d83b746ecb32f9b51536768f4fc2f19db\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35141378/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35304913", "body": "Try `make shared_lib`\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35304913/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35419174", "body": "Can you please add basic merge operator sanity tests to db/c_test.c (few puts, few gets, nothing major)? Can you also explain reason behind removing `free(filter)` at line 125?\n\nOtherwise looks good, thanks a lot!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35419174/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35421748", "body": "Hey @mlin , are you interested in finishing this or should we take over?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35421748/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35563501", "body": "Somehow if the function returns a pointer the unwritten rule is that the caller is responsible for cleaning the memory. Can we make sure that client allocates the memory in the C space?\n\nAlternative solution could be to add a Delete method that would be default call free(filter), and client can implement its own solution for freeing the memory if he wishes.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35563501/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35655819", "body": "Updated CONTRIBUTING.md to address the issue.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35655819/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35815688", "body": "Hey @matthewvon, feel free to run it on rotational disks and let us know the results!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35815688/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35946929", "body": "Tnx!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35946929/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35952511", "body": "Merge operator test seg faulted. Here's the fix: https://github.com/facebook/rocksdb/commit/2bf1151a25de5c711d0c863973c2d995e1137c60\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35952511/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35958981", "body": "Here's a quick fix: https://github.com/facebook/rocksdb/commit/6ed450a58cd0bf9f299b0e279ce762125b79deea\n\nHowever, this highlights a bigger issue where we deadlock on ENOSPC. We have an internal task for this and hope to clean it up soon.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35958981/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36187599", "body": "@slachowsky, yes, that's what we want. Transparently read older files, but write new files in the new format. I.e. we support backward compatibility, but not forward compatibility.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36187599/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36313206", "body": "@haoboxu this is your code, right?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36313206/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36699085", "body": "Should be fixed by https://github.com/facebook/rocksdb/commit/e3f396f1eaea52fdfb65f7248afd039abc3b275c\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36699085/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36776693", "body": "Tnx! :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36776693/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37053784", "body": "tnx @yumiOS \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37053784/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37261228", "body": "We have a fix coming out, thanks for reporting @brooksbp \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37261228/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37269234", "body": "Fixed with https://github.com/facebook/rocksdb/commit/fac58c05046b6fb253bbdb31a218a5b55350b85e\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37269234/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37333506", "body": "Fixed by https://github.com/facebook/rocksdb/commit/6c66bc08d9fdee1b0a3d1d3e6df557b02fcd0e7c\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37333506/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37670397", "body": "Your compiler is overly concerned :)\n\nFixed by https://github.com/facebook/rocksdb/commit/6c72079d77ffee55ecd15856e90808bfb892980c\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37670397/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37672389", "body": "https://github.com/facebook/rocksdb/commit/f74659ac9f4a934a35e6eb54f14d586747c722a1\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37672389/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37673434", "body": "How about now? https://github.com/facebook/rocksdb/commit/56dce9bf8eade79bda167c2722bc77c879feb9a7\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37673434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37676318", "body": "yay!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37676318/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37689342", "body": "https://github.com/facebook/rocksdb/commit/2bad3cb0db7f510ccba4ef3b29437cec1d12220a\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37689342/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37706272", "body": "The code looks great @slachowsky!\n\nCan you:\n1) Add more unit tests (especially the one where DB with checksum crc32 can open a DB written with checksum xxHash)\n2) merge all the commits together\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37706272/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/4706659", "body": "Yes, the only difference is that we do that check only in non-FB environment - we have other requirements for FB environment.\n\nThanks for your contribution!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4706659/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4967549", "body": "@liukai we also need to add -DROCKSDB_FALLOCATE_PRESENT to our fbcode build\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4967549/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4967903", "body": "https://github.com/facebook/rocksdb/commit/52ea1be90aeac4560a4993cda2c48b4dc084f2ce\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4967903/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5017176", "body": "I think it would be good to merge this diff into master. @liukai thoughts?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5017176/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5017275", "body": "I wasn't thinking about merging this exact diff, but rather doing the same thing (replacing some vectors with autovectors) in master branch. But I have no blocked tasks, so don't worry about it :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5017275/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5103105", "body": "I do `make release` when I do some regression/performance testing. Creating shared library is taking few minutes (?) and I don't really need it. Can we compile shared library outside of `make release`? Maybe `make librocks.so` or `make shared`?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5103105/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5273584", "body": "Can we remove this rename from the performance branch and then do the rename in master (after the performance is pushed)? Rename will make resolving merge conflicts a bit more difficult.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5273584/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5274683", "body": "aaah, great :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5274683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5341963", "body": "yoeref? :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5341963/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/6070787", "body": "Does it make sense to compile tests with -DNDEBUG?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/6070787/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/6070800", "body": "Unless you have a good usecase of compiling tests with NDEBUG, I should probably do something like\n\n```\n#ifdef NDEBUG\n#error Please compile tests without NDEBUG flag \n#endif\n```\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/6070800/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/7156332", "body": "How about now https://github.com/facebook/rocksdb/commit/28b367db15f444c172260b2247b4a782ba387049 \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/7156332/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/7634387", "body": "Yup, it's already asserted in table building\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/7634387/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/9073736", "body": "Adam, just saw your message on IRC. Try running `make clean; make check`. `make check` doesn't work after `make release`, you need to clean the build before.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/9073736/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/13292309", "body": "Tnx for building buildtime Dieter! I found buildtime on an example travis file so decided to try it out. It gives us nice stats :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/13292309/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/13473071", "body": "We're trying to make all `-Wall` and `-Wextra` happy for all the compilers out there: https://github.com/facebook/rocksdb/blob/master/Makefile#L167\n\nCan you please send in a PR to fix this warning?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/13473071/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17475622", "body": "Can you instead use MemEnv, which is already present in both linux and windows and is used for other tests?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17475622/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/22406181", "body": "Hey @lightmark, can you please also update version.h, so that I can guard the change in the public API with the version macro?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/22406181/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/8975285", "body": "This means lots of memcpy and it's all done inside of the mutex. Can you refactor the code such that InputSummary directly takes `char* output` (parameter of Summary) and returns the size (so that you can keep writing) (maybe Summary can also take `char **output` and advance the pointer automatically)? You can do similar thing with FileSizeSummary.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/8975285/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/8975384", "body": "Take a look at DBTest::FilesPerLevel()\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/8975384/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9550978", "body": "Why do you need both defines? Is there a use case where only one will be present and rocksdb should work correctly/partially?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9550978/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9551218", "body": "Can you just move all the delete[] uncompressed here outside of the switch? If uncompressed is null, delete is noop, so you're fine even in failure case.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9551218/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9551418", "body": "Is this what `make format` did? We keep the asterisks by the type, not the variable name, so: `int* decompress_size`.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9551418/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9551558", "body": "The indent here seems off.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9551558/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9551637", "body": "I'm not familiar with LZ4 compression. Does LZ4HC get uncompressed by LZ4_Uncompress()?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9551637/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9551687", "body": "Good catch :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9551687/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9551724", "body": "We have braces even in one-line ifs\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9551724/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9836042", "body": "`size_t n = operand_list.size();`\n:)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9836042/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9836049", "body": "size_t i\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9836049/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9836087", "body": "Is this four space indent? We use two. You might be able to just run `make format` and have it fix all of this for you.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9836087/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9837844", "body": "Looks like this causes valgrind failures in c_test. Run the test with `valgrind --leak-check=full ./c_test`. Let me know if you can't repro.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9837844/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/10631885", "body": "Line is longer than 80 chars. Can you please run `make format`?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/10631885/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/10664575", "body": "Can you document the format of different footer versions?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/10664575/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/12870024", "body": "Can you remove the comments here and in the destructor?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/12870024/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/12870090", "body": "We don't create a new directory on a NewDirectory() call.\n\nHere are the comments from `include/rocksdb/env.h`:\n\n```\n// Create an object that represents a directory. Will fail if directory\n// doesn't exist. If the directory exists, it will open the directory\n// and create a new Directory object.\n```\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/12870090/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/12870125", "body": "Style -- space after coma. Can you run `make format`?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/12870125/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/12870143", "body": "Remove `.c_str()` call\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/12870143/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/12905900", "body": "Can you wrap this in an anonymous namespace?\n\nAlso, RocksDB's style function name: PrintThreadInfo()\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/12905900/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/13955647", "body": "Destroy cfilter?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/13955647/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/14661129", "body": "no need for rocksdb::\n\nalso, why a pointer?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/14661129/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/15903751", "body": "track :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/15903751/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16291645", "body": "This is not actual data always. Please comment that there are two cases:\n1. this is actual data and Slice data is pointing to it\n2. this is nullptr, in which case Slice data points to something else that is guaranteed to be alive while the file is opened\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16291645/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16552541", "body": "How does this work? Isn't `contents` invalid in this line, since it's been moved in the previous one?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16552541/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16552563", "body": "Please move this change to a separate PR https://github.com/facebook/rocksdb/pull/237\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16552563/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16552691", "body": "Why do you still need this? Wasn't original motivation to have std::unique_ptr<> manage the data?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16552691/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16552743", "body": "This is not used anywhere.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16552743/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16556825", "body": "Yes, but why do we have `bool owned_` here? Shouldn't Block only hold BlockContents?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16556825/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16839585", "body": "Why do we still need this constructor?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16839585/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16839739", "body": "Can we also get rid of this bool here? If it was heap_allocated, then just make allocation point to the allocated space. Otherwise, allocation can be nullptr\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16839739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16839794", "body": "We should assign the allocation when the allocation happens, not here. That way we don't need to keep `heap_allocated` around at all.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16839794/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16840524", "body": "I'm not sure that this does the right thing. Would `write_pre_and_post_process_time` measure preprocessing and postprocessing time correctly?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16840524/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16840589", "body": "Maybe call it PERF_TIMER_SCOPE or PERF_TIMER_GUARD? \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16840589/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16898569", "body": "You need to restart this timer here.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16898569/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16898573", "body": "This timer can't be a guard timer. It is timing pre- and post- processing times. With this change, it would also count processing time, right?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16898573/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16932983", "body": "Don't you need to put this in separate scope with file->Read()? We only want to measure Read() time.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16932983/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17126021", "body": "There are bunch of these defines all across the code-base. Do you need to make the changes there, too?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17126021/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17487282", "body": "If you call the non-ownership constructor, cachable() won't get initialized, right?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17487282/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17487287", "body": "Same here\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17487287/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17488108", "body": "Can we not repeat this code? I.e. do the same thing as in Block's constructors\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17488108/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17488540", "body": "convention -- always use {} for code blocks, even if it's one statement.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17488540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17488833", "body": "Do you need this? If status is not OK, client shouldn't use *contents, right?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17488833/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17488876", "body": "You don't need break if you're returning here :)\nHowever, I would prefer that we `break;` and then return Status::OK() at the end.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17488876/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17488976", "body": "convention -- {} around return status\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17488976/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17613880", "body": "Is the problem only in how the unit tests use the Block? Can we rather change the unit tests?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17613880/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/18101991", "body": "how about rocksdb instead of leveldb? :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/18101991/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/18232150", "body": "We put & with the type rather than the name. So this would be: \n\n```\n   std::vector<uint32_t>& keys_hashes\n```\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/18232150/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/19435837", "body": "Why MutableCFOptions twice?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/19435837/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20382288", "body": "Can you please destroy the DB before you destroy the options?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20382288/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20387133", "body": "Actually this should be fine because we copy the Options struct internally.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20387133/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20915021", "body": "Can you put this back in?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20915021/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20940325", "body": "You mean -momit-leaf-frame-pointer here? Can you comment here something like \"ppc64 doesn't support -momit-leaf-frame-pointer\"\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20940325/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20940369", "body": "Why -lgflags here? This should already be included.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20940369/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20940405", "body": "Can you try hooking these up in `build_tools/build_detect_platform`? It seems like the right place to add new flags for platform compatibility.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20940405/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21034862", "body": "Check out `build_tools/build_detect_platform`\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21034862/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21035295", "body": "Ah, got it. However, if you want to indicate to the Makefile that gflags is present, that detection happens in `build_tools/build_detect_platform`: https://github.com/facebook/rocksdb/blob/master/build_tools/build_detect_platform#L207. So you might want to add those additional parameters there, so that gflags gets automatically detected by the script\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21035295/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21809609", "body": "err is not touched if there's no error. So you need to initialize it to zero before the call.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21809609/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21809634", "body": "Do you need to destroy this? Can you try running valgrind with this binary?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21809634/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/22667386", "body": "can you use std::shared_ptr? Also, no need for a space between > > in C++11 :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/22667386/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/22667889", "body": "Hm, do we even need this structure? We can just use file_infos_ since it contains all the data, right?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/22667889/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/23657556", "body": "Can you pls add a newline here?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/23657556/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/24106203", "body": "Would it make sense to move some of these targets to java/Makefile?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/24106203/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/24104719", "body": "Can we also make Env\\* configurable? Clients might want to pass HdfsEnv here, effectively backing up RocksDB to HDFS.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/24104719/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/25445512", "body": "What. FTR, this was introduced by https://reviews.facebook.net/D19047, but I'm not sure why it wasn't fixed.\n\ncc @siying you know more about this code, is this correction correct?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/25445512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/30807138", "body": "some blocks in the table might be compressed, while others might not be (because the compression ratio for those blocks is not worth the trouble). it might be better to add an if around this:\n\n```\nif (found_compression_type != kNoCompression) {\n        new_table_properties->compression_type = found_compression_type; \n```\n\n   }\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/30807138/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/30807244", "body": "Since this is a debug code-path, it might be useful to convert the compression type to string here. currently this will output compression type 0, 1, 2, etc. it'll be better if we actually dump \"no_compression\", \"snappy\", etc...\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/30807244/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/30807587", "body": "maybe we can also say the percentage of blocks that are compressed vs. blocks that aren't (because of that logic): https://github.com/facebook/rocksdb/blob/master/table/block_based_table_builder.cc#L332\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/30807587/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/31647663", "body": "Ah, this fails the commit with \n\n```\n10:26:28 db/c.cc:1087:19: error: variable length array of non-POD element type 'rocksdb::Slice'\n```\n\nI have to revert this, sorry :(\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/31647663/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/31648742", "body": "It only failed on clang, not on g++. Interesting.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/31648742/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/31647160", "body": "let's also test rocksdb_muliti_get_cf() to increase code coverage\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/31647160/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/32161681", "body": "maybe we can also update this to gcc 4.8\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/32161681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28353910", "body": "we usually force push it\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28353910/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28363042", "body": "`arc diff` will automatically force-push I think\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28363042/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/32657873", "body": "can you comment on the fact that user has to free the resulting pointer?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/32657873/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34394658", "body": "I think this will fail valgrind. Can you please use unique_ptr for this?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34394658/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34394719", "body": "We don't usually run our tests in release build.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34394719/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396207", "body": "same here, i think this leaks memory. not sure if valgrind works on windows, but it might be good to run it, if it does.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396207/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396425", "body": "Rule of five requires us to declare either all of {copy, copy-assign, move, move-assign, destructor}, or none of them. See here: http://en.cppreference.com/w/cpp/language/rule_of_three\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396425/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396631", "body": "Feel free to replace all of our Mutex and CondVar classes with C++11 equivalent (even in posix). However, you don't need to do it as part of this patch. We can do it later.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396631/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396699", "body": "I'm curious why do you need a separate implementation of compression functions? I thought they're platform independent.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396699/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396919", "body": "At some point it would be good to contribute this back to MySQL project, since this file was copied from there. Please add a TODO into the COMMIT.md file in this directory.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34396919/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34397178", "body": "You also need to define copy constructor and copy assignment, right?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34397178/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34397263", "body": "Ah, `alloca` gets bytes from stack, got it. This is fine then. db_test perf is not very critical :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34397263/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34397304", "body": "alloca takes from stack, not heap. so this is not an issue.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34397304/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34536787", "body": "This will not work in public header files since public header files are not compiled. We don't pass the defines to our customers.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34536787/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34756728", "body": "Hmm, appending a string and expecting it's going to be null-terminated is a bit risky. How about something like this:\n\n```\n wal_info = wal_info + file + \" size: \" + std::to_string(file_size) + \" ; \";\n```\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34756728/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/37314153", "body": "Are you sure this is the case? Another comment in this same file says: \"// seqno within the same key are in decreasing order\"\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/37314153/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/37602631", "body": "Yeah, that's a good idea. We already parse that key\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/37602631/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/37602661", "body": "Hm, actually, line 131 might not catch all the keys, because MergeUntil calls Next()\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/37602661/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/37653648", "body": "@siying no need to merge this, since I fixed the issue with https://reviews.facebook.net/D42087 (also took the unit test from this PR)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/37653648/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/38953170", "body": "As a start, can we use the same options as for the default column family? Options for default column family can get modified through the command line argument.s\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/38953170/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39091604", "body": "Our code style requires braces around single-line statements, too.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39091604/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/40850763", "body": "Do we really need to include tool sources in our library? This will increase our library size. Can you just add TOOL_SOURCES under amalgamation?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/40850763/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/40854900", "body": "Got it, I missed the change in src.mk, thanks.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/40854900/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "koushikpaul1": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/1955", "title": "Tomcat crashes while trying to close RocksDb", "body": "I am facing the below issue while trying to close rocksdb in java. The tomcat immediately crashes while trying to close rocksdb. I am using rocksdbjni-4.9.0.jar.\r\n\r\n```\r\nA fatal error has been detected by the Java Runtime Environment:\r\n#\r\n#  SIGSEGV (0xb) at pc=0x00007f9c5c827ab2, pid=16470, tid=140309579605760\r\n#\r\n# JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14)\r\n# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode linux-amd64 compressed oops)\r\n# Problematic frame:\r\n# C  [librocksdbjni-linux64.so+0x175ab2]  Java_org_rocksdb_DBOptions_setCreateIfMissing+0x2\r\n#\r\n# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try \"ulimit -c unlimited\" before starting Java again\r\n#\r\n# An error report file with more information is saved as:\r\n# /usr/local/apache-tomcat-8.5.9/webapps/hs_err_pid16470.log\r\n#\r\n# If you would like to submit a bug report, please visit:\r\n#   http://bugreport.java.com/bugreport/crash.jsp\r\n# The crash happened outside the Java Virtual Machine in native code.\r\n# See problematic frame for where to report the bug.\r\n```", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/1955/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "raiusa": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/1715", "title": "rocksdb UnsatisfiedLinkError ", "body": "I got following error when tried to use kafka stream .\r\nI am using `0.10.1.0` version of kafka-client and kafka-stream. I read on internet that this issue has been fixed \r\nwith this version as it uses `rocksdbjni-4.9.0 ja`r. \r\nAny help.\r\n\r\n```\r\nException in thread \"StreamThread-1\" java.lang.UnsatisfiedLinkError: Local\\Temp\\librocksdbjni6935746072594020003.dll: Can't find dependent libraries\r\n```", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/1715/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bluesalt": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/1565", "title": "The behaviour of GetUpdatesSince  is not the same as documented", "body": "Here is the document of the API\r\n\r\n```C++\r\n  // Sets iter to an iterator that is positioned at a write-batch containing\r\n  // seq_number. If the sequence number is non existent, it returns an iterator\r\n  // at the first available seq_no after the requested seq_no\r\n  // Returns Status::OK if iterator is valid\r\n  // Must set WAL_ttl_seconds or WAL_size_limit_MB to large values to\r\n  // use this api, else the WAL files will get\r\n  // cleared aggressively and the iterator might keep getting invalid before\r\n  // an update is read.\r\n  virtual Status GetUpdatesSince(\r\n      SequenceNumber seq_number, unique_ptr<TransactionLogIterator>* iter,\r\n      const TransactionLogIterator::ReadOptions&\r\n          read_options = TransactionLogIterator::ReadOptions()) = 0;\r\n```\r\n\r\nIt is clear that the iterator should be *after* the requested seq_no if the requested seq_no is not existing.  I have a  db which currently holds two sequence number 1, 4.  However, the following programming emits \r\n```\r\n1\r\n4\r\n```\r\nPer the document, only `4` should be emitted.  Do I misunderstand the doc ? \r\n\r\nHere is the program:\r\n\r\n```C++\r\n#include <iostream>\r\n#include \"rocksdb/db.h\"\r\n\r\nusing namespace std;\r\n\r\nint main() {\r\n    rocksdb::DB* db;\r\n    rocksdb::Options options;\r\n    rocksdb::Status status = rocksdb::DB::OpenForReadOnly(options, \"/tmp/kv\", &db);\r\n\r\n    unique_ptr<rocksdb::TransactionLogIterator> iter;\r\n    status = db->GetUpdatesSince(2, &iter);\r\n    rocksdb::WriteBatch wb;\r\n    if (status.ok()) {\r\n        for (; iter->Valid(); iter->Next()) {\r\n            auto batch_result = iter->GetBatch();\r\n\r\n            wb = *batch_result.writeBatchPtr;\r\n            cout << batch_result.sequence << endl;\r\n        }\r\n    }\r\n\r\n    delete db;\r\n\r\n    return 0;\r\n}\r\n```", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/1565/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 1, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "koldat": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/1541", "title": "Rocks JNI does not dispose family handle", "body": "Dropping family is just marking family as dropped internally. But files are deleted after handle is disposed. JNI leaks the pointer when user calls dropFamily.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/1541/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/2283", "title": "JNI direct buffer support for basic operations", "body": "It is very useful to support direct ByteBuffers in Java. It allows to have zero memory copy and some serializers are using that directly so one do not need to create byte[] array for it.\r\n\r\nThis change also contains some fixes for Windows JNI build.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alexreg": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/1473", "title": "Makefile does not install tools", "body": "The tools are built, but `install` does not install them, nor is there a separate `install-tools` target.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/1473/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yijiem": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/1412", "title": "receive SIGSEGV inside VersionSet::LogAndApply call", "body": "Hi team. We are using rocksdb 4.1.0 and we hit a segfault inside VersionSet::LogAndApply when we CreateColumnFamily, here is the function call stack and logs:\n\n**call stack:**\n#0  0x00007ff4fc0e4444 in rocksdb::VersionSet::LogAndApply(rocksdb::ColumnFamilyData*, rocksdb::MutableCFOptions const&, rocksdb::VersionEdit*, rocksdb::InstrumentedMutex*, rock\n\nsdb::Directory*, bool, rocksdb::ColumnFamilyOptions const*) () from /usr/lib/librocksdb.so.4.1\n#1  0x00007ff4fc07a81e in rocksdb::DBImpl::CreateColumnFamily(rocksdb::ColumnFamilyOptions const&, std::string const&, rocksdb::ColumnFamilyHandle**) ()\n   from /usr/lib/librocksdb.so.4.1\n\n**logs:**\n\nOct 20 04:31:29 localhost lrse[2836]: [kvs.INFO] (2836) Created column family [ConsistentSnapshot] (ID 2)\nOct 20 04:31:29 localhost lrse[2836]: [kvs.ERR] (2836) MANIFEST write: IO error: /rfs/lrse/meta/info/meta/MANIFEST-000004: Device or resource busy\nOct 20 04:31:29 localhost lrse[2836]: [kvs.INFO] (2836) ManifestContains: checking /rfs/lrse/meta/info/meta/MANIFEST-000004\nOct 20 04:31:29 localhost lrse[2836]: [kvs.INFO] (2836) ManifestContains: IO error: /rfs/lrse/meta/info/meta/MANIFEST-000004: Software caused connection abort\nOct 20 04:31:29 localhost lrse[2836]: [kvs.INFO] (2836) ManifestContains: is unable to reopen the manifest file  /rfs/lrse/meta/info/meta/MANIFEST-000004\nOct 20 04:31:29 localhost pm[1695]: [pm.ERR]: Output from lrse: /opt/rbt/bin/lrse (pid 2836) received signal 11 (SIGSEGV) dumping core\n\nWe would like to exit gracefully in this case. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/1412/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mritun": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/1395", "title": "db_bench crash with cuckoo table", "body": "Hey fine folks,\n\nThanks for RockDB. I just checked out the master and ran db_bench with cuckoo table format. It soon crashed.\n\n```\ngit hash 21e8daced5a2db9ba55138c60a320676306f2088\nOS: MacOS 10.11.6\nXCode 8\n```\n\n```\n$ ~/repos/rocksdb % ./db_bench -db tmp-rocks-db -benchmarks=fillsync -use_cuckoo_table\nInitializing RocksDB Options from the specified file\nInitializing RocksDB Options from command-line flags\nRocksDB:    version 4.12\nKeys:       16 bytes each\nValues:     100 bytes each (50 bytes after compression)\nEntries:    1000000\nPrefix:    0 bytes\nKeys per prefix:    0\nRawSize:    110.6 MB (estimated)\nFileSize:   62.9 MB (estimated)\nWrite rate: 0 bytes/second\nCompression: Snappy\nMemtablerep: skip_list\nPerf Level: 1\nWARNING: Assertions are enabled; benchmarks unnecessarily slow\n------------------------------------------------\nInitializing RocksDB Options from the specified file\nInitializing RocksDB Options from command-line flags\nDB path: [tmp-rocks-db]\nput error: IO error: tmp-rocks-db/000006.sst: Bad address\nlibc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: mutex lock failed: Invalid argument\nReceived signal 6 (Abort trap: 6)\n#0   Invalid connection: com.apple.coresymbolicationd   0x00102440 (in db_bench)\n#1   Invalid connection: com.apple.coresymbolicationd   abort (in libsystem_c.dylib) + 129\n#2   Invalid connection: com.apple.coresymbolicationd   __cxa_bad_cast (in libc++abi.dylib) + 0\n#3   Invalid connection: com.apple.coresymbolicationd   default_terminate_handler() (in libc++abi.dylib) + 243\n#4   Invalid connection: com.apple.coresymbolicationd   _objc_terminate() (in libobjc.A.dylib) + 124\n#5   Invalid connection: com.apple.coresymbolicationd   std::__terminate(void (*)()) (in libc++abi.dylib) + 8\n#6   Invalid connection: com.apple.coresymbolicationd   __cxxabiv1::exception_cleanup_func(_Unwind_Reason_Code, _Unwind_Exception*) (in libc++abi.dylib) + 0\n#7   Invalid connection: com.apple.coresymbolicationd   std::__1::__throw_system_error(int, char const*) (in libc++.1.dylib) + 77\n#8   Invalid connection: com.apple.coresymbolicationd   rocksdb::SyncPoint::Process(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, void*) (in db_bench) (sync_point.cc:123)\n#9   Invalid connection: com.apple.coresymbolicationd   rocksdb::PosixLogger::Flush() (in db_bench) (posix_logger.h:58)\n#10  Invalid connection: com.apple.coresymbolicationd   rocksdb::DBImpl::PurgeObsoleteFiles(rocksdb::JobContext const&, bool) (in db_bench) (vector:449)\n#11  Invalid connection: com.apple.coresymbolicationd   rocksdb::DBImpl::BackgroundCallFlush() (in db_bench) (db_impl.cc:3195)\n#12  Invalid connection: com.apple.coresymbolicationd   rocksdb::DBImpl::BGWorkFlush(void*) (in db_bench) (db_impl.cc:3025)\n#13  Invalid connection: com.apple.coresymbolicationd   rocksdb::ThreadPoolImpl::BGThread(unsigned long) (in db_bench) (threadpool_imp.cc:92)\n#14  Invalid connection: com.apple.coresymbolicationd   rocksdb::BGThreadWrapper(void*) (in db_bench) (threadpool_imp.cc:257)\n#15  Invalid connection: com.apple.coresymbolicationd   _pthread_body (in libsystem_pthread.dylib) + 131\n#16  Invalid connection: com.apple.coresymbolicationd   _pthread_body (in libsystem_pthread.dylib) + 0\n#17  Invalid connection: com.apple.coresymbolicationd   thread_start (in libsystem_pthread.dylib) + 13\n[1]    17576 abort      ./db_bench -db tmp-rocks-db -benchmarks=fillsync -use_cuckoo_table\n```\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/1395/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thenewvu": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/1193", "title": "Is there any document for building/integrating rocksdb for/into android/ios app ?", "body": "I saw that rocksdb supports Android and iOS but I didn't find any document for building/integrating it for/into those platforms. So if I'm correct (that it indeed supports those platforms), could you guys give some time to write some document for it ? Such as:\n- Common use cases for using Rocksdb on those platforms (when should use, when should not use, ...).\n- How to build it for Android/iOS.\n- Specific build options for those platforms.\n- Good practices when wrapping/using it on those platforms.\n\nThank you.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/1193/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "guileen": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/814", "title": "How to compile rocksdb with lz4 support?", "body": "There is no document for how to compile rocksdb with lz4 support, I work on it for whole afternoon, failed.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/814/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "fourlastor": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/734", "title": "[Java] initializing the native library on Android fails", "body": "Hello,\n\nI'm not sure if you're already supporting Android as a platform (I'm guessing yes because a quick search shows there are precompiler directives to check `OS_ANDROID`).\n\nBasically trying to initialize the native library via `RocksDB.loadLibrary()` fails at https://github.com/facebook/rocksdb/blob/master/java/src/main/java/org/rocksdb/NativeLibraryLoader.java#L101 due to missing `Files`, `File#toPath` and `StandardCopyOption`.\n\nI'm using the [jar in Maven](http://search.maven.org/#artifactdetails%7Corg.rocksdb%7Crocksdbjni%7C3.13.1%7Cjar).\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/734/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "archiecobbs": {"issues": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/691", "title": "[Java] Assertion failure + JVM crash with rocksdbjni running under Tomcat", "body": "Using rocksjni with a Java web application running in Tomcat, the JVM crashed with this log message:\n\n```\nException in thread \"Thread-10\" java.lang.NoClassDefFoundError: org/rocksdb/InfoLogLevel\nCaused by: java.lang.ClassNotFoundException: org.rocksdb.InfoLogLevel\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\njava: ./java/./rocksjni/portal.h:743: static _jclass* rocksdb::InfoLogLevelJni::getJClass(JNIEnv*): Assertion `jclazz != nullptr' failed.\n```\n\nHere's the relevant code:\n\n```\n    // Get the java class id of org.rocksdb.WBWIRocksIterator.WriteType.\n    static jclass getJClass(JNIEnv* env) {\n      jclass jclazz = env->FindClass(\"org/rocksdb/InfoLogLevel\");\n      if (jclazz == nullptr)\n        env->ExceptionDescribe();\n      assert(jclazz != nullptr);\n      return jclazz;\n    }\n```\n\nThe class `org/rocksdb/InfoLogLevel` is certainly present, inside the rocksdbjni JAR file under `WEB-INF/lib`, so what's going on?\n\nHere's my theory: as we all know Tomcat creates separate `ClassLoader`s for each web application. A web application's Java classes and library classes are not available from the system class loader; instead, you must find them using the loader that Tomcat has created for that web application.\n\nUnfortunately, when rocksdb is setup by rocksdbjni and it wants to log something, it uses `LoggerJniCallback::Logv()`, which looks like this:\n\n```\n/**\n * Get JNIEnv for current native thread\n */\nJNIEnv* LoggerJniCallback::getJniEnv() const {\n  JNIEnv *env;\n  jint rs = m_jvm->AttachCurrentThread(reinterpret_cast<void **>(&env), NULL);\n  assert(rs == JNI_OK);\n  return env;\n}\n\n...\n\nvoid LoggerJniCallback::Logv(const InfoLogLevel log_level,\n    const char* format, va_list ap) {\n  if (GetInfoLogLevel() <= log_level) {\n    JNIEnv* env = getJniEnv();\n\n     ....\n\n    }\n    m_jvm->DetachCurrentThread();\n  }\n}\n```\n\nNote that the current thread is not associated with the JVM at all, which is why it has to put the call back into Java-land inside a `AttachCurrentThread()`/`DetachCurrentThread()` pair. But that means that the corresponding Java thread no longer has a same context class loader. So it defaults to the system class loader and so the attempt to load `org/rocksdb/InfoLogLevel` fails.\n\nSuggested solution:\n1. Upon creation of a new RocksDB instance, store a reference to the current thread's context class loader inside the native code.\n2. Whenever the native code needs to load a class, invoke `ClassLoader.findClass()` directly using the saved loader reference.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/691/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "IslamAbdelRahman": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/c615689bb54d80b744ea90a62e11dd2b5592b34a", "message": "Support skipping bloom filters for SstFileWriter\n\nSummary:\nAdd an option for SstFileWriter to skip building bloom filters\nCloses https://github.com/facebook/rocksdb/pull/3360\n\nDifferential Revision: D6709120\n\nPulled By: IslamAbdelRahman\n\nfbshipit-source-id: 964d4bce38822a048691792f447bcfbb4b6bd809"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/9089373a011e08b8a9116128bdcb3a8f2762c665", "message": "Fix DeleteScheduler::MarkAsTrash() handling existing trash\n\nSummary:\nDeleteScheduler::MarkAsTrash() don't handle existing .trash files correctly\nThis cause rocksdb to not being able to delete existing .trash files on restart\nCloses https://github.com/facebook/rocksdb/pull/3261\n\nDifferential Revision: D6548003\n\nPulled By: IslamAbdelRahman\n\nfbshipit-source-id: c3800639412e587a690062c63076a5a08881e0e6"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/05993155ef72e0beb5286593f7ab63774dc137a1", "message": "Mark files as trash by using .trash extension\n\nSummary:\nSstFileManager move files that need to be deleted into a trash directory.\nDeprecate this behaviour and instead add \".trash\" extension to files that need to be deleted\nCloses https://github.com/facebook/rocksdb/pull/2970\n\nDifferential Revision: D5976805\n\nPulled By: IslamAbdelRahman\n\nfbshipit-source-id: 27374ece4315610b2792c30ffcd50232d4c9a343"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/addfe1ef4b611b8ac47a24f8d7aad92a20d5cb0b", "message": "Fix tombstone scans in SeekForPrev outside prefix\n\nSummary:\nWhen doing a Seek() or SeekForPrev() we should stop the moment we see a key with a different prefix as start if ReadOptions:: prefix_same_as_start was set to true\n\nRight now we don't stop if we encounter a tombstone outside the prefix while executing SeekForPrev()\nCloses https://github.com/facebook/rocksdb/pull/3067\n\nDifferential Revision: D6149638\n\nPulled By: IslamAbdelRahman\n\nfbshipit-source-id: 7f659862d2bf552d3c9104a360c79439ceba2f18"}], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/2768", "title": "Update SstFileManager to also smooth deletes for WAL files", "body": "WAL files could be huge which will cause stalls if they are deleted quickly.\r\nThis patch update SstFileManager to also rate limit the deletes for WAL files", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Sp1l": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/6f5ba0bf5bcb763c656743078c05fd3868f290dd", "message": "Fix building on FreeBSD\n\nSummary:\nFreeBSD uses jemalloc as the base malloc implementation.\nThe patch has been functional on FreeBSD as of the MariaDB 10.2 port.\nCloses https://github.com/facebook/rocksdb/pull/3386\n\nDifferential Revision: D6765742\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: d55dbc082eecf640ef3df9a21f26064ebe6587e8"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "falkevik": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/4decff6fa8c4d46e905a66d439394c4bfb889a69", "message": "Add possibility to change ttl on open DB\n\nSummary:\nWe have seen cases where it could be good to change TTL on already open DB.\nChange ttl in TtlCompactionFilterFactory on open db.\nNext time a filter is created, it will filter accroding to the set TTL.\n\nIs this something that could be useful for others?\nAny downsides?\nCloses https://github.com/facebook/rocksdb/pull/3292\n\nDifferential Revision: D6731993\n\nPulled By: miasantreble\n\nfbshipit-source-id: 73b94d69237b11e8730734389052429d621a6b1e"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ajkr": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/46e599fc6b47e47c1a988189114085c2b13ab49e", "message": "fix live WALs purged while file deletions disabled\n\nSummary:\nWhen calling `DisableFileDeletions` followed by `GetSortedWalFiles`, we guarantee the files returned by the latter call won't be deleted until after file deletions are re-enabled. However, `GetSortedWalFiles` didn't omit files already planned for deletion via `PurgeObsoleteFiles`, so the guarantee could be broken.\n\nWe fix it by making `GetSortedWalFiles` wait for the number of pending purges to hit zero if file deletions are disabled. This condition is eventually met since `PurgeObsoleteFiles` is guaranteed to be called for the existing pending purges, and new purges cannot be scheduled while file deletions are disabled. Once the condition is met, `GetSortedWalFiles` simply returns the content of DB and archive directories, which nobody can delete (except for deletion scheduler, for which I plan to fix this bug later) until deletions are re-enabled.\nCloses https://github.com/facebook/rocksdb/pull/3341\n\nDifferential Revision: D6681131\n\nPulled By: ajkr\n\nfbshipit-source-id: 90b1e2f2362ea9ef715623841c0826611a817634"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/266d85fbec80b129dbb6be28e75fe383a429b778", "message": "fix DBTest.AutomaticConflictsWithManualCompaction\n\nSummary:\nAfter af92d4ad112f192693f6017f24f9ae1b00e1f053, only exclusive manual compaction can have conflict. dc360df81ec48e56a5d9cee4adb7f11ef0ca82ac updated the conflict-checking test case accordingly. But we missed the point that exclusive manual compaction can only conflict with automatic compactions scheduled after it, since it waits on pending automatic compactions before it begins running.\n\nThis PR updates the test case to ensure the automatic compactions are scheduled after the manual compaction starts but before it finishes, thus ensuring a conflict. I also cleaned up the test case to use less space as I saw it cause out-of-space error on travis.\nCloses https://github.com/facebook/rocksdb/pull/3375\n\nDifferential Revision: D6735162\n\nPulled By: ajkr\n\nfbshipit-source-id: 020530a4e150a4786792dce7cec5d66b420cb884"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/43549c7d59c40dee7af1c955cb23a7dfa1227e5a", "message": "Prevent unnecessary calls to PurgeObsoleteFiles\n\nSummary:\nSplit `JobContext::HaveSomethingToDelete` into two functions: itself and `JobContext::HaveSomethingToClean`. Now we won't call `DBImpl::PurgeObsoleteFiles` in cases where we really just need to call `JobContext::Clean`. The change is needed because I want to track pending calls to `PurgeObsoleteFiles` for a bug fix, which is much simpler if we only call it after `FindObsoleteFiles` finds files to delete.\nCloses https://github.com/facebook/rocksdb/pull/3350\n\nDifferential Revision: D6690609\n\nPulled By: ajkr\n\nfbshipit-source-id: 61502e7469288afe16a663a1b7df345baeaf246f"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/ba295cda29daee3ffe58549542804efdfd969784", "message": "replace DBTest.HugeNumbersOfLevel with a more targeted test case\n\nSummary:\nThis test often causes out-of-space error when run on travis. We don't want such stress tests in our unit test suite.\n\nThe bug in #596, which this test intends to expose, can be repro'd as long as the bottommost level(s) are empty when CompactRange is called. I rewrote the test to cover this simple case without writing a lot of data.\nCloses https://github.com/facebook/rocksdb/pull/3362\n\nDifferential Revision: D6710417\n\nPulled By: ajkr\n\nfbshipit-source-id: 9a1ec85e738c813ac2fee29f1d5302065ecb54c5"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/6d7e3b9faf3b4043a6c28a31e2ac2828e1a47420", "message": "fix Gemfile.lock nokogiri dependencies\n\nSummary:\nI installed the ruby dependencies and ran `bundle update nokogiri`. It depends on a newer version of \"mini_portile2\" which I missed in 9c2f64e1488d577746686189d4d127dcf3a7f6e0. Now `bundle install` works again.\nCloses https://github.com/facebook/rocksdb/pull/3361\n\nDifferential Revision: D6710164\n\nPulled By: ajkr\n\nfbshipit-source-id: 9a08d6cc6400ef495b715b3d68b04ce3f3367031"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/9c2f64e1488d577746686189d4d127dcf3a7f6e0", "message": "Update Gemfile.lock\n\nSummary:\nbump nokogiri number\nCloses https://github.com/facebook/rocksdb/pull/3358\n\nDifferential Revision: D6708596\n\nPulled By: ajkr\n\nfbshipit-source-id: 6662c3ba4994374ecf8a13928e915b655a980b70"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/0c6e8be9e205d434a29418410155397f2a762e58", "message": "Fix directory name for db_basic_test\n\nSummary:\nIt was using the same directory as `db_options_test` so transiently failed when unit tests were run in parallel.\nCloses https://github.com/facebook/rocksdb/pull/3352\n\nDifferential Revision: D6691649\n\nPulled By: ajkr\n\nfbshipit-source-id: bee433484fec4faedd5cadf2db3c92fdcc99a170"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/bafec6bb30c8da09558f1af4278dcbe1defbdfa1", "message": "Fix checkpoint_test directory setup/cleanup\n\nSummary:\n- Change directory name from \"db_test\" to \"checkpoint_test\". Previously it used the same directory as `db_test`\n- Systematically cleanup snapshot and snapshot staging directories before each test. Previously a failed test run caused subsequent runs to fail, particularly when the first failure caused \"snapshot.tmp\" to not be cleaned up.\nCloses https://github.com/facebook/rocksdb/pull/3351\n\nDifferential Revision: D6691015\n\nPulled By: ajkr\n\nfbshipit-source-id: 4fc2ac2e21ff2617ea0e96297c5132b5f2eefd79"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/0f0d2ab95a2107256498e7be2bbd3d4e8c0c7c32", "message": "fix DBImpl instance variable naming\n\nSummary:\ngot confused while reading `FindObsoleteFiles` due to thinking it's a local variable, so renamed it properly\nCloses https://github.com/facebook/rocksdb/pull/3342\n\nDifferential Revision: D6684797\n\nPulled By: ajkr\n\nfbshipit-source-id: a4df0aae1cccce99d4dd4d164aadc85b17707132"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/ea8ccd2267db7f565bb9718d17f48f2339883248", "message": "fix powerpc java static build\n\nSummary:\nadded support for C and asm files as required for e612e317409e8a9d74cf05db0bd733403305f459.\nCloses https://github.com/facebook/rocksdb/pull/3299\n\nDifferential Revision: D6612479\n\nPulled By: ajkr\n\nfbshipit-source-id: 6263ed7c1602f249460421825c76b5721f396163"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/f00e176c5b5f87888e49b5cc6042c4285ffe4652", "message": "fix ForwardIterator reference to temporary object\n\nSummary:\nFixes the following ASAN error:\n\n```\n==2108042==ERROR: AddressSanitizer: stack-use-after-scope on address 0x7fc50ae9b868 at pc 0x7fc5112aff55 bp 0x7fff9eb9dc10 sp 0x7fff9eb9dc08\n=== How to use this, how to get the raw stack trace, and more: fburl.com/ASAN ===\nREAD of size 8 at 0x7fc50ae9b868 thread T0\nSCARINESS: 23 (8-byte-read-stack-use-after-scope)\n     #0 rocksdb/dbformat.h:164                   rocksdb::InternalKeyComparator::user_comparator() const\n     #1 librocksdb_src_rocksdb_lib.so+0x1429a7d  rocksdb::RangeDelAggregator::InitRep(std::vector<...> const&)\n     #2 librocksdb_src_rocksdb_lib.so+0x142ceae  rocksdb::RangeDelAggregator::AddTombstones(std::unique_ptr<...>)\n     #3 librocksdb_src_rocksdb_lib.so+0x1382d88  rocksdb::ForwardIterator::RebuildIterators(bool)\n     #4 librocksdb_src_rocksdb_lib.so+0x1382362  rocksdb::ForwardIterator::ForwardIterator(rocksdb::DBImpl*, rocksdb::ReadOptions const&, rocksdb::ColumnFamilyData*, rocksdb::SuperVersion*)\n     #5 librocksdb_src_rocksdb_lib.so+0x11f433f  rocksdb::DBImpl::NewIterator(rocksdb::ReadOptions const&, rocksdb::ColumnFamilyHandle*)\n     #6 rocksdb/src/include/rocksdb/db.h:382     rocksdb::DB::NewIterator(rocksdb::ReadOptions const&)\n     #7 rocksdb/db_range_del_test.cc:807         rocksdb::DBRangeDelTest_TailingIteratorRangeTombstoneUnsupported_Test::TestBody()\n    #18 rocksdb/db_range_del_test.cc:1006        main\n\nAddress 0x7fc50ae9b868 is located in stack of thread T0 at offset 104 in frame\n     #0 librocksdb_src_rocksdb_lib.so+0x13825af  rocksdb::ForwardIterator::RebuildIterators(bool)\n```\nCloses https://github.com/facebook/rocksdb/pull/3300\n\nDifferential Revision: D6612989\n\nPulled By: ajkr\n\nfbshipit-source-id: e7ea2ed914c1b80a8a29d71d92440a6bd9cbcc80"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/1563801bce4896ebdf5eab6651cddf989923c789", "message": "blog post for auto-tuned rate limiter\n\nSummary:\nWrote the blog post.\nCloses https://github.com/facebook/rocksdb/pull/3289\n\nDifferential Revision: D6599031\n\nPulled By: ajkr\n\nfbshipit-source-id: 77ee553196f225f20c56112d2c015b6fa14f1b83"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/a79c7c05e8f3e5b1051ec2a32b9a7220b6e41cc8", "message": "fix backup meta-file buffer overrun\n\nSummary:\n- check most times after calling snprintf that the buffer didn't fill up. Previously we'd proceed and use `buf_size - len` as the length in subsequent calls, which underflowed as those are unsigned size_t.\n- replace some memcpys with snprintf for consistency\nCloses https://github.com/facebook/rocksdb/pull/3255\n\nDifferential Revision: D6541464\n\nPulled By: ajkr\n\nfbshipit-source-id: 8610ea6a24f38e0a37c6d17bc65b7c712da6d932"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/5a7e08468ad2f03b957e255aefdfa81b95a6810a", "message": "fix ThreadStatus for bottom-pri compaction threads\n\nSummary:\nadded `ThreadType::BOTTOM_PRIORITY` which is used in the `ThreadStatus` object to indicate the thread is used for bottom-pri compactions. Previously there was a bug where we mislabeled such threads as `ThreadType::LOW_PRIORITY`.\nCloses https://github.com/facebook/rocksdb/pull/3270\n\nDifferential Revision: D6559428\n\nPulled By: ajkr\n\nfbshipit-source-id: 96b1a50a9c19492b1a5fd1b77cf7061a6f9f1d1c"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/e3814a860818531fd8a36d3dd908382e21839bf4", "message": "revert fbcode build behavior\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/3242\n\nDifferential Revision: D6514255\n\nPulled By: ajkr\n\nfbshipit-source-id: c39fa8e745866b052649d02bf339e794d77e96a3"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/2e3a00987e75fdbfea2a06991b83d9bcf5742908", "message": "fix ASAN for DeleteFilesInRange test case\n\nSummary:\nerror message was\n\n```\n==3095==ERROR: AddressSanitizer: stack-use-after-scope on address 0x7ffd18216c40 at pc 0x0000005edda1 bp 0x7ffd18215550 sp 0x7ffd18214d00\n...\nAddress 0x7ffd18216c40 is located in stack of thread T0 at offset 1952 in frame\n     #0 internal_repo_rocksdb/db_compaction_test.cc:1520 rocksdb::DBCompactionTest_DeleteFileRangeFileEndpointsOverlapBug_Test::TestBody()\n```\n\nIt was unsafe to have slices referring to the temporary string objects' buffers, as those strings were destroyed before the slices were used. Fixed it by assigning the strings returned by `Key()` to local variables.\nCloses https://github.com/facebook/rocksdb/pull/3238\n\nDifferential Revision: D6507864\n\nPulled By: ajkr\n\nfbshipit-source-id: dd07de1a0070c6748c1ab4f3d7bd31f9a81889d0"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/53a516ab5817b1455ebc0265b689aae349aa6aa4", "message": "update history for recent commits\n\nSummary:\nI browsed through the history since 5.9 was released and found some changes worth mentioning in HISTORY.md.\nCloses https://github.com/facebook/rocksdb/pull/3237\n\nDifferential Revision: D6506472\n\nPulled By: ajkr\n\nfbshipit-source-id: 627ce9f94ca33df9f0f231a9c5ced3624b05506c"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/78d1a5ec7263e380700b5fa36b2ffcd1cb265a2f", "message": "Preserve overlapping file endpoint invariant\n\nSummary:\nFix for #2833.\n\n- In `DeleteFilesInRange`, use `GetCleanInputsWithinInterval` instead of `GetOverlappingInputs` to make sure we get a clean cut set of files to delete.\n- In `GetCleanInputsWithinInterval`, support nullptr as `begin_key` or `end_key`.\n- In `GetOverlappingInputsRangeBinarySearch`, move the assertion for non-empty range away from `ExtendFileRangeWithinInterval`, which should be allowed to return an empty range (via `end_index < begin_index`).\nCloses https://github.com/facebook/rocksdb/pull/2843\n\nDifferential Revision: D5772387\n\nPulled By: ajkr\n\nfbshipit-source-id: e554e8461823c6be82b21a9262a2da02b3957881"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/57056bb60643e390c170ae3166d297a971587c93", "message": "gflags in cmake on linux\n\nSummary:\nWe should use it if available otherwise the tools builds never work. Thanks to #3212, we can set -DGFLAGS=1 and it'll be independent of the namespace with which gflags was compiled.\nCloses https://github.com/facebook/rocksdb/pull/3214\n\nDifferential Revision: D6462214\n\nPulled By: ajkr\n\nfbshipit-source-id: db4e5f1b905322e3119554a9d01b57532c499384"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/63f1c0a57d7afd8a4730985cf720118794045b99", "message": "fix gflags namespace\n\nSummary:\nI started adding gflags support for cmake on linux and got frustrated that I'd need to duplicate the build_detect_platform logic, which determines namespace based on attempting compilation. We can do it differently -- use the GFLAGS_NAMESPACE macro if available, and if not, that indicates it's an old gflags version without configurable namespace so we can simply hardcode \"google\".\nCloses https://github.com/facebook/rocksdb/pull/3212\n\nDifferential Revision: D6456973\n\nPulled By: ajkr\n\nfbshipit-source-id: 3e6d5bde3ca00d4496a120a7caf4687399f5d656"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/ed3af9ef9957866300c144cd9e60a60e2a490583", "message": "improve ldb CLI option support\n\nSummary:\n- Made CLI arguments take precedence over options file when both are provided. Note some of the CLI args are not settable via options file, like `--compression_max_dict_bytes`, so it's necessary to allow both ways of providing options simultaneously.\n- Changed `PrepareOptionsForOpenDB` to update the proper `ColumnFamilyOptions` if one exists for the user's `--column_family_name` argument. I supported this only in the base class, `LDBCommand`, so it works for the general arguments. Will defer adding support for subcommand-specific arguments.\n- Made the command fail if `--try_load_options` is provided and loading options file returns NotFound. I found the previous behavior of silently continuing confusing.\nCloses https://github.com/facebook/rocksdb/pull/3144\n\nDifferential Revision: D6270544\n\nPulled By: ajkr\n\nfbshipit-source-id: 7c2eac9f9b38720523d74466fb9e78db53561367"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/c85f8ccca39a571e58f3cfd379c3147730be5b9f", "message": "convert null terminator in ascii dump\n\nSummary:\nThe ASCII output is almost always useless to me as the first '\\0' byte in the key or value causes it to stop printing. Since all characters are already surrounded by spaces, \"\\ 0\" (how we display a backslash followed by a zero) and \"\\0\" (how this PR displays a null terminator) are distinguishable. My assumption is the value of seeing all the bytes outweighs the value of the alignment we had before, where we always had one character followed by one space.\nCloses https://github.com/facebook/rocksdb/pull/3203\n\nDifferential Revision: D6428651\n\nPulled By: ajkr\n\nfbshipit-source-id: aafc978a51e9ea029cfe3e763e2bb0e1751b9ccf"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/1bdb44de9520abd9402f62caff4fd452c18fbbb6", "message": "optimize file ingestion checks for range deletion overlap\n\nSummary:\nBefore we were checking every file in the level which was unnecessary. We can piggyback onto the code for checking point-key overlap, which already opens all the files that could possibly contain overlapping range deletions. This PR makes us check just the range deletions from those files, so no extra ones will be opened.\nCloses https://github.com/facebook/rocksdb/pull/3179\n\nDifferential Revision: D6358125\n\nPulled By: ajkr\n\nfbshipit-source-id: 00e200770fdb8f3cc6b1b2da232b755e4ba36279"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/e27f60b1c867b0b60dbff26d7a35777e6bb9f14b", "message": "distinguish kZSTDNotFinalCompression in compression string\n\nSummary:\nThis confused some users who were getting compression type from the logs.\nCloses https://github.com/facebook/rocksdb/pull/3153\n\nDifferential Revision: D6294964\n\nPulled By: ajkr\n\nfbshipit-source-id: 3c813376d33682dc6ccafc9a78df1a2e2528985e"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/07c2738ffa6bdb21caba5c5f7aa4cfe131d25ba7", "message": "prefer enabling cpu features via -march/-mcpu\n\nSummary:\nIf possible, use -march or -mcpu to get enable all features available on the local CPU or architecture. Only if this is impossible, we will manually set -msse4.2. It should be safe as there'll be a warning printed if `USE_SSE` is set and the provided flags are insufficient to support SSE4.2.\nCloses https://github.com/facebook/rocksdb/pull/3156\n\nDifferential Revision: D6304703\n\nPulled By: ajkr\n\nfbshipit-source-id: 030a53491263300cae7fafb429114d87acc828ef"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/93f69cb93a3716fb80c9a491844ae2a6f145ba57", "message": "use bottommost compression when base level is bottommost\n\nSummary:\nThe previous compression type selection caused unexpected behavior when the base level was also the bottommost level. The following sequence of events could happen:\n\n- full compaction generates files with `bottommost_compression` type\n- now base level is bottommost level since all files are in the same level\n- any compaction causes files to be rewritten `compression_per_level` type since bottommost compression didn't apply to base level\n\nI changed the code to make bottommost compression apply to base level.\nCloses https://github.com/facebook/rocksdb/pull/3141\n\nDifferential Revision: D6264614\n\nPulled By: ajkr\n\nfbshipit-source-id: d7aaa8675126896684154a1f2c9034d6214fde82"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/114896c4e0411adf64da8fca8429a955550ff0ae", "message": "db_bench compression options\n\nSummary:\n- moved existing compression options to `InitializeOptionsGeneral` since they cannot be set through options file\n- added flag for `zstd_max_train_bytes` which was recently introduced by #3057\nCloses https://github.com/facebook/rocksdb/pull/3128\n\nDifferential Revision: D6240460\n\nPulled By: ajkr\n\nfbshipit-source-id: 27dbebd86a55de237ba6a45cc79cff9214e82ebc"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/65c95d9c598414745d91eab3c02d2bd658282cda", "message": "support db_bench compact benchmark on bottommost files\n\nSummary:\nWithout this option, running the compact benchmark on a DB containing only bottommost files simply returned immediately.\nCloses https://github.com/facebook/rocksdb/pull/3138\n\nDifferential Revision: D6256660\n\nPulled By: ajkr\n\nfbshipit-source-id: e3b64543acd503d821066f4200daa201d4fb3a9d"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/cfb120f7371da3ae4e1c64fbb383eb96c5426233", "message": "fix CopyFile status checks\n\nSummary:\ncopied from internal diff D6156261\nCloses https://github.com/facebook/rocksdb/pull/3124\n\nDifferential Revision: D6230167\n\nPulled By: ajkr\n\nfbshipit-source-id: 17926bb1152d607556364e3aacfec0ef3c115748"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/24ad4306001e0e9ec13ec29f11e50b4a7b070bf4", "message": "pass key/value samples through zstd compression dictionary generator\n\nSummary:\nInstead of using samples directly, we now support passing the samples through zstd's dictionary generator when `CompressionOptions::zstd_max_train_bytes` is set to nonzero. If set to zero, we will use the samples directly as the dictionary -- same as before.\n\nNote this is the first step of #2987, extracted into a separate PR per reviewer request.\nCloses https://github.com/facebook/rocksdb/pull/3057\n\nDifferential Revision: D6116891\n\nPulled By: ajkr\n\nfbshipit-source-id: 70ab13cc4c734fa02e554180eed0618b75255497"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/c4c1f961e7264c0e1e11f222e5b90c8ef7eb1d86", "message": "dynamically change current memtable size\n\nSummary:\nPreviously setting `write_buffer_size` with `SetOptions` would only apply to new memtables. An internal user wanted it to take effect immediately, instead of at an arbitrary future point, to prevent OOM.\n\nThis PR makes the memtable's size mutable, and makes `SetOptions()` mutate it. There is one case when we preserve the old behavior, which is when memtable prefix bloom filter is enabled and the user is increasing the memtable's capacity. That's because the prefix bloom filter's size is fixed and wouldn't work as well on a larger memtable.\nCloses https://github.com/facebook/rocksdb/pull/3119\n\nDifferential Revision: D6228304\n\nPulled By: ajkr\n\nfbshipit-source-id: e44bd9d10a5f8c9d8c464bf7436070bb3eafdfc9"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/6778690b51ac90f3ad2c67b9cf0a6d822ba6b220", "message": "fix duplicate definition of GetEntryType()\n\nSummary:\nIt's also defined in db/dbformat.cc per 7fe3b32896ecbb21d67ec52fccb713cb9bc6a644\nCloses https://github.com/facebook/rocksdb/pull/3111\n\nDifferential Revision: D6219140\n\nPulled By: ajkr\n\nfbshipit-source-id: 0f2b14e41457334a4665c6b7e3f42f1a060a0f35"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/cd124215df2b5adc19b5dc649e38142d9c25a3b7", "message": "release 5.9\n\nSummary:\nupdated HISTORY.md and version.h for the release.\nCloses https://github.com/facebook/rocksdb/pull/3110\n\nDifferential Revision: D6218645\n\nPulled By: ajkr\n\nfbshipit-source-id: 99ab8473e9088b02d7596e92351cce7a60a99e93"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/4d43c6a6a43c8d4f0c5dd3ba300020bf370ec291", "message": "db_stress snapshot compatibility with reopens\n\nSummary:\n- Release all snapshots before crashing and reopening the DB. Without this, we may attempt to release snapshots from an old DB using a new DB. That tripped an assertion.\n- Release multiple snapshots in the same operation if needed. Without this, we would sometimes leak snapshots.\nCloses https://github.com/facebook/rocksdb/pull/3098\n\nDifferential Revision: D6194923\n\nPulled By: ajkr\n\nfbshipit-source-id: b9c89bcca7ebcbb6c7802c616f9d1175a005aadf"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/b7bc9cc0384209738066fa41d778228c3332510f", "message": "fix tracking oldest snapshot for bottom-level compaction\n\nSummary:\nThe assertion was caught by `MySQLStyleTransactionTest/MySQLStyleTransactionTest.TransactionStressTest/5` when run in a loop. The caller doesn't track whether the released snapshot is oldest, so let this function handle that case.\nCloses https://github.com/facebook/rocksdb/pull/3080\n\nDifferential Revision: D6185257\n\nPulled By: ajkr\n\nfbshipit-source-id: 4b3015c11db5d31e46521a00af568546ef4558cd"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/6a9335dbbb3ecc87fb1cb8701be39b3ec16beb20", "message": "always drop tombstones compacted to bottommost level\n\nSummary:\nProblem was in bottommost compaction, when an L0->L0 compaction happened and L0 was bottommost. Then we'd preserve tombstones according to `Compaction::KeyNotExistsBeyondOutputLevel`, while zeroing seqnum according to `CompactionIterator::PrepareOutput`, thus triggering the assertion in `PrepareOutput`. To fix, we can just drop tombstones in L0->L0 when the output is \"bottommost\", i.e., the compaction includes the oldest L0 file and there's nothing at lower levels.\nCloses https://github.com/facebook/rocksdb/pull/3085\n\nDifferential Revision: D6175742\n\nPulled By: ajkr\n\nfbshipit-source-id: 8ab19a2e001496f362e9eb0a71757e2f6ecfdb3b"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/95667383db6eb356331b214868c3586d7ab74967", "message": "implement lower bound for iterators\n\nSummary:\n- for `SeekToFirst()`, just convert it to a regular `Seek()` if lower bound is specified\n- for operations that iterate backwards over user keys (`SeekForPrev`, `SeekToLast`, `Prev`), change `PrevInternal` to check whether user key went below lower bound every time the user key changes -- same approach we use to ensure we stay within a prefix when `prefix_same_as_start=true`.\nCloses https://github.com/facebook/rocksdb/pull/3074\n\nDifferential Revision: D6158654\n\nPulled By: ajkr\n\nfbshipit-source-id: cb0e3a922e2650d2cd4d1c6e1c0f1e8b729ff518"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/9b18cc236304b82c3bffa793006cba964fee0835", "message": "single-file bottom-level compaction when snapshot released\n\nSummary:\nWhen snapshots are held for a long time, files may reach the bottom level containing overwritten/deleted keys. We previously had no mechanism to trigger compaction on such files. This particularly impacted DBs that write to different parts of the keyspace over time, as such files would never be naturally compacted due to second-last level files moving down. This PR introduces a mechanism for bottommost files to be recompacted upon releasing all snapshots that prevent them from dropping their deleted/overwritten keys.\n\n- Changed `CompactionPicker` to compact files in `BottommostFilesMarkedForCompaction()`. These are the last choice when picking. Each file will be compacted alone and output to the same level in which it originated. The goal of this type of compaction is to rewrite the data excluding deleted/overwritten keys.\n- Changed `ReleaseSnapshot()` to recompute the bottom files marked for compaction when the oldest existing snapshot changes, and schedule a compaction if needed. We cache the value that oldest existing snapshot needs to exceed in order for another file to be marked in `bottommost_files_mark_threshold_`, which allows us to avoid recomputing marked files for most snapshot releases.\n- Changed `VersionStorageInfo` to track the list of bottommost files, which is recomputed every time the version changes by `UpdateBottommostFiles()`. The list of marked bottommost files is first computed in `ComputeBottommostFilesMarkedForCompaction()` when the version changes, but may also be recomputed when `ReleaseSnapshot()` is called.\n- Extracted core logic of `Compaction::IsBottommostLevel()` into `VersionStorageInfo::RangeMightExistAfterSortedRun()` since logic to check whether a file is bottommost is now necessary outside of compaction.\nCloses https://github.com/facebook/rocksdb/pull/3009\n\nDifferential Revision: D6062044\n\nPulled By: ajkr\n\nfbshipit-source-id: 123d201cf140715a7d5928e8b3cb4f9cd9f7ad21"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/d75793d6b4146410b557d39e2bee9cd70b9b07be", "message": "db_stress support long-held snapshots\n\nSummary:\nAdd options to `db_stress` (correctness testing tool) to randomly acquire snapshot and release it after some period of time. It's useful for correctness testing of #3009, as well as other parts of compaction that behave differently depending on which snapshots are held.\nCloses https://github.com/facebook/rocksdb/pull/3038\n\nDifferential Revision: D6086501\n\nPulled By: ajkr\n\nfbshipit-source-id: 3ec0d8666c78ac507f1f808887c4ff759ba9b865"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/f8b5bb2fd82a39ab4c0b27d21bd492bc61177742", "message": "remove unused code\n\nSummary:\nfixup 6a541afcc4d1e5b6e6d78e288b9bee3bb2a933b5. This code didn't do anything because (1) `bytes_per_sync` is assigned in `EnvOptions`'s constructor; and (2) `OptimizeForCompactionTableWrite`'s return value was ignored, even though its only purpose is to return something.\nCloses https://github.com/facebook/rocksdb/pull/3055\n\nDifferential Revision: D6114132\n\nPulled By: ajkr\n\nfbshipit-source-id: ea4831770930e9cf83518e13eb2e1934d1f5487c"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/57fd4a823b79dcd982dc67d50018632bd31fcd45", "message": "update HISTORY with recent changes\n\nSummary:\nWe should mention these:\n\n- `EventListener::OnStallConditionsChanged()` in 01542400a87ad130aed790bf895c029082745cbe\n- `DeleteRange()` fix in 966b32b57c673774ea677fe3a40d255e6c8916ba\nCloses https://github.com/facebook/rocksdb/pull/3054\n\nDifferential Revision: D6113989\n\nPulled By: ajkr\n\nfbshipit-source-id: d5e058e1ab07570df22936e8d5939fb30fb4d381"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/731895214bb729f4b748c8051d2d4c81927ff47f", "message": "db_bench randomtransaction print throughput\n\nSummary:\nprint throughput in MB/s upon finishing randomtransaction benchmark\nCloses https://github.com/facebook/rocksdb/pull/3016\n\nDifferential Revision: D6070426\n\nPulled By: ajkr\n\nfbshipit-source-id: 69df43beed4c374a36d826e761ca3a83e1fdcbf5"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/8dd0a7e11abad6bc32e59a5cba8769961e085312", "message": "add comment in SuperVersion referencing logic\n\nSummary:\nThe referencing logic is super confusing so added a comment at the part that took me longest to figure out.\nCloses https://github.com/facebook/rocksdb/pull/2996\n\nDifferential Revision: D6034969\n\nPulled By: ajkr\n\nfbshipit-source-id: 9cc2e744c1f79d6d57d378f86ed59238a5f583db"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/70aa9421533e560de99f9385b725b64002e93237", "message": "fix file numbers after repair\n\nSummary:\nThe file numbers assigned post-repair were sometimes smaller than older files' numbers due to `LogAndApply` saving the wrong next file number in the manifest.\n\n- Mark the highest file seen during repair as used before `LogAndApply` so the correct next file number will be stored.\n- Renamed `MarkFileNumberUsedDuringRecovery` to `MarkFileNumberUsed` since now it's used during repair in addition to during recovery\n- Added `TEST_Current_Next_FileNo` to expose the next file number for the unit test.\nCloses https://github.com/facebook/rocksdb/pull/2988\n\nDifferential Revision: D6018083\n\nPulled By: ajkr\n\nfbshipit-source-id: 3f25cbf74439cb8f16dd12af90b67f9f9f75e718"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/5a6ad9d52a568e53e52c2b6b9d4d51a185b691d6", "message": "release build treat warnings as errors\n\nSummary:\nfixing warnings is important, especially for release code.\nCloses https://github.com/facebook/rocksdb/pull/2971\n\nDifferential Revision: D5980596\n\nPulled By: ajkr\n\nfbshipit-source-id: 04f4ea3fb005dcda33d60342e4361e380bc4dfb1"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/1026e794a368c1a9c98f442e24e1af12d23b3ffb", "message": "rate limit auto-tuning\n\nSummary:\nDynamic adjustment of rate limit according to demand for background I/O. It increases by a factor when limiter is drained too frequently, and decreases by the same factor when limiter is not drained frequently enough. The parameters for this behavior are fixed in `GenericRateLimiter::Tune`. Other changes:\n\n- make rate limiter's `Env*` configurable for testing\n- track num drain intervals in RateLimiter so we don't have to rely on stats, which may be shared across different DB instances from the ones that share the RateLimiter.\nCloses https://github.com/facebook/rocksdb/pull/2899\n\nDifferential Revision: D5858704\n\nPulled By: ajkr\n\nfbshipit-source-id: cc2bac30f85e7f6fd63655d0a6732ef9ed7403b1"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/5b2cb64bfb87e1d77025220d23ac7bf3ac8ecefd", "message": "Prevent threads from respawning during joining\n\nSummary:\nPreviously the thread pool might be non-empty after joining since concurrent submissions could spawn new threads. This problem didn't affect our background flush/compaction thread pools because the `shutting_down_` flag prevented new jobs from being submitted during/after joining. But I wanted to be able to reuse the `ThreadPool` without such external synchronization.\nCloses https://github.com/facebook/rocksdb/pull/2953\n\nDifferential Revision: D5951920\n\nPulled By: ajkr\n\nfbshipit-source-id: 0efec7d0056d36d1338367da75e8b0c089bbc973"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/821887036e5235c827029d14decb185bea01ec4b", "message": "pin L0 filters/indexes for compaction outputs\n\nSummary:\nWe need to tell the iterator the compaction output file's level so it can apply proper optimizations, like pinning filter and index blocks when user enables `pin_l0_filter_and_index_blocks_in_cache` and the output file's level is zero.\nCloses https://github.com/facebook/rocksdb/pull/2949\n\nDifferential Revision: D5945597\n\nPulled By: ajkr\n\nfbshipit-source-id: 2389decf9026ffaa32d45801a77d002529f64a62"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/880411f54cdcb8da216722bc2f7bbe9ae6859e00", "message": "disable populating block cache for in-place updates\n\nSummary:\nThere's no point populating the block cache during this read. The key we read is guaranteed to be overwritten with a new `kValueType` key immediately afterwards, so can't be accessed again. A user was seeing high turnover of data blocks, at least partially due to this.\nCloses https://github.com/facebook/rocksdb/pull/2959\n\nDifferential Revision: D5961672\n\nPulled By: ajkr\n\nfbshipit-source-id: e7cb27c156c5db3b32af355c780efb99dbdf087c"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/5df172da2f6acd3e3e6cd928bd0b66db722357df", "message": "fix deletion-triggered compaction in table builder\n\nSummary:\nIt was broken when `NotifyCollectTableCollectorsOnFinish` was introduced. That function called `Finish` on each of the `TablePropertiesCollector`s, and `CompactOnDeletionCollector::Finish()` was resetting all its internal state. Then, when we checked whether compaction is necessary, the flag had already been cleared.\n\nFixed above issue by avoiding resetting internal state during `Finish()`. Multiple calls to `Finish()` are allowed, but callers cannot invoke `AddUserKey()` on the collector after any finishes.\nCloses https://github.com/facebook/rocksdb/pull/2936\n\nDifferential Revision: D5918659\n\nPulled By: ajkr\n\nfbshipit-source-id: 4f05e9d80e50ee762ba1e611d8d22620029dca6b"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/c2f6e45aa3cc8b7a7f3fa217b11de83a7ac576f1", "message": "prevent nullptr dereference in table reader error case\n\nSummary:\nA user encountered segfault on the call to `CacheDependencies()`, probably because `NewIndexIterator()` failed before populating `*index_entry`. Let's avoid the call in that case.\nCloses https://github.com/facebook/rocksdb/pull/2939\n\nDifferential Revision: D5928611\n\nPulled By: ajkr\n\nfbshipit-source-id: 484be453dbb00e5e160e9c6a1bc933df7d80f574"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/4708a6875c29d4879ebcb6173149363883f94df4", "message": "Repair DBs with trailing slash in name\n\nSummary:\nProblem:\n\n- `DB::SanitizeOptions` strips trailing slash from `wal_dir` but not `dbname`\n- We check whether `wal_dir` and `dbname` refer to the same directory using string equality: https://github.com/facebook/rocksdb/blob/master/db/repair.cc#L258\n- Providing `dbname` with trailing slash causes default `wal_dir` to be misidentified as a separate directory.\n- Then the repair tries to add all SST files to the `VersionEdit` twice (once for `dbname` dir, once for `wal_dir`) and fails with coredump.\n\nSolution:\n\n- Add a new `Env` function, `AreFilesSame`, which uses device and inode number to check whether files are the same. It's currently only implemented in `PosixEnv`.\n- Migrate repair to use `AreFilesSame` to check whether `dbname` and `wal_dir` are same. If unsupported, falls back to string comparison.\nCloses https://github.com/facebook/rocksdb/pull/2827\n\nDifferential Revision: D5761349\n\nPulled By: ajkr\n\nfbshipit-source-id: c839d548678b742af1166d60b09abd94e5476238"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/fc7476bec1169c44998de846d26b52ef9b4bf42a", "message": "fix populating range deletions in forward iterator\n\nSummary:\nfixes #2902\nCloses https://github.com/facebook/rocksdb/pull/2917\n\nDifferential Revision: D5887175\n\nPulled By: ajkr\n\nfbshipit-source-id: 364e292c636a3238bfc53b0fb9a01ff2f82dcbb9"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/8fc3de3c622896d49480a53eac0ebb20301a645b", "message": "make rate limiter a general option\n\nSummary:\nit's unsupported in options file, so the flag should be respected by db_bench even when an options file is provided.\nCloses https://github.com/facebook/rocksdb/pull/2910\n\nDifferential Revision: D5869836\n\nPulled By: ajkr\n\nfbshipit-source-id: f67f591ae083e95e989f86b6fad50765d2e3d855"}], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3381", "title": "Add delay before flush in CompactRange to avoid write stalling", "body": "- Refactored logic for checking write stall condition to a helper function: `GetWriteStallConditionAndCause`. Now it is decoupled from the logic for updating WriteController / stats in `RecalculateWriteStallConditions`, so we can reuse it for predicting whether write stall will occur.\r\n- Updated `CompactRange` to first check whether the one additional immutable memtable / L0 file would cause stalling before it flushes. If so, it waits until that is no longer true.\r\n- Updated `bg_cv_` to be signaled on `SetOptions` calls. The stall conditions `CompactRange` cares about can change when (1) flush finishes, (2) compaction finishes, or (3) options dynamically change. The cv was already signaled for (1) and (2) but not yet for (3).\r\n\r\nTest Plan:\r\n\r\n- couple new unit tests, `make check -j64`", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3275", "title": "fix for checkpoint directory with trailing slash(es)", "body": "previously if `checkpoint_dir` contained a trailing slash, we'd attempt to create the `.tmp` directory under `checkpoint_dir` due to simply concatenating `checkpoint_dir + \".tmp\"`. This failed because `checkpoint_dir` hadn't been created yet and our directory creation is non-recursive. This PR fixes the issue by always creating the `.tmp` directory in the same parent as `checkpoint_dir` by stripping trailing slashes before concatenating.\r\n\r\nTest Plan:\r\n\r\ncommand: `./ldb checkpoint --db=/data/compaction_bench/dbbench/ --checkpoint_dir=/data/compaction_bench/dbbench_checkpoint/`\r\n\r\noutput before this PR: `Failed: IO error: While mkdir: /data/compaction_bench/dbbench_checkpoint6/.tmp: No such file or directory`\r\n\r\noutput after this PR: `OK`", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/2987", "title": "Apply compression dictionary to file from which it was generated", "body": "- Move sampling logic from `CompactionJob` to `BlockBasedTableBuilder` as now we generate dictionary per file rather than per subcompaction.\r\n- Buffer uncompressed data blocks and all keys in-memory. We take a second pass through these during `Finish()` to compress/write data blocks and generate the index. The keys are buffered for convenience, even though they're redundantly stored in the uncompressed data blocks, so we can replay them to `OnKeyAdded` and `AddIndexEntry`.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/2973", "title": "log decimal TID on linux", "body": "identify the thread using the output of `gettid()` syscall on Linux, which is a system-wide unique ID, unlike `pthread_self()`. Also changed from hex to decimal to be compatible with tools like `top`.\r\n\r\nTest Plan:\r\n\r\n- make check -j64\r\n- run db_bench and correlate `top` entries with log entries\r\n\r\n```\r\n$ top -H\r\n...\r\n782508 andrewkr  20   0  319272  47864   8884 R 99.7  0.0   0:04.18  4 db_bench\r\n...\r\n```\r\n\r\n```\r\n$ grep '782508' /dev/shm/dbbench/LOG | head -1\r\n2017/10/05-13:23:17.517942 782508 [db/db_impl_write.cc:1162] [default] New memtable created with log file: #6. Immutable memtables: 0.\r\n```", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/15686145", "body": "missed the commit message and can't edit it with git -- it should've been:\n\n```\nSummary:\nThis is part of a series of diffs to eliminate circular dependencies between\ndirectories (e.g., db/* files depending on util/* files and vice-versa).\n\nI submitted individual diffs for the more complex changes (D53361 and\nD53343). Now there were only simple changes left, so I bundled them\ntogether in this diff.\n\nMotivation for changes:\n\n- util/auto_roll_logger{.cc,.h,_test.cc} moved to db/ because their logic is specific to database logs\n- util/db_info_dumper.{h,cc} moved to db/ by similar reasoning\n- unused db includes removed from util/testutil.h, util/options.cc, util/file_util.cc\n- util/skiplistrep.cc and util/vectorrep.cc moved to memtable/ since they're memtable implementations\n- moved the tests for running a DB on mock/memory environments into DBTest since they require a full DB to test.\n\nTest Plan: waiting on \"make commit-prereq -j32\", so far everything's compiled and looks good\n\nReviewers: yhchiang, IslamAbdelRahman, sdong\n\nReviewed By: sdong\n\nSubscribers: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D53373\n```\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/15686145/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17439462", "body": "I will fix it. btw, \"observe common rules\" is not useful feedback -- we sometimes miss things especially for platforms not widely used internally.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17439462/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17439753", "body": "addresses comment in #1120 \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17439753/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17457737", "body": "Agreed. We rely on realpath(3) for which I couldn't find a replacement on Windows. Do you know of one/would you be willing to implement it in port/win? It's needed by ChrootEnv, which is used to isolate the db/backup envs in the filesystem, so we can catch the case where our backup logic uses the wrong env.\n\nFor now I can change this test to just not use ChrootEnv in Windows, but it might be better longer-term if our Windows tests catch the same bugs as our Linux tests.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17457737/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17475597", "body": "We introduced chrootenv due to this problem: in our backup logic, we had a bug where the backup env was used to read database files (db env should have been used). The test cases all passed despite this bug because db env and backup env shared the same local file system, so it was possible to read database files with backup env. But then the code broke in production when backup env was a remote filesystem\n\nThe chrootenv isolates db/backup envs in the local file system so now accessing files using the wrong env will fail.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17475597/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17475923", "body": "@igorcanadi MemEnv and MockEnv do not support directory operations required for backup. We implemented ChrootEnv so we get the directory operations for free by using the filesystem.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17475923/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19048180", "body": "oops, i totally missed that this doesn't handle the case where db_options_.wal_dir == dbname_\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19048180/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19048589", "body": "see https://reviews.facebook.net/D64029\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19048589/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23244066", "body": "@zhangjinpeng1987, It should be available since 5.6.0. Let me know if you still think there's an issue with it.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23244066/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23361973", "body": "these aren't the right arguments for RandomAccessFileReader.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23361973/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "vogel76": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/bf6f03f3cdbc51637820655c07151fe4e2efda3f", "message": "Issue #3370 Broken CMakeLists.txt\n\nSummary:\nIssue #3370 Simple fixes to make RocksDB project working also as a submodule of other bigger one.\nCloses https://github.com/facebook/rocksdb/pull/3372\n\nDifferential Revision: D6729595\n\nPulled By: ajkr\n\nfbshipit-source-id: eee2589e7a7c4322873dff8510eebd050301c54c"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xiaosuo": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/0a7ba0e54826a01fb2117d3b2431973d25763242", "message": "Fix memleak when DB::DeleteFile()\n\nSummary:\nBecause the corresponding read_first_record_cache_ item wasn't\nerased, memory leaked.\nCloses https://github.com/facebook/rocksdb/pull/1712\n\nDifferential Revision: D4363654\n\nPulled By: ajkr\n\nfbshipit-source-id: 7da1adcfc8c380e4ffe05b8769fc2221ad17a225"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/b8cea7cc279fe609de85b7ce4f50d4ff4f90047f", "message": "VersionBuilder: Erase with iterators for better performance\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/3007\n\nDifferential Revision: D6077701\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: a6fd5b8a23f4feb1660b9ce027f651a7e90352b3"}], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/1702", "title": "Set status to NoSpace when max allowed space was reached", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "newpoo": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/204af1ecccb6ed8110ee04cf9130594cfcb3af27", "message": "add WriteBatch::WriteBatch(std::string&&)\n\nSummary:\nto save a string copy for some use cases.\n\nThe change is pretty straightforward, please feel free to let me know if you want to suggest any tests for it.\nCloses https://github.com/facebook/rocksdb/pull/3349\n\nDifferential Revision: D6706828\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 873ce4442937bdc030b395c7f99228eda7f59eb7"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/19351323", "body": "Is this a typo? I couldn't find any rindex() function in std.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19351323/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19804806", "body": "I guess env_hdfs.cc is not compiled with the default rocksdb setting. We need \"USE_HDFS=1\" to trigger this compilation error.\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19804806/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19804884", "body": "FWIW: We used \"sed -i -- 's/std::rindex/rindex/g' ./util/env_hdfs.cc\" to get around this error.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19804884/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23835844", "body": "Why do we want to fadvise away the page cache when direct io is not used?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23835844/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "maysamyabandeh": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/00b33c2474ef5fc07f10b7bf4c5684eebe4436c6", "message": "WritePrepared Txn: address some pending TODOs\n\nSummary:\nThis patch addresses a couple of minor TODOs for WritePrepared Txn such as double checking some assert statements at runtime as well, skip extra AddPrepared in non-2pc transactions, and safety check for infinite loops.\nCloses https://github.com/facebook/rocksdb/pull/3302\n\nDifferential Revision: D6617002\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: ef6673c139cb49f64c0879508d2f573b78609aca"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/1c9ada59ccbf000f8abd97a2efbb19ed2008b34c", "message": "Remove assert(s.ok()) from ::DeleteFile\n\nSummary:\nDestroyDB that is used in tests loops over the files returned by ::GetChildren and delete them one by one. Such files might be already deleted in the file system (during DeleteObsoleteFileImpl for example) but will get actually deleted with a delay sometimes before ::DeleteFile is called on the file name. We have some test failures where FaultInjectionTestEnv::DeleteFile fails on assert(s.ok()) during DestroyDB. This patch removes the assert statement to fix that.\nCloses https://github.com/facebook/rocksdb/pull/3324\n\nDifferential Revision: D6659545\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 4c9552fbcd494dcf3e61d475c11fc965c4388b2c"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/02a2c117324348f241db9452c51bab8134c97ead", "message": "Blog post for WritePrepared Txn\n\nSummary:\nBlog post to introduce the next generation of transaction engine at RocksDB.\nCloses https://github.com/facebook/rocksdb/pull/3296\n\nDifferential Revision: D6612932\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 5bfa91ce84e937f5e4346bbda5a4725d0a7fd131"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/0ef3fdd73272c01d6a843cc7a8675187bb40f1ff", "message": "Disable need_log_sync on bg err\n\nSummary:\nWhen there is a background error PreprocessWrite returns without marking the logs synced. If we keep need_log_sync to true, it would try to sync them at the end, which would break the logic. The patch would unset need_log_sync if the logs end up not being marked for sync in PreprocessWrite.\nCloses https://github.com/facebook/rocksdb/pull/3293\n\nDifferential Revision: D6602347\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 37ee04209e8dcfd78de891654ce50d0954abeb38"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/0faa026db66cac0b2aae85f84b04395ba3c7a376", "message": "WritePrepared Txn: make buck tests parallel\n\nSummary:\nThe TSAN version of tests could take quite long. Make the buck tests parallel to avoid timeouts.\nCloses https://github.com/facebook/rocksdb/pull/3280\n\nDifferential Revision: D6581594\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 3f8476d8c69f0183e394fa8a2089dd8d4e90c90c"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/78c2eedb4fde8801922ad020061a7503070bf25c", "message": "fix release order in validateNumberOfEntries\n\nSummary:\nScopedArenaIterator should be defined after range_del_agg so that it destructs the assigned iterator, which depends on range_del_agg, before it range_del_agg is already destructed.\nCloses https://github.com/facebook/rocksdb/pull/3281\n\nDifferential Revision: D6592332\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 89a15d8ed13d0fc856b0c47dce3d91778738dbac"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/a6d3c762df2dc40c913ee6c460ad1b7d2f7363bd", "message": "WritePrepared Txn: non-2pc write in one round\n\nSummary:\nCurrently non-2pc writes do the 2nd dummy write to actually commit the transaction. This was necessary to ensure that publishing the commit sequence number will be done only from one queue (the queue that does not write to memtable). This is however not necessary when we have only one write queue, which is actually the setup that would be used by non-2pc writes. This patch eliminates the 2nd write when two_write_queues are disabled by updating the commit map in the 1st write.\nCloses https://github.com/facebook/rocksdb/pull/3277\n\nDifferential Revision: D6575392\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 8ab458f7ca506905962f9166026b2ec81e749c46"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/95583e1532118b070cfcc548b09007f2368d5eb1", "message": "db_stress: skip snapshot check if cf is dropped\n\nSummary:\nWe added a new verification that ensures a value that snapshot reads when is released is the same as when it was created. This test however fails when the cf is dropped in between. The patch skips the tests if that was the case.\nCloses https://github.com/facebook/rocksdb/pull/3279\n\nDifferential Revision: D6581584\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: afe37d371c0f91818d2e279b3949b810e112e8eb"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/cd2e5cae7f134aae9ded3403e7f4ed5f1a23abd7", "message": "WritePrepared Txn: make db_stress transactional\n\nSummary:\nAdd \"--use_txn\" option to use transactional API in db_stress, default being WRITE_PREPARED policy, which is the main intention of modifying db_stress. It also extend the existing snapshots to verify that before releasing a snapshot a read from it returns the same value as before.\nCloses https://github.com/facebook/rocksdb/pull/3243\n\nDifferential Revision: D6556912\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 1ae31465be362d44bd06e635e2e9e49a1da11268"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/546a63272fe77ffdf91f230283fd248af14798f9", "message": "disableWAL with WriteImplWALOnly\n\nSummary:\nCurrently WriteImplWALOnly simply returns when disableWAL is set. This is an incorrect behavior since it does not allocated the sequence number, which is a side-effect of writing to the WAL. This patch fixes the issue.\nCloses https://github.com/facebook/rocksdb/pull/3262\n\nDifferential Revision: D6550974\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 745a83ae8f04e7ca6c8ffb247d6ef16c287c52e7"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/35dfbd58ddbe46357edc038ff66ebfa238ccb250", "message": "WritePrepared Txn: GC old_commit_map_\n\nSummary:\nGarbage collect entries from old_commit_map_ when the corresponding snapshots are released.\nCloses https://github.com/facebook/rocksdb/pull/3247\n\nDifferential Revision: D6528478\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 15d1566d85d4ac07036bc0dc47418f6c3228d4bf"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/36911f55dd4e75b05865d17e7b234454429bdee2", "message": "WritePrepared Txn: stress test\n\nSummary:\nAugment the existing MySQLStyleTransactionTest to check for more core case scenarios. The changes showed effective in revealing the bugs reported in https://github.com/facebook/rocksdb/pull/3205 and https://github.com/facebook/rocksdb/pull/3101\nCloses https://github.com/facebook/rocksdb/pull/3222\n\nDifferential Revision: D6476862\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 5068497702d67ffc206a58ed96f8578fbb510137"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/18dcf7f98d909cfd2093acbba907c41c2f6a8e61", "message": "WritePrepared Txn: PreReleaseCallback\n\nSummary:\nAdd PreReleaseCallback to be called at the end of WriteImpl but before publishing the sequence number. The callback is used in WritePrepareTxn to i) update the commit map, ii) update the last published sequence number in the 2nd write queue. It also ensures that all the commits will go to the 2nd queue.\nThese changes will ensure that the commit map is updated before the sequence number is published and used by reading snapshots. If we use two write queues, the snapshots will use the seq number published by the 2nd queue. If we use one write queue (the default, the snapshots will use the last seq number in the memtable, which also indicates the last published seq number.\nCloses https://github.com/facebook/rocksdb/pull/3205\n\nDifferential Revision: D6438959\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: f8b6c434e94bc5f5ab9cb696879d4c23e2577ab9"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/b72b3c6f515674b7090b1f631ce7b2420bbe7d95", "message": "WritePrepared Txn: Add MultiGet to DB\n\nSummary:\nThis patch implements MultiGet API for WritePreparedTxnDB and update the existing unit tests.\nCloses https://github.com/facebook/rocksdb/pull/3196\n\nDifferential Revision: D6401493\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 51501a1e32645fc2da8680e77a50035f6530f2cc"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/e59cb2a19b2971707d75552ae517586ca0785a6a", "message": "Add seq_per_batch to WriteWithCallbackTest\n\nSummary:\nAugment WriteWithCallbackTest to also test when seq_per_batch is true.\nCloses https://github.com/facebook/rocksdb/pull/3195\n\nDifferential Revision: D6398143\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 7bc4218609355ec20fed25df426a8455ec2390d3"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/30285ee31c31e873136eadaf037931200ab1e3bc", "message": "Fix calculating filter partition target size\n\nSummary:\nblock_size_deviation is in percentage while the partition size is in bytes. The current code fails to take that into account resulting into very large target size for filter partitions.\nCloses https://github.com/facebook/rocksdb/pull/3187\n\nDifferential Revision: D6376069\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 276546fc68f50e0da32c462abb46f6cf676db9b2"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/0213990b3ab030a24587116e65a9c915d14b93ef", "message": "Move static variables out of the header file\n\nSummary:\nStatic variables in header files will be instantiated in every file that includes the header file. This patch moves some of them from options_helper.h to its .cc files. It also moves the static variable out of the offset_of since the template function could also lead to multiple instantiation perhaps due to inlining.\n\nFixes https://github.com/facebook/rocksdb/issues/3176\nCloses https://github.com/facebook/rocksdb/pull/3178\n\nDifferential Revision: D6363794\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: d0a07f061b4d992ab4e0de2706e622131d258fdd"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/54b43563be4add99fcc49329769fcb715c26b8b9", "message": "WritePrepared Txn: Refactoring WriteCallback\n\nSummary:\nRefactor the logic around WriteCallback in the write path to clarify when and how exactly we advance the sequence number and making sure it is consistent across the code.\nCloses https://github.com/facebook/rocksdb/pull/3168\n\nDifferential Revision: D6324312\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 9a34f479561fdb2a5d01ef6d37a28908d03bbe33"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/53863b76f90182c4df7f24ffdbb1daace8a91637", "message": "WritePrepared Txn: fix bug with Rollback seq\n\nSummary:\nThe sequence number was not properly advanced after a rollback marker. The patch extends the existing unit tests to detect the bug and also fixes it.\nCloses https://github.com/facebook/rocksdb/pull/3157\n\nDifferential Revision: D6304291\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 1b519c44a5371b802da49c9e32bd00087a8da401"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/175d5d6a9e7c6138e678429618759504f7f7e6e4", "message": "Properly destruct rebuilding_trx_\n\nSummary:\nWhen testing rebuilding_trx_ in MemTableInserter might still be set before the tests finishes which would cause ASAN alarms for leaks. This patch deletes the pointers in MemTableInserter destructor.\nCloses https://github.com/facebook/rocksdb/pull/3162\n\nDifferential Revision: D6317113\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: a68be70709a4fff7ac2b768660119311968f9c21"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/2515266725167e2b5002c89867159eb8b977c1f6", "message": "WritePrepared Txn: Refactoring TrackKeys\n\nSummary:\nThis patch clarifies and refactors the logic around tracked keys in transactions.\nCloses https://github.com/facebook/rocksdb/pull/3140\n\nDifferential Revision: D6290258\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 03b50646264cbcc550813c060b180fc7451a55c1"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/2edc92bc28fabd4c1cf6f6869dddd8ddf4ee186a", "message": "WritePrepared Txn: cross-compatibility test\n\nSummary:\nAdd tests to ensure that WritePrepared and WriteCommitted policies are cross compatible when the db WAL is empty. This is important when the admin want to switch between the policies. In such case, before the switch the admin needs to empty the WAL by i) committing/rollbacking all the pending transactions, ii) FlushMemTables\nCloses https://github.com/facebook/rocksdb/pull/3118\n\nDifferential Revision: D6227247\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: bcde3d92c1e89cda3b9cfa69f6a20af5d8993db7"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/857adf388fd1de81b8749bf1e5fe20edf6f8a8c8", "message": "WritePrepared Txn: Refactor conf params\n\nSummary:\nSummary of changes:\n- Move seq_per_batch out of Options\n- Rename concurrent_prepare to two_write_queues\n- Add allocate_seq_only_for_data_\nCloses https://github.com/facebook/rocksdb/pull/3136\n\nDifferential Revision: D6304458\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 08e685bfa82bbc41b5b1c5eb7040a8ca6e05e58c"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/25fbd9a99666e5e5ecd842bdfd7154f8d9839485", "message": "Remove the experimental notes about partitioning\n\nSummary:\nThis patch will remove the existing comments that declare partitioning indexes and filters as experimental.\nCloses https://github.com/facebook/rocksdb/pull/3115\n\nDifferential Revision: D6222227\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 6179ec43b22c518494051b674d91c9e1b54d4ac0"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/60d83df23dcb791ef729fe661daa816ef8ddda32", "message": "WritePrepared Txn: Move DB class to its own file\n\nSummary:\nMove  WritePreparedTxnDB from pessimistic_transaction_db.h to its own header, write_prepared_txn_db.h\nCloses https://github.com/facebook/rocksdb/pull/3114\n\nDifferential Revision: D6220987\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 18893fb4fdc6b809fe117dabb544080f9b4a301b"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/02693f64fcfaf9af84e04190c38fe3604879d89a", "message": "WritePrepared Txn: ValidateSnapshot\n\nSummary:\nImplements ValidateSnapshot for WritePrepared txns and also adds a unit test to clarify the contract of this function.\nCloses https://github.com/facebook/rocksdb/pull/3101\n\nDifferential Revision: D6199405\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: ace509934c307ea5d26f4bbac5f836d7c80fd240"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/17731a43a6e6a212097c1d83392f81d310ffe2fa", "message": "WritePrepared Txn: Optimize for recoverable state\n\nSummary:\nGetCommitTimeWriteBatch is currently used to store some state as part of commit in 2PC. In MyRocks it is specifically used to store some data that would be needed only during recovery. So it is not need to be stored in memtable right after each commit.\nThis patch enables an optimization to write the GetCommitTimeWriteBatch only to the WAL. The batch will be written to memtable during recovery when the WAL is replayed. To cover the case when WAL is deleted after memtable flush, the batch is also buffered and written to memtable right before each memtable flush.\nCloses https://github.com/facebook/rocksdb/pull/3071\n\nDifferential Revision: D6148023\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 2d09bae5565abe2017c0327421010d5c0d55eaa7"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/c1cf94c7877ee11ed6ec5baae178691d3b10ace1", "message": "WritePrepared Txn: sort indexes before batch collapse\n\nSummary:\nThe collapse of duplicate keys in write batch needs to sort the indexes of duplicate keys since it only checks the index in the batch with the head of the list of duplicate keys.\nCloses https://github.com/facebook/rocksdb/pull/3093\n\nDifferential Revision: D6186800\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: abc9ae8c2f1840445a5584f925cf86ecc6f37154"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/63822eb761a1c45d255e5676512153d213698b7c", "message": "Enable two write queues for transactions\n\nSummary:\nEnable concurrent_prepare flag for WritePrepared transactions and extend the existing transaction tests with this config.\nCloses https://github.com/facebook/rocksdb/pull/3046\n\nDifferential Revision: D6106534\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 88c8d21d45bc492beb0a131caea84a2ac5e7d38c"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/3ef55d2c7c0e40422b52e53e4e3e958580f61d09", "message": "Split CompactionFilterWithValueChange\n\nSummary:\nThe test currently times out when it is run under tsan. This patch split it into 4 tests.\nCloses https://github.com/facebook/rocksdb/pull/3047\n\nDifferential Revision: D6106515\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 03a28cdf8b1c097be2361b1b0cc3dc1acf2b5d63"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/7e3823898177af96f0580f2d28ee3d3a5037c767", "message": "WritePrepared Txn: Disable GC during recovery\n\nSummary:\nDisables GC during recovery of a WritePrepared txn db to avoid GCing uncommitted key values.\nCloses https://github.com/facebook/rocksdb/pull/2980\n\nDifferential Revision: D6000191\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: fc4d522c643d24ebf043f811fe4ecd0dd0294675"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/ec6c5383d0f2eb7aef2fd37195e77f5ed3520d08", "message": "WritePrepared Txn: end-to-end tests\n\nSummary:\nEnable WritePrepared policy for existing transaction tests.\nCloses https://github.com/facebook/rocksdb/pull/2972\n\nDifferential Revision: D5993614\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: d1eb53e2920c4e2a56434bb001231c98426f3509"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/4e3c3d8c6a69ab3ebf91b7a7292dbb53cd49b9f9", "message": "WritePrepared Txn: duplicate keys\n\nSummary:\nWith WriteCommitted, when the write batch has duplicate keys, the txn db simply inserts them to the db with different seq numbers and let the db ignore/merge the duplicate values at the read time. With WritePrepared all the entries of the batch are inserted with the same seq number which prevents us from benefiting from this simple solution.\n\nThis patch applies a hackish solution to unblock the end-to-end testing. The hack is to be replaced with a proper solution soon. The patch simply detects the duplicate key insertions, and mark the previous one as obsolete. Then before writing to the db it rewrites the batch eliminating the obsolete keys. This would incur a memcpy cost. Furthermore handing duplicate merge would require to do FullMerge instead of simply ignoring the previous value, which is not handled by this patch.\nCloses https://github.com/facebook/rocksdb/pull/2969\n\nDifferential Revision: D5976337\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 114e65b66f137d8454ff2d1d782b8c05da95f989"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/283d60761ea64111de8d494db4c08f544b18b5c1", "message": "fix valgrind leak report in unit test\n\nSummary:\nI cannot locally reproduce the valgrind leak report but based on my code inspection not deleting txn1 might be the reason.\n```\n==197848== 2,990 (544 direct, 2,446 indirect) bytes in 1 blocks are definitely lost in loss record 15 of 16\n==197848==    at 0x4C2D06F: operator new(unsigned long) (in /usr/local/fbcode/gcc-5-glibc-2.23/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==197848==    by 0x7D5B31: rocksdb::WritePreparedTxnDB::BeginTransaction(rocksdb::WriteOptions const&, rocksdb::TransactionOptions const&, rocksdb::Transaction*) (pessimistic_transaction_db.cc:173)\n==197848==    by 0x7D80C1: rocksdb::PessimisticTransactionDB::Initialize(std::vector<unsigned long, std::allocator<unsigned long> > const&, std::vector<rocksdb::ColumnFamilyHandle*, std::allocator<rocksdb::ColumnFamilyHandle*> > const&) (pessimistic_transaction_db.cc:115)\n==197848==    by 0x7DC42F: rocksdb::WritePreparedTxnDB::Initialize(std::vector<unsigned long, std::allocator<unsigned long> > const&, std::vector<rocksdb::ColumnFamilyHandle*, std::allocator<rocksdb::ColumnFamilyHandle*> > const&) (pessimistic_transaction_db.cc:151)\n==197848==    by 0x7D8CA0: rocksdb::TransactionDB::WrapDB(rocksdb::DB*, rocksdb::TransactionDBOptions const&, std::vector<unsigned long, std::allocator<unsigned long> > const&, std::vector<rocksdb::ColumnFamilyHandle*, std::allocator<rocksdb::ColumnFamilyHandle*> > const&, rocksdb::TransactionDB**) (pessimistic_transaction_db.cc:275)\n==197848==    by 0x7D9F26: rocksdb::TransactionDB::Open(rocksdb::DBOptions const&, rocksdb::TransactionDBOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<rocksdb::ColumnFamilyDescriptor, std::allocator<rocksdb::ColumnFamilyDescriptor> > const&, std::vector<rocksdb::ColumnFamilyHandle*, std::allocator<rocksdb::ColumnFamilyHandle*> >*, rocksdb::TransactionDB**) (pessimistic_transaction_db.cc:227)\n==197848==    by 0x7DB349: rocksdb::TransactionDB::Open(rocksdb::Options const&, rocksdb::TransactionDBOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, rocksdb::TransactionDB**) (pessimistic_transaction_db.cc:198)\n==197848==    by 0x52ABD2: rocksdb::TransactionTest::ReOpenNoDelete() (transaction_test.h:87)\n==197848==    by 0x51F7B8: rocksdb::WritePreparedTransactionTest_BasicRecoveryTest_Test::TestBody() (write_prepared_transaction_test.cc:843)\n==197848==    by 0x857557: HandleSehExceptionsInMethodIfSupported<testing::Test, void> (gtest-all.cc:3824)\n==197848==    by 0x857557: void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (gtest-all.cc:3860)\n==197848==    by 0x84E7EB: testing::Test::Run() [clone .part.485] (gtest-all.cc:3897)\n==197848==    by 0x84E9BC: Run (gtest-all.cc:3888)\n==197848==    by 0x84E9BC: testing::TestInfo::Run() [clone .part.486] (gtest-all.cc:4072)\n```\nCloses https://github.com/facebook/rocksdb/pull/2963\n\nDifferential Revision: D5968856\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 2ac512bbcad37dc8eeeffe4f363978913354180c"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/d27258d3a6865f34527e633ff0ac60d888706dff", "message": "WritePrepared Txn: Rollback\n\nSummary:\nImplement the rollback of WritePrepared txns. For each modified value, it reads the value before the txn and write it back. This would cancel out the effect of transaction. It also remove the rolled back txn from prepared heap.\nCloses https://github.com/facebook/rocksdb/pull/2946\n\nDifferential Revision: D5937575\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: a6d3c47f44db3729f44b287a80f97d08dc4e888d"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/2b22baf3044c417832d2352ee968b1eaf65cbb00", "message": "Add a template for issues\n\nSummary:\nThis template reminds the users to use issues only for bug reports. The template is written according to the github guidelines at https://help.github.com/articles/creating-an-issue-template-for-your-repository/\nCloses https://github.com/facebook/rocksdb/pull/2948\n\nDifferential Revision: D5943558\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: c83b5d211ea8e334107141967689b2f0c453bbc9"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/ab0542f5ec6e7c7e405267eaa2e2a603a77d570b", "message": "Fix for when block.cache_handle is nullptr\n\nSummary:\nWhen using with compressed cache it is possible that the status is ok but the block is not actually added to the block cache. The patch takes this case into account.\nCloses https://github.com/facebook/rocksdb/pull/2945\n\nDifferential Revision: D5937613\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 5428cf1115e5046b3d01ab78d26cb181122af4c6"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/385049baf29d2c1ce7c96f5db2b0fc497639b1b1", "message": "WritePrepared Txn: Recovery\n\nSummary:\nRecover txns from the WAL. Also added some unit tests.\nCloses https://github.com/facebook/rocksdb/pull/2901\n\nDifferential Revision: D5859596\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 6424967b231388093b4effffe0a3b1b7ec8caeb0"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/c70586621c400d4df3d3620c4c0a4176f4ed7847", "message": "Blog post for 5.8 release\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/2942\n\nDifferential Revision: D5932858\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: e11f52a0b08d65149bb49d99d1dbc82cb5a96fa0"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/aa67bae6cfff29114d543a8ffea3d5eddea06b09", "message": "Break down PinnedDataIteratorRandomized\n\nSummary:\nIts timing out under tsan.\nCloses https://github.com/facebook/rocksdb/pull/2928\n\nDifferential Revision: D5911766\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 2faacc07752ac8713a3a2abb5a4c4b7ae3bdf208"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/60beefd6e043967165af2d2d8944fefa580a2383", "message": "WritePrepared Txn: Advance seq one per batch\n\nSummary:\nBy default the seq number in DB is increased once per written key. WritePrepared txns requires the seq to be increased once per the entire batch so that the seq would be used as the prepare timestamp by which the transaction is identified. Also we need to increase seq for the commit marker since it would give a unique id to the commit timestamp of transactions.\n\nTwo unit tests are added to verify our understanding of how the seq should be increased. The recovery path requires much more work and is left to another patch.\nCloses https://github.com/facebook/rocksdb/pull/2885\n\nDifferential Revision: D5837843\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: a08960b93d727e1cf438c254d0c2636fb133cc1c"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/c57050b7707fd5bb5b4fe2bf9bc60650dbb0ba11", "message": "Use the default copy constructor in Options\n\nSummary:\nOur current implementation of (semi-)copy constructor of DBOptions and ColumnFamilyOptions seems to intend value by value copy, which is what the default copy constructor does anyway. Moreover not using the default constructor has the risk of forgetting to add newly added options.\n\nAs an example, allow_2pc seems to be forgotten in the copy constructor which was causing one of the unit tests not seeing its effect.\nCloses https://github.com/facebook/rocksdb/pull/2888\n\nDifferential Revision: D5846368\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 1ee92a2aeae93886754b7bc039c3411ea2458683"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/21568222", "body": "You are right. I must have messed it up when wrapping up the patch. Will correct it.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21568222/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21569403", "body": "In Step 3 the only change is that the write to memtable are disabled. Can you explain how the writes to WAL becomes twice by this change?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21569403/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21569795", "body": "True. Making Prepare more expensive was in Step 4.1. Can you explain more your changes for making Commit lighter in Step 3? Currently the write to mem_table is disabled so ::WriteImpl will just write to the WAL the commit marker. Can it be any lighter?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21569795/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21609290", "body": "Let me remind that this is a very rough estimation. A theory that we want to validate is that whether the major cost comes from writing to memtable or not. You mentioned that the experiments are quite fast (1h). How about we do the experiment with the current patch and decide the next move based on the results?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21609290/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23356943", "body": "TsTcSync is called before transaction starts, rather than before each read.\r\nGood point that we are implicitly syncing via accessing the seq number. ", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23356943/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23425073", "body": "I checked the binary the function call was still there. I also checked the the function implementation and it does not seem to be optimized out:\r\n\r\n```\r\n0000000000bef750 <_ZN7rocksdb6DBImpl8TsTcReadEm>:\r\n  bef750:       48 89 f0                mov    %rsi,%rax\r\n  bef753:       48 ba db 34 b6 d7 82    movabs $0x431bde82d7b634db,%rdx\r\n  bef75a:       de 1b 43\r\n  bef75d:       48 f7 e2                mul    %rdx\r\n  bef760:       48 c1 ea 12             shr    $0x12,%rdx\r\n  bef764:       48 69 d2 40 42 0f 00    imul   $0xf4240,%rdx,%rdx\r\n  bef76b:       48 29 d6                sub    %rdx,%rsi\r\n  bef76e:       48 c1 e6 03             shl    $0x3,%rsi\r\n  bef772:       48 8b 44 37 08          mov    0x8(%rdi,%rsi,1),%rax\r\n  bef777:       c3                      retq\r\n  bef778:       0f 1f 84 00 00 00 00    nopl   0x0(%rax,%rax,1)\r\n  bef77f:       00\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23425073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23425182", "body": "If here we do a memory_order_release store to `ts_tc_map_[seq % 1000000]` then the reader would have to do a memory_order_acquire load on all the entries in ts_tc_map_ to guarantee that it has all such updates before its transaction starts. Or perhaps I misunderstood your comment?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23425182/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23425400", "body": "Yeah the function call is there too.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23425400/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23425463", "body": "```\r\n$ objdump -d mysqld.twobatch.rebased.enabled.mematprepare.tstc.iter.relaxed.final | grep _ZN7rocksdb6DBImpl8TsTcReadEm\r\n0000000000bef750 <_ZN7rocksdb6DBImpl8TsTcReadEm>:\r\n  c3e5b7:       e8 94 11 fb ff          callq  bef750 <_ZN7rocksdb6DBImpl8TsTcReadEm>\r\n  c3ea31:       e8 1a 0d fb ff          callq  bef750 <_ZN7rocksdb6DBImpl8TsTcReadEm>\r\n  c40a9a:       e8 b1 ec fa ff          callq  bef750 <_ZN7rocksdb6DBImpl8TsTcReadEm>\r\n  c429fa:       e8 51 cd fa ff          callq  bef750 <_ZN7rocksdb6DBImpl8TsTcReadEm>\r\n```", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23425463/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23425614", "body": "And just to be 100% sure here is the binary code for this specific line:\r\n\r\n```\r\n   0x0000000000c40a7d <+493>:   jb     0xc40db0 <rocksdb::DBIter::FindNextUserEntryInternal(bool, bool)+1312>\r\n   0x0000000000c40a83 <+499>:   mov    0x1a8(%rbx),%rdi\r\n   0x0000000000c40a8a <+506>:   mov    0x50(%rsp),%rsi\r\n   0x0000000000c40a8f <+511>:   add    $0x1,%rax\r\n   0x0000000000c40a93 <+515>:   mov    %rax,0xf8(%rbx)\r\n   0x0000000000c40a9a <+522>:   callq  0xbef750 <rocksdb::DBImpl::TsTcRead(unsigned long)>\r\n   0x0000000000c40a9f <+527>:   mov    0x60(%rbx),%rax\r\n   0x0000000000c40aa3 <+531>:   cmp    %rax,0x50(%rsp)\r\n   0x0000000000c40aa8 <+536>:   ja     0xc40c70 <rocksdb::DBIter::FindNextUserEntryInternal(bool, bool)+992>\r\n```", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23425614/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "chrislusf": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/24e2c1640d9808a9dc18b770427aa024670f5edf", "message": "add support for allow_ingest_behind in C API\n\nSummary:\nhttps://github.com/facebook/rocksdb/wiki/Creating-and-Ingesting-SST-files\n\nNeed to expose these functions in the C API to be used by Go bindings.\nCloses https://github.com/facebook/rocksdb/pull/3011\n\nDifferential Revision: D6679563\n\nPulled By: sagar0\n\nfbshipit-source-id: 536f844ddaeb0172c6d7e416d2a75e8f9e57c8ef"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "burtonli": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/b5c99cc908a7e34fa65d588b2706c33000786935", "message": "Disable onboard cache for compaction output\n\nSummary:\nFILE_FLAG_WRITE_THROUGH is for disabling device on-board cache in windows API, which should be disabled if user doesn't need system cache.\nThere was a perf issue related with this, we found during memtable flush, the high percentile latency jumps significantly. During profiling, we found those high latency (P99.9) read requests got queue-jumped by write requests from memtable flush and takes 80ms or even more time to wait, even when SSD overall IO throughput is relatively low.\n\nAfter enabling FILE_FLAG_WRITE_THROUGH, we rerun the test found high percentile latency drops a lot without observable impact on writes.\n\nScenario 1: 40MB/s + 40MB/s  R/W compaction throughput\n\n\u00a0Original | FILE_FLAG_WRITE_THROUGH | Percentage reduction\n---------------------------------------------------------------\nP99.9 | 56.897 ms | 35.593 ms | -37.4%\nP99 | 3.905 ms | 3.896 ms | -2.8%\n\nScenario 2:  14MB/s + 14MB/s R/W compaction throughput, cohosted with 100+ other rocksdb instances have manually triggered memtable flush operations (memtable is tiny), creating a lot of randomized the small file writes operations during test.\n\nOriginal | FILE_FLAG_WRITE_THROUGH | Percentage reduction\n---------------------------------------------------------------\nP99.9 | 86.227   ms | 50.436 ms | -41.5%\nP99 | 8.415   ms | 3.356 ms | -60.1%\nCloses https://github.com/facebook/rocksdb/pull/3225\n\nDifferential Revision: D6624174\n\nPulled By: miasantreble\n\nfbshipit-source-id: 321b86aee9d74470840c70e5d0d4fa9880660a91"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wouterbeek": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/58b841b3568aec6f2f7489720135074dd709df7b", "message": "FIXED: string buffers potentially too small to fit formatted write\n\nSummary:\nThis fixes the following warnings when compiled with GCC7:\n\nutil/transaction_test_util.cc: In static member function \u2018static rocksdb::Status rocksdb::RandomTransactionInserter::DBGet(rocksdb::DB*, rocksdb::Transaction*, rocksdb::ReadOptions&, uint16_t, uint64_t, bool, uint64_t*, std::__cxx11::string*, bool*)\u2019:\nutil/transaction_test_util.cc:75:8: error: \u2018snprintf\u2019 output may be truncated before the last format character [-Werror=format-truncation=]\n Status RandomTransactionInserter::DBGet(\n        ^~~~~~~~~~~~~~~~~~~~~~~~~\nutil/transaction_test_util.cc:84:11: note: \u2018snprintf\u2019 output between 5 and 6 bytes into a destination of size 5\n   snprintf(prefix_buf, sizeof(prefix_buf), \"%.4u\", set_i + 1);\n   ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nutil/transaction_test_util.cc: In static member function \u2018static rocksdb::Status rocksdb::RandomTransactionInserter::Verify(rocksdb::DB*, uint16_t, uint64_t, bool, rocksdb::Random64*)\u2019:\nutil/transaction_test_util.cc:245:8: error: \u2018snprintf\u2019 output may be truncated before the last format character [-Werror=format-truncation=]\n Status RandomTransactionInserter::Verify(DB* db, uint16_t num_sets,\n        ^~~~~~~~~~~~~~~~~~~~~~~~~\nutil/transaction_test_util.cc:268:13: note: \u2018snprintf\u2019 output between 5 and 6 bytes into a destination of size 5\n     snprintf(prefix_buf, sizeof(prefix_buf), \"%.4u\", set_i + 1);\n     ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nCloses https://github.com/facebook/rocksdb/pull/3295\n\nDifferential Revision: D6609411\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 33f0add471056eb59db2f8bd4366e6dfbb1a187d"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yingsu00": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/f54d7f5feaff09b0e2bf749ecd77043bcd4141b2", "message": "Port 3 way SSE4.2 crc32c implementation from Folly\n\nSummary:\n**# Summary**\n\nRocksDB uses SSE crc32 intrinsics to calculate the crc32 values but it does it in single way fashion (not pipelined on single CPU core). Intel's whitepaper () published an algorithm that uses 3-way pipelining for the crc32 intrinsics, then use pclmulqdq intrinsic to combine the values. Because pclmulqdq has overhead on its own, this algorithm will show perf gains on buffers larger than 216 bytes, which makes RocksDB a perfect user, since most of the buffers RocksDB call crc32c on is over 4KB. Initial db_bench show tremendous CPU gain.\n\nThis change uses the 3-way SSE algorithm by default. The old SSE algorithm is now behind a compiler tag NO_THREEWAY_CRC32C. If user compiles the code with NO_THREEWAY_CRC32C=1 then the old SSE Crc32c algorithm would be used. If the server does not have SSE4.2 at the run time the slow way (Non SSE) will be used.\n\n**# Performance Test Results**\nWe ran the FillRandom and ReadRandom benchmarks in db_bench. ReadRandom is the point of interest here since it calculates the CRC32 for the in-mem buffers. We did 3 runs for each algorithm.\n\nBefore this change the CRC32 value computation takes about 11.5% of total CPU cost, and with the new 3-way algorithm it reduced to around 4.5%. The overall throughput also improved from 25.53MB/s to 27.63MB/s.\n\n1) ReadRandom in db_bench overall metrics\n\n    PER RUN\n    Algorithm | run | micros/op | ops/sec |Throughput (MB/s)\n    3-way      |  1   | 4.143   | 241387 | 26.7\n    3-way      |  2   | 3.775   | 264872 | 29.3\n    3-way      | 3    | 4.116   | 242929 | 26.9\n    FastCrc32c|1  | 4.037   | 247727 | 27.4\n    FastCrc32c|2  | 4.648   | 215166 | 23.8\n    FastCrc32c|3  | 4.352   | 229799 | 25.4\n\n     AVG\n    Algorithm     |    Average of micros/op |   Average of ops/sec |    Average of Throughput (MB/s)\n    3-way           |     4.01                               |      249,729                 |      27.63\n    FastCrc32c  |     4.35                              |     230,897                  |      25.53\n\n 2)   Crc32c computation CPU cost (inclusive samples percentage)\n    PER RUN\n    Implementation\u00a0| run |\u00a0 TotalSamples   |\u00a0Crc32c percentage\n    3-way \u00a0               |  1\u00a0 \u00a0 |\u00a0 4,572,250,000 | 4.37%\n    3-way \u00a0               |  2\u00a0 \u00a0 |\u00a0 3,779,250,000\u00a0| 4.62%\n    3-way \u00a0               |  3\u00a0 \u00a0 |\u00a0 4,129,500,000\u00a0| 4.48%\n    FastCrc32c\u00a0 \u00a0   \u00a0|  1\u00a0 \u00a0 |\u00a0 4,663,500,000\u00a0| 11.24%\n    FastCrc32c\u00a0 \u00a0   \u00a0|  2\u00a0 \u00a0 |\u00a0 4,047,500,000\u00a0| 12.34%\n    FastCrc32c\u00a0 \u00a0   \u00a0|  3\u00a0 \u00a0 |\u00a0 4,366,750,000\u00a0| 11.68%\n\n **# Test Plan**\n     make -j64 corruption_test && ./corruption_test\n      By default it uses 3-way SSE algorithm\n\n     NO_THREEWAY_CRC32C=1 make -j64 corruption_test && ./corruption_test\n\n    make clean && DEBUG_LEVEL=0 make -j64 db_bench\n    make clean && DEBUG_LEVEL=0 NO_THREEWAY_CRC32C=1 make -j64 db_bench\nCloses https://github.com/facebook/rocksdb/pull/3173\n\nDifferential Revision: D6330882\n\nPulled By: yingsu00\n\nfbshipit-source-id: 8ec3d89719533b63b536a736663ca6f0dd4482e9"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "guoxiao": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/aa6509d8e43f3e741d3b3805eb1dce7bbae32c82", "message": "Fix build for linux\n\nSummary:\n* Include `unistd.h` for `sleep(3)`\n* Include `sys/time.h` for `gettimeofday(3)`\n* Include `utils/random.h` for `Random64`\n\nError messages:\n\nutilities/persistent_cache/hash_table_bench.cc: In constructor \u2018rocksdb::HashTableBenchmark::HashTableBenchmark(rocksdb::HashTableImpl<long unsigned int, std::__cxx11::basic_string<char> >*, size_t, size_t, size_t, size_t)\u2019:\nutilities/persistent_cache/hash_table_bench.cc:76:28: error: \u2018sleep\u2019 was not declared in this scope\n       /* sleep override */ sleep(1);\n                            ^~~~~\nutilities/persistent_cache/hash_table_bench.cc:76:28: note: suggested alternative: \u2018strsep\u2019\n       /* sleep override */ sleep(1);\n                            ^~~~~\n                            strsep\nutilities/persistent_cache/hash_table_bench.cc: In member function \u2018void rocksdb::HashTableBenchmark::RunRead()\u2019:\nutilities/persistent_cache/hash_table_bench.cc:107:5: error: \u2018Random64\u2019 was not declared in this scope\n     Random64 rgen(time(nullptr));\n     ^~~~~~~~\nutilities/persistent_cache/hash_table_bench.cc:107:5: note: suggested alternative: \u2018random_r\u2019\n     Random64 rgen(time(nullptr));\n     ^~~~~~~~\n     random_r\nutilities/persistent_cache/hash_table_bench.cc:110:18: error: \u2018rgen\u2019 was not declared in this scope\n       size_t k = rgen.Next() % max_prepop_key;\n                  ^~~~\nutilities/persistent_cache/hash_table_bench.cc: In static member function \u2018static uint64_t rocksdb::HashTableBenchmark::NowInMillSec()\u2019:\nutilities/persistent_cache/hash_table_bench.cc:153:5: error: \u2018gettimeofday\u2019 was not declared in this scope\n     gettimeofday(&tv, /*tz=*/nullptr);\n     ^~~~~~~~~~~~\nmake[2]: *** [CMakeFiles/hash_table_bench.dir/build.make:63: CMakeFiles/hash_table_bench.dir/utilities/persistent_cache/hash_table_bench.cc.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:3346: CMakeFiles/hash_table_bench.dir/all] Error 2\nmake[1]: *** Waiting for unfinished jobs....\nCloses https://github.com/facebook/rocksdb/pull/3283\n\nDifferential Revision: D6594850\n\nPulled By: ajkr\n\nfbshipit-source-id: fd83957338c210cdfd253763347aafd39476824f"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Orvid": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/b4d88d712878de25a74b68c0475de473f76db462", "message": "Fix the build with MSVC 2017\n\nSummary:\nThere were a few places where MSVC's implicit truncation warnings were getting triggered, which was causing the MSVC build to fail due to warnings being treated as errors. This resolves the issues by making the truncations in some places explicit, and by making it so there are no truncations of literals.\n\nFixes #3239\nSupersedes #3259\nCloses https://github.com/facebook/rocksdb/pull/3273\n\nReviewed By: yiwu-arbug\n\nDifferential Revision: D6569204\n\nPulled By: Orvid\n\nfbshipit-source-id: c188cf1cf98d9acb6d94b71875041cc81f8ff088"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "miasantreble": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/51c2ea0feba305668f1cb9957adc29f42dce92f6", "message": "Reduce heavy hitter for Get operation\n\nSummary:\nThis PR addresses the following heavy hitters in `Get` operation by moving calls to `StatisticsImpl::recordTick` from `BlockBasedTable` to `Version::Get`\n\n- rocksdb.block.cache.bytes.write\n- rocksdb.block.cache.add\n- rocksdb.block.cache.data.miss\n- rocksdb.block.cache.data.bytes.insert\n- rocksdb.block.cache.data.add\n- rocksdb.block.cache.hit\n- rocksdb.block.cache.data.hit\n- rocksdb.block.cache.bytes.read\n\nThe db_bench statistics before and after the change are:\n\n|1GB block read|Children      |Self  |Command          |Shared Object        |Symbol|\n|---|---|---|---|---|---|\n|master:     |4.22%     |1.31%  |db_bench  |db_bench  |[.] rocksdb::StatisticsImpl::recordTick|\n|updated:    |0.51%     |0.21%  |db_bench  |db_bench  |[.] rocksdb::StatisticsImpl::recordTick|\n|     \t     |0.14%     |0.14%  |db_bench  |db_bench  |[.] rocksdb::GetContext::record_counters|\n\n|1MB block read|Children      |Self  |Command          |Shared Object        |Symbol|\n|---|---|---|---|---|---|\n|master:    |3.48%     |1.08%  |db_bench  |db_bench  |[.] rocksdb::StatisticsImpl::recordTick|\n|updated:    |0.80%     |0.31%  |db_bench  |db_bench  |[.] rocksdb::StatisticsImpl::recordTick|\n|    \t     |0.35%     |0.35%  |db_bench  |db_bench  |[.] rocksdb::GetContext::record_counters|\nCloses https://github.com/facebook/rocksdb/pull/3172\n\nDifferential Revision: D6330532\n\nPulled By: miasantreble\n\nfbshipit-source-id: 2b492959e00a3db29e9437ecdcc5e48ca4ec5741"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/bb5ed4b1d1b5fd891f857247496eb9bfc333a6ac", "message": "exclude DynamicUniversalCompactionOptions from ROCKSDB_LITE\n\nSummary:\nsince [SetOptions](https://github.com/facebook/rocksdb/blob/master/db/db_impl.cc#L494) is not supported in ROCKSDB_LITE\nRight now unit test under lite is broken\nCloses https://github.com/facebook/rocksdb/pull/3253\n\nDifferential Revision: D6539428\n\nPulled By: miasantreble\n\nfbshipit-source-id: 13172b8ecbd75682330726498ea198969bc3e637"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/fcc8a6574d76947b1967747eb523ad1e458d1a3c", "message": "Make Universal compaction options dynamic\n\nSummary:\nLet me know if more test coverage is needed\nCloses https://github.com/facebook/rocksdb/pull/3213\n\nDifferential Revision: D6457165\n\nPulled By: miasantreble\n\nfbshipit-source-id: 3f944abff28aa7775237f1c4f61c64ccbad4eea9"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/5fac4729ccaa72ff5500c525e7ff014523182588", "message": "make compaction_readahead_size_ thread safe\n\nSummary:\nthis should fix the failing tsan_check\nCloses https://github.com/facebook/rocksdb/pull/3192\n\nDifferential Revision: D6390004\n\nPulled By: miasantreble\n\nfbshipit-source-id: 6cadfc6f68febb1a77b0abcdb5416570dad926a5"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/a6c6b8b38c6a1b7f6ae566e5de02108d2e5ea84d", "message": "Revert \"No need for Restart Interval for meta blocks\"\n\nSummary:\nSee [issue 3169](https://github.com/facebook/rocksdb/issues/3169) for more information\n\nThis reverts commit 593d3de37171d99a761ce2ab34ffa12654acd055.\nCloses https://github.com/facebook/rocksdb/pull/3188\n\nDifferential Revision: D6379271\n\nPulled By: miasantreble\n\nfbshipit-source-id: 88f9ed67ba52237ad9b6f7251db83672b62d7537"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/32e31d49d1319cc20f2dc7c91a91db88c13f5c7e", "message": "Make DBOption compaction_readahead_size dynamic\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/3004\n\nDifferential Revision: D6056141\n\nPulled By: miasantreble\n\nfbshipit-source-id: 56df1630f464fd56b07d25d38161f699e0528b7f"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/30e4e01e051ceb3ddfb418e98db9783efc25210c", "message": "add missing else\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/3121\n\nDifferential Revision: D6229415\n\nPulled By: miasantreble\n\nfbshipit-source-id: 57c7ad2fddf5dd6b8d7e3aaf6f62348151327dfb"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/0f3b36964e14cc75ede68b8ab5d956de195742df", "message": "Fix counter for memtable updates\n\nSummary:\nRight now in `PutCFImpl` we always increment NUMBER_KEYS_UPDATED counter for both in-place update or insertion. This PR fixes this by using the correct counter for either case.\nCloses https://github.com/facebook/rocksdb/pull/2986\n\nDifferential Revision: D6016300\n\nPulled By: miasantreble\n\nfbshipit-source-id: 0aed327522e659450d533d1c47d3a9f568fac65d"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/e2548366e1d794ae73a68baa661d1d20609d0ede", "message": "add GetLiveFiles and GetLiveFilesMetaData for BlobDB\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/2976\n\nDifferential Revision: D5994759\n\nPulled By: miasantreble\n\nfbshipit-source-id: 985c31dccb957cb970c302f813cd07a1e8cb6438"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/593d3de37171d99a761ce2ab34ffa12654acd055", "message": "No need for Restart Interval for meta blocks\n\nSummary:\nIn SST files, restart interval helps us search in data blocks. However, some meta blocks will be read sequentially, so there's no need for restart points. Restart interval will introduce extra space in the block (https://github.com/facebook/rocksdb/blob/master/table/block_builder.cc#L80). We will see if we can remove this redundant space. (Maybe set restart interval to infinite.)\nCloses https://github.com/facebook/rocksdb/pull/2940\n\nDifferential Revision: D5930139\n\nPulled By: miasantreble\n\nfbshipit-source-id: 92b1b23c15cffa90378343ac846b713623b19c21"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/1d6700f9e6e7d066ef0f13e399613cd425a0088d", "message": "Add test kPointInTimeRecoveryCFConsistency\n\nSummary:\nContext/problem:\n\n- CFs may be flushed at different times\n- A WAL can only be deleted after all CFs have flushed beyond end of that WAL.\n- Point-in-time recovery might stop upon reaching the first corruption.\n- Some CFs may have already flushed beyond that point, while others haven't. We should fail the Open() instead of proceeding with inconsistent CFs.\nCloses https://github.com/facebook/rocksdb/pull/2900\n\nDifferential Revision: D5863281\n\nPulled By: miasantreble\n\nfbshipit-source-id: 180dbaf83d96c804cff49b3c406312a4ae61313e"}], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3333", "title": "Use block cache to track memory usage when ReadOptions.fill_cache=false", "body": "ReadOptions.fill_cache is set in compaction inputs and can be set by users in their queries too. It tells RocksDB not to put a data block used to block cache.\r\n\r\nThe memory used by the data block is, however, not trackable by users.\r\n\r\nTo make the system more manageable, we can cost the block to block cache while using it, and then release it after using.\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3272", "title": "[WIP] Skip obsolete files during recovery", "body": "Right now during crash recovery Rocksdb replays all logs found in order of log number and relies on `recovering_log_number_` to ignore duplicate replays. This PR simplifies the process by first collecting the smallest log number(`min_log_num`) among all column families from the manifest, and skipping all obsolete WAL logs whose log number is strictly smaller than `min_log_num`.\r\n\r\nThis change might cause some regression in txn related tests and will be updated soon.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/2977", "title": "fix condition for fsync wal before deletion", "body": "fixing the condition for fsync: it should be number of CF greater than 1 instead of 0", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "souvik1997": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/4bcb7fb148a8d80ccedbd7447b2361481a42676a", "message": "Update transaction_test_util.cc\n\nSummary:\nFixes a compile error on gcc 7.2.1 (-Werror=format-truncation=).\nCloses https://github.com/facebook/rocksdb/pull/3248\n\nDifferential Revision: D6546515\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: bd78cca63f2af376faceccb1838d2d4cc9208fef"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "flarnie": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/1b0c58d7e7a8904d552241dd35d5842ef0ff8593", "message": "Add Code of Conduct\n\nSummary:\nIn the past Facebook didn't promote including a Code of Conduct when creating new projects, and many projects skipped this important document. Let's fix it. :)\n\n**why make this change?:**\nFacebook Open Source provides a Code of Conduct statement for all\nprojects to follow, to promote a welcoming and safe open source community.\n\nExposing the COC via a separate markdown file is a standard being\npromoted by Github via the Community Profile in order to meet their Open\nSource Guide's recommended community standards.\n\nAs you can see, adding this file will improve [the rocksdb community profile](https://github.com/facebook/rocksdb/community)\nchecklist and increase the visibility of our COC.\n\n**test plan:**\nViewing it on my branch -\n<img width=\"1008\" alt=\"screen shot 2017-12-03 at 5 05 45 pm\" src=\"https://user-images.githubusercontent.com/1114467/33532198-66012a56-d84c-11e7-8fab-29ed410bd600.png\">\n<img width=\"1015\" alt=\"screen shot 2017-12-03 at 5 05 59 pm\" src=\"https://user-images.githubusercontent.com/1114467/33532199-661813d8-d84c-11e7-941e-94754dd481e5.png\">\n\n**issue:**\ninternal task t23481323\nCloses https://github.com/facebook/rocksdb/pull/3219\n\nReviewed By: yiwu-arbug\n\nDifferential Revision: D6494234\n\nPulled By: flarnie\n\nfbshipit-source-id: 55b59db335cc5546f3a1c968322b9281a3dc3aaf"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "adamnovak": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/a37d7345966d30c9a552537551818b9159054fce", "message": "Add ROCKSDB_DISABLE_* environment variables\n\nSummary:\nShould fix #3036.\nCloses https://github.com/facebook/rocksdb/pull/3042\n\nDifferential Revision: D6452921\n\nPulled By: sagar0\n\nfbshipit-source-id: eaf11e43fee1f8747006530cfc0c7a358f1c2f0f"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "a-robinson": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/4634c735a8bb4f83b8099928fb12b50ad8df7b88", "message": "Update DBOptions::IncreaseParallelism to use newer background settings\n\nSummary:\nThe Options header file recommends using max_background_jobs rather than\ndirectly setting max_background_compactions or max_background_flushes.\n\nI've personally seen a performance problem where stalls were happening\nbecause the one background flushing thread was blocked that was fixed\nby this change -\nhttps://github.com/cockroachdb/cockroach/issues/19699#issuecomment-347672485\nCloses https://github.com/facebook/rocksdb/pull/3208\n\nDifferential Revision: D6473178\n\nPulled By: ajkr\n\nfbshipit-source-id: 67c892ceb7b1909d251492640cb15a0f2262b7ed"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "benesch": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/3e40a5e8325905e32b15a646d914a571ff2633d2", "message": "add missing config checks to CMakeLists.txt\n\nSummary:\nBring CMakeLists.txt back up to parity with build_detect_platform.\nCloses https://github.com/facebook/rocksdb/pull/3211\n\nDifferential Revision: D6452908\n\nPulled By: ajkr\n\nfbshipit-source-id: 93f5f336ad7eff6ecf65dec47bfaf114dd24cfb2"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/7891af8b53be9a1a2181b46767561fad8e862a2e", "message": "expose a hook to skip tables during iteration\n\nSummary:\nAs discussed on the mailing list ([\"Skipping entire SSTs while iterating\"](https://groups.google.com/forum/#!topic/rocksdb/ujHCJVLrHlU)), this patch adds a `table_filter` to `ReadOptions` that allows specifying a callback to be executed during iteration before each table in the database is scanned. The callback is passed the table's properties; the table is scanned iff the callback returns true.\n\nThis can be used in conjunction with a `TablePropertiesCollector` to dramatically speed up scans by skipping tables that are known to contain irrelevant data for the scan at hand.\n\nWe're using this [downstream in CockroachDB](https://github.com/cockroachdb/cockroach/blob/master/pkg/storage/engine/db.cc#L2009-L2022) already. With this feature, under ideal conditions, we can reduce the time of an incremental backup in  from hours to seconds.\n\nFYI, the first commit in this PR fixes a segfault that I unfortunately have not figured out how to reproduce outside of CockroachDB. I'm hoping you accept it on the grounds that it is not correct to return 8-byte aligned memory from a call to `malloc` on some 64-bit platforms; one correct approach is to infer the necessary alignment from `std::max_align_t`, as done here. As noted in the first commit message, the bug is tickled by having a`std::function` in `struct ReadOptions`. That is, the following patch alone is enough to cause RocksDB to segfault when run from CockroachDB on Darwin.\n\n```diff\n --- a/include/rocksdb/options.h\n+++ b/include/rocksdb/options.h\n@@ -1546,6 +1546,13 @@ struct ReadOptions {\n   // Default: false\n   bool ignore_range_deletions;\n\n+  // A callback to determine whether relevant keys for this scan exist in a\n+  // given table based on the table's properties. The callback is passed the\n+  // properties of each table during iteration. If the callback returns false,\n+  // the table will not be scanned.\n+  // Default: empty (every table will be scanned)\n+  std::function<bool(const TableProperties&)> table_filter;\n+\n   ReadOptions();\n   ReadOptions(bool cksum, bool cache);\n };\n```\n\n/cc danhhz\nCloses https://github.com/facebook/rocksdb/pull/2265\n\nDifferential Revision: D5054262\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: dd6b28f2bba6cb8466250d8c5c542d3c92785476"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/c0208dffbefd8cafd398db8c0e08348037486b3d", "message": "arena: derive alignment unit from std::max_align_t\n\nSummary:\nAs raised in #2265, the arena allocator will return memory that is improperly aligned to store a `std::function` on macOS. Oddly, I'm unable to tickle this bug without adding a `std::function` field to `struct ReadOptions`\u2014but my proposal in #2265 does exactly that.\n\nIn any case, here's a simple reproduction. Apply this bogus patch to get a `std::function` into `struct ReadOptions`\n\n```\n --- a/include/rocksdb/options.h\n+++ b/include/rocksdb/options.h\n@@ -1035,6 +1035,8 @@ struct ReadOptions {\n   // Default: 0\n   uint64_t max_skippable_internal_keys;\n\n+  std::function<void()> foo;\n+\n   ReadOptions();\n   ReadOptions(bool cksum, bool cache);\n };\n```\n\nthen compile `db_properties_test` *with ubsan* and run `ReadLatencyHistogramByLevel`:\n\n```\n$ make COMPILE_WITH_UBSAN=1 db_properties_test\n$ ./db_properties_test --gtest_filter=DBPropertiesTest.ReadLatencyHistogramByLevel\n```\n\nubsan will complain about several misaligned accesses:\n\n```\nNote: Google Test filter = DBPropertiesTest.ReadLatencyHistogramByLevel\n[==========] Running 1 test from 1 test case.\n[----------] Global test environment set-up.\n[----------] 1 test from DBPropertiesTest\n[ RUN      ] DBPropertiesTest.ReadLatencyHistogramByLevel\nutil/coding.h:372:12: runtime error: load of misaligned address 0x00010d85516c for type 'const unsigned long', which requires 8 byte alignment\n0x00010d85516c: note: pointer points here\n  01 00 34 57 00 00 00 00  02 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  78 24 82 0a 01 00 00 00\n              ^\nutil/coding.h:362:3: runtime error: store to misaligned address 0x7fff5733fac4 for type 'unsigned long', which requires 8 byte alignment\n0x7fff5733fac4: note: pointer points here\n  01 00 00 00 00 00 00 00  02 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  80 1d 96 0d 01 00 00 00\n              ^\nutil/coding.h:372:12: runtime error: load of misaligned address 0x00010d85516c for type 'const unsigned long', which requires 8 byte alignment\n0x00010d85516c: note: pointer points here\n  01 00 34 57 00 00 00 00  02 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  78 24 82 0a 01 00 00 00\n              ^\nversion_set.cc:854: runtime error: constructor call on misaligned address 0x00010dbfa5c8 for type 'rocksdb::(anonymous namespace)::LevelFileIteratorState', which requires 16 byte alignment\n0x00010dbfa5c8: note: pointer points here\n 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\nversion_set.cc:512: runtime error: constructor call on misaligned address 0x00010dbfa5c8 for type 'rocksdb::(anonymous namespace)::LevelFileIteratorState', which requires 16 byte alignment\n0x00010dbfa5c8: note: pointer points here\n 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\nversion_set.cc:505: runtime error: constructor call on misaligned address 0x00010dbfa5e8 for type 'rocksdb::ReadOptions', which requires 16 byte alignment\n0x00010dbfa5e8: note: pointer points here\n 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\noptions.h:931: runtime error: constructor call on misaligned address 0x00010dbfa5e8 for type 'rocksdb::ReadOptions', which requires 16 byte alignment\n0x00010dbfa5e8: note: pointer points here\n 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\noptions.h:931: runtime error: constructor call on misaligned address 0x00010dbfa628 for type 'std::__1::function<void ()>', which requires 16 byte alignment\n0x00010dbfa628: note: pointer points here\n 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\nfunctional:1583: runtime error: constructor call on misaligned address 0x00010dbfa628 for type 'std::__1::function<void ()>', which requires 16 byte alignment\n0x00010dbfa628: note: pointer points here\n 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/functional:1585:9: runtime error: member access within misaligned address 0x00010dbfa628 for type 'std::__1::function<void ()>', which requires 16 byte alignment\n0x00010dbfa628: note: pointer points here\n 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/functional:1585:9: runtime error: store to misaligned address 0x00010dbfa648 for type '__base *' (aka '__base<void ()> *'), which requires 16 byte alignment\n0x00010dbfa648: note: pointer points here\n 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\ndb/version_set.cc:864:29: runtime error: upcast of misaligned address 0x00010dbfa5c8 for type 'rocksdb::(anonymous namespace)::LevelFileIteratorState', which requires 16 byte alignment\n0x00010dbfa5c8: note: pointer points here\n 00 00 00 00  a0 db 70 0a 01 00 00 00  00 00 00 00 00 00 00 00  90 14 98 0d 01 00 00 00  00 00 00 00\n              ^\ndb/version_set.cc:521:12: runtime error: member access within misaligned address 0x00010dbfa5c8 for type 'rocksdb::(anonymous namespace)::LevelFileIteratorState', which requires 16 byte alignment\n0x00010dbfa5c8: note: pointer points here\n 00 00 00 00  a0 db 70 0a 01 00 00 00  00 00 00 00 00 00 00 00  90 14 98 0d 01 00 00 00  00 00 00 00\n              ^\ndb/version_set.cc:521:12: runtime error: load of misaligned address 0x00010dbfa5d8 for type 'rocksdb::TableCache *', which requires 16 byte alignment\n0x00010dbfa5d8: note: pointer points here\n 00 00 00 00  90 14 98 0d 01 00 00 00  00 00 00 00 00 00 00 00  01 01 ff ff ff ff ff ff  00 00 00 00\n              ^\ndb/version_set.cc:522:9: runtime error: member access within misaligned address 0x00010dbfa5c8 for type 'rocksdb::(anonymous namespace)::LevelFileIteratorState', which requires 16 byte alignment\n0x00010dbfa5c8: note: pointer points here\n 00 00 00 00  a0 db 70 0a 01 00 00 00  00 00 00 00 00 00 00 00  90 14 98 0d 01 00 00 00  00 00 00 00\n              ^\ndb/version_set.cc:522:9: runtime error: reference binding to misaligned address 0x00010dbfa5e8 for type 'const rocksdb::ReadOptions', which requires 16 byte alignment\n0x00010dbfa5e8: note: pointer points here\n 00 00 00 00  01 01 ff ff ff ff ff ff  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\ndb/version_set.cc:522:24: runtime error: member access within misaligned address 0x00010dbfa5c8 for type 'rocksdb::(anonymous namespace)::LevelFileIteratorState', which requires 16 byte alignment\n0x00010dbfa5c8: note: pointer points here\n 00 00 00 00  a0 db 70 0a 01 00 00 00  00 00 00 00 00 00 00 00  90 14 98 0d 01 00 00 00  00 00 00 00\n              ^\ndb/version_set.cc:522:38: runtime error: member access within misaligned address 0x00010dbfa5c8 for type 'rocksdb::(anonymous namespace)::LevelFileIteratorState', which requires 16 byte alignment\n0x00010dbfa5c8: note: pointer points here\n 00 00 00 00  a0 db 70 0a 01 00 00 00  00 00 00 00 00 00 00 00  90 14 98 0d 01 00 00 00  00 00 00 00\n              ^\ndb/version_set.cc:522:57: runtime error: member access within misaligned address 0x00010dbfa5c8 for type 'rocksdb::(anonymous namespace)::LevelFileIteratorState', which requires 16 byte alignment\n0x00010dbfa5c8: note: pointer points here\n 00 00 00 00  a0 db 70 0a 01 00 00 00  00 00 00 00 00 00 00 00  90 14 98 0d 01 00 00 00  00 00 00 00\n              ^\ndb/version_set.cc:522:57: runtime error: load of misaligned address 0x00010dbfa678 for type 'rocksdb::RangeDelAggregator *', which requires 16 byte alignment\n0x00010dbfa678: note: pointer points here\n 01 00 00 00  d0 a1 bf 0d 01 00 00 00  00 00 00 00 00 00 00 00  f8 db 70 0a 01 00 00 00  00 00 00 00\n              ^\ndb/version_set.cc:523:54: runtime error: member access within misaligned address 0x00010dbfa5c8 for type 'rocksdb::(anonymous namespace)::LevelFileIteratorState', which requires 16 byte alignment\n0x00010dbfa5c8: note: pointer points here\n 00 00 00 00  a0 db 70 0a 01 00 00 00  00 00 00 00 00 00 00 00  90 14 98 0d 01 00 00 00  00 00 00 00\n              ^\ndb/version_set.cc:523:54: runtime error: load of misaligned address 0x00010dbfa668 for type 'rocksdb::HistogramImpl *', which requires 16 byte alignment\n0x00010dbfa668: note: pointer points here\n 01 00 00 00  c8 88 a5 0d 01 00 00 00  00 00 00 00 01 00 00 00  d0 a1 bf 0d 01 00 00 00  00 00 00 00\n              ^\ndb/version_set.cc:524:9: runtime error: member access within misaligned address 0x00010dbfa5c8 for type 'rocksdb::(anonymous namespace)::LevelFileIteratorState', which requires 16 byte alignment\n0x00010dbfa5c8: note: pointer points here\n 00 00 00 00  a0 db 70 0a 01 00 00 00  00 00 00 00 00 00 00 00  90 14 98 0d 01 00 00 00  00 00 00 00\n              ^\ndb/version_set.cc:524:47: runtime error: member access within misaligned address 0x00010dbfa5c8 for type 'rocksdb::(anonymous namespace)::LevelFileIteratorState', which requires 16 byte alignment\n0x00010dbfa5c8: note: pointer points here\n 00 00 00 00  a0 db 70 0a 01 00 00 00  00 00 00 00 00 00 00 00  90 14 98 0d 01 00 00 00  00 00 00 00\n              ^\ndb/version_set.cc:524:62: runtime error: member access within misaligned address 0x00010dbfa5c8 for type 'rocksdb::(anonymous namespace)::LevelFileIteratorState', which requires 16 byte alignment\n0x00010dbfa5c8: note: pointer points here\n 00 00 00 00  a0 db 70 0a 01 00 00 00  00 00 00 00 00 00 00 00  90 14 98 0d 01 00 00 00  00 00 00 00\n              ^\ndb/table_cache.cc:228:33: runtime error: reference binding to misaligned address 0x00010dbfa5e8 for type 'const rocksdb::ReadOptions', which requires 16 byte alignment\n0x00010dbfa5e8: note: pointer points here\n 00 00 00 00  01 01 ff ff ff ff ff ff  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\ntable/block_based_table_reader.cc:1554:41: runtime error: reference binding to misaligned address 0x00010dbfa5e8 for type 'const rocksdb::ReadOptions', which requires 16 byte alignment\n0x00010dbfa5e8: note: pointer points here\n 00 00 00 00  01 01 ff ff ff ff ff ff  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\ntable/block_based_table_reader.cc:1396:21: runtime error: reference binding to misaligned address 0x00010dbfa5e8 for type 'const rocksdb::ReadOptions', which requires 16 byte alignment\n0x00010dbfa5e8: note: pointer points here\n 00 00 00 00  01 01 ff ff ff ff ff ff  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\ninclude/rocksdb/options.h:931:8: runtime error: reference binding to misaligned address 0x00010dbfa628 for type 'const std::function<void ()>', which requires 16 byte alignment\n0x00010dbfa628: note: pointer points here\n 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/functional:1584:13: runtime error: load of misaligned address 0x00010dbfa648 for type '__base *const' (aka '__base<void ()> *const'), which requires 16 byte alignment\n0x00010dbfa648: note: pointer points here\n 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  c8 a5 97 0d 01 00 00 00  38 36 9b 0d\n              ^\ntable/block_based_table_reader.cc:1555:24: runtime error: reference binding to misaligned address 0x00010dbfa5e8 for type 'const rocksdb::ReadOptions', which requires 16 byte alignment\n0x00010dbfa5e8: note: pointer points here\n 00 00 00 00  01 01 ff ff ff ff ff ff  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\ndb/table_cache.cc:244:54: runtime error: load of misaligned address 0x00010dbfa618 for type 'const bool', which requires 16 byte alignment\n0x00010dbfa618: note: pointer points here\n 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\ndb/table_cache.cc:246:49: runtime error: reference binding to misaligned address 0x00010dbfa5e8 for type 'const rocksdb::ReadOptions', which requires 16 byte alignment\n0x00010dbfa5e8: note: pointer points here\n 00 00 00 00  01 01 ff ff ff ff ff ff  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\ndb/version_set.cc:532:12: runtime error: member access within misaligned address 0x00010dbfa5c8 for type 'rocksdb::(anonymous namespace)::LevelFileIteratorState', which requires 16 byte alignment\n0x00010dbfa5c8: note: pointer points here\n 00 00 00 00  a0 db 70 0a 01 00 00 00  00 00 00 00 00 00 00 00  90 14 98 0d 01 00 00 00  00 00 00 00\n              ^\ndb/version_set.cc:532:12: runtime error: member access within misaligned address 0x00010dbfa5e8 for type 'const rocksdb::ReadOptions', which requires 16 byte alignment\n0x00010dbfa5e8: note: pointer points here\n 00 00 00 00  01 01 ff ff ff ff ff ff  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\ndb/version_set.cc:532:26: runtime error: load of misaligned address 0x00010dbfa5f8 for type 'const rocksdb::Slice *const', which requires 16 byte alignment\n0x00010dbfa5f8: note: pointer points here\n 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\nversion_set.cc:493: runtime error: member call on misaligned address 0x00010dbfa5c8 for type 'rocksdb::(anonymous namespace)::LevelFileIteratorState', which requires 16 byte alignment\n0x00010dbfa5c8: note: pointer points here\n 00 00 00 00  a0 db 70 0a 01 00 00 00  00 00 00 00 00 00 00 00  90 14 98 0d 01 00 00 00  00 00 00 00\n              ^\nversion_set.cc:493: runtime error: member call on misaligned address 0x00010dbfa5e8 for type 'rocksdb::ReadOptions', which requires 16 byte alignment\n0x00010dbfa5e8: note: pointer points here\n 00 00 00 00  01 01 ff ff ff ff ff ff  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\noptions.h:931: runtime error: member call on misaligned address 0x00010dbfa5e8 for type 'rocksdb::ReadOptions', which requires 16 byte alignment\n0x00010dbfa5e8: note: pointer points here\n 00 00 00 00  01 01 ff ff ff ff ff ff  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\noptions.h:931: runtime error: member call on misaligned address 0x00010dbfa628 for type 'std::__1::function<void ()>', which requires 16 byte alignment\n0x00010dbfa628: note: pointer points here\n 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\nfunctional:1765: runtime error: member call on misaligned address 0x00010dbfa628 for type 'std::__1::function<void ()>', which requires 16 byte alignment\n0x00010dbfa628: note: pointer points here\n 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/functional:1766:9: runtime error: member access within misaligned address 0x00010dbfa628 for type 'std::__1::function<void ()>', which requires 16 byte alignment\n0x00010dbfa628: note: pointer points here\n 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/functional:1766:9: runtime error: load of misaligned address 0x00010dbfa648 for type '__base *' (aka '__base<void ()> *'), which requires 16 byte alignment\n0x00010dbfa648: note: pointer points here\n 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  c8 a5 97 0d 01 00 00 00  38 36 9b 0d\n              ^\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/functional:1766:27: runtime error: member access within misaligned address 0x00010dbfa628 for type 'std::__1::function<void ()>', which requires 16 byte alignment\n0x00010dbfa628: note: pointer points here\n 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/functional:1768:14: runtime error: member access within misaligned address 0x00010dbfa628 for type 'std::__1::function<void ()>', which requires 16 byte alignment\n0x00010dbfa628: note: pointer points here\n 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00\n              ^\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/functional:1768:14: runtime error: load of misaligned address 0x00010dbfa648 for type '__base *' (aka '__base<void ()> *'), which requires 16 byte alignment\n0x00010dbfa648: note: pointer points here\n 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  c8 a5 97 0d 01 00 00 00  38 36 9b 0d\n              ^\n[       OK ] DBPropertiesTest.ReadLatencyHistogramByLevel (1599 ms)\n[----------] 1 test from DBPropertiesTest (1599 ms total)\n\n[----------] Global test environment tear-down\n[==========] 1 test from 1 test case ran. (1599 ms total)\n[  PASSED  ] 1 test.\n```\n\nSo it seems the root cause is that the internal implementation of `std::function` on macOS (and perhaps with libc++ generally?) requires 16-byte aligned memory, but the arena allocator only guarantees that the returned memory will be `sizeof(void*)` aligned, which is only 8-byte alignment on my machine. This patch solves the problem by adjusting the allocator to derive the necessary alignment from `alignof(std::max_align_t)`, which is properly 16 bytes on my machine.\n\nAs I mentioned in #2265, none of RocksDB's tests will cause this unaligned access to actually abort the process, but, on macOS, linking CockroachDB against a version of RocksDB with the above patch and letting it run for just a few seconds will cause a SIGABRT.\n\n```\nProcess 19792 stopped\n* thread #2, stop reason = EXC_BAD_ACCESS (code=EXC_I386_GPFLT)\n    frame #0: 0x0000000004f5e78f cockroach`DBNewIter + 95\ncockroach`DBNewIter:\n->  0x4f5e78f <+95>:  callq  *0x28(%rax)\n    0x4f5e792 <+98>:  jmp    0x4f5e79e                 ; <+110>\n    0x4f5e794 <+100>: movq   -0x50(%rbp), %rcx\n    0x4f5e798 <+104>: movq   %rax, %rdi\n(lldb) bt\n* thread #2, stop reason = EXC_BAD_ACCESS (code=EXC_I386_GPFLT)\n  * frame #0: 0x0000000004f5e78f cockroach`DBNewIter + 95\n```\n\nI'd get you a backtrace, but [Go doesn't include cgo debug information on macOS](https://github.com/golang/go/issues/6942). I've also tried building against libc++ on Linux, where debug information would be available, but I can't seem to trigger the bug there.\n\nIn any case, this PR both fixes the segfault in CockroachDB and fixes the warnings reported by ubsan.\nCloses https://github.com/facebook/rocksdb/pull/2347\n\nDifferential Revision: D5108596\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: bd5e4323b2ce915ed4fe78e123cb8996aec75a00"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zertosh": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/fad14050ae3e610f7d3a65a0f85b692bab7a59a3", "message": "Remove `import` use from TARGETS\n\nSummary:\nWe're moving away from `import`. The equivalent internal construct that\ngets the directory from `fbcode/` is `package_name()`. This is a\nSkylark friendly wrapper around [`get_base_path`].\n\nThe additional whitespace change is from running `python ./buckifier/buckify_rocksdb.py`.\n\n[`get_base_path`]: https://buckbuild.com/function/get_base_path.html\nCloses https://github.com/facebook/rocksdb/pull/3210\n\nReviewed By: yiwu-arbug\n\nDifferential Revision: D6451242\n\nPulled By: zertosh\n\nfbshipit-source-id: 445757261de0ec89d5d332c1ba9af097086326dc"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kapitan-k": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/75d57a5d53822eac857e7438a4c0bb786cdf046e", "message": "C API: Add some block based table options\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/3159\n\nDifferential Revision: D6428220\n\nPulled By: sagar0\n\nfbshipit-source-id: 60508d09b5281f54b907a1c40e9631fc08343131"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/f1c5eaba56460f23eb8c7395746fe49d419cb3cc", "message": "updated c ingestexternalfileoptions for ingest behind\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/3151\n\nDifferential Revision: D6293861\n\nPulled By: ajkr\n\nfbshipit-source-id: f8db0a71509d1cd8237f2d377bf9e1bb0464bdbf"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tamird": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/cf0d6aa0070b73acc894b7ee8bef54c8f4538a06", "message": "CMake cross platform Java support and add JNI to travis\n\nSummary:\nRewrite `java/CMakeLists.txt` to take advantage of CMake's cross platform\nJava support.\n\nadamretter\nCloses https://github.com/facebook/rocksdb/pull/2301\n\nDifferential Revision: D5070724\n\nPulled By: sagar0\n\nfbshipit-source-id: 999aee9bd39da2b24a5fe493a2eb0e9af6072dc7"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/21564923", "body": "Looks like this commit broke tests:\r\n```\r\nutil/delete_scheduler_test.cc:495: Failure\r\nExpected: (time_spent_deleting) > (expected_penlty * 0.9), actual: 319221 vs 360000\r\nterminate called after throwing an instance of 'testing::internal::GoogleTestFailureException'\r\n  what():  util/delete_scheduler_test.cc:495: Failure\r\nExpected: (time_spent_deleting) > (expected_penlty * 0.9), actual: 319221 vs 360000\r\n/bin/bash: line 1: 64050 Aborted                 (core dumped) ./$t\r\nmake: *** [check_some] Error 1\r\n```\r\nSee https://travis-ci.org/facebook/rocksdb/builds/216068363", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21564923/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21678314", "body": "this method returns `bool` now, so this doesn't compile. looks like it should be `return false;`.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21678314/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "jsdt": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/022c598abb87668572eb0c3ac7e9fc41dd856bbb", "message": "Fix minor typo in comment\n\nSummary:\nmean -> meant\nCloses https://github.com/facebook/rocksdb/pull/3202\n\nDifferential Revision: D6426443\n\nPulled By: sagar0\n\nfbshipit-source-id: adaf07218580ee6903986fa5686de92f43f420e1"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "PhaniShekhar": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/4b65cfc723e054cc5f3cf83afc497110edc8c240", "message": "Support for block_cache num_shards and other config via option string.\n\nSummary:\nProblem: Option string accepts only cache_size as parameter for block_cache which is specified as \"block_cache=1M\".\nIt doesn't accept other parameters like num_shards etc.\n\nChanges :\n1) ParseBlockBasedTableOption in block_based_table_factory is edited to accept cache options in the format \"block_cache=<cache_size>:<num_shard_bits>:<strict_capacity_limit>:<high_pri_pool_ratio>\".\nOptions other than cache_size are optional to maintain backward compatibility. The changes are valid for block_cache_compressed as well.\nFor example, \"block_cache=1M:6:true:0.5\", \"block_cache=1M:6:true\", \"block_cache=1M:6\" and \"block_cache=1M\" are all valid option strings.\n\n2) Corresponding unit tests are added.\nCloses https://github.com/facebook/rocksdb/pull/3108\n\nDifferential Revision: D6420997\n\nPulled By: sagar0\n\nfbshipit-source-id: cdea8b785688d2802907974af27225ccc1c0cd43"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/65a9cd616876c7a1204e1a50990400e4e1f61d7e", "message": "Use L1 size as estimate for L0 size in LevelCompactionBuilder::GetPathID\n\nSummary:\nFix for [2461](https://github.com/facebook/rocksdb/issues/2461).\n\nProblem: When using multiple db_paths setting with RocksDB, RocksDB incorrectly calculates the size of L1 in LevelCompactionBuilder::GetPathId.\n\nmax_bytes_for_level_base is used as L0 size and L1 size is calculated as (L0 size * max_bytes_for_level_multiplier). However, L1 size should be max_bytes_for_level_base.\n\nSolution: Use max_bytes_for_level_base as L1 size. Also, use L1 size as the estimated size of L0.\nCloses https://github.com/facebook/rocksdb/pull/2903\n\nDifferential Revision: D5885442\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: 036da1c9298d173b9b80479cc6661ee4b7a951f6"}], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3102", "title": "Support for Column family specific paths.", "body": "In this change, an option to set different paths for different column families is added.\r\nThis option is set via cf_paths setting of ColumnFamilyOptions. This option will work in a similar fashion to db_paths setting. Cf_paths is a vector of Dbpath values which contains a pair of the absolute path and target size. Multiple levels in a Column family can go to different paths if cf_paths has more than one path. \r\nTo maintain backward compatibility, if cf_paths is not specified for a column family, db_paths setting will be used. Note that, if db_paths setting is also not specified, RocksDB already has code to use db_name as the only path. \r\n\r\nChanges : \r\n1) A new member \"cf_paths\" is added to ImmutableCfOptions. This is set, based on cf_paths setting of ColumnFamilyOptions and db_paths setting of ImmutableDbOptions.  This member is used to identify the path information whenever files are accessed. \r\n2) Validation checks are added for cf_paths setting based on existing checks for db_paths setting.\r\n3) DestroyDB, PurgeObsoleteFiles etc. are edited to support multiple cf_paths.\r\n4) Unit tests are added appropriately.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/2698", "title": "Option to create Statistic with Histogram windowing implementation.", "body": "Option to create Statistic with Histogram windowing implementation. Expose sum, percentile functions for histogram statistics.\r\n\r\nHistogram Windowing implementation is submitted as part of #1026 to introduce a computation window for histogram.\r\nThe current change is an extension to enable usage of histogram windowing with statistics.\r\nIt also adds API in statistics to expose sum and percentile of histograms.\r\n\r\nChanges in this patch:\r\n\r\nA parameter is added to CreateDBStatistics() to enable usage of histogram windowing.\r\nA method sum() is added to Histogram interface.\r\ngetHistogramPercentile() and getHistogramSum() are added to Statistics interface.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "glittershark": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/2f09524762c2ea44a9d1bb752352d80a954be242", "message": "Expose all remaining read and write options via the C API\n\nSummary:\nExpose read and write options via the C API\nCloses https://github.com/facebook/rocksdb/pull/3185\n\nDifferential Revision: D6389658\n\nPulled By: sagar0\n\nfbshipit-source-id: 1848912750329a476805b3cb2f315e7b71f61472"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/fe187e7e6c2ee181dfd072dccce1c75eaaf746fc", "message": "Add Elixir to the list of language bindings\n\nSummary: Closes https://github.com/facebook/rocksdb/pull/3183\n\nDifferential Revision: D6386140\n\nPulled By: maysamyabandeh\n\nfbshipit-source-id: ca71d54edd741c3b7d9676ee2bcf584a5d49bc35"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "anand1976": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/d394a6bb48b79b13822702cd5af8a0d85a2fd8f8", "message": "Add a ticker stat for number of keys skipped during iteration\n\nSummary:\nThis diff adds a new ticker stat, NUMBER_ITER_SKIP, to count the\nnumber of internal keys skipped during iteration. Keys can be skipped\ndue to deletes, or lower sequence number, or higher sequence number\nthan the one requested.\n\nAlso, fix the issue when StatisticsData is naturally aligned on cacheline boundary,\npadding becomes a zero size array, which the Windows compiler doesn't\nlike. So add a cacheline worth of padding in that case to keep it happy.\nWe cannot conditionally add padding as gcc doesn't allow using sizeof\nin preprocessor directives.\nCloses https://github.com/facebook/rocksdb/pull/3177\n\nDifferential Revision: D6353897\n\nPulled By: anand1976\n\nfbshipit-source-id: 441d5a09af9c4e22e7355242dfc0c7b27aa0a6c2"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "giorgioazzinnaro": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/0996e140bd4da46ce46049c9fd1eae61f3541dee", "message": "Added ProfaneDB\n\nSummary:\nAdded my project [ProfaneDB](https://profanedb.gitlab.io/) which uses RocksDB as the main storage engine\nCloses https://github.com/facebook/rocksdb/pull/3182\n\nDifferential Revision: D6370847\n\nPulled By: ajkr\n\nfbshipit-source-id: ff4bb6bdbc6e42fdb88bd793a84a0e04c129b6ae"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gdavidsson": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/2d04ed65e4acdf8577cf076453af71cc75657644", "message": "Make trash-to-DB size ratio limit configurable\n\nSummary:\nAllow users to configure the trash-to-DB size ratio limit, so\nthat ratelimits for deletes can be enforced even when larger portions of\nthe database are being deleted.\nCloses https://github.com/facebook/rocksdb/pull/3158\n\nDifferential Revision: D6304897\n\nPulled By: gdavidsson\n\nfbshipit-source-id: a28dd13059ebab7d4171b953ed91ce383a84d6b3"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shligit": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/eefd75a228fc2c50174c0a306918c73ded22ace7", "message": "Stream\n\nSummary:\nAdd a simple policy for NVMe write time life hint\nCloses https://github.com/facebook/rocksdb/pull/3095\n\nDifferential Revision: D6298030\n\nPulled By: shligit\n\nfbshipit-source-id: 9a72a42e32e92193af11599eb71f0cf77448e24d"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/33c7d4ccd90aa640eea840e041da082bfe5a480e", "message": "Make writable_file_max_buffer_size dynamic\n\nSummary:\nThe DBOptions::writable_file_max_buffer_size can be changed dynamically.\nCloses https://github.com/facebook/rocksdb/pull/3053\n\nDifferential Revision: D6152720\n\nPulled By: shligit\n\nfbshipit-source-id: aa0c0cfcfae6a54eb17faadb148d904797c68681"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lth": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/e03377c7fd86f117c6c64d35955140dab400eb3c", "message": "Add lock wait time as a perf context counter\n\nSummary:\nAdds two new counters:\n\n`key_lock_wait_count` counts how many times a lock was blocked by another transaction and had to wait, instead of being granted the lock immediately.\n`key_lock_wait_time` counts the time spent acquiring locks.\nCloses https://github.com/facebook/rocksdb/pull/3107\n\nDifferential Revision: D6217332\n\nPulled By: lth\n\nfbshipit-source-id: 55d4f46da5550c333e523263422fd61d6a46deb9"}, {"url": "https://api.github.com/repos/facebook/rocksdb/commits/88ed1f6ea601b13b1eb416e50588ebe48dd93641", "message": "Allow upgrades from nullptr to some merge operator\n\nSummary:\nCurrently, RocksDB does not allow reopening a preexisting DB with no merge operator defined, with a merge operator defined. This means that if a DB ever want to add a merge operator, there's no way to do so currently.\n\nFix this by adding a new verification type `kByNameAllowFromNull` which will allow old values to be nullptr, and new values to be non-nullptr.\nCloses https://github.com/facebook/rocksdb/pull/2958\n\nDifferential Revision: D5961131\n\nPulled By: lth\n\nfbshipit-source-id: 06179bebd0d90db3d43690b5eb7345e2d5bab1eb"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mikhail-antonov": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/7fe3b32896ecbb21d67ec52fccb713cb9bc6a644", "message": "Added support for differential snapshots\n\nSummary:\nThe motivation for this PR is to add to RocksDB support for differential (incremental) snapshots, as snapshot of the DB changes between two points in time (one can think of it as diff between to sequence numbers, or the diff D which can be thought of as an SST file or just set of KVs that can be applied to sequence number S1 to get the database to the state at sequence number S2).\n\nThis feature would be useful for various distributed storages layers built on top of RocksDB, as it should help reduce resources (time and network bandwidth) needed to recover and rebuilt DB instances as replicas in the context of distributed storages.\n\nFrom the API standpoint that would like client app requesting iterator between (start seqnum) and current DB state, and reading the \"diff\".\n\nThis is a very draft PR for initial review in the discussion on the approach, i'm going to rework some parts and keep updating the PR.\n\nFor now, what's done here according to initial discussions:\n\nPreserving deletes:\n - We want to be able to optionally preserve recent deletes for some defined period of time, so that if a delete came in recently and might need to be included in the next incremental snapshot it would't get dropped by a compaction. This is done by adding new param to Options (preserve deletes flag) and new variable to DB Impl where we keep track of the sequence number after which we don't want to drop tombstones, even if they are otherwise eligible for deletion.\n - I also added a new API call for clients to be able to advance this cutoff seqnum after which we drop deletes; i assume it's more flexible to let clients control this, since otherwise we'd need to keep some kind of timestamp < -- > seqnum mapping inside the DB, which sounds messy and painful to support. Clients could make use of it by periodically calling GetLatestSequenceNumber(), noting the timestamp, doing some calculation and figuring out by how much we need to advance the cutoff seqnum.\n - Compaction codepath in compaction_iterator.cc has been modified to avoid dropping tombstones with seqnum > cutoff seqnum.\n\nIterator changes:\n - couple params added to ReadOptions, to optionally allow client to request internal keys instead of user keys (so that client can get the latest value of a key, be it delete marker or a put), as well as min timestamp and min seqnum.\n\nTableCache changes:\n - I modified table_cache code to be able to quickly exclude SST files from iterators heep if creation_time on the file is less then iter_start_ts as passed in ReadOptions. That would help a lot in some DB settings (like reading very recent data only or using FIFO compactions), but not so much for universal compaction with more or less long iterator time span.\n\nWhat's left:\n\n - Still looking at how to best plug that inside DBIter codepath. So far it seems that FindNextUserKeyInternal only parses values as UserKeys, and iter->key() call generally returns user key. Can we add new API to DBIter as internal_key(), and modify this internal method to optionally set saved_key_ to point to the full internal key? I don't need to store actual seqnum there, but I do need to store type.\nCloses https://github.com/facebook/rocksdb/pull/2999\n\nDifferential Revision: D6175602\n\nPulled By: mikhail-antonov\n\nfbshipit-source-id: c779a6696ee2d574d86c69cec866a3ae095aa900"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zshipko": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/386a57e6ef08fbcc2f9b92217fa4a8c9873386c9", "message": "Fix build on OpenBSD\n\nSummary:\nA few simple changes to allow RocksDB to be built on OpenBSD. Let me know if any further changes are needed.\nCloses https://github.com/facebook/rocksdb/pull/3061\n\nDifferential Revision: D6138800\n\nPulled By: ajkr\n\nfbshipit-source-id: a13a17b5dc051e6518bd56a8c5efd1d24dd81b0c"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zawlazaw": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/57fcdc264aa0f2bb481fdad9d4a1a7846395c6e1", "message": "added missing subcodes and improved error message for missing enum values\n\nSummary:\nJava's `Status.SubCode` was out of sync with `include/rocksdb/status.h:SubCode`.\n\nWhen running out of disc space this led to an `IllegalArgumentException` because of an invalid status code, rather than just returning the corresponding status code without an exception.\n\nI added the missing status codes.\n\nBy this, we keep the behaviour of throwing an `IllegalArgumentException` in case of newly added status codes that are defined in C but not in Java.\n\nWe could think of an alternative strategy: add in Java another code \"UnknownCode\" which acts as a catch-all for all those status codes that are not yet mirrored from C to Java. This approach would never throw an exception but simply return a non-OK status-code.\n\nI think the current approach of throwing an Exception in case of a C/Java inconsistency is fine, but if you have some opinion on the alternative strategy, then feel free to comment here.\nCloses https://github.com/facebook/rocksdb/pull/3050\n\nDifferential Revision: D6129682\n\nPulled By: sagar0\n\nfbshipit-source-id: f2bf44caad650837cffdcb1f93eb793b43580c66"}], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/2857", "title": "Add ByteBuffer-based put-method to SstFileWriter", "body": "This PR adds another variant of the put-method to SstFileWriter, namely with the signature `put(ByteBuffer, ByteBuffer)`.\r\nThis allows for the most efficient data transfer from Java to rocksdb.\r\nSee issue #2668 for a detailed motivation.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "amostyaev": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/ee2b1ec1e8776d6ed154f2d66435be8e2a620da5", "message": "Fix unstable floating point exception\n\nSummary:\nFix unstable floating point exception, tested on Windows, 64-bit build.\nThe problem appeared in `SetCapacity()` method at line\n\n`high_pri_pool_capacity_ = capacity_ * high_pri_pool_ratio_;`\n\n`high_pri_pool_ratio_` was not initialized at that moment, because\n`SetHighPriorityPoolRatio()` is called after `SetCapacity()`. So,\n`high_pri_pool_ratio_` contained garbage, which caused \"Floating point\nexception\" sometimes.\nCloses https://github.com/facebook/rocksdb/pull/3052\n\nDifferential Revision: D6111161\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: d170329111ad12b4bf9bbcf37bcb6411523438ae"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wurikiji": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/7deed2b43c02a33a8c5106fb2eb6025658be76c2", "message": "Fix a typo in a comment\n\nSummary:\ninstad of for specific level -> instead of a specific level\nCloses https://github.com/facebook/rocksdb/pull/3040\n\nDifferential Revision: D6090811\n\nPulled By: sagar0\n\nfbshipit-source-id: 499edef0a6f596c448f61791e6aca8f5cce08e9c"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "codeeply": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/f7843f30a8af07bc2d817830dba7b350f392b257", "message": "Move ~Comparator define to comparator.h\n\nSummary:\nWhen I impl my own comparator, and build in release mode.\nThe following compile error occurs.\nundefined reference to `typeinfo for rocksdb::Comparator'\n\nThis fix allows users build with RTTI off when has their own comparator.\nCloses https://github.com/facebook/rocksdb/pull/3008\n\nDifferential Revision: D6077354\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 914c26dbab72f0ad1f0e15f8666a3fb2f10bfed8"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tchaikov": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/019aa7074ceadffd6fb871f96465b3785370901f", "message": "cmake: pass \"-msse4.2\" to when building crc32c.cc if HAVE_SSE42\n\nSummary:\nit turns out that, with older GCC shipped from centos7, the SSE42\nintrinsics are not available even with \"target\" specified. so we\nneed to pass \"-msse42\" for checking compiler's sse4.2 support and\nfor building crc32c.cc which uses sse4.2 intrinsics for crc32.\n\nSigned-off-by: Kefu Chai <tchaikov@gmail.com>\nCloses https://github.com/facebook/rocksdb/pull/2950\n\nDifferential Revision: D6032298\n\nPulled By: siying\n\nfbshipit-source-id: 124c946321043661b3fb0a70b6cdf4c9c5126ab4"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/19844802", "body": "@wjwithagen i am not sure if we need this comma.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19844802/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19874334", "body": "s/STEQUAL/STREQUAL/", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19874334/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "schischi": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/01542400a87ad130aed790bf895c029082745cbe", "message": "Inform caller when rocksdb is stalling writes\n\nSummary:\nAdd a new function in Listener to let the caller know when rocksdb\nis stalling writes.\nCloses https://github.com/facebook/rocksdb/pull/2897\n\nDifferential Revision: D5860124\n\nPulled By: schischi\n\nfbshipit-source-id: ee791606169aa64f772c86f817cebf02624e05e1"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "aclamk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/75f7f42d41d8659b43a839bd31570a81e562eb7e", "message": "Added CPU prefetch for skiplist\n\nSummary:\nThis change causes following changes result of test:\n./db_bench --writes 10000000 --benchmarks=\"fillrandom\" --compression_type none\nfrom\nfillrandom   :       3.177 micros/op 314804 ops/sec;   34.8 MB/s\nto\nfillrandom   :       2.777 micros/op 360087 ops/sec;   39.8 MB/s\nCloses https://github.com/facebook/rocksdb/pull/2961\n\nDifferential Revision: D5977822\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 1ea77707bffa978b1592b0c5d0fe76bfa1930f8d"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ysmiles": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/8c724f5c7fb6dbba35db2a43d52c24592c215acf", "message": "Default one to rocksdb:x64-windows\n\nSummary:\nThe default one will try to install rocksdb:x86-windows, which would lead to failing of the build at the last step (CMake Error, Rocksdb only supports x64). Because it will try to install a serials of x86 version package, and those cannot proceed to rocksdb:x86-windows building. By using rocksdb:x64-windows, we can make sure to install x64 version.\nTested on Win10 x64.\nCloses https://github.com/facebook/rocksdb/pull/2941\n\nDifferential Revision: D5937139\n\nPulled By: sagar0\n\nfbshipit-source-id: 15637fe23df59326a0e607bd4d5c48733e20bae3"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "TheRushingWookie": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/6a541afcc4d1e5b6e6d78e288b9bee3bb2a933b5", "message": "Make bytes_per_sync and wal_bytes_per_sync mutable\n\nSummary:\nSUMMARY\nMoves the bytes_per_sync and wal_bytes_per_sync options from immutableoptions to mutable options. Also if wal_bytes_per_sync is changed, the wal file and memtables are flushed.\nTEST PLAN\nran make check\nall passed\n\nTwo new tests SetBytesPerSync, SetWalBytesPerSync check that after issuing setoptions with a new value for the var, the db options have the new value.\nCloses https://github.com/facebook/rocksdb/pull/2893\n\nReviewed By: yiwu-arbug\n\nDifferential Revision: D5845814\n\nPulled By: TheRushingWookie\n\nfbshipit-source-id: 93b52d779ce623691b546679dcd984a06d2ad1bd"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yaozongyou": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/8ae81684e9589234475beca84f16632cd0a74be5", "message": "Update cmake_minimum_required to 2.8.12.\n\nSummary:\nHello,\n\ncurrent master branch declares cmake_minimum_required (VERSION 2.8.11)\nbut cmake gives the following error:\n\n[  6%] CMake Error at CMakeLists.txt:658 (install):\n  install TARGETS given unknown argument \"INCLUDES\".\n\nCMake Error at src/CMakeLists.txt:658 (install): install TARGETS given unknown argument \"INCLUDES\".\n\nbecause this argument not supported on CMake versions prior 2.8.12\nCloses https://github.com/facebook/rocksdb/pull/2904\n\nDifferential Revision: D5863430\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 0f7230e080add472ad4b87836b3104ea0b971a38"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "orgads": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/34ebadf9309a0ee08d68bcb62dc1ac8a4109bf26", "message": "Fix MinGW build\n\nSummary:\nsnprintf is defined as _snprintf, which doesn't exist in the std\nnamespace.\nCloses https://github.com/facebook/rocksdb/pull/2298\n\nDifferential Revision: D5070457\n\nPulled By: yiwu-arbug\n\nfbshipit-source-id: 6e1659ac3e86170653b174578da5a8ed16812cbb"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wpc": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebook/rocksdb/commits/e4234fbdcf2b7adcf539a8e0fb3988dcfc0a5528", "message": "collecting kValue type tombstone\n\nSummary:\nIn our testing cluster, we found large amount tombstone has been promoted to kValue type from kMerge after reaching the top level of compaction. Since we used to only collecting tombstone in merge operator, those tombstones can never be collected.\n\nThis PR addresses the issue by adding a GC step in compaction filter, which is only for kValue type records. Since those record already reached the top of compaction (no earlier data exists) we can safely remove them in compaction filter without worrying old data appears.\n\nThis PR also removes an old optimization in cassandra merge operator for single merge operands.  We need to do GC even on a single operand, so the optimation does not make sense anymore.\nCloses https://github.com/facebook/rocksdb/pull/2855\n\nReviewed By: sagar0\n\nDifferential Revision: D5806445\n\nPulled By: wpc\n\nfbshipit-source-id: 6eb25629d4ce917eb5e8b489f64a6aa78c7d270b"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/316273772", "body": "@scv119 I updated the PR:\r\n*  use std::chrono lib for current time in microseconds\r\n*  make the behavior purging or converting to tombstone configurable. And in the java code will set default behavior to converting expired column to tombstone ", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/316273772/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/316879069", "body": "@sagar0 and @scv119 I updated the PR base on the review feedback:\r\n* use correct license file header (https://fburl.com/v2otiqbi)\r\n* use std:chrono for time point conversion and comparison (https://fburl.com/v5quy8ip) (this actually fixed an int overflow bug) \r\n* some typo fixes (https://fburl.com/37iiw3l0)\r\n* Use \"*Ttl\" instead of \"*TTL\" for consistency\r\n* add document to CassandraCompactionFilter (https://fburl.com/xdfakph4)", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/316879069/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/322939193", "body": "@sagar0 this looks good to me. I will start working column tombstone right after this merged", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/322939193/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/329948038", "body": "@sagar0 and @scv119  I have clean up the test. The PR is up for review. For the refactoring to reduce duplication between those 3 loops: I originally thought to use lambda,  but kind of worried about the overhead. Let's leave it there for now.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/329948038/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "gfosco": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3389", "title": "Update endif/else behavior for unreachable code error on Windows.", "body": "Per #3367", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "quark-zju": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3339", "title": "crc32: suppress -Wimplicit-fallthrough warnings", "body": "Workaround a bunch of \"implicit-fallthrough\" compiler errors, like:\r\n\r\n```\r\nutil/crc32c.cc:533:7: error: this statement may fall through [-Werror=implicit-fallthrough=]\r\n   crc = _mm_crc32_u64(crc, *(uint64_t*)(buf + offset));\r\n       ^\r\nutil/crc32c.cc:1016:9: note: in expansion of macro \u2018CRCsinglet\u2019\r\n         CRCsinglet(crc0, next, -2 * 8);\r\n         ^~~~~~~~~~\r\nutil/crc32c.cc:1017:7: note: here\r\n       case 1:\r\n```\r\n\r\nTest Plan:\r\nBuild rocksdb using gcc 7.2.0 on Arch Linux with success.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "benclay": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3338", "title": "Callback bridge for Java-based CompactionFilter", "body": "Similar to `AbstractCompactionFilterFactory` and `AbstractComparator`, this diff contains an abstract base class for `rocksdb::CompactionFilter` written in Java. It uses a similar set of techniques to cache relevant field / method IDs at startup, passes scalars back into Java and relies on threadlocals to avoid `DirectSlice` finalize churn.\r\n\r\nUsing this implementation we've been able to run a Java compaction filter under Manhattan production workloads with a ~10% performance reduction.  It's not free but is good enough for some usecases.\r\n\r\nThere are a few unresolved issues with this implementation that I wanted to discuss with the community. They're marked with `TODO(benclay)` below.\r\n\r\n- Detaching compaction threads after every callback is extremely expensive. On our OpenJDK8 JVM the `DetachCurrentThread` call internally serializes on a mutex while releasing monitors, causing compactions and thus writes to eventually stall.  As a result, in this diff **we are not detaching at all**.  However, I am concerned about upstreaming this as-is this because folks might explicitly take locks in their CompactionFilter implementations and need those monitors to be automatically released.  I am looking for feedback from the community on this problem.  There are a few solutions from Rocks side I can think of, but am open to more ideas:\r\n  - Add a lifecycle hook (perhaps to CompactionFilterFactory?) when the compaction thread is shutting down. At that point we can detach the thread from the JVM.  This still risks the thread crashing and never detaching though.\r\n  - Implement a batching interface to CompactionFilter so we aren't making so many roundtrips and the detach cost is amortized.  This would help performance more generally but is probably more invasive - I haven't looked into the implications on Rocks side.\r\n  - Detach every Nth call - this seems very racy and doesn't guarantee that the thread detaches on the final call, so it seems like a non-starter.\r\n- I couldn't take advantage of @adamretter 's `RocksCallbackObject` regime because Java disallows multiple inheritance and I needed to inherit from `AbstractCompactionFilter`.  Right now that class and its C++ companion `JniCallback` are fairly basic, but over time could export some useful performance enhancements.  I thought about shimming a new `ICompactionFilter` interface **below** `AbstractCompactionFilter` and have that be the primary unit for interacting with `ColumnFamilyOptions`. Upon further examination that will somewhat break the tradition of having abstract base classes with package-private `nativeHandle_` member variables, which is how `ColumnFamilyOptions` is binding the `CompactionFilter` handle on the C++ side.  Open to suggestions here as well.\r\n\r\n@sagar0 @adamretter \r\n \r\nEDIT: There is a test issue that I'm still chasing down.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gritzko": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3304", "title": "expose WAL iterator in the C API", "body": "A minor change: I wrapped TransactionLogIterator for the C API.\r\nI needed that for the golang binding.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "double16": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3143", "title": "Add shared library for musl-libc", "body": "Add the jni library for musl-libc, specifically for incorporating into Alpine based docker images. The classifier is `musl64`.\r\n\r\nI have signed the CLA electronically.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pdvian": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3106", "title": "options: Fix coverity issues", "body": "Summary:\r\noptions/cf_options.cc:\r\n 77      memtable_insert_with_hint_prefix_extractor(\r\n\r\nCID 1396208 (#1 of 1): Uninitialized scalar field (UNINIT_CTOR)\r\n2. uninit_member: Non-static class member info_log_level is not initialized in this constructor nor in any functions that it calls.\r\n 78          cf_options.memtable_insert_with_hint_prefix_extractor.get()) {}\r\n\r\ninclude/rocksdb/advanced_options.h:\r\n\r\nAdvancedColumnFamilyOptions::AdvancedColumnFamilyOptions() {\r\n 40  assert(memtable_factory.get() != nullptr);\r\n\r\nCID 1405359 (#1 of 1): Uninitialized scalar field (UNINIT_CTOR)\r\n2. uninit_member: Non-static class member max_mem_compaction_level is not initialized in this constructor nor in any functions that it calls.\r\n 41}\r\n\r\nAs max_mem_compaction_level is not supported anymore, undefine it.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dcclyde": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3059", "title": "Fix DeleteFilesInRange so keys never reappear", "body": "In the previous implementation of DeleteFilesInRange, keys could reappear if we drop a wider range at a higher level, making the tombstone in the upper level disappear while the key it covered in the lower level doesn't. The second commit in this pull request reimplements DeleteFilesInRange to use the following strategy:\r\n1) Initialize the deletion range to the lower and upper bounds specified by the user.\r\n2) Start from the bottom level and iterate upward. At each level:\r\n    i) pick the widest clean-cut set of files that fall entirely in the range.\r\n    ii) Reset the deletion range to the min and max userkeys of the clean-cut set of files from step (i).\r\n\r\nTo support this, the first commit in this pull request modifies several functions VersionStorageInfo to accept ranges with no lower and/or upper bound. This is a \"fix\" in the sense that the functions' comments already promised the new behaviour.\r\n\r\nI've added a new unit test in db_compaction_test which passes now but fails when using the old implementation.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cooljiansir": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/3001", "title": "handle error with kSkipAnyCorruptedRecords", "body": "in kSkipAnyCorruptedRecords mode, continue to replay the log.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kamasubb": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/2966", "title": "CRC32 PPC Fast Zero Optimization", "body": "Fast Zero optimization for CRC32 computation in PowerPC", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/2869", "title": "CMake changes for CRC32 Optimization on PowerPC", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lucky": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/2921", "title": "Add instructions for INSTALL_PATH make variable", "body": "", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/339397485", "body": "@sagar0 I'm sorry, I still waiting on legal's decision here. :slightly_frowning_face: ", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/339397485/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/359550183", "body": "I'm sorry for the wait. I haven't heard anything from legal but I'll ping them again today.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/359550183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "lixiaoy1": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/2829", "title": "Delete duplicated key/value pairs recursively", "body": "Delete duplicated key/value pairs recursively<br>\r\n    This is to implement the idea: http://pad.ceph.com/p/rocksdb-wal-improvement\r\n    Add a new flush style called kFlushStyleDedup which users can config by setting\r\n    flush_style=kFlushStyleDedup. When flush is triggered, it dedups the key/value\r\n    pairs in the oldest memtable against other memtables before flushing the\r\n    oldest memtable into L0.<br>\r\n    In Ceph Bluestore, it uses rocksdb to store metadata and WAL logs. For small write\r\n    IOs(default < 16k), it at first writes the data to Rocksdb, secondly writes the data\r\n    to disk, and then deletes the data in Rocksdb. With the merge style, it makes the\r\n    data written to SST files as little as possible. This especially can benefit when the\r\n    disk is busy.<br>\r\n    Signed-off-by: Xiaoyan Li <xiaoyan.li@intel.com>", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "assafein": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/2795", "title": "Align persistent cache blocks with device pages", "body": "When persistent cache appends blocks to a file, these blocks may spread over several device pages. In order to read these blocks, the cache has to read their corresponding pages entirely, potentially creating significant read amplification. For example, to read a 4K block that does not start at the beginning of a page (and thus, spreads over two pages), the cache will actually have to read 8K.\r\nTo reduce read bandwidth from the persistent cache device, this diff introduces an option to align blocks with pages, such that each block will spread over the minimal number of pages possible. When needed, a page will be padded with zeros in order to align a block with the beginning of next page.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mkosieradzki": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/2780", "title": "Replication-related improvements", "body": "Solves #2775 and #2771", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stevelittle": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/1831", "title": "Fix for Issue:1780", "body": "https://github.com/facebook/rocksdb/issues/1780\r\n\r\nMove the check of lockedFiles to before we attempt to open the lock file\r\nThis way we avoid opening a file which might already be open.\r\nMore importantly, we avoid *closing* that file, which would release any\r\nfcntl locks that we have, enabling concurrent access from other processes\r\nand potential file corruption", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "min-mwei": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/1682", "title": "add column family support for TxLog iteration", "body": "\"From Dhruba Borthakur: WriterBatch handler does not expose the column family. If you can make a patch for it and contribute it back to the rocksdb repository, that would be fantastic!\" \r\n\r\nThis adds some of the supported callbacks from the C++ side. I made the new callback methods non-abstract for backwards compatibility. ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hshankar": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/1570", "title": "Changing Java multiGet API to return List<byte[]> instead of Map<byte[], byte[]>", "body": "Changing Java `multiGet` API to return `List<byte[]>` instead of `Map<byte[], byte[]>`. This is for two reasons:\r\n1. Java arrays implement `equals()` as reference equality instead\r\nof comparing each value. This means a HashMap should never have an array\r\nas key, since anyone calling `get()` with a new array key with the exact same contents\r\nas an existing key will get different results. It happens to work\r\ncurrently because the `byte[]` reference is used as it is in the `Map` key.\r\nBut it is a confusing API for the caller because they cannot make this\r\nassumption.\r\n2. `List<byte[]>` keeps it close to the native API (which returns\r\n`byte[][]`). It avoids the perf cost of creating a `Map`. If a caller does\r\nnot need `Map` functionality, the API should not force it unnecessarily,\r\nespecially in perf-critical use cases.\r\n\r\nIdeally, the existing method should be deprecated and the new one added.\r\nBut that would require creating a new method name. multiGet is the best\r\nname for this method. So I am trying to avoid changing it. This should\r\nbe a simple change for the clients to make, so as long as the major version\r\nis bumped up, it should be ok to break backwards compatibility. Please suggest\r\nif you think there are better ways to do this. I am not familiar with the rocksdb\r\nversioning protocol.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "maxogden": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28611483", "body": "looks like they're working on it but \"The work to make sure mac os compiles rocksdb is not completed yet.\": https://github.com/facebook/rocksdb/commit/21587760b912aa133760ec10c38912b051a86f2c\n\nalso is there a public issue tracker other than this one?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28611483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "liukai": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28612364", "body": "@mreiferson I'm still working on it :-)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28612364/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28612518", "body": "Fixed, thank you for pointing out.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28612518/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28727178", "body": "Rocksdb now works on Mac. Please refer to https://github.com/facebook/rocksdb/blob/master/COMPILATION.md to get more details.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28727178/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28728172", "body": "@deepak1556, a quick question, can you successfully run `/proc/sys/kernel/random/uuid` in your platform?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28728172/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28731126", "body": ":+1: \n@igorcanadi  you're awesome!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28731126/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28873073", "body": "Thanks, already fixed.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28873073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28937921", "body": "I think I'll revise the installation guide for mac users. Seems updating to the latest xcode is not enough.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28937921/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28939045", "body": "@wildthink, can you try the updated instructions for mac again: https://github.com/facebook/rocksdb/blob/master/INSTALL.md\n\nLet me know if there is still any problem.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28939045/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29924620", "body": "@spiritloose can you give us some information about your development environment?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29924620/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29966454", "body": "I got your point @spiritloose, I submitted a diff and hope that can help you to solve the problem.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29966454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31231519", "body": "@icycrystal4, I used gcc 4.8 in maverick but didn't see such problem.\nI'll take a look at this and fix it. \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31231519/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31231744", "body": "That makes sense.\n\nI'll assign this to myself for now. Once I have bandwidth I'll be glad to add this feature. But if some one else is interested, please feel free to send us the feature request.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31231744/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31231934", "body": "Assign to me for now. Pull request welcomed.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31231934/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31238300", "body": "It looks pretty good, thank you!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31238300/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31474480", "body": "Can you elaborate this problem a little bit more? What are the \"useless values\"?\n\nRight now when compacting sst files, we'll not load the whole file; instead, we'll read 1 block (by default it is 4k) to the memory.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31474480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31676369", "body": "Will take a look.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31676369/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31677000", "body": "seems `size_t` is defined differently in your environment. I've get rid the mix use of `unsigned long` and `size_t` and hope it can solve your problem.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31677000/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31680573", "body": "New version looks much more clear, thank you!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31680573/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/32052286", "body": "Thanks for the fix.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/32052286/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34490658", "body": "I just checked our internal third-party release libraries: we are able to access lz4 library in our internal release.\n\nI think with this diff, other team can benefit from lz4 seamlessly in next release.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34490658/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34515645", "body": "@alberts I think the lint issue showed up because you didn't run \"arc lint\" or \"arc diff\" in rocksdb root dir (sorry that's a known issue).\n\nSimilarly make format works for uncommitted files or your last committed diff. If you want to apply `make format` to all the lines you've touched so far, can you squash your diffs into one (which is also a recommended at Facebook: we use \"git commit --amend\" to make sure one code review associates with one git commit).\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34515645/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34830800", "body": "I use clang 5.0 MacOS but didn't reproduce it (but I do saw that problem in some other platform).\n\nI'll submit a diff to fix #3 soon.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34830800/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/4643173", "body": "As our primary production platform is Linux, We do not (or plan to) run any production workload on mac.\nWe made sure rocksdb can be compiled on OSX just for convenience :-)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4643173/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4698738", "body": "Igor, -fPIC will break the 3rd party release.\n\nWe'd enable -fPIC only when we are not in development environment. I'll come up with another diff to fix this.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4698738/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4971181", "body": "@igorcanadi that's right, thank you!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4971181/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5017237", "body": "merging back might be a little bit difficult. Previous I made changes in performance since comparatively it's like a superset of master branch. Merging back to master will result in quite some conflicts.\n\nDo you have any task depend on this change? \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5017237/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5103654", "body": "It does take a long time to compile.\n\nTo speed up our daily development, I'll separate that process to `make shared`.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5103654/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5273945", "body": "I actually fine with the change.\nWe've resolve this issue once. I think git merge, with all the commit history is smart to avoid doing the conflict resolution in next merge. Even if there's conflict, I think resolving it is very simple :-)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5273945/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5342960", "body": "Good catch. Will change it my current diff\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5342960/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "dhruba": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28620393", "body": "That was the original License from open source leveldb and we did not want to change the original license. So, the original licenses is retained lines-for-line.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28620393/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28671925", "body": "Can you pl provide more \"detailed\" info on how to reproduce this problem. wt platform? wt compiler?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28671925/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28672286", "body": "https://reviews.facebook.net/\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28672286/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28672334", "body": "Can we update the INSTALL.md (or its equivalent) to let people know how to install snappy?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28672334/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29479831", "body": "Thus should be fixed via https://github.com/facebook/rocksdb/commit/98968ba937f5be35b8de84fa1ff2764808ca85ce\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29479831/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29479842", "body": "Please reopen task if your memory leak persists \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29479842/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29543986", "body": "Fixed https://github.com/facebook/rocksdb/commit/38feca4f35ca123d71d869b8f7bb814bf442ea12\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29543986/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29544041", "body": "If you look at https://github.com/facebook/rocksdb/wiki/Performance-Benchmarks#2-bulk-load-of-keys-in-random-order, you will find the command line options to load data in bulk-load fashion.\n\necho \"Bulk load database into L0....\"\nbpl=10485760;overlap=10;mcz=2;del=300000000;levels=2;ctrig=10000000; delay=10000000; stop=10000000; wbn=30; mbc=20; mb=1073741824;wbs=268435456; dds=1; sync=0; r=1000000000; t=1; vs=800; bs=65536; cs=1048576; of=500000; si=1000000; ./db_bench --benchmarks=fillrandom --disable_seek_compaction=1 --mmap_read=0 --statistics=1 --histogram=1 --num=$r --threads=$t --value_size=$vs --block_size=$bs --cache_size=$cs --bloom_bits=10 --cache_numshardbits=4 --open_files=$of --verify_checksum=1 --db=/data/mysql/leveldb/test --sync=$sync --disable_wal=1 --compression_type=zlib --stats_interval=$si --compression_ratio=50 --disable_data_sync=$dds --write_buffer_size=$wbs --target_file_size_base=$mb --max_write_buffer_number=$wbn --max_background_compactions=$mbc --level0_file_num_compaction_trigger=$ctrig --level0_slowdown_writes_trigger=$delay --level0_stop_writes_trigger=$stop --num_levels=$levels --delete_obsolete_files_period_micros=$del --min_level_to_compress=$mcz --max_grandparent_overlap_factor=$overlap --stats_per_interval=1 --max_bytes_for_level_base=$bpl --memtablerep=vector --use_existing_db=0 --disable_auto_compactions=1 --source_compaction_factor=10000000\necho \"Running manual compaction to do a global sort map-reduce style....\"\nbpl=10485760;overlap=10;mcz=2;del=300000000;levels=2;ctrig=10000000; delay=10000000; stop=10000000; wbn=30; mbc=20; mb=1073741824;wbs=268435456; dds=1; sync=0; r=1000000000; t=1; vs=800; bs=65536; cs=1048576; of=500000; si=1000000; ./db_bench --benchmarks=compact --disable_seek_compaction=1 --mmap_read=0 --statistics=1 --histogram=1 --num=$r --threads=$t --value_size=$vs --block_size=$bs --cache_size=$cs --bloom_bits=10 --cache_numshardbits=4 --open_files=$of --verify_checksum=1 --db=/data/mysql/leveldb/test --sync=$sync --disable_wal=1 --compression_type=zlib --stats_interval=$si --compression_ratio=50 --disable_data_sync=$dds --write_buffer_size=$wbs --target_file_size_base=$mb --max_write_buffer_number=$wbn --max_background_compactions=$mbc --level0_file_num_compaction_trigger=$ctrig --level0_slowdown_writes_trigger=$delay --level0_stop_writes_trigger=$stop --num_levels=$levels --delete_obsolete_files_period_micros=$del --min_level_to_compress=$mcz --max_grandparent_overlap_factor=$overlap --stats_per_interval=1 --max_bytes_for_level_base=$bpl --memtablerep=vector --use_existing_db=1 --disable_auto_compactions=1 --source_compaction_factor=10000000\ndu -s -k test\n504730832   test\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29544041/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30113094", "body": "The rocksdb C++ api is far more richer than the leveldb api. But we have (sadly) failed to enhance the C api in the same way. If you would like to bring the C api in par with C++ api, that will be a great contribution to the rocksdb github repo.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30113094/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30179267", "body": "This is very interesting.\n\n> db/db_impl.cc: result.block_cache = NewLRUCache(67108864);\n\nAt first, i thought that you have a sign-extension problem with the parameter that you are passing to NewLRUCache, where you are specifying a v v large size for the cache. But maybe not.\n\nCan you pl send me the first 500 lines of your log file (named LOG)? It will list the configs that the DB is using.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30179267/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30528820", "body": "I have not run any benchmarks comparing the two. If you do run any benchmarks, can you pl share it with us?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30528820/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31140753", "body": "The design discussions happen at https://www.facebook.com/groups/rocksdb.dev/, please join this group, \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31140753/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34407052", "body": "I have not (yet) looked at the new logs, but I have never seen that increasing/decreasing target_file_size_base  has much effect on performance.  \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34407052/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34490815", "body": "Ur write amplification is\n\nAmplification cumulative: 3.8 write, 5.5 compaction\nWrites cumulative: 699713 total, 699713 batches, 1.0 per batch, 399.36 ingest GB\n\nMost of the stalls occur because of too many files in L0. Compaction is using  upto 100 MB/sec. Total data ingest is about 400 GB in about 100 minutes, which is about 70 MB/sec. With a write amplification of 5, you are writing 350 MB/sec to the flash drive. I suspect that you are maxing out the throughput on the flash storage.\nWrites cumulative: 699713 total, 699713 batches, 1.0 per batch, 399.36 ingest GB\n\nOne alternative to reduce stalls is to use Universal compaction and shard your data into 10 rocksdb database, each with 20 GB in size. Is this possible?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34490815/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34604982", "body": "great request, especially for an in-memory database.\n\nwe should start refactoring the rocksdb code to make checksum pluggable. \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34604982/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34605037", "body": "we can make checksum optional, but will be bad for real production use-case. i like ur other idea of using xxhash\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34605037/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35782569", "body": "This is a good patch and definitely adds lots of value to RocksDB. Thanks for submitting the pull request. I have a request for some slight enhancements.\n\nIn the current code, each table file has a 5 bytes footer. The first byte specifies the type of CompressionType. This patch overloads this byte to also store the ChecksumType. This works, but one problem is that if we ever want to add some more metadata to the footer, we will be backward incompatible. To solve this problem, I have the following suggestion:\n\nUse the existing single-byte in the footer to represent two things: a TableFormatVersion and a CompressionType. The first four bits could be TableFormatVersion and the remaining 4 bits could be CompressionType. This will keep it backward compatible. We could define all pre-existing tables to have a TableFormatVersion=0, while this patch will set TableFormatVersion=1 for all tables that will be created in the future. For tables that have TableFormatVersion of 1, the footer will be 6 bytes (instead of 5 bytes). The additional byte can store the ChecksumType.\n\nThe reason I prefer the above approach is that it allows us to be able to store additional metadata with each table in the future without introducing backward incompatibility. Does this sound reasonable?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35782569/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36093318", "body": "i would vote for keeping crc32c as the default, but add Options to set it to xxhash if requested.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36093318/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36969856", "body": "The above comments show how to do bulk load which should solve your problem\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36969856/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/4584830", "body": "Yes!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4584830/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4590840", "body": "I think you are right \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4590840/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5103191", "body": "igor: can you instead do\nmake OPT=-O2 -DNDEBUG librocksdb.a\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/5103191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/6071161", "body": "It will be great if all (relevant) unit tests run to success even when using -DNDEBUG or not using -DNDEBUG.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/6071161/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17945521", "body": "@IvRRimum do you have a workaround for this problem?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17945521/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41431699", "body": "Maybe we need a IOSTATS_TIMER_GUARD here\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41431699/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41433208", "body": "It makes sense to make the default 'false' for now because this is a critical piece of code and might need some time to bake. But in the future, we should make the default to be 'true' so that we can get the perf advantage for all use-cases.\n\nBTW, how many log-files does it recycle?  Will it always alternate between two log files?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41433208/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "kgcrom": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28627708", "body": "I'm using 4.8.1. \n\n```\nkgcrom@lucas:~/sponge$ g++ --version\ng++ (Ubuntu 4.8.1-2ubuntu1~12.04) 4.8.1\nCopyright (C) 2013 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n```\n\nHave you finised compile without above header? \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28627708/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28706618", "body": "@igorcanadi @vinothchandar  :+1:  with only install build-essentials and upgrade gcc/g++, install gflags\n\nThank you\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28706618/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "mreiferson": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28634218", "body": "@liukai good to hear :)\n\nIf it's at all helpful here is my WIP: https://github.com/mreiferson/rocksdb/commit/906ceafdda7826fddb0321d1e7a43c4dd41de801\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28634218/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "pborreli": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28639943", "body": "@igorcanadi already signed facebook's CLA for facebook/react-page#8 (sending signed pdf to @stoyan)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28639943/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "veeve": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28641232", "body": "@pborreli - thanks for the contribution\n\ngithub needs a like button :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28641232/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dyu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28646477", "body": "I'm getting these erros:\ndb/db_bench.cc:14:27: fatal error: gflags/gflags.h: No such file or directory\ncompilation terminated.\n\non ubuntu 13.04 gcc 4.7.3\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28646477/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28696080", "body": "A complete list of dependencies (required and optional) would be a good start.\nThe docs has been pretty good so far except for install/build docs :-)\n\nI successfully compiled after installing libgflags-dev (thanks @mdcallag).\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28696080/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31249065", "body": "Perhaps the instructions should be in INSTALL.md then.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31249065/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31274159", "body": "Take a look at the patch from https://github.com/facebook/rocksdb/pull/48 (apply the fix in the comment of @vmg)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31274159/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31274204", "body": "@vmg's fix is the correct one.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31274204/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/4724246", "body": "For one, they use an ancient version of the linux kernel (2.x with upstream patches).  I'm guessing its probably similar to google (as they have a lot of ex-googlers).\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4724246/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "jkeys089": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28651836", "body": "Do you have bz2 and snappy libs installed?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28651836/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28652273", "body": "I just pulled in everything in build_tools/build_detect_platform (https://github.com/facebook/rocksdb/blob/master/build_tools/build_detect_platform#L186) and the tests now run fine for me.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28652273/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "vinothchandar": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28664649", "body": "I am trying to add a storage engine to Voldemort based on rocksdb. Also running into trouble compiling snappy on Ubuntu 12.04 LTS. Seems like the makefile has some hardcoded paths..\n\n```\n$:~/projects/rocksdb/snappy/snappy-1.0.5$ make\nCDPATH=\"${ZSH_VERSION+.}:\" && cd . && /bin/sh /home/dhruba/local/externals/java/leveldb/source/snappy-1.0.5/missing --run aclocal-1.11 -I m4\n/bin/sh: 0: Can't open /home/dhruba/local/externals/java/leveldb/source/snappy-1.0.5/missing\nmake: *** [aclocal.m4] Error 12\n$:~/projects/rocksdb/snappy/snappy-1.0.5$ grep -Ri \"dhruba\" Makefile | wc -l\n11\n```\n\nIt would be great if you guys can put together a guide on building.. \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28664649/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28703440", "body": "Works now.. Thanks guys! \n\nFor others: On a fresh 12.04 LTS, the process is as roughly follows\n\n```\n1. sudo apt-get install libsnappy-dev\n2. Upgrade gcc/g++ to 4.8\nSee http://askubuntu.com/questions/312620/how-do-i-install-gcc-4-8-1-on-ubuntu-13-04\n(it is for 13.04. but the process is common I think)\n3. sudo apt-get install build-essentials devscripts\n4. Install gflags\n(its not 1-command if you are < 12.10)\nsee http://askubuntu.com/questions/312173/installing-gflags-12-04?rq=1\n\n\n```\n\nthen make, make install in rocksdb and done. \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28703440/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28732150", "body": "+1  or should I say \"like\" :P .. Thanks for adding the installation instructions guys.. \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28732150/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "deepak1556": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28678040", "body": "platform  - 3.10.9-200.fc19.x86_64\ncompiler - gcc version 4.8.1\n\nrunning `make check` on the latest master caused this error. was able to successfully compile the package.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28678040/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28729775", "body": "@liukai yup able to run it successfully. generates random uuid\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28729775/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "wildthink": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28945187", "body": "Not there yet :-(\nRunning OSX 10.8.5 (Mountain Lion)\n\nrocksdb git:(master) \u2717 gcc -v\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\nApple LLVM version 5.0 (clang-500.2.79) (based on LLVM 3.3svn)\nTarget: x86_64-apple-darwin12.5.0\n\n## Thread model: posix\n\n\u279c  rocksdb git:(master) \u2717 xcode-select --version\nxcode-select version 2311.\n\u279c  rocksdb git:(master) \u2717 xcode-select --install\nxcode-select: Error: unknown command option '--install'.\n\nxcode-select: Report or change the path to the active\n              Xcode installation for this machine.\n\nUsage: xcode-select --print-path\n           Prints the path of the active Xcode folder\n   or: xcode-select --switch <xcode_path>\n           Sets the path for the active Xcode folder\n   or: xcode-select --version\n\n##            Prints the version of xcode-select\n\nbrew tap homebrew/dupes; brew install gcc47 --use-llvm\n=>\n\u279c  rocksdb git:(master) \u2717 brew tap homebrew/dupes; brew install gcc47 --use-llvm\nError: Already tapped!\nError: No available formula for gcc47 \nSearching taps...\nError: 404 Not Found\nPlease report this bug:\n    https://github.com/mxcl/homebrew/wiki/troubleshooting\n/System/Library/Frameworks/Ruby.framework/Versions/1.8/usr/lib/ruby/1.8/open-uri.rb:277:in `open_http'\n/usr/local/Library/Homebrew/cmd/search.rb:99:in`value'\n/usr/local/Library/Homebrew/cmd/search.rb:99:in `search_taps'\n/System/Library/Frameworks/Ruby.framework/Versions/1.8/usr/lib/ruby/1.8/open-uri.rb:230:in`inject'\n/usr/local/Library/Homebrew/cmd/search.rb:96:in `each'\n/usr/local/Library/Homebrew/cmd/search.rb:96:in`inject'\n/usr/local/Library/Homebrew/cmd/search.rb:96:in `search_taps'\n/usr/local/Library/Homebrew/cmd/install.rb:41:in`install'\n/usr/local/Library/brew.rb:91:in `send'\n/usr/local/Library/brew.rb:91\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28945187/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "juneng603": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28953581", "body": "@wildthink mayby you need to install a newer version of xcode.\n\n```\n$  xcode-select --version\nxcode-select version 2333.\n```\n\nand the option '--install' of a xcode-select command is working for me.\n\nif it is not the latest version of a xcode, 'xcode-select --install' should work normally. please, try to install a xcode again, It will be better to quick-fix for this problem\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/28953581/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29590842", "body": "Oops :0 thanks for your quick fix.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29590842/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30101938", "body": "In case of me, this one doesn't work.\n\n```\nbrew tap homebrew/dupes; brew install gcc47 --use-llvm\n```\n\ninstead of it, the below line makes me working on it.\n\n```\nbrew tap homebrew/version; brew install gcc47 --use-llvm\n```\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30101938/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30192923", "body": ":+1: \n\n@sepeth if you want to some hands for your contributions, I could be with you. :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30192923/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30213797", "body": "@sepeth ok, I've checked your branch for the jobs. Lets think about how we can implement it in a best way.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30213797/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/4639997", "body": "depending on your commits, posix_fadvice() or Fadvice will not be anything in Mac OS. Is this no side-effects for the performance on it?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4639997/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "beginnerlan": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29137869", "body": "Append /usr/local/lib to LD_LIBRARY_PATH make it work. I have libgflags.so and libsnappy,so in /usr/local/lib\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29137869/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "Photonios": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29315127", "body": "No, you don't want to do that either. Using position independent code on x86 (when not required) slows down execution, even when the loss is minimal. Also, as @liukai pointed out in a comment on your commit, it apparently breaks 3rd party releases. Maybe a separate makefile option to at least be able to build with -fPIC without modifying the make file? Like:\n\nmake fpic \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29315127/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29320134", "body": "It still builds here, so yes :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29320134/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/4706583", "body": "It should be noted that 'libgflags-dev' is not in all well-known repositories.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4706583/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4706610", "body": "This is exactly my commit.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4706610/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4706692", "body": "Maybe you would like to share, how and why it's different. I guess other people (including myself) would love to know what makes Facebook's environment different from everybody else... :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4706692/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "stinkymatt": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29360495", "body": "Well this is weird. A git pull/make clean seems to have cleared this up.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29360495/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "Kangmo": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29491257", "body": "Thanks, Dhruba for the quick fix, I will run the test again, and let you know the result within 4 hours.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29491257/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29492751", "body": "After the fix  98968ba, the issue is gone. Thanks, Dhruba!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29492751/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29499438", "body": "Good point, @igorcanadi. Definitely I missed the link you provided. \n\nBecause apt-get libgflags-dev works in Ubuntu 13, and even apt-get does not work in CentOS,\nHow about I rewrite INSTALL.md to have different sections for (1) Ubuntu 12, (2) Ubuntu 13 (3) CentOS instead of simple Linux section?\n\nIn user's perspective running apt-get is much quick and easy than downloading gflags and building it in Ubuntu 13.\nAlso Newbies in CentOS may frustrated after running apt-get on CentOS. ;-)\n\n-Kangmo\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29499438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "PragathaM": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29529729", "body": "ReadMe has below reference for documentation\nSee doc/index.html for more explanation.\nSee doc/impl.html for a brief overview of the implementation.\n\nBut Under doc folder impl.html is missing as per read me text.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29529729/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "yedf": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29591515", "body": "My calculation has included the read IO, so I include \"2*\" term.\nGiven the speed of read/write is 126k/17k, just about 7.5, Then I think the io-amp is the bottleneck. Maybe making the memtable huge or keep more memtables in memory is a worthy strategy.\n\nAnd another non trivial strategy is to split range dynamicly like bigtable or hbase. Then it can keep the number of levels low.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29591515/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "lclc": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29846497", "body": "./db_test: error while loading shared libraries: libgflags.so.2: cannot open shared object file: No such file or directory\n\nMy libgflags.so is in /usr/local/lib64/\nI've made a link in /usr/lib64 but that didn't help. Where does it look for this lib?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29846497/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30250887", "body": "Hm, I tried again today (without any changes) and now it works..\nThanks for your help.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30250887/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "spiritloose": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29956917", "body": "@liukai My development environments are:\n\nMac OS X Mavericks\n\n```\n$ cat .git/refs/heads/master ; uname -a ; g++ -v\n92e8316118b8bf330e1e7f025252be380940e941\nDarwin jiro.local 13.0.0 Darwin Kernel Version 13.0.0: Thu Sep 19 22:22:27 PDT 2013; root:xnu-2422.1.72~6/RELEASE_X86_64 x86_64\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\nApple LLVM version 5.0 (clang-500.2.79) (based on LLVM 3.3svn)\nTarget: x86_64-apple-darwin13.0.0\nThread model: posix\n```\n\nUbuntu 13.10 (Saucy Salamander) x86_64 on Virtual Box (host is Mac)\n\n```\n$ cat .git/refs/heads/master ; uname -a ; g++ -v\n92e8316118b8bf330e1e7f025252be380940e941\nLinux vagrant-ubuntu-saucy-64 3.11.0-14-generic #21-Ubuntu SMP Tue Nov 12 17:04:55 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux\nUsing built-in specs.\nCOLLECT_GCC=g++\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/4.8/lto-wrapper\nTarget: x86_64-linux-gnu\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu/Linaro 4.8.1-10ubuntu9' --with-bugurl=file:///usr/share/doc/gcc-4.8/README.Bugs --enable-languages=c,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-4.8 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --with-gxx-include-dir=/usr/include/c++/4.8 --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-gnu-unique-object --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\nThread model: posix\ngcc version 4.8.1 (Ubuntu/Linaro 4.8.1-10ubuntu9)\n```\n\nI think that any one cc file which is linked should #include \"db/db_statistics.h\" to make the function public.\n\nRegards,\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29956917/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29966534", "body": "Thanks! I already confirmed that diff!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29966534/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29956917", "body": "@liukai My development environments are:\n\nMac OS X Mavericks\n\n```\n$ cat .git/refs/heads/master ; uname -a ; g++ -v\n92e8316118b8bf330e1e7f025252be380940e941\nDarwin jiro.local 13.0.0 Darwin Kernel Version 13.0.0: Thu Sep 19 22:22:27 PDT 2013; root:xnu-2422.1.72~6/RELEASE_X86_64 x86_64\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\nApple LLVM version 5.0 (clang-500.2.79) (based on LLVM 3.3svn)\nTarget: x86_64-apple-darwin13.0.0\nThread model: posix\n```\n\nUbuntu 13.10 (Saucy Salamander) x86_64 on Virtual Box (host is Mac)\n\n```\n$ cat .git/refs/heads/master ; uname -a ; g++ -v\n92e8316118b8bf330e1e7f025252be380940e941\nLinux vagrant-ubuntu-saucy-64 3.11.0-14-generic #21-Ubuntu SMP Tue Nov 12 17:04:55 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux\nUsing built-in specs.\nCOLLECT_GCC=g++\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/4.8/lto-wrapper\nTarget: x86_64-linux-gnu\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu/Linaro 4.8.1-10ubuntu9' --with-bugurl=file:///usr/share/doc/gcc-4.8/README.Bugs --enable-languages=c,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-4.8 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --with-gxx-include-dir=/usr/include/c++/4.8 --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-gnu-unique-object --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\nThread model: posix\ngcc version 4.8.1 (Ubuntu/Linaro 4.8.1-10ubuntu9)\n```\n\nI think that any one cc file which is linked should #include \"db/db_statistics.h\" to make the function public.\n\nRegards,\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29956917/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29966534", "body": "Thanks! I already confirmed that diff!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/29966534/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "biendltb": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30078537", "body": "A nice tut:\nhttp://bienuit.wordpress.com/2013/12/04/how-to-install-rocksdb-on-ubuntu-12-04-lts/\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30078537/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "sepeth": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30113772", "body": "Great then, I am on it :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30113772/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30205058", "body": "Updated pull request and signed the CLA.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30205058/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30208856", "body": "@igorcanadi I sent it :)\n\n@junyoungkim Great :) I started with renames, and I will keep c-api branch updated in my fork as I progress. We can continue from there, I think.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30208856/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30245122", "body": "Yes\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30245122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31248953", "body": "In order to create shared lib, you should run `make librocksdb.so`.\n\nIt is not at the default target, because facebook does not use shared libs. See #29 \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31248953/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31272934", "body": "what does your `build_config.mk` file look like?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31272934/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31377162", "body": "@ramanala hello, hardest part of it is to complete C api, I believe. I've been doing somethings on that part (but some other things in my life kept me away from doing it), and still stuck with merge operator. In the mean time, I am playing with rocksdb from python. It seems easy to do with ctypes.\n\n@stephan-hof Thank you. I am also considering CFFI, beside ctypes. and also pypy seems to suggest CFFI too.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31377162/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31377381", "body": "See #50 \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31377381/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "ankgup87": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30197308", "body": "Hi Dhruba,\n\nThanks for following up on this issue. I actually found a memory leak in JNI layer (will update JNI repo with fix). \n\nWill close the issue. Thanks again and apologies for raising a false alarm :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30197308/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31580683", "body": "Hi James,\n\nCan you please post statistics regarding rocksdb compactions obtained by calling getProperty function with rocksdb.stats as the parameter. This will give information about stalls due to compactions and also how much IO cost is incurred due to compactions.\n\nRegarding long time taken during DB opening, please check size of your manifest file. I have seen that DB takes long time to open due to large manifest file size which can be controlled by max_manifest_file_size. \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31580683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31654405", "body": "Hi James,\n\nPlease limit manifest file small if you want DB to open quickly. In my tests, DB opens in less than a minute if manifest file is < 20MB. \n\nRocksdb developers might have a better idea as to what is the optimum size of manifest file.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31654405/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/12904521", "body": "Can you please add javadoc for this method explaining what this method is for and when this method should be called.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/12904521/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/18405348", "body": "Should this depend on rocksdbjavastaticrelease?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/18405348/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/18501625", "body": "Can you please move this documentation \"Bloom filter policy that uses a bloom filter with approximately the specified number of bits per key.\" to javadoc?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/18501625/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/18501639", "body": "If \"Bloom filter policy that uses a bloom filter with approximately the specified number of bits per key.\" is moved to javadoc then this can be deleted from here.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/18501639/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/18501666", "body": "Documentation about bits_per_key and use_block_based_builder is put on all 3 constructors. Can you please put it on only one constructor (preferably one which takes all the parameters).\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/18501666/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16159350", "body": "Can you please comment if you want this class to be package private? Also, can you please add javadocs for this class and all of the methods?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16159350/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16178436", "body": "Can you please comment why AbstractSlice (base class) contains a method for DirectSlice (sub-class)?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16178436/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16179098", "body": "Instead of using setHandle, please return handle from this function. This way AbstractSliceJni::setHandle and corresponding functions in AbstractSliceJni will not be needed.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16179098/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16181186", "body": "Yes, you are right and we will have to release data in dispose call. If you call env->ReleaseByteArrayElements in dispose function then you will also have to remember jbytearray. Will it better if you memcpy the ptrData obtained from env->GetByteArrayElements? This way you won't have to call env->ReleaseByteArrayElements in dispose function and you can just call delete on the natively allocated char*:\n1. jbyte\\* ptrData = env->GetByteArrayElements(data, &isCopy);\n2. char\\* buf = new char[len];\n3. memcpy from ptrData, buf\n4. Initialize Slice with buf\n   5.relesaeByteArrayElements\n\nIn dispose, just delete buf and you are done.\n\nThoughts?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16181186/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16181338", "body": "please delete underlying data before calling clear.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16181338/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16195601", "body": "please insert new line at end of the file.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16195601/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16195638", "body": "please add documentation for this class.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16195638/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16196983", "body": "as this will be executed in multi-threaded code, can you please comment how using class member variables for java slices (m_jSliceA, m_jSliceB) will work? Won't multiple threads override this value?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16196983/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16199406", "body": "m_name was not allocated on heap and thus delete can't be used. It will be garbage collected when this variable goes out of scope.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16199406/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16199556", "body": "do you need to copy utf into some native buffer before releasing utf? Won't releasing utf release memory held by utf and this name will be pointing to invalid pointer?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16199556/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/18377974", "body": "Hi Adam,\n\nApologies for getting back so late on this comment. Can you please comment why it will cause object creation overhead if we return long back from these functions? These JNI functions will just return long and handle will be set in JAVA classes. I looked at the code where this setHandle function is called and it seems that in most places this long value can be easily returned.\n\nPlease correct me if I am wrong.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/18377974/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "mmazaheri": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30562095", "body": "I simply asking about functionality and their support and reliability from a developer perspective.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30562095/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "gauravchaudhary2": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30987849", "body": " Resolved by: adding  \"PLATFORM_LDFLAGS=\"$PLATFORM_LDFLAGS -lgflags\" to 187th line of (build_detect_platform) file of (build_tools) folder of rocksdb database\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/30987849/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "icycrystal4": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31103311", "body": "Does it means i successfuly build the rocksdb on my Mavericks?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31103311/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31103587", "body": "Does it means i successfuly build the rocksdb on my Mavericks?\nJust cause i comment off the \"table_properties_collector_test\" in the Makefile, everything goes right.\n\nll | awk '{print \"    \"$0}'\n\n```\ntotal 228056\n-rw-r--r--    1 crystal  staff       607 Dec 20 17:40 CONTRIBUTING.md\n-rw-r--r--    1 crystal  staff      2431 Dec 20 17:40 INSTALL.md\n-rw-r--r--    1 crystal  staff      1646 Dec 20 17:40 LICENSE\n-rw-r--r--    1 crystal  staff     16895 Dec 23 11:56 Makefile\n-rw-r--r--    1 crystal  staff     17154 Dec 23 11:55 Makefile.12.23\n-rw-r--r--    1 crystal  staff      1408 Dec 20 17:40 PATENTS\n-rw-r--r--    1 crystal  staff      2892 Dec 20 17:40 README\n-rw-r--r--    1 crystal  staff        96 Dec 20 17:40 README.fb\n-rwxr-xr-x    1 crystal  staff   2112804 Dec 23 11:56 arena_test\n-rwxr-xr-x    1 crystal  staff   2153804 Dec 23 11:56 auto_roll_logger_test\n-rwxr-xr-x    1 crystal  staff   2164356 Dec 23 11:58 backupable_db_test\n-rwxr-xr-x    1 crystal  staff   2111420 Dec 23 11:59 blob_store_bench\n-rwxr-xr-x    1 crystal  staff   2143444 Dec 23 11:56 blob_store_test\n-rwxr-xr-x    1 crystal  staff   2115468 Dec 23 11:56 block_test\n-rwxr-xr-x    1 crystal  staff   2125764 Dec 23 11:56 bloom_test\n-rw-r--r--    1 crystal  staff      2165 Dec 23 11:56 build_config.mk\ndrwxr-xr-x   10 crystal  staff       340 Dec 23 10:32 build_tools\n-rwxr-xr-x    1 crystal  staff   2110916 Dec 23 11:56 c_test\n-rwxr-xr-x    1 crystal  staff   2187076 Dec 23 11:56 cache_test\n-rwxr-xr-x    1 crystal  staff   2188312 Dec 23 11:56 coding_test\n-rwxr-xr-x    1 crystal  staff   2180664 Dec 23 11:56 corruption_test\ndrwxr-xr-x    4 crystal  staff       136 Dec 20 17:40 coverage\n-rwxr-xr-x    1 crystal  staff   2116976 Dec 23 11:56 crc32c_test\ndrwxr-xr-x  128 crystal  staff      4352 Dec 23 11:58 db\n-rwxr-xr-x    1 crystal  staff   2248372 Dec 23 11:56 db_bench\n-rwxr-xr-x    1 crystal  staff   2106236 Dec 23 11:59 db_repl_stress\n-rwxr-xr-x    1 crystal  staff   2193412 Dec 23 11:59 db_stress\n-rwxr-xr-x    1 crystal  staff   2688068 Dec 23 11:56 db_test\n-rwxr-xr-x    1 crystal  staff   2130324 Dec 23 11:56 dbformat_test\n-rwxr-xr-x    1 crystal  staff   2165124 Dec 23 11:58 deletefile_test\ndrwxr-xr-x    7 crystal  staff       238 Dec 20 17:40 doc\n-rwxr-xr-x    1 crystal  staff   2132304 Dec 23 11:56 env_test\n-rwxr-xr-x    1 crystal  staff   2111556 Dec 23 11:56 filelock_test\n-rwxr-xr-x    1 crystal  staff   2128648 Dec 23 11:57 filename_test\n-rwxr-xr-x    1 crystal  staff   2151800 Dec 23 11:57 filter_block_test\ndrwxr-xr-x    6 crystal  staff       204 Dec 20 17:40 hdfs\ndrwxr-xr-x    3 crystal  staff       102 Dec 20 17:40 helpers\n-rwxr-xr-x    1 crystal  staff   2120948 Dec 23 11:57 histogram_test\ndrwxr-xr-x    4 crystal  staff       136 Dec 20 17:40 include\n-rwxr-xr-x    1 crystal  staff   2098664 Dec 23 11:59 ldb\n-rw-r--r--    1 crystal  staff    704672 Dec 23 11:57 libmemenv.a\n-rw-r--r--    1 crystal  staff  29860872 Dec 23 11:56 librocksdb.a\ndrwxr-xr-x    3 crystal  staff       102 Dec 20 17:40 linters\n-rwxr-xr-x    1 crystal  staff   2221864 Dec 23 11:57 log_test\n-rwxr-xr-x    1 crystal  staff   2119824 Dec 23 11:57 manual_compaction_test\n-rwxr-xr-x    1 crystal  staff   1548728 Dec 23 11:57 memenv_test\n-rwxr-xr-x    1 crystal  staff   2134028 Dec 23 11:57 merge_test\ndrwxr-xr-x   14 crystal  staff       476 Dec 23 11:13 port\n-rwxr-xr-x    1 crystal  staff   2258772 Dec 23 11:57 redis_test\n-rwxr-xr-x    1 crystal  staff   2159780 Dec 23 11:57 reduce_levels_test\n-rwxr-xr-x    1 crystal  staff   2094136 Dec 23 11:56 signal_test\n-rwxr-xr-x    1 crystal  staff   2202616 Dec 23 11:58 simple_table_db_test\n-rwxr-xr-x    1 crystal  staff   2148472 Dec 23 11:58 skiplist_test\n-rwxr-xr-x    1 crystal  staff   2108508 Dec 23 11:59 sst_dump\n-rwxr-xr-x    1 crystal  staff   2184224 Dec 23 11:58 stringappend_test\ndrwxr-xr-x   53 crystal  staff      1802 Dec 23 11:59 table\n-rwxr-xr-x    1 crystal  staff   2232352 Dec 23 11:59 table_test\ndrwxr-xr-x   18 crystal  staff       612 Dec 23 11:59 tools\n-rwxr-xr-x    1 crystal  staff   2180052 Dec 23 11:58 ttl_test\ndrwxr-xr-x  139 crystal  staff      4726 Dec 23 11:57 util\ndrwxr-xr-x    7 crystal  staff       238 Dec 20 17:40 utilities\n-rwxr-xr-x    1 crystal  staff   2111580 Dec 23 11:58 version_edit_test\n-rwxr-xr-x    1 crystal  staff   2190760 Dec 23 11:58 version_set_test\n-rwxr-xr-x    1 crystal  staff   2156352 Dec 23 11:58 write_batch_test\n```\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31103587/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "forhappy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31107329", "body": "I find that rocksdb's C API is much the same with leveldb's C API, I have written a python leveldb binding based on its C API before(https://github.com/forhappy/cpy-leveldb), so I can make it for rocksdb much easier, I'm on it too ;-).\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31107329/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "rhtyd": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31140955", "body": "Thanks Dhruba :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31140955/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31442318", "body": "Thanks for the fix :+1:  I was about to open a pull request on the same :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31442318/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "mlin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31182361", "body": "Looked in the code, with assertions turned on the vector memtable has O(n<sup>2</sup>) insertion time...guess I should have heeded the warning :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31182361/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36298363", "body": "Sorry for delayed reply- 5e3aeb5f8e addressed the specific comments and I have no further immediate plans. All yours!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36298363/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "vmg": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31261070", "body": "Bumping this. https://github.com/facebook/rocksdb/commit/43c386b72ee834c88a1a22500ce1fc36a8208277 from @jamesgolick breaks the build on Linux when `fallocate` is actually present. The right fix would be as follows:\n\n```\nCOMMON_FLAGS=\"$COMMON_FLAGS -DROCKSDB_FALLOCATE_PRESENT\"\n```\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31261070/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "BastianAl": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31273084", "body": "This is the contents of the build_config.mk file\n\n/rocksdbjni/rocksdb$ cat build_config.mk \nSOURCES=db/builder.cc db/c.cc db/db_filesnapshot.cc db/dbformat.cc db/db_impl.cc db/db_impl_readonly.cc db/db_iter.cc db/db_statistics.cc db/db_stats_logger.cc db/filename.cc db/log_reader.cc db/log_writer.cc db/memtable.cc db/memtablelist.cc db/merge_helper.cc db/merge_operator.cc db/repair.cc db/table_cache.cc db/table_properties_collector.cc db/transaction_log_impl.cc db/version_edit.cc db/version_set.cc db/version_set_reduce_num_levels.cc db/write_batch.cc table/block_based_table_builder.cc table/block_based_table_factory.cc table/block_based_table_reader.cc table/block_builder.cc table/block.cc table/filter_block.cc table/flush_block_policy.cc table/format.cc table/iterator.cc table/merger.cc table/two_level_iterator.cc util/arena_impl.cc util/auto_roll_logger.cc util/blob_store.cc util/bloom.cc util/build_version.cc util/cache.cc util/coding.cc util/comparator.cc util/crc32c.cc util/env.cc util/env_hdfs.cc util/env_posix.cc util/filter_policy.cc util/hash.cc util/hash_skiplist_rep.cc util/histogram.cc utilities/backupable/backupable_db.cc utilities/merge_operators/put.cc utilities/merge_operators/string_append/stringappend2.cc utilities/merge_operators/string_append/stringappend.cc utilities/merge_operators/uint64add.cc utilities/redis/redis_lists.cc utilities/ttl/db_ttl.cc util/ldb_cmd.cc util/ldb_tool.cc util/logging.cc util/murmurhash.cc util/options.cc util/perf_context.cc util/skiplistrep.cc util/slice.cc util/statistics.cc util/status.cc util/string_util.cc util/vectorrep.cc  /home/data/rocksdbjni/rocksdb/port/stack_trace.cc /home/data/rocksdbjni/rocksdb/port/port_posix.cc  \nSOURCESCPP=\nMEMENV_SOURCES=helpers/memenv/memenv.cc\nCC=cc\nCXX=g++\nPLATFORM=OS_LINUX\nPLATFORM_LDFLAGS= -lpthread -lrt -lsnappy -lgflags -lz -lbz2\nVALGRIND_VER=\nPLATFORM_CCFLAGS=  -lpthread -lrt -DROCKSDB_FALLOCATE_PRESENT -DSNAPPY -DGFLAGS -DZLIB -DBZIP2 \nPLATFORM_CXXFLAGS=-std=gnu++11   -lpthread -lrt -DROCKSDB_FALLOCATE_PRESENT -DSNAPPY -DGFLAGS -DZLIB -DBZIP2 \nPLATFORM_SHARED_CFLAGS=-fPIC\nPLATFORM_SHARED_EXT=so\nPLATFORM_SHARED_LDFLAGS= -shared -Wl,-soname -Wl,\nPLATFORM_SHARED_VERSIONED=true\nEXEC_LDFLAGS=\nJEMALLOC_INCLUDE=\nJEMALLOC_LIB=\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31273084/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31276043", "body": "That worked perfectly. Thank you.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31276043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "stephan-hof": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31345628", "body": "Hi @sepeth have you considered other methods for wrapping ? \n\nFor example Cython has great capabilities to wrap c++ code and the generated code works under python2 and python3\n\nOnly drawback, it generates code for cPython only. For a more 'interpreter-independent' way you should also check out CFFI. The 'wrap-performance' on pypy is outstanding (because the JIT knows of the C-functions and types and calls them directly) and on cPython comparable to ctypes.\n\nHowever I don't know the status of CFFI for Jython/IronPython, but if you really target this two interpreters you should also check if they support ctypes properly.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31345628/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ramanala": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31376650", "body": "Hello @sepeth , Any idea on  when the python bindings for RocksDB will be available? \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31376650/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31378838", "body": "@sepeth: Thanks.. Fix mentioned in #48 seems to be working. Will close this. \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31378838/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "markhpc": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31381448", "body": "I just tried to compile master on 64bit 13.04 and hit the same issue.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31381448/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31382015", "body": "fix for this problem is here: https://github.com/facebook/rocksdb/pull/48\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31382015/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "anatol": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31459277", "body": "yep, it fixed (the fix is not released yet).\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31459277/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31459277", "body": "yep, it fixed (the fix is not released yet).\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31459277/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "jamesgolick": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31580720", "body": "The manifest file was only 140MB when it was taking a long time to open. Is that too big?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31580720/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31580720", "body": "The manifest file was only 140MB when it was taking a long time to open. Is that too big?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/31580720/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/32072503", "body": "@igorcanadi I built a way to dump the stats for this db and it looks like the `memtable_compaction` numbers are really high under Stalls(secs).\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/32072503/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/32103461", "body": "I'm seeing stalls all over actually: \n\nStalls(secs): 8428.776 level0_slowdown, 3164.682 level0_numfiles, 2052.582 memtable_compaction, 0.000 leveln_slowdown\nStalls(count): 7710150 level0_slowdown, 64 level0_numfiles, 286 memtable_compaction, 0 leveln_slowdown\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/32103461/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/32128733", "body": "https://gist.github.com/jamesgolick/bed71f531b89d0bfc804\n\nThe workload is all merges. It's an activity feed timeline storage service, so fanning out activity feed event IDs (16 byte) to their friends' timelines.\n\nThe storage system is SSD and highest utilization I've seen on the disk array is 3%, although with a fairly high await of ~3.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/32128733/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "ghost": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/249800210", "body": "Thank you for your pull request.  As you may know, we require contributors to sign our Contributor License Agreement, and we don't seem to have you on file and listed as active anymore.  In order for us to review and merge your code, please email cla@fb.com with your details so we can update your status.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/249800210/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/249816701", "body": "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/249816701/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/256966455", "body": "@dhruba Thanks for reply. Unfortunately it isn't reproducible on demand. I am doing a series of stress tests and this one popped up in one of the tests, but it took many hours to show up.\n\nI do have the coredump available so I can execute commands on it if it could help determine whether this was caused by our application code.\n\nThanks,\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/256966455/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/258579470", "body": "@siying Thanks for the follow up. Quick question. \nIf Snappy is not able to decompress this, could this be caused by memory corruption. In other words, could it be that bugs in our application (like buffer overflow) is corrupting that data? \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/258579470/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/258581177", "body": "@siying we have repeated the stress test that caused this several times and I haven't seen it again. \nWe have fixed several crashes in our app in between (although those crashes were caused by de-referencing bad pointers not memory corruption such as a buffer overflow). The only difference between the above settings and what we currently run is that now we run with a memtable size of 4 Mb. I will run a test with a memtable of 32 M in the next few days to see if I can reproduce it again.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/258581177/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "daaku": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/208988907", "body": "Done! Added some tests.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/208988907/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "prashanthellina": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/157562376", "body": "@agiardullo I can do that. Would it be okay to break the current C-API as a part of this fix?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/157562376/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/157901972", "body": "@agiardullo Alright. Will do this.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/157901972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/157906540", "body": "@igorcanadi I see the CPU usage for `ldb` hovering around 110% (in top. I guess it is >100% due to some parallelization during read operation within RocksDB). I did make sure to have my dataset cached in RAM by running the command multiple times. In summary, it does look bottlenecked on CPU. I compiled RocksDB release 4.0 using \"make all\". I've always assumed that this would provide me with an optimized build.\n\n@siying I am on Linux. I will try the test again with your suggestion. Thanks.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/157906540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/157908548", "body": "@igorcanadi Oh! I will try this right away. The INSTALL.md I got as a part of the 4.0 release did not make a mention of this though.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/157908548/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/157914121", "body": "@igorcanadi I am glad I found out during benchmarking :) Thank you pointing this out :+1: \n\nI tried building using \"make release\" and produced an `ldb` binary and repeated the test but I still see the same slowdown.\n\nAs per @siying's suggestion, I tried increasing block size but that also did not help.\n\nI am curious as to what in RocksDB could be causing this much of slowdown. Also, I think this is a general use-case that others might need too. Will such support be within the scope of RocksDB (or even within the realm of possibility considering the generality you'd want to maintain)?\n\n@igorcanadi As an aside, when I did \"make release\", the shared object was not produced. I used to generate that using \"make shared_lib\". How do I get the shared lib with optimizations turned on?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/157914121/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/157915276", "body": "@igorcanadi Please ignore the question about the shared lib in release mode. INSTALL.md says explicitly that it is built in release mode.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/157915276/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/158114315", "body": "@dhruba Here is the info you asked for. Please let me know if you need access to the actual data files. It will take a while to upload but I can do it if it will help you in debugging.\n\n``` bash\ndu -sh testdb\n2.4G    testdb\n\ndu -s testdb\n2508304 testdb\n```\n\n``` bash\nls -l testdb.lz4\n-rw-r--r-- 1 root root 2406468395 Nov 19 17:36 testdb.lz4\n\nls -lh testdb.lz4\n-rw-r--r-- 1 root root 2.3G Nov 19 17:36 testdb.lz4\n```\n\n``` bash\ncat /proc/meminfo\nMemTotal:       32630460 kB\nMemFree:         6375392 kB\nMemAvailable:   15646088 kB\nBuffers:          147400 kB\nCached:          9216628 kB\n```\n\n``` bash\nls -lR testdb\ntestdb:\ntotal 2516492\n-rw-r--r-- 1 root root         0 Nov 19 17:19 000003.log\n-rw-r--r-- 1 root root 271181850 Nov 19 17:29 000144.sst\n-rw-r--r-- 1 root root 271193467 Nov 19 17:29 000145.sst\n-rw-r--r-- 1 root root 271156030 Nov 19 17:29 000146.sst\n-rw-r--r-- 1 root root 271179933 Nov 19 17:29 000147.sst\n-rw-r--r-- 1 root root 271200640 Nov 19 17:29 000148.sst\n-rw-r--r-- 1 root root 271184466 Nov 19 17:29 000149.sst\n-rw-r--r-- 1 root root 271196925 Nov 19 17:29 000150.sst\n-rw-r--r-- 1 root root 271180814 Nov 19 17:30 000151.sst\n-rw-r--r-- 1 root root 271176053 Nov 19 17:30 000152.sst\n-rw-r--r-- 1 root root 123555718 Nov 19 17:30 000153.sst\n-rw-r--r-- 1 root root        16 Nov 19 17:19 CURRENT\n-rw-r--r-- 1 root root        37 Nov 19 17:19 IDENTITY\n-rw-r--r-- 1 root root         0 Nov 19 17:19 LOCK\n-rw-r--r-- 1 root root     12902 Nov 19 17:32 LOG\n-rw-r--r-- 1 root root    332610 Nov 19 17:30 LOG.old.1447950737275076\n-rw-r--r-- 1 root root     12902 Nov 19 17:32 LOG.old.1447950750313951\n-rw-r--r-- 1 root root     17381 Nov 19 17:30 MANIFEST-000005\n```\n\nThe DB has 10 million records of randomly generated data. Here are a few records. The keys are md5 hashes so I would expect they are distributed evenly. Note that the key is stored in the JSON value under \"id\" field.\n\n``` json\n{\"City\":\"Port Breanaborough\",\"Country\":\"Georgia\",\"Desc\":\"Coaegresco unde aestas vinculum. Solvo adeptio commodi animi vulgus explicabo deficio vorago. Tempus quasi recusandae adeptio quam error volup cruciamentum.\",\"Email\":\"beth@watsica.name\",\"JobTitle\":\"Future Factors Liason\",\"Name\":\"Alan O'Connell\",\"PhoneNumber\":\"067.355.2016\",\"State\":\"Delaware\",\"StreetAddress\":\"6273 Waters Heights\",\"UserName\":\"ryan\",\"id\":\"0000023f507999464aa2b78875b7e5d6\"}\n{\"City\":\"Wisozkmouth\",\"Country\":\"Pakistan\",\"Desc\":\"Summisse ago quia. Cogito placeat accedo eaque beneficium. Corroboro attero suppellex.\",\"Email\":\"marguerite.littel@konopelskihudson.biz\",\"JobTitle\":\"Principal Directives Designer\",\"Name\":\"Ricky Kautzer\",\"PhoneNumber\":\"(328)724-7847\",\"State\":\"Connecticut\",\"StreetAddress\":\"000 Heathcote Ports\",\"UserName\":\"danyka\",\"id\":\"0000071c69e6b287096827a4173c94c1\"}\n{\"City\":\"Mattside\",\"Country\":\"Turkmenistan\",\"Desc\":\"Censura et tutis expedita. Paulatim sit vestigium. Ea accusantium aliquam est et.\",\"Email\":\"eliezer@rennerklein.com\",\"JobTitle\":\"Regional Communications Officer\",\"Name\":\"Marlon Wisozk\",\"PhoneNumber\":\"340-858-6112 x004\",\"State\":\"Kansas\",\"StreetAddress\":\"8150 D'Amore Parks\",\"UserName\":\"isabell\",\"id\":\"000009891526c0ade7180f8423792063\"}\n{\"City\":\"Jeanmouth\",\"Country\":\"Israel\",\"Desc\":\"Repellendus adsidue velut. Qui solus demum ubi ut tremo succurro. Ullam rerum officiis ullam sit vultuosus voluptate neque.\",\"Email\":\"van_beier@daniel.info\",\"JobTitle\":\"National Accounts Specialist\",\"Name\":\"Pasquale Goyette\",\"PhoneNumber\":\"1-141-867-3674 x255\",\"State\":\"Arizona\",\"StreetAddress\":\"718 Predovic Crossroad\",\"UserName\":\"modesta_emard\",\"id\":\"00000ce845c00cbf0686c992fc369df4\"}\n{\"City\":\"North Irwin\",\"Country\":\"Greece\",\"Desc\":\"Aestivus cohibeo sublime vorago aliquam ultio traho. Capio quae curatio excepturi bardus cubo cena consequatur. Acidus peccatus reiciendis et.\",\"Email\":\"ian@pollich.net\",\"JobTitle\":\"Central Markets Liason\",\"Name\":\"Winona Howell\",\"PhoneNumber\":\"1-702-828-5406 x773\",\"State\":\"New Jersey\",\"StreetAddress\":\"336 Murray Prairie\",\"UserName\":\"karolann.hyatt\",\"id\":\"00000f7264c27ba6fea0c837ed6aa0aa\"}\n{\"City\":\"Tracymouth\",\"Country\":\"Somalia\",\"Desc\":\"Laborum talis totidem id auditor. Cinis cimentarius tracto optio laborum. Vorax id terebro nobis exercitationem.\",\"Email\":\"teagan_marquardt@johnson.org\",\"JobTitle\":\"District Marketing Orchestrator\",\"Name\":\"Alvena O'Keefe\",\"PhoneNumber\":\"048.368.8436 x3026\",\"State\":\"Delaware\",\"StreetAddress\":\"82442 Sadye Ramp\",\"UserName\":\"ivah\",\"id\":\"0000104cd168386a335ba6bf6e32219d\"}\n{\"City\":\"West Eliane\",\"Country\":\"Fiji\",\"Desc\":\"Laudantium solum iusto quis ea speculum hic abduco. At timor antepono et nesciunt arceo aperio distinctio. Amitto velit spiritus.\",\"Email\":\"evans@walter.name\",\"JobTitle\":\"Product Communications Orchestrator\",\"Name\":\"Olaf Marvin\",\"PhoneNumber\":\"1-704-417-5835\",\"State\":\"Tennessee\",\"StreetAddress\":\"587 Reid Streets\",\"UserName\":\"hershel_windler\",\"id\":\"000010d95384a6ba3d57dd870e7b337c\"}\n{\"City\":\"Lake Sharonland\",\"Country\":\"Chile\",\"Desc\":\"Magni suasoria sperno charisma verbera facilis. Statua censura aspernatur sono sumptus consequatur. Comparo non canonicus terminatio consectetur et audax charisma.\",\"Email\":\"arnulfo@hyatt.name\",\"JobTitle\":\"District Directives Representative\",\"Name\":\"Ryann Bins PhD\",\"PhoneNumber\":\"472-518-4843 x88780\",\"State\":\"Minnesota\",\"StreetAddress\":\"6732 Yundt Crest\",\"UserName\":\"gina\",\"id\":\"0000174d1d38072889d47e51b587a10c\"}\n{\"City\":\"Janessaberg\",\"Country\":\"Pakistan\",\"Desc\":\"Temporibus rerum quas denego voluptates. Nisi voluptatem vulnero molestiae magnam. Architecto et sono solum adipisci coadunatio ustilo.\",\"Email\":\"mary.grady@hintz.com\",\"JobTitle\":\"International Research Agent\",\"Name\":\"Ms. Jammie Cormier\",\"PhoneNumber\":\"1-041-154-4453 x82328\",\"State\":\"Michigan\",\"StreetAddress\":\"125 Fisher Greens\",\"UserName\":\"carlo\",\"id\":\"0000180e94707c0d90547614c17076bf\"}\n{\"City\":\"East Don\",\"Country\":\"Kenya\",\"Desc\":\"Bellum certus laboriosam ut. Enim decet repudiandae iusto eum cornu. Distinctio degusto ager spectaculum temporibus sequi vis creber.\",\"Email\":\"micaela@russel.org\",\"JobTitle\":\"Chief Infrastructure Agent\",\"Name\":\"Aleen Bergstrom\",\"PhoneNumber\":\"578-548-5556 x5762\",\"State\":\"Pennsylvania\",\"StreetAddress\":\"44143 Bill Harbors\",\"UserName\":\"kristina\",\"id\":\"00001816d766bb450f138ce3721f8f78\"}\n```\n\nI've captured the first 500 LOG lines in this gist - https://gist.github.com/prashanthellina/a46d43f976c899bb30e2\n\nI ran the timings again\n\n``` bash\ntime cat testdb.lz4 | lz4c -d > /dev/null\nreal    0m3.546s\nuser    0m2.772s\nsys     0m1.008s\n\ntime ldb --db=testdb scan > /dev/null\nreal    0m9.148s\nuser    0m8.628s\nsys     0m0.512s\n```\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/158114315/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/158138594", "body": "@dhruba I ran the test again while monitoring for writes using iostat. I did see some writes very briefly (I suspect to LOG given that it was only a few kB/s). I have uploaded the files to a location you can access. I'd like to share this URL with you privately instead of pasting here - Can you please tell me how I can reach you?\n\nI will try modifying `ldb` command source with these settings and report the numbers.\n\n@igorcanadi Is there a way I separate the time taken for warmup and the actual scan? I am using the `ldb` tool.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/158138594/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/158146644", "body": "@igorcanadi Will attempt a test that will ignore setup time of db.\n\n@dhruba I tried my previous test with the settings you proposed (except 256MB block size) and the timing went up to 15s.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/158146644/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/158148640", "body": "@dhruba I compacted db to make block size 256MB and got scan timings. Strangely the time has ballooned up to 62s!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/158148640/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/158152784", "body": "@dhruba Options.allow_mmap_reads = 1, Options.table_cache_numshardbits = 6, Options.no_block_cache: 0, block_size=8MB produced a marginal improvement (around 0.3s). I am going to now try isolating db setup time for actual scan time.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/158152784/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/158160366", "body": "@igorcanadi @dhruba I collected timings by excluding db setup from the measurement (timer starts just before iterator creation and ends on iterator closure). It appears very negligible amount of time was being spent in db setup (<0.2s) in this case (I guess because db is already compacted?). The iteration time still remains approximately the same as previous measurements. One thing that I noticed is that when I commented out fprintf (for outputting kv data to stdout), the total time dropped to 5 seconds from 9 seconds (almost a 50% drop). However, the baseline test (using lz4c decompression) does include time taken by that tool to write to stdout so not sure this matters at all. In summary, there is still around a 2.0 to 2.5x difference in iteration speed between lz4c baseline and RocksDB.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/158160366/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/158173832", "body": "@IslamAbdelRahman I sent an email with access info for the data files. I am compiling your patch. Will report timings once done.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/158173832/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/158175720", "body": "@IslamAbdelRahman Your patch helped! The timing is now down to 5.3s. Now RocksDB (ldb) is only 50% slower than the baseline case. \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/158175720/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/158195088", "body": "@dhruba That is a definite yes. While in an ideal world, the difference in perf can be much lesser, the decision to use RocksDB for my use-case is easily made as I can avoid writing code to manage data on the disk myself.\n\nThank you everyone in helping to get to the bottom of this.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/158195088/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ivan": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/48014541", "body": "It looks like the JNI header it finds, `/usr/lib/gcc/x86_64-linux-gnu/4.8/include/jni.h` is a GNU Classpath header that doesn't have enough `const jbyte`.  Doing any one of these fixed the build errors for me:\n\n1) Adding `-I/usr/lib/jvm/java-8-oracle/include` to `OPT` in `Makefile`\n\n2) Adding `-I/usr/lib/jvm/java-7-openjdk-amd64/include` to `OPT` in `Makefile`\n\n3) Changing all of the `reinterpret_cast<const jbyte*>` to `(jbyte*)`\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/48014541/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/48017640", "body": "Or, now that I read https://github.com/facebook/rocksdb/wiki/RocksJava-Basics, setting `JAVA_HOME` works too.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/48017640/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/49359542", "body": "Thanks, I confirm latest master does not crash\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/49359542/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/49361700", "body": "Yes, latest master, 296e340753f23f213655ff1d4549c73fa0262038\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/49361700/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/49387656", "body": "Thank you for the fixes, it builds fine with clang now.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/49387656/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "kjk": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/41754556", "body": "I tracked down the change: https://github.com/facebook/rocksdb/commit/ac2fe728327be75c8c289d4e3ebf8587d88c518d\n\nSeems was inadvertent.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/41754556/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/41754597", "body": "Maybe a comment in the makefile would help prevent such changes in the future.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/41754597/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "bpot": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/43524131", "body": "@igorcanadi that seems like a good solution to me, conceptually simpler at least.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/43524131/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/43684614", "body": "@yhchiang cool, I will test that and see if it fixes the issue with my test script this evening\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/43684614/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/43698167", "body": "I've attached a change set that fixes the issue for me.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/43698167/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/43830893", "body": "@ankgup87 added Javadoc and squashed.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/43830893/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/44031447", "body": "@yhchiang I may be missing something but it looks like the RocksDB instance that BackupableDB.open creates is always internal. It doesn't seem like something someone using BackupableDB needs to know.\n\nAlso, do we still need to keep the reference (`db_`) to the RocksDB object? It doesn't really do anything for us once we zero out the native handle.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/44031447/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/44319063", "body": "@yhchiang Great, I just went ahead and made the change to remove the `db_` field.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/44319063/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "zpao": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/266555492", "body": "CLA is ok (commenting in case the bot doesn't)", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/266555492/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/316768011", "body": "Do you want to remove `arcanist_util/` as well?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/316768011/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "commit_comments": [], "review_comments": []}, "dbrock": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/239207106", "body": "Status? I'm getting this error right now.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/239207106/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/239277689", "body": "Oops, sorry! :sob: Works!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/239277689/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "benoitc": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/176079661", "body": "checking for `armv7l`  on raspberry 2 does the tricks (and the tests from our CI pass) . What would be the proper patch to add its support? Btw why enabling this function? Why not simply using `-02` and no `-fno-omit-frame-pointer` or `-momit-leaf-frame-pointer` ? \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/176079661/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/173383710", "body": "@igorcanadi yes the issue here is that some os shipped GCC with custom templates that allow the overloading. The comment here is more descriptive:\n\nhttps://github.com/leo-project/erocksdb/issues/12#issuecomment-173125008\n\na reasonnable solution would be patching the rocksdb utility to be more strict with its c++ usage imo. Thoughts?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/173383710/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/325117605", "body": "i can see some instructions to build onios but none on android . Is this possible to build rocksdb lite for android ? Any example on how to do it if possible? ", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/325117605/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/338371399", "body": "bump", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/338371399/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/17027205", "body": "why there is no tag about it? \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17027205/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "alberts": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34357846", "body": "@igorcanadi no reads at this point. just testing writes.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34357846/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34370551", "body": "Thanks, I'll do some tests. Could you explain the relationship between max_bytes_for_level_base and target_file_size_base? I'm guessing level base is the sum of all files (each approximately of target size)?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34370551/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34389080", "body": "Thanks. Here's another log:\n\nhttps://drive.google.com/file/d/0B0b-iwt-kSG2SE40NGxZZUpLRlk/edit?usp=sharing\n\nI still hit a 10 minute stall at around 23:19.\n\nLogs from around that time included. Looking forward to more debug logging.\n\n2014/02/06-22:56:42.837503 7f14dcff9700 compacted to: files[28 474 870 0 0 0 ], 99.1 MB/sec, level 2, files in(1, 2) out(4) MB in(64.1, 128.2) out(192.2), read-write-amplify(6.0) write-amplify(3.0) OK\n2014/02/06-22:56:43.252372 7f14f57fa700 compacted to: files[28 472 873 0 0 0 ], 121.2 MB/sec, level 2, files in(2, 2) out(5) MB in(128.2, 128.1) out(256.2), read-write-amplify(4.0) write-amplify(2.0) OK\n2014/02/06-22:56:43.256281 7f14b0996700 compacted to: files[28 469 876 0 0 0 ], 78.8 MB/sec, level 2, files in(3, 2) out(5) MB in(179.8, 128.1) out(307.9), read-write-amplify(3.4) write-amplify(1.7) OK\n2014/02/06-23:09:52.407980 7f14f57fa700 compacted to: files[36 1353 876 0 0 0 ], 313.8 MB/sec, level 1, files in(28, 469) out(1353) MB in(55573.6, 28631.1) out(84127.6), read-write-amplify(3.0) write-amplify(1.5) OK\n2014/02/06-23:28:34.589486 7f14bccca700 compacted to: files[28 2466 876 0 0 0 ], 327.8 MB/sec, level 1, files in(36, 1353) out(2466) MB in(71452.9, 84127.6) out(155472.7), read-write-amplify(4.4) write-amplify(2.2) OK\n2014/02/06-23:28:48.287452 7f14dd7fa700 compacted to: files[28 2465 877 0 0 0 ], 74.6 MB/sec, level 2, files in(1, 1) out(2) MB in(64.1, 64.1) out(128.2), read-write-amplify(4.0) write-amplify(2.0) OK\n2014/02/06-23:28:48.289521 7f14f4ff9700 compacted to: files[28 2462 880 0 0 0 ], 74.1 MB/sec, level 2, files in(1, 1) out(2) MB in(64.1, 62.0) out(126.1), read-write-amplify(3.9) write-amplify(2.0) OK\n2014/02/06-23:28:48.289651 7f14f57fa700 compacted to: files[28 2462 880 0 0 0 ], 73.2 MB/sec, level 2, files in(1, 1) out(2) MB in(64.1, 64.1) out(128.2), read-write-amplify(4.0) write-amplify(2.0) OK\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34389080/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34390946", "body": "@mdcallag Stack trace here:\nhttps://gist.github.com/alberts/aaa52cdec468e99ea1c6\nReasonably straight-forward. Calling this through the Go wrapper, so 128 threads simulating incoming connections with available data blocks inside rocksdb::DBImpl::Write.\nOne thread in DoCompactionWork. Miscellaneous others.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34390946/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34401965", "body": "Hello. Testing with 1d08140e81 + a few patches to the C API. Log here:\n\nhttps://drive.google.com/file/d/0B0b-iwt-kSG2SHRVUVdORHU0WVE/edit?usp=sharing\n\nStall starts around 2014/02/07 02:47:00.244781. Interestingly, no statistics are printed during that entire time:\n\n2014/02/07-02:40:45.697182 7f6cdeffd700 STATISTCS:\n2014/02/07-02:41:50.897204 7f6ce4ff9700 STATISTCS:\n2014/02/07-02:42:55.387432 7f6cddffb700 STATISTCS:\n2014/02/07-02:43:58.933508 7f6ce57fa700 STATISTCS:\n2014/02/07-02:45:02.117604 7f6cdeffd700 STATISTCS:\n2014/02/07-02:46:03.902854 7f6cdffff700 STATISTCS:\n2014/02/07-02:57:25.729889 7f6ce7fff700 STATISTCS:\n2014/02/07-02:58:31.813396 7f6ce5ffb700 STATISTCS:\n\nIt's like the compaction never pauses and just keep going with something.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34401965/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34407760", "body": "I'm going to decrease the stats dump period a bit. It gives an interesting view on the matter. Looking at the stats in the logs so far, it seems the DB goes above my limit slowdown writer trigger of 32, and then increases until it hits the stop writes trigger, and then goes back down to 32, but never below that.\n\nWould it make sense to try universal compaction here?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34407760/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34409388", "body": "FWIW, when it's stalling, about 80% of a core is being used, iostat looks like this:\n\n```\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nfioa              0.00     0.00 1535.00  201.00 196468.00 196914.20   453.21     0.00    3.82    0.24   31.16   0.00   0.00\n```\n\nand the top functions for the process according to perf top are rocksdb::crc32c::Extend which calls rocksdb::crc32c::Fast_CRC32 and memcpy stuff. So a faster hash function will help a bit. xxhash looks promising, but I need to test more carefully.\n\nI think I agree with the statement that I'm basically writing too fast, but it seems the slowdown at 32 level0s has almost no effect until I hit the stop limit of 64 and stall hard.\n\nSo I think for me it's more a CPU thing than a disk thing. You said \"There can be at most one concurrent compaction at level0\", so I guess the only way to go faster here is to figure out how to parallelize this work?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34409388/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34413275", "body": "I call SetBackgroundThreads(16) and SetBackgroundThreads(16, Env::HIGH).\nLOG says: Options.max_background_compactions: 16\nI haven't set max_background_flushes, but it seems that only becomes important when you share an env between DBs. Haven't done that yet.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34413275/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34483509", "body": "Found the problem:\n\nhttps://code.google.com/p/lz4/issues/detail?id=114\n\nSome numbers when compiling liblz4.so with -O3:\n\nCompression: snappy\ncompress: 4.865 micros/op 205553 ops/sec;  802.9 MB/s (output: 55.1%)\nuncompress: 0.861 micros/op 1162073 ops/sec; 4539.3 MB/s\n\nCompression: lz4\ncompress: 5.952 micros/op 168009 ops/sec;  656.3 MB/s (output: 55.4%)\nuncompress: 0.677 micros/op 1476476 ops/sec; 5767.5 MB/s\n\nCompression: lz4hc\ncompress: 66.506 micros/op 15036 ops/sec;   58.7 MB/s (output: 55.4%)\nuncompress: 0.648 micros/op 1543885 ops/sec; 6030.8 MB/s\n\nTypical inputs should compress smaller with lz4hc than with lz4.\n\nThis pull request is ready to merge unless you have more comments.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34483509/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34490669", "body": "Hi. Makes sense. I haven't experimented with bigger target_file_size_base yet. What's the reasoning behind why it might help?\n\n2677091118 records in 3h1m21.811418742s: 246015 records/sec 135 MiB/sec... wrote 1431 GiB to the FIO drive before compaction hit first ENOSPC\n\nThis is the total throughput measured on the put side including all the stall time. So my understanding is that if I write to this database at a bit less than 135 MiB/sec, I should be able to avoid the stalls.\n\nI am repeating the test with Snappy compression enabled now, and then I'll try universal compaction.\n\nFeature request: it would be _very_ useful to have an API to Write a batch with a timeout, so that if my RocksDB is stalling under me, I can at least easily signal an error and/or arrange to send the data I'm trying to write somewhere else (another DB, or /dev/null).\n\nIt seems that any way to optimize or parallelize the compaction that is stalling might help. I don't think I'm I/O bound on this process (I'll make some flame graphs soon). Low hanging fruit: don't checksum so much, or checksum faster. Anything else?\n\nI am basically building a big circular buffer to retain some high volume time series data in order for a few hours, with a few additional indexes (please finish column families! :)).\n\n@mdcallag thanks for the advice. I've been reaching the same conclusions.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34490669/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34491220", "body": "@dhruba Yep, I'm planning to shard and test universal compaction. I was just trying to get a good feeling for what a single shard can handle before I do further development.\nI think the flash storage might be able to keep up most of the time, but I'll gather some more information to confirm.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34491220/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34504876", "body": "Think I addressed most of the issues.\n\narc lint found 2, but it also seems my setup isn't quite right yet:\n\n```\nwhich: no checkCpp in (...)\nwhich: no cpplint in (...)\n[2014-02-07 21:16:56] ERROR 8: Undefined index: db/db_bench.cc at [/home/albert/rocksdb/linters/cpp_linter/FbcodeCppLinter.php:71]\n  #0 FbcodeCppLinter::getCppLintOutput(db/db_bench.cc) called at [/home/albert/rocksdb/linters/cpp_linter/FbcodeCppLinter.php:50]\n  #1 FbcodeCppLinter::lintPath(db/db_bench.cc) called at [/home/albert/arcanist/src/lint/engine/ArcanistLintEngine.php:297]\n  #2 ArcanistLintEngine::run() called at [/home/albert/arcanist/src/workflow/ArcanistLintWorkflow.php:359]\n  #3 ArcanistLintWorkflow::run() called at [/home/albert/arcanist/scripts/arcanist.php:321]\n```\n\nCould you add a page on the wiki on how contributors should set up arcanist and cpplint?\n\nFinally, it's probably time to make format master again. It helps to run make format a few times in a row to see the issues with it not being idempotent.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34504876/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34525688", "body": "I've rolled up the work into a single commit now.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34525688/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34560329", "body": "FWIW, here's the log from a run with universal compaction:\n\nhttps://drive.google.com/file/d/0B0b-iwt-kSG2aklKeWhkak0wWUU/edit?usp=sharing\n\nOver 3 hours, the throughput measured from at the put side is about 20% less than with level-style compaction. Universal compaction still stalled in this test, which I don't quite understand. As I see it, with a workload that has keys arriving mostly in order (and probably completely ordered by the time the memtables go to L0), universal style compaction should do pretty well. I will investigate more.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34560329/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34562300", "body": "Here's a flame graph of the level style compaction in action: http://alberts.github.io/fg.html\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34562300/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34566303", "body": "With compression turned off for levels 0 and 1 with level style compaction, RocksDB is still CPU bound when compacting L0 and L1 and the CRC32 checksum on reads make up a significant part of the work that it has to do. Can abd70ec be tweaked to make this optional again?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34566303/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34569125", "body": "Maybe it's just supposed to be GetLiveFilesMetaData?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34569125/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35103576", "body": "LGTM\n\nI will probably switch my Go code to use gorocksdb instead of my gorocks package as soon as this is merged.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35103576/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35841801", "body": "More complete code is in pull request #87.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35841801/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/6070753", "body": "This breaks the build if you try to compile tests with -DNDEBUG... maybe TEST(DBTest, TransactionLogIteratorRace) should be excluded by an ifdef in this case.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/6070753/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9554876", "body": "Probably not. I'll get rid of the LZ4HC one.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9554876/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9554904", "body": "It would help me if you could make format the master branch before I retry this. make format doesn't seem to be idempotent.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9554904/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9554921", "body": "make format again...\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9554921/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9554937", "body": "Yep. Slower but better compression, but the same fast decompression.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9554937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9554954", "body": "Just following the style I found. Will fix.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/9554954/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "henglinli": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34534849", "body": "I need exception disabled rocksdb\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34534849/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "lisyarus": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34747163", "body": "Yes, 2nd case seems to be fixed in master.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/34747163/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "latermoon": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35152020", "body": "when I run make librocksdb.so, I get \"make: **\\* No rule to make target `librocksdb.so'.  Stop.\"\nand I can't continue\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35152020/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "tecbot": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35561337", "body": "Hi Igor,\n- I fixed the size_t and the four space indent.\n- I removed `free(filter)` because you didn't know whether the client has allocated the memory in the C space. The client should take care of it this is also the problem why the valgrind test is failing but I didn't know how I can fix it or do you think we should force the client to allocate the return value in the C space?\n\nFor the test I have currently no time. I could do it maybe on Friday.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35561337/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35836428", "body": "@igorcanadi \n\nI decided to implement your alternative solution also I added a test case for the merge operator.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35836428/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35954321", "body": "Oh no, I didn't see the small message at the end of the test. Sry for that :/\nand the rename is of course correct, argh...\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35954321/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "slachowsky": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35949049", "body": "That sounds like a reasonable idea, but having to support multiple trailer lengths makes maintaining support for older files more onerous.  This is the block trailer, not the table footer, so I think it will appear at the end of every block, potentially many times in a file.  Are you certain you want to increase the size of the block trailer?  I don't see what more of value could be put into the  block trailer beyond how to validate and decompress the data therein, anything else should probably go into some sort of index or meta-table.  Thoughts?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35949049/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35954891", "body": "I agree, there is little benefit in being able to switch the checksum-type on a per block basis, it just happened to be the least-invasive change.  Updating the footer to contain the checksum type sounds reasonable, where version defines the size/semantics of the footer/trailer.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/35954891/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36187014", "body": "The Options exists, I've reverted it to crc32c by default.  Regarding expanding the Footer to include a version, a new magic number and the checksum type, that is simple enough... but will ripple through several layers and require some method of passing the checksum type back down to ReadBlockContents().  Do we want to be able to transparently read older files, but write only newer ones?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36187014/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "cnstar9988": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36087190", "body": "2.4GHZ  4 CPU(24 cores),  512G RAM.\nI tested on another SSD with write and read. rocksdb is too slow compare to leveldb.\nI use nmon and pstack,  nmon indicates that there is too many cpu +sys, pstack indiate on pread64.\n\nI think default  options->allow_mmap_reads = false is the problem.\nI change options->allow_mmap_reads = true, run well.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36087190/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36093361", "body": "My app, more than 10 threads.\neach threads: put key-value into db is key not exist; Get old value if key is exist and then overwrite with newValue.\n\nmaybe 10x slower, (key=20bytes, value=30bytes, reccount=1000 0000).\n\n> > > I assume your test database was not cached in the RocksDB/LevelDB block cache but did fit in the OS filesystem cache.\n\nI use default options.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/36093361/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "brooksbp": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37104217", "body": "Machine info:\n\n4 core processor, 8 GB memory\n1 TB WD Blue HDD\nLinux wave 3.13.0-8-generic #28-Ubuntu SMP Tue Feb 11 17:55:27 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux\ngcc (Ubuntu 4.8.2-16ubuntu6) 4.8.2\nlibc6 2.19-0ubuntu2\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37104217/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37141286", "body": "Took about 11 runs to hit it again:\n\n```\n==== Test DBTest.UniversalCompactionCompressRatio1\n==== Test DBTest.UniversalCompactionCompressRatio2\nsz != write_size: Bad file descriptor\ndb_test: ./util/posix_logger.h:143: virtual void rocksdb::PosixLogger::Logv(const char*, __va_list_tag*): Assertion `sz == write_size' failed.\n```\n\nI have roughly 800 GB free of the 1 TB disk, so I wouldn't think that running out of disk space would be the issue.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37141286/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ljinfb": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37221351", "body": "https://reviews.facebook.net/D16725 hopefully fixes this issue\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37221351/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/6725353", "body": "Agree with you. However, this implementation is deprecated in the coming release. We will replace it with ForwardIterator.cc\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/6725353/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/7736220", "body": "I will fix that\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/7736220/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/7736449", "body": "fix: https://reviews.facebook.net/D23193\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/7736449/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16911941", "body": "@tdfischer we will need to resume this counter after WAL write, right? That is the \"post\" part. Did I miss that in your patch?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16911941/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "zhenchuan": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37671883", "body": "thanks @igorcanadi ,and another question here,\n\ng++ -g -Wall -Werror -I. -I./include -std=c++11  -DROCKSDB_PLATFORM_POSIX  -DOS_MACOSX -DROCKSDB_ATOMIC_PRESENT -DSNAPPY -DZLIB -DBZIP2   -DHAVE_JEMALLOC -O2 -fno-omit-frame-pointer -momit-leaf-frame-pointer -Woverloaded-virtual -c util/crc32c.cc -o util/crc32c.o \nutil/crc32c.cc:294:24: error: unused function 'LE_LOAD64'\n      [-Werror,-Wunused-function]\nstatic inline uint64_t LE_LOAD64(const uint8_t _p) {\n                       ^\n1 error generated.\nmake: *_\\* [util/crc32c.o] Error 1\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37671883/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37673075", "body": "@igorcanadi thanks for you quick reply.\nbut error difference and  continues :\n\ng++ -g -Wall -Werror -I. -I./include -std=c++11  -DROCKSDB_PLATFORM_POSIX  -DOS_MACOSX -DROCKSDB_ATOMIC_PRESENT -DSNAPPY -DZLIB -DBZIP2   -DHAVE_JEMALLOC -O2 -fno-omit-frame-pointer -momit-leaf-frame-pointer -Woverloaded-virtual -c util/crc32c.cc -o util/crc32c.o \nutil/crc32c.cc:294:2: error: unterminated conditional directive\n#ifdef **SSE4_2**\n ^\n1 error generated.\nmake: **\\* [util/crc32c.o] Error 1\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37673075/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37674237", "body": "everything looks fine now , many thanks...\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/issues/comments/37674237/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "maoy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/4580949", "body": "Did you mean GetLiveTableMetaData?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4580949/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4589365", "body": "I'm not sure we are ready to deprecate GetLiveFiles() yet. Looking at GetLiveTableMetaData(), it misses the following:\n- CURRENT and MANIFEST file names (only sst files)\n- manifest file size\n- option to flush.\n\nI don't fully understand the implications but at least the first two items seem pretty important.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4589365/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "gubatron": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/4609110", "body": "takes longer to write the commit message than the actual fix, what a nightmare working like that.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/4609110/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "Chilledheart": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/6396956", "body": "clang won't compile with this.\nIn fact replacing `static_cast` with `reinterpret_cast` works.\n\nsee more #155\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/6396956/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "bgrainger": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/6725121", "body": "Using `>=` with a `bool` field is unusual; I think the more explicit `>= (is_prev_inclusive_ ? 0 : 1)` would be clearer (assuming that's the intended logic).\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/6725121/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "ermanpattuk": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/7155588", "body": "Nope. It didn't work. The compiler still complains about next_id not being initialized.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/7155588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "naveenatceg": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/7156942", "body": "28b367d seems to solve the issue for me.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/7156942/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "spencerkimball": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/7308111", "body": "Oops. This is the correct fix. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/7308111/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/7311255", "body": "That works too.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/7311255/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "wankai": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/7634377", "body": "because it has been asserted at table building?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/7634377/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "tea-dragon": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/7736072", "body": "this broke the java jni code. it was using this constructor.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/7736072/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/7737636", "body": "nice turnaround time. thanks.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/7737636/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "fyrz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/9070142", "body": "@adamretter i can take care of it after christmas :/\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/9070142/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/18122555", "body": "I adjusted the name from leveldb.ReverseBytewiseComparator to rocksdb.ReverseBytewiseComparator. \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/18122555/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/19436557", "body": "@igorcanadi  solved\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/19436557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "Downchuck": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/12325944", "body": "Typo: Sllow [sic] instead of Allow.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/12325944/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/13450172", "body": "By virtue of warnings:\n\n```\nprivate field 'hugetlb_size_' is not used\n```\n\nBecause:\n\n```\n-Wunused-private-field\n```\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/13450172/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "ruleant": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/13289069", "body": "Hi there,\n\nThanks for trying out Buildtime Trend!\nI noticed you enabled the Buildtime Trend webhook in Travis CI to analyse the build process data and timings of your project.\n\nIf you have questions or feature requests, don't hesitate to ask. We have a mailing list and a twitter account where you can reach us : https://buildtimetrend.github.io/#contact\n\nI'm also interested to hear why you started using Buildtime Trend, what your expectations are, or how you found out about Buildtime Trend.\n\nI hope you find using Buildtime Trend useful and that it can help you gain some insights about your build process!\n\nKind regards,\n\nDieter\nBuildtime Trend founder and developer\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/13289069/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/13309831", "body": "Hi,\n\nI'm glad you like it. Where did you find the example Travis file? I'm\ncurious to know who is using Buildtime Trend. :)\n\nKind regards,\n\nDieter\n\n2015-09-17 18:29 GMT+02:00 Igor Canadi notifications@github.com:\n\n> Tnx for building buildtime Dieter! I found buildtime on an example travis\n> file so decided to try it out. It gives us nice stats :)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/facebook/rocksdb/commit/2b676d5bbefc974f747e2d7eb62b757a760fa535#commitcomment-13292309\n> .\n## \n\nKind regards,\n\nDieter Adriaenssens\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/13309831/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "jsteemann": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/13462173", "body": "Yes, I saw that in the TravisCI build afterwards. I had used g++ 4.9, which didn't show a warning for this. Otherwise I wouldn't have pushed. Which compiler and settings should be considered the reference when developing rocksdb? Obviously clang shows that warning, but others may not. I usually compile with g++ and `-wdeprecated-declarations`. Then there are tons of warnings, so I already have downgraded my compiler options.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/13462173/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/13486648", "body": "I think it was already fixed by this commit yesterday:\n\ncommit 0fdb4f1688ebe160944c30c213f5afad4b13fb0f\nAuthor: Yueh-Hsuan Chiang yhchiang@fb.com\nDate:   Mon Sep 28 11:40:29 2015 -0700\n\n```\nFixed a compile warning in util/arena.cc when hugetlb is not supported.\n\nSummary:\nFixed the following compile warning when hugetlb is not supported.\n\n    ./util/arena.h:102:10: error: private field 'hugetlb_size_' is not used [-Werror,-Wunused-private-field]\n      size_t hugetlb_size_ = 0;\n             ^\n    1 error generated.\n\nTest Plan: make db_stress\n\nReviewers: igor, sdong, anthony, IslamAbdelRahman\n\nSubscribers: dhruba, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D47691\n```\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/13486648/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "SherlockNoMad": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/14102972", "body": "Thank you.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/14102972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/14197827", "body": "Hi @satnam6502, appveyor CI build start to failed after this change: \nhttps://ci.appveyor.com/project/Facebook/rocksdb/build/1.0.75 \nI believe it's related to your \"Add RocksDb/GeoDb Iterator interface\" commit. \nCould you please take a look at this? Thank you :)\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/14197827/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "satnam6502": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/14170214", "body": "Ugh, I messed up trying to do my arc land manually with git -- I think this need to be reverted?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/14170214/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "aloukissas": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/14464240", "body": "unfortunately, the use of this new API breaks Win7, which has a rather big installation base :/\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/14464240/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "gunnarku": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/15003539", "body": "@yuslepukhin This instance of <tt>ROCKSDB_PRIszt</tt> usage has \"<tt>%</tt>\" specified. Please let me know if I'm somehow misreading your comment.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/15003539/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/15023514", "body": "@yuslepukhin No problem :-) Thank you for reading the incoming changes and helping to find bugs!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/15023514/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "kradhakrishnan": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/17494175", "body": "Can you please point me to build error ? It will be easier to fix.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17494175/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17510190", "body": "Thank you for taking care of the build break!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17510190/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17635675", "body": "I am looking into a fix\n\nFrom: Dmitri Smirnov notifications@github.com\nReply-To: facebook/rocksdb reply@reply.github.com\nDate: Thursday, May 26, 2016 at 11:25 AM\nTo: facebook/rocksdb rocksdb@noreply.github.com\nCc: Karthikeyan Radhakrishnan krad@fb.com, Author author@noreply.github.com\nSubject: Re: [facebook/rocksdb] Direct IO capability for RocksDB (f89caa1)\n\nWould a util/aligned_buffer.h be a good alternative so it continues to build on all platforms?\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHubhttps://github.com/facebook/rocksdb/commit/f89caa127baa086cb100976b14da1a531cf0e823#commitcomment-17635039\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17635675/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "msb-at-yahoo": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/17511345", "body": "any chance the build system will unify on cmake? in my opinion, cmake is a bit clunky, but so is the gmake based system used for the unix builds: one clunky build system is better than two.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17511345/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "IvRRimum": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/17933288", "body": "This breaks things pyrocksdb, why did you remove it ?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17933288/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17948131", "body": "@siying @dhruba I ended up using older version of rocksdb - 3a2bccc8451a8b49df9c869144d27bb4a4ce713a\n\nHope this helps someone.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/17948131/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "jalexanderqed": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/18329033", "body": "Fixed in https://github.com/facebook/rocksdb/pull/1224\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/18329033/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "sgalles": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/18640939", "body": "@adamretter this broke the windows build (same problem than https://github.com/facebook/rocksdb/issues/1220).\n`std::unique_ptr` should be used right ?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/18640939/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/18640952", "body": "@adamretter this broke the windows build (same problem than https://github.com/facebook/rocksdb/issues/1220).\n`std::unique_ptr` should be used right ?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/18640952/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/18678565", "body": "Thank you @adamretter !\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/18678565/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "aorenste": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/19364459", "body": "Weird - I don't see it either but I'm not sure how it would compile if it weren't defined.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19364459/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "JoelMarcey": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/19823690", "body": "This is what is was before this \"fix\". The baseurl is just changing on the fly on us spontaneously. \r\n\r\n![screenshot 2016-11-14 20 10 03](https://cloud.githubusercontent.com/assets/3757713/20292964/d0e9ffe8-aaa6-11e6-9d30-88904595e833.png)\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/19823690/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "vaintroub": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/20598512", "body": "\"lib64\" that should have been made configurable. not everything has lib64. Not everything that is not WIN32 is Linux", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/20598512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/20603006", "body": "I just notices that there might be a problem with this.  Take a look below is the MySQL platform defaults file ( this grew quite ugly now with a lot of repetitions, \r\nfrom a couple of lines at the beginning). https://github.com/mysql/mysql-server/blob/5.7/cmake/install_layout.cmake . This shows libraries are usually installed into /usr/lib, or /usr/lib64, depending on platform and bitness and package type (i.e libraries usually go in \"lib\" in tar.gz or zip distributions) . My actual preference at this stage would be an option to disable all packaging, as we (MariaDB)  would like to have rocksdb as submodule and at some point just do ADD_SUBDIRECTORY and tell rocksdb via option(s) to build a single static library there that we link to our storage engine. There is a while lot needs to be done to that CMakeLists.txt to accomplish this already, as this cmake forces own compile flags, and unconditionally builds everything,  including tests and benchmarks. Adding unconditional INSTALL() actually is going to make our goal somewhat harder to achieve :\r\n)", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/20603006/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "adsharma": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/20598799", "body": "Is this what you're suggesting?\r\n\r\nhttp://public.kitware.com/pipermail/cmake/2013-July/055374.html\r\n\r\nSounds good to me.\r\n\r\nRe: platform specific parts of cmake\r\n\r\nMy preference is to break this file into common and platform specific ones, so we don't step on each other's toes. Some discussion in:\r\n\r\nhttps://github.com/facebook/rocksdb/pull/1358\r\nhttps://github.com/facebook/rocksdb/pull/1135\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/20598799/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "reidHoruff": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/21568185", "body": "Is this correct? We want to keep memtable disabled in prepare stage (only write prepare section to WAL, this is current behavior). We want to disable memtable on line 272 and only write to WAL, but this  time only the commit-time WriteBatch + commit marker.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21568185/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21569299", "body": "We also want to remove this line so that we are not writing our batch to the WAL twice.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21569299/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21569313", "body": "Only commit-time batch + commit marker. GetWriteBatch()->GetWriteBatch() was written to WAL during prepare phase.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21569313/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21569736", "body": "Actually you are correct. I missed the MarkWalTerminationPoint() call which means this portion of the batch is ignored by the WAL already.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21569736/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21569751", "body": "But I think we still want to move the last batch append to be more similar to our hypothetical setup. We are still operating on the WriteBatch twice. Ideally prepare with do both WAL and MEM insert from the single WriteBatch. Commit just writes a small marker.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21569751/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21608998", "body": "I am trying to model this: Prepare -> (write batch to WAL only), Commit -> (Write commit-time-batch + commit marker to WAL only). Existing behavior requires sending the entire write batch through WriteImpl at both prepare + commit. We want to remove the Amend() at line 269 also to model desired behavior more closely.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21608998/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21609627", "body": "ok sounds good.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/21609627/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "boolean5": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/22206229", "body": "How can I reproduce this? I'm not getting an error when I run c_test.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/22206229/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "al13n321": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/22553475", "body": "I just tried running logdevice with rocksdb from master, and this change breaks it completely. We easily pin enough memtables to go over the limit in pinned memtables alone; so rocksdb starts doing a flush after every write (1).\r\n\r\nCan you please make these two options separate? E.g., keep db_write_buffer_size as the limit for mutable_memtable_memory_usage(), and add db_write_buffer_max_memory as the limit for memory_usage(). Then we can at least set the latter to infinity and get the old behavior (except for block-based memory accounting, but that should be fine).\r\n\r\nWhen the immutable memtables take a big fraction of `buffer_size()`, increasing the rate of flushes won't help much; it would make more sense to stall writes in this case. Something like `memory_usage() + std::min(buffer_size() / 2, mutable_memtable_memory_usage() - memory_usage()) >= buffer_size()` would prevent the rate of flushes from growing too big.\r\n\r\n(1) Which makes situation worse because each of these tiny (~1KB) flushed memtables now counts as 4 MB (arena block size that we use); in this state it's enough to pin ~20 KB's worth of memtables to take the pinned memory usage over the limit of 650 MB, so we never recover from this.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/22553475/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/22553539", "body": "Just curious: why is this class in `include/rocksdb/`? Is DB's WriteBufferManager exposed to the user? If not, would it be possible to expose it to the user? That would be useful for logdevice: we're planning to disable automatic flushes in rocksdb and have our own flush policy; we could use DB's WriteBufferManager to check if a flush is needed, instead of rolling our own memtable size accounting through custom MemTableRep.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/22553539/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/22661185", "body": "Oh, I see, there's write_buffer_manager in Options.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/22661185/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "hyc": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/23133699", "body": "When did you get permission from Google/The LevelDB Authors to remove their copyright and license terms?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/23133699/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "absolute8511": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/25331432", "body": "Maybe missing else?", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/25331432/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "Kindred23": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/comments/25915832", "body": "include \"db/pre_release_callback.h\"\r\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/25915832/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/comments/25915842", "body": "\t\t   // Dummy keys to avoid compaction trivially move files and get around actual\r\n   // compaction logic.", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/comments/25915842/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "caiosba": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/10632283", "body": "Oh sorry, I ran `make format` before and it pointed nothing:\n\n$ make format\nbuild_tools/format-diff.sh\nNothing needs to be reformatted!\n\nBTW, I fixed it on hand and pushed.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/10632283/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "dallasmarlow": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/12509042", "body": "@siying looks like you're right, updating the pr now.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/12509042/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "rdallman": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/14668976", "body": "Simply trying to follow convention from `rocksdb_universal_compaction_options_t` since they're similar ideas. Will make change but think `rocksdb_universal_compaction_options_t` should be updated accordingly for uniformity's sake (and fewer pointers). \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/14668976/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/31648250", "body": "gah, sorry. i'll make a vector. tests ran \u00af_(\u30c4)_/\u00af\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/31648250/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/31648352", "body": "can do\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/31648352/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/31685957", "body": "done: https://github.com/facebook/rocksdb/pull/622/files#diff-ade08658a6ad910c75f0508bcbcd2671R814\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/31685957/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "vinniefalco": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16556509", "body": "This actually makes more sense if you look at the usage pattern. The BlockContents has the unique_ptr, along with the size field that are always used with the pointer.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16556509/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16556552", "body": "`heap_allocated` should not be part of the struct, else what's the point?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16556552/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16562820", "body": "You're right. I think there was a mixup in the branch\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16562820/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "tdfischer": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16867780", "body": "Correct. This is here because figuring out where to 'properly' set it takes a lot more debugging time, though I've now made another branch that simply refactors a lot of this and at the same time wraps up a solution for #222.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16867780/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16874943", "body": "Sounds good. I like PERF_TIMER_GUARD.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16874943/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16911013", "body": "Not quite. Once PERF_TIMER_STOP is called, the timer will not update counters upon destruction:\n\nhttps://github.com/facebook/rocksdb/pull/242/files#diff-62dc0c9a1ea8f4f54dc8848b8ae4177eR50\n\nCalling Stop() sets start_ to 0. The next time Stop() is called, start_ evaluates to false so the counter is not updated. Stop() is called here:\n\nhttps://github.com/tdfischer/rocksdb/blob/perf-timer-destructors/db/db_impl.cc#L3999\n\nAny destructors called after that line will not result in metrics being updated.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16911013/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16926071", "body": "You are correct about needing to resume it. Seems I slipped up and removed it on accident. Should be back now with this latest commit.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16926071/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17249024", "body": "Yes. Fixed in a fresh rebase.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17249024/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17249040", "body": "Done.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17249040/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17249116", "body": "Seems that it isn't as straightforward as thought. I've been able to remove cachable_and compression_type_, but moving data_ and size_ around causes tests to fail. I'll have to investigate it further though this patch already introduces some big simplifications.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17249116/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17249225", "body": "The reason for the change was to take advantage of return value optimizations, and to prevent an extra pointer dereference.\n\nThe Status\\* parameter in these functions is only updated upon failure, where previously the BlockContents\\* parameter in these functions was updated on almost every call regardless of success.\n\nHowever, I can change it back if the improvements from this are more negligible than I think.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17249225/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17612592", "body": "This is true. The non-ownership constructor is just a hack for the unit tests to work. Implementing a BlockContents copy constructor seems like it would solve this.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17612592/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17617618", "body": "Pushed a commit that fixes this.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17617618/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "yhchiang": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16224329", "body": "Maybe you can consider using thread local variable (i.e., `__thread`) to achieve comparator.  In such way each thread can has its own cached data.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/16224329/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "vladb38": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17674421", "body": "Here I need to modify the parameter description\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/17674421/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "lalinsky": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20336638", "body": "On my machine, it's enough to add `kPageSize`, but I wanted to make sure the test passes everywhere and rounding it up doesn't hurt.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20336638/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20336965", "body": "No, the standalone `st_blocks` variable is a left-over from time when it was used in the test below that I just changed. It used to check that the number of blocks before closing the file is greater than the number of blocks after and this is where it remembered the previous count.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20336965/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20337194", "body": "Ok, you edited the comment, so adding a new reply. If you round it to the nearest page up and then calculate the number of blocks from that, it will give the number as it does right now, so this does not work.\n\nThe size is 1048576 (1024*1024), which is exactly 256 pages. If you use that formular, you get 2048 blocks. The actual number of blocks I get on my machine is 2056.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20337194/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20337827", "body": "That works, but what is the advantage over my version? Logically it doesn't make too much sense to me, so it seems like a more complicated, but still arbitrary formula.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20337827/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20338643", "body": "I don't really understand how does the block allocation work, so it's hard to say which version is correct. The point of mine was to just extend the size by some not-completely-arbitrary constant, so that the number of blocks will be always lower than that.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20338643/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20381353", "body": "Sorry for mixing in an unrelated change, but I thought this was minor enough for a separate pull request.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20381353/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20381372", "body": "Not a complete test, just to make sure it compiles and runs.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20381372/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20382390", "body": "Then I have to either change the \"cleanup\" phase, or open a leave the previous database open.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20382390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20384023", "body": "The whole test is kind of messy, there is a lot of state. Technically, destroying just the options should be fine, because `CuckooTableFactory` copies the structure. Alternatively, I could make the test complete separate, not using any of the  shared variables.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20384023/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "zerebubuth": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20929413", "body": "Ooops! Thanks for spotting that, I missed that in the merge. Sorry.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/20929413/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "eile": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21021985", "body": "Yes, meant that. Will resubmit soon.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21021985/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21022036", "body": "Where would that happen? This is the only occurrence of gflags in this file.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21022036/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21022156", "body": "It's nothing to do with the platform, but rather with the user environment. It allows passing additional flags. I use for example\n\n```\nmake -j8 -f Makefile EXTRA_CFLAGS=-I${BUILDYARD_INSTALL_PATH}/include EXTRA_CXXFLAGS=-I${BUILDYARD_INSTALL_PATH}/include EXTRA_LDFLAGS=-L${BUILDYARD_INSTALL_PATH}/lib shared_lib\n```\n\nwhere the install path contains dependencies such as gflags.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21022156/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21074707", "body": "Done.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21074707/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21074713", "body": "Thanks, done.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21074713/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "haneefmubarak": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21057442", "body": "`err` is nonzero even though it was successful\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/21057442/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "robertabcd": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/22701958", "body": "Okay. These will be fixed in the next patch.\nbtw, unique_ptr is used before my patch. Is this allowed but not shared_ptr?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/22701958/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/22702156", "body": "Files may be shared among backups. Each backup is represented by a BackupMeta. file_infos_ contains files mentioned in all meta files. Need a way to store which backup contains which files.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/22702156/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "alabid": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/23658365", "body": "Done.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/23658365/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "pshareghi": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28350274", "body": "My machine is not the most powerful out there.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28350274/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28351574", "body": "I can definitely change that. Should I submit a separate commit on top of the current one (which would be the cleaner approach), or should I --amend and force push it? What is the process here?\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28351574/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28355198", "body": "@igorcanadi, in that case, what happens to the diff I uploaded to Phabricator? I use git and Stash all the time. I am not very familiar with Phabricator. What will I have to do (what command to run) to keep the diff in Phabricator in sync with this pull request (or does that even matter)?\n\nhttps://reviews.facebook.net/D37005\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28355198/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28365462", "body": "Great work! I will add that as well. Since stats_interval_seconds overrides stats_interval, I will keep both.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28365462/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28367209", "body": "Thanks, done!\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28367209/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28367252", "body": "@mdcallag, I increased parallelism per your suggestion.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/28367252/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "HolodovAlexander": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/32669332", "body": "done\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/32669332/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "qinzuoyan": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34757438", "body": "snprintf() ensures that the str will be terminated with a null character, so no risk here :)\nbut space of str[8] may be not enough, str[16] would be better.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/34757438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/37374062", "body": "Yes, in the iterator, \u201cseqno within the same key are in decreasing order\u201d. But in the merge deque, seqno are in increasing order, refer to https://github.com/facebook/rocksdb/blob/master/db/merge_helper.cc#L153.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/37374062/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "javigon": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/38016702", "body": "I did not make it virtual to disable direct IO by default. I can do so and implement in posix and HDFS backends.\nRegarding the name, direct IO is supported by the backend implementing WritableFile, so it does not go all the way up to user options (though it might be in the future).\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/38016702/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/38018641", "body": "I will apply the changes and resubmit tomorrow.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/38018641/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "JohnPJenkins": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39093221", "body": "Reformatted. BTW, do you run the code through a tool or spot-check on code review? Just curious...\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/39093221/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "edsrzf": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/40852529", "body": "Tool sources were already included in the library, so this is not adding any new bloat. (See src.mk, where TOOL_SOURCES is a new variable and the contents used to be a part of LIB_SOURCES.)\n\nThey could be removed, I think, but lots of rules in the Makefile assume that they're there, so the changes became significant when I tried to separate them completely.\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/40852529/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "liewegas": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41466120", "body": "It could, but there were only a handful of changes needed to existing tests.  And we'd probably want to add this again the next time we have some option around log::Writer (like using a stronger CRC, or direct_io, or aio, or...) \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41466120/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41466296", "body": "I think it depends on the other logging parameter (max_write_buffer_number). In our case, I plan to set that to something largish (16?) along with min_write_buffer_to_merge=6 or something so that I can minimize the number of short-lived records that get rewritten. \n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41466296/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41466342", "body": "Yeah there's no cap.  I sort of assumed there wouldn't ever be that many to begin with but I suppose some other feature or config change may result in a large number of older logs that are suddenly available to get expired.  I'll add a cap...\n", "reactions": {"url": "https://api.github.com/repos/facebook/rocksdb/pulls/comments/41466342/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}}}}