{"_default": {"1": {"caabernathy": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/ac67d54bf291eb9546d5480ffa4b2c2ef9ff0740", "message": "Updated readme\n\nArchive message"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mdcallag": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/108b1db77412f5c70476893246259372b6e2cc39", "message": "Merge pull request #30 from ellitron/master\n\nFixed incorrect latency reporting for load phase."}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/c9e43cc21c2bbbef2c2f0e02a748b89192ea936d", "message": "Merge pull request #23 from yoshinorim/doc_fix\n\nFixing id1_type index instruction."}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/2b48bc2f783ecc35d79d86f404b67dee39d07de6", "message": "Merge pull request #21 from mdcallag/master\n\nMake it faster for MySQL"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/54555648", "body": "I am away until Monday. Do we need id1_type, id2_type in the linkbench\nschema? I wasn't sure these were used anywhere.\n\nOn Thu, Sep 4, 2014 at 10:48 AM, Yoshinori Matsunobu <\nnotifications@github.com> wrote:\n\n> Without adding id2, range scans not including id2 column do not use\n> covering index, if storage\n> engines do not include primary key values within secondary index (i.e.\n> \n> ## MyISAM).\n> \n> You can merge this Pull Request by running\n> \n>   git pull https://github.com/yoshinorim/linkbench doc_fix\n> \n> Or view, comment on, or merge it at:\n> \n>   https://github.com/facebook/linkbench/pull/23\n> Commit Summary\n> - Fixing id1_type index instruction. Without adding id2, range scans\n> \n> File Changes\n> - _M_ README.md\n>   https://github.com/facebook/linkbench/pull/23/files#diff-0 (2)\n> \n> Patch Links:\n> - https://github.com/facebook/linkbench/pull/23.patch\n> - https://github.com/facebook/linkbench/pull/23.diff\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/facebook/linkbench/pull/23.\n\n## \n\nMark Callaghan\nmdcallag@gmail.com\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/54555648/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/59827620", "body": "There is a new owner in town (probably me). If you want this merged can you rebase?\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/59827620/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/59827765", "body": "I am probably the new project owner. If you still want this merged please rebase.\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/59827765/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/59827965", "body": "If you want this merged please rebase. \n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/59827965/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/65681013", "body": "We will reconsider this if you rebase.\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/65681013/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/65681043", "body": "We will reconsider this if you rebase.\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/65681043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/65681067", "body": "We will reconsider this if you rebase.\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/65681067/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/110366730", "body": "thank you\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/110366730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "jdellithorpe": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/7608d49662c3826fe4d5640fd13d932afbdb76d5", "message": "Fixed incorrect latency reporting for load phase.\n\nLinkBenchLoad and NodeLoader call the recordLatency method of LatencyStats with\ntime measured in nanoseconds, instead of microseconds, as recordLatnecy\nexpects. This fix scales the time measurement by 1000 to convert to\nmicroseconds."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "maykov": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/0d59ee617cbe6c3a91dde01b175be249c1f29721", "message": "Fixing the build break again\n\nSummary: There was one more place in pom.xml which required version 0.14.0 of the plugin\n\nTest Plan: built on my Laptop while connected to public wi-fi. I had to install Java 1.8. The swift plugin doesn't compile with 1.6.\n\nReviewers: MarkCallaghan\n\nReviewed By: MarkCallaghan\n\nDifferential Revision: https://reviews.facebook.net/D24561"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/7ce37562c1af9b9a49adca35969d32ebafb22f05", "message": "Fixing the build break\n\nSummary: The swift package version 14.0-Snapshot is not released externally. This diff downgrades the dependency to 13.2 which is the latest swift version released externally. For example, see: https://oss.sonatype.org/index.html#nexus-search;gav~com.facebook.swift~swift-generator~~~~kw,versionexpand\n\nTest Plan: built on my devbox\n\nReviewers: MarkCallaghan\n\nReviewed By: MarkCallaghan\n\nDifferential Revision: https://reviews.facebook.net/D23985"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/5c879c9df64756c8eb86e6203f7e1c5a4671c46d", "message": "rocksdb thrift file maintenance\n\nSummary:\n- Modified the pom.xml project file to automatically create .java stubs\n- Removed raw rocks db functions from the .thrift file\n\nTest Plan: ran linkbench against rocks db server\n\nReviewers: andrewcox, MarkCallaghan\n\nReviewed By: MarkCallaghan\n\nDifferential Revision: https://reviews.facebook.net/D19593"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/4f693b73279854ef375d4d5333c358ca98a4c32f", "message": "use Tao API for fbobject\n\nSummary: as title\n\nTest Plan: run linkbench\n\nReviewers: emayanke, MarkCallaghan, jiangxw\n\nReviewed By: MarkCallaghan\n\nSubscribers: ljin\n\nDifferential Revision: https://reviews.facebook.net/D14997"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yoshinorim": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/adb03917f6ebae63d70889bd850dc84da1c36182", "message": "Fixing id1_type index instruction. Without adding id2, range scans\nnot including id2 column do not use covering index, if storage\nengines do not include primary key values within secondary index\n(i.e. MyISAM)."}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/54664505", "body": "We don't have id1_type, id2_type columns and they are not used anywhere.\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/54664505/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "jiangxw": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/9aade44f74a6091ffebfb9ee29877ad2821ed2c5", "message": "Merge pull request #22 from jiangxw/master\n\nget linkbench working with rocks db"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "timarmstrong": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/251b5ca4fbaa4747e8d9f8dfbb92137a1355880a", "message": "Merge pull request #16 from colorant/statistic\n\nMinor fix of requester statistics"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/ba9f80d6512b20ff2690bd4a4ec5101e093fd537", "message": "Fix corner case where hidden link is expunged\n\nSummary: Github Issue #9.  DeleteLink doesn't expunge hidden link.\n\nTest Plan: Ran MySQL Link store test\n\nReviewers: dhruba, emayanke\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D10605"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/183f876cbd6c3dd1d76b5104be6208dd041d6d95", "message": "Fix potential race condition with delete.  Document ACID.\n\nSummary:\nFix race condition that can occur with deletes where two concurrent queries change visibility or delete the row.  This is fixed by acquiring a row lock upon the initial read of the row.\n\nImprove documentation of how we maintain consistency of counts with\nInnoDB transactions in other cases.\n\nTest Plan: Ran MySQL tests\n\nReviewers: dhruba, emayanke, MarkCallaghan\n\nReviewed By: MarkCallaghan\n\nDifferential Revision: https://reviews.facebook.net/D10659"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/25b065ef450b0487909bf9daf38ef603506d4985", "message": "Merge pull request #15 from colorant/gennode\n\nFix genNode to fill version and time in right field."}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/a3575dd808bbf175100d4e07a4a955f023061bea", "message": "Consistent handling of link count version and timestamp\n\nSummary:\nWe don't handle version and timestamp for link count table\nconsistently between deleteLink and addLink.  This change\nstandardises so that time is timestamp of last update,\nand version is updated on every change.\n\nTest Plan: ran MySqlLinkTest\n\nReviewers: dhruba, emayanke\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D10611"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/e5ff5d8d98f693ae8cae045c12973c225cbac4aa", "message": "Specify locale for distribution file scanner.\n\nSummary:\nError reported in github issue #2\n\nPreviously the default locale would be used.  Some locales do not use the period as the decimal separator.\n\nTest Plan: mvn test succeeds\n\nReviewers: emayanke, dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D10299"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/0b8d5c9b5270cf601e25438da369d7ec7fcc6842", "message": "Document assumptions about node ID allocation more clearly\n\nTest Plan: Ran ant test\n\nReviewers: emayanke, dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D9999"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/2fc7ab8a1d0e7f4214102810e9403a189448591a", "message": "Convert LinkBench to use maven\n\nSummary:\nmoved java sources to standard maven folder\n\ninclude sources in distribution\n\nfinished mavenizing the project, changed readme accordingly, made sure that final jar file is bundled with dependencies, so nothing else but that file needs to be on classpath\n\nfixed final jar name\n\nMake maven happier by specifying version\n\nImprove test running with maven\n\n* Add option to skip slow tests\n* Only run provider-specific (e.g MySQL) tests when explicitly asked to\n\nMake TimerTest more tolerant to avoid extra failures\n\nTest Plan: Ran mvn package\n\nReviewers: emayanke, dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D10011"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/d429300205238941f0a00d20bea2cd66f525c67a", "message": "Improve test running with maven\n\n* Add option to skip slow tests\n* Only run provider-specific (e.g MySQL) tests when explicitly asked to"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/752ef4bab29c126371204bbf6232753c04360e20", "message": "Make maven happier by specifying version"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/32da2d3dfc862268c6a324bb72a6c47a769dcc4c", "message": "Merge mavenization of LinkBench"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/930ea9e43d12dccac516795efab445ba65caf681", "message": "Document fields of Link/Node\n\nSummary:\nClarify fields in each data type, in particular the different function of\ntimestamp in the two data types.\n\nTest Plan: Just added comments.  It still compiles\n\nReviewers: dhruba, emayanke\n\nReviewed By: emayanke\n\nDifferential Revision: https://reviews.facebook.net/D9957"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/64441b5d6dbc1162dd905d2cd4994311928312e4", "message": "Add more meaningful error messages to RealDistribution when loading from file\n\nSummary: To help with diagnosing failures like Github issue #2\n\nTest Plan:\nant test passes\n\nEdited distribution file to make it malformed to confirm that error is detected\n\nReviewers: dhruba, emayanke\n\nReviewed By: emayanke\n\nDifferential Revision: https://reviews.facebook.net/D9951"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/6e201764a5de5b477442309586e1784fac1c7906", "message": "Add linter to arcanist config.  Fix whitespace errors\n\nSummary:\nArcanist text linter to detect basic formatting problems like\ntabs and trailing whitespace.\n\nClean up whitespace errors to avoid noise in future diffs\nfrom whitespace fixes\n\nTest Plan: Still compiles ok\n\nReviewers: emayanke, dhruba\n\nReviewed By: emayanke\n\nDifferential Revision: https://reviews.facebook.net/D8301"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/24a48fb97645da689c6a78b4ce09ae7c2fb16c58", "message": "Remove hardcoded maven settings.xml ref\n\nSummary:\nThis stops ant build from succeeding since settings.xml not bundled with\nlinkbnech.\n\nProxy settings can be configured globally e.g. with $HOME/.ant/settings.xml\n\nTest Plan: ant succeeds in compiling\n\nReviewers: emayanke, dhruba\n\nReviewed By: emayanke\n\nDifferential Revision: https://reviews.facebook.net/D8295"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/6438dae65cc7c6c54334fe99a5adb643d66975f7", "message": "Close stores properly at end of load and request\n\nSummary: Previously linkbench load and request didn't clean up stores nicely\n\nTest Plan: ant test passes\n\nReviewers: emayanke, dhruba\n\nReviewed By: emayanke\n\nDifferential Revision: https://reviews.facebook.net/D8289"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/f3eebfa8a2e88ab6a4d5c2830618ba3454b586c4", "message": "Fix minor error in example code\n\nSummary: Database name was wrong\n\nTest Plan: Not applicable\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D7233"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/0bad8d25fc83e8b4025ecbdabd9d9288467250ef", "message": "Improve handling of MySQL errors in LinkBench\n\nSummary:\nClassify JDBC sql states into ones that merit retry or not\nE.g. Lock wait timeout errors can be handled by restarting transaction\n\nLog details of all SQLExceptions\n\nWrap all LinkStoreMysql operations with retry logic\n\nTest Plan: Checked that ant test still passes\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nCC: MarkCallaghan\n\nDifferential Revision: https://reviews.facebook.net/D7191"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/affc37499156b46ed51fa005e76bd553ed7e5203", "message": "Move ApproxHarmonic.java to distributions folder\n\nSummary: For some reason I accidentally had it in the wrong place\n\nTest Plan: Checked that it compiled ok\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D7185"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/a62f8a7a7218944e9651d9c0a43f0decace338af", "message": "Add apache 2.0 license\n\nSummary:\n* Add LICENSE file to root directory\n* Add boilerplate comment to top of all .java files\n* Except for Harmonic.java which is partially derived from Apache Commons Math\n* Split harmonic approximation out from Harmonic.java so that Facebook code is separated from Apache Commons derived code.\n\nTest Plan: Check files still compile\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D7125"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/d51f67be758980eec93da1d3bcc866c532f66851", "message": "Update libraries and license info\n\nSummary:\nUpdate MySql Connector/J to latest release.\n\nInclude Connector/J source.\n\nAdd notice mentioning licensing of included libraries.\n\nTest Plan: ant test passes\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D7131"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/c94dae2888fb31ca11caaea9d77a5d27add3f383", "message": "Fix bugs with distributions on large numbers\n\nSummary:\nUniform choose function didn't work correctly and produced out-of-range results\ndue to logic errors in random number generation\n\nZipf distribution didn't support large number for no particular reason\n\nTest Plan: ant test passes.  Added test to check values generated in range.\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D7095"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/f48c4de026bc4549c9ef4376fe3bd58cc9392be2", "message": "Speed up zipf distribution\n\nSummary:\nFor large n (billions) and shape <= 1.0, can use a sufficiently accurate approximation\nthat reduces calculation time from minutes to milliseconds.\n\nThis speeds up benchmark initialization greatly\n\nAdd log message to warn about long Zipf distribution calculation time\nfor (unusual) cases where approximation doesn't work.\n\nTest Plan:\nAdded test to check error is small ( < 0.05%)\n\nChecked that ant test passes\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D7101"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/59f6fd4231cd24cbb1956608f19911b8e5c4c56a", "message": "Fix a typo in readme.  Fix ascii art not rendering right"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/58ddde42457ba2c547e41641053faeffefcb2129", "message": "Rewrite README files in markdown format for Github\n\nSummary:\nREADME.md will be the front page for LinkBench on github.\nIt has been rewritten with more detailed instructions for using\nLinkBench\n\nDataModel.md replaces README_Overview.txt\n\nTest Plan: Rendered markdown file and checked it looked ok.\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D7083"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/926f25f4c604bd00118c2ffbcc2d55110e2e5a7e", "message": "Fix bug with config files where overriding from command line didn't work\n\nTest Plan: Ran benchmark, test that -D argument took effect\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D7077"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/a7412854c950355ef4fdb9febe1cbbfe97828391", "message": "Fix workload file to match intended\n\nSummary:\nI made an error in transcribing settings from our internal config file:\nlink_type_count should be 2\n\nTest Plan: not applicable\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D7065"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/dfcf4269b50b5263a41d61271c7f98d59133ff8d", "message": "Remove unused id1_type and id2_type fields from link\n\nSummary: Not needed so remove from MySql schema, LinkBench driver and tests\n\nTest Plan: ant test passes\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D6645"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/bc0f5e27d7bbffe9d32f722a686e39bdebb44ccb", "message": "Update sample config file to reflect latest work\n\nSummary:\nUse parameters derived from real workload analysis\n\nLogically group options\n\nBetter document what settings do\n\nMake randomid2max setting optional since it is unused in standard config\n\nTest Plan: Checked new config file works by copying, filling in mysql connection parameters, then doing test benchmark run\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D6957"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/f739d42419de6d5c26efceca66dd8955dd064731", "message": "Add warmup time setting\n\nSummary:\nCan now specify a warmup time in seconds during which no stats are recorded.\n\nRefactor stats to enable more control over display.\n\nTest Plan:\nant test passes\n\nDid test load and request, inspected output to make sure times and counters are right.\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D6735"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/a07281289bb8cd4ddd72e12c3998ebdd71b418ec", "message": "Add in payload generator driver to test compressibility\n\nSummary:\nTweak object motif to get ~60% compressibility and assoc to get ~30%\n\nThis driver was used to discover parameters for data generators that had similar compressibility to real data.\n\nTest Plan: Ran and inspected output\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D6621"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/88b7e650ddb27de96f6648a33338f5b1593c0b98", "message": "Cleanup information reporting for load and request phases\n\nSummary:\n* Remove misleading \"expected\" link count\n* Clean up log messages a bit\n* Remove crufy from output\n* Add new optional config file keys to control progress report frequency to remove hardcoding\n\nTest Plan:\nant test passes\n\ndid small test run\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D6531"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/872f9229e476861347798dcc655364db5d068044", "message": "Remove unneeded id2_vis index on linktable\n\nSummary:\nSecondary index isn't required by any queries in workload.\n\nRemove from instructions and tests\n\nTest Plan: ant test passes\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D6477"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/b3a6f55dec72a5495e58a134fd5219af6c72fe29", "message": "Fix id2 generation\n\nSummary:\nMake sure no duplicate keys for gets\n\nExtract constants for uniqueness params and document\n\nTest Plan:\nant test passes\n\nDid test run to make sure output is sensible\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D6471"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/3e79fed2a5e5ceb31ad83ad1f79f330b8a0ef2e0", "message": "Tweaks for driver code for link modification\n\nSummary:\nMove parameter selection outside of timer region\n\nRandomly choose link type in a few places where missing\n\nChange linkstore interface to report whether links found\n\nAdd in logging to log result of add/update/delete for validation\n\nTest Plan: ant test passes\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D6453"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/7128217e7f14e4e00513b25b188e6a1956fb3266", "message": "Fix motif data generator to work for small buffers\n\nSummary: Previously had not inserted motif at first chunk.  This was unnecessary and meant that for short buffers there was no redundancy\n\nTest Plan: ant test\n\nReviewers: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D6447"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/96fdd890eb75c2b3df6de585509c9b427401fed3", "message": "Bugfix for reading value from config file\n\nSummary: tried to read int instead of double\n\nTest Plan: not needed\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D6429"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/a5d6e63c82072a84dd2a6ffeed5eb90875d88ef8", "message": "Alter history cache algorithm to reduce skew\n\nSummary:\nPreviously the way caching worked tended to result in skewed behaviour:\n* frequently accessed ids ended up having many cache entries\n* entries were never evicted so we often kept reaccessing the very tail of the lists\n\nThis commit changes the cache algorithm so that\n* There is only one cache entry per (id1, link_type)\n* Entries are evicted once the full list has been retrieved\n\nTest Plan: Adapted unit tests to reflect fact that history cache more often empty\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D6399"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/0efda201fe9a37285157264cb3ab83621c3ec8ce", "message": "Fix bug caused by using contains instead of containsKey\n\nSummary: Blending of distributions wasn't activated correctly because of use of wrong method\n\nTest Plan: ant test passes\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D6381"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/4d94803f11f0c688f6333b38f9d9c2b0678fcaad", "message": "Add in support for blending in an uncorrelated\n\nSummary:\nWe see an imperfect correlation between outdegree and frequency of access\nfor link reads/writes.  In order to support such partial correlation, this\ncommit adds support for mixing together a perfectly correlated and perfectly\nuncorrelated distribution to choose ids for access.\n\nThis can bring down average range scan size to more realistic levels.\n\nSome optional config file entries were added to support this.\n\nTest Plan:\nChanged unit test to exercise this functionality.\n\nant tests passes\n\ndid test load and request with command line client\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D6363"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/0eeb23522cf0aa067665f03012cfcc34722b754d", "message": "Uniform distribution for node deletes\n\nSummary:\nPreviously node deletes were distributed in the same way as updates & reads.\nThis makes little sense as\na) we don't want to delete the most frequently read nodes\nb) nodes can only be deleted once\n\nWe allow a node delete distribution to be configured separately from\nupdates.  By default use uniform distribution for deletes.\n\nHad to update default config file with updated keys.  Derivative config files will need to be updated.\n\nTest Plan: ant test passes\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D6333"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/708b635703b3086b907e99db6bc6ec5fb00ce97b", "message": "Fix config file, improve error reporting\n\nSummary:\nFix errononeous key/value names in default config file.\n\nAdd better error checking to report to user if key is missing.  Alter\nusage of config file so that all config key lookups are checked.\n\nTest Plan:\nant test passes\ndid test load/run using default config file (only with database parameters filled in)\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D6249"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/7264c57332fddcb2e5d80068bb99caa19cfadb2a", "message": "Fix LinkBench to not depend on Mysql bug 46675\n\nSummary:\nPreviously LinkBench depended on the behaviour identified in Mysql bug 46675\nfor determining how to update link counts.  Recent versions of MySql (both on the 5.5 and 5.6 releases) have had this bug fixed.\n\nThis revision fixes the problem by using the affected rows count, which can distinguish between the three cases.\n\nTest Plan:\nant test passes on mysql server versions 5.5.24-ubuntu and 5.1.53-faceboo\n(with and without the bugfix in question)\n\nReviewers: dhruba, vamsi\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D6009"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/8093245493a1b3b2feecd64926bb13f39c9b5728", "message": "New Motif data generator\n\nSummary:\nTry to generate somewhat more realistic data that has some of the\ncompressibility properties of real data.  The basic idea is that\na data generator has a set of \"motifs\" that will repeatedly occur\nin the output data.  This is implemented by having an internal buffer of\nrandom data from which fragments are inserted into the output.\n\nTest Plan:\nUnit test to exercise code.\nTest the compressibility of the output by compressing with gzip to make\nsure that the compreesibility of the output data is being changed in the\nway we'd expect as parameters vary.\n\nTest the performance of the data generator to make sure it won't be\na performance bottleneck.  It seems like it is at worse marginally slower\nthan the uniform data generator, and at best somewhat faster (if there\nare many repeated motifs).  In either case it won't be a bottleneck.\n\nReviewers: dhruba, vamsi\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D5289"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/d04cc3802facb023318ae733546f46cf26ba72b6", "message": "Multiple link types for LinkBench\n\nSummary:\nAdd in config parameter to control number of link types (default to 1 if\nnot provided).\n\nThe number of links per id1 is the same as before the change, but these\nlinks are divided between the n link types.  If we number the link types\n1 to n, the links are distributed in a round robin fashion.  E.g. if\nthere are five types and two links for id1=1234, then there is a link of type 1 and a\nlink of type 2.  If there are five types, there is a link of each type.\n\nThis change makes the count table much bigger.  Switch bulk count add\nlogic to use \"REPLACE INTO\" to speed up loading of link counts.\n\nTest Plan:\nant test\n\nDid a small benchmark run to check that average range scan size was\nreduced proportionally (it was)\n\nReviewers: dhruba, vamsi\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D5253"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/c034afd1344e9a4ccefaf33e1077f30c048326d3", "message": "Fix error in real distribution: miscalculated total number of id1s\n\nSummary:\nNow I calculate the number of id1s with zero reads as (number of\nFBObjects - number of unique id1s read).\n\nFix other minor bugs: too large tolerance with error checking,\nincorrect initialisation of RealDistribution for new modes\n\nTest Plan: ant test\n\nReviewers: dhruba, vamsi\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D5211"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/ff9c37533d9ed0b039d5b55f3471188112474cb2", "message": "Update real distribution data\n\nSummary:\nAdd in data for node read and write access patterns\n\nUpdate existing distributions to use data from most recent analysis\n\nTest Plan: ant unit\n\nReviewers: dhruba, vamsi\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D5181"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/d3d90f4ff0826e3f910e94bdbfb6c822a3ec3a6a", "message": "Fix id2 selection so that we get expected hits/misses\n\nSummary:\nWe want the operation id2s to be chosen so that link updates and deletes are mostly for existing (id1, id2) links, and so that link inserts are mostly for non-existent (id1, id2) links.  Add test and fix bug so that this is the case.\n\nRefactor so that nlinks calculation in one place rather than two\n\nStop startid1 being silently changed\n\nTest Plan:\nant test passes\n\nAdded test to make sure behaves as expecte\n\nran test, checked that expected number of\n\nReviewers: dhruba, vamsi\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D5157"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/93d4d5393e770b48deddb869ebbbdbac7c2de1a1", "message": "Variable data size for nodes and links\n\nSummary:\nPreviously all links/nodes had same data length.  This isn't very\nrealistic, so this diff makes the data length variable.  The\ndistribution is log-normal, which has a similar shape to the observed\nreal distribution.  The median size values in the config file give a\ndistribution with the same mean size as the real distribution.\n\nAdd log-normal distribution.\n\nRefactor the initialisation for the request somewhat since it was\ngetting really unwieldy with additions.\n\nTest Plan: ant test <-- all tests pass\n\nReviewers: dhruba, vamsi\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D5121"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/912dbe35c838f97f4dc1ab427cdf5deb88b419a7", "message": "Different access distributions for node reads and writes.\n\nSummary:\nPrevious access distributions for node read and write was same.\nChange this so that r/w dists are different in line with link\ndistributions.\n\nAdd test for full graph store (fbobject + assoc) workload including\nloading and requesting so that new code (and node requesting in general)\nis exercised.\n\nTest Plan:\nAdded new test to exercise code\n\nant test works\n\ndid test load and request run, works fine\n\nReviewers: dhruba, vamsi\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D4971"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/ea93e56a8f35ae31f2a30b4458f9d22719d0da88", "message": "Specify distribution in terms of mean instead of scaling factor\n\nSummary:\nIt is more convenient to specify the mean # of links instead of the total number of links, as you don't need to update the config when you change the number of id1s.\n\nUpdate config file keys and stats distribtions\n\nTest Plan: ant test passes\n\nReviewers: dhruba, vamsi\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D5055"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/67b5f2429a982004a3b2eaf7334957c8d4ef80a8", "message": "Commit optimization for MySql data provider\n\nSummary:\nAvoid a roundtrip in some cases on addLink operation by including commit inline with other statement.\n\nIn my testing this reduced add and update latency by a couple of ms\n\nTest Plan:\nant test\n\nAdded additional test to verify correct behaviour.\n\nReviewers: dhruba, vamsi\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D5127"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/00f161b685b01b32fbc47ab03a58739dc53f903a", "message": "Fix some rough edges in database initialization\n\nSummary:\nRan into some problems doing large tests on a new database.\n\nDefer initialization of linkstore by requesters and loaders.\nThis helps if there is high latency since the connections are now opened\nconcurrently.  It also avoids store timeouts if stores are created\nbefore other startup logic (e.g. initializing ZipfDistribution for large n)\n\nDon't connect to database \"test\" by default - it may not actually exist!\n\nThrow exception instead of just quitting - easier to get stack trace and\ndiagnose\n\nTest Plan:\nant test\n\nran benchmark for 30000000 id1s on test database\n\nReviewers: dhruba, vamsi\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D5061"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/4a96dcc22d80ce0df0f5e12faedacb122486e381", "message": "Update running calculation of stats\n\nSummary:\nRefactor stats collection in separate package, add javadocs to explain difference between two\nstats classes.\n\nWe currently collect samples of statistics and print them out at regular\nintervals.\n\nAugment this so that it collects a balanced sample across the time\ninterval.\n\nAugment this so that it prints out the mean value (needed immediately to\ncheck that average range scan size is representative.\n\nLog progress to csv file so that latency v. time can be analysed\n\nTest Plan: ant test\n\nReviewers: dhruba, vamsi\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D5049"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/d3f83c80c4560dd5038724a681f34c58e436a330", "message": "Change MySqlLinkStore to send inline commit\n\nSummary:\nSave some client-server roundtrips on single-statement transactions by\nappending commit to sql query.\n\nTest Plan: ant test passes\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D5037"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/9822ddd4cd759ae003bef36e462061a1cad46202", "message": "Revamp stats collection\n\nSummary:\nIncrease granularity of latency measurement in ranges < 1ms and >100ms\n\nCompute mean, maximum alongside percentiles\n\nOutput data in structured format to csv file\n\nTest Plan:\nadd test for bucketing since that seemed most error-prone\n\nant test\n\ndid test benchmark and checked csv output was sane\n\nReviewers: dhruba, vamsi\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D4995"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/c3604d2cbbcd29312ca376fd8ea51bfaa652db9b", "message": "Add in link multiget support for LinkBench\n\nSummary:\nReal workload has multiget requests, where a small number of keys are\nselected.  The pattern of requests is loosely approximated with a\ngeometric distribution.  The specified parameter gives a distribution\nwith the same mean as observed\n\nAdd MultiGet to linkstore, in such a way that it doesn't break existing\nlinkstore implementations by defaulting to calling getLink multiple\ntimes.\n\nTest Plan:\nunit test for multiget on link store\n\nunit test for geometric distribution\n\nunit test for requester with multiget enabled/disable\n\nant test -> all tests pass\n\nReviewers: dhruba, vamsi\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D4947"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/9ab244b430384ba8be2a8096a33b214475a9eb0c", "message": "Better caching of zipf calcs\n\nSummary:\nIn some cases multiple zipf distributions are used with different\nparameters.  In this case it's useful to cache multiple zetan\ncalculations so that it doesn't take a long time to initialize a\ndistribution per thread.\n\nTest Plan: ant test\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nCC: vamsi\n\nDifferential Revision: https://reviews.facebook.net/D5031"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/ef58c2b102ba0dc4243c33313bd5d77bb6d2f80d", "message": "Add in historical range queries.\n\nSummary:\nBefore this diff, only the first 10000 assocs for big assoc were every\nretrieved.\n\nThis change simulates the process of the client fetching the first 10000\nassocs, then selecting the next 10000 based on timestamp.\n\nA small list of bounded size is maintained in the Requester of (id1/assoc_type) pairs\nwhere the 10000 row limit was hit, along with the timestamp of the last\nassoc.  This data can then be used to construct queries for the further\nback history.\n\nThe % of get_linklist requests which request history data (not the most\nrecent rows) is controlled by a config key \"getlinklist_history\"\n\nAdded the ability to change the row limit from the default (10000) to\nanother value.  This is used for unit testing currently, but in future\ncould be used to experiment with different row limits.\n\nTest Plan:\nant test\n\nNew unit test added that exercises requester history query code path.\n\nReviewers: dhruba, vamsi\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D4941"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/e83073c1f74fc2ceff9f58ec5996060f412e5a88", "message": "Extract id2 generation logic and clean up.\n\nSummary:\nThe same logic had been copied and pasted to a few locations, and\nthere was interdependent logic for id2s in the Load and Request files.\n\nThis diff puts all the logic into the ID2Chooser file, making it\neasier to understand and test the ID2 selection logic\n\nAlso extract timestamp creation logic for loading branch and modify so\nthat id2 order doesn't match timestamp order\n\nTest Plan:\nAdded unit test for id2 chooser\n\nAs usual, passes existing unit tests\n\nReviewers: dhruba, vamsi\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D4935"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/92fa3a9d7529e12bf80aea907010cf64c8be2744", "message": "Load speed improvements, incl. new shuffler\n\nSummary:\nNew invertible shuffler implementation that allows us to go from the\n real ids (where the location of hot rows is randomized) to the\nranked ids (where the ids are ordered from most to least hot) and\nback again.  This simplifies the workload generation, since we\ncan now directly work out, e.g. the expected number of links\nof id x.\n\nSwitch workload to use this.  Now loads data in order of id1 to reduce\nbuffer pool churn on large databases, which can speed up loading a lot\nonce the size of the db is larger than RAM.\n\nCache zipf distribution calculation to speed up initialization for large id\nspaces (I realized this was took several minutes when doing some test loads)\n\nTest Plan:\nAdded unit test for new shuffler\n\nPasses existing unit tests and test benchmark run\n\nReviewers: dhruba, vamsi\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D4875"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/ce428d542ce1162f85bfdd49de607918475ad395", "message": "Refactor ID choosing logic into AccessDistribution\n\nSummary:\nThe purpose of this change is to:\na) separate out id selection logic for easier understanding and testing\nb) allow probability distributions to directly drive id selection\nc) make it easier and more uniform to have different id selection\n  mechanism for node/link reads/writes (don't need to special-case\n  everything)\n\nUse AccessDistribution class to contain logic for choosing IDs,\nconsolidating existing choices and allowing arbitrary\nProbabilityDistribution classes to be used for choosing IDs (including\nZipf, Uniform, etc).\n\nNode, write and read distributions are implemented using\nAccessDistribution classes now, which will make it easier in future to\ne.g. have separate distributions for node read/writes.\n\nUpdate config file format to use names rather than magic numbers\n\nTest Plan:\nPasses unit tests and test benchmark run\n\nAdded additional unit tests for access distributions\n\nReviewers: dhruba, vamsi\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D4857"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/702fb1aa7ace2efa3ecc8364d0117d63476efef8", "message": "Consolidate probability distributions, add Zipf as links opetion\n\nSummary:\nCreate LinkDistribution class to encapsulate link distribution logic and extract the complicated cases and configuration from the LinkBenchLoad and LinkBenchRequest classes.\n\nZipf distribution (closely connected to pareto) is now an options for\ndistributing links.\n\nExtend probability distribution with density and quantile functions\nrequired for links and useful for testing.\n\nRefactor so that RealDistribution is based on the ProbabilityDistribution framework, with a generic PiecewiseDistribution implementation that is easier to test independently.  PiecewiseDistribution doesn't support all probability distribution methods yet, but does support the ones required for getting number of links.\n\nChanges config file settings so that link distribution is specified by keyword rather than magic number (this is not backwards compatible, but should be a bit usability improvement going forwards).\n\nFix bugs revealed by new unit tests\n\nMinor changes such as consistently using numbers in the [0.0, 1.0]\nrange to represent probabilities (before sometimes percentages were\nused).\n\nTest Plan:\nAdded additional probablity distribution tests\n\nStill passes tests and passed small benchmark test\n\nReviewers: dhruba, vamsi\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D4851"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/76915bf1f063c72042cc7d64dfc06bb495a2bd51", "message": "Fully integrate fbobject workload into Linkbench\n\nSummary:\nAdd in fbobjects (aka nodes) to the load and request phases.  A separate thread\nloads objects during the load phase.  Each requester thread also makes\nfbobject operations during request phase.\n\nMinor changes to NodeStore API that were needed for this: initialize and\nreset after errors, support bulk loading of nodes, change addNode function\nso that it doesn't modify input arguments.\n\nA lot of the work was in allowing configuration of the workload, and in\nparticular dynamically loading and configuring different implementations\nfor different distributions\n\nExtracted data generation code into separate class that can be\nconfigured and shared between link and node data generation\n\nAdded in generic probability distribution framework and start using it\nfor nodes (can use for links later).\nInclude uniform and zipfian distributions to start off.\n\nA better handling of failures during request phase (abort after certain\nnumber of failures).\n\nAdd in missing commits for a couple of mysql operations\n\nTest Plan:\nant test\n\nTest loading and requesting for a small benchmark with bin/linkbench\n\nAdded tests for data distributions\n\nNeed to add more thorough unit tests for loading and requesting of objects\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nCC: vamsi\n\nDifferential Revision: https://reviews.facebook.net/D4779"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/ed7d5441dfb1703adf9b2e7196150f1e7fc060f4", "message": "Fix flaws in LinkBench build/test process.\n\nSummary:\nChange jar naming so that linkbench always runs latest built jar\n(previously it would often use an old one with a different date)\n\nChange tests so that they now run with assertions turned on: previously\nby accident they had been running with assertions disabled.\n\nThis meant there were a couple of silly bugs in the code that the\nassertions were meant to detect: these are now fixed.\n\nTest Plan: ant test\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nCC: vamsi\n\nDifferential Revision: https://reviews.facebook.net/D4863"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/e9ddab88ee2673291b85c6e62e5b8848ac9cf823", "message": "Refactoring: clean up random config keys in code\n\nSummary:\nConsolidate shared config keys into single file rather than\nhaving them duplicated everywhere.\n\nAlso add separate linkstore and nodestore config keys in prepartiong\nfor future changes.\n\nConvert string argument to enum\n\nTest Plan: ant test\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nCC: vamsi\n\nDifferential Revision: https://reviews.facebook.net/D4773"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/a08fb64a5756c347d70fc9fdf730e9f277176008", "message": "Add in storage implementations for graph nodes\n\nSummary:\nNodeStore is separate interface to LinkStore for now, so that we don't\nhave to immediately add new methods to all LinkStore implementations.\n\nHave not yet integrated NodeStore into actual workload.\n\nIn-memory and my-sql implementations for node storage\n\nBasic unit tests for node storage\n\nFixed sql inject bugs, handling of binary data\n\nFix binary data handling, regression tests included\n\nTest Plan:\nant test\n\nAlso test loading and requesting data with mysql\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nCC: vamsi\n\nDifferential Revision: https://reviews.facebook.net/D4725"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/9259d55fa936a45eccda6b78b1f78cc5b4977b34", "message": "Add option to throttle request rate per thread.\n\nSummary:\nSimulate poisson process with inter-arrival times exponentially\ndistributed. Includes unit tests for request rate functionality.\n\nAlso work on consolidating random number generation.  The random number\ngeneration in the original implementation creates many Random() instances,\neach of which is seeded with the current time at construction time.\nThis is not really ideal as the Java RNG is fast but not particularly\ngood quality and the stream of numbers from a single RNG is likely to\nbe better quality that that from many RNGs with correlated seeds.\n\nAlso working towards deterministic random number generation, where\na single initial seed determines the entire request workload. This\nallows workloads and tests to be reproducible.  That seed is used for\na high-quality but slow RNG, which then generates seeds for new\nfaster RNGs that can be passed to threads in such a way that the calls to\neach RNG always occur in the same order.  The request workload is now\ndeterministic except for the id1 selection algorithm in\nRealDistribution, which is inherently non-deterministic because it\nmutates arrays shared between threads.\n\nTest Plan:\nant test\n\nTo check deterministm, ran small request workload multiple types, checked that the count of each type of operation stayed the same (to check determinism)\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nCC: vamsi\n\nDifferential Revision: https://reviews.facebook.net/D4695"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/d2c72be505e7b3ae84d5577240d1016e58a97982", "message": "Further extensions to LinkBench tests, bugfixes\n\nSummary:\nAdd more linkstore tests for basic operations, tweak test framework to\nfix problems uncovered by tests\n\nSmall tests for LinkStore update and retrieval operations to check\noperations behaving as expected\n\nBasic test for linkbench requester that runs small single-threader\nrequest workload and checks operation counts.\n\nAllow for concurrent memory store access\n\nMiscellaneous bugfixes revealed by tests\n\nTest Plan: ant test -> all tests pass\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nCC: vamsi\n\nDifferential Revision: https://reviews.facebook.net/D4665"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/36efdb4f6d17477fc267cf8ecc6537d3500f6faa", "message": "Initial work on JUnit test suite for Linkbench\n\nSummary:\n* Generic basic tests for link store/load/request drivers\n* In-memory link store to use for lightweight tests of other components\n* Split test code and other code\n* Convert realDistribution test into JUnit test, switch to calculating\n  RMS error between two distributions which will make it easier to pick\n  a threshold for good versus bad matches\n* Fix binary search bug\n\nTest Plan:\n1. Setup mysql test database on localhost\n2. ant test -> all new tests run and pass on my dev server\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nCC: vamsi\n\nDifferential Revision: https://reviews.facebook.net/D4653"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/7d97a5725b152ddd16b9508a4d4bd367e953f182", "message": "Linkbench usability improvements\n\nSummary:\nAllow specifying config properties through command line arguments\n\nAdd progress reporting to read stage\n\nImprove reporting of progress information: % done, ops/sec\n\nTask ID: #1144990\n\nBlame Rev:\n\nTest Plan:\nTested overriding config parameters\n\nRan load, checked that progress information was correct\n\nRan request, checked that progress information was correct\n\nMinor refactoring: switch from int for phase to enum\n\nReviewers: vamsi, dhruba\n\nDifferential Revision: https://reviews.facebook.net/D3975"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/15e203b45b9b1f642443fe18e20f854cb7c599e5", "message": "Further optimizations to bulk loading\n\nSummary:\nIncremental bulk loading of counts: insert into the count table\nas we go.  This should be more IO friendly that rescanning the\nlink table.\n\nAlso add option to disable binary logging for load.\nBinary logging might not be necessary when bulk-loading the\ndata base in many scenarios.  This diff adds the option\nto disable binary logging while bulk loading\nfor LinkStore benchmarks without restarting mysqld.\n\nTask ID: #1143674\n\nBlame Rev:\n\nTest Plan:\nRan load, checked resulting tables for sanity\nChecked that binary logs did not grow during loading process\n\nReviewers: vamsi, dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D3927"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/fda92177542f97128d05d4f640a438df98c09f3e", "message": "Work queue model for loaders\n\nSummary:\nThis diff addresses the tail off in loading throughput towards the\nend of the loading process due to skew in the chunk sizes given to the\nloader threads. Because of the distribution of links, some chunks of the\nid1 space have many more links to load than others.  The most robust\napproach I could think of to solve this was to have a smaller pool of\nloader threads (~10) and dynamically assign the chunks to threads with\na work queue.  If we break the work up into chunks such that\nleast work, then all of the threads will likely run out of work at\nnearly the same time.\n\nSince the per-thread progress reports aren't meaningful in this model,\nI added in a global progress report and have chunk progress reports if\ndebug logging is turned on.\n\nTask ID: #1143673\n\nBlame Rev:\n\nTest Plan: Ran loader for 10e7 id1s and checked table sizes and top counts for sanity\n\nReviewers: vamsi, dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D3921"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/15516df2512e3976ecad502fcc26e0056577f6bc", "message": "Added in bulk loading capability for LinkBench\n\nSummary:\nInstead of loading links one-by-one and updating counts each time,\ndo inserts with many links, and create the count table with a single\nquery at the end.\n\nThis gives a big speedup: the load completes 5x-6x faster on the small\nloads I've tried.  The two changes (batching inserts, and doing the\ncount table at the end) seem to contribute roughly equally to the\nspeedup.\n\nI haven't tried to tune the thread count or the batch size yet, so there\ncould be even bigger gains from that.\n\nMade a couple of other minor changes to code org since it made it easier\nto implement the bulk loading\n\nTask ID: #1143673\n\nBlame Rev:\n\nTest Plan:\nRan loader and compared with output when default loading turned off\n(distribution and row counts approximately the same).\n\nReviewers: vamsi, dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D3903"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/abb0cfdee1790c98deee7074ba691efbddf486e3", "message": "Improve logging output for linkbench\n\nSummary:\nThis work is basically groundwork to make it easier to do perf tests for Linkbench loading.  Apologies for the long diff.\n\nGoal 1 - add timestamp, thread ID and log level to log, to allow better log\nanalysis\nGoal 2 - allow for more configurable logging to file\n\nSwitched to using log4j for logging (instead of integer debug\nlevels and printlns)\n\nTimestamp and thread ID are added to all log messages, enabling further\nlog analysis.\n\nAllows command line parameter -L to log to file.  In this case, only\nwarnings and errors are printed to the console.\n\nInstead of integer debug levels, use log4j's named levels (WARN, INFO,\nDEBUG, TRACE, etc).  Integer debug levels in config files still\nsupported\n\nAdd in option to compile with debug symbols with ant -Ddebug=true, so\nthat stacktraces are more usable.\n\nTask ID: #1143673\n\nBlame Rev:\n\nTest Plan: Ran short load and request benchmarks to completion, worked fine.\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D3855"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/21b33376676185c21993d4c5b24d5f9903cec9b8", "message": "Better error handling for configuration and other unrecoverable errors\n\nSummary:\nA few changes to reduce stacktrace spew and print more sensible error\nmessages for common errors\n\nBail out immediately if database connection can't be created, to provide\nbetter error reporting for config errors.\n\nBail out immediately if a thread throws an exception that it doesn't\ncatch itself, to avoid leaving the benchmark in the weird state where\nsome threads finish but don't.\n\nTask ID: #1144990\n\nBlame Rev:\n\nTest Plan:\nRun with bad config file, check concise error message\nRun with good config file, check that it still works\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D3831"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/e0802fb0c562faaa2c1255f797a61759bf056171", "message": "Cleanup of LinkBench CLI and instructions\n\nSummary:\nSwitch to using more standard CLI with command line switches, using\nApache Commons CLI.  This now gives us better error handling and command\nline help.\n\nFix minor bugs in run.sh and rename to linkbench.  Documentation now\nassumes this is the usual way to run LinkBench.\n\nTask ID: #1144990\nBlame Rev:\n\nTest Plan:\n// Check that this works from any directory\nexport PATH=/path/to/linkbench/bin:$PATH\ncd ~\n\n// This should print out usage information\nlinkbench\n\n// This should complain about bad arguments and print usage\nlinkbench -q\nlinkbench blah blah\n\n// Assuming database is setup, these should execute the loading and\n// requesting stages of the benchmark\nlinkbench -l\nlinkbench -r\n\n// This should do the requesting using an alternative config file\nlinkbench -r -c alternative.config\n\nRevert Plan:\nNot applicable\n\nReviewers: dhruba\n\nReviewed By: dhruba\n\nDifferential Revision: https://reviews.facebook.net/D3825"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/15912986", "body": "You're right, this is a little strange even though it works.  The proposed fix isn't quite right since it doesn't compile the tests. It should be easy to factor that out though - I'm taking a look now.\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/15912986/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/15963292", "body": "I should have known when @emayanke added the Maven Ant task that it was a Trojan horse to mavenise the whole project :).  I don't have any strong feelings about Maven vs Ant and it seems like so far that most people prefer Maven, so I'll test this locally and then merge it in.  Any objections? @dhruba ?\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/15963292/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/16296868", "body": "We moved to Maven, rendering this irrelevant.\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/16296868/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/17198517", "body": "Yup, the handling of version and time are inconsistent between addLink and deleteLink.  I'm going to fix the handling of both of these,\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/17198517/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/17489691", "body": "Fixed this with commit a3575dd808bbf175100d4e07a4a955f023061bea .  Thanks for the issue report.\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/17489691/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/17540664", "body": "I confirmed that they were switched, thanks for catching this.\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/17540664/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/18429759", "body": "The fix looks good: endTime was actually the time limit for the benchmark run, so doesn't reflect actual elapsed time.\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/18429759/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "colorant": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/a32e89587335e3d8866c57c53fc3c118c0b4ac5b", "message": "Minor fix of requester statistics"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/2687b73be773af471e55cdf4283e53edb66891c1", "message": "Fix genNode to fill version and time in right field."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bachmanm": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/2913f84996bdde772b20041ee099de7953695661", "message": "fixed final jar name"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/7a64659047320fde20269835a7eab00faf37f8d0", "message": "finished mavenizing the project, changed readme accordingly, made sure that final jar file is bundled with dependencies, so nothing else but that file needs to be on classpath"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/8f6a0a5114aa1394f985902ed33565de790d0111", "message": "include sources in distribution"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/0de85edad5ea2c946444a5a98c856c7fe7c0926c", "message": "mavenized the project"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/183f1f0e4d1879e274522f1a945ba24665dd5cd9", "message": "moved java sources to standard maven folder"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/48699bbaf0e3714a2717fe77d3951b6e480acccd", "message": "added gitignore"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/15900653", "body": "feel free to make it a separate branch btw, no Java files changed.\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/15900653/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "dhruba": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/e9c757302c0360d73bbab5cdac003451162b7bee", "message": "Description on running unit tests.\n\nSummary:\nDescription on running unit tests on MySQL.\n\nTest Plan:\n\nReviewers:\n\nCC:\n\nTask ID: #\n\nBlame Rev:"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/eb8400fa859d70d4a5b600555c538809fa264c30", "message": "Handle exceptions and errors better.\n\nSummary:\n1. There were places where exceptions were silenty eaten up. This\n   is a probem when debugging error cases.\n2. The System was not shutting down cleanly wen ti encountered\n   errors. The System.exit() hangs because it waits for an elegant\n   shutdown to occur, which involves all threads to voluntarily\n   exit. Instead, invoke Runtime.halt.\n3. The bin/linkench was not picking up the libraries stored in the\n   lib directory.\n\nTest Plan: Run linkbench.\n\nDifferential Revision: https://reviews.facebook.net/D8835"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/6675578c973025f1810c7c166f39233395c7aa06", "message": "Update the documentation to pick up the mysql library from\nthe lib directory\n\nSummary:\n\nTest Plan:\n\nReviewers:\n\nCC:\n\nTask ID: #\n\nBlame Rev:"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/437f6cde2a55a5aaa5a6ce87e6978ae9784e1385", "message": "Added hbase jar from apache hbase trunk version r1341265"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/7bd07e677aa56e767a0c5797332453a8562be3d8", "message": "Add support for code reviews"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/416e9bbf5f87573434763ff5706ddb3c7a65a69c", "message": "Initial code for Facebook LinkBench"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/15974231", "body": "Sounds good to me\n\nSent from my iPhone\n\nOn Apr 5, 2013, at 8:41 AM, Tim Armstrong notifications@github.com wrote:\n\n> I should have known when @emayanke added the Maven Ant task that it was a Trojan horse to mavenise the whole project :). I don't have any strong feelings about Maven vs Ant and it seems like so far that most people prefer Maven, so I'll test this locally and then merge it in. Any objections? @dhruba ?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/15974231/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "emayanke": {"issues": [], "commits": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/ae82ed7153458829c76a72caae06d9bcb0c22995", "message": "Remove line causing the database to close twice and fault\n\nSummary:\nRemoved linkStore.close line that was added before closeStores was introduced.\nThis is what was causing linkbench for rocksdb to not run successfully with multiple\nthreads. Now it runs for both load and request phases with multiple threads.\n\nTest Plan:\nant dist; load phase with loaders=10, request phase with requesters=100\nand number of requests=500000\n\nReviewers: dhruba, tarmstrong, vamsi, sheki\n\nReviewed By: sheki\n\nDifferential Revision: https://reviews.facebook.net/D8793"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/5872d41a87b6c248ede79d7806f693e2066bfc8f", "message": "Correct the loop in getLinkListImpl for rocksdb\n\nSummary: missed an i++\n\nTest Plan: ant clean;ant dist;<load> and <request> phases with large values\n\nReviewers: kosievdmerwe, tarmstrong\n\nReviewed By: kosievdmerwe\n\nDifferential Revision: https://reviews.facebook.net/D8589"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/70e5f4b8d866b294c6f4853f129b9db872ee264a", "message": "Closing the client in linkbench for rocksdb only when the last thread calls\n\nSummary: multiple close calls by differnet threads during the load and request phases were causing errors because all the threads use the same client in linkbench for rocksdb. Implemented 2 synchronized methods in LinkStoreRocksDb.java that let close go forward only if its the last thread\n\nTest Plan: ant clean;ant dist; Successful load and request phases\n\nReviewers: tarmstrong\n\nReviewed By: tarmstrong\n\nCC: dhruba, vamsi, kosievdmerwe, heyongqiang, sheki, leveldb\n\nDifferential Revision: https://reviews.facebook.net/D8535"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/3fe9d006e2a3c97bf1c503d12a471d9c7ce8320e", "message": "Use version 0.4.0 of open-source swift that eliminates the client-closing bugs that we were facing with linkbench\n\nSummary: we can simply use the version 0.4.0 of open-source swift that andrewcox pushed instead of using the special swift branch to get linkbench java client to close gracefully\n\nTest Plan: ant dist;<load> and <request> phases\n\nReviewers: tarmstrong, dhruba, kosievdmerwe, andrewcox\n\nReviewed By: tarmstrong\n\nDifferential Revision: https://reviews.facebook.net/D8505"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/commits/7481a67cfad296eed94bd93ac8d3511c566c536b", "message": "Adding assoc schema APIs for rocksdb so that linkbench can run rocksdb.\n\nSummary:\nLinkStoreRocksDb.java contains the assoc-schema APIs that enable\nlinkbench to be run over rocksdb. swift has been incorporated to make the thrift\ncalls using a java client. It uses netty and is faster. Am still working on it. Checking it in so that it can be tested by others\n\nTest Plan: ~/linkbench/bin/linkbench -c config/LinkConfigRocksDb.properties \u2013l; <Run the rocks server in fbcode simultaneously>\n\nReviewers: dhruba, tarmstrong\n\nReviewed By: tarmstrong\n\nCC: heyongqiang, sheki, zshao, kosievdmerwe, vamsi, andrewcox\n\nDifferential Revision: https://reviews.facebook.net/D8115"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "samE8": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/pulls/34", "title": "Update LinkStoreMysql.java", "body": "Allow usage with later mysql versions without needing to configure SSL:\n\"&useSSL=false\"\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sheki": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/15926844", "body": "This is great. @timarmstrong can you merge this some-how\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/15926844/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "jexp": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/15950664", "body": "+1 much appreciated @bachmanm \n\nHad to do a lot of fixing for maven deps and lost much time with that!\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/15950664/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "facebook-github-bot": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/108132050", "body": "Thank you for your pull request and welcome to our community. We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file. In order for us to review and merge your code, please sign up at https://code.facebook.com/cla - and if you have received this in error or have any questions, please drop us a line at cla@fb.com. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/108132050/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/108135334", "body": "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/108135334/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/173118345", "body": "Thank you for your pull request and welcome to our community. We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file. In order for us to review and merge your code, please sign up at https://code.facebook.com/cla - and if you have received this in error or have any questions, please drop us a line at cla@fb.com. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/facebookarchive/linkbench/issues/comments/173118345/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}}}}