{"_default": {"1": {"madprogrammer": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4496", "title": "dispatch/signal.py docs don't reflect real behaviour", "body": "## Checklist\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [X] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nSee https://github.com/celery/celery/blob/120770929f4a37c5373a378b75b5c41a99702af9/celery/utils/dispatch/signal.py#L253\r\n\r\n## Expected behavior\r\nDocumented behaviour matches actual behaviour -> change documentation\r\n\r\n## Actual behavior\r\nErrors thrown by signal receivers will not be propagated, unlike stated in current docs.\r\n\r\nPR will be submitted if needed.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4496/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "me115": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4495", "title": "celery quit immediately after starting", "body": "I use celery in django.  when I start the debug consumer process,  it quit immediately after starting.\r\n\r\nmy input command:\r\n`celery -A common worker --loglevel=DEBUG`\r\nps: common is my module name\r\n\r\noutput shows:\r\n\r\n> celery -A common worker --loglevel=DEBUG\r\n>  \r\n>  -------------- celery@n6-026-137 v4.0.2 (latentcall)\r\n> ---- **** ----- \r\n> --- * ***  * -- Linux-4.4.0-33.bm.1-amd64-x86_64-with-debian-8.8 2018-01-22 22:47:51\r\n> -- * - **** --- \r\n> - ** ---------- [config]\r\n> - ** ---------- .> app:         common:0x7f1029bc6590\r\n> - ** ---------- .> transport:   redis://10.14.23.45:3907/0\r\n> - ** ---------- .> results:     disabled://\r\n> - *** --- * --- .> concurrency: 32 (prefork)\r\n> -- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\r\n> --- ***** ----- \r\n>  -------------- [queues]\r\n>                 .> default_queue    exchange=cap_exchange(topic) key=default\r\n>                 .> workflow_queue   exchange=cap_exchange(topic) key=workflow\r\n> \r\n> [tasks]\r\n>   . celery.accumulate\r\n>   . celery.backend_cleanup\r\n>   . celery.chain\r\n>   . celery.chord\r\n>   . celery.chord_unlock\r\n>   . celery.chunks\r\n>   . celery.group\r\n>   . celery.map\r\n>   . celery.starmap\r\n>   . common.celery.debug_task\r\n>   . workflow_task\r\n> \r\n> [2018-01-22 22:47:52,873: INFO/MainProcess] Connected to redis://10.14.23.45:3907/0\r\n> [2018-01-22 22:47:52,878: INFO/MainProcess] mingle: searching for neighbors\r\n> [2018-01-22 22:47:53,890: INFO/MainProcess] mingle: all alone\r\n> **[2018-01-22 22:47:53,905: DEBUG/MainProcess] Canceling task consumer...**\r\n> [2018-01-22 22:47:54,946: DEBUG/MainProcess] Canceling task consumer...\r\n> [2018-01-22 22:47:54,946: DEBUG/MainProcess] Closing consumer channel...\r\n> [2018-01-22 22:47:54,947: DEBUG/MainProcess] removing tasks from inqueue until task handler finished\r\n> \r\n\r\nhere you see,  after connected to redis broken. it quit immediaterly without any log. I donn't know how to debug.\r\n\r\nmy  project dir:\r\nworkdir/\r\n.............../manage.py  (django manage.py file)\r\n............../common/celery.py\r\n............../common/settings.py (include celery.py)\r\n\r\ncelery.py 's content:\r\n```\r\nfrom __future__ import unicode_literals, print_function, absolute_import\r\nimport sys\r\nimport os\r\nfrom celery import Celery\r\nimport logging\r\n\r\nreload(sys)\r\nsys.setdefaultencoding('utf-8')\r\n\r\nlogger = logging.getLogger('celery')\r\n\r\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'common.settings')\r\napp = Celery('common')\r\napp.config_from_object('django.conf:settings')\r\napp.autodiscover_tasks()\r\n\r\n\r\n@app.task(bind=True)\r\ndef debug_task(self):\r\n    logger.error(\"!!!Debug Task!!!\")\r\n    logger.debug('Requests:{0!r}'.format(self.request))\r\n```\r\n\r\n\r\n## Checklist\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.1.0 py:2.7.9\r\n              billiard:3.5.0.3 redis:2.10.1\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:disabled\r\n\r\n\r\n\r\n## Steps to reproduce\r\n\r\n## Expected behavior\r\n\r\n## Actual behavior\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4495/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cfriver": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4494", "title": "(celery 4.1.0) -Ofair doesn't work", "body": "When 2 worker w-a and w-b are started, one second submission of the 2 task intervals of the t-a and t-b will have a probability of prefetching unfairness.\r\nFor example, w-a gets the t-a task and is executing, w-b is idle, and the t-b task is delivered at this time or will be allocated to w-a. This is not what I want, when w-b idle should be allocated to w-b. I tried to close the prefetch with -Ofair, but -Ofair did not work \r\n\r\ncelery version 4.1.0\r\nredis version 4.0.2\r\n\r\ncelery worker -A celery_worker -c 1 -l info -Ofair\r\nCELERYD_PREFETCH_MULTIPLIER = 1\r\nCELERY_TASK_ACKS_LATE = True\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4494/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "strawposter": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4492", "title": "separate setting different queues", "body": "Does celery support different settings for different queues through a single application config? For example, I want to have a different value for worker_prefetch_multiplier for queues of long tasks and queues of short tasks.\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4492/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/129431292", "body": "Hi, George\r\nThis code sample is intended when BROKER_URL == BACKEND_URL.\r\nI will agree that this is not flexible, but it is fixable. It does not matter.\r\nThis can be redesigned to receive exactly BACKEND_URL.\r\nThe important thing is that the celery bakend still does not support the sentinel.\r\nI also wrote that I would be grateful if someone would rewrite this code more correctly, but so far I have not seen a significant contribution.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/129431292/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/129435155", "body": "This code demonstrates an approach that is based on the following principles:\r\n- it is necessary to use a a child class that inherits from the RedisBackend;\r\n- it is necessary to use a kombu.Connection, because the whole backend uses kombu.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/129435155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "xunto": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4491", "title": "[BUG] ready() always returns False for groups", "body": "## Steps to reproduce\r\n\r\n```python\r\nsent = group(\r\n    celery_app.signature('task1', args=arg),\r\n    celery_app.signature('task2', args=arg),\r\n    celery_app.signature('task3', args=arg)\r\n).apply_async()\r\n\r\nwhile not sent.ready():\r\n    pass\r\n\r\nprint(\"test\")\r\n```\r\n\r\n## Expected behavior\r\nI expect ```ready()``` to return ```True``` when all tasks are finished.\r\n\r\n## Actual behavior\r\nLooks like this code will never finish execution as ```ready()``` always return ```False``` even if all tasks are finished. Tested on celery master branch.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4491/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nitinmeharia": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4489", "title": "django.db.utils.DatabaseError: DatabaseWrapper objects created in a thread can only be used in that same thread", "body": "### Application details:\r\n```\r\n    software -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.4\r\n            billiard:3.5.0.3 redis:2.10.6\r\n    platform -> system:Darwin arch:64bit imp:CPython\r\n    loader   -> celery.loaders.app.AppLoader\r\n    settings -> transport:redis results:redis://localhost:6379/2\r\n```\r\n\r\n### Error log\r\n```\r\n    Signal handler <bound method DjangoWorkerFixup.on_worker_process_init of <celery.fixups.django.DjangoWorkerFixup object at 0x108e46fd0>> raised: DatabaseError(\"DatabaseWrapper objects created in a thread can only be used in that same thread. The object with alias 'default' was created in thread id 140736440837056 and this is thread id 4441538184.\",)\r\n    Traceback (most recent call last):\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/celery/utils/dispatch/signal.py\", line 227, in send\r\n        response = receiver(signal=self, sender=sender, **named)\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/celery/fixups/django.py\", line 154, in on_worker_process_init\r\n        self._close_database()\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/celery/fixups/django.py\", line 186, in _close_database\r\n        conn.close()\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/django/db/backends/base/base.py\", line 283, in close\r\n        self.validate_thread_sharing()\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/django/db/backends/base/base.py\", line 542, in validate_thread_sharing\r\n        % (self.alias, self._thread_ident, thread.get_ident())\r\n    django.db.utils.DatabaseError: DatabaseWrapper objects created in a thread can only be used in that same thread. The object with alias 'default' was created in thread id 140736440837056 and this is thread id 4441538184.\r\n```\r\n\r\n## Steps to reproduce\r\n```\r\ncelery multi restart w1 -A proj -l info\r\n```\r\nIts a django 1.11 application where we are getting this error.\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4489/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Kurara": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4488", "title": "launching worker from python: error with configuration from object.", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nVersion celery: 4.1.0 (latentcall)\r\n\r\nI'm trying to launch a worker from python code. When I use the class CeleryCommand with 'worker' option, it works. But if I add the option '--detach' or 'multi' the broker configuration is wrong. The code is:\r\n\r\n```\r\napp.config_from_object(config_module)\r\ncelerycmd = CeleryCommand(app)\r\ncelerycmd.execute_from_commandline(argv=[prog_name, 'worker', 'app_name', pidfile, logfile, loglevel])\r\n```\r\nor (which is not working)\r\n\r\n`celerycmd.execute_from_commandline(argv=[prog_name, 'multi', 'start', 'app_name', pidfile, logfile, loglevel])`\r\n\r\n# Expected behavior\r\nGet the same broker as in the configuration, not the default one\r\n\r\n## Actual behavior\r\n\r\nlog_file:\r\n\r\n```\r\n[2018-01-15 15:08:58,471: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672//\r\n[2018-01-15 15:08:58,506: INFO/MainProcess] mingle: searching for neighbors\r\n[2018-01-15 15:08:59,647: INFO/MainProcess] mingle: all alone\r\n[2018-01-15 15:08:59,672: INFO/MainProcess] app_name@hostname ready.\r\n```\r\n\r\nFYI: I'm using lower case configuration. \r\n\r\nI could provide you what the object has in debug mode. Just ask me and I post it. When I debugged, I think the 'app' had the configuration of the file when it was inside the function _execute_from_commandline_, I can't understand when it loses it.\r\n\r\nI tought that maybe the problem is that I don't provide the configuration at the begining of the file, where app is declared, but in a function. So I tested to add the broker directly when I create the app: \r\n`app = Celery('app_name', broker_url='broker@...')`\r\nbut it didn't work either.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4488/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "MShekow": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4486", "title": "Memory hogging in client when using RPC (with RabbitMQ)", "body": "## Description\r\nIn my scenario I have a client program that puts thousands of tasks on the queue (`generate_data.delay()`). The workers produce a result that is of considerable size (suppose each result uses 1 MB of memory). The result is pickled back, and the client processes the results in some way, whenever results are available. In other words, once `AsyncResult.ready() == True`, I `get()` the result and do something with it.\r\nUsing `objgraph` I found out that celery never releases the result data.\r\n\r\n## My configuration\r\n```\r\nSoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.2\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Windows arch:32bit, WindowsPE imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:pyamqp results:rpc:///\r\nbroker_url: 'amqp://guest:********@localhost:5672//'\r\nresult_backend: 'rpc:///'\r\nresult_serializer: 'pickle'\r\ntask_serializer: 'pickle'\r\naccept_content: ['pickle']\r\n```\r\n\r\n\r\n## Steps to reproduce\r\nWorker program simply returns large objects, e.g.:\r\n```\r\nclass Data:\r\n    def __init__(self):\r\n        self.data = b'1' * 1024 * 1024\r\n\r\n@app.task\r\ndef generate_data() -> Data:\r\n    time.sleep(random.uniform(0, 0.2))\r\n    d = Data()\r\n    return d\r\n```\r\nThe client program retrieves results whenever they are available:\r\n```\r\nresults = {}\r\n\r\nfor i in range(2000):\r\n    async_result = generate_data.delay()\r\n    results[async_result] = True\r\n\r\nlogger.info(\"Put {} jobs on process queue!\".format(len(results)))\r\n\r\nresults_collected = 0\r\nwhile True:\r\n    # get results that are ready now\r\n    ready_results = [async_result for async_result, _ in results.items() if async_result.ready()]\r\n    if not ready_results:\r\n        time.sleep(10)\r\n        continue\r\n\r\n    results_collected += len(ready_results)\r\n\r\n    logger.info(\"Processing {} results. Got {} results so far\".format(len(ready_results), results_collected))\r\n    for ready_result in ready_results:\r\n        # we don't actually use the data - a real program would process the data somehow\r\n        result_data = ready_result.get()  # type: Data\r\n        del results[ready_result]\r\n\r\n    gc.collect()\r\n\r\n    # Exit loop once all results were processed:\r\n    if not results:\r\n        break\r\nlogger.info(\"Finished collecting all results\")\r\n```\r\n\r\n## Expected behavior\r\nWhen my client no longer has a reference to neither the actual data returned by `AsyncResult.get()`, nor the `AsyncResult` object itself, the memory of the data and the `AsyncResult` should be freed by celery.\r\n\r\n## Actual behavior\r\nMemory is being hogged so quickly that my client process (32-bit) dies soon. The reason is the huge size of `MESSAGE_BUFFER_MAX` (8192) and the oddly-hardcoded `bufmaxsize` (1000) in `BufferMap`. Are there any reasons for these huge buffers?", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4486/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thedrow": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4483", "title": "Add test coverage for #4356", "body": "#4356 is a critical fix for a bug that occurs when messages migrate between Celery 3 and Celery 4 clusters.\r\nDue to it's severity It was merged without proper test coverage.\r\nWe need to ensure this code is covered by the appropriate unit tests.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4434", "title": "Run the integration tests in a different build stage", "body": "We need to find a way to run the integration tests in a different build stage so that the unit tests will before them.\r\nThe unit tests are much quicker to execute and thus should run first to free up build resources for other contributors.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4423", "title": "Document that tasks are now documented automatically by celery", "body": "See https://github.com/celery/celery/pull/4422", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4423/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4407", "title": "Replace billiard with distex or rewrite billiard in a similar fashion", "body": "[distex](https://github.com/erdewit/distex) is a completely asynchronous process pool.\r\nGiven that Celery 5.0 will use the asyncio event loop as a core component an asynchronous process pool is crucial for us if the project were to succeed.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4407/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4214", "title": "Add an official Docker image", "body": "In order to complete #4213 we need an official Docker image to use when deploying to Kubernetes or elsewhere.\r\nThe official image will provide a base image for all users interested in deploying Celery using Docker.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4214/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4213", "title": "Add a Celery chart for Helm", "body": "Kubernetes is a very common deployment target these days.\r\nThe most common way to deploy an application to Kubernetes is using the Helm package manager.\r\nWe need to create a public chart and contribute it to the [main charts repository](https://github.com/kubernetes/charts).", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4213/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4183", "title": "Grant Releasse Access to all maintainers", "body": "We have some libraries that have not been updated such as django-celery-beat and django-celery-results.\r\nI don't have time to release them at the moment and I haven't tracked their status as of yet.\r\n\r\nIn order to prevent a situation where one person is the bottleneck for a release again we should: \r\n- [x] Grant @auvipy release rights to all celery related projects\r\n- [x] <s>Grant @georgepsarakis release rights to all celery related projects</s>\r\n- [ ] Grant me release rights to celery-sphinx-theme so I can provide the release rights to other current maintainers. @ask Can you please grant me the permissions to release that library.\r\n\r\nNote that in order to release you need to reuse the PGP key I provided in the previous package. Feel free to contact me for instructions.\r\n\r\nPlease provide your PyPi usernames as soon as possible.\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4152", "title": "Warn method is not defined in Manager", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n```\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.1.0 py:3.6.1\r\n            billiard:3.5.0.3 py-amqp:2.2.1\r\nplatform -> system:Linux arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:disabled\r\n```\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n```python\r\nfrom celery import Celery.\r\nfrom celery.contrib.testing.manager import Manager\r\n\r\napp = Celery()\r\nmanager = Manager(app)\r\n\r\ndef some_callback():\r\n  pass\r\n\r\nmanager.wait_for(some_callback, Exception, emit_warning=True)\r\n```\r\n\r\n## Expected behavior\r\nManager should print a warning when the condition has not been met yet.\r\n\r\n## Actual behavior\r\nAn `AttributeError` is raised since the `warn()` method is not defined.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4152/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/2668", "title": "Automatic detection of hanging workers", "body": "There's a way to detect hanging workers, even if it's unreliable, which will allow us to debug problems.\nWhat we can do is to gather in the main process how much time a task usually takes on average and  if a task takes longer than a configurable threshold above the average the main process will try to send a SIGUSR1 to the process that the task was dispatched to if it's available for the operating system.\nThe threshold should be configurable per task.\nIf the worker doesn't respond with some acknowledgement that a task was cancelled after reaching the task's hard limit we also need to notify a user for a possible hang.  \n\nThat means that we need to track to which child process are we sending the task to.\nThe average processing time of a task can also be a nice feature for celery inspect. \n\nAs far as implementation goes this could be implemented easily for the prefork pool in the main process using threads in the following fashion:\n\n``` python\ndef detect_hang(task, source):\n  time.sleep(task.max_hang_threshold)\n  if not task.completed():\n    os.kill(source.pid, signals.SIGUSR1)\n```\n\nFor other pool types we can fork and have another process do the same as the implementation above.\n@ask @malinoff @PMickael What do you guys think?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/2668/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/2547666a1ea13b27bc13ef296ae43a163ecd4ab3", "message": "Don't cover this branch as well."}, {"url": "https://api.github.com/repos/celery/celery/commits/3af6a635cf90a4452db4e87c2326579ebad750c2", "message": "Merge branch 'master' into master"}, {"url": "https://api.github.com/repos/celery/celery/commits/bd0ed23c81b20fd75c0e2188fcf78e4d74898953", "message": "Use editable installs to measure code coverage correctly."}, {"url": "https://api.github.com/repos/celery/celery/commits/0a0fc0fbf698d30e9b6be29661e0d447548cc47c", "message": "Report coverage to terminal as well."}, {"url": "https://api.github.com/repos/celery/celery/commits/4b4bf2a4eee65871d3e9c0e96d94b23aa2ee602c", "message": "Report coverage correctly (#4445)\n\n* Report coverage correctly.\r\n\r\nAs it turns out this repository does not report coverage to codecov at all since the path to the executables has changed at some point.\r\nThis should fix the problem.\r\n\r\n* Readd code coverage badge."}, {"url": "https://api.github.com/repos/celery/celery/commits/dbd59d9fc988ad0f04ae710c32066c5ff62374ef", "message": "Added bandit to the build matrix."}, {"url": "https://api.github.com/repos/celery/celery/commits/2ae00362179897968cfbd1fc8d61b648356769a7", "message": "Added bandit to lint for security issues."}, {"url": "https://api.github.com/repos/celery/celery/commits/56b94c327244fad4933706fbddc02eeea508d21a", "message": "Prettify test output."}, {"url": "https://api.github.com/repos/celery/celery/commits/ebd98fa4d36bb8003c2f46dbd16e9888af13720f", "message": "Parallel doc lints (#4435)\n\n* Bump sphinx.\r\n\r\n* Update copyright year. Mark the celerydocs & the celery.contrib.sphinx extensions as read_parallel_safe.\r\n\r\n* Install from git for now :(\r\n\r\n* Fix flake8 errors."}, {"url": "https://api.github.com/repos/celery/celery/commits/83cecb8ad15881dbe0e6f05c1b568088ccf0e41a", "message": "Add couchbase dependency to CI + Python 3/PyPy fixes (#4313)\n\n* Add couchbase dependency to CI.\r\n\r\n* Install libcouchbase-dev.\r\n\r\n* Remove obsolete test.\r\n\r\n* Don't cover cffi branch.\r\n\r\n* Ignore unused import since it monkeypatches the couchbase client.\r\n\r\n* Use https when pulling the couchbase key.\r\n\r\n* Install couchbase-cffi only on PyPy.\r\n\r\n* Fix flake8 (again?)"}, {"url": "https://api.github.com/repos/celery/celery/commits/7e87cb1dbf02b1798384cc35918dea63ef5bf08d", "message": "Fix couchdb keys (#4166)\n\n* Fix couchdb keys problem on Python 3. Addresses code review in #3861.\r\n\r\n* isort.\r\n\r\n* fix build failure."}, {"url": "https://api.github.com/repos/celery/celery/commits/06cc3ca91d40f63d3d77a8517e84d1de5479460d", "message": "Ignore localhost for linkcheck."}, {"url": "https://api.github.com/repos/celery/celery/commits/86d3a8ea6898484c73f8f6b971ca1c4ed29e617e", "message": "Run documentation lints with two jobs."}, {"url": "https://api.github.com/repos/celery/celery/commits/f6134367080fb1cdb3a7dc5383862c67519509c4", "message": "Fix formatting."}, {"url": "https://api.github.com/repos/celery/celery/commits/0de2dc81481754366d006160df13d1baaab813fb", "message": "Added an integration test for #4427 (#4428)\n\n* Added an integration test for #4427.\r\n\r\n* Skip if not redis.\r\n\r\n* fixup! Skip if not redis."}, {"url": "https://api.github.com/repos/celery/celery/commits/28c2c09d380c62f9e17776811735a5c8c4ed8320", "message": "Introduced Build Stages to our build process (#4429)\n\n* Move lint to another build stage.\r\n\r\n* Lint first, test later.\r\n\r\n* Run flake8 with two jobs."}, {"url": "https://api.github.com/repos/celery/celery/commits/acf850643d0c0e0e140b43ea68c9a6141f4752f8", "message": "Downgrade pytest to a version below 3.3 since the new log capture feature breaks our tests."}, {"url": "https://api.github.com/repos/celery/celery/commits/3b330d975dc84e236a3539b75a21f5c1f9b22b5f", "message": "Added a test to verify that second order replace works as expected. Fixes #3116."}, {"url": "https://api.github.com/repos/celery/celery/commits/120770929f4a37c5373a378b75b5c41a99702af9", "message": "Install isort from git. Fix import order."}, {"url": "https://api.github.com/repos/celery/celery/commits/7c86c96185e358f572fe3aa6ab14c995a47f22b8", "message": "Install all dependencies so isort will work correctly."}, {"url": "https://api.github.com/repos/celery/celery/commits/2ef6845336fea79039e248701af03a9b08fa53e7", "message": "Fix import sorting."}, {"url": "https://api.github.com/repos/celery/celery/commits/f148709062f728104dcca32d475d11af4a496be3", "message": "Fix append to an empty chain. Fixes #4047. (#4402)\n\nThis bug is a regression from Celery 3.x. A test was added to ensure no further regressions will occur."}, {"url": "https://api.github.com/repos/celery/celery/commits/cd534f50bf628c855b1cb58317cc4890a52cbd27", "message": "Fix lint errors."}, {"url": "https://api.github.com/repos/celery/celery/commits/a3c377474ab1109a26de5169066a4fae0d30524b", "message": "Add to build matrix."}, {"url": "https://api.github.com/repos/celery/celery/commits/d0f5300691ca594f2311daf542aa63367622c027", "message": "More isort fixes. Added tox task."}, {"url": "https://api.github.com/repos/celery/celery/commits/e6a6cced7f8bd7d6cba21374653aebad4f8fc65e", "message": "isort."}, {"url": "https://api.github.com/repos/celery/celery/commits/5867895d6eb162066dc8b4a188e36345016745ff", "message": "Fix the rest of the flake8 errors."}, {"url": "https://api.github.com/repos/celery/celery/commits/b2355463366cc30ba5af518e1587fefb87f30c28", "message": "Fix flake8 errors."}, {"url": "https://api.github.com/repos/celery/celery/commits/ae5190d40a78f00ab658f39ff2b9ea3ae5ad3d31", "message": "autopep8 on celery."}, {"url": "https://api.github.com/repos/celery/celery/commits/4ce1f7f4d081c78e8cc5fef291ba6fa76d7c5ebe", "message": "autopep8 on unit tests."}, {"url": "https://api.github.com/repos/celery/celery/commits/0997e377d9e4da261841dc4af6a08e6d77eca4d4", "message": "Ignore new flake8 errors."}, {"url": "https://api.github.com/repos/celery/celery/commits/d59518f5fb68957b2d179aa572af6f58cd02de40", "message": "Make __all__ immutable. (#4315)"}, {"url": "https://api.github.com/repos/celery/celery/commits/fe03eaeebd13e03693b5aaa61eb795518f4520a5", "message": "Report coverage for all result backends."}, {"url": "https://api.github.com/repos/celery/celery/commits/5af199c94f3bf08dfaa93c36c83c2147b5f15639", "message": "Bump version: 4.0.2 \u2192 4.1.0"}, {"url": "https://api.github.com/repos/celery/celery/commits/1ac91af13404cab80c7afc78da2669fce5dee6d7", "message": "Bump PyPy version in build to 5.8.0 (#4128)\n\n* Bump PyPy to 5.8.0.\r\n\r\n* Test with PyPy 5.3.1 as well.\r\n\r\n* Fix bash script.\r\n\r\n* Minor corrections in Travis CI PyPy section\r\n\r\n* Detect PyPy environment\r\n\r\n* Fix Bash string matching for PyPy environment\r\n\r\n* Use sudo when running apt"}, {"url": "https://api.github.com/repos/celery/celery/commits/97ec0bfe35e3ab1c9f8d35291b00b5fe07985a3e", "message": "Update before installing dependencies."}, {"url": "https://api.github.com/repos/celery/celery/commits/93e6049d4aa6648c5d81a05776d13dbc2e6b25bb", "message": "Retry downloading dynamodb if it fails."}, {"url": "https://api.github.com/repos/celery/celery/commits/8d00f9d1d299f8d3a2ab3e4919ff13496d5d6e9f", "message": "Upgrade build environment to trusty."}, {"url": "https://api.github.com/repos/celery/celery/commits/889d1a83ed639deefcb0f37801a5f10be4c7c2aa", "message": "Revert \"Fix setting name for task_eager_propagates\" (#3880)"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4268", "title": "Add Var.CI integration", "body": "This pull request aims to demonstrate the power of [VarCI](https://var.ci/), the missing assistant for GitHub issues.\n\n--\n_Automated response by [Var.CI](https://var.ci)_ :robot:", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/3982", "title": "Added failing test cases for #3885", "body": "The tests were contributed by @robpogorzelski\r\n\r\n*Note*: Before submitting this pull request, please review our [contributing\r\nguidelines](https://docs.celeryproject.org/en/master/contributing.html).\r\n\r\n## Description\r\n\r\nAttempt to fix #3885 without hurting eager execution of the canvas.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/12121865", "body": "@PMickael Please do it as soon as possible since you're blocking merging some of our PRs.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/12121865/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/12148694", "body": "Oh, I'll fix that.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/12148694/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/12148724", "body": "Looks like @ask fixed it without telling anyone :P https://github.com/celery/celery/commit/5b025713018ad0b86619c84f221f8f54ea2c711d\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/12148724/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/12171598", "body": "@ask We use eager tasks sometimes in our celery code.\nLet's not introduce new bugs.\nHow do we fix this?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/12171598/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/12237094", "body": "Let's open an issue about it to ensure we don't forget to deal with this.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/12237094/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/17874041", "body": "Why did you do that? Does it make the build slower?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/17874041/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/17944899", "body": "Why did we have that then? Wouldn't the GC release _cache automatically when it's no longer in use?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/17944899/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/18035546", "body": "We need dedicated maintainers. Ones that are very familiar with Redis internals.\nIt's not a matter of how many stale tickets we have.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18035546/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/18105620", "body": "That's possible.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18105620/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/20573041", "body": "I'm working on a PR right now and the build is too slow. Can we enable it back please? We can manually invalidate the cache if there's a problem with a specific build but they don't happen often.", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/20573041/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/24824340", "body": "We want to merge a few more fixes first.", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/24824340/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/24836539", "body": "We want to release every bugfix but the test suite and amount of issues we have to handle is enormous.", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/24836539/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/26233445", "body": "I was sure I did that. Thank you :)", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/26233445/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/13642552", "body": "Actually, `q = getattr(obj, '__qualname__', obj.__name__)` looks more efficient. \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/13642552/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/13642641", "body": "``` python\ntry:\n  stack = self._local.stack\n  return stack\nexcept AttributeError:\n  return []\n```\n\nIs a bit more idiomatic and it does not create a new list when it is not needed.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/13642641/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/32373503", "body": "I understand why you are doing this but I'm really not 100% sure that is will be performant when using PyPy.  \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/32373503/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/42311163", "body": "I'm not sure.\n@ask @malinoff ?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/42311163/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/33041395", "body": "~~This should probably be more generic.\nI don't think that merging something that is specific to mongo exceptions is useful.~~\nOh this is just for the mongo backend.\nI thought it's for tasks. I will review later on in full detail. Sorry about that.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/33041395/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/94314849", "body": "Is this test still flaky?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/94314849/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/94314857", "body": "Is this test still flaky?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/94314857/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/94762477", "body": "What happens when the module has side effects? You'll get a remote code execution vulnerability if you can inject a result somehow.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/94762477/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110514182", "body": "Any reason why https://docs.python.org/2/library/functions.html#callable wasn't used?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110514182/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110535071", "body": "I don't think we care about old style classes or rather we won't care soon. Let's open an issue about it for Celery 5.x.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110535071/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110514082", "body": "I think entry is a better name than model. Models are specific to ORMs.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110514082/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110514122", "body": "There's no need for a separate if statement here. Use `or` instead.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110514122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110514143", "body": "Let's use more meaningful names here. old_schedule and new_schedule works imo.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110514143/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110535117", "body": "It looks like it's a better idea to extract the additional condition outside of the comprehension.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110535117/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/111665717", "body": "I don't think we need to change this file.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/111665717/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/111665730", "body": "While we're at it can you update the link? https://bitbucket.org/micktwomey/pyiso8601", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/111665730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/119980878", "body": "mul means multiply yes and what do you mean by t0?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/119980878/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/120007584", "body": "`si()` the same as `s()` just immutable.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/120007584/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108028615", "body": "Yes, that is resolved there.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108028615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/111389346", "body": "Also, why isn't this a dictionary comprehension? We don't support Python 2.6 anymore. ", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/111389346/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128693698", "body": "flake8 is complaining about a missing space after the `,`.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128693698/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128846093", "body": "flake8 is complaining again because of line length.\r\nPlease extract the heartbeat result to a variable.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128846093/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128891922", "body": "I have no idea. You'll have to ask @ask ", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128891922/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128891923", "body": "What do you mean by not reviewed?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128891923/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128916260", "body": "Shouldn't we update this document as well?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128916260/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128921848", "body": "But we need to create changelog.rst as well right?\r\nBefore we merge, let's make a readthedocs build to verify that this branch produces the correct results.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128921848/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128958980", "body": "https://github.com/celery/celery/blob/master/docs/changelog.rst includes Changelog.\r\nNevermind...", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128958980/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890816", "body": "flake8 is complaining about the lack of spaces between `-`", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890816/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890819", "body": "flake8 is complaining about the lack of spaces between `-`", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890819/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/129786818", "body": "What happens if SO_REUSEADDR is not supported on the system you are using?\r\nSO_REUSEADDR was added in Linux 3.9 which is fairly recent. Also, it might not be supported on other systems as well.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/129786818/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/129898963", "body": "This is what happens if you just copy/paste without reading... :blush: ", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/129898963/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/130019489", "body": "why are we importing stuff here?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/130019489/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133640515", "body": "Please provide code examples for how to override the request.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133640515/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132024972", "body": "Pretty sure this doesn't belong here since tornado workers pool should be optional.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132024972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133134569", "body": "So this PR is no longer relevant?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133134569/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133979685", "body": "What happens if tornado is installed but not used as the workers pool?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133979685/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132823718", "body": "This dependency is not needed in Python 3.x.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132823718/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132823760", "body": "You don't need this code anymore.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132823760/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132823777", "body": "`now` should be equal to the diff between the time the LimitedSet was initialized and the current value.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132823777/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132844948", "body": "From the documentation:\r\n> Return the value (in fractional seconds) of a monotonic clock, i.e. a clock that cannot go backwards. The clock is not affected by system clock updates. The reference point of the returned value is undefined, so that only the difference between the results of consecutive calls is valid.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132844948/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132844985", "body": "@caronc Have you read the comment about this test?\r\nThis is no longer the case with the monotonic clock.\r\nWe probably need to adjust this test to use the real clock.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132844985/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/143326018", "body": "So after re-reading the test I think it is necessary to ensure ordering.\r\nYou can just remove the comment.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/143326018/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/143346436", "body": "We need to address this before merging", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/143346436/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/149594219", "body": "Use a tuple instead of a list since this is a constant.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/149594219/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "PromyLOPh": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4480", "title": "Application is not thread-safe", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n```\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.5.4\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Linux arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:rpc:///\r\n\r\nresult_backend: 'rpc:///'\r\nbroker_url: 'amqp://guest:********@localhost:5672//'\r\n```\r\n\r\n## Steps to reproduce\r\n\r\nUsing code from the user manual, but two threads running concurrently:\r\n\r\n```python\r\nfrom celery import Celery\r\nimport time\r\nfrom threading import Thread\r\n\r\napp = Celery('test', broker='amqp://guest@localhost//', backend='rpc://')\r\n\r\n@app.task(bind=True)\r\ndef hello(self, a, b):\r\n    time.sleep(1)\r\n    self.update_state(state=\"PROGRESS\", meta={'progress': 50})\r\n    time.sleep(1)\r\n    self.update_state(state=\"PROGRESS\", meta={'progress': 90})\r\n    time.sleep(1)\r\n    return 'hello world: %i' % (a+b)\r\n\r\nif __name__ == '__main__':\r\n    def run ():\r\n        handle = hello.delay (1, 2)\r\n        print (handle.get ())\r\n    t1 = Thread (target=run)\r\n    t2 = Thread (target=run)\r\n    t1.start ()\r\n    t2.start ()\r\n    t1.join ()\r\n    t2.join ()\r\n```\r\n\r\n## Expected behavior\r\n\r\n```\r\nhello world: 3\r\nhello world: 3\r\n```\r\n\r\n## Actual behavior\r\n\r\nDifferent exceptions, depending on timing. For instance:\r\n\r\n```\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 154, in add_pending_result\r\n    self._maybe_resolve_from_buffer(result)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 160, in _maybe_resolve_from_buffer\r\n    result._maybe_set_cache(self._pending_messages.take(result.id))\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/utils/collections.py\", line 871, in take\r\n    raise self.Empty()\r\nqueue.Empty\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 456, in channel\r\n    return self.channels[channel_id]\r\nKeyError: None\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib64/python3.5/threading.py\", line 862, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"test.py\", line 19, in run\r\n    print (handle.get ())\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/result.py\", line 193, in get\r\n    self.backend.add_pending_result(self)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 156, in add_pending_result\r\n    self._add_pending_result(result.id, result, weak=weak)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 166, in _add_pending_result\r\n    self.result_consumer.consume_from(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 81, in consume_from\r\n    return self.start(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 56, in start\r\n    self._connection.default_channel, [initial_queue],\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/connection.py\", line 821, in default_channel\r\n    self._default_channel = self.channel()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/connection.py\", line 266, in channel\r\n    chan = self.transport.create_channel(self.connection)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/transport/pyamqp.py\", line 100, in create_channel\r\n    return connection.channel()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 459, in channel\r\n    channel.open()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/channel.py\", line 432, in open\r\n    spec.Channel.Open, 's', ('',), wait=spec.Channel.OpenOk,\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 59, in send_method\r\n    return self.wait(wait, returns_tuple=returns_tuple)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 79, in wait\r\n    self.connection.drain_events(timeout=timeout)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 468, in drain_events\r\n    while not self.blocking_read(timeout):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 473, in blocking_read\r\n    frame = self.transport.read_frame()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/transport.py\", line 252, in read_frame\r\n    payload = read(size)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/transport.py\", line 417, in _read\r\n    s = recv(n - len(rbuf))\r\nsocket.timeout: timed out\r\n```\r\nor\r\n\r\n```\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 154, in add_pending_result\r\n    self._maybe_resolve_from_buffer(result)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 160, in _maybe_resolve_from_buffer\r\n    result._maybe_set_cache(self._pending_messages.take(result.id))\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/utils/collections.py\", line 871, in take\r\n    raise self.Empty()\r\nqueue.Empty\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib64/python3.5/threading.py\", line 862, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"test.py\", line 19, in run\r\n    print (handle.get ())\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/result.py\", line 193, in get\r\n    self.backend.add_pending_result(self)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 156, in add_pending_result\r\n    self._add_pending_result(result.id, result, weak=weak)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 166, in _add_pending_result\r\n    self.result_consumer.consume_from(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 81, in consume_from\r\n    return self.start(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 59, in start\r\n    self._consumer.consume()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/messaging.py\", line 477, in consume\r\n    self._basic_consume(T, no_ack=no_ack, nowait=False)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/messaging.py\", line 598, in _basic_consume\r\n    no_ack=no_ack, nowait=nowait)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/entity.py\", line 737, in consume\r\n    arguments=self.consumer_arguments)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/channel.py\", line 1564, in basic_consume\r\n    wait=None if nowait else spec.Basic.ConsumeOk,\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 59, in send_method\r\n    return self.wait(wait, returns_tuple=returns_tuple)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 79, in wait\r\n    self.connection.drain_events(timeout=timeout)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 471, in drain_events\r\n    while not self.blocking_read(timeout):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 476, in blocking_read\r\n    frame = self.transport.read_frame()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/transport.py\", line 254, in read_frame\r\n    'Received {0:#04x} while expecting 0xce'.format(ch))\r\namqp.exceptions.UnexpectedFrame: Received 0x3c while expecting 0xce\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kzidane": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4471", "title": "DisabledBackend when starting flask --with-threads?", "body": "I'm trying to use Celery for one of my applications and experiencing a strange behavior that I'm not sure why it's caused.\r\n\r\nTo replicate, here's a simple Flask app:\r\n\r\n    # application.py\r\n    from celery.contrib.abortable import AbortableAsyncResult\r\n    from flask import Flask\r\n    from tasks import add\r\n\r\n    app = Flask(__name__)\r\n\r\n    @app.route(\"/\")\r\n    def index():\r\n        # start the task and return its id\r\n        return add.delay(42, 50).task_id\r\n\r\n\r\n    @app.route(\"/state/<task_id>\")\r\n    def result(task_id):\r\n        # return current task state\r\n        return AbortableAsyncResult(task_id).state\r\n\r\nand here's a Celery app and a task:\r\n\r\n    # tasks.py\r\n    from celery import Celery\r\n    from celery.contrib.abortable import AbortableTask\r\n\r\n\r\n    app = Celery(\r\n        \"tasks\",\r\n        # use sqlite database as result backend (also tried rpc://)\r\n        backend=\"db+sqlite:///celerydb.sqlite\",\r\n        broker=\"pyamqp://localhost\"\r\n    )\r\n\r\n    @app.task(bind=True, base=AbortableTask)\r\n    def add(self, x, y):\r\n        return x + y\r\n\r\nRunning the Celery worker:\r\n\r\n    $ celery -A tasks worker --loglevel=info\r\n     -------------- celery@7677a80760b4 v4.1.0 (latentcall)\r\n    ---- **** ----- \r\n    --- * ***  * -- Linux-4.10.0-42-generic-x86_64-with-debian-jessie-sid 2018-01-02 20:22:48\r\n    -- * - **** --- \r\n    - ** ---------- [config]\r\n    - ** ---------- .> app:         tasks:0x7f339a52dfd0\r\n    - ** ---------- .> transport:   amqp://guest:**@localhost:5672//\r\n    - ** ---------- .> results:     sqlite:///celerydb.sqlite\r\n    - *** --- * --- .> concurrency: 8 (prefork)\r\n    -- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\r\n    --- ***** ----- \r\n     -------------- [queues]\r\n                .> celery           exchange=celery(direct) key=celery\r\n                \r\n\r\n    [tasks]\r\n      . tasks.add\r\n\r\n    [2018-01-02 20:22:48,585: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672//\r\n    [2018-01-02 20:22:48,592: INFO/MainProcess] mingle: searching for neighbors\r\n    [2018-01-02 20:22:49,608: INFO/MainProcess] mingle: all alone\r\n    [2018-01-02 20:22:49,636: INFO/MainProcess] celery@7677a80760b4 ready.\r\n\r\n\r\nRunning the Flask app:\r\n\r\n    $ FLASK_APP=application.py flask run --with-threads\r\n      * Serving Flask app \"application\"\r\n      * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\r\n\r\n\r\nHitting `/` with `curl` starts the task and returns its id without any problems:\r\n\r\n    $ curl http://localhost:5000\r\n    f1b752a1-c9c0-4a81-857d-9ecbdd4004ef\r\n\r\n\r\nCelery's output at this point:\r\n\r\n    [2018-01-02 20:29:28,974: INFO/MainProcess] Received task: tasks.add[f1b752a1-c9c0-4a81-857d-9ecbdd4004ef]\r\n    [2018-01-02 20:29:29,000: INFO/ForkPoolWorker-1] Task tasks.add[f1b752a1-c9c0-4a81-857d-9ecbdd4004ef] succeeded in 0.02414485500776209s: 92\r\n\r\nBut trying to get the state of the task \r\n\r\n    $ curl http://localhost:5000/state/f1b752a1-c9c0-4a81-857d-9ecbdd4004ef\r\n\r\n\r\nresults in the following error:\r\n\r\n    AttributeError: 'DisabledBackend' object has no attribute '_get_task_meta_for'\r\n\r\neven though the backend seems to be configured per the `backend` argument to `Celery` and its output? I also tried setting `CELERY_RESULT_BACKEND` and `result_backend` using `app.conf.update`, but no luck!\r\n\r\nWhat's interesting is that this problem disappears if I drop the `--with-threads` option from the `flask run` command. Any idea why this might be caused and how to work around it if possible?\r\n\r\nAdditional details:\r\n\r\n    $ celery --version\r\n    4.1.0 (latentcall)\r\n    $ flask --version\r\n    Flask 0.12.2\r\n    Python 3.6.0 (default, Oct 30 2017, 05:46:44) \r\n    [GCC 4.8.4]\r\n\r\nFull traceback:\r\n\r\n    Traceback (most recent call last):\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1982, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1614, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1517, in handle_user_exception\r\n    reraise(exc_type, exc_value, tb)\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/_compat.py\", line 33, in reraise\r\n    raise value\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1612, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1598, in dispatch_request\r\n    return self.view_functions[rule.endpoint](**req.view_args)\r\n      File \"/root/application.py\", line 14, in result\r\n    return AbortableAsyncResult(task_id).state\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/celery/result.py\", line 436, in state\r\n    return self._get_task_meta()['status']\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/celery/result.py\", line 375, in _get_task_meta\r\n    return self._maybe_set_cache(self.backend.get_task_meta(self.id))\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/celery/backends/base.py\", line 352, in get_task_meta\r\n    meta = self._get_task_meta_for(task_id)\r\n    AttributeError: 'DisabledBackend' object has no attribute '_get_task_meta_for'\r\n    127.0.0.1 - - [02/Jan/2018 20:46:28] \"GET /state/f1b752a1-c9c0-4a81-857d-9ecbdd4004ef HTTP/1.1\" 500 -\r\n\r\nThank you!", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "keaneokelley": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4470", "title": "Call to ResultSet.get() hangs indefinitely even though all tasks in set have completed", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.   \r\n\r\n> software -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.4\r\n>             billiard:3.5.0.3 redis:2.10.6\r\n> platform -> system:Linux arch:64bit, ELF imp:CPython\r\n> loader   -> celery.loaders.app.AppLoader\r\n> settings -> transport:redis results:redis:///\r\n> \r\n> broker_url: 'redis://localhost:6379//'\r\n> result_backend: 'redis:///'\r\n> \r\n\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nLooks very similar to https://github.com/celery/celery/issues/2672\r\n\r\nConsider the following code, an oversimplification of what I am trying to do in a real application:\r\n```python\r\nfrom celery import Celery\r\nfrom celery.result import ResultSet\r\n\r\ncelery = Celery('celtest', backend='redis://', broker='redis://')\r\n\r\n\r\n@celery.task(bind=True)\r\ndef test_task(self, a: int, b: int):\r\n    self.update_state(state='RESULTS', meta={'test': a + b})\r\n    return a + b\r\n\r\n\r\ndef callback(data):\r\n    print(data)\r\n\r\n\r\ndef launch_many_tasks():\r\n    r = ResultSet([])\r\n    for i in range(10):\r\n        r.add(test_task.delay(i, i + 1))\r\n    r.get(on_message=callback)\r\n    print(\"Done!\")\r\n\r\n\r\nif __name__ == '__main__':\r\n    launch_many_tasks()\r\n```\r\n\r\n## Expected behavior\r\nThe application will print out the results of each task (in the callback function), print \"Done!\", and exit.\r\n## Actual behavior\r\nThe results of each task are printed out in the callback function. The execution hangs at `r.get()`, with or without the `on_message` argument. The Celery console shows that 10 tasks were started and completed successfully. Using Flower, it shows that all tasks are completed, nothing is waiting or in a queue somewhere.\r\n\r\nThis traceback was obtained after a SIGINT, perhaps indicating where things are hung:\r\n```\r\nTraceback (most recent call last):\r\n  File \"celtest.py\", line 27, in <module>\r\n    launch_many_tasks()\r\n  File \"celtest.py\", line 22, in launch_many_tasks\r\n    r.get(on_message=callback)\r\n  File \"/usr/local/lib/python3.6/dist-packages/celery-4.1.0-py3.6.egg/celery/result.py\", line 644, in get\r\n    on_interval=on_interval,\r\n  File \"/usr/local/lib/python3.6/dist-packages/celery-4.1.0-py3.6.egg/celery/result.py\", line 762, in join_native\r\n    on_message, on_interval):\r\n  File \"/usr/local/lib/python3.6/dist-packages/celery-4.1.0-py3.6.egg/celery/backends/async.py\", line 142, in iter_native\r\n    for _ in self._wait_for_pending(result, no_ack=no_ack, **kwargs):\r\n  File \"/usr/local/lib/python3.6/dist-packages/celery-4.1.0-py3.6.egg/celery/backends/async.py\", line 255, in _wait_for_pending\r\n    on_interval=on_interval):\r\n  File \"/usr/local/lib/python3.6/dist-packages/celery-4.1.0-py3.6.egg/celery/backends/async.py\", line 56, in drain_events_until\r\n    yield self.wait_for(p, wait, timeout=1)\r\n  File \"/usr/local/lib/python3.6/dist-packages/celery-4.1.0-py3.6.egg/celery/backends/async.py\", line 65, in wait_for\r\n    wait(timeout=timeout)\r\n  File \"/usr/local/lib/python3.6/dist-packages/celery-4.1.0-py3.6.egg/celery/backends/redis.py\", line 77, in drain_events\r\n    m = self._pubsub.get_message(timeout=timeout)\r\n  File \"/usr/local/lib/python3.6/dist-packages/redis/client.py\", line 2513, in get_message\r\n    response = self.parse_response(block=False, timeout=timeout)\r\n  File \"/usr/local/lib/python3.6/dist-packages/redis/client.py\", line 2428, in parse_response\r\n    if not block and not connection.can_read(timeout=timeout):\r\n  File \"/usr/local/lib/python3.6/dist-packages/redis/connection.py\", line 619, in can_read\r\n    bool(select([sock], [], [], timeout)[0])\r\nKeyboardInterrupt\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4470/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "fnordian": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4465", "title": "Celery.close() leaks redis connections", "body": "## Checklist\r\n\r\n- [X] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [X] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.3\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Linux arch:64bit imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\n\r\n```python\r\nimport time\r\nfrom celery import Celery\r\n\r\ndef run_celery_task(taskname):\r\n    with Celery(broker='redis://redis:6379/0', backend='redis://redis:6379/0') as celery:\r\n        res = celery.send_task(taskname)\r\n        print(res)\r\n\r\nfor i in range(0, 100):\r\n    run_celery_task(\"test\")\r\n\r\ntime.sleep(100)\r\n```\r\n\r\n```bash\r\nnetstat -tn | grep 6379 | grep ESTABLISHED | wc -l\r\n```\r\n## Expected behavior\r\n\r\nWhen the `with Celery`-block terminates, I expect all connections to redis being closed.\r\n\r\n## Actual behavior\r\n\r\nNot all connections are closed. When the for-loop finishes, there > 100 open connections to redis.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4465/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Kimice": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4464", "title": "Maximum recursion depth exceeded while calling a Python object", "body": "Hi, when I call celery apply_async function, sometimes it raise Exception like this. It seems like getting config failed. This bug can't always reappear. I guess celery may be not init correctly. I'm so confused with this bug.\r\n\r\ncelery==4.1.0\r\n\r\nI init celery with flask like this.\r\n\r\n```\r\ndef make_celery(app):\r\n    celery = Celery(app.import_name, backend=app.config['CELERY_RESULT_BACKEND'],\r\n                    broker=app.config['CELERY_BROKER_URL'])\r\n    celery.conf.update(app.config)\r\n    celery.config_from_object('App.celery_custom.celery_config')\r\n    TaskBase = celery.Task\r\n    class ContextTask(TaskBase):\r\n        abstract = True\r\n        def __call__(self, *args, **kwargs):\r\n            with app.app_context():\r\n                return TaskBase.__call__(self, *args, **kwargs)\r\n    celery.Task = ContextTask\r\n    return celery\r\n\r\ncelery_app = make_celery(flask_app)\r\n\r\n@celery_app.task(bind=True)\r\ndef checkInstance(self, a, b):\r\n    pass\r\n\r\ncheck_task = checkInstance.apply_async(args=['123', '123'], queue='123')\r\n```\r\n\r\n```\r\nLOG:\r\n[2017-12-26 08:42:58,659]: logs_util.py[line:64] [pid:25680] ERROR Traceback (most recent call last):\r\n  File \"./App/views/experiment_views.py\", line 377, in create_experiment_and_run\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/task.py\", line 521, in apply_async\r\n    if app.conf.task_always_eager:\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/utils/collections.py\", line 431, in __getitem__\r\n    return getitem(k)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/utils/collections.py\", line 280, in __getitem__\r\n    return mapping[_key]\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/utils/objects.py\", line 44, in __get__\r\n    value = obj.__dict__[self.__name__] = self.__get(obj)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/base.py\", line 148, in data\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/base.py\", line 911, in _finalize_pending_conf\r\n    conf = self._conf = self._load_config()\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/base.py\", line 921, in _load_config\r\n    self.loader.config_from_object(self._config_source)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/loaders/base.py\", line 128, in config_from_object\r\n    obj = self._smart_import(obj, imp=self.import_from_cwd)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/loaders/base.py\", line 146, in _smart_import\r\n    return imp(path)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/loaders/base.py\", line 106, in import_from_cwd\r\n    package=package,\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/utils/imports.py\", line 100, in import_from_cwd\r\n    with cwd_in_path():\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 84, in helper\r\n    return GeneratorContextManager(func(*args, **kwds))\r\nRuntimeError: maximum recursion depth exceeded while calling a Python object\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4464/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lexabug": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4462", "title": "INFO log messages land to stderr", "body": "I've set up a basic application with Django 2.0 and Celery 4.1.0 with debug task (as it described [here](http://docs.celeryproject.org/en/latest/django/first-steps-with-django.html#using-celery-with-django)) and one custom task in application tasks module.\r\nMy module code look like this:\r\n```\r\n# Create your tasks here\r\nfrom __future__ import absolute_import, unicode_literals\r\nfrom celery import shared_task\r\nfrom django.conf import settings\r\nfrom celery.utils.log import get_task_logger\r\n\r\nlogger = get_task_logger(__name__)\r\n\r\n@shared_task(name='validate_user_email', ignore_result=True, bind=True)\r\ndef validate_user_email(self, user_id):\r\n    logger.info('%s email verified', user_id)\r\n```\r\n\r\nWhen I redirect streams (stdout and stderr) to different logs  (info.log and error.log) info.log is silent while error.log contains log entries with levels WARNING and INFO.\r\n\r\nCommand I execute celery with is: `celery -A email_validation worker -Q validate --concurrency 5 --maxtasksperchild 100 -l info > info.log 2> error.log`\r\n\r\nCelery config:\r\n```\r\nCELERY_BROKER_URL = '******************'\r\nCELERY_BROKER_HEARTBEAT = 900\r\nCELERY_BROKER_HEARTBEAT_CHECKRATE = 15\r\nCELERY_RESULT_BACKEND = 'amqp'\r\nCELERY_WORKER_PREFETCH_MULTIPLIER = 1\r\nCELERY_WORKER_MAX_TASKS_PER_CHILD = 100\r\nCELERY_TASK_ACKS_LATE = True\r\nCELERY_ENABLE_UTC = False\r\nCELERY_TIMEZONE = 'US/Eastern'\r\nCELERY_WORKER_DISABLE_RATE_LIMITS = True\r\nCELERY_EVENT_QUEUE_TTL = 1\r\nCELERY_EVENT_QUEUE_EXPIRES = 60\r\nCELERY_RESULT_EXPIRES = 3600\r\nCELERY_TASK_IGNORE_RESULT = True\r\nCELERY_WORKER_HIJACK_ROOT_LOGGER = True\r\n```\r\n\r\nHow can make celery to post INFO log messages to stdout? ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4462/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cajbecu": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4457", "title": "Connection to broker lost. Trying to re-establish the connection: OSError: [Errno 9] Bad file descriptor", "body": "## Checklist\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Software\r\ncelery==4.1.0\r\nkombu==4.1.0\r\namqp==2.2.2\r\nPython 3.6.1\r\nbroker: rabbitmq 3.6.14\r\nresult backend: redis\r\n\r\n## Steps to reproduce\r\n1. celery -A proj worker -Q Q1 --autoscale=10,1 -Ofair --without-gossip --without-mingle --heartbeat-interval=60 -n Q1\r\n2. celery lost connection to broker\r\n3. after restarting affected worker the connection is successfully re-established and the worker starts processing tasks\r\n\r\n## Expected behavior\r\ncelery should re-establish connection to broker\r\n\r\n## Actual behavior\r\ncelery tries to re-establish connection to broker but fails with this error message (which is repeated every second) until manually restarted:\r\n```\r\n[user] celery.worker.consumer.consumer WARNING 2017-12-18 00:38:27,078 consumer: \r\nConnection to broker lost. Trying to re-establish the connection...\r\nTraceback (most recent call last):\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/consumer/consumer.py\", line 320, in start\r\n    blueprint.start(self)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/consumer/consumer.py\", line 596, in start\r\n    c.loop(*c.loop_args())\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/loops.py\", line 47, in asynloop\r\n    obj.controller.register_with_event_loop(hub)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/worker.py\", line 217, in register_with_event_loop\r\n    description='hub.register',\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/bootsteps.py\", line 151, in send_all\r\n    fun(parent, *args)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/components.py\", line 178, in register_with_event_loop\r\n    w.pool.register_with_event_loop(hub)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/concurrency/prefork.py\", line 134, in register_with_event_loop\r\n    return reg(loop)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/concurrency/asynpool.py\", line 476, in register_with_event_loop\r\n    for fd in self._fileno_to_outq]\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/concurrency/asynpool.py\", line 476, in <listcomp>\r\n    for fd in self._fileno_to_outq]\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/kombu/async/hub.py\", line 207, in add_reader\r\n    return self.add(fds, callback, READ | ERR, args)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/kombu/async/hub.py\", line 158, in add\r\n    self.poller.register(fd, flags)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/kombu/utils/eventio.py\", line 67, in register\r\n    self._epoll.register(fd, events)\r\nOSError: [Errno 9] Bad file descriptor\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4457/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thiagogalesi": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4454", "title": "Celery does not consider authSource on mongodb backend URLs", "body": "Version: Celery 4.0.2 (from looking at the changes since then it seems there is no change addressing this issue here: https://github.com/celery/celery/commits/master/celery/backends/mongodb.py )\r\n\r\n(Edit) Confirmed with the following versions as well:\r\namqp==2.2.2\r\nbilliard==3.5.0.3\r\ncelery==4.1.0\r\nkombu==4.1.0\r\npymongo==3.6.0\r\n\r\nCelery Report\r\n\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:2.7.8\r\n            billiard:3.5.0.3 py-amqp:2.2.1\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\nsettings -> transport:amqp results:disabled\r\n\r\n\r\n## Steps to reproduce\r\n\r\nGive Celery a Backend URL pointing to a MongoDB instance with authentication and username/password (user/pwd set on the Admin DB by default) in the format:\r\n\r\nmongodb://user:pass@your-server/your_db?authSource=admin\r\n\r\n(Please see http://api.mongodb.com/python/current/examples/authentication.html#default-database-and-authsource and http://api.mongodb.com/python/current/api/pymongo/mongo_client.html?highlight=authsource )\r\n\r\n## Expected behavior\r\n\r\nCelery authenticates the user in the admin database (this is the same as passing --authenticationDatabase to the mongo client or the same url to MongoClient)\r\n\r\n## Actual behavior\r\n\r\nCelery tries to authenticate the user on the your_db database (failing to authenticate)\r\n\r\n## Workaround (not recommended)\r\n\r\nChange the db on the URL to /admin (this db shouldn't be used to store arbitrary data normally)", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yanliguo": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4451", "title": "Celery (4.1.0) worker stops to consume new message when actives is empty  ", "body": "Hi there,\r\n   I was using celery to dispatch some long running task recently, and finding a way to disable prefetch. Now the config is:\r\n\r\nacks_late = True\r\nconcurrency = 1\r\nprefetch_multiplier = 1\r\n-Ofair\r\n\r\nActually, workers are still prefetching tasks. And I also have a monitor job to revoke tasks when a task is stuck in reserved state for a long time (let's say 5 minutes) or the task is outputing valid result.  \r\n\r\nOne thing wired is that, some workers stopped consuming new tasks when the actives is empty and the reseved task is revoked.  Has anybody ever met with this issue ? any help will be appreciated.\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4451/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "italomaia": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4450", "title": "PENDING state, what if it meant just one thing?", "body": "According to docs, PENDING state has the following meaning:\r\n\r\n> Task is waiting for execution or unknown. Any task id that\u2019s not known is implied to be in the pending state.\r\n\r\nAre there plans to make pending mean one thing only? It is quite confusing to handle a state that can mean \"waiting\" or \"lost\". ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4450/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dfresh613": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4449", "title": "ValueFormatError when processing chords with couchbase result backend", "body": "Hi it seems like when I attempt to process groups of chords, the couchbase result backend is consistently failing to unlock the chord when reading from the db:\r\n\r\n`celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2] retry: Retry in 1s: ValueFormatError()`\r\n\r\nThis behavior does not occur with the redis result backend, i can switch between them and see that the error unlocking only occurs on couchbase.\r\n\r\n## Steps to reproduce\r\nAttempt to process a chord with couchbase backend using pickle serialization.\r\n\r\n## Expected behavior\r\nChords process correctly, and resulting data is fed to the next task\r\n\r\n## Actual behavior\r\nCelery is unable to unlock the chord from the result backend\r\n\r\n## Celery project info: \r\n```\r\ncelery -A ipaassteprunner report\r\n\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:2.7.10\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Darwin arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:pyamqp results:couchbase://isadmin:**@localhost:8091/tasks\r\n\r\ntask_serializer: 'pickle'\r\nresult_serializer: 'pickle'\r\ndbconfig: <ipaascommon.ipaas_config.DatabaseConfig object at 0x10fbbfe10>\r\ndb_pass: u'********'\r\nIpaasConfig: <class 'ipaascommon.ipaas_config.IpaasConfig'>\r\nimports:\r\n    ('ipaassteprunner.tasks',)\r\nworker_redirect_stdouts: False\r\nDatabaseConfig: u'********'\r\ndb_port: '8091'\r\nipaas_constants: <module 'ipaascommon.ipaas_constants' from '/Library/Python/2.7/site-packages/ipaascommon/ipaas_constants.pyc'>\r\nenable_utc: True\r\ndb_user: 'isadmin'\r\ndb_host: 'localhost'\r\nresult_backend: u'couchbase://isadmin:********@localhost:8091/tasks'\r\nresult_expires: 3600\r\niconfig: <ipaascommon.ipaas_config.IpaasConfig object at 0x10fbbfd90>\r\nbroker_url: u'amqp://guest:********@localhost:5672//'\r\ntask_bucket: 'tasks'\r\naccept_content: ['pickle']\r\n```\r\n### Additional Debug output\r\n```\r\n[2017-12-13 15:39:57,860: INFO/MainProcess] Received task: celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2]  ETA:[2017-12-13 20:39:58.853535+00:00] \r\n[2017-12-13 15:39:57,861: DEBUG/MainProcess] basic.qos: prefetch_count->27\r\n[2017-12-13 15:39:58,859: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x10b410b90> (args:('celery.chord_unlock', 'e3139ae5-a67d-4f0c-8c54-73b1e19433d2', {'origin': 'gen53678@silo2460', 'lang': 'py', 'task': 'celery.chord_unlock', 'group': None, 'root_id': '0acd3e0d-7532-445c-8916-b5fc8a6395ab', u'delivery_info': {u'priority': None, u'redelivered': False, u'routing_key': u'celery', u'exchange': u''}, 'expires': None, u'correlation_id': 'e3139ae5-a67d-4f0c-8c54-73b1e19433d2', 'retries': 311, 'timelimit': [None, None], 'argsrepr': \"('90c64bef-21ba-42f9-be75-fdd724375a7a', {'chord_size': 2, 'task': 'ipaassteprunner.tasks.transfer_data', 'subtask_type': None, 'kwargs': {}, 'args': (), 'options': {'chord_size': None, 'chain': [...], 'task_id': '9c6b5e1c-2089-4db7-9590-117aeaf782c7', 'root_id': '0acd3e0d-7532-445c-8916-b5fc8a6395ab', 'parent_id': 'c27c9565-19a6-4683-8180-60f0c25007e9', 'reply_to': '0a58093c-6fdd-3458-9a34-7d5e094ac6a8'}, 'immutable': False})\", 'eta': '2017-12-13T20:39:58.853535+00:00', 'parent_id': 'c27c9565-19a6-4683-8180-60f0c25007e9', u'reply_to':... kwargs:{})\r\n[2017-12-13 15:40:00,061: DEBUG/MainProcess] basic.qos: prefetch_count->26\r\n[2017-12-13 15:40:00,065: DEBUG/MainProcess] Task accepted: celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2] pid:53679\r\n[2017-12-13 15:40:00,076: INFO/ForkPoolWorker-6] Task celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2] retry: Retry in 1s: ValueFormatError()\r\n```\r\n\r\n### Stack trace from chord unlocking failure\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/Library/Python/2.7/site-packages/celery/app/trace.py\", line 374, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n  File \"/Library/Python/2.7/site-packages/celery/app/trace.py\", line 629, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n  File \"/Library/Python/2.7/site-packages/celery/app/builtins.py\", line 75, in unlock_chord\r\n    raise self.retry(countdown=interval, max_retries=max_retries)\r\n  File \"/Library/Python/2.7/site-packages/celery/app/task.py\", line 689, in retry\r\n    raise ret\r\nRetry: Retry in 1s\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4449/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yutkin": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4438", "title": "Celery doesn't write RECEIVED state into MongoDB", "body": "When a number of tasks in a queue surpass a number of workers, new added tasks are not writing in a backend. In other words, I want to write task state (RECEIVED) into DB immediately after its invocation. It is possible?\r\n\r\nP.S. I'm using MongoDB as backend. ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/4414", "title": "Storing additional information about celery tasks in mongodb", "body": "I want to store in MongoDB additional information about a task. For example, an ID of a user that created the task. Example:\r\n```\r\n    {\r\n      \"_id\" : \"a77db490-d090-4c9d-a48c-b6f766bee902\",\r\n      \"status\" : \"SUCCESS\",\r\n      \"result\" : \"8\",\r\n      \"date_done\" : ISODate(\"2017-11-17T10:16:28.942Z\"),\r\n      \"traceback\" : \"null\",\r\n      \"children\" : \"[]\",\r\n\r\n      \"task_creator\": \"username\" // <---- How can I add such field?\r\n\r\n    }\r\n```\r\nThis is needed for simplification of querying (my app is statefull). How can I do it?", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4414/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "canassa": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4426", "title": "Task is executed twice when the worker restarts", "body": "Currently using Celery 4.1.0\r\n\r\n## Steps to reproduce\r\n\r\nStart a new project using RabbitMQ and register the following task:\r\n\r\n```python\r\nfrom django.core.cache import cache\r\n\r\n@shared_task(bind=True)\r\ndef test_1(self):\r\n    if not cache.add(self.request.id, 1):\r\n        raise Exception('Duplicated task {}'.format(self.request.id))\r\n```\r\n\r\nNow start 2 workers. I used gevent with a concurrency of 25 for this test:\r\n\r\n```\r\ncelery worker -A my_proj -Q my_queue -P gevent -c 25\r\n```\r\n\r\nOpen a python shell and fire a a bunch of tasks:\r\n\r\n```python\r\nfrom myproj.tasks import test_1\r\n\r\nfor i in range(10000):\r\n    test_1.apply_async()\r\n```\r\n\r\nNow quickly do a warm shutdown (Ctrl+c) in one of the workers while it's still processing the tasks, you should see the errors popping in the second worker:\r\n\r\n```\r\nERROR    Task my_proj.tasks.test_1[e28e6760-1371-49c9-af87-d196c59375e9] raised unexpected: Exception('Duplicated task e28e6760-1371-49c9-af87-d196c59375e9',)\r\nTraceback (most recent call last):\r\n  File \"/code/virtualenv/CURRENT/lib/python3.5/site-packages/celery/app/trace.py\", line 374, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n  File \"/code/virtualenv/CURRENT/lib/python3.5/site-packages/celery/app/trace.py\", line 629, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n  File \"/code/scp/python/my_proj/tasks.py\", line 33, in test_1\r\n    raise Exception('Duplicated task {}'.format(self.request.id))\r\nException: Duplicated task e28e6760-1371-49c9-af87-d196c59375e9\r\n```\r\n\r\n## Expected behavior\r\n\r\nSince I am not using late acknowledgment and I am not killing the workers I wasn't expecting the tasks to execute again.\r\n\r\n## Actual behavior\r\n\r\nThe tasking are being executed twice, this is causing some problems in our servers because we restart our works every 15 minutes or so in order to avoid memory leaks.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4426/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kn-id": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4424", "title": "Celery Crash: Unrecoverable error when using QApplication in main process", "body": "Error happens when there's some queues already added before worker started and max task per child is 1\r\n\r\n## Checklist\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      software -> celery:4.1.0 (latentcall) kombu:4.1.0 py:2.7.12\r\n                         billiard:3.5.0.3 py-amqp:2.2.2\r\n      platform -> system:Linux arch:64bit, ELF imp:CPython\r\n      loader   -> celery.loaders.app.AppLoader\r\n      settings -> transport:amqp results:disabled\r\n- [ ] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n1. create instance of QApplication when main worker started\r\n```\r\n@celeryd_after_setup.connect\r\nfrom PyQt4.QtGui import QApplication\r\ndef init_worker(sender, **k):\r\n    QApplication([])\r\n```\r\n2. create task\r\n```\r\n@app.task\r\ndef job():\r\n    print 'hello'\r\n```\r\n3. add some queues (2 - 3 per thread. so if there's 4 worker child then there's 8 or more queues)\r\n\r\n4. start worker with max task per child 1\r\n```\r\ncelery worker -A proj --loglevel=INFO --max-tasks-per-child=1\r\n```\r\n\r\n## Expected behavior\r\n- run queues successfuly\r\n## Actual behavior\r\nCelery Crash after 1 queue per child\r\n```\r\n2017-12-07 10:20:19,594: INFO/MainProcess] Received task: proj.tasks.job[c6413dc2-ad62-4033-a69b-39556276f789]  \r\n[2017-12-07 10:20:19,595: INFO/MainProcess] Received task: proj.tasks.job[f1c10b1c-03ae-4220-9c7f-2cdf4afc61e3]  \r\n[2017-12-07 10:20:19,596: INFO/MainProcess] Received task: proj.tasks.job[d54f4554-4517-470f-8e14-adedcb93a46e]  \r\n[2017-12-07 10:20:19,596: INFO/MainProcess] Received task: proj.tasks.job[6255e5e6-d4c8-4d87-8075-642bca9e6a6d]  \r\n[2017-12-07 10:20:19,700: INFO/ForkPoolWorker-1] Task proj.tasks.job[ca856d5c-f3cc-45d4-9fbc-665753f5d1d2] succeeded in 0.00115608799388s: None\r\n[2017-12-07 10:20:19,701: INFO/ForkPoolWorker-4] Task proj.tasks.job[9ae27611-e8e5-4e08-9815-1e56e2ad1565] succeeded in 0.00130111300678s: None\r\n[2017-12-07 10:20:19,701: INFO/ForkPoolWorker-3] Task proj.tasks.job[f5aa7c6a-4142-4a38-8814-c60424196826] succeeded in 0.00129756200477s: None\r\n[2017-12-07 10:20:19,702: INFO/ForkPoolWorker-2] Task proj.tasks.job[eb13b5c5-8865-4992-8b9e-6672c909fd59] succeeded in 0.00100053900678s: None\r\n[2017-12-07 10:20:19,710: INFO/MainProcess] Received task: proj.tasks.job[01700061-c69c-4f4c-abf2-e6ba200772bd]  \r\n[2017-12-07 10:20:19,711: INFO/MainProcess] Received task: proj.tasks.job[a27d7a7b-2c58-4689-8b98-2c0a4ceaea9f]  \r\n[2017-12-07 10:20:19,713: INFO/MainProcess] Received task: proj.tasks.job[4c4a5685-23d5-4178-89cc-9ce4ad5a3509]  \r\n[2017-12-07 10:20:19,714: INFO/MainProcess] Received task: proj.tasks.job[44a079a3-aacf-48c5-a76b-a061bdced1d6]\r\n[2017-12-07 01:41:03,591: CRITICAL/MainProcess] Unrecoverable error: AttributeError(\"'error' object has no attribute 'errno'\",)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/worker.py\", line 203, in start\r\n    self.blueprint.start(self)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/bootsteps.py\", line 370, in start\r\n    return self.obj.start()\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/consumer/consumer.py\", line 320, in start\r\n    blueprint.start(self)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/consumer/consumer.py\", line 596, in start\r\n    c.loop(*c.loop_args())\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/loops.py\", line 88, in asynloop\r\n    next(loop)\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/async/hub.py\", line 354, in create_loop\r\n    cb(*cbargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/concurrency/asynpool.py\", line 444, in _event_process_exit\r\n    self.maintain_pool()\r\n  File \"/usr/local/lib/python2.7/dist-packages/billiard/pool.py\", line 1307, in maintain_pool\r\n    self._maintain_pool()\r\n  File \"/usr/local/lib/python2.7/dist-packages/billiard/pool.py\", line 1298, in _maintain_pool\r\n    joined = self._join_exited_workers()\r\n  File \"/usr/local/lib/python2.7/dist-packages/billiard/pool.py\", line 1165, in _join_exited_workers\r\n    self.process_flush_queues(worker)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/concurrency/asynpool.py\", line 1175, in process_flush_queues\r\n    readable, _, _ = _select(fds, None, fds, timeout=0.01)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/concurrency/asynpool.py\", line 183, in _select\r\n    if exc.errno == errno.EINTR:\r\nAttributeError: 'error' object has no attribute 'errno'\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4424/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jenstroeger": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4420", "title": "How to unpack serialzied task arguments?", "body": "When iterate over all currently scheduled tasks\r\n```python\r\nfor task in chain.from_iterable(my_app.control.inspect().scheduled().values()):\r\n    print(task)\r\n```\r\nI get a dictionary `task['request']` which contains a serialization of the tasks\u2019 arguments in `task['request']['args']` (and `'kwargs`). Both are strings:\r\n```\r\n'args': \"('5', {'a': 'b'})\",\r\n'kwargs': '{}', \r\n```\r\nIt\u2019s not [JSON](https://www.json.org/) nor [msgpack](https://msgpack.org/). How can I unpack that `args` string into a Python tuple again? Celery must have a helper function for that somewhere? (Anything to do with `argsrepr` and `kwargsrepr` and [`saferepr`](https://github.com/celery/celery/blob/master/celery/utils/saferepr.py)?)", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4420/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/4400", "title": "Same task runs multiple times at once?", "body": "The issue is a repost of an unattended Google groups post [_Same task runs multiple times?_](https://groups.google.com/forum/#!topic/celery-users/8jjjsFkdCQI)\r\n\r\n```\r\n> ./bin/celery -A celery_app report\r\n\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.1\r\n            billiard:3.5.0.3 redis:2.10.6\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis://localhost:6379/2\r\n\r\nbroker_url: 'redis://localhost:6379/2'\r\nresult_backend: 'redis://localhost:6379/2'\r\ntask_serializer: 'json'\r\nresult_serializer: 'json'\r\naccept_content: ['json']\r\ntimezone: 'Europe/Berlin'\r\nenable_utc: True\r\nimports: 'tasks'\r\ntask_routes: {\r\n 'tasks': {'queue': 'celery-test-queue'}}\r\n```\r\nMy application schedules a single group of two, sometimes three tasks, each of which with their own ETA within one hour. When the ETA arrives, I see the following in my celery log:\r\n```\r\n[2017-11-20 09:55:34,470: INFO/ForkPoolWorker-2] Task tasks._test_exec[bd08ab85-28a8-488f-ba03-c2befde10054] succeeded in 33.81780316866934s: None\r\n[2017-11-20 09:55:34,481: INFO/ForkPoolWorker-2] Task tasks._test_exec[bd08ab85-28a8-488f-ba03-c2befde10054] succeeded in 0.009824380278587341s: None\r\n[2017-11-20 09:55:34,622: INFO/ForkPoolWorker-2] Task tasks._test_exec[bd08ab85-28a8-488f-ba03-c2befde10054] succeeded in 0.14010038413107395s: None\r\n\u2026\r\n[2017-11-20 09:55:37,890: INFO/ForkPoolWorker-8] Task tasks._test_exec[bd08ab85-28a8-488f-ba03-c2befde10054] succeeded in 0.012678759172558784s: None\r\n[2017-11-20 09:55:37,891: INFO/ForkPoolWorker-2] Task tasks._test_exec[bd08ab85-28a8-488f-ba03-c2befde10054] succeeded in 0.01177949644625187s: None\r\n[2017-11-20 09:55:37,899: INFO/ForkPoolWorker-8] Task tasks._test_exec[bd08ab85-28a8-488f-ba03-c2befde10054] succeeded in 0.008250340819358826s: None\r\n\u2026\r\n```\r\nThis can repeat dozens of times. **Note** the first task\u2019s 33 seconds execution time, and the use of different workers!\r\n\r\nI have no explanation for this behavior, and would like to understand what\u2019s going on here.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4400/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/4300", "title": "Revoked task still executes?", "body": "This issue is a continuation of issues https://github.com/celery/celery/issues/4295 (see for Celery details) and https://github.com/celery/celery/issues/4299.\r\n\r\nI am trying to [cancel a scheduled task](http://docs.celeryproject.org/en/latest/faq.html#can-i-cancel-the-execution-of-a-task) using [revoke()](http://docs.celeryproject.org/en/latest/reference/celery.result.html#celery.result.AsyncResult.revoke), but the task still executes. Here\u2019s the Celery log:\r\n```\r\n[2017-09-28 17:52:18,811: INFO/MainProcess] Received task: app.tasks._do_singlething[0e4f0131-b4e6-4209-865e-e91b73e92f28]  ETA:[2017-09-28 07:53:18.766382+00:00] \r\n[2017-09-28 17:52:32,091: INFO/MainProcess] Tasks flagged as revoked: 0e4f0131b4e64209865ee91b73e92f28\r\n[2017-09-28 17:53:18,831: WARNING/ForkPoolWorker-1] Hello from executing task 0e4f0131-b4e6-4209-865e-e91b73e92f28\r\n[2017-09-28 17:53:18,841: INFO/ForkPoolWorker-1] Task app.tasks._do_singlething[0e4f0131-b4e6-4209-865e-e91b73e92f28] succeeded in 0.06189309403998777s: None\r\n```\r\nCode snippet that revokes is the same as in issue https://github.com/celery/celery/issues/4299.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4300/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/4299", "title": "Chained tasks lose parent relationship?", "body": "This issue is a continuation of issue https://github.com/celery/celery/issues/4295.\r\n\r\nNow that both tasks are created, debug information shows their `parent` relation:\r\n```\r\n2017-09-28 16:29:16,138 INFO  [srv][MainThread] id=c18c417d-b825-4188-a55d-75faa86aa666 status=PENDING parent=b1f6d993-3c55-4280-93a8-f885747d53f9\r\n2017-09-28 16:29:16,171 INFO  [srv][MainThread] id=b1f6d993-3c55-4280-93a8-f885747d53f9 status=PENDING parent=None\r\n```\r\nThe first task of the chain, `_do_something()`, is also received by the broker:\r\n```\r\n[2017-09-28 16:29:22,801: INFO/MainProcess] Received task: app.tasks._do_something[b1f6d993-3c55-4280-93a8-f885747d53f9]  ETA:[2017-09-28 07:29:22.759187+00:00]\r\n```\r\nThe second task `_do_otherthing()` won\u2019t show until the first task runs.\r\n\r\nHowever, while both tasks are `PENDING`, I\u2019d like to [revoke()](http://docs.celeryproject.org/en/latest/reference/celery.result.html#celery.result.AsyncResult.revoke) the whole chain. To do so, I thought to iterate over the [`result.parent`](http://docs.celeryproject.org/en/latest/reference/celery.result.html#celery.result.ResultBase.parent) of the chain result, only to find that the newly created `AsyncResult` for the second task `_do_otherthing()` has no `parent` value set anymore?\r\n```python\r\ntask_id = chain_result.task_id # This is the AsyncResult object from celery.chain().\r\nresult = _do_otherthing.AsyncResult(task_id)\r\n# Now, c18c417d-b825-4188-a55d-75faa86aa666 has lost its parent value?!\r\nwhile result:\r\n    result.revoke()\r\n    result = result.parent\r\n```\r\nI would have expected to find the `parent` relation of all tasks in that chain in tact.\r\n\r\nSomewhat related, there is a question on Stackoverflow [\u201cCelery: clean way of revoking the entire chain from within a task\u201d](https://stackoverflow.com/questions/23793928/celery-clean-way-of-revoking-the-entire-chain-from-within-a-task), but all solutions seem rather clumsy to me.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4299/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/4295", "title": "ETA string is not parsed into datetime when chaining tasks", "body": "I\u2019m setting up two tasks in a [Chain](http://docs.celeryproject.org/en/latest/userguide/canvas.html#chains) like so:\r\n```python\r\n    @app.task(bind=True)\r\n    def _do_something(self, arg):\r\n        pass # First task to run.\r\n\r\n    @app.task(bind=True)\r\n    def _do_otherthing(self, arg):\r\n        pass # Second task to run.\r\n\r\n    def schedule_tasks():\r\n        \u2026\r\n        task_args = (\"Grumble\",)\r\n        something_eta = datetime.utcnow() + datetime.get_timedelta(seconds=20)\r\n        something_sig = _do_something.signature(task_args, eta=something_eta, immutable=True)\r\n        otherthing_eta = datetime.utcnow() + datetime.get_timedelta(seconds=40)\r\n        otherthing_sig = _do_otherthing.signature(task_args, eta=otherthing_eta, immutable=True)\r\n\r\n        task_chain = celery.chain(something_sig, otherthing_sig)                                    \r\n        result = task_chain.delay()                                                                     \r\n```\r\nThe first task `do_something()` schedules and executes fine, and then while scheduling the second task an exception raises:\r\n```bash\r\n[2017-09-28 10:56:50,048: WARNING/ForkPoolWorker-4] /\u2026/lib/python3.5/site-packages/celery/app/trace.py:549: RuntimeWarning: Exception raised outside body: AttributeError(\"'str' object has no attribute 'isoformat'\",):\r\nTraceback (most recent call last):\r\n  File \"/\u2026/lib/python3.5/site-packages/celery/app/trace.py\", line 431, in trace_task\r\n    parent_id=uuid, root_id=root_id,\r\n  File \"/\u2026/lib/python3.5/site-packages/celery/canvas.py\", line 222, in apply_async\r\n    return _apply(args, kwargs, **options)\r\n  File \"/\u2026/lib/python3.5/site-packages/celery/app/task.py\", line 536, in apply_async\r\n    **options\r\n  File \"/\u2026/lib/python3.5/site-packages/celery/app/base.py\", line 729, in send_task\r\n    root_id, parent_id, shadow, chain,\r\n  File \"/\u2026/lib/python3.5/site-packages/celery/app/amqp.py\", line 333, in as_task_v2\r\n    eta = eta and eta.isoformat()\r\nAttributeError: 'str' object has no attribute 'isoformat'\r\n```\r\nMy suspicion is that the second task\u2019s `eta` is stored as a string, but when actually scheduled (see [`Signature.apply_async()`](https://github.com/celery/celery/blob/master/celery/canvas.py#L221) code) that string is _not_ parsed back into a [datetime](https://docs.python.org/3/library/datetime.html#datetime.datetime) object again, thus causing the exception further downstream.\r\n\r\nThe following hack works for [amqp.py:333](https://github.com/celery/celery/blob/master/celery/app/amqp.py#L333), but is likely not the proper fix:\r\n```python\r\neta = eta and (eta if isinstance(eta, str) else eta.isoformat())\r\n```\r\n\r\nCurrent Celery environment:\r\n```\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.5.4\r\n            billiard:3.5.0.3 redis:2.10.6\r\nplatform -> system:Darwin arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis://localhost:6379/0\r\n\r\naccept_content: ['json']\r\ntask_serializer: 'json'\r\nresult_serializer: 'json'\r\nenable_utc: True\r\nresult_backend: 'redis://localhost:6379/0'\r\nbroker_url: 'redis://localhost:6379/0'\r\ntimezone: 'Europe/Berlin'\r\nimports: 'srv.celery.tasks'\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4295/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Woile": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4415", "title": "Celerybeat not delivered to correct queue if specified in options", "body": "## Checklist\r\n\r\n- [x] I have included the version of celery in the issue.\r\n```\r\n>>> celery --version\r\n4.0.2 (latentcall)\r\n```\r\nand master\r\n```\r\n>>> celery --version\r\n4.1.0 (latentcall)\r\n```\r\n\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n\r\n1. Create a debug task:\r\n```\r\n@app.task(bind=True)\r\ndef debug_task(self, word):\r\n    logger.setLevel(logging.DEBUG)\r\n    logger.info('Is this your word? %s', word)\r\n    print('Request: {0!r}'.format(self.request))\r\n```\r\n2. Add a periodic task like this:\r\n```\r\napp.add_periodic_task(crontab(\r\n        minute='*/1',  # run every minute\r\n    ), debug_task.s('foo'),\r\n        options={'queue': 'foo',\r\n                       'routing_key': 'foo'})\r\n```\r\n\r\n3. Run 2 workers with different queues and a beat\r\n```\r\ncelery worker -A proj worker -Q foo -n foo@%h --loglevel=DEBUG\r\ncelery worker -A proj worker -Q bar -n bar@%h --loglevel=DEBUG\r\ncelery -A main.celery_app beat --loglevel=DEBUG\r\n```\r\n4. Wait and observe.\r\n\r\n## Expected behavior\r\n\r\nThe task is delivered to the right queue according to [the docs](http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html), options will be passed to `apply_async`, so if I add `queue` it should go to the correct queue\r\n\r\n>options\r\n>\r\n> Execution options (dict).\r\n>\r\n> This can be any argument supported by apply_async() \u2013 exchange, routing_key, expires, and so on.\r\n\r\n## Actual behavior\r\nThe task is handled by any queue.\r\n\r\n## Solution\r\n\r\nI found that if I specify `queue` as an attribute of `add_periodic_task` it starts working correctly. But I couldn't find any related information in the documentations. Like this:\r\n```\r\napp.add_periodic_task(crontab(\r\n        minute='*/1',  # run every minute\r\n    ), debug_task.s('foo'),\r\n        queue='foo',\r\n        options={'queue': 'foo',\r\n                       'routing_key': 'foo'})\r\n```\r\n\r\nAm I missing something here? ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4415/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "knightjoel": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4413", "title": "Docs: correct the default value for worker_task_log_format", "body": "## Checklist\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n    - This is a documentation issue. Present in v4.1.0 and in master.\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n#317 should have updated docs/userguide/configuration.rst with the new default for the `worker_task_log_format` config option.\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4413/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jheld": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4410", "title": "RunTimeError: Acquire on closed pool try to use control inspect", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.1.0 py:2.7.13 or (py:2.7.12)\r\n            billiard:3.5.0.3 redis:2.10.5\r\nplatform -> system:Darwin arch:64bit imp:CPython . (though usually: system:Linux arch:64bit, ELF)\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis://localhost:6380/\r\n\r\nBROKER_TRANSPORT_OPTIONS: {\r\n    'fanout_patterns': True, 'fanout_prefix': True}\r\nCELERY_TASK_COMPRESSION: 'gzip'\r\nCELERY_TIMEZONE: 'UTC'\r\nCELERY_RESULT_SERIALIZER: 'json'\r\nCELERY_BROKER_URL: u'redis://localhost:6380//'\r\nCELERY_TASK_SERIALIZER: 'json'\r\nCELERY_RESULT_EXPIRES: 60\r\nCELERY_ACCEPT_CONTENT: ['application/json']\r\nTIME_ZONE: 'UTC'\r\nCELERY_MESSAGE_COMPRESSION: 'gzip'\r\nCELERY_TASK_ALWAYS_EAGER: False\r\nCELERY_RESULT_BACKEND: u'redis://localhost:6380/'\r\n\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\nAlso occurs on celery 4.1.0.\r\n\r\n## Steps to reproduce\r\n\r\nTry to use the `control` module. In my case, I'm getting the `active_queues()`.\r\n\r\n## Expected behavior\r\n\r\nI expect that so long as the system is in a good state, I should be able to get the info from within the `control` module. I don't understand exactly why sometimes the pool is closed and other times it's not.\r\n\r\n## Actual behavior\r\n\r\nThis might be the same issue as in #1839 \r\n\r\nThe code will have a runtime error, so I am unable to query for the data I need from celery.\r\n\r\n```\r\nFile \"/.../tasks.py\", line 80, in workers_on_queue\r\n    for k, v in six.viewitems(celery_app.control.inspect().active_queues()):\r\n  File \"/.../lib/python2.7/site-packages/celery/app/control.py\", line 116, in active_queues\r\n    return self._request('active_queues')\r\n  File \"/.../lib/python2.7/site-packages/celery/app/control.py\", line 81, in _request\r\n    timeout=self.timeout, reply=True,\r\n  File \"/.../lib/python2.7/site-packages/celery/app/control.py\", line 436, in broadcast\r\n    limit, callback, channel=channel,\r\n  File \"/.../lib/python2.7/site-packages/kombu/pidbox.py\", line 315, in _broadcast\r\n    serializer=serializer)\r\n  File \"/.../lib/python2.7/site-packages/kombu/pidbox.py\", line 285, in _publish\r\n    with self.producer_or_acquire(producer, chan) as producer:\r\n  File \"/usr/local/Cellar/python/2.7.13_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/.../lib/python2.7/site-packages/kombu/pidbox.py\", line 247, in producer_or_acquire\r\n    with self.producer_pool.acquire() as producer:\r\n  File \"/.../lib/python2.7/site-packages/kombu/resource.py\", line 74, in acquire\r\n    raise RuntimeError('Acquire on closed pool')\r\n```\r\n\r\nThis only happens when we're using the `control` module. Sometimes it works okay.\r\n\r\nThis code path was even in a retry-loop, so in the end it still failed to execute.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4410/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/4386", "title": "redis connection issues on 4.x", "body": "## Checklist\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n   celery 4.0.2\r\n- [ ] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n\r\nThis is not easy to reproduce. But, it does happen from time to time and it has blown up our usage of `beat` and the workers consuming tasks correctly.\r\n\r\n## Expected behavior\r\n\r\nWe don't want to see the error continue to happen -- the workers and producers should be able to reconnect to the broker correctly.\r\n\r\nIs there anything we can do to help? Anywhere to look?\r\n\r\n## Actual behavior\r\n\r\nThey go into a retry loop and never come out. Other OS system issue may occur throughout this time.\r\n\r\nhttps://github.com/celery/celery/issues/2442", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4386/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/3988", "title": "Beat task run on-load", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:2.7.12\r\n            billiard:3.5.0.2 redis:2.10.5\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nLoad celery beat with a task added to the periodic scheduler for every 30 minutes. It will not run until 30 minutes have passed.\r\n## Expected behavior\r\nI have some tasks that have long-running intervals (30 minutes, 1 day), and I'd like beat to have an option to run the task on-load, and then on the interval. I imagine this would be a setting on the periodic scheduler init for that task.\r\n## Actual behavior\r\nThe task does not run until the interval has passed one time.\r\n\r\n\r\nI wouldn't mind submitting the patch myself, assuming it's something the project is interested in at all.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3988/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "exrich": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4409", "title": "Chunks Performance Issue", "body": "I have between 5000-10000 tasks to execute and to minimize the number of messages sent I'm putting them in chunks.  Whilst trying to get 5000 tasks to execute in under a second I noticed a delay in retrieving chunked results.  I'm running Redis results backend, AMPQ broker and Celery 4.1.0\r\n\r\nNo significant delay in sending the tasks:\r\n\r\n%timeit tasks.add.chunks([(1,1)], 1)()\r\n1000 loops, best of 3: 818 \u00b5s per loop\r\n%timeit tasks.add.delay(1,1)\r\n1000 loops, best of 3: 490 \u00b5s per loop\r\n\r\nBut retrieving is slow:\r\n\r\n%timeit tasks.add.chunks([(1,1)], 1)().get()\r\n1 loop, best of 3: 502 ms per loop\r\n%timeit tasks.add.delay(1,1).get()\r\n1000 loops, best of 3: 1.56 ms per loop\r\n\r\nI thought it might be the db interval but the docs say it doesn't apply to Redis and adding interval=0.01 makes no difference:\r\n\r\n%timeit tasks.add.chunks([(1,1)], 1)().get(interval=0.01)\r\n1 loop, best of 3: 505 ms per loop\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4409/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sebastiaopf": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4408", "title": "Celery + Django: Task discover fails silently when wrong permissions in log file", "body": "## Checklist\r\n\r\n- [X] I have included the output of ``celery -A proj report`` in the issue.\r\n- [ ] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n- Create a Django Project and some celery task\r\n- Configure a Django logging handler to use 'file'\r\n- Allow write permissions on the django log file to the user who runs django, but not the user who runs the Celery worker\r\n- EDIT: Change into the celery user (`su - celery` for example), to simulate how the service will execute\r\n- Try to start celery worker with something like `/usr/local/bin/celery -A my_app worker -Q loan -l debug --logfile=/var/log/celery/celery.log --pidfile=/var/run/celery/celery.pid`\r\n\r\n## Expected behavior\r\nCelery worker should start and discover the tasks OR Celery worker should fail and inform it can't discover the tasks because (?) the current user can't write to the Django log file.\r\n\r\n## Actual behavior\r\nCelery worker starts, but fails to discover the tasks. But it doesn't show any error message, event with `-l debug`.\r\n\r\nOK, I'm not sure if this is a bug in Celery or Django, or even if it's a bug at all. At least I'm leaving this here if someone else has the same problem. It took me some time to figure this out, so maybe I can save someone else's time.\r\n\r\nAt first I thought it was a problem with Systemd, but after sudo'ing into the Celery user and trying to run the command manually, i found that it fails to discover the tasks. The same command works when ran under root. The celery user has write permissions to all its folders, and has read permissions on the Django project folder. What it didn't have (and I never thought it would need) is write permissions on the Django log file/folder. After I gave the Celery user write permission to the file, it started discovering the tasks fine. The telling symptom for me was that when sudo'ing into the Celery user i couldn't run the Django manage.py command (should I need to?).\r\n\r\nI think that even if this isn't a bug in Celery itself, it would benefit users if it could show a warning message telling that it couldn't write to the Django log file (or whatever reason is the root cause for this).\r\n\r\nAttached report file is from when the Celery user didn't have the write permissions to the Django log file.\r\n\r\n[report.txt](https://github.com/celery/celery/files/1510220/report.txt)\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4408/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thenets": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4406", "title": "[Site] Link error \"operate with other languages\"", "body": "Page not found at http://www.celeryproject.org/ :\r\n\r\n![image](https://user-images.githubusercontent.com/2138276/33258196-d7485890-d33f-11e7-8610-bf6c61330203.png)\r\n\r\n![image](https://user-images.githubusercontent.com/2138276/33258235-fa062f2e-d33f-11e7-9683-2dd44e856fb0.png)\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4406/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dmachlin1": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4405", "title": "Group->Chain->Group tries to access backend in task_always_eager mode", "body": "Issue with Celery 4.1.0 trying to access backend (redis) in `task_always_eager` mode\r\nThe following structure (group -> chain -> group)\r\nFails on `ConnectionError: Error 61`\r\nReplacing the last group to a task succeeds\r\n## Steps to reproduce\r\n```\r\n@app.task()\r\ndef add():\r\n\treturn \r\n\r\ngroup(add.si(),\r\n      chain(add.si(),\r\n            group(add.si(), add.si()))\r\n).delay()\r\n```\r\nTrace:\r\n\r\n```\r\n../../../env/lib/python2.7/site-packages/celery/canvas.py:182: in delay\r\n    return self.apply_async(partial_args, partial_kwargs)\r\n../../../env/lib/python2.7/site-packages/celery/canvas.py:978: in apply_async\r\n    return self.apply(args, kwargs, **options)\r\n../../../env/lib/python2.7/site-packages/celery/canvas.py:1011: in apply\r\n    sig.apply(args=args, kwargs=kwargs, **options) for sig, _ in tasks\r\n../../../env/lib/python2.7/site-packages/celery/canvas.py:1049: in _prepared\r\n    yield task, task.freeze(group_id=group_id, root_id=root_id)\r\n../../../env/lib/python2.7/site-packages/celery/canvas.py:605: in freeze\r\n    self.app, _id, group_id, chord, clone=False,\r\n../../../env/lib/python2.7/site-packages/celery/canvas.py:675: in prepare_steps\r\n    root_id=root_id, group_id=group_id, chord=chord_body,\r\n../../../env/lib/python2.7/site-packages/celery/canvas.py:1108: in freeze\r\n    return self.app.GroupResult(gid, results)\r\n../../../env/lib/python2.7/site-packages/celery/result.py:828: in __init__\r\n    ResultSet.__init__(self, results, **kwargs)\r\n../../../env/lib/python2.7/site-packages/celery/result.py:468: in __init__\r\n    self._on_full = ready_barrier or barrier(results)\r\n../../../env/lib/python2.7/site-packages/vine/synchronization.py:57: in __init__\r\n    [self.add_noincr(p) for p in promises or []]\r\n../../../env/lib/python2.7/site-packages/vine/synchronization.py:82: in add_noincr\r\n    p.then(self)\r\n../../../env/lib/python2.7/site-packages/celery/result.py:99: in then\r\n    self.backend.add_pending_result(self, weak=weak)\r\n../../../env/lib/python2.7/site-packages/celery/backends/async.py:157: in add_pending_result\r\n    self._add_pending_result(result.id, result, weak=weak)\r\n../../../env/lib/python2.7/site-packages/celery/backends/async.py:167: in _add_pending_result\r\n    self.result_consumer.consume_from(task_id)\r\n../../../env/lib/python2.7/site-packages/celery/backends/redis.py:75: in consume_from\r\n    return self.start(task_id)\r\n../../../env/lib/python2.7/site-packages/celery/backends/redis.py:57: in start\r\n    self._consume_from(initial_task_id)\r\n../../../env/lib/python2.7/site-packages/celery/backends/redis.py:82: in _consume_from\r\n    self._pubsub.subscribe(key)\r\n../../../env/lib/python2.7/site-packages/redis/client.py:2229: in subscribe\r\n    ret_val = self.execute_command('SUBSCRIBE', *iterkeys(new_channels))\r\n../../../env/lib/python2.7/site-packages/redis/client.py:2161: in execute_command\r\n    self._execute(connection, connection.send_command, *args)\r\n../../../env/lib/python2.7/site-packages/redis/client.py:2172: in _execute\r\n    connection.connect()\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4405/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Raznak": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4394", "title": "Task queue date", "body": "Hi,\r\n\r\nTo use Celery, I need some extra data passed to the task.\r\nI need the date when the task was to be processed.\r\n\r\nI tried something in the beat.py file:\r\n\r\n```\r\ndef apply_async(self, entry, producer=None, advance=True, **kwargs):\r\n         entry = self.reserve(entry) if advance else entry\r\n        task = self.app.tasks.get(entry.task)\r\n\r\n        try:\r\n            if task:\r\n                # I add the date here.\r\n                entry.args[0]['time_sent'] = datetime.datetime.now()\r\n                return task.apply_async(entry.args, entry.kwargs,\r\n                                        producer=producer,\r\n                                        **entry.options)\r\n            else:\r\n                return self.send_task(entry.task, entry.args, entry.kwargs,\r\n                                      producer=producer,\r\n                                      **entry.options)\r\n        except Exception as exc:  # pylint: disable=broad-except\r\n            reraise(SchedulingError, SchedulingError(\r\n                \"Couldn't apply scheduled task {0.name}: {exc}\".format(\r\n                    entry, exc=exc)), sys.exc_info()[2])\r\n        finally:\r\n            self._tasks_since_sync += 1\r\n            if self.should_sync():\r\n                self._do_sync()\r\n```\r\n\r\nI'm sure there is a better way, but I can't find any.\r\n\r\nThank you\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4394/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "janpom": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4392", "title": "Intermittent failure when sending a task to queue", "body": "I'm repeatedly running into errors when sending a task to celery queue (using RabbitMQ as a broker). This happens intermittently, typically after successfully sending and successfully processing several hundreds tasks over a period of ~60 minutes.\r\n\r\nFrom the logs it looks like celery opens a connection to the broker and immediately closes it again; then opens another one, closes it again, etc. This happens in a very quick succession (a few millisecs apart).\r\n\r\nHere's the log from the process that sends the task:\r\n\r\n```\r\n2017-11-15 18:27:27,890 : MainProcess : DEBUG : Start from server, version: 0.9, properties: {'copyright': 'Copyright (C) 2007-2015 Pivotal Software, Inc.', 'capabilities': {'consumer_priorities': True, 'per_consumer_qos': True, 'connec\r\ntion.blocked': True, 'consumer_cancel_notify': True, 'authentication_failure_close': True, 'publisher_confirms': True, 'exchange_exchange_bindings': True, 'basic.nack': True}, 'information': 'Licensed under the MPL.  See http://www.rabb\r\nitmq.com/', 'product': 'RabbitMQ', 'version': '3.5.7', 'platform': 'Erlang/OTP', 'cluster_name': 'rabbit@hetrad2'}, mechanisms: [b'AMQPLAIN', b'PLAIN'], locales: ['en_US']\r\n2017-11-15 18:27:27,890 : MainProcess : DEBUG : using channel_id: 1\r\n2017-11-15 18:27:27,891 : MainProcess : DEBUG : Channel open\r\n2017-11-15 18:27:27,891 : MainProcess : DEBUG : Closed channel #1\r\n2017-11-15 18:27:27,895 : MainProcess : DEBUG : Start from server, version: 0.9, properties: {'copyright': 'Copyright (C) 2007-2015 Pivotal Software, Inc.', 'capabilities': {'consumer_priorities': True, 'per_consumer_qos': True, 'connec\r\ntion.blocked': True, 'consumer_cancel_notify': True, 'authentication_failure_close': True, 'publisher_confirms': True, 'exchange_exchange_bindings': True, 'basic.nack': True}, 'information': 'Licensed under the MPL.  See http://www.rabb\r\nitmq.com/', 'product': 'RabbitMQ', 'version': '3.5.7', 'platform': 'Erlang/OTP', 'cluster_name': 'rabbit@hetrad2'}, mechanisms: [b'AMQPLAIN', b'PLAIN'], locales: ['en_US']\r\n2017-11-15 18:27:27,895 : MainProcess : DEBUG : using channel_id: 1\r\n2017-11-15 18:27:27,896 : MainProcess : DEBUG : Channel open\r\n2017-11-15 18:27:27,896 : MainProcess : DEBUG : Closed channel #1\r\n2017-11-15 18:27:27,900 : MainProcess : DEBUG : Start from server, version: 0.9, properties: {'copyright': 'Copyright (C) 2007-2015 Pivotal Software, Inc.', 'capabilities': {'consumer_priorities': True, 'per_consumer_qos': True, 'connec\r\ntion.blocked': True, 'consumer_cancel_notify': True, 'authentication_failure_close': True, 'publisher_confirms': True, 'exchange_exchange_bindings': True, 'basic.nack': True}, 'information': 'Licensed under the MPL.  See http://www.rabb\r\nitmq.com/', 'product': 'RabbitMQ', 'version': '3.5.7', 'platform': 'Erlang/OTP', 'cluster_name': 'rabbit@hetrad2'}, mechanisms: [b'AMQPLAIN', b'PLAIN'], locales: ['en_US']\r\n2017-11-15 18:27:27,900 : MainProcess : DEBUG : using channel_id: 1\r\n2017-11-15 18:27:27,901 : MainProcess : DEBUG : Channel open\r\n2017-11-15 18:27:27,901 : MainProcess : DEBUG : Closed channel #1\r\n2017-11-15 18:27:27,905 : MainProcess : DEBUG : Start from server, version: 0.9, properties: {'copyright': 'Copyright (C) 2007-2015 Pivotal Software, Inc.', 'capabilities': {'consumer_priorities': True, 'per_consumer_qos': True, 'connec\r\ntion.blocked': True, 'consumer_cancel_notify': True, 'authentication_failure_close': True, 'publisher_confirms': True, 'exchange_exchange_bindings': True, 'basic.nack': True}, 'information': 'Licensed under the MPL.  See http://www.rabb\r\nitmq.com/', 'product': 'RabbitMQ', 'version': '3.5.7', 'platform': 'Erlang/OTP', 'cluster_name': 'rabbit@hetrad2'}, mechanisms: [b'AMQPLAIN', b'PLAIN'], locales: ['en_US']\r\n2017-11-15 18:27:27,905 : MainProcess : DEBUG : using channel_id: 1\r\n2017-11-15 18:27:27,906 : MainProcess : DEBUG : Channel open\r\n2017-11-15 18:27:27,906 : MainProcess : DEBUG : Closed channel #1\r\n2017-11-15 18:27:27,910 : MainProcess : DEBUG : Start from server, version: 0.9, properties: {'copyright': 'Copyright (C) 2007-2015 Pivotal Software, Inc.', 'capabilities': {'consumer_priorities': True, 'per_consumer_qos': True, 'connec\r\ntion.blocked': True, 'consumer_cancel_notify': True, 'authentication_failure_close': True, 'publisher_confirms': True, 'exchange_exchange_bindings': True, 'basic.nack': True}, 'information': 'Licensed under the MPL.  See http://www.rabb\r\nitmq.com/', 'product': 'RabbitMQ', 'version': '3.5.7', 'platform': 'Erlang/OTP', 'cluster_name': 'rabbit@hetrad2'}, mechanisms: [b'AMQPLAIN', b'PLAIN'], locales: ['en_US']\r\n2017-11-15 18:27:27,910 : MainProcess : DEBUG : using channel_id: 1\r\n2017-11-15 18:27:27,911 : MainProcess : DEBUG : Channel open\r\n2017-11-15 18:27:27,911 : MainProcess : DEBUG : Closed channel #1\r\n```\r\n\r\nHere's the RabbitMQ log:\r\n\r\n```\r\n=INFO REPORT==== 15-Nov-2017::18:27:27 ===\r\naccepting AMQP connection <0.18576.185> (<ipaddress>:33520 -> <ipaddress>:5672)\r\n\r\n=WARNING REPORT==== 15-Nov-2017::18:27:27 ===\r\nclosing AMQP connection <0.18576.185> (<ipaddress>:33520 -> <ipaddress>:5672):\r\nconnection_closed_abruptly\r\n\r\n=INFO REPORT==== 15-Nov-2017::18:27:27 ===\r\naccepting AMQP connection <0.18585.185> (<ipaddress>:33522 -> <ipaddress>:5672)\r\n\r\n=WARNING REPORT==== 15-Nov-2017::18:27:27 ===\r\nclosing AMQP connection <0.18585.185> (<ipaddress>:33522 -> <ipaddress>:5672):\r\nconnection_closed_abruptly\r\n\r\n=INFO REPORT==== 15-Nov-2017::18:27:27 ===\r\naccepting AMQP connection <0.18594.185> (<ipaddress>:33524 -> <ipaddress>:5672)\r\n\r\n=WARNING REPORT==== 15-Nov-2017::18:27:27 ===\r\nclosing AMQP connection <0.18594.185> (<ipaddress>:33524 -> <ipaddress>:5672):\r\nconnection_closed_abruptly\r\n\r\n=INFO REPORT==== 15-Nov-2017::18:27:27 ===\r\naccepting AMQP connection <0.18603.185> (<ipaddress>:33526 -> <ipaddress>:5672)\r\n\r\n=WARNING REPORT==== 15-Nov-2017::18:27:27 ===\r\nclosing AMQP connection <0.18603.185> (<ipaddress>:33526 -> <ipaddress>:5672):\r\nconnection_closed_abruptly\r\n\r\n=INFO REPORT==== 15-Nov-2017::18:27:27 ===\r\naccepting AMQP connection <0.18612.185> (<ipaddress>:33528 -> <ipaddress>:5672)\r\n\r\n=WARNING REPORT==== 15-Nov-2017::18:27:27 ===\r\nclosing AMQP connection <0.18612.185> (<ipaddress>:33528 -> <ipaddress>:5672):\r\nconnection_closed_abruptly\r\n\r\n=WARNING REPORT==== 15-Nov-2017::18:27:28 ===\r\nclosing AMQP connection <0.28780.155> (<ipaddress>:43396 -> <ipaddress>:5672):\r\nconnection_closed_abruptly\r\n```\r\n\r\n## ``celery -A proj report``\r\n\r\n```\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.5.0+\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:rpc:///\r\n\r\nCELERY_TASK_SERIALIZER: 'pickle'\r\nCELERYD_TASK_LOG_FORMAT: ('[%(asctime)s: '\r\n '%(levelname)s/%(processName)s/%(process)d] '\r\n '[%(task_name)s(%(task_id)s)] %(message)s')\r\nCELERY_RESULT_SERIALIZER: 'pickle'\r\nBROKER_URL: 'amqp://<username>:********@<ipaddress>:5672//'\r\nCELERYD_LOG_FORMAT: ('[%(asctime)s: '\r\n '%(levelname)s/%(processName)s/%(process)d] '\r\n '%(message)s')\r\nCELERY_RESULT_BACKEND: 'rpc:///'\r\nCELERY_ACCEPT_CONTENT: ['pickle']\r\nCELERYD_MAX_MEMORY_PER_CHILD: 10485760\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4392/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "fredley": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4391", "title": "Final task run during stopwait with multi does not emit success/failure event", "body": "## Checklist\r\n\r\n- Celery Version 4.1.0\r\n\r\n## Steps to reproduce\r\n\r\n* Using celery multi with the `-E` flag: `celery multi start 2 -A proj -E --pidfile=/tmp/celery-%n.pid`\r\n* Using a camera (e.g. as provided by django_celery_monitor): `celery events -A proj --camera proj.camera.Camera --frequency=2.0`\r\n* Start a long-running task: `test_task.delay(seconds_to_sleep=20)`\r\n* `stopwait` the workers: `celery multi stopwait 2 -E --pidfile=/tmp/celery-%n.pid`\r\n\r\n## Expected behavior\r\n\r\nThe camera receives a `SUCCESS` OR `FAILED` result for the task\r\n\r\n## Actual behavior\r\n\r\nThe camera receives a `STARTED` message for the task at every shutter\r\n\r\n## Furthermore\r\n\r\nWhen registering for real-time events with a monitor, the success or failed callback is not triggered for the last event.\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4391/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jaddison": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4388", "title": "Systemd: Reloading celery shuts it down cleanly, but won't start back up", "body": "## Checklist\r\n\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n- [x] I have included the output of ``celery -A proj report`` in the issue:\r\n```\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.5.2\r\n            billiard:3.5.0.3 redis:2.10.6\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:disabled\r\n\r\nCACHES: {\r\n    'default': {   'BACKEND': 'django_redis.cache.RedisCache',\r\n                   'LOCATION': 'redis://localhost:6379/1',\r\n                   'TIMEOUT': 3600}}\r\n\r\nCELERY_TASK_COMPRESSION: 'gzip'\r\nCELERY_TASK_IGNORE_RESULT: True\r\nCELERY_ACCEPT_CONTENT: ['pickle', 'json', 'msgpack', 'yaml']\r\nCELERY_BROKER_URL: 'redis://localhost:6379/0'\r\n\r\nDEBUG: False\r\nINSTALLED_APPS:\r\n    ('django.contrib.auth',\r\n 'django.contrib.contenttypes',\r\n 'django.contrib.sessions',\r\n 'django.contrib.sites',\r\n 'django.contrib.messages',\r\n 'django.contrib.admin',\r\n 'django.contrib.sitemaps',\r\n 'django.contrib.staticfiles',\r\n 'django.contrib.humanize',\r\n 'django.contrib.redirects',\r\n 'django.contrib.gis',\r\n\r\n 'django_extensions',\r\n)\r\n```\r\n## Steps to reproduce\r\n\r\nOn Ubuntu 16.04.\r\n\r\n1. set up celery via systemd according to the celery docs\r\n2. start celery via `sudo service <your-celery-service-name> start`\r\n3. try to reload celery via `sudo service <your-celery-service-name> reload`\r\n\r\n## Expected behavior\r\n\r\nCelery should gracefully reload.\r\n\r\n## Actual behavior\r\n\r\nCelery **does not** gracefully reload. It shuts down, but never restarts. \r\n\r\nAfter issuing the reload request (and subsequent failure to start), the output of `sudo journalctl -xe` doesn't seem to show anything nefarious:\r\n```\r\nNov 15 04:33:04 ip-172-31-44-219 sudo[12972]:   ubuntu : TTY=pts/0 ; PWD=/home/ubuntu ; USER=root ; COMMAND=/usr/sbin/service myproj-celery reload\r\nNov 15 04:33:04 ip-172-31-44-219 sudo[12972]: pam_unix(sudo:session): session opened for user root by ubuntu(uid=0)\r\nNov 15 04:33:04 ip-172-31-44-219 systemd[1]: Reloading myproj celery worker.\r\n-- Subject: Unit myproj-celery.service has begun reloading its configuration\r\n-- Defined-By: systemd\r\n-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel\r\n--\r\n-- Unit myproj-celery.service has begun reloading its configuration\r\nNov 15 04:33:07 ip-172-31-44-219 sh[12979]: celery multi v4.1.0 (latentcall)\r\nNov 15 04:33:07 ip-172-31-44-219 sh[12979]: > Stopping nodes...\r\nNov 15 04:33:07 ip-172-31-44-219 sh[12979]:         > worker1@ip-172-31-44-219: TERM -> 12962\r\nNov 15 04:33:07 ip-172-31-44-219 sh[12979]: > Waiting for 1 node -> 12962.....\r\nNov 15 04:33:07 ip-172-31-44-219 sh[12979]:         > worker1@ip-172-31-44-219: OK\r\nNov 15 04:33:07 ip-172-31-44-219 sh[12979]: > Restarting node worker1@ip-172-31-44-219: OK\r\nNov 15 04:33:07 ip-172-31-44-219 sh[12979]: > Waiting for 1 node -> None...\r\nNov 15 04:33:07 ip-172-31-44-219 sh[12992]: celery multi v4.1.0 (latentcall)\r\nNov 15 04:33:07 ip-172-31-44-219 sh[12992]: > worker1@ip-172-31-44-219: DOWN\r\nNov 15 04:33:07 ip-172-31-44-219 systemd[1]: Reloaded myproj celery worker.\r\n-- Subject: Unit myproj-celery.service has finished reloading its configuration\r\n-- Defined-By: systemd\r\n-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel\r\n--\r\n-- Unit myproj-celery.service has finished reloading its configuration\r\n--\r\n-- The result is done.\r\nNov 15 04:33:07 ip-172-31-44-219 sudo[12972]: pam_unix(sudo:session): session closed for user root\r\n```\r\n\r\nPerhaps `CELERYD_PID_FILE=\"/var/run/celery/%N.pid\"` being unknown is the problem? Meaning, we cannot tell Systemd (via `PIDFile=`) what the actual location of the PID file is, because it's generated by celery itself, no?", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4388/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "georgepsarakis": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4387", "title": "Release version 4.2.0", "body": "I think that it would be a good idea to release a new version that includes all bug fixes so far. @thedrow @auvipy thoughts?\r\n\r\nI can help with the changelog.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4387/reactions", "total_count": 8, "+1": 8, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/issues/4225", "title": "Run tests with dependency master branches", "body": "I believe it would make sense to have an overview of how changes in other projects and\r\nprimarily on Kombu, affect Celery, so issues can be caught early during development on all projects. In order not to mark builds as failed, the [allow_failures](https://docs.travis-ci.com/user/customizing-the-build#Rows-that-are-Allowed-to-Fail) parameter can be used in Travis CI.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4225/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/issues/3964", "title": "Use callable in utils.functional.head_from_fun", "body": "After the discussion in #3952, we should investigate whether an improvement can be applied using the builtin `callable` instead of `hasattr(fun, '__call__')`.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3964/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/a7915054d0e1e896c9ccf5ff0497dd8e3d5ed541", "message": "Integration test to verify PubSub unsubscriptions (#4468)\n\n* [Redis Backend] Integration test to verify PubSub unsubscriptions\r\n\r\n* Import sequence for isort check\r\n\r\n* Re-order integration tasks import"}, {"url": "https://api.github.com/repos/celery/celery/commits/9ab0971fe28462b667895d459d198ef6dd761c89", "message": "Add --diff flag in isort in order to display changes (#4469)"}, {"url": "https://api.github.com/repos/celery/celery/commits/87b263bcea88756d870d19f27af9cb54c6f860cf", "message": "Correct calculation of application current time with timezone (#4173)\n\n* Use datetime.astimezone to adjust current time\r\n\r\n* Remove import statements from Celery.now"}, {"url": "https://api.github.com/repos/celery/celery/commits/c2f1e6c254fdacfea8f57cbf9352a73f763fb83d", "message": "Changelog for version 4.1.0 (#4151)\n\n* Move 4.0 changelog to history, draft changelog 4.1.0\r\n\r\n* Categorize entries and summarize documentation contributors\r\n\r\n* Refine changelog entries\r\n\r\n* Finalize categorization and editing of changes\r\n\r\n* Update filename"}, {"url": "https://api.github.com/repos/celery/celery/commits/81140551dcb8cce9a75e1b75ddb74c20d1e7649d", "message": "Resolve pip UnicodeEncoderError for PyPy environments (#4138)\n\n* Upgrade pip for PyPy environments\r\n\r\n* Specify version in pip upgrade\r\n\r\n* Suppress progress bar"}, {"url": "https://api.github.com/repos/celery/celery/commits/9a5c53ba15f5691b3abe8be9ff888a6d954bf7af", "message": "Resolve issue with Redis integration test in Python3"}, {"url": "https://api.github.com/repos/celery/celery/commits/8c4e6dd8a391d57583a5d7bb636f2e385716775e", "message": "Allow class methods to define tasks"}, {"url": "https://api.github.com/repos/celery/celery/commits/38943e6000bec54b219df1a3de99d17e0b5f01bf", "message": "Add regression test for chain duplication in chords (#3771)"}, {"url": "https://api.github.com/repos/celery/celery/commits/108b36230263cdbd392ceaf62377f88226fa7301", "message": "Allow unicode message for exception raised in task (#3903)"}, {"url": "https://api.github.com/repos/celery/celery/commits/eac96241d2f4d4d2dfe21c071537431518e01718", "message": "Fix task ETA issues when timezone is defined in configuration (#3867)\n\n* Fix wrong task ETA when using timezone setting\r\n\r\n* Optimization: reduce polling interval in test cases\r\n\r\n* Test cases for App.now, App.uses_utc_timezone\r\n\r\n* Fix PEP8 and pydocstyle issues"}, {"url": "https://api.github.com/repos/celery/celery/commits/9c950b47eca2b4e93fd2fe52cf80f158e6cf97ad", "message": "AWS DynamoDB result backend (#3736)\n\n* Add result backend for AWS DynamoDB\r\n\r\n* Dependencies for DynamoDB result backend\r\n\r\n* Add DynamoDB backend in aliases\r\n\r\n* Test cases for DynamoDB result backend\r\n\r\n* Documentation for DynamoDB backend\r\n\r\n* Configurable endpoint URL for DynamoDB local instance\r\n\r\n* Enable integration tests for DynamoDB result backend\r\n\r\n- Run before_install script only for integration environments\r\n\r\n* Fix invalid type error for primary key in Python3\r\n\r\n* Add Python 3.6 in Travis CI build matrix\r\n\r\n- Instruct Travis CI to include Python 3.6 interpreter in jobs\r\n- Optimize Travis CI build matrix\r\n\r\n* Optimize Travis CI build matrix\r\n\r\n* Fix endless loop in logger_isa (Python 3.6)\r\n\r\n* Add test cases for AWS client construction\r\n\r\n- Add/improve log messages during table initialization\r\n- Enable skipped unit tests due to missing dependency boto3\r\n\r\n* Use explicit hash seed value for apicheck tox environment\r\n\r\n- Related Sphinx issue: https://github.com/sphinx-doc/sphinx/issues/2324"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/95074138", "body": "If you allow me a suggestion here, I would propose to organize the shutdown states in a single point:\r\n\r\n```python\r\nshutdown_states = {CLOSE, TERMINATE}\r\nwhile blueprint.state not in shutdown_states:\r\n    ...\r\n    if blueprint.state not in shutdown_states:\r\n```\r\n\r\nI believe that way it will be easier to avoid regressions in the future by modifying the states set in one of the state checks. What do you think?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/95074138/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/96131287", "body": "Can you please explain something here? Since `soft_timeout` is lower than `timeout`:\r\n\r\n```python\r\nwith gevent.Timeout(5, SoftTimeLimitExceeded):\r\n      with gevent.Timeout(6):\r\n            gevent.sleep(5.2)\r\n```\r\n\r\nThe above code block won't always raise `SoftTimeLimitExceeded` and cancel the function execution?\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/96131287/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/99281458", "body": "Perhaps it would be best if you cleared the Redis list, just in case the test is repeated:\r\n\r\n```python\r\nredis_connection.delete('redis-echo')\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/99281458/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/100966060", "body": "I think you need `is` instead of equality check here.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/100966060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/100968420", "body": "You are right, if it is re-instantiated by another process, it will not be the same object. However, how will equality check work for functions in general? I am not aware of any concrete method of performing this check. See also [here](http://stackoverflow.com/questions/20059011/check-if-two-python-functions-are-equal).", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/100968420/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/100971027", "body": "I think there is a way, you can specify the full import path as a string and perform a dynamic import. This project performs lazy imports: https://github.com/Russell91/pythonpy/blob/master/pythonpy/__main__.py\r\n\r\nBy importing the same exact function, you will have consistency on the function objects.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/100971027/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/102548107", "body": "In order to maintain Python2/3 compatibility, I believe it is generally advisable to use the compatibility functions in [five](https://github.com/celery/celery/blob/master/celery/five.py) (which in its turn uses [vine.five](https://github.com/celery/vine/blob/master/vine/five.py)).\r\n\r\nSo, for consistency perhaps it would be better to use `string`:\r\n\r\n```\r\nfrom celery.five import string\r\n\r\ndata = {'_id': string(key), 'value': value}\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/102548107/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108054423", "body": "If `a` and `b` are dictionaries, the order of the keys is not guaranteed to be the same.\r\n\r\nYou can convert each `keys` collection to `set` to compare them correctly.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108054423/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108054683", "body": "The [schedule](https://github.com/celery/celery/blob/master/celery/schedules.py#L171) class has defined the `_eq__` method, so I think you can compare directly the dictionaries `self.old_schedulers` and `self.schedule`, you should not need an additional function to achieve this.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108054683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108054691", "body": "Could you please add a test case verifying the flow for this logical branch?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108054691/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108062627", "body": "Indeed the test you added seems to verify the failure case (tried it with `master`).", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108062627/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108054266", "body": "I believe the first change in this line should be reverted: `\"unknown\".` seems the correct version.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108054266/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108054302", "body": "Perhaps: \r\n- `Make sure the backend is configured correctly`\r\n- `Make sure the backend is initialized correctly`\r\n\r\nWhat do you think?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108054302/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110006584", "body": "For uniformity and readability I think it is preferrable to surround `=` with spaces:\r\n\r\n```\r\nes_retry_on_timeout = False\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110006584/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110007261", "body": "This documentation change belongs to line 1057 (above `.. conf-riak-result-backend:`).", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110007261/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110007600", "body": "If someone sets maximum retries to 0, the setting value will revert to the default. So, perhaps you need to check explicitly for `None`?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110007600/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110522177", "body": "I did not want to interfere with the previous logic here, as generally I believe it is not easy to detect functions correctly.\r\n\r\nFor example, one difference I can think is with old-style classes:\r\n\r\n```\r\nclass A:\r\n    pass\r\n\r\nprint callable(A) # True\r\nprint hasattr(A, '__call__') # False - would be True for new-style classes\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110522177/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110548332", "body": "I think this is a `Task` class attribute so you do not need `getattr`. Also, shouldn't this change be applied to `apply`?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110548332/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110548334", "body": "Interesting. So this only affects backends that implement the `on_task_call` method?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110548334/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110719427", "body": "It is generally more accurate to use object identity test for `True`, `False`, `None`:\r\n\r\n```\r\nassert self.backend.es_retry_on_timeout is True\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110719427/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110719928", "body": "The test is failing because the `ElasticsearchBackend._index` method has not been called.\r\n\r\n```\r\nresult = x._index(...)\r\nassert result == expected_result\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110719928/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110817698", "body": "I think you should convert this object to string in order to take into account this [change](https://github.com/celery/celery/pull/3961/files#diff-731afbc33e1f1197b570c2d5506600d2R109), so that the assertion works:\r\n\r\n```\r\nx._server.index.assert_called_once_with(\r\n    id=string(sentinel.task_id),\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110817698/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110825482", "body": "Since you are updating the configuration and these parameters are set in the constructor, you need to re-initialize the backend instance, so that it re-reads the configuration:\r\n\r\n```\r\nself.backend = ElasticsearchBackend(app=self.app)\r\n```\r\n\r\nThis should resolve the build job errors.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110825482/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/105542228", "body": "Possible typo here: `CERT_REQUIRED` ", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/105542228/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/111878946", "body": "If anyone knows a way to rewrite these `tox` sections in a less repetitive way, please let me know.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/111878946/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/118818159", "body": "I am not sure about this change, could you please explain the rephrasing?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/118818159/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/125725199", "body": "From what I have tested with `_params_from_url` no replacement will occur here, because the SSL options are passed as the `defaults` dictionary. Would you mind explaining why this is required?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/125725199/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/106839684", "body": "This line can be simplified by using a dictionary comprehension (Celery 4 requires Python 2.7 and above). Also, you can avoid creating a new list in Python 2 by using the compatibility function `items` from `five`:\r\n\r\n```\r\nfrom celery.five import items\r\n\r\nbody = {string(key): value for key, value in items(body)}\r\n```\r\n\r\nWhat do you think?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/106839684/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/107010649", "body": "Totally unrelated but since you are using this backend perhaps you can help. Why isn't the `id` argument value passed to `index` here? Shouldn't this be:\r\n\r\n```\r\nreturn self.server.index(\r\n    id=id,\r\n    index=self.index,\r\n    doc_type=self.doc_type,\r\n    body=body,\r\n    **kwargs\r\n)\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/107010649/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/107834800", "body": "I think this is resolved in #3855.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/107834800/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/114056652", "body": "I would suggest to move this function at the top of the file and retain the name `info`, thus keeping the changes to a minimum.\r\nAlso, I think you need to add documentation in the function docstring in order to resolve the Travis CI errors.\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/114056652/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108064474", "body": "I think you should also provide a unit test to verify this.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108064474/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108232513", "body": "Seems right, yes.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108232513/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128912801", "body": "By intuition alone, it seemed more appropriate to not use edge case values (the endpoint) here, as I would like to display that jitter **does** affect the result.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128912801/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128913079", "body": "When I was writing this function, it seemed kind of odd that it should handle both full jitter and custom deviation. After looking at `tenacity` source code and reading the AWS blog post, I thought that perhaps retaining the simple Full Jitter algorithm, was the best choice here.\r\n\r\nFor example if you set a jitter deviation of 1:\r\n- `factor=2, retries=1` yields a countdown of 4 , resulting in a delay in the range `[3, 5]`\r\n- `factor=2, retries=4` yields a countdown of 32 , resulting in a delay in the range `[31, 33]`\r\n\r\nSo in the fourth retry, the jitter deviation is proportionately negligible and you may be actually increasing the possibility of e.g. HTTP calls to a service happening almost at the same time, whereas using a uniform distribution of the delay time of 32, and have let's say 10 concurrent tasks, there is a greater chance that they will be retried at a more timely manner.\r\n\r\n```\r\n# Full jitter\r\n>>> sorted([random.randrange(32) for _ in range(10)])\r\n>>> [7, 15, 16, 20, 22, 23, 25, 27, 29, 31]\r\n# Custom deviation\r\n>>> sorted([32 + random.randrange(2) for _ in range(10)])\r\n>>> [32, 32, 32, 32, 32, 32, 33, 33, 33, 33]\r\n```\r\n\r\nI thought that it would be better to use the simple version for now, and get some user feedback. If you have an actual use case of a service that implements a different algorithm, please let me know.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128913079/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890776", "body": "@auvipy I am still refining these entries.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890776/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128891385", "body": "Do the release names refer to Autechre songs/albums?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128891385/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128891760", "body": "This last section here is not reviewed.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128891760/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128891954", "body": "I haven't categorized or summarized these 3-4 entries here.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128891954/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128891960", "body": "Well comparing to previous releases it seems like it :smile: :\r\n- Cipater\r\n- Chiastic Slide\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128891960/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128921770", "body": "Thanks, updated.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128921770/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128922415", "body": "@thedrow the file https://github.com/celery/celery/blob/master/Changelog at the root of the repository is what you mention. I have modified it accordingly. Unless I didn't understand your comment.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128922415/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128951070", "body": "Yes, the documentation should be updated, I didn't make any changes until we finalized on the structure and settings. If you agree with the changes then we should somehow transfer these to your PR. Please tell me how you would like to proceed.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128951070/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/129365605", "body": "This module contains the implementation of the result backend, so my question is why are the settings for the broker used here?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/129365605/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/125520326", "body": "I think this option is of type integer so perhaps it would be more intuitive for future readers to default it to 0. You can also omit the `if` block below.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/125520326/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/125520419", "body": "Same comment as above, types should probably not be mixed (boolean / integer).", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/125520419/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/125522163", "body": "If you could add another test case that verifies the jitter flow it would be nice!", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/125522163/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/125522171", "body": "The more compact form could potentially be (I think the `if` block is not required):\r\n```\r\ncountdown = retry_backoff * (2 ** task.request.retries) # will be zero if retry_backoff = 0\r\ncountdown += random.randrange(retry_jitter + 1) * random.choice((1, -1)) # if retry_jitter is 0, the result is always zero too\r\n# Adjust according to maximum backoff\r\ncountdown = min(retry_backoff_max, countdown)\r\n```\r\n\r\n- `random.randrange` excludes the endpoint that is why I added `1`. This was also excluding defining `1` as a possible jitter value.\r\n- If `retry_jitter` is 0, `countdown` remains the same.\r\n- If `retry_jitter` is 2, and delay is set to 8, then, possible random values are `[0, 1, 2]`, and `countdown` will be in the range `[6, 10]`.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/125522171/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890049", "body": "Thanks for the changes! Do you mind if I try this change on this branch then?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890049/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/129394158", "body": "The unlimited delay is not supported now, so could you please remove it? I don't believe that this would be desired probably.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/129394158/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/129394477", "body": "Sorry this is actually unused. Could you please remove it? Or I can do it if you allow me changes.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/129394477/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/129394601", "body": "Same as above, the `random.choice` patching is not required.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/129394601/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/123870827", "body": "This is probably a PEP8 error: `, start=`.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/123870827/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/123870842", "body": "What are the alternatives for `self.method` here?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/123870842/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/130021048", "body": "No good reason. Fixed in latest commit, thanks.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/130021048/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133123699", "body": "Out of curiosity, does this mean sending the `SIGSTOP` or `SIGTERM` signal?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133123699/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133509753", "body": "Thanks for looking into this. Do you mind updating this comment to use `send the termination` or `send the SIGTERM`?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133509753/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133523033", "body": "I would suggest to add an assertion on the ID as well.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133523033/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133523201", "body": "Did you happen to locate other instantiations of `GroupResult`? Perhaps the `parent` argument should be added in more calls.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133523201/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133530941", "body": "Sorry, the equality checks for the ID as well. How about this assertion then:\r\n\r\n```\r\nassert second_result == result\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133530941/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133631196", "body": "Sounds right.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133631196/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133631386", "body": "Yes, it seems to be a class variable in `ResultBase`, so I would keep 818, but remove 828 as it is redundant.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133631386/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133806458", "body": "I have done some checking on `as_tuple` and `result_from_tuple` calls as well, I believe the changes do not affect these.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133806458/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133806902", "body": "Perhaps add two simple test cases: one for `__eq__` and one for `as_tuple`. What do you think?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133806902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/136144506", "body": "This is a protected method. You should probably create instead a public method or getter in `ScheduleEntry`.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/136144506/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/136410868", "body": "Since this was a protected method, other people may have already been using it in sub-classes. This was the main reason I suggested that you add a new method or property getter.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/136410868/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/138412963", "body": "Can you please explain the logic here? I cannot figure out the use of `reduce`. It seems that the concept is to have the next task signature become the error callback?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/138412963/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/138696570", "body": "I would go with this version since it is far more readable. Also, I think that using `_with_list_option` is not required.\r\n\r\n```\r\ntasks = [task.clone() for task in self.tasks]\r\nerror_handler = self.options.get('link_error')\r\nif error_handler is not None:   \r\n    for task in tasks:\r\n        task.on_error(error_handler)\r\n```\r\n\r\nWhat do you think?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/138696570/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/138798420", "body": "Glad you agreed with the suggestions :smile: ! One question here, is `link_error` a list of functions? ", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/138798420/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/139233910", "body": "Seems this is a repeated pattern, the more DRY approach would be to use a function for these 5 occurrences.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/139233910/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/139308968", "body": "Would it be more straightforward to disable the `backend` altogether perhaps? See below:\r\n\r\n```\r\nfrom celery.backends.base import DisabledBackend\r\n\r\nself.app.backend = DisabledBackend()\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/139308968/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/140576685", "body": "Just for easier flow understanding I guess you can use `options` instead of `d` here.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/140576685/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/140576973", "body": "There are multiple `from_dict` methods in this module. For example [here](https://github.com/celery/celery/pull/4278/files#diff-e27381cb5c64031dd28aa6855c78cfddR862). Aren't these affected by this issue as well?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/140576973/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/140693682", "body": "The application configuration has already been resolved at this stage, so I think this test case does not actually prove the issue. Configuration cannot be changed dynamically.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/140693682/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132809770", "body": "Implementation and Python2/3 compatibility for the `monotonic` time function is already included in https://github.com/celery/vine/blob/1.0/vine/five.py#L63 , so perhaps it would be better to reuse this?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132809770/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132825611", "body": "@thedrow why do you need it to be relative?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132825611/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/143347073", "body": "@thedrow isn't generating a unique value here the requirement? `monotonic` achieves just that.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/143347073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/138422862", "body": "`AMQP.create_task_message` attempts to be protocol-agnostic, so I am not sure that we should apply a conditional based on protocol version. I would suggest to add an additional variadic keyworded-argument list in the signature of [AMQP.as_task_v1](https://github.com/celery/celery/blob/master/celery/app/amqp.py#L401):\r\n\r\n```\r\ndef as_task_v1(..., **compat_kwargs):\r\n```\r\n\r\nThen both signatures will be able to receive the keyworded arguments `kwargsrepr` & `argsrepr`, but only `as_task_v2` will be actually using them.\r\n\r\nWhat do you think?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/138422862/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/138693890", "body": "Thanks for applying the suggested changes. Although it would work, you don't need `**compat_kwargs` in this signature, so I would recommend to perform as little changes as possible.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/138693890/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/139786498", "body": "I think you do not need to account for `AttributeError` anymore. The failing test case can be re-formatted as follows:\r\n\r\n```\r\ndef test_find_related_module(self):\r\n        with patch('importlib.import_module') as imp:\r\n            imp.return_value = Mock()\r\n            imp.return_value.__path__ = 'foo'\r\n            base.find_related_module(base, 'tasks')\r\n\r\n            imp.side_effect = ImportError()\r\n            base.find_related_module('celery.loaders.base', 'tasks')\r\n\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/139786498/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/150392841", "body": "Could you please update the preceding note about `boto` too? Thanks!", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/150392841/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "jsjohns": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4378", "title": "Unexpected task delay w/ Redis + acks_late", "body": "There is a ~2s delay between execution of tasks when Celery is configured to use Redis as the broker and `acks_late` is enabled.\r\n\r\nThis behavior disappears when either:\r\n\r\n- `task_acks_late` is disabled\r\n- AMQP is used as the transport\r\n- The line `timeout = 0 if timeout and timeout < 0 else round((timeout or 0) * 1e3)` in `kombu.utils.eventio._poll.poll` is changed to `timeout = 0 if timeout and timeout < 0 else round((timeout or 0) * 10)` (i.e. `poll()` timeout reduced)\r\n\r\nI expected that the task execution delay would not be significantly influenced by the choice of broker or the value of `task_acks_late`.\r\n\r\nThis behavior has been tested on macOS 10.13.1 and Ubuntu 16.04 with Celery 4.1.0 and Redis 4.0.1. It can be reproduced with the following scripts:\r\n\r\n`request.py`:\r\n\r\n```\r\nfrom celery import group\r\nfrom worker import noop\r\n\r\ntasks = []\r\nfor i in range(5):\r\n  tasks.append(noop.s(i))\r\n\r\ngroup(tasks).apply_async()\r\n```\r\n\r\n`worker.py`:\r\n\r\n```\r\nfrom celery import Celery\r\nimport settings\r\n\r\napp = Celery()\r\napp.config_from_object(settings)\r\n\r\n@app.task(bind=True)\r\ndef noop(self, input):\r\n    return \"noop: %s\" % input\r\n```\r\n\r\n`settings.py`:\r\n\r\n```\r\nbroker_url = \"redis://\"\r\nworker_prefetch_multiplier = 1\r\ntask_acks_late = True\r\n```\r\n\r\nThe output of a run exhibiting the bad behavior follows (with a few debugging statements added). Notice the timestamps of the lines beginning with \"Received task\"):\r\n\r\n```\r\n$ celery worker -A worker -l info -c 1 \r\n \r\n -------------- celery@io v4.1.0 (latentcall)\r\n---- **** ----- \r\n--- * ***  * -- Darwin-17.2.0-x86_64-i386-64bit 2017-11-10 15:57:38\r\n-- * - **** --- \r\n- ** ---------- [config]\r\n- ** ---------- .> app:         __main__:0x108b71b70\r\n- ** ---------- .> transport:   redis://localhost:6379//\r\n- ** ---------- .> results:     disabled://\r\n- *** --- * --- .> concurrency: 1 (prefork)\r\n-- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\r\n--- ***** ----- \r\n -------------- [queues]\r\n                .> celery           exchange=celery(direct) key=celery\r\n                \r\n\r\n[tasks]\r\n  . worker.noop\r\n\r\n[2017-11-10 15:57:38,397: INFO/MainProcess] Connected to redis://localhost:6379//\r\n[2017-11-10 15:57:38,406: INFO/MainProcess] mingle: searching for neighbors\r\n[2017-11-10 15:57:39,427: INFO/MainProcess] mingle: all alone\r\n[2017-11-10 15:57:39,435: INFO/MainProcess] celery@io ready.\r\n[2017-11-10 15:57:39,436: WARNING/MainProcess] [EVENTS]: on_readable(8)(8)->R, on_result_readable(5)(5)->R, on_readable(15)(15)->R, on_readable(18)(18)->R\r\n[2017-11-10 15:57:40,724: WARNING/MainProcess] [EVENTS]: on_readable(8)(8)->R\r\n[2017-11-10 15:57:40,882: WARNING/MainProcess] [EVENTS]: on_readable(8)(8)->R\r\n[2017-11-10 15:57:40,883: INFO/MainProcess] Received task: worker.noop[950a3898-1eac-42a2-bf4d-e17916b4dbe6]  \r\n[2017-11-10 15:57:40,884: WARNING/MainProcess] [EVENTS]: schedule_writes(4)->W\r\n[2017-11-10 15:57:40,884: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 15:57:40,885: INFO/ForkPoolWorker-1] Task worker.noop[950a3898-1eac-42a2-bf4d-e17916b4dbe6] succeeded in 0.0005565830001614813s: 'noop: 0'\r\n[2017-11-10 15:57:40,886: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 15:57:40,886: WARNING/MainProcess] -> acknowledge()\r\n[2017-11-10 15:57:40,886: WARNING/MainProcess] <- acknowledge()\r\n[2017-11-10 15:57:41,432: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 15:57:41,434: WARNING/MainProcess] [EVENTS]: on_readable(8)(8)->R, on_readable(15)(15)->R\r\n[2017-11-10 15:57:41,435: INFO/MainProcess] Received task: worker.noop[655bacca-03fc-4bc5-93aa-28cc47683705]  \r\n[2017-11-10 15:57:41,435: WARNING/MainProcess] [EVENTS]: schedule_writes(4)->W\r\n[2017-11-10 15:57:41,436: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 15:57:41,436: INFO/ForkPoolWorker-1] Task worker.noop[655bacca-03fc-4bc5-93aa-28cc47683705] succeeded in 8.905499998945743e-05s: 'noop: 1'\r\n[2017-11-10 15:57:41,437: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 15:57:41,437: WARNING/MainProcess] -> acknowledge()\r\n[2017-11-10 15:57:41,437: WARNING/MainProcess] <- acknowledge()\r\n[2017-11-10 15:57:43,434: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 15:57:43,436: WARNING/MainProcess] [EVENTS]: on_readable(8)(8)->R, on_readable(15)(15)->R\r\n[2017-11-10 15:57:43,437: INFO/MainProcess] Received task: worker.noop[4f1036b3-288d-46e3-8465-76101e8667f0]  \r\n[2017-11-10 15:57:43,438: WARNING/MainProcess] [EVENTS]: schedule_writes(4)->W\r\n[2017-11-10 15:57:43,438: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 15:57:43,438: INFO/ForkPoolWorker-1] Task worker.noop[4f1036b3-288d-46e3-8465-76101e8667f0] succeeded in 0.00012072099980287021s: 'noop: 2'\r\n[2017-11-10 15:57:43,439: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 15:57:43,439: WARNING/MainProcess] -> acknowledge()\r\n[2017-11-10 15:57:43,439: WARNING/MainProcess] <- acknowledge()\r\n[2017-11-10 15:57:44,428: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 15:57:44,429: WARNING/MainProcess] [EVENTS]: on_readable(8)(8)->R\r\n[2017-11-10 15:57:44,430: INFO/MainProcess] Received task: worker.noop[58c1635d-2460-45e8-bd65-3b693b1234e0]  \r\n[2017-11-10 15:57:44,431: WARNING/MainProcess] [EVENTS]: schedule_writes(4)->W\r\n[2017-11-10 15:57:44,431: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 15:57:44,431: INFO/ForkPoolWorker-1] Task worker.noop[58c1635d-2460-45e8-bd65-3b693b1234e0] succeeded in 0.00012039100010952097s: 'noop: 3'\r\n[2017-11-10 15:57:44,432: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 15:57:44,432: WARNING/MainProcess] -> acknowledge()\r\n[2017-11-10 15:57:44,432: WARNING/MainProcess] <- acknowledge()\r\n[2017-11-10 15:57:44,435: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 15:57:44,436: WARNING/MainProcess] [EVENTS]: on_readable(8)(8)->R\r\n[2017-11-10 15:57:44,437: INFO/MainProcess] Received task: worker.noop[0adbc12e-d7aa-46fc-8ac3-f077560ff132]  \r\n[2017-11-10 15:57:44,438: WARNING/MainProcess] [EVENTS]: schedule_writes(4)->W\r\n[2017-11-10 15:57:44,438: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 15:57:44,438: INFO/ForkPoolWorker-1] Task worker.noop[0adbc12e-d7aa-46fc-8ac3-f077560ff132] succeeded in 9.108599988394417e-05s: 'noop: 4'\r\n[2017-11-10 15:57:44,439: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 15:57:44,439: WARNING/MainProcess] -> acknowledge()\r\n[2017-11-10 15:57:44,439: WARNING/MainProcess] <- acknowledge()\r\n[2017-11-10 15:57:45,440: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 15:57:45,441: WARNING/MainProcess] [EVENTS]: on_readable(15)(15)->R\r\n[2017-11-10 15:57:46,471: WARNING/MainProcess] [EVENTS]: on_readable(8)(8)->R\r\n[2017-11-10 15:57:47,446: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 15:57:47,447: WARNING/MainProcess] [EVENTS]: on_readable(15)(15)->R\r\n[2017-11-10 15:57:47,794: WARNING/MainProcess] [EVENTS]: on_readable(8)(8)->R\r\n[2017-11-10 15:57:48,398: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 15:57:48,816: WARNING/MainProcess] [EVENTS]: on_readable(8)(8)->R\r\n[2017-11-10 15:57:49,433: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 15:57:49,437: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 15:57:49,448: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 15:57:49,449: WARNING/MainProcess] [EVENTS]: on_readable(15)(15)->R\r\n```\r\n\r\nWith the following `settings.py` (`task_acks_late` disabled)...\r\n\r\n```\r\nbroker_url = \"redis://\"\r\nworker_prefetch_multiplier = 1\r\ntask_acks_late = False\r\n```\r\n\r\n...the behavior is as follows (good):\r\n\r\n```\r\n$ celery worker -A worker -l info -c 1 \r\n \r\n -------------- celery@io v4.1.0 (latentcall)\r\n---- **** ----- \r\n--- * ***  * -- Darwin-17.2.0-x86_64-i386-64bit 2017-11-10 16:02:21\r\n-- * - **** --- \r\n- ** ---------- [config]\r\n- ** ---------- .> app:         __main__:0x10f222be0\r\n- ** ---------- .> transport:   redis://localhost:6379//\r\n- ** ---------- .> results:     disabled://\r\n- *** --- * --- .> concurrency: 1 (prefork)\r\n-- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\r\n--- ***** ----- \r\n -------------- [queues]\r\n                .> celery           exchange=celery(direct) key=celery\r\n                \r\n\r\n[tasks]\r\n  . worker.noop\r\n\r\n[2017-11-10 16:02:21,817: INFO/MainProcess] Connected to redis://localhost:6379//\r\n[2017-11-10 16:02:21,826: INFO/MainProcess] mingle: searching for neighbors\r\n[2017-11-10 16:02:22,849: INFO/MainProcess] mingle: all alone\r\n[2017-11-10 16:02:22,857: INFO/MainProcess] celery@io ready.\r\n[2017-11-10 16:02:22,858: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R, on_readable(18)(18)->R, on_readable(16)(16)->R\r\n[2017-11-10 16:02:23,010: WARNING/MainProcess] [EVENTS]: on_readable(8)(8)->R\r\n[2017-11-10 16:02:23,829: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 16:02:23,831: WARNING/MainProcess] [EVENTS]: on_readable(18)(18)->R\r\n[2017-11-10 16:02:24,345: WARNING/MainProcess] [EVENTS]: on_readable(8)(8)->R\r\n[2017-11-10 16:02:24,471: WARNING/MainProcess] [EVENTS]: on_readable(8)(8)->R\r\n[2017-11-10 16:02:24,472: INFO/MainProcess] Received task: worker.noop[8fce37e5-2faa-4c09-a213-b6fd4392e6e7]  \r\n[2017-11-10 16:02:24,473: WARNING/MainProcess] [EVENTS]: schedule_writes(4)->W\r\n[2017-11-10 16:02:24,474: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:02:24,474: WARNING/MainProcess] -> acknowledge()\r\n[2017-11-10 16:02:24,474: WARNING/MainProcess] <- acknowledge()\r\n[2017-11-10 16:02:24,475: INFO/ForkPoolWorker-1] Task worker.noop[8fce37e5-2faa-4c09-a213-b6fd4392e6e7] succeeded in 0.0006347779999487102s: 'noop: 0'\r\n[2017-11-10 16:02:24,476: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:02:24,476: WARNING/MainProcess] [EVENTS]: on_readable(8)(8)->R\r\n[2017-11-10 16:02:24,476: INFO/MainProcess] Received task: worker.noop[50aa07bf-59ce-4aa3-83f1-a175be3645d6]  \r\n[2017-11-10 16:02:24,477: WARNING/MainProcess] [EVENTS]: schedule_writes(4)->W\r\n[2017-11-10 16:02:24,477: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:02:24,477: WARNING/MainProcess] -> acknowledge()\r\n[2017-11-10 16:02:24,477: INFO/ForkPoolWorker-1] Task worker.noop[50aa07bf-59ce-4aa3-83f1-a175be3645d6] succeeded in 5.139399945619516e-05s: 'noop: 1'\r\n[2017-11-10 16:02:24,477: WARNING/MainProcess] <- acknowledge()\r\n[2017-11-10 16:02:24,478: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:02:24,478: WARNING/MainProcess] [EVENTS]: on_readable(8)(8)->R\r\n[2017-11-10 16:02:24,479: INFO/MainProcess] Received task: worker.noop[4b15600c-73af-4050-bd70-cf3275837663]  \r\n[2017-11-10 16:02:24,479: WARNING/MainProcess] [EVENTS]: schedule_writes(4)->W\r\n[2017-11-10 16:02:24,479: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:02:24,479: WARNING/MainProcess] -> acknowledge()\r\n[2017-11-10 16:02:24,479: WARNING/MainProcess] <- acknowledge()\r\n[2017-11-10 16:02:24,479: INFO/ForkPoolWorker-1] Task worker.noop[4b15600c-73af-4050-bd70-cf3275837663] succeeded in 7.287799962796271e-05s: 'noop: 2'\r\n[2017-11-10 16:02:24,480: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:02:24,480: WARNING/MainProcess] [EVENTS]: on_readable(8)(8)->R\r\n[2017-11-10 16:02:24,481: INFO/MainProcess] Received task: worker.noop[2af967de-e6e1-4a18-a124-433ec831f6c0]  \r\n[2017-11-10 16:02:24,481: WARNING/MainProcess] [EVENTS]: schedule_writes(4)->W\r\n[2017-11-10 16:02:24,481: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:02:24,481: WARNING/MainProcess] -> acknowledge()\r\n[2017-11-10 16:02:24,481: INFO/ForkPoolWorker-1] Task worker.noop[2af967de-e6e1-4a18-a124-433ec831f6c0] succeeded in 4.8488000174984336e-05s: 'noop: 3'\r\n[2017-11-10 16:02:24,481: WARNING/MainProcess] <- acknowledge()\r\n[2017-11-10 16:02:24,482: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:02:24,482: WARNING/MainProcess] [EVENTS]: on_readable(8)(8)->R\r\n[2017-11-10 16:02:24,483: INFO/MainProcess] Received task: worker.noop[fdd33e95-ab15-41cc-ae18-3fd7de2d4c22]  \r\n[2017-11-10 16:02:24,483: WARNING/MainProcess] [EVENTS]: schedule_writes(4)->W\r\n[2017-11-10 16:02:24,483: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:02:24,483: WARNING/MainProcess] -> acknowledge()\r\n[2017-11-10 16:02:24,483: WARNING/MainProcess] <- acknowledge()\r\n[2017-11-10 16:02:24,483: INFO/ForkPoolWorker-1] Task worker.noop[fdd33e95-ab15-41cc-ae18-3fd7de2d4c22] succeeded in 5.3837999985262286e-05s: 'noop: 4'\r\n[2017-11-10 16:02:24,484: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:02:25,762: WARNING/MainProcess] [EVENTS]: on_readable(8)(8)->R\r\n[2017-11-10 16:02:25,834: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 16:02:25,836: WARNING/MainProcess] [EVENTS]: on_readable(18)(18)->R\r\n```\r\n\r\nWith the following `settings.py` (AMPQ transport)...\r\n\r\n```\r\nbroker_url = \"ampq://\"\r\nworker_prefetch_multiplier = 1\r\ntask_acks_late = True\r\n```\r\n\r\n...the behavior is as follows (also good):\r\n\r\n```\r\n$ celery worker -A worker -l info -c 1 \r\n \r\n -------------- celery@io v4.1.0 (latentcall)\r\n---- **** ----- \r\n--- * ***  * -- Darwin-17.2.0-x86_64-i386-64bit 2017-11-10 16:01:05\r\n-- * - **** --- \r\n- ** ---------- [config]\r\n- ** ---------- .> app:         __main__:0x10ca54ba8\r\n- ** ---------- .> transport:   amqp://guest:**@localhost:5672//\r\n- ** ---------- .> results:     disabled://\r\n- *** --- * --- .> concurrency: 1 (prefork)\r\n-- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\r\n--- ***** ----- \r\n -------------- [queues]\r\n                .> celery           exchange=celery(direct) key=celery\r\n                \r\n\r\n[tasks]\r\n  . worker.noop\r\n\r\n[2017-11-10 16:01:05,908: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672//\r\n[2017-11-10 16:01:05,918: INFO/MainProcess] mingle: searching for neighbors\r\n[2017-11-10 16:01:06,942: INFO/MainProcess] mingle: all alone\r\n[2017-11-10 16:01:06,956: INFO/MainProcess] celery@io ready.\r\n[2017-11-10 16:01:07,061: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:01:08,959: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 16:01:08,961: WARNING/MainProcess] [EVENTS]: on_readable(<kombu.transport.pyamqp.Connection object at 0x10cd42748>, <Hub@0x10cc904e0: R:3 W:0>)(8)->R\r\n[2017-11-10 16:01:10,961: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 16:01:10,963: WARNING/MainProcess] [EVENTS]: on_readable(<kombu.transport.pyamqp.Connection object at 0x10cd42748>, <Hub@0x10cc904e0: R:3 W:0>)(8)->R\r\n[2017-11-10 16:01:11,948: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 16:01:11,958: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 16:01:12,968: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 16:01:12,970: WARNING/MainProcess] [EVENTS]: on_readable(<kombu.transport.pyamqp.Connection object at 0x10cd42748>, <Hub@0x10cc904e0: R:3 W:0>)(8)->R\r\n[2017-11-10 16:01:13,669: WARNING/MainProcess] [EVENTS]: on_readable(<kombu.transport.pyamqp.Connection object at 0x10cd42748>, <Hub@0x10cc904e0: R:3 W:0>)(8)->R\r\n[2017-11-10 16:01:13,669: INFO/MainProcess] Received task: worker.noop[61d56c61-3826-4eae-9ed5-deb21dda7873]  \r\n[2017-11-10 16:01:13,670: WARNING/MainProcess] [EVENTS]: schedule_writes(4)->W\r\n[2017-11-10 16:01:13,670: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:01:13,671: INFO/ForkPoolWorker-1] Task worker.noop[61d56c61-3826-4eae-9ed5-deb21dda7873] succeeded in 0.0006549120007548481s: 'noop: 0'\r\n[2017-11-10 16:01:13,672: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:01:13,672: WARNING/MainProcess] -> acknowledge()\r\n[2017-11-10 16:01:13,672: WARNING/MainProcess] <- acknowledge()\r\n[2017-11-10 16:01:13,673: WARNING/MainProcess] [EVENTS]: on_readable(<kombu.transport.pyamqp.Connection object at 0x10cd42748>, <Hub@0x10cc904e0: R:3 W:0>)(8)->R\r\n[2017-11-10 16:01:13,673: INFO/MainProcess] Received task: worker.noop[78f0e1ca-aaf9-49e9-9e2a-4eb76ebe69d5]  \r\n[2017-11-10 16:01:13,674: WARNING/MainProcess] [EVENTS]: schedule_writes(4)->W\r\n[2017-11-10 16:01:13,674: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:01:13,674: INFO/ForkPoolWorker-1] Task worker.noop[78f0e1ca-aaf9-49e9-9e2a-4eb76ebe69d5] succeeded in 6.282100002863444e-05s: 'noop: 1'\r\n[2017-11-10 16:01:13,674: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:01:13,674: WARNING/MainProcess] -> acknowledge()\r\n[2017-11-10 16:01:13,675: WARNING/MainProcess] <- acknowledge()\r\n[2017-11-10 16:01:13,675: WARNING/MainProcess] [EVENTS]: on_readable(<kombu.transport.pyamqp.Connection object at 0x10cd42748>, <Hub@0x10cc904e0: R:3 W:0>)(8)->R\r\n[2017-11-10 16:01:13,675: INFO/MainProcess] Received task: worker.noop[da3c8078-b593-45ae-971b-ad24475ef392]  \r\n[2017-11-10 16:01:13,676: WARNING/MainProcess] [EVENTS]: schedule_writes(4)->W\r\n[2017-11-10 16:01:13,676: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:01:13,676: INFO/ForkPoolWorker-1] Task worker.noop[da3c8078-b593-45ae-971b-ad24475ef392] succeeded in 8.325599992531352e-05s: 'noop: 2'\r\n[2017-11-10 16:01:13,677: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:01:13,677: WARNING/MainProcess] -> acknowledge()\r\n[2017-11-10 16:01:13,677: WARNING/MainProcess] <- acknowledge()\r\n[2017-11-10 16:01:13,678: WARNING/MainProcess] [EVENTS]: on_readable(<kombu.transport.pyamqp.Connection object at 0x10cd42748>, <Hub@0x10cc904e0: R:3 W:0>)(8)->R\r\n[2017-11-10 16:01:13,679: INFO/MainProcess] Received task: worker.noop[24b7b946-5151-4268-869e-4c22a4d4d556]  \r\n[2017-11-10 16:01:13,679: WARNING/MainProcess] [EVENTS]: schedule_writes(4)->W\r\n[2017-11-10 16:01:13,679: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:01:13,679: INFO/ForkPoolWorker-1] Task worker.noop[24b7b946-5151-4268-869e-4c22a4d4d556] succeeded in 5.8243000239599496e-05s: 'noop: 3'\r\n[2017-11-10 16:01:13,680: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:01:13,680: WARNING/MainProcess] -> acknowledge()\r\n[2017-11-10 16:01:13,680: WARNING/MainProcess] <- acknowledge()\r\n[2017-11-10 16:01:13,681: WARNING/MainProcess] [EVENTS]: on_readable(<kombu.transport.pyamqp.Connection object at 0x10cd42748>, <Hub@0x10cc904e0: R:3 W:0>)(8)->R\r\n[2017-11-10 16:01:13,681: INFO/MainProcess] Received task: worker.noop[95f23dd8-0b0b-42b7-9ee0-2a6e40dc3bd4]  \r\n[2017-11-10 16:01:13,681: WARNING/MainProcess] [EVENTS]: schedule_writes(4)->W\r\n[2017-11-10 16:01:13,681: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:01:13,681: INFO/ForkPoolWorker-1] Task worker.noop[95f23dd8-0b0b-42b7-9ee0-2a6e40dc3bd4] succeeded in 4.8175999836530536e-05s: 'noop: 4'\r\n[2017-11-10 16:01:13,682: WARNING/MainProcess] [EVENTS]: on_result_readable(5)(5)->R\r\n[2017-11-10 16:01:13,682: WARNING/MainProcess] -> acknowledge()\r\n[2017-11-10 16:01:13,682: WARNING/MainProcess] <- acknowledge()\r\n[2017-11-10 16:01:14,971: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 16:01:14,972: WARNING/MainProcess] [EVENTS]: on_readable(<kombu.transport.pyamqp.Connection object at 0x10cd42748>, <Hub@0x10cc904e0: R:3 W:0>)(8)->R\r\n[2017-11-10 16:01:16,952: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 16:01:16,960: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 16:01:16,975: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 16:01:16,977: WARNING/MainProcess] [EVENTS]: on_readable(<kombu.transport.pyamqp.Connection object at 0x10cd42748>, <Hub@0x10cc904e0: R:3 W:0>)(8)->R\r\n[2017-11-10 16:01:18,980: WARNING/MainProcess] [EVENTS]:\r\n[2017-11-10 16:01:18,981: WARNING/MainProcess] [EVENTS]: on_readable(<kombu.transport.pyamqp.Connection object at 0x10cd42748>, <Hub@0x10cc904e0: R:3 W:0>)(8)->R\r\n```\r\n\r\nReport below:\r\n\r\n```\r\ncelery -A worker report\r\n\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.3\r\n            billiard:3.5.0.3 redis:2.10.6\r\nplatform -> system:Darwin arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:disabled\r\n\r\nbroker_url: 'redis://localhost:6379//'\r\ntask_acks_late: False\r\nworker_prefetch_multiplier: 1\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4378/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stevenwbe": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4377", "title": "Using a class based task as errback results in AttributeError '_header_'", "body": "## Checklist\r\n\r\n- [X] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n[celery-report.txt](https://github.com/celery/celery/files/1456808/celery-report.txt)\r\n- [ ] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nCreate a tasks file:\r\n```\r\nfrom celery import Celery,Task\r\ncelery = Celery('tasks', broker='redis://localhost:6379/6',backend='redis://localhost:6379/6')\r\n\r\n@celery.task\r\ndef ErrTask1(req):\r\n    print('failure')\r\n\r\nclass CustomTask(Task):\r\n    def run(self,var):\r\n        print('running %s' % var)\r\n        if var == \"err\":\r\n            raise Exception(\"err\")\r\nCustomTask = celery.register_task(CustomTask())\r\n\r\nclass ErrTask2(Task):\r\n        def run(self):\r\n                    print('failure')\r\nErrTask2 = celery.register_task(ErrTask2())\r\n\r\n```\r\nstart celery with it:\r\n`celery worker -A tasks:celery -l info`\r\n\r\nStart a python shell and start the CustomTask with an error callback:\r\n\r\n```\r\nfrom tasks import CustomTask,ErrTask1,ErrTask2\r\na = CustomTask.apply_async(['err'],link_error=ErrTask1.s())\r\na.result\r\nb = CustomTask.apply_async(['err'],link_error=ErrTask2.s())\r\nb.result\r\n```\r\n## Expected behavior\r\nWhen running with ErrTask1 as error callback ( the one with the decorator) we get the expected result:\r\n```\r\n>>> a = CustomTask.apply_async(['err'],link_error=ErrTask1.s())\r\n>>> a.result\r\nException(u'err',)\r\n```\r\n## Actual behavior\r\nWhen running with ErrTask2 as error callback ( the one that is defined classbased) we get the Attributeerror:\r\n```\r\n>>> b = CustomTask.apply_async(['err'],link_error=ErrTask2.s())\r\n>>> b.result\r\nAttributeError(u\"'ErrTask2' object has no attribute '__header__'\",)\r\n```\r\nIt seems the changed behaviour on the error callback in celery 4 has something to do with it. The class based tasks miss some attributes which doesn't seem to be an issue for normal execution but only for the error callback. Probably there is a difference in initialization between `celery.register_task()` and the decorator.\r\n\r\nWorkaround is to define the error callbacks with the task decorator. ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4377/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shazoom": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4376", "title": "Closing down and restarting event monitoring is buggy", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery \r\n\r\nI installed in a virtualenv using on PiPy (but put celery last to make sure it didn't automatically fulfill dependencies):\r\n\r\n     $ pip install https://github.com/celery/billiard/zipball/master#egg=billiard\r\n     $ pip install https://github.com/celery/py-amqp/zipball/master#egg=amqp\r\n     $ pip install https://github.com/celery/kombu/zipball/master#egg=kombu\r\n     $ pip install https://github.com/celery/vine/zipball/master#egg=vine\r\n     $ pip install https://github.com/celery/celery/zipball/master#egg=celery\r\n\r\n## Steps to reproduce\r\nRun the attached file with interactive mode in the Python shell (-i) and run the function test(). \r\n\r\n### Further information\r\nStopping and starting events in this way almost works.  Run `othertest()`, from cel_events_test.py, 3 times and it works 2 out of 3 times. **The second run will hang (^c it and the third run will work) when the job is submitted to the queue** (this is the problem). I added some hacky sleeps to allow time for the jobs to execute but it should demonstrate the problem.\r\n\r\n[cel_events_test.py.zip](https://github.com/celery/celery/files/1456371/cel_events_test.py.zip)\r\n\r\n        from time import sleep\r\n        from multiprocessing import Process, Manager\r\n        from celery import Celery\r\n\r\n        app = Celery(backend='rpc://', broker='amqp://')\r\n\r\n        @app.task(name='primesOf')\r\n        def primesOf(x):\r\n            multiples = []\r\n            results = []\r\n            for i in range(2, x + 1):\r\n                if i not in multiples:\r\n                    results.append(i)\r\n                    for j in range(i * i, x + 1, i):\r\n                        multiples.append(j)\r\n            return results\r\n\r\n\r\n        class Monitor():\r\n\r\n            def __init__(self, app, event_type):\r\n                self.app = app\r\n                self.event_type = event_type\r\n\r\n            def __enter__(self):\r\n                print('setting up monitor %s' % self.event_type)\r\n                self.monitor, self.changes = self.new_monitor(self.app, self.event_type)\r\n                print('done with %s' % self.event_type)\r\n\r\n            def __exit__(self, *args):\r\n                print('terminating monitor: %s' % self.event_type)\r\n                self.monitor.terminate()\r\n\r\n            @staticmethod\r\n            def mkMon(app, shared_list, event_type):\r\n                state = app.events.State()\r\n\r\n                def announce(event):\r\n                    state.event(event)\r\n                    job = state.tasks.get(event['uuid'])\r\n                    print('%s: %s[%s] %s' % (event_type, job.name, job.uuid, job.info(),))\r\n                    shared_list.append(job.uuid)\r\n\r\n                with app.connection() as conn:\r\n                    recv_succeeded = app.events.Receiver(conn, handlers={\r\n                        event_type: announce,\r\n                        '*': state.event,\r\n                    })\r\n                    recv_succeeded.capture(limit=None, timeout=None, wakeup=True)\r\n\r\n            @classmethod\r\n            def new_monitor(cls, app, event_type):\r\n                manager = Manager()\r\n                shared_list = manager.list()\r\n                mon_process = Process(target=cls.mkMon, args=(app, shared_list, event_type))\r\n                mon_process.start()\r\n                return mon_process, shared_list\r\n\r\n\r\n        def test():\r\n            'This one shows the broken behaviour'\r\n\r\n            with Monitor(app, 'task-succeeded') as smon, Monitor(app, 'task-failed') as fmon, Monitor(app, 'task-started') as tmon, Monitor(app, 'task-received') as rmon:\r\n                print('waiting for monitors to setup')\r\n                sleep(3)\r\n                print('submitting jobs')\r\n                jobs = [primesOf.apply_async((x,)) for x in range(9)]\r\n                sleep(3)\r\n\r\n            print('\\n\\nnow test submitting a job')\r\n            j = primesOf.delay(10)\r\n            sleep(3)\r\n            print(j.result)\r\n\r\n            print('\\n\\nSetup event monitors again')\r\n            with Monitor(app, 'task-succeeded') as smon, Monitor(app, 'task-failed') as fmon, Monitor(app, 'task-started') as tmon, Monitor(app, 'task-received') as rmon:\r\n                print('submitting a job again (this call to delay won\\'t return)')\r\n                j = primesOf.delay(10)\r\n                sleep(3)\r\n                print(j.result)\r\n\r\n\r\n        def otherTest():\r\n            'Run this 3 times. The second run will hang; ^c it and the third run will work.'\r\n\r\n            with Monitor(app, 'task-succeeded') as smon, Monitor(app, 'task-failed') as fmon, Monitor(app, 'task-started') as tmon, Monitor(app, 'task-received') as rmon:\r\n                print('waiting for monitors to setup')\r\n                sleep(3)\r\n                print('submitting jobs')\r\n                jobs = [primesOf.apply_async((x,)) for x in range(9)]\r\n                sleep(3)\r\n\r\n## Expected behavior\r\nnote: this refers to the `test()` function in the linked file: cel_events_test.py)\r\n1. create event monitors running in separate `multiprocessing.Process` objects and wrapped in a context manager\r\n2. submit several jobs to an event queue\r\n3. the events will be printed to the shell\r\n3. exit the `with` block and the context managers will `terminate()` the processes monitoring celery events\r\n4. submit a job to the queue\r\n5. print the result\r\n5. create a new set of event monitors as before\r\n6. submit another job to the queue\r\n7. the events will be printed to the shell\r\n7. print the result\r\n\r\n## Actual behavior \r\nnote: this refers to the `test()` function in the linked file: cel_events_test.py)\r\n1. create event monitors running in separate `multiprocessing.Process` objects and wrapped in a context manager\r\n2. submit several jobs to an event queue\r\n3. the events will be printed to the shell\r\n3. exit the `with` block and the context managers will `terminate()` the processes monitoring celery events\r\n4. submit a job to the queue\r\n5. print the result\r\n5. create a new set of event monitors as before\r\n6. submit another job to the queue\r\n6. nothing (it hangs)\r\n\r\n## Report Output\r\n    $celery -A cel_events_test report\r\n\r\n    software -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.5.2 billiard:3.5.0.3 py-amqp:2.2.2\r\n    platform -> system:Linux arch:64bit, ELF imp:CPython\r\n    loader   -> celery.loaders.app.AppLoader\r\n    settings -> transport:amqp results:rpc:///\r\n\r\n    result_backend: 'rpc:///'\r\n    broker_url: 'amqp://guest:********@localhost:5672//'\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4376/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "RootLUG": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4375", "title": "Celery consumer ignores can_consume in tokenbucket implementation", "body": "software -> celery:4.1.0 (latentcall) kombu:4.1.0 py:2.7.14\r\n            billiard:3.5.0.3 redis:2.10.6\r\nplatform -> system:Linux arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis://:**@<redis_ip>/7\r\n\r\ntask_serializer: 'json'\r\nbroker_transport_options: {\r\n    'fanout_patterns': True, 'fanout_prefix': True, 'visibility_timeout': 18000}\r\nresult_serializer: 'json'\r\nworker_max_tasks_per_child: 5\r\nbroker_url: u'redis://:********@<redis_ip>:6379/7'\r\ntask_soft_time_limit: 36000\r\naccept_content: ['json']\r\nresult_expires: 18000\r\ntask_default_queue: 'default'\r\nresult_backend: u'redis://:********@<redis_ip>/7'\r\n\r\n\r\n## Steps to reproduce\r\n\r\nCreate a custom TokenBucket for rate limiting tasks that depends when `False` is returned in `bucket.can_consume`. For example a token bucket that implements singleton concurrency for a task that returns False until previous task is completed.\r\n\r\n## Expected behavior\r\nwhen bucket's function `can_consume` returns False, task should not be queued for execution using `bucket.add` function but instead `can_consume` should be retried until it returns `True`. According to docs, when `can_consume` returns True it will also consume token, otherwise no action is taken and False is returned. Celery consumer completely ignores this facts which may result in unexpected behavior when other then default token bucket is used.\r\n\r\nAlternatively for better support of custom tokenbuckets/rate limiters for tasks, scheduling of task should not depend on expected_time as that put restriction that it would be near impossible to implement other rate limiting bucket that are not time depended. (for example singleton concurrency queue as mentioned above, at least not with bunch of ugly hacks). A value of `None` in `bucket.expected_time` could be used to ignore the expected time of bucket refill if the rate limiting bucket is not time based.\r\n\r\nAnother way to resolve this issue can be to enclose `bucket.add` into try except block so the bucket can deny `add` of a task into the queue for processing by raising an exception. This would allow then easy implementation of token bucket that put restriction on concurrency per task or some other properties.\r\n\r\n## Actual behavior\r\nCelery `_schedule_bucket_request` under `celery/worker/consumer/consumer.py` is completely ignoring when function `bucket.can_consume` will return \"False\" and task is scheduled using `expected_time` in this case for execution anyway using bucket.add to add it into the queue", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4375/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "SreejeshPM": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4373", "title": "How to restart a celery worker in development with out \"Ready delay\".", "body": "I have changes in Task.py which i am changing/developing frequently, so i will be restarting my celery worker many times to reflect the code changes. The problem is every time i have a code change worker is taking around 15-16 min to get ready. I tried restarting with --purge command and,changed  to a new redis queue. Even tried reinstalling Celery.\r\nMy Workenvironment : \r\n\r\ncelery (4.0.2)\r\nPython 2.7\r\nredis (2.10.5)\r\n\r\nPlease help its extremly time consuming with everyline of code i change. \r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4373/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "greenfrog82": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4372", "title": "When LOGGING key exists in setting.py, celeryd can't restart", "body": "Hi, I'm user celery 3.x\r\nI'm using celery as daemon mode with Django application.\r\nI added LOGGING key into settings.py of Django. \r\nThe owner of log file path which is LOGGING key is apache. \r\nAnd then I restart celeryd as below command.\r\n\r\n```\r\n$ /etc/init.d/celeryd restart\r\n```\r\nBut I saw the below message.\r\n\r\n![image](https://user-images.githubusercontent.com/22334078/32540683-c5125d12-c4b0-11e7-86ce-da600c42ba77.png)\r\n\r\nThe reason of the above image is because owner of log file path. The owner of log file path is apache and celery process can't access the log file.  First, I resolved this problem how I changed access permission of log file to 666 using chown. But this is not best practice which manage log file.\r\n\r\nDoes celery using LOGGING key from settings.py?\r\nIf celery use LOGGING key, how does celery use LOGGING key? and what is LOGGING key for and How to prevent to use LOGGING key in celery?\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4372/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Twista": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4363", "title": "AttributeError 'list' object has no attribute 'decode' with redis backend", "body": "## Checklist\r\n\r\n- [X] I have included the output of ``celery -A proj report`` in the issue.\r\n```\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.5.2\r\n            billiard:3.5.0.3 redis:2.10.5\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis://:**@****************\r\n\r\ntask_ignore_result: True\r\naccept_content: {'pickle'}\r\nresult_serializer: 'pickle'\r\nresult_backend: 'redis://:********@************************'\r\ntask_serializer: 'pickle'\r\ntask_send_sent_event: True\r\nbroker_url: 'redis://:********@************************'\r\n```\r\nredis-server version, both 2.x and 3.x\r\n\r\n## Steps to reproduce\r\n\r\nHello, i'm not sure what can cause the problems and already tried to find a simillar solution, but no luck so far. Therefore opening issue here, hopefully it helps\r\n\r\nThe issue is described there as well (not by me): https://github.com/andymccurdy/redis-py/issues/612\r\n\r\nSo far the experience is, its happen in both cases, where backend is and isn't involved (means just when calling `apply_async(...)`)\r\n\r\nException when calling `apply_async()`\r\n![attributeerror___list__object_has_no_attribute__decode_](https://user-images.githubusercontent.com/1297511/32371218-57760bf6-c090-11e7-9fe5-d41f55d80721.png)\r\n\r\nException when calling `.get()` (also this one has int, instead of list)\r\n![attributeerror___list__object_has_no_attribute__decode_](https://user-images.githubusercontent.com/1297511/32371277-8fd3fbfc-c090-11e7-8eae-ad3e0d5f3ced.png)\r\n\r\nHope it helps\r\n\r\n## Expected behavior\r\n\r\nTo not throw the error.\r\n\r\n## Actual behavior\r\n\r\nAttributeError: 'list' object has no attribute 'decode'\r\n\r\n\r\nThanks!", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4363/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chrisspen": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4359", "title": "Celery ignores BROKER_URL vhost", "body": "## Steps to reproduce\r\n1. Create an arbitrary Celery task.\r\n2. Create a project with a BROKER_URL pointing to an RabbitMQ vhost, like \r\n\r\n    amqp://guest:**@127.0.0.1:5672/myapp\r\n\r\n3. Start a Celery worker with this BROKER_URL like:\r\n\r\n    celery worker -A myapp -l info\r\n\r\n4. Launch the Celery task via:\r\n\r\n    manage.py shell\r\n    >>> from myapp.tasks import test_task; test_task.delay()\r\n\r\n5. Task is never executed.\r\n6. Kill Celery worker and change BROKER_URL to use the default \"/\" vhost, like:\r\n\r\n    amqp://guest:**@127.0.0.1:5672//\r\n\r\n7. Re-launch your Celery worker.\r\n8. Notice that the previous task is immediately executed, implying the first call to `test_task.delay()` did not send it to the correct BROKER_URL, and instead sent it to one with the vhost removed.\r\n\r\n## Expected behavior\r\n\r\nTasks should respect the vhost section in the BROKER_URL.\r\n\r\n## Actual behavior\r\n\r\nTasks are seemingly hard-coded to always use the default vhost, causing them to be executed by the wrong workers in a multi-vhost environment.\r\n\r\nThis was tested in Celery 4.1.0 with Django 1.11.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4359/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "unclewizard": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4355", "title": "client unexpectedly closed TCP connection with status check", "body": "## Checklist\r\n\r\n```\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:2.7.12\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:disabled\r\n```\r\n\r\n## Steps to reproduce\r\n\r\nFollow the \"first steps with django\" documentation to create a barebones celery app: http://docs.celeryproject.org/en/latest/django/first-steps-with-django.html\r\n\r\nFor a broker, use RabbitMQ 3.6.11. I have also tested with RabbitMQ 3.6.12. I set my broker in settings.py as follows:\r\n\r\n```\r\nBROKER_URL = 'amqp://guest:guest@localhost:5672/'\r\n```\r\n\r\nFinally, run ``celery worker -A appname status`` \r\n\r\n## Expected behavior\r\n\r\nNo warnings in the broker logs\r\n\r\n## Actual behavior\r\n\r\nThe rabbitmq server prints the following output (depending on configuration it may show up in the log file. For me it was in /var/log/rabbitmq/rabbit@vagrant-ubuntu-trusty-64.log)\r\n\r\n```\r\n=WARNING REPORT==== 31-Oct-2017::23:56:19 ===\r\nclosing AMQP connection <0.663.0> (127.0.0.1:35985 -> 127.0.0.1:5672, vhost: '/', user: 'guest'):\r\nclient unexpectedly closed TCP connection\r\n```\r\n\r\nThe status check reports \"OK\" and the worker is consuming tasks normally, so this doesn't appear to be a connectivity or network issue.\r\n\r\nThis wouldn't be so bad, but it also seems to trigger a memory leak at the broker. If you run this 1000s of times, you will notice the rabbitmq memory usage increase even without queueing any actual messages.\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4355/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "erebus1": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4354", "title": "Celery not return tasks in pool on shutdown", "body": "## Environment\r\n1. We have latest 4.1 celery version\r\nand latest 3.0.37 kombu version\r\n2. Using redis as a broker\r\n## The problem:\r\n1. When I have a simple task which call itself:\r\n\r\n```\r\n    @celery.task\r\n    def test():\r\n        sleep(2)\r\n        print(\"before %s\" % redis.client.llen('celery'))\r\n        print test.delay()\r\n        print(\"after %s\" % redis.client.llen('celery'))\r\n        sleep(1)\r\n        print(\"after sleep %s\" % redis.client.llen('celery'))\r\n```\r\nThen during warm shutdown, sometimes (roughly each second shut down), task do not continue executing after next celery startup\r\n\r\nAfter debugging we found that the problem probably with a way that kombu use to pop messages from redis, it use brpop with timeout 1s.\r\n\r\nSo in fact we have the next flow:\r\n1. kombu send brpop command to redis (1s timeout)\r\n1. warm shutdown command\r\n2. task start execution\r\n3. task put itself in queue via delay command\r\n4. redis return message to main process kombu (info from tcp dump) but no callback executed from kombu consumer\r\n5. celery stoped\r\n\r\nIt seems that there is a race condition between celery / komby stop consuming and worker putting new task in queue\r\n\r\n## Logs:\r\n\r\n### celery logs (with some additionals prints):\r\n\r\n```[2017-10-30 13:45:20,594: DEBUG/MainProcess] | Worker: Preparing bootsteps.\r\n[2017-10-30 13:45:20,595: DEBUG/MainProcess] | Worker: Building graph...\r\n[2017-10-30 13:45:20,596: DEBUG/MainProcess] | Worker: New boot order: {Timer, Hub, Pool, Autoscaler, Beat, StateDB, Consumer}\r\n[2017-10-30 13:45:20,606: DEBUG/MainProcess] | Consumer: Preparing bootsteps.\r\n[2017-10-30 13:45:20,607: DEBUG/MainProcess] | Consumer: Building graph...\r\n[2017-10-30 13:45:20,622: DEBUG/MainProcess] | Consumer: New boot order: {Connection, Events, Mingle, Tasks, Control, Agent, Gossip, Heart, event loop}\r\n \r\n -------------- celery@Kasianov-notebook v4.1.0 (latentcall)\r\n---- **** ----- \r\n--- * ***  * -- Linux-3.16.0-38-generic-x86_64-with-LinuxMint-17.2-rafaela 2017-10-30 13:45:20\r\n-- * - **** --- \r\n- ** ---------- [config]\r\n- ** ---------- .> app:         app:0x7fabb9c5d350\r\n- ** ---------- .> transport:   redis://localhost:6379//\r\n- ** ---------- .> results:     redis://localhost:6379/\r\n- *** --- * --- .> concurrency: 1 (prefork)\r\n-- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\r\n--- ***** ----- \r\n -------------- [queues]\r\n                .> celery           exchange=celery(direct) key=celery\r\n                \r\n\r\n[tasks]\r\n  . app.tasks.auth.send_sms_verification_for_company\r\n  . app.tasks.auth.send_sms_with_password_change_url_async\r\n  . app.tasks.auth.test\r\n  . celery.accumulate\r\n  . celery.backend_cleanup\r\n  . celery.chain\r\n  . celery.chord\r\n  . celery.chord_unlock\r\n  . celery.chunks\r\n  . celery.group\r\n  . celery.map\r\n  . celery.starmap\r\n\r\n[2017-10-30 13:45:20,631: DEBUG/MainProcess] | Worker: Starting Hub\r\n[2017-10-30 13:45:20,631: DEBUG/MainProcess] ^-- substep ok\r\n[2017-10-30 13:45:20,632: DEBUG/MainProcess] | Worker: Starting Pool\r\n[2017-10-30 13:45:20,800: DEBUG/MainProcess] ^-- substep ok\r\n[2017-10-30 13:45:20,800: DEBUG/MainProcess] | Worker: Starting Consumer\r\n[2017-10-30 13:45:20,801: DEBUG/MainProcess] | Consumer: Starting Connection\r\n[2017-10-30 13:45:20,814: INFO/MainProcess] Connected to redis://localhost:6379//\r\n[2017-10-30 13:45:20,814: DEBUG/MainProcess] ^-- substep ok\r\n[2017-10-30 13:45:20,815: DEBUG/MainProcess] | Consumer: Starting Events\r\n[2017-10-30 13:45:20,825: DEBUG/MainProcess] ^-- substep ok\r\n[2017-10-30 13:45:20,826: DEBUG/MainProcess] | Consumer: Starting Mingle\r\n[2017-10-30 13:45:20,826: INFO/MainProcess] mingle: searching for neighbors\r\n[2017-10-30 13:45:20,848: WARNING/MainProcess] consumeconsumeconsumeconsumeconsumeconsumeconsumeconsumeconsumeconsume\r\n[2017-10-30 13:45:20,848: WARNING/MainProcess] start consumestart consumestart consumestart consumestart consumestart consumestart consumestart consumestart consumestart consume\r\n[2017-10-30 13:45:20,848: WARNING/MainProcess] <type 'instancemethod'>\r\n[2017-10-30 13:45:20,849: WARNING/MainProcess] <class 'kombu.transport.redis.Channel'>\r\n[2017-10-30 13:45:21,852: INFO/MainProcess] mingle: all alone\r\n[2017-10-30 13:45:21,852: DEBUG/MainProcess] ^-- substep ok\r\n[2017-10-30 13:45:21,852: DEBUG/MainProcess] | Consumer: Starting Tasks\r\n[2017-10-30 13:45:21,866: DEBUG/MainProcess] ^-- substep ok\r\n[2017-10-30 13:45:21,866: DEBUG/MainProcess] | Consumer: Starting Control\r\n[2017-10-30 13:45:21,872: WARNING/MainProcess] consumeconsumeconsumeconsumeconsumeconsumeconsumeconsumeconsumeconsume\r\n[2017-10-30 13:45:21,873: WARNING/MainProcess] start consumestart consumestart consumestart consumestart consumestart consumestart consumestart consumestart consumestart consume\r\n[2017-10-30 13:45:21,873: WARNING/MainProcess] <type 'instancemethod'>\r\n[2017-10-30 13:45:21,873: WARNING/MainProcess] <class 'kombu.transport.redis.Channel'>\r\n[2017-10-30 13:45:21,874: DEBUG/MainProcess] ^-- substep ok\r\n[2017-10-30 13:45:21,874: DEBUG/MainProcess] | Consumer: Starting Gossip\r\n[2017-10-30 13:45:21,879: WARNING/MainProcess] consumeconsumeconsumeconsumeconsumeconsumeconsumeconsumeconsumeconsume\r\n[2017-10-30 13:45:21,881: WARNING/MainProcess] start consumestart consumestart consumestart consumestart consumestart consumestart consumestart consumestart consumestart consume\r\n[2017-10-30 13:45:21,881: WARNING/MainProcess] <type 'instancemethod'>\r\n[2017-10-30 13:45:21,881: WARNING/MainProcess] <class 'kombu.transport.redis.Channel'>\r\n[2017-10-30 13:45:21,882: DEBUG/MainProcess] ^-- substep ok\r\n[2017-10-30 13:45:21,882: DEBUG/MainProcess] | Consumer: Starting Heart\r\n[2017-10-30 13:45:21,884: DEBUG/MainProcess] ^-- substep ok\r\n[2017-10-30 13:45:21,884: DEBUG/MainProcess] | Consumer: Starting event loop\r\n[2017-10-30 13:45:21,885: DEBUG/MainProcess] | Worker: Hub.register Pool...\r\n[2017-10-30 13:45:21,886: WARNING/MainProcess] consumeconsumeconsumeconsumeconsumeconsumeconsumeconsumeconsumeconsume\r\n[2017-10-30 13:45:21,886: WARNING/MainProcess] start consumestart consumestart consumestart consumestart consumestart consumestart consumestart consumestart consumestart consume\r\n[2017-10-30 13:45:21,886: WARNING/MainProcess] <type 'instancemethod'>\r\n[2017-10-30 13:45:21,887: WARNING/MainProcess] <class 'kombu.transport.redis.Channel'>\r\n[2017-10-30 13:45:21,887: INFO/MainProcess] celery@Kasianov-notebook ready.\r\n[2017-10-30 13:45:21,887: DEBUG/MainProcess] basic.qos: prefetch_count->4\r\n[2017-10-30 13:45:22,053: WARNING/MainProcess] basic callback {'body': 'W1tdLCB7fSwgeyJjaG9yZCI6IG51bGwsICJjYWxsYmFja3MiOiBudWxsLCAiZXJyYmFja3MiOiBudWxsLCAiY2hhaW4iOiBudWxsfV0=', 'headers': {'origin': 'gen21580@Kasianov-notebook', 'lang': 'py', 'task': 'app.tasks.auth.test', 'group': None, 'root_id': '92178f34-5010-4efb-bdca-7bdf516e991c', 'expires': None, 'retries': 0, 'timelimit': [None, None], 'argsrepr': '()', 'eta': None, 'parent_id': '889b9640-cb35-44d4-a8d4-e115b462cc22', 'id': 'c2e23369-e87e-4bdc-a60e-eb95e4df29c5', 'kwargsrepr': '{}'}, 'content-type': 'application/json', 'properties': {'body_encoding': 'base64', 'delivery_info': {'routing_key': 'celery', 'exchange': u''}, 'delivery_mode': 2, 'priority': 0, 'correlation_id': 'c2e23369-e87e-4bdc-a60e-eb95e4df29c5', 'reply_to': '3be4ac41-09d4-3633-a2ae-60497a3d9899', 'delivery_tag': '14f33974-7100-4cd3-a387-1ed6f879d655'}, 'content-encoding': 'utf-8'}\r\n[2017-10-30 13:45:22,055: WARNING/MainProcess] consume_callback, (<Message object at 0x7fabb9371d60 with details {'body_length': 77, 'delivery_info': {'routing_key': 'celery', 'exchange': u''}, 'properties': {'correlation_id': 'c2e23369-e87e-4bdc-a60e-eb95e4df29c5'}, 'state': u'RECEIVED', 'content_type': 'application/json', 'delivery_tag': '14f33974-7100-4cd3-a387-1ed6f879d655'}>,), {}\r\n[2017-10-30 13:45:22,055: INFO/MainProcess] <Message object at 0x7fabb9371d60 with details {'body_length': 77, 'delivery_info': {'routing_key': 'celery', 'exchange': u''}, 'properties': {'correlation_id': 'c2e23369-e87e-4bdc-a60e-eb95e4df29c5'}, 'state': u'RECEIVED', 'content_type': 'application/json', 'delivery_tag': '14f33974-7100-4cd3-a387-1ed6f879d655'}>\r\n[2017-10-30 13:45:22,056: INFO/MainProcess] D True\r\n[2017-10-30 13:45:22,056: INFO/MainProcess] xReceived task: app.tasks.auth.test[c2e23369-e87e-4bdc-a60e-eb95e4df29c5]  \r\n[2017-10-30 13:45:22,057: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x7fabb9bffb90> (args:('app.tasks.auth.test', 'c2e23369-e87e-4bdc-a60e-eb95e4df29c5', {'origin': 'gen21580@Kasianov-notebook', 'lang': 'py', 'task': 'app.tasks.auth.test', 'group': None, 'root_id': '92178f34-5010-4efb-bdca-7bdf516e991c', u'delivery_info': {u'priority': 0, u'redelivered': None, u'routing_key': 'celery', u'exchange': u''}, 'expires': None, u'correlation_id': 'c2e23369-e87e-4bdc-a60e-eb95e4df29c5', 'retries': 0, 'timelimit': [None, None], 'argsrepr': '()', 'eta': None, 'parent_id': '889b9640-cb35-44d4-a8d4-e115b462cc22', u'reply_to': '3be4ac41-09d4-3633-a2ae-60497a3d9899', 'id': 'c2e23369-e87e-4bdc-a60e-eb95e4df29c5', 'kwargsrepr': '{}'}, '[[], {}, {\"chord\": null, \"callbacks\": null, \"errbacks\": null, \"chain\": null}]', 'application/json', 'utf-8') kwargs:{})\r\n[2017-10-30 13:45:22,058: DEBUG/MainProcess] Task accepted: app.tasks.auth.test[c2e23369-e87e-4bdc-a60e-eb95e4df29c5] pid:22707\r\n[2017-10-30 13:45:23,888: WARNING/MainProcess] basic callback {'body': 'eyJzd19zeXMiOiAiTGludXgiLCAiY2xvY2siOiAyLCAidGltZXN0YW1wIjogMTUwOTM2MzkyMy44ODU1NDEsICJob3N0bmFtZSI6ICJjZWxlcnlAS2FzaWFub3Ytbm90ZWJvb2siLCAicGlkIjogMjI2OTEsICJzd192ZXIiOiAiNC4xLjAiLCAidXRjb2Zmc2V0IjogLTIsICJsb2FkYXZnIjogWzAuODMsIDAuOTQsIDEuMDNdLCAicHJvY2Vzc2VkIjogMSwgImFjdGl2ZSI6IDEsICJmcmVxIjogMi4wLCAidHlwZSI6ICJ3b3JrZXItaGVhcnRiZWF0IiwgInN3X2lkZW50IjogInB5LWNlbGVyeSJ9', 'headers': {'hostname': 'celery@Kasianov-notebook'}, 'content-type': 'application/json', 'properties': {'priority': 0, 'body_encoding': 'base64', 'delivery_info': {'routing_key': 'worker.heartbeat', 'exchange': 'celeryev'}, 'delivery_mode': 1, 'delivery_tag': 'bbc9e6ce-a408-4a4e-bb75-42a3d6bef426'}, 'content-encoding': 'utf-8'}\r\n[2017-10-30 13:45:23,889: WARNING/MainProcess] consume_callback, (<Message object at 0x7fabb92ac218 with details {'body_length': 279, 'delivery_info': {'routing_key': 'worker.heartbeat', 'exchange': 'celeryev'}, 'properties': {}, 'state': u'RECEIVED', 'content_type': 'application/json', 'delivery_tag': 'bbc9e6ce-a408-4a4e-bb75-42a3d6bef426'}>,), {}\r\n[2017-10-30 13:45:23,890: WARNING/MainProcess] on_message, (<Message object at 0x7fabb92ac218 with details {'body_length': 279, 'delivery_info': {'routing_key': 'worker.heartbeat', 'exchange': 'celeryev'}, 'properties': {}, 'state': u'RECEIVED', 'content_type': 'application/json', 'delivery_tag': 'bbc9e6ce-a408-4a4e-bb75-42a3d6bef426'}>,), {}\r\n[2017-10-30 13:45:24,086: WARNING/ForkPoolWorker-1] before 0\r\n[2017-10-30 13:45:24,120: WARNING/MainProcess] basic callback {'body': 'W1tdLCB7fSwgeyJjaG9yZCI6IG51bGwsICJjYWxsYmFja3MiOiBudWxsLCAiZXJyYmFja3MiOiBudWxsLCAiY2hhaW4iOiBudWxsfV0=', 'headers': {'origin': 'gen22707@Kasianov-notebook', 'lang': 'py', 'task': 'app.tasks.auth.test', 'group': None, 'root_id': '92178f34-5010-4efb-bdca-7bdf516e991c', 'expires': None, 'retries': 0, 'timelimit': [None, None], 'argsrepr': '()', 'eta': None, 'parent_id': 'c2e23369-e87e-4bdc-a60e-eb95e4df29c5', 'id': '9a2175e8-b300-422a-a8b2-9b34319ed6c0', 'kwargsrepr': '{}'}, 'content-type': 'application/json', 'properties': {'body_encoding': 'base64', 'delivery_info': {'routing_key': 'celery', 'exchange': u''}, 'delivery_mode': 2, 'priority': 0, 'correlation_id': '9a2175e8-b300-422a-a8b2-9b34319ed6c0', 'reply_to': 'ff72ad1a-efb7-38d0-a59d-26d3cfd07c2d', 'delivery_tag': '3c34af07-9190-4271-af12-5b395138bc90'}, 'content-encoding': 'utf-8'}\r\n[2017-10-30 13:45:24,120: WARNING/ForkPoolWorker-1] 9a2175e8-b300-422a-a8b2-9b34319ed6c0\r\n[2017-10-30 13:45:24,121: WARNING/ForkPoolWorker-1] after 0\r\n[2017-10-30 13:45:24,121: WARNING/MainProcess] consume_callback, (<Message object at 0x7fabb92ac2b0 with details {'body_length': 77, 'delivery_info': {'routing_key': 'celery', 'exchange': u''}, 'properties': {'correlation_id': '9a2175e8-b300-422a-a8b2-9b34319ed6c0'}, 'state': u'RECEIVED', 'content_type': 'application/json', 'delivery_tag': '3c34af07-9190-4271-af12-5b395138bc90'}>,), {}\r\n[2017-10-30 13:45:24,121: INFO/MainProcess] <Message object at 0x7fabb92ac2b0 with details {'body_length': 77, 'delivery_info': {'routing_key': 'celery', 'exchange': u''}, 'properties': {'correlation_id': '9a2175e8-b300-422a-a8b2-9b34319ed6c0'}, 'state': u'RECEIVED', 'content_type': 'application/json', 'delivery_tag': '3c34af07-9190-4271-af12-5b395138bc90'}>\r\n[2017-10-30 13:45:24,122: INFO/MainProcess] D True\r\n[2017-10-30 13:45:24,122: INFO/MainProcess] xReceived task: app.tasks.auth.test[9a2175e8-b300-422a-a8b2-9b34319ed6c0]  \r\n[2017-10-30 13:45:25,122: WARNING/ForkPoolWorker-1] after sleep 0\r\n[2017-10-30 13:45:25,123: WARNING/ForkPoolWorker-1] 140375523388944\r\n[2017-10-30 13:45:25,124: WARNING/ForkPoolWorker-1] 140375523388944\r\n[2017-10-30 13:45:25,124: INFO/ForkPoolWorker-1] Task app.tasks.auth.test[c2e23369-e87e-4bdc-a60e-eb95e4df29c5] succeeded in 3.06584376702s: None\r\n[2017-10-30 13:45:25,125: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x7fabb9bffb90> (args:('app.tasks.auth.test', '9a2175e8-b300-422a-a8b2-9b34319ed6c0', {'origin': 'gen22707@Kasianov-notebook', 'lang': 'py', 'task': 'app.tasks.auth.test', 'group': None, 'root_id': '92178f34-5010-4efb-bdca-7bdf516e991c', u'delivery_info': {u'priority': 0, u'redelivered': None, u'routing_key': 'celery', u'exchange': u''}, 'expires': None, u'correlation_id': '9a2175e8-b300-422a-a8b2-9b34319ed6c0', 'retries': 0, 'timelimit': [None, None], 'argsrepr': '()', 'eta': None, 'parent_id': 'c2e23369-e87e-4bdc-a60e-eb95e4df29c5', u'reply_to': 'ff72ad1a-efb7-38d0-a59d-26d3cfd07c2d', 'id': '9a2175e8-b300-422a-a8b2-9b34319ed6c0', 'kwargsrepr': '{}'}, '[[], {}, {\"chord\": null, \"callbacks\": null, \"errbacks\": null, \"chain\": null}]', 'application/json', 'utf-8') kwargs:{})\r\n[2017-10-30 13:45:25,127: DEBUG/MainProcess] Task accepted: app.tasks.auth.test[9a2175e8-b300-422a-a8b2-9b34319ed6c0] pid:22707\r\n[2017-10-30 13:45:25,890: WARNING/MainProcess] basic callback {'body': 'eyJzd19zeXMiOiAiTGludXgiLCAiY2xvY2siOiA0LCAidGltZXN0YW1wIjogMTUwOTM2MzkyNS44ODg4ODcsICJob3N0bmFtZSI6ICJjZWxlcnlAS2FzaWFub3Ytbm90ZWJvb2siLCAicGlkIjogMjI2OTEsICJzd192ZXIiOiAiNC4xLjAiLCAidXRjb2Zmc2V0IjogLTIsICJsb2FkYXZnIjogWzAuNzYsIDAuOTMsIDEuMDNdLCAicHJvY2Vzc2VkIjogMiwgImFjdGl2ZSI6IDEsICJmcmVxIjogMi4wLCAidHlwZSI6ICJ3b3JrZXItaGVhcnRiZWF0IiwgInN3X2lkZW50IjogInB5LWNlbGVyeSJ9', 'headers': {'hostname': 'celery@Kasianov-notebook'}, 'content-type': 'application/json', 'properties': {'priority': 0, 'body_encoding': 'base64', 'delivery_info': {'routing_key': 'worker.heartbeat', 'exchange': 'celeryev'}, 'delivery_mode': 1, 'delivery_tag': 'f2aad458-99f4-4bba-85c3-79ebec1aa2bb'}, 'content-encoding': 'utf-8'}\r\n[2017-10-30 13:45:25,890: WARNING/MainProcess] consume_callback, (<Message object at 0x7fabb9371d60 with details {'body_length': 279, 'delivery_info': {'routing_key': 'worker.heartbeat', 'exchange': 'celeryev'}, 'properties': {}, 'state': u'RECEIVED', 'content_type': 'application/json', 'delivery_tag': 'f2aad458-99f4-4bba-85c3-79ebec1aa2bb'}>,), {}\r\n[2017-10-30 13:45:25,891: WARNING/MainProcess] on_message, (<Message object at 0x7fabb9371d60 with details {'body_length': 279, 'delivery_info': {'routing_key': 'worker.heartbeat', 'exchange': 'celeryev'}, 'properties': {}, 'state': u'RECEIVED', 'content_type': 'application/json', 'delivery_tag': 'f2aad458-99f4-4bba-85c3-79ebec1aa2bb'}>,), {}\r\n[2017-10-30 13:45:27,134: WARNING/ForkPoolWorker-1] before 0\r\n[2017-10-30 13:45:27,138: WARNING/MainProcess] basic callback {'body': 'W1tdLCB7fSwgeyJjaG9yZCI6IG51bGwsICJjYWxsYmFja3MiOiBudWxsLCAiZXJyYmFja3MiOiBudWxsLCAiY2hhaW4iOiBudWxsfV0=', 'headers': {'origin': 'gen22707@Kasianov-notebook', 'lang': 'py', 'task': 'app.tasks.auth.test', 'group': None, 'root_id': '92178f34-5010-4efb-bdca-7bdf516e991c', 'expires': None, 'retries': 0, 'timelimit': [None, None], 'argsrepr': '()', 'eta': None, 'parent_id': '9a2175e8-b300-422a-a8b2-9b34319ed6c0', 'id': '9a842e43-05f7-4d99-8ba0-53dc74a34c4c', 'kwargsrepr': '{}'}, 'content-type': 'application/json', 'properties': {'body_encoding': 'base64', 'delivery_info': {'routing_key': 'celery', 'exchange': u''}, 'delivery_mode': 2, 'priority': 0, 'correlation_id': '9a842e43-05f7-4d99-8ba0-53dc74a34c4c', 'reply_to': 'ff72ad1a-efb7-38d0-a59d-26d3cfd07c2d', 'delivery_tag': '551fafd3-c26a-4a16-abc3-9fc3f89b4371'}, 'content-encoding': 'utf-8'}\r\n[2017-10-30 13:45:27,138: WARNING/ForkPoolWorker-1] 9a842e43-05f7-4d99-8ba0-53dc74a34c4c\r\n[2017-10-30 13:45:27,139: WARNING/ForkPoolWorker-1] after 0\r\n[2017-10-30 13:45:27,140: WARNING/MainProcess] consume_callback, (<Message object at 0x7fabb9371df8 with details {'body_length': 77, 'delivery_info': {'routing_key': 'celery', 'exchange': u''}, 'properties': {'correlation_id': '9a842e43-05f7-4d99-8ba0-53dc74a34c4c'}, 'state': u'RECEIVED', 'content_type': 'application/json', 'delivery_tag': '551fafd3-c26a-4a16-abc3-9fc3f89b4371'}>,), {}\r\n[2017-10-30 13:45:27,140: INFO/MainProcess] <Message object at 0x7fabb9371df8 with details {'body_length': 77, 'delivery_info': {'routing_key': 'celery', 'exchange': u''}, 'properties': {'correlation_id': '9a842e43-05f7-4d99-8ba0-53dc74a34c4c'}, 'state': u'RECEIVED', 'content_type': 'application/json', 'delivery_tag': '551fafd3-c26a-4a16-abc3-9fc3f89b4371'}>\r\n[2017-10-30 13:45:27,141: INFO/MainProcess] D True\r\n[2017-10-30 13:45:27,141: INFO/MainProcess] xReceived task: app.tasks.auth.test[9a842e43-05f7-4d99-8ba0-53dc74a34c4c]  \r\n[2017-10-30 13:45:27,892: WARNING/MainProcess] basic callback {'body': 'eyJzd19zeXMiOiAiTGludXgiLCAiY2xvY2siOiA2LCAidGltZXN0YW1wIjogMTUwOTM2MzkyNy44OTA5MjcsICJob3N0bmFtZSI6ICJjZWxlcnlAS2FzaWFub3Ytbm90ZWJvb2siLCAicGlkIjogMjI2OTEsICJzd192ZXIiOiAiNC4xLjAiLCAidXRjb2Zmc2V0IjogLTIsICJsb2FkYXZnIjogWzAuNzYsIDAuOTMsIDEuMDNdLCAicHJvY2Vzc2VkIjogMiwgImFjdGl2ZSI6IDEsICJmcmVxIjogMi4wLCAidHlwZSI6ICJ3b3JrZXItaGVhcnRiZWF0IiwgInN3X2lkZW50IjogInB5LWNlbGVyeSJ9', 'headers': {'hostname': 'celery@Kasianov-notebook'}, 'content-type': 'application/json', 'properties': {'priority': 0, 'body_encoding': 'base64', 'delivery_info': {'routing_key': 'worker.heartbeat', 'exchange': 'celeryev'}, 'delivery_mode': 1, 'delivery_tag': 'b2d73761-cde3-46a9-b6ae-18418694c74c'}, 'content-encoding': 'utf-8'}\r\n[2017-10-30 13:45:27,893: WARNING/MainProcess] consume_callback, (<Message object at 0x7fabb92ac0e8 with details {'body_length': 279, 'delivery_info': {'routing_key': 'worker.heartbeat', 'exchange': 'celeryev'}, 'properties': {}, 'state': u'RECEIVED', 'content_type': 'application/json', 'delivery_tag': 'b2d73761-cde3-46a9-b6ae-18418694c74c'}>,), {}\r\n[2017-10-30 13:45:27,893: WARNING/MainProcess] on_message, (<Message object at 0x7fabb92ac0e8 with details {'body_length': 279, 'delivery_info': {'routing_key': 'worker.heartbeat', 'exchange': 'celeryev'}, 'properties': {}, 'state': u'RECEIVED', 'content_type': 'application/json', 'delivery_tag': 'b2d73761-cde3-46a9-b6ae-18418694c74c'}>,), {}\r\n[2017-10-30 13:45:28,141: WARNING/ForkPoolWorker-1] after sleep 0\r\n[2017-10-30 13:45:28,142: WARNING/ForkPoolWorker-1] 140375523284368\r\n[2017-10-30 13:45:28,143: WARNING/ForkPoolWorker-1] 140375523284368\r\n[2017-10-30 13:45:28,143: INFO/ForkPoolWorker-1] Task app.tasks.auth.test[9a2175e8-b300-422a-a8b2-9b34319ed6c0] succeeded in 3.01614009804s: None\r\n[2017-10-30 13:45:28,145: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x7fabb9bffb90> (args:('app.tasks.auth.test', '9a842e43-05f7-4d99-8ba0-53dc74a34c4c', {'origin': 'gen22707@Kasianov-notebook', 'lang': 'py', 'task': 'app.tasks.auth.test', 'group': None, 'root_id': '92178f34-5010-4efb-bdca-7bdf516e991c', u'delivery_info': {u'priority': 0, u'redelivered': None, u'routing_key': 'celery', u'exchange': u''}, 'expires': None, u'correlation_id': '9a842e43-05f7-4d99-8ba0-53dc74a34c4c', 'retries': 0, 'timelimit': [None, None], 'argsrepr': '()', 'eta': None, 'parent_id': '9a2175e8-b300-422a-a8b2-9b34319ed6c0', u'reply_to': 'ff72ad1a-efb7-38d0-a59d-26d3cfd07c2d', 'id': '9a842e43-05f7-4d99-8ba0-53dc74a34c4c', 'kwargsrepr': '{}'}, '[[], {}, {\"chord\": null, \"callbacks\": null, \"errbacks\": null, \"chain\": null}]', 'application/json', 'utf-8') kwargs:{})\r\n[2017-10-30 13:45:28,147: DEBUG/MainProcess] Task accepted: app.tasks.auth.test[9a842e43-05f7-4d99-8ba0-53dc74a34c4c] pid:22707\r\n^C\r\nworker: Hitting Ctrl+C again will terminate all running tasks!\r\n\r\nworker: Warm shutdown (MainProcess)\r\n[2017-10-30 13:45:29,787: DEBUG/MainProcess] | Worker: Closing Hub...\r\n[2017-10-30 13:45:29,788: DEBUG/MainProcess] | Worker: Closing Pool...\r\n[2017-10-30 13:45:29,788: DEBUG/MainProcess] | Worker: Closing Consumer...\r\n[2017-10-30 13:45:29,788: DEBUG/MainProcess] | Worker: Stopping Consumer...\r\n[2017-10-30 13:45:29,788: DEBUG/MainProcess] | Consumer: Closing Connection...\r\n[2017-10-30 13:45:29,788: DEBUG/MainProcess] | Consumer: Closing Events...\r\n[2017-10-30 13:45:29,789: DEBUG/MainProcess] | Consumer: Closing Mingle...\r\n[2017-10-30 13:45:29,789: DEBUG/MainProcess] | Consumer: Closing Tasks...\r\n[2017-10-30 13:45:29,789: DEBUG/MainProcess] | Consumer: Closing Control...\r\n[2017-10-30 13:45:29,789: DEBUG/MainProcess] | Consumer: Closing Gossip...\r\n[2017-10-30 13:45:29,790: DEBUG/MainProcess] | Consumer: Closing Heart...\r\n[2017-10-30 13:45:29,790: DEBUG/MainProcess] | Consumer: Closing event loop...\r\n[2017-10-30 13:45:29,790: DEBUG/MainProcess] | Consumer: Stopping event loop...\r\n[2017-10-30 13:45:29,791: DEBUG/MainProcess] | Consumer: Stopping Heart...\r\n[2017-10-30 13:45:29,793: DEBUG/MainProcess] | Consumer: Stopping Gossip...\r\n[2017-10-30 13:45:29,795: WARNING/MainProcess] close\r\n[2017-10-30 13:45:29,795: WARNING/MainProcess] set([])\r\n[2017-10-30 13:45:29,795: WARNING/MainProcess] []\r\n[2017-10-30 13:45:29,795: WARNING/MainProcess] []\r\n[2017-10-30 13:45:29,796: DEBUG/MainProcess] | Consumer: Stopping Control...\r\n[2017-10-30 13:45:29,798: WARNING/MainProcess] close\r\n[2017-10-30 13:45:29,798: WARNING/MainProcess] set([u'None2'])\r\n[2017-10-30 13:45:29,798: WARNING/MainProcess] [u'celery@Kasianov-notebook.celery.pidbox']\r\n[2017-10-30 13:45:29,799: WARNING/MainProcess] [u'celery@Kasianov-notebook.celery.pidbox']\r\n[2017-10-30 13:45:29,799: DEBUG/MainProcess] | Consumer: Stopping Tasks...\r\n[2017-10-30 13:45:29,799: DEBUG/MainProcess] Canceling task consumer...\r\n[2017-10-30 13:45:29,800: DEBUG/MainProcess] | Consumer: Stopping Mingle...\r\n[2017-10-30 13:45:29,800: DEBUG/MainProcess] | Consumer: Stopping Events...\r\n[2017-10-30 13:45:29,800: DEBUG/MainProcess] | Consumer: Stopping Connection...\r\n[2017-10-30 13:45:29,800: DEBUG/MainProcess] | Worker: Stopping Pool...\r\n[2017-10-30 13:45:30,154: WARNING/ForkPoolWorker-1] before 0\r\n[2017-10-30 13:45:30,156: WARNING/ForkPoolWorker-1] b4cced72-a331-4bdd-b310-c87b2fa96b72\r\n[2017-10-30 13:45:30,157: WARNING/ForkPoolWorker-1] after 0\r\n[2017-10-30 13:45:31,158: WARNING/ForkPoolWorker-1] after sleep 0\r\n[2017-10-30 13:45:31,159: WARNING/ForkPoolWorker-1] 140375523389072\r\n[2017-10-30 13:45:31,159: WARNING/ForkPoolWorker-1] 140375523389072\r\n[2017-10-30 13:45:31,160: INFO/ForkPoolWorker-1] Task app.tasks.auth.test[9a842e43-05f7-4d99-8ba0-53dc74a34c4c] succeeded in 3.01320653799s: None\r\n[2017-10-30 13:45:32,178: DEBUG/MainProcess] | Worker: Stopping Hub...\r\n[2017-10-30 13:45:32,178: DEBUG/MainProcess] | Consumer: Shutdown Heart...\r\n[2017-10-30 13:45:32,179: DEBUG/MainProcess] | Consumer: Shutdown Gossip...\r\n[2017-10-30 13:45:32,179: WARNING/MainProcess] close\r\n[2017-10-30 13:45:32,180: WARNING/MainProcess] set([])\r\n[2017-10-30 13:45:32,180: WARNING/MainProcess] []\r\n[2017-10-30 13:45:32,180: DEBUG/MainProcess] | Consumer: Shutdown Control...\r\n[2017-10-30 13:45:32,180: WARNING/MainProcess] close\r\n[2017-10-30 13:45:32,180: WARNING/MainProcess] set([])\r\n[2017-10-30 13:45:32,181: WARNING/MainProcess] []\r\n[2017-10-30 13:45:32,181: DEBUG/MainProcess] | Consumer: Shutdown Tasks...\r\n[2017-10-30 13:45:32,181: DEBUG/MainProcess] Canceling task consumer...\r\n[2017-10-30 13:45:32,181: DEBUG/MainProcess] Closing consumer channel...\r\n[2017-10-30 13:45:32,181: DEBUG/MainProcess] | Consumer: Shutdown Events...\r\n[2017-10-30 13:45:32,182: WARNING/MainProcess] close\r\n[2017-10-30 13:45:32,182: WARNING/MainProcess] set([])\r\n[2017-10-30 13:45:32,182: WARNING/MainProcess] []\r\n[2017-10-30 13:45:32,183: DEBUG/MainProcess] | Consumer: Shutdown Connection...\r\n[2017-10-30 13:45:32,183: WARNING/MainProcess] close\r\n[2017-10-30 13:45:32,183: WARNING/MainProcess] set([])\r\n[2017-10-30 13:45:32,183: WARNING/MainProcess] []\r\n[2017-10-30 13:45:32,184: WARNING/MainProcess] []\r\n[2017-10-30 13:45:32,184: DEBUG/MainProcess] removing tasks from inqueue until task handler finished\r\n```\r\n### redis monitor logs:\r\n```\r\n509363921.889267 [0 127.0.0.1:58203] \"PSUBSCRIBE\" \"/0.celeryev/worker.*\"\r\n1509363922.053465 [0 127.0.0.1:58196] \"BRPOP\" \"celery\" \"celery\\x06\\x163\" \"celery\\x06\\x166\" \"celery\\x06\\x169\" \"1\"\r\n1509363922.054911 [0 127.0.0.1:58197] \"MULTI\"\r\n1509363922.054946 [0 127.0.0.1:58197] \"ZADD\" \"unacked_index\" \"1509363922.054393\" \"14f33974-7100-4cd3-a387-1ed6f879d655\"\r\n1509363922.054990 [0 127.0.0.1:58197] \"HSET\" \"unacked\" \"14f33974-7100-4cd3-a387-1ed6f879d655\" \"[{\\\"body\\\": \\\"W1tdLCB7fSwgeyJjaG9yZCI6IG51bGwsICJjYWxsYmFja3MiOiBudWxsLCAiZXJyYmFja3MiOiBudWxsLCAiY2hhaW4iOiBudWxsfV0=\\\", \\\"headers\\\": {\\\"origin\\\": \\\"gen21580@Kasianov-notebook\\\", \\\"lang\\\": \\\"py\\\", \\\"task\\\": \\\"app.tasks.auth.test\\\", \\\"group\\\": null, \\\"root_id\\\": \\\"92178f34-5010-4efb-bdca-7bdf516e991c\\\", \\\"expires\\\": null, \\\"retries\\\": 0, \\\"timelimit\\\": [null, null], \\\"argsrepr\\\": \\\"()\\\", \\\"eta\\\": null, \\\"parent_id\\\": \\\"889b9640-cb35-44d4-a8d4-e115b462cc22\\\", \\\"id\\\": \\\"c2e23369-e87e-4bdc-a60e-eb95e4df29c5\\\", \\\"kwargsrepr\\\": \\\"{}\\\"}, \\\"content-type\\\": \\\"application/json\\\", \\\"properties\\\": {\\\"body_encoding\\\": \\\"base64\\\", \\\"delivery_info\\\": {\\\"routing_key\\\": \\\"celery\\\", \\\"exchange\\\": \\\"\\\"}, \\\"delivery_mode\\\": 2, \\\"priority\\\": 0, \\\"correlation_id\\\": \\\"c2e23369-e87e-4bdc-a60e-eb95e4df29c5\\\", \\\"reply_to\\\": \\\"3be4ac41-09d4-3633-a2ae-60497a3d9899\\\", \\\"delivery_tag\\\": \\\"14f33974-7100-4cd3-a387-1ed6f879d655\\\"}, \\\"content-encoding\\\": \\\"utf-8\\\"}, \\\"\\\", \\\"celery\\\"]\"\r\n1509363922.055212 [0 127.0.0.1:58197] \"EXEC\"\r\n1509363922.057616 [0 127.0.0.1:58196] \"BRPOP\" \"celery\" \"celery\\x06\\x163\" \"celery\\x06\\x166\" \"celery\\x06\\x169\" \"1\"\r\n1509363922.059320 [0 127.0.0.1:58197] \"MULTI\"\r\n1509363922.059357 [0 127.0.0.1:58197] \"ZREM\" \"unacked_index\" \"14f33974-7100-4cd3-a387-1ed6f879d655\"\r\n1509363922.059385 [0 127.0.0.1:58197] \"HDEL\" \"unacked\" \"14f33974-7100-4cd3-a387-1ed6f879d655\"\r\n1509363922.059407 [0 127.0.0.1:58197] \"EXEC\"\r\n1509363923.169864 [0 127.0.0.1:58196] \"BRPOP\" \"celery\" \"celery\\x06\\x163\" \"celery\\x06\\x166\" \"celery\\x06\\x169\" \"1\"\r\n1509363923.887039 [0 127.0.0.1:58205] \"PUBLISH\" \"/0.celeryev/worker.heartbeat\" \"{\\\"body\\\": \\\"eyJzd19zeXMiOiAiTGludXgiLCAiY2xvY2siOiAyLCAidGltZXN0YW1wIjogMTUwOTM2MzkyMy44ODU1NDEsICJob3N0bmFtZSI6ICJjZWxlcnlAS2FzaWFub3Ytbm90ZWJvb2siLCAicGlkIjogMjI2OTEsICJzd192ZXIiOiAiNC4xLjAiLCAidXRjb2Zmc2V0IjogLTIsICJsb2FkYXZnIjogWzAuODMsIDAuOTQsIDEuMDNdLCAicHJvY2Vzc2VkIjogMSwgImFjdGl2ZSI6IDEsICJmcmVxIjogMi4wLCAidHlwZSI6ICJ3b3JrZXItaGVhcnRiZWF0IiwgInN3X2lkZW50IjogInB5LWNlbGVyeSJ9\\\", \\\"headers\\\": {\\\"hostname\\\": \\\"celery@Kasianov-notebook\\\"}, \\\"content-type\\\": \\\"application/json\\\", \\\"properties\\\": {\\\"priority\\\": 0, \\\"body_encoding\\\": \\\"base64\\\", \\\"delivery_info\\\": {\\\"routing_key\\\": \\\"worker.heartbeat\\\", \\\"exchange\\\": \\\"celeryev\\\"}, \\\"delivery_mode\\\": 1, \\\"delivery_tag\\\": \\\"bbc9e6ce-a408-4a4e-bb75-42a3d6bef426\\\"}, \\\"content-encoding\\\": \\\"utf-8\\\"}\"\r\n1509363924.086632 [0 127.0.0.1:58206] \"LLEN\" \"celery\"\r\n1509363924.117498 [0 127.0.0.1:58207] \"PING\"\r\n1509363924.119048 [0 127.0.0.1:58208] \"MULTI\"\r\n1509363924.119068 [0 127.0.0.1:58208] \"LLEN\" \"celery\"\r\n1509363924.119074 [0 127.0.0.1:58208] \"LLEN\" \"celery\\x06\\x163\"\r\n1509363924.119078 [0 127.0.0.1:58208] \"LLEN\" \"celery\\x06\\x166\"\r\n1509363924.119083 [0 127.0.0.1:58208] \"LLEN\" \"celery\\x06\\x169\"\r\n1509363924.119087 [0 127.0.0.1:58208] \"EXEC\"\r\n1509363924.119404 [0 127.0.0.1:58208] \"SADD\" \"_kombu.binding.celery\" \"celery\\x06\\x16\\x06\\x16celery\"\r\n1509363924.119838 [0 127.0.0.1:58208] \"LPUSH\" \"celery\" \"{\\\"body\\\": \\\"W1tdLCB7fSwgeyJjaG9yZCI6IG51bGwsICJjYWxsYmFja3MiOiBudWxsLCAiZXJyYmFja3MiOiBudWxsLCAiY2hhaW4iOiBudWxsfV0=\\\", \\\"headers\\\": {\\\"origin\\\": \\\"gen22707@Kasianov-notebook\\\", \\\"root_id\\\": \\\"92178f34-5010-4efb-bdca-7bdf516e991c\\\", \\\"expires\\\": null, \\\"id\\\": \\\"9a2175e8-b300-422a-a8b2-9b34319ed6c0\\\", \\\"kwargsrepr\\\": \\\"{}\\\", \\\"lang\\\": \\\"py\\\", \\\"retries\\\": 0, \\\"task\\\": \\\"app.tasks.auth.test\\\", \\\"group\\\": null, \\\"timelimit\\\": [null, null], \\\"parent_id\\\": \\\"c2e23369-e87e-4bdc-a60e-eb95e4df29c5\\\", \\\"argsrepr\\\": \\\"()\\\", \\\"eta\\\": null}, \\\"content-type\\\": \\\"application/json\\\", \\\"properties\\\": {\\\"priority\\\": 0, \\\"body_encoding\\\": \\\"base64\\\", \\\"correlation_id\\\": \\\"9a2175e8-b300-422a-a8b2-9b34319ed6c0\\\", \\\"reply_to\\\": \\\"ff72ad1a-efb7-38d0-a59d-26d3cfd07c2d\\\", \\\"delivery_info\\\": {\\\"routing_key\\\": \\\"celery\\\", \\\"exchange\\\": \\\"\\\"}, \\\"delivery_mode\\\": 2, \\\"delivery_tag\\\": \\\"3c34af07-9190-4271-af12-5b395138bc90\\\"}, \\\"content-encoding\\\": \\\"utf-8\\\"}\"\r\n1509363924.120977 [0 127.0.0.1:58206] \"LLEN\" \"celery\"\r\n1509363924.121116 [0 127.0.0.1:58197] \"MULTI\"\r\n1509363924.121133 [0 127.0.0.1:58197] \"ZADD\" \"unacked_index\" \"1509363924.120737\" \"3c34af07-9190-4271-af12-5b395138bc90\"\r\n1509363924.121170 [0 127.0.0.1:58197] \"HSET\" \"unacked\" \"3c34af07-9190-4271-af12-5b395138bc90\" \"[{\\\"body\\\": \\\"W1tdLCB7fSwgeyJjaG9yZCI6IG51bGwsICJjYWxsYmFja3MiOiBudWxsLCAiZXJyYmFja3MiOiBudWxsLCAiY2hhaW4iOiBudWxsfV0=\\\", \\\"headers\\\": {\\\"origin\\\": \\\"gen22707@Kasianov-notebook\\\", \\\"lang\\\": \\\"py\\\", \\\"task\\\": \\\"app.tasks.auth.test\\\", \\\"group\\\": null, \\\"root_id\\\": \\\"92178f34-5010-4efb-bdca-7bdf516e991c\\\", \\\"expires\\\": null, \\\"retries\\\": 0, \\\"timelimit\\\": [null, null], \\\"argsrepr\\\": \\\"()\\\", \\\"eta\\\": null, \\\"parent_id\\\": \\\"c2e23369-e87e-4bdc-a60e-eb95e4df29c5\\\", \\\"id\\\": \\\"9a2175e8-b300-422a-a8b2-9b34319ed6c0\\\", \\\"kwargsrepr\\\": \\\"{}\\\"}, \\\"content-type\\\": \\\"application/json\\\", \\\"properties\\\": {\\\"body_encoding\\\": \\\"base64\\\", \\\"delivery_info\\\": {\\\"routing_key\\\": \\\"celery\\\", \\\"exchange\\\": \\\"\\\"}, \\\"delivery_mode\\\": 2, \\\"priority\\\": 0, \\\"correlation_id\\\": \\\"9a2175e8-b300-422a-a8b2-9b34319ed6c0\\\", \\\"reply_to\\\": \\\"ff72ad1a-efb7-38d0-a59d-26d3cfd07c2d\\\", \\\"delivery_tag\\\": \\\"3c34af07-9190-4271-af12-5b395138bc90\\\"}, \\\"content-encoding\\\": \\\"utf-8\\\"}, \\\"\\\", \\\"celery\\\"]\"\r\n1509363924.121312 [0 127.0.0.1:58197] \"EXEC\"\r\n1509363924.122635 [0 127.0.0.1:58196] \"BRPOP\" \"celery\" \"celery\\x06\\x163\" \"celery\\x06\\x166\" \"celery\\x06\\x169\" \"1\"\r\n1509363925.121919 [0 127.0.0.1:58206] \"LLEN\" \"celery\"\r\n1509363925.128586 [0 127.0.0.1:58197] \"MULTI\"\r\n1509363925.128613 [0 127.0.0.1:58197] \"ZREM\" \"unacked_index\" \"3c34af07-9190-4271-af12-5b395138bc90\"\r\n1509363925.128638 [0 127.0.0.1:58197] \"HDEL\" \"unacked\" \"3c34af07-9190-4271-af12-5b395138bc90\"\r\n1509363925.128658 [0 127.0.0.1:58197] \"EXEC\"\r\n1509363925.476244 [0 127.0.0.1:58196] \"BRPOP\" \"celery\" \"celery\\x06\\x163\" \"celery\\x06\\x166\" \"celery\\x06\\x169\" \"1\"\r\n1509363925.889793 [0 127.0.0.1:58205] \"PUBLISH\" \"/0.celeryev/worker.heartbeat\" \"{\\\"body\\\": \\\"eyJzd19zeXMiOiAiTGludXgiLCAiY2xvY2siOiA0LCAidGltZXN0YW1wIjogMTUwOTM2MzkyNS44ODg4ODcsICJob3N0bmFtZSI6ICJjZWxlcnlAS2FzaWFub3Ytbm90ZWJvb2siLCAicGlkIjogMjI2OTEsICJzd192ZXIiOiAiNC4xLjAiLCAidXRjb2Zmc2V0IjogLTIsICJsb2FkYXZnIjogWzAuNzYsIDAuOTMsIDEuMDNdLCAicHJvY2Vzc2VkIjogMiwgImFjdGl2ZSI6IDEsICJmcmVxIjogMi4wLCAidHlwZSI6ICJ3b3JrZXItaGVhcnRiZWF0IiwgInN3X2lkZW50IjogInB5LWNlbGVyeSJ9\\\", \\\"headers\\\": {\\\"hostname\\\": \\\"celery@Kasianov-notebook\\\"}, \\\"content-type\\\": \\\"application/json\\\", \\\"properties\\\": {\\\"priority\\\": 0, \\\"body_encoding\\\": \\\"base64\\\", \\\"delivery_info\\\": {\\\"routing_key\\\": \\\"worker.heartbeat\\\", \\\"exchange\\\": \\\"celeryev\\\"}, \\\"delivery_mode\\\": 1, \\\"delivery_tag\\\": \\\"f2aad458-99f4-4bba-85c3-79ebec1aa2bb\\\"}, \\\"content-encoding\\\": \\\"utf-8\\\"}\"\r\n1509363926.678576 [0 127.0.0.1:58196] \"BRPOP\" \"celery\" \"celery\\x06\\x163\" \"celery\\x06\\x166\" \"celery\\x06\\x169\" \"1\"\r\n1509363927.134485 [0 127.0.0.1:58206] \"LLEN\" \"celery\"\r\n1509363927.138225 [0 127.0.0.1:58208] \"LPUSH\" \"celery\" \"{\\\"body\\\": \\\"W1tdLCB7fSwgeyJjaG9yZCI6IG51bGwsICJjYWxsYmFja3MiOiBudWxsLCAiZXJyYmFja3MiOiBudWxsLCAiY2hhaW4iOiBudWxsfV0=\\\", \\\"headers\\\": {\\\"origin\\\": \\\"gen22707@Kasianov-notebook\\\", \\\"root_id\\\": \\\"92178f34-5010-4efb-bdca-7bdf516e991c\\\", \\\"expires\\\": null, \\\"id\\\": \\\"9a842e43-05f7-4d99-8ba0-53dc74a34c4c\\\", \\\"kwargsrepr\\\": \\\"{}\\\", \\\"lang\\\": \\\"py\\\", \\\"retries\\\": 0, \\\"task\\\": \\\"app.tasks.auth.test\\\", \\\"group\\\": null, \\\"timelimit\\\": [null, null], \\\"parent_id\\\": \\\"9a2175e8-b300-422a-a8b2-9b34319ed6c0\\\", \\\"argsrepr\\\": \\\"()\\\", \\\"eta\\\": null}, \\\"content-type\\\": \\\"application/json\\\", \\\"properties\\\": {\\\"priority\\\": 0, \\\"body_encoding\\\": \\\"base64\\\", \\\"correlation_id\\\": \\\"9a842e43-05f7-4d99-8ba0-53dc74a34c4c\\\", \\\"reply_to\\\": \\\"ff72ad1a-efb7-38d0-a59d-26d3cfd07c2d\\\", \\\"delivery_info\\\": {\\\"routing_key\\\": \\\"celery\\\", \\\"exchange\\\": \\\"\\\"}, \\\"delivery_mode\\\": 2, \\\"delivery_tag\\\": \\\"551fafd3-c26a-4a16-abc3-9fc3f89b4371\\\"}, \\\"content-encoding\\\": \\\"utf-8\\\"}\"\r\n1509363927.139366 [0 127.0.0.1:58206] \"LLEN\" \"celery\"\r\n1509363927.139867 [0 127.0.0.1:58197] \"MULTI\"\r\n1509363927.139884 [0 127.0.0.1:58197] \"ZADD\" \"unacked_index\" \"1509363927.139237\" \"551fafd3-c26a-4a16-abc3-9fc3f89b4371\"\r\n1509363927.139916 [0 127.0.0.1:58197] \"HSET\" \"unacked\" \"551fafd3-c26a-4a16-abc3-9fc3f89b4371\" \"[{\\\"body\\\": \\\"W1tdLCB7fSwgeyJjaG9yZCI6IG51bGwsICJjYWxsYmFja3MiOiBudWxsLCAiZXJyYmFja3MiOiBudWxsLCAiY2hhaW4iOiBudWxsfV0=\\\", \\\"headers\\\": {\\\"origin\\\": \\\"gen22707@Kasianov-notebook\\\", \\\"lang\\\": \\\"py\\\", \\\"task\\\": \\\"app.tasks.auth.test\\\", \\\"group\\\": null, \\\"root_id\\\": \\\"92178f34-5010-4efb-bdca-7bdf516e991c\\\", \\\"expires\\\": null, \\\"retries\\\": 0, \\\"timelimit\\\": [null, null], \\\"argsrepr\\\": \\\"()\\\", \\\"eta\\\": null, \\\"parent_id\\\": \\\"9a2175e8-b300-422a-a8b2-9b34319ed6c0\\\", \\\"id\\\": \\\"9a842e43-05f7-4d99-8ba0-53dc74a34c4c\\\", \\\"kwargsrepr\\\": \\\"{}\\\"}, \\\"content-type\\\": \\\"application/json\\\", \\\"properties\\\": {\\\"body_encoding\\\": \\\"base64\\\", \\\"delivery_info\\\": {\\\"routing_key\\\": \\\"celery\\\", \\\"exchange\\\": \\\"\\\"}, \\\"delivery_mode\\\": 2, \\\"priority\\\": 0, \\\"correlation_id\\\": \\\"9a842e43-05f7-4d99-8ba0-53dc74a34c4c\\\", \\\"reply_to\\\": \\\"ff72ad1a-efb7-38d0-a59d-26d3cfd07c2d\\\", \\\"delivery_tag\\\": \\\"551fafd3-c26a-4a16-abc3-9fc3f89b4371\\\"}, \\\"content-encoding\\\": \\\"utf-8\\\"}, \\\"\\\", \\\"celery\\\"]\"\r\n1509363927.140074 [0 127.0.0.1:58197] \"EXEC\"\r\n1509363927.142170 [0 127.0.0.1:58196] \"BRPOP\" \"celery\" \"celery\\x06\\x163\" \"celery\\x06\\x166\" \"celery\\x06\\x169\" \"1\"\r\n1509363927.891929 [0 127.0.0.1:58205] \"PUBLISH\" \"/0.celeryev/worker.heartbeat\" \"{\\\"body\\\": \\\"eyJzd19zeXMiOiAiTGludXgiLCAiY2xvY2siOiA2LCAidGltZXN0YW1wIjogMTUwOTM2MzkyNy44OTA5MjcsICJob3N0bmFtZSI6ICJjZWxlcnlAS2FzaWFub3Ytbm90ZWJvb2siLCAicGlkIjogMjI2OTEsICJzd192ZXIiOiAiNC4xLjAiLCAidXRjb2Zmc2V0IjogLTIsICJsb2FkYXZnIjogWzAuNzYsIDAuOTMsIDEuMDNdLCAicHJvY2Vzc2VkIjogMiwgImFjdGl2ZSI6IDEsICJmcmVxIjogMi4wLCAidHlwZSI6ICJ3b3JrZXItaGVhcnRiZWF0IiwgInN3X2lkZW50IjogInB5LWNlbGVyeSJ9\\\", \\\"headers\\\": {\\\"hostname\\\": \\\"celery@Kasianov-notebook\\\"}, \\\"content-type\\\": \\\"application/json\\\", \\\"properties\\\": {\\\"priority\\\": 0, \\\"body_encoding\\\": \\\"base64\\\", \\\"delivery_info\\\": {\\\"routing_key\\\": \\\"worker.heartbeat\\\", \\\"exchange\\\": \\\"celeryev\\\"}, \\\"delivery_mode\\\": 1, \\\"delivery_tag\\\": \\\"b2d73761-cde3-46a9-b6ae-18418694c74c\\\"}, \\\"content-encoding\\\": \\\"utf-8\\\"}\"\r\n1509363928.141194 [0 127.0.0.1:58206] \"LLEN\" \"celery\"\r\n1509363928.147977 [0 127.0.0.1:58197] \"MULTI\"\r\n1509363928.148017 [0 127.0.0.1:58197] \"ZREM\" \"unacked_index\" \"551fafd3-c26a-4a16-abc3-9fc3f89b4371\"\r\n1509363928.148039 [0 127.0.0.1:58197] \"HDEL\" \"unacked\" \"551fafd3-c26a-4a16-abc3-9fc3f89b4371\"\r\n1509363928.148064 [0 127.0.0.1:58197] \"EXEC\"\r\n1509363928.282117 [0 127.0.0.1:58196] \"BRPOP\" \"celery\" \"celery\\x06\\x163\" \"celery\\x06\\x166\" \"celery\\x06\\x169\" \"1\"\r\n1509363929.484594 [0 127.0.0.1:58196] \"BRPOP\" \"celery\" \"celery\\x06\\x163\" \"celery\\x06\\x166\" \"celery\\x06\\x169\" \"1\"\r\n1509363929.792864 [0 127.0.0.1:58205] \"PUBLISH\" \"/0.celeryev/worker.offline\" \"{\\\"body\\\": \\\"eyJzd19zeXMiOiAiTGludXgiLCAiY2xvY2siOiA4LCAidGltZXN0YW1wIjogMTUwOTM2MzkyOS43OTE2MTQsICJob3N0bmFtZSI6ICJjZWxlcnlAS2FzaWFub3Ytbm90ZWJvb2siLCAicGlkIjogMjI2OTEsICJzd192ZXIiOiAiNC4xLjAiLCAidXRjb2Zmc2V0IjogLTIsICJsb2FkYXZnIjogWzAuNzYsIDAuOTMsIDEuMDNdLCAicHJvY2Vzc2VkIjogMywgImFjdGl2ZSI6IDEsICJmcmVxIjogMi4wLCAidHlwZSI6ICJ3b3JrZXItb2ZmbGluZSIsICJzd19pZGVudCI6ICJweS1jZWxlcnkifQ==\\\", \\\"headers\\\": {\\\"hostname\\\": \\\"celery@Kasianov-notebook\\\"}, \\\"content-type\\\": \\\"application/json\\\", \\\"properties\\\": {\\\"priority\\\": 0, \\\"body_encoding\\\": \\\"base64\\\", \\\"delivery_info\\\": {\\\"routing_key\\\": \\\"worker.offline\\\", \\\"exchange\\\": \\\"celeryev\\\"}, \\\"delivery_mode\\\": 1, \\\"delivery_tag\\\": \\\"3a3d43e0-cc44-41fe-a05e-3cbb746536a0\\\"}, \\\"content-encoding\\\": \\\"utf-8\\\"}\"\r\n1509363929.793528 [0 127.0.0.1:58203] \"UNSUBSCRIBE\" \"/0.celeryev/worker.*\"\r\n1509363929.794522 [0 127.0.0.1:58209] \"SREM\" \"_kombu.binding.celeryev\" \"worker.#\\x06\\x16\\x06\\x16celeryev.d4e64017-e307-4a73-a304-18dc80da9a71\"\r\n1509363929.794785 [0 127.0.0.1:58209] \"MULTI\"\r\n1509363929.794807 [0 127.0.0.1:58209] \"DEL\" \"celeryev.d4e64017-e307-4a73-a304-18dc80da9a71\"\r\n1509363929.794817 [0 127.0.0.1:58209] \"DEL\" \"celeryev.d4e64017-e307-4a73-a304-18dc80da9a71\\x06\\x163\"\r\n1509363929.794827 [0 127.0.0.1:58209] \"DEL\" \"celeryev.d4e64017-e307-4a73-a304-18dc80da9a71\\x06\\x166\"\r\n1509363929.794836 [0 127.0.0.1:58209] \"DEL\" \"celeryev.d4e64017-e307-4a73-a304-18dc80da9a71\\x06\\x169\"\r\n1509363929.794846 [0 127.0.0.1:58209] \"EXEC\"\r\n1509363929.797285 [0 127.0.0.1:58210] \"SREM\" \"_kombu.binding.celery.pidbox\" \"\\x06\\x16\\x06\\x16celery@Kasianov-notebook.celery.pidbox\"\r\n1509363929.797705 [0 127.0.0.1:58210] \"MULTI\"\r\n1509363929.797737 [0 127.0.0.1:58210] \"DEL\" \"celery@Kasianov-notebook.celery.pidbox\"\r\n1509363929.797766 [0 127.0.0.1:58210] \"DEL\" \"celery@Kasianov-notebook.celery.pidbox\\x06\\x163\"\r\n1509363929.797787 [0 127.0.0.1:58210] \"DEL\" \"celery@Kasianov-notebook.celery.pidbox\\x06\\x166\"\r\n1509363929.797807 [0 127.0.0.1:58210] \"DEL\" \"celery@Kasianov-notebook.celery.pidbox\\x06\\x169\"\r\n1509363929.797830 [0 127.0.0.1:58210] \"EXEC\"\r\n1509363930.154092 [0 127.0.0.1:58206] \"LLEN\" \"celery\"\r\n1509363930.156149 [0 127.0.0.1:58208] \"LPUSH\" \"celery\" \"{\\\"body\\\": \\\"W1tdLCB7fSwgeyJjaG9yZCI6IG51bGwsICJjYWxsYmFja3MiOiBudWxsLCAiZXJyYmFja3MiOiBudWxsLCAiY2hhaW4iOiBudWxsfV0=\\\", \\\"headers\\\": {\\\"origin\\\": \\\"gen22707@Kasianov-notebook\\\", \\\"root_id\\\": \\\"92178f34-5010-4efb-bdca-7bdf516e991c\\\", \\\"expires\\\": null, \\\"id\\\": \\\"b4cced72-a331-4bdd-b310-c87b2fa96b72\\\", \\\"kwargsrepr\\\": \\\"{}\\\", \\\"lang\\\": \\\"py\\\", \\\"retries\\\": 0, \\\"task\\\": \\\"app.tasks.auth.test\\\", \\\"group\\\": null, \\\"timelimit\\\": [null, null], \\\"parent_id\\\": \\\"9a842e43-05f7-4d99-8ba0-53dc74a34c4c\\\", \\\"argsrepr\\\": \\\"()\\\", \\\"eta\\\": null}, \\\"content-type\\\": \\\"application/json\\\", \\\"properties\\\": {\\\"priority\\\": 0, \\\"body_encoding\\\": \\\"base64\\\", \\\"correlation_id\\\": \\\"b4cced72-a331-4bdd-b310-c87b2fa96b72\\\", \\\"reply_to\\\": \\\"ff72ad1a-efb7-38d0-a59d-26d3cfd07c2d\\\", \\\"delivery_info\\\": {\\\"routing_key\\\": \\\"celery\\\", \\\"exchange\\\": \\\"\\\"}, \\\"delivery_mode\\\": 2, \\\"delivery_tag\\\": \\\"470cfc21-46db-4655-aa18-8aa78ff4aea8\\\"}, \\\"content-encoding\\\": \\\"utf-8\\\"}\"\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4354/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/1844ece4b091ec454389ea378bae4917fa48b899", "message": "fix issue with not running due tasks after beat restart (#4493)\n\n* fix issue with not running due tasks after beat restart\r\n\r\n* add myself to contibutors\r\n\r\n* fix import sort"}, {"url": "https://api.github.com/repos/celery/celery/commits/187026887ae0e39da87888393dbb6ff34c88936a", "message": "fix crontab description of month_of_year behaviour (#4227)\n\n* fix crontab description of month_of_year behaviour\r\n\r\n* fix examle description"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "futujaos": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4345", "title": "Task-related events are not automatically deleted for MongoDB broker", "body": "_This is a duplicate of https://github.com/celery/kombu/issues/811, maybe this repo is more suitable place for this issue._\r\n\r\nCelery version: 4.1.0\r\n\r\nWhen I started celery worker with option [`--task-events`](http://docs.celeryproject.org/en/latest/reference/celery.bin.worker.html#cmdoption-celery-worker-e), I noticed that the events are not automatically deleted from the queue.\r\n\r\nWhile investigating this issue I found two problems:\r\n1. There is no implementation for method [`prepare_queue_arguments`](https://github.com/celery/kombu/blob/de87d2f1fa45bf120b4d8a82e1978d4052a46853/kombu/transport/base.py#L98) in [MongoDB channel](https://github.com/celery/kombu/blob/de87d2f1fa45bf120b4d8a82e1978d4052a46853/kombu/transport/mongodb.py#L80) class. Because of that, channel can't return settings `x-message-ttl` and `x-expires` for celery queues. The implementation may be similar to the [RabbitMQ implementation](https://github.com/celery/kombu/blob/de87d2f1fa45bf120b4d8a82e1978d4052a46853/kombu/transport/librabbitmq.py#L64).\r\n2. [The code](https://github.com/celery/kombu/blob/de87d2f1fa45bf120b4d8a82e1978d4052a46853/kombu/transport/mongodb.py#L171) that inserts task-related event into a MongoDB `messages` collection is trying to get field `expired_at` from the queue with name `celeryev-{GUID}`, but there is no queue with such name in collection `events`, so field `expired_at` for task-related events equals `null`, and because of that it's not removed from `messages` by TTL index.\r\n\r\nWe created cron task, which removes all documents from `messages` collection with `queue` field like `celeryev*`, as a temporary workaround, but it would be great if automatic deletion of task-related events worked for MongoDB.\r\n\r\nThanks in advance and sorry for my poor English.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4345/reactions", "total_count": 4, "+1": 4, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zhangzhen": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4342", "title": "celery tasks cannot run after cythonizing", "body": "Python: 3.5\r\nCelery: 4.1.0\r\n\r\nThe code for celery tasks has been cythonized.\r\nI started a worker as follows:\r\n`celery worker -l info -A chrombackend.app.celery --concurrency=1`\r\nI got the following error:\r\n```\r\n...\r\n  File \"/home/zhangz/Envs/chromgo2/lib/python3.5/site-packages/celery/app/base.py\", line 453, in _task_from_fun\r\n    '__header__': staticmethod(head_from_fun(fun, bound=bind)),\r\n  File \"/home/zhangz/Envs/chromgo2/lib/python3.5/site-packages/celery/utils/functional.py\", line 284, in head_from_fun\r\n    namespace = {'__name__': fun.__module__}\r\nAttributeError: 'method-wrapper' object has no attribute '__module__'\r\n```\r\nCan someone help me to solve this problem?\r\n\r\nCheers,\r\nZhen Zhang", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4342/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 1, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dsphper": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4340", "title": "Persistent data not restored.", "body": "Environment\r\nPython: 3.4\r\nCelery: 4.1.0\r\nFlower: 0.9.0\r\nCentos: 7.0\r\n--persistent flag is used. Celery version v4.1.0.\r\n\r\nIf I create a couple of tasks, they run as expected.\r\n\r\nAfter I send a SIGINT:\r\n\r\n[D 150923 14:43:09 events:96] Saving state to 'flower'...\r\n[D 150923 14:43:09 events:97] <State: events=54 tasks=4>\r\n\r\nThe DB file 'flower' clearly contains the correct data. When I start flower again:\r\n\r\n[D 150923 14:47:35 events:76] Loading state from 'flower'...\r\n[D 150923 14:47:35 events:80] <State: events=0 tasks=0>\r\n\r\nIf I run Python and load the file with shelve:\r\n\r\n> f['events']\r\n<State: events=0 tasks=0>\r\n\r\nSo, something isn't working correctly when shelve reads the file.\r\nWait all day.\r\nPlease help me.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4340/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vladcalin": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4334", "title": "Improving the development process", "body": "Hello celery community.\r\n\r\nI have a proposition that I believe can improve the development process and allow new contributors to get involved easier in the development of celery:\r\n\r\nI propose to have some easy to spin up development process using docker compose or vagrant that will build the development process in one shot without having to configure databases, brokers, the celery app instance, etc.\r\n\r\nI am thinking about having a single script that will spin up a ready development with the required dependecies.\r\n\r\nSome high-level example of what I am thinking:\r\n\r\n```\r\n$ setup_celery_dev python=3.6 os=ubuntu broker=redis backend=cassandra test_project=../weird-bug-example\r\n```\r\nAnd after running the command, the developer should have a VM with ubuntu, python3.6, celery and all dependencies installed with redis,cassandra extras, and a mounted volume with some source code that can reproduce some bug, and maybe some aliases for starting a worker, starting celerybeat, etc.\r\n\r\nAlso, a tag for \"Easy picklings\" (like django has) can allow new contributors to pick easier some issues to resolve as a way to learning to contribute to this project.\r\n\r\nWhat do you think?", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4334/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wosc": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4323", "title": "PROC_ALIVE_TIMEOUT should always apply (and be configurable)", "body": "I have verified that the issue exists against the `master` branch of Celery, although I'm currently using 4.0.2.\r\n\r\nAfter setting a `task_time_limit`, the worker process was killed when running up against the timeout, but then did not restart. The log showed lots of \"celery.concurrency.asynpool  Timed out waiting for UP message from <Process(PoolWorker-19, started daemon)>\". I've tracked this down to mean that the worker process did not start up within [PROC_ALIVE_TIMEOUT](https://github.com/celery/celery/blob/master/celery/concurrency/asynpool.py#L90). (Which makes sense, since we have to do somewhat expensive setup in `on_worker_process_init` in this project.)\r\n\r\nNow I have two questions:\r\n\r\n1. This timeout obviously does not apply when the worker is started normally, only when it is killed by a timeout, since otherwise we'd never get a worker to run at all, since our setup always takes longer than 4 seconds. Is that intentional? I found it quite confusing, like, why wouldn't the worker restart, when it normally starts up fine?\r\n2. Can PROC_ALIVE_TIMEOUT be configured? From my reading the code it looks hardcoded; I've resorted to monkeypatching it for now.\r\n\r\nThanks for your consideration.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4323/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mvaled": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4321", "title": "Worker hangs and stop responding after max tasks per worker is reached (sometimes).", "body": "This is an ongoing investigation.  What I'm putting here what I can see, so far.\r\n\r\nI have seen this problem since 4.0.2, and now I've just spot it in 4.1.0.  These are the versions of the packages I'm using:\r\n\r\n```\r\nkombu = 4.1.0\r\ncelery = 4.1.0\r\namqp = 2.2.1\r\nbilliard = 3.5.0.3\r\nredis = 2.10.6\r\n```\r\n\r\nThe problem seems to happen when a worker with `-c 1`, and a `worker_max_tasks_per_child=2000`.  This worker also processes tasks from a single queue 'odoo-8.0.cdr'. Just after the 2000th task it hangs.  Strangely, the issue does not happen every time the worker reaches this threshold, but when it happens it happens at it.  \r\n\r\n![screenshot from 2017-10-10 12-02-08](https://user-images.githubusercontent.com/270825/31397028-e2dbf624-adb2-11e7-909e-b75add72870e.png)\r\n\r\nThis is a production server so I can't really test with master branch.  I'll try to reproduce on a development box later.\r\n\r\nstracing the Worker yields:\r\n\r\n```\r\nroot@mercurio:~# strace -p 9105\r\nProcess 9105 attached\r\nrecvfrom(27, \r\n```\r\n\r\nI waited for 5+ minutes and it didn't received anything.  The file descriptors:\r\n\r\n```\r\n[celeryd: 9105 mercurio   13u  sock       0,6      0t0 273693676 can't identify protocol\r\n[celeryd: 9105 mercurio   14u  sock       0,6      0t0 273693673 can't identify protocol\r\n[celeryd: 9105 mercurio   15u  0000       0,9        0      4047 [eventpoll]\r\n[celeryd: 9105 mercurio   16u  sock       0,6      0t0 273693679 can't identify protocol\r\n[celeryd: 9105 mercurio   17u  0000       0,9        0      4047 [eventpoll]\r\n[celeryd: 9105 mercurio   18u  0000       0,9        0      4047 [eventpoll]\r\n[celeryd: 9105 mercurio   19u  sock       0,6      0t0 273693682 can't identify protocol\r\n[celeryd: 9105 mercurio   20u  sock       0,6      0t0 273693685 can't identify protocol\r\n[celeryd: 9105 mercurio   21u  sock       0,6      0t0 273693942 can't identify protocol\r\n[celeryd: 9105 mercurio   22u  sock       0,6      0t0 273693945 can't identify protocol\r\n[celeryd: 9105 mercurio   23u  sock       0,6      0t0 273693948 can't identify protocol\r\n[celeryd: 9105 mercurio   24u  sock       0,6      0t0 273693951 can't identify protocol\r\n[celeryd: 9105 mercurio   25u  IPv4 288763673      0t0       TCP localhost.localdomain:56030->localhost.localdomain:6379 (ESTABLISHED)\r\n[celeryd: 9105 mercurio   26r  FIFO       0,8      0t0 288763672 pipe\r\n[celeryd: 9105 mercurio   27u  IPv4 288763676      0t0       TCP localhost.localdomain:56031->localhost.localdomain:6379 (ESTABLISHED)\r\n```\r\n\r\nshow that 27 is the redis connection, but there are several fds for which `lsof` can't indentify protocol.  Half-closed connections?\r\n\r\nThe ForkPoolWorker strace shows:\r\n\r\n```\r\nroot@mercurio:~# strace -p 23943\r\nProcess 23943 attached\r\nread(7, \r\n```\r\n\r\nI also waited for 5+ minutes here to see if anything happen.  Nothing.  `lsof` shows:\r\n\r\n```\r\n[celeryd: 23943 mercurio    6r   CHR    1,9      0t0     18535 /dev/urandom\r\n[celeryd: 23943 mercurio    7r  FIFO    0,8      0t0 273693670 pipe\r\n[celeryd: 23943 mercurio   10r   CHR    1,9      0t0     18535 /dev/urandom\r\n[celeryd: 23943 mercurio   11w  FIFO    0,8      0t0 273693671 pipe\r\n[celeryd: 23943 mercurio   12r   CHR    1,3      0t0     18531 /dev/null\r\n[celeryd: 23943 mercurio   15u  0000    0,9        0      4047 [eventpoll]\r\n[celeryd: 23943 mercurio   17u  0000    0,9        0      4047 [eventpoll]\r\n[celeryd: 23943 mercurio   26w  FIFO    0,8      0t0 288763672 pipe\r\n```\r\n\r\nSo 7 is a FIFO pipe (I don't know which).\r\n\r\nI'm sure that the queue gets a task every minute, since our application issues a new 'odoo-8.0.cdr' task per minute (like a clock).  I can see the backlog:\r\n\r\n```\r\nroot@mercurio:~# redis-cli -n 9  llen 'odoo-8.0.cdr'\r\n(integer) 4586\r\n```\r\n\r\nOne minute later:\r\n\r\n```\r\nroot@mercurio:~# redis-cli -n 9  llen 'odoo-8.0.cdr'\r\n(integer) 4587\r\n```\r\n\r\nTo resume operation I have to restart the whole worker and this managed to process all the backlog (all tasks expire after 65s in the queue) and then, most of the time the worker keeps worker even beyond the 2000th task.\r\n\r\nKilling the child process shows that the worker does not even reap the child:\r\n\r\n```\r\nroot@mercurio:~# ps ax | grep cdr\r\n 9105 ?        S      2:32 [celeryd: cdr-1@mercurio:MainProcess] -active- (celery worker -l INFO -n cdr-1@%h -c1 -Q odoo-8.0.cdr)\r\n23943 ?        Z      0:00 [[celeryd: cdr-1] <defunct>\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4321/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/215376d8f873aad59eb8a7e4f96ca6b0834844cb", "message": "Allow to have a custom request (#3977)\n\n* Allow custom Request, aka custom `on_timeout`.\r\n\r\nAllowing a custom Request eases the task of handling timeouts (even hard\r\ntimeouts).\r\n\r\nRationale\r\n\r\nSome (poorly written) bits of code catch exceptions quite broadly:\r\n\r\n  try:\r\n      ...\r\n  except:\r\n      ...\r\n\r\nThis hurts tasks when a SoftTimeLimitError is raised inside such blocks of\r\ncode.  Rewriting those smelly bits of code can take a lot of effort, and\r\nsometimes, the code belongs to a third-party library which makes the task even\r\nharder.\r\n\r\nUsing a custom request allows to catch hard time limits.\r\n\r\nYour app can be customized like:\r\n\r\n   from celery import Task as BaseTask\r\n   from celery.worker.request import Request as BaseRequest\r\n\r\n   class Request(BaseRequest):\r\n       def on_timeout(self, soft, timeout):\r\n          super(Request, self).on_timeout(soft, timeout)\r\n          if not soft:\r\n\t     print('Something hard hit me!')\r\n\r\n    class MyTask(BaseTask):\r\n        Request = Request\r\n\r\n    @app.task(base=MyTask, bind=True)\r\n    def sometask(self):\r\n        pass\r\n\r\n* Check signatures' types have a default Request.\r\n\r\n* Test Request is customizable per Task class.\r\n\r\n* Document custom requests.\r\n\r\n* Exemplify the usage of the custom requests."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/133750478", "body": "Done.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133750478/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "julmarci": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4317", "title": "Strange coupling between sender and worker when having massive tasks, gevent and openurl.", "body": "## Checklist\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\nEnvironment: celery 4.1, rabbitmq and rpc backend.\r\n\r\nI'm using apscheduler in order to schedule massive groups of tasks (hundred or thousands) and be able to dynamically change that scheduling. Both apscheduler and celery worker are using gevent so that huge number of corrutines (pseudo-threads) can be launch concurrently to access urls with async I/O.\r\n\r\nRight now I'm just doing some tests, so I open ulrs like google.com or bing.com which reply very fast. The weird issue is that everything runs quick and smoothly right up until the very moment when the tasks in the worker finish. At this point the rest of results start to arrive exactly one per second while the previous ones were arriving in no time.\r\n\r\nSome hints:\r\n- This only happens when the tasks open an url (with urllib2 or requests, it doesn't matter)\r\n- It happens indistinctly launching a few or a lot of tasks, when the worker finish (at second 1 or at second 20), the frequency of result arrivals slow downs.\r\n- This happens when I do AsyncResult.get() but not when using AsyncResult.result (but in this case I don't know how to remove the tasks from the queue :-(  )\r\n- This only happens when I launch all the tasks at once. If I do all the work in a for loop everything runs alright.\r\n\r\nThis is driving me crazy. Any idea of what can be happening?\r\nIn fact, for my work what would come best would be knowing how to delete tasks after doing the AsyncResult.result (by the way that gevent works)\r\n\r\n\r\n## Update\r\n\r\nI've been playing with the tests looking for a workaround and what I have found is:\r\n\r\n- this issue happens with the rpc backend but not with amqp. However, you know, amqp is deprecated and not efficient.\r\n- I tryed to gather the results with a pool of threads instead of greenlets and the consequence is a \"socket.timeout\" (again it does not happen when using amqp backend)\r\n- and the weirdest thing is that if I call the AsyncResult.ready() function before the AsyncResult.get() this strange coupling goes away but the tasks messages are not acknowledge (the same thing that ocurred when reading just with the AsyncResult.result) This is the state of queue messages when the tasks are completed (as shown with Flower):\r\n\r\n         Ready: 1\r\n         Unacked: 499\r\n         Total: 500\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4317/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "codingjoe": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4316", "title": "pseudo random jitter can cause dead lock", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\nVersions affected: None (not yet released)\r\nOn master: Yes, added in 0d5b840af1890a9a499a339aa3256445b43837dc\r\n\r\n## Steps to reproduce\r\n\r\nhttps://github.com/celery/celery/blob/d59518f5fb68957b2d179aa572af6f58cd02de40/celery/utils/time.py#L390-L404\r\n\r\nImage you have two workers that pull the same task from a queue at the same time and fail, do tue a lock for example. If you would retry and backoff, we have the `jitter` that should prevent those tasks to be executed at the same time and thus preventing a deadlock. But `random.seed` will default to the current system time, if there is no better function provided. This can mean that two if two workers would fail at the same milli-second, they would in fact end up in a dead lock, since the `jitter` will be the same, since the seed is the same.\r\n\r\n## Expected behavior\r\n\r\nIn python 3.5 we have secrets, which would prevent this. In Python 2 we have to find an alternative. If we have a unique identifier for a worker, we can add this to the epoch as a new and unique seed.\r\n\r\n## Actual behavior\r\n\r\nTwo or more tasks could be dead locked because of the pseudo random `jitter`.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4316/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/20182656", "body": "Cool, I had the same issue. I'd much appreciate this to be released in 4.0.2 \u2764\ufe0f ", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/20182656/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/24441502", "body": "Is there any chance this is getting released soon? I see this already in the documentation, but is it going to stay? Would be good if there would be a 4.2 release for that.", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/24441502/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/24825012", "body": "@thedrow yes, sorry. I saw your milestone right after you I posted this. I understand that a project of this scale has a larger release cycle and doesn't release every single commit.", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/24825012/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/24825131", "body": "https://github.com/celery/celery/issues/4316", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/24825131/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "johnarnold": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4309", "title": "Results backend has minimal data", "body": "This came up in:\r\nhttps://github.com/mher/flower/pull/724\r\nand\r\nhttps://github.com/mher/flower/issues/542\r\n\r\nFlower subscribes to celery events and caches them.  However, if flower misses an event (or many, i.e. if it's rebooted), it's cache is inconsistent.  Games played with persisting the cache don't solve the missed event problem.\r\n\r\nThe simple suggestion is \"get state from the results backend;\" however, the results backend has minimal metadata about the task.\r\n\r\nIs there a design decision specifically to NOT update the results backend with \"TaskState\" data?  I get that events are intended for realtime consumption, and make sense for in-flight state changes (task sent, etc) but it seems like the worker would write the task state to the backend when it's \"finished\" with a task?\r\n\r\nI started going down the road of \"implement a snapshot camera, archive to database,\" but that seems really redundant with setting up a backend.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4309/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4490", "title": "Add task properties to AsyncResult, store in backend", "body": "This pr extends AsyncResult and Backend in order to store additional properties of the task execution.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/149858867", "body": "If the module is being used in a task, it's already inside a trust boundary.  So some trust is assumed.\r\n\r\nMy head goes towards sandboxing the Exception inspection.  Like, run it in a locked down subprocess, just to get the signature.  I need to look at the code closer though.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/149858867/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "donkopotamus": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4308", "title": "Worker dies if unable to decode a message", "body": "## Checklist\r\n```\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:2.7.13\r\n            billiard:3.5.0.3 py-amqp:2.2.1\r\nplatform -> system:Windows arch:64bit, WindowsPE imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:pyamqp results:rpc:///\r\n\r\nresult_serializer: 'some-customised-json'\r\nworker_max_tasks_per_child: 1\r\nworker_prefetch_multiplier: 1\r\ntask_track_started: True\r\naccept_content: set(['json', 'some-customised-json'])\r\nbroker_url: u'amqp://guest: .....'\r\nresult_backend: u'rpc:///'\r\n```\r\n## Steps to reproduce\r\n\r\nUse a custom serialiser that is capable of encoding an object yet not capable of decoding it.\r\n\r\n## Expected behavior\r\n\r\nThe worker should reject or swallow the message.\r\n\r\n## Actual behavior\r\n\r\nA message with a payload that the worker is unable to decode will crash the worker.  This might happen:\r\n\r\n- if the message is expired, then `req.revoked` causes an implicit decoding (to check if it is a chord); or\r\n- when the request is invoked via `consumer.on_task_request`\r\n\r\nIf the message is carrying an undecodable payload then at both these points a `kombu.DecodeError` could be raised and remain uncaught, causing the worker to die.\r\n\r\n```\r\n# in celery/worker/strategy.py:default\r\ndef task_message_handler(message, body, ack, reject, callbacks,\r\n                         to_timestamp=to_timestamp):\r\n    ... \r\n    req = Req(\r\n        message,\r\n        on_ack=ack, on_reject=reject, app=app, hostname=hostname,\r\n        eventer=eventer, task=task, connection_errors=connection_errors,\r\n        body=body, headers=headers, decoded=decoded, utc=utc,\r\n    )\r\n    if _does_info:\r\n        info('Received task: %s', req)\r\n    if (req.expires or req.id in revoked_tasks) and req.revoked(): # <==== DecodeError possible\r\n        return\r\n    ... \r\n    if req.eta:\r\n    else:\r\n        ... \r\n        handle(req)  # <==== DecodeError possible\r\n```\r\n\r\nIn contrast, protocol version 1 style messages were decoded earlier and a decode error handled, in this stanza:\r\n\r\n```\r\n# in celery/worker/consumer/consumer.py:Consumer.create_task_handler:on_task_received\r\ntry:\r\n    type_ = message.headers['task']                # protocol v2\r\nexcept TypeError:\r\n    return on_unknown_message(None, message)\r\nexcept KeyError:\r\n    try:\r\n        payload = message.decode()                 # <=== decode errors were caught\r\n    except Exception as exc:  # pylint: disable=broad-except\r\n        return self.on_decode_error(message, exc)\r\n    try:\r\n        type_, payload = payload['task'], payload  # protocol v1\r\n    except (TypeError, KeyError):\r\n        return on_unknown_message(payload, message)\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4308/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/cd89518cf2ef18aaf739eac06aaf28a2e3d0fffa", "message": "Handle possibility there are no workers (#4074)\n\nFixes issue where an exception is raised if eg `celery graph workers` is invoked when there are no workers at all."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jobec": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4307", "title": "crontab schedule does not work like a real crontab entry when combining day or week and day of month", "body": "Celery's crontab schedules don't follow the \"official\" crontab schedules when combining a day of week and day of month value. **I like Celery's behavior more than that of crontab, but this should be explicitly mentioned in the documentation.** Because, if you're using online calculators like http://crontab.guru for decoding more difficult crontabs, the \"next date\" they advertise is wrong.\r\n\r\nAccording to the [crontab(5) manpage](https://www.freebsd.org/cgi/man.cgi?crontab(5)):\r\n\r\n> Note: The day of a command's execution can be specified by two fields - day of month, and day of week. If both fields are restricted (ie, aren't *), the command will be run **when either field matches** the current time. For example,\r\n\"30 4 1,15 * 5\" would cause a command to be run at 4:30 am on the 1st and 15th of each month, plus every Friday. \r\n\r\n\r\n\r\n## Steps to reproduce\r\nCelery beat entry:\r\n```python\r\nCELERY_BEAT_SCHEDULE = {\r\n    'example': {\r\n        'task': 'my.task',\r\n        'schedule': crontab(minute=0, hour=22, day_of_month='1-7', day_of_week='saturday')\r\n    }\r\n}\r\n```\r\n## Expected behavior\r\nTask runs on first 7 days of the month **AND** every saturday.\r\n\r\n## Actual behavior\r\nTask runs on a saturday that falls in first 7 days of the month. (Which is actually good behavior AFAIC)", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4307/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/4099", "title": "link_error argument of apply_async doesn't work as documented", "body": "## Checklist\r\n```\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:3.5.3\r\n            billiard:3.5.0.2 redis:2.10.5\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis://localhost/\r\n```\r\n\r\nThe documentation about linking tasks at http://docs.celeryproject.org/en/master/userguide/calling.html#linking-callbacks-errbacks doesn't work. It also doesn't match the documentation at http://docs.celeryproject.org/en/master/userguide/canvas.html#chains => \"Here's an example errback...\" which does work.\r\n\r\n## Steps to reproduce\r\nFollow the code example for linking tasks at http://docs.celeryproject.org/en/master/userguide/calling.html#linking-callbacks-errbacks\r\n\r\n## Expected behavior\r\nPrints the error and traceback\r\n\r\n## Actual behavior\r\nproduces a `RuntimeError: Never call result.get() within a task!` exception.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4099/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mooshu1x2": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4305", "title": "Improve Elasticsearch configuration/need support for pipelines", "body": "Story\r\n-----\r\nI've been using Kibana as a dashboard to monitor my results in aggregate. I would like to store and parse celery results in my Elasticsearch backend to be viewed in Kibana. Currently this can be accomplished by setting up a pipeline (Elasticsearch 5.x and above). This request is to add an optional setting in the Elasticsearch backend configuration to point to a pipeline id. Ultimately, it would be nice to further configure the Elasticsearch backend by passing in additional parameters since the options are limited currently (e.g. control refresh rate, support multiple Elasticsearch endpoints).\r\n\r\nIf possible, I can provide a solution and submit a merge request. \r\n\r\nIdeally, this is how the changes in configuration would look like:\r\n\r\nProposed (similar to Cassandra backend)\r\n-----------------------------------------\r\n\r\n```python\r\nbackend = 'elasticsearch'\r\nelasticsearch_servers = ['localhost']\r\nelasticsearch_auth_kwargs = {\r\n    username: 'elastic',\r\n    password: 'changeme'\r\n}\r\nelasticearch_port = 9200\r\nelasticsearch_index = 'celery'\r\nelasticsearch_doctype = 'task'\r\nelasticsearch_pipeline = None\r\nelasticsearch_refresh_rate = '5s'\r\nelasticsearch_retry_on_timeout = False\r\nelasticsearch_max_retries = 3\r\nelasticsearch_timeout = 10.0\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4305/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "paramono": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4304", "title": "Celery Beat daemonization with systemd (clarify docs?)", "body": "I would like to daemonize launch of celery beat. I am using systemd.\r\n\r\n[Periodic Tasks page](https://github.com/celery/celery/blob/fcec01f6e041a70e5ddd061beba5fccb32d74e24/docs/userguide/periodic-tasks.rst) in the docs says the following:\r\n\r\n>To daemonize beat see [daemonizing](https://github.com/celery/celery/blob/fcec01f6e041a70e5ddd061beba5fccb32d74e24/docs/userguide/daemonizing.rst).\r\n\r\nAnd I see that there are different initd scripts for celery and celery beat. However, the celery.service example for systemd works with celery multi only.\r\n\r\nSo what is the preferred way to run beat in production with systemd? Should I create a separate service for celery beat, or should I use single systemd service and supply celery multi with the `--beat` option, like suggested here: https://stackoverflow.com/a/23353596/5618728\r\n\r\nThe Periodic Tasks page says:\r\n\r\n>You can also embed beat inside the worker by enabling the workers -B option, this is convenient if you\u2019ll never run more than one worker node, but it\u2019s not commonly used and for that reason isn\u2019t recommended for production use:\r\n\r\nThe same probably applies to running `celery multi` with `--beat`, but I wanted to know for sure what are the best practices for systemd users.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4304/reactions", "total_count": 7, "+1": 7, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "allanlei": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4298", "title": "group.skew() removes all tasks if group input is generator", "body": "## Checklist\r\n\r\n- [X] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n\r\n      python 3.6.2\r\n      celery==4.1.0\r\n      amqp==2.2.1\r\n      billiard==3.5.0.3\r\n\r\n- [X] I have verified that the issue exists against the `master` branch of Celery.\r\n      As of `be55de6`\r\n\r\n## Steps to reproduce\r\n```\r\n@celery.task(bind=True)\r\ndef noop(task):\r\n    pass\r\n```\r\n\r\n1. `group = celery.group(noop.si() for i in range(10))`\r\n2. `group.skew()`  -> `group(<empty>)`\r\n3. `group.apply_async()` or `group()`  -> `<GroupResult: 66fd517e-123b-4299-8cbe-9183b3f02626 []>`\r\n\r\n## Expected behavior\r\n```\r\n<GroupResult: d610c923-e939-4199-9bb7-7bc89daa2ccb [9152bd60-f815-4bbe-a407-a003db34b19d, 7fc2d0ac-72ab-48b1-9083-be0f3b50b00d, d7c0a00a-85c3-4780-88f9-23be8da6fb73, 3737944f-91a4-4886-95c6-4960ef4764a7, a8da96ea-d2c4-4876-8026-e0c24c7d508a, 83ee5316-fb2f-472b-8e94-284144f92438, 5715a751-f28d-4deb-ac15-114854570fde, bc1646bf-cec9-4740-8e74-411d7b66dcca, 337aa1f3-5606-45b2-af68-5c5527289019, f5fffb98-f2f5-4423-a656-8c1d7e93291a]>\r\n```\r\n\r\n## Actual behavior\r\n\r\n```\r\n<GroupResult: 66fd517e-123b-4299-8cbe-9183b3f02626 []>\r\n```\r\n\r\n### Notes\r\nIf you `print(group)`(or something that evaluates `self.tasks`) before calling `group.skew()`, it works normally. ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4298/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Checkroth": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4284", "title": "Environment variable for Broker URL is prioritized over app settings", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nCelery version: 4.0.2 (latentcall)\r\nIssue is about the celery python app, which we are currently using with django (not django-celery).\r\nIssue can be seen here: https://github.com/celery/celery/blob/master/celery/app/utils.py#L85-L106\r\n\r\nMy team had a misunderstanding about Celery settings and environment variables (we thought the celery app would set the config from environment variables automatically using `config_from_object`). We had CELERY_RESULT_BACKEND and CELERY_BROKER_URL set in our environment variables. This worked perfectly for a while, until we started doing operations that relied on the result backend.\r\n\r\nIt took a while to figure out _why_ the broker url was working fine but the backend was not. After some investigation, I realized that the broker url wasn't set at all, and that the call to `broker_url` was actually using the environment variable we had set instead of the app settings.\r\n\r\n## Expected behavior\r\nCelery broker settings should take priority. Currently, even if we had properly configured celery, a stray environment variable could break our application.\r\n\r\nIdeally, the broker url would not be read from environment variables at all, at least not at the level in which it is currently read, as this only causes confusion. I am not familiar with why it is implemented this way, but it looks like it is probably a bit of legacy code.\r\n\r\n## Actual behavior\r\nCelery broker url is read from environment variables before app settings.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4284/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4285", "title": "Add failing test for broker url priority ref issue #4284", "body": "## Description\r\nThis PR is to assist in showing the issue described here: https://github.com/celery/celery/issues/4284\r\n\r\nAll this PR does is add a failing test that _should_ be passing, if the issue above is resolved.\r\n\r\nThis PR does not solve the issue mentioned. I leave that up to the discretion of a more knowledgeable party, if they decide that it is indeed an issue at all. I do not expect this PR to be merged unless the issue is resolved and you want this test to remain in the repository.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/140698917", "body": "I got that feeling when I wrote this, however calling `self.app.conf.broker_url` after setting it to `prepatch_broker_url`, but before setting `CELERY_BROKER_URL` in the environment, it does return the `prepatch_broker_url` string\r\n\r\nI've added a second test demonstrating this", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/140698917/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "agladkov": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4281", "title": "Different behavior on group's link/link_error usage", "body": "## Steps to reproduce\r\n\r\n    g = group(sig1, sig2)\r\n    g.link(sig3) # links to sig1\r\n    g.link_error(sig4) # links to sig1\r\n    g.apply_async(link=sig3) # raises TypeError\r\n    g.apply_async(link_error=sig4) # raises TypeError\r\n\r\n## Expected behavior\r\n\r\n    g.link(sig3) # should rise TypeError exception with referencing to chord\r\n    g.link_error(sig4) # should rise TypeError exception with referencing to individual error handlers, or silently apply link_error to each task", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4281/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/9a1064e71b753c9195efa0d83f52556794e5f8ef", "message": "Care about chain's link_error tasks (#4240)\n\n* Assign link_error sugnatures to each task\r\n\r\nissue #4232\r\nChain could have link_error signatures for error processing.\r\nJoining chains copies tasks from other chain to original one.\r\nThus copying loses other chain's link_error signatures.\r\nAssigning chain's link_error signatures to each task could have\r\nthe same effect.\r\nEach task from other chain are cloned to leave original ones as is.\r\n\r\n* Clone full chain's state\r\n\r\nIn appending task to chain clone full chain's state and\r\nappend task to chain's tasks.\r\n\r\n* Fix chaining to chains\r\n\r\n* Add test for keeping link_error on chaining\r\n\r\n* Fix chaining group to chain\r\n\r\n* Fix indentation according to PEP8\r\n\r\n* Fix blank lines\r\n\r\n* Avoid reduce function usage\r\n\r\n* Move common code to separate method"}, {"url": "https://api.github.com/repos/celery/celery/commits/e3055959839c342ed9be7f63dd9f89646d8ac489", "message": "Allow to create group with single task (fixes issue #4255) (#4280)\n\n* Allow to create group with single task in a list (#4255)\r\n\r\n* Test ability to create group with single task"}, {"url": "https://api.github.com/repos/celery/celery/commits/028df35ef8af828f179b870375f9d5510fc63b9c", "message": "Do not modify passed dict (fixes issue #4223) (#4278)\n\n* Do not modify passed dict (fixes issue #4223)\r\n\r\n* Use options variable instead of d"}, {"url": "https://api.github.com/repos/celery/celery/commits/aa6ceaec10919911ca4122111bd727cdd2d9f255", "message": "Fix required boto3 version\n\nIssue #4242"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/138437103", "body": "With `reduce` I'm assigning tasks from `link_error` of current chain to individual tasks with respect to their order.\r\nThis code equals to:\r\n\r\n    new_tasks = []\r\n    for task in self.tasks:\r\n      new_task = task.clone()\r\n      new_tasks.append(new_task)\r\n      for error_handler in self._with_list_option('link_error'):\r\n        new_task.on_error(error_handler)\r\n    return _chain(seq_concat_item(new_tasks, other), app=self._app)`", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/138437103/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/138715053", "body": "OK, I'll update my branch.\r\nI've used `_with_list_option` because `link_error` is a list of tasks and there are some logic on setting it in case of `MutableSequence`.\r\nBut here we just reading it, so I can use `.options.get('link_error', [])`", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/138715053/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/138799247", "body": "Yes, `link_error` is list of signatures.\r\nI've wrote it on your code suggestion ;)", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/138799247/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/139240270", "body": "I've though about it when applying your suggestion :)\r\nWait for one more commit.\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/139240270/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/140614100", "body": "No, in other places it just uses dict values, and only here it modifies it.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/140614100/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/140614125", "body": "OK.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/140614125/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "dogrocker": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4270", "title": "Celery can't revoke task, Then it run tasks twice, or duplicate task", "body": "Hello! I have problem I used celery with redis to send sms reminder, But now celery alway send me 2 sms/task. `(INFO/ForkPoolWorker-1, INFO/ForkPoolWorker-2)`, Celery can't revoke task and it alway discard task like this\r\n\r\n```\r\ndiscarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\n```\r\n\r\nWhen it can't revoke task the `INFO/ForkPoolWorker-1`, `INFO/ForkPoolWorker-2` alway ran the same task. So it send me 2 sms.\r\n\r\n## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n- `celery==4.1.0`\r\n\r\n## Steps to reproduce\r\n\r\n## Expected behavior\r\nRun only one task.\r\n\r\n## Actual behavior\r\nThis logs I don't know how to explain this problem.\r\n\r\n```\r\ncelery_1    | [2017-09-17 09:00:01,461: INFO/MainProcess] Tasks flagged as revoked: e94ae261-91e4-4cd4-96e2-e00fce4e40e9\r\ncelery_1    | [2017-09-17 09:00:01,470: INFO/ForkPoolWorker-2] Task patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9] succeeded in 0.4674046079162508s: 'sms appointment_id: 119'\r\ncelery_1    | [2017-09-17 09:00:01,471: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:01,472: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:01,472: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:01,473: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:01,712: INFO/MainProcess] Tasks flagged as revoked: e94ae261-91e4-4cd4-96e2-e00fce4e40e9\r\ncelery_1    | [2017-09-17 09:00:01,718: INFO/ForkPoolWorker-1] Task patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9] succeeded in 0.7128307099919766s: 'sms appointment_id: 119'\r\ncelery_1    | [2017-09-17 09:00:01,721: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:01,721: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,051: INFO/MainProcess] Tasks flagged as revoked: dfc7c761-7ab4-47c3-a820-b793d277ed70\r\ncelery_1    | [2017-09-17 09:00:02,060: INFO/ForkPoolWorker-2] Task patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70] succeeded in 0.5810814599972218s: 'sms appointment_id: 118'\r\ncelery_1    | [2017-09-17 09:00:02,061: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,062: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,062: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,063: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,063: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,063: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,064: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,065: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,065: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,065: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,066: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,066: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,066: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,067: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,067: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,067: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,069: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,069: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,069: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,070: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,070: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,070: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,071: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,071: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,071: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,072: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,072: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,072: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,074: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,074: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,075: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,075: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,075: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,076: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,076: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,076: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,077: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,077: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,077: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,078: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,078: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,078: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,079: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,079: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,079: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,080: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,080: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,080: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,081: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,081: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,081: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,082: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,082: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,082: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,083: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,083: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,083: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,083: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,085: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,086: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,086: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,086: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,086: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,087: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,087: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,087: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,087: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,088: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,088: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,088: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,088: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,089: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,089: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,089: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,089: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,090: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,090: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,090: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,090: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,091: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,091: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,091: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,091: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,091: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,092: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,092: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,092: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,092: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,093: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,093: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,093: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,093: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,094: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,094: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,094: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,094: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,094: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,095: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,095: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,095: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,095: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,096: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,096: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,096: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,097: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,097: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,097: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,098: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,098: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,098: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,098: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,099: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,099: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,100: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,100: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,105: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,106: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,107: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,107: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,107: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,108: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,108: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,108: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,108: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,109: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,109: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,110: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,110: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,110: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,111: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,111: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,111: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,112: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,112: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,112: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,113: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,113: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,113: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,113: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,113: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,162: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,163: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,163: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,163: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,164: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,164: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,164: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,165: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,165: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70]\r\ncelery_1    | [2017-09-17 09:00:02,165: INFO/MainProcess] Discarding revoked task: patient.tasks.send_sms_reminder[e94ae261-91e4-4cd4-96e2-e00fce4e40e9]\r\ncelery_1    | [2017-09-17 09:00:02,193: INFO/MainProcess] Tasks flagged as revoked: dfc7c761-7ab4-47c3-a820-b793d277ed70\r\ncelery_1    | [2017-09-17 09:00:02,198: INFO/ForkPoolWorker-1] Task patient.tasks.send_sms_reminder[dfc7c761-7ab4-47c3-a820-b793d277ed70] succeeded in 0.47451397380791605s: 'sms appointment_id: 118'\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4270/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "orlitzky": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4266", "title": "celery creates PID files after dropping privileges", "body": "The celeryd daemon creates its PID file after dropping privileges to the non-root user/group specified by `--uid` and `--gid`. With many init scripts, this can be exploited by that unprivileged user to kill root processes, since when the daemon is stopped, root will send a SIGTERM to the contents of the PID file (which are controlled by the non-root user). For example, in `extra/generic-init.d/celeryd`, we find code like,\r\n\r\n```sh\r\n_get_pids() {\r\n    found_pids=0\r\n    my_exitcode=0\r\n\r\n    for pidfile in $(_get_pidfiles); do\r\n        local pid=`cat \"$pidfile\"`\r\n\r\n...\r\n\r\nrestart_workers_graceful () {\r\n    echo \"WARNING: Use with caution in production\"\r\n    echo \"The workers will attempt to restart, but they may not be able to.\"\r\n    local worker_pids=\r\n    worker_pids=`_get_pids`\r\n    [ \"$one_failed\" ] && exit 1\r\n\r\n    for worker_pid in $worker_pids; do\r\n        local failed=\r\n        kill -HUP $worker_pid 2> /dev/null || failed=true\r\n```\r\n\r\nIn my opinion, the simplest way to prevent this sort of thing (without making every init script author jump through serious hoops) is to create the PID file _before_ dropping privileges.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4266/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jamesmallen": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4261", "title": "celery beat subscribes to task meta", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n```\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.1\r\n            billiard:3.5.0.3 py-amqp:2.2.1\r\nplatform -> system:Linux arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:redis://redis:6379/\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1. Configure a result_backend and set up a beat_schedule that includes a task\r\n1. Run `celery beat`\r\n\r\n## Expected behavior\r\n\r\n1. Tasks are started and there is no traffic between `beat` and the result backend\r\n\r\n`beat` shouldn't need to communicate with the backend because it doesn't do anything with the results\r\n\r\n## Actual behavior\r\n\r\n1. `beat` subscribes to the meta on the backend server for every task that it creates. Using `tcpdump`, I can actually watch `SUBSCRIBE` messages being sent for every task, regardless of the `ignore_result` setting.\r\n\r\nThis is actually a larger issue due to #3812 - the `beat` process does not unsubscribe ever, and at a certain point begins to send massive subscription requests to the Redis server. We saw giant spikes in outbound traffic from beat which seemed fishy, and traced it to this issue.\r\n\r\n## Workaround\r\n\r\nThe way we have worked around this in the meantime is to have a separate config for `beat` that doesn't have a `result_backend` set.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4261/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4083", "title": "Tasks with expires fail with AttributeError(\"'str' object has no attribute 'isoformat'\",)", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n\r\n```\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:3.6.1\r\n            billiard:3.5.0.2 py-amqp:2.1.4\r\nplatform -> system:Darwin arch:64bit imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\nsettings -> transport:amqp results:disabled\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1. Create a task that retries via raise `self.retry()`\r\n1. Call the task with `send_task` and set `expires`\r\n\r\n## Expected behavior\r\n\r\nThe task should retry until the expires time is hit or the max_retries value is hit\r\n\r\n## Actual behavior\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/site-packages/celery/app/task.py\", line 684, in retry\r\n    S.apply_async()\r\n  File \"/usr/lib/python3.6/site-packages/celery/canvas.py\", line 221, in apply_async\r\n    return _apply(args, kwargs, **options)\r\n  File \"/usr/lib/python3.6/site-packages/celery/app/task.py\", line 535, in apply_async\r\n    **options\r\n  File \"/usr/lib/python3.6/site-packages/celery/app/base.py\", line 729, in send_task\r\n    root_id, parent_id, shadow, chain,\r\n  File \"/usr/lib/python3.6/site-packages/celery/app/amqp.py\", line 334, in as_task_v2\r\n    expires = expires and expires.isoformat()\r\nAttributeError: 'str' object has no attribute 'isoformat'\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4083/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/6bdf5aea3a715734ccee75236389b11358c77c58", "message": "create task message using additional v2 parameters (#4260)\n\n* create task message using additional v2 parameters\r\n\r\n* removing conditional and adding compat_kwargs to as_task methods\r\n\r\n* Update amqp.py\r\n\r\n* Adding tests for kwargsrepr/argsrepr, upgrading test_tasks to use task protocol v2\r\n\r\n* Adding properties argument to allow checking for task properties values"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4262", "title": "Disable backend for beat", "body": "## Description\r\n\r\nWhen using `beat` (at least in its standalone form), it is not necessary to subscribe to events on the result backend. These subscriptions happen in the `on_task_call` method of the backend. This PR ensures that no `SUBSCRIBE` messages are sent.\r\n\r\nFixes #4261 ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/139513987", "body": "I didn't realize there was that - that might be a less hacky way to resolve this issue instead of monkeypatching the `on_task_call` method", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/139513987/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/139522771", "body": "I've implemented the suggested change and added a test to ensure the backend has been replaced", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/139522771/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/138429073", "body": "Excellent point - for some reason I thought the `as_task` methods were coming from another library. I agree that it makes more sense to update those and remove the conditional. Updating now...", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/138429073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/138729286", "body": "Sure thing - PR updated!", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/138729286/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "chutes": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4247", "title": "Unit Testing: self.request.chain is None when CELERY_TASK_ALWAYS_EAGER=True", "body": "using celery v4.1.0\r\n\r\nDuring unit testing I discovered that when CELERY_TASK_ALWAYS_EAGER=True, self.request.chain = None, while in production, self.request.chain has value.\r\n\r\nThis is problematic during unit testing since behavior between unit tests and production are dissimilar.\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4247/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "samfrances": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4244", "title": "Group of chains: arguments not passed to chains", "body": "This issue effects the celery 3.1.19 and celery 4.1.0, but somewhat differently.\r\n\r\nThe problem arises with a group of chains, of the following form:\r\n\r\n```\r\nworkflow = group(\r\n    chain(task1.s(), task2.s()),\r\n    chain(task3.s(), task4.s())\r\n)\r\n```\r\nWhere all tasks take one positional argument.\r\n\r\nIn 4.1.0, calling `workflow.apply_async((n,))` for any value `n` will result in:\r\n\r\n```\r\nTypeError: task1() takes exactly 1 argument (0 given)\r\n```\r\n\r\nIn 3.1.19, calling `workflow.apply_async((n,))` for any value `n` will work initially. However, if you trigger the task repeatedly in the same process, `task1` and `task2` start getting more than one arguments. The additional arguments are always arguments that have been passed in previous invocations.\r\n\r\nMore details, including scripts that can be used to reproduce this behaviour, are included in the **steps to reproduce** section below.\r\n\r\n\r\nOutput of ``celery -A proj report`` for 4.1.0:\r\n\r\n```\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:2.7.12\r\n            billiard:3.5.0.3 redis:2.10.5\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis://localhost:6379/\r\n\r\nCELERY_TASK_SERIALIZER: 'json'\r\nCELERY_RESULT_SERIALIZER: 'json'\r\nCELERY_ACCEPT_CONTENT: ['json']\r\nCELERY_SEND_EVENTS: True\r\nCELERY_EVENT_SERIALIZER: 'json'\r\nBROKER_URL: u'redis://localhost:6379//'\r\nCELERY_RESULT_BACKEND: u'redis://localhost:6379/'\r\n```\r\n\r\nOutput of ``celery -A proj report`` for 3.1.19:\r\n\r\n```\r\nsoftware -> celery:3.1.19 (Cipater) kombu:3.0.37 py:2.7.12\r\n            billiard:3.3.0.23 redis:2.10.5\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis://localhost:6379/\r\n\r\nCELERY_EVENT_SERIALIZER: 'json'\r\nCELERY_TASK_SERIALIZER: 'json'\r\nCELERY_RESULT_SERIALIZER: 'json'\r\nCELERY_ACCEPT_CONTENT: ['json']\r\nCELERY_SEND_EVENTS: True\r\n```\r\n\r\n\r\n## Steps to reproduce\r\n\r\nFor 4.1.0, the following script will reproduce the problem:\r\n\r\n```\r\nfrom celery import Celery, group, chain, signature\r\n\r\nbroker = 'redis://localhost:6379/'\r\napp = Celery('experiments2', backend=broker, broker=broker)\r\napp.conf.update(\r\n    CELERY_SEND_EVENTS=True,\r\n    CELERY_TASK_SERIALIZER='json',\r\n    CELERY_RESULT_SERIALIZER='json',\r\n    CELERY_EVENT_SERIALIZER='json',\r\n    CELERY_ACCEPT_CONTENT=['json'],\r\n)\r\n\r\n\r\n@app.task\r\ndef times2(a):\r\n    print \"run times2({})\".format(a)\r\n    return a * 2\r\n\r\n\r\n@app.task\r\ndef times3(a):\r\n    print \"run times3({})\".format(a)\r\n    return a * 3\r\n\r\n\r\n@app.task\r\ndef times5(a):\r\n    print \"run times5({})\".format(a)\r\n    return a * 5\r\n\r\n\r\n@app.task\r\ndef negative(a):\r\n    print \"run negative({})\".format(a)\r\n    return -a\r\n\r\n\r\nworkflow = group(\r\n    chain(times2.s(), negative.s()),\r\n    chain(times3.s(), times5.s())\r\n)\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    workflow.apply_async((1,))\r\n\r\n```\r\n\r\nFor 3.1.19, change the `if __name_ == \"__main__\"` part as follows:\r\n\r\n```\r\nif __name__ == \"__main__\":\r\n\r\n    for n in range(20):\r\n\r\n        res1 = workflow.apply_async(args=(n,))\r\n        print res1.get()\r\n```\r\n\r\n## Expected behavior\r\n\r\nI repeat the workflow for reference:\r\n```\r\nworkflow = group(\r\n    chain(times2.s(), negative.s()),\r\n    chain(times3.s(), times5.s())\r\n)\r\n```\r\nThe expected behaviour (as far as I understand) is that when calling `workflow.apply_async((n,))`, each of the chains in the group will be passed `n`. So, the final result will be equivalent to:\r\n```\r\n[\r\n    negative(times2(n)),\r\n    times5(times3(n))\r\n]\r\n```\r\n\r\n## Actual behavior\r\n\r\nCelery 4.1.0 gives the following error (this is from the Python script, not in the celery worker output):\r\n\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n/home/sam/Documents/learning_projects/celery_canvas/experiments2.py in <module>()\r\n     58     for n in range(10):\r\n     59 \r\n---> 60         res1 = workflow.apply_async(args=(n,))\r\n     61         print res1.get()\r\n\r\n/home/sam/Documents/learning_projects/celery_canvas/venv/lib/python2.7/site-packages/celery/canvas.pyc in apply_async(self, args, kwargs, add_to_parent, producer, link, link_error, **options)\r\n    984         p = barrier()\r\n    985         results = list(self._apply_tasks(tasks, producer, app, p,\r\n--> 986                                          args=args, kwargs=kwargs, **options))\r\n    987         result = self.app.GroupResult(group_id, results, ready_barrier=p)\r\n    988         p.finalize()\r\n\r\n/home/sam/Documents/learning_projects/celery_canvas/venv/lib/python2.7/site-packages/celery/canvas.pyc in _apply_tasks(self, tasks, producer, app, p, add_to_parent, chord, args, kwargs, **options)\r\n   1060                                 chord=sig.options.get('chord') or chord,\r\n   1061                                 args=args, kwargs=kwargs,\r\n-> 1062                                 **options)\r\n   1063 \r\n   1064                 # adding callback to result, such that it will gradually\r\n\r\n/home/sam/Documents/learning_projects/celery_canvas/venv/lib/python2.7/site-packages/celery/canvas.pyc in apply_async(self, args, kwargs, **options)\r\n    564             return self.apply(args, kwargs, **options)\r\n    565         return self.run(args, kwargs, app=app, **(\r\n--> 566             dict(self.options, **options) if options else self.options))\r\n    567 \r\n    568     def run(self, args=(), kwargs={}, group_id=None, chord=None,\r\n\r\n/home/sam/Documents/learning_projects/celery_canvas/venv/lib/python2.7/site-packages/celery/canvas.pyc in run(self, args, kwargs, group_id, chord, task_id, link, link_error, publisher, producer, root_id, parent_id, app, **options)\r\n    594             # Issue #3379.\r\n    595             options['chain'] = tasks if not use_link else None\r\n--> 596             first_task.apply_async(**options)\r\n    597             return results[0]\r\n    598 \r\n\r\n/home/sam/Documents/learning_projects/celery_canvas/venv/lib/python2.7/site-packages/celery/canvas.pyc in apply_async(self, args, kwargs, route_name, **options)\r\n    219         # pylint: disable=too-many-function-args\r\n    220         #   Borks on this, as it's a property\r\n--> 221         return _apply(args, kwargs, **options)\r\n    222 \r\n    223     def _merge(self, args=(), kwargs={}, options={}, force=False):\r\n\r\n/home/sam/Documents/learning_projects/celery_canvas/venv/lib/python2.7/site-packages/celery/app/task.pyc in apply_async(self, args, kwargs, task_id, producer, link, link_error, shadow, **options)\r\n    516                 pass\r\n    517             else:\r\n--> 518                 check_arguments(*(args or ()), **(kwargs or {}))\r\n    519 \r\n    520         app = self._get_app()\r\n\r\nTypeError: times2() takes exactly 1 argument (0 given)\r\n```\r\n\r\nOn the other hand, running 3.1.19 with the appropriately altered script (see above), gives the error, this time in the output of the celery worker:\r\n\r\n```\r\n$ celery -A experiments2 worker --loglevel=INFO --concurrency=4                              \r\n \r\n -------------- celery@sam-VirtualBox v3.1.19 (Cipater)\r\n---- **** ----- \r\n--- * ***  * -- Linux-4.10.0-33-generic-x86_64-with-Ubuntu-16.04-xenial\r\n-- * - **** --- \r\n- ** ---------- [config]\r\n- ** ---------- .> app:         experiments2:0x7f2a9dfa2a90\r\n- ** ---------- .> transport:   redis://localhost:6379//\r\n- ** ---------- .> results:     redis://localhost:6379/\r\n- *** --- * --- .> concurrency: 4 (prefork)\r\n-- ******* ---- \r\n--- ***** ----- [queues]\r\n -------------- .> celery           exchange=celery(direct) key=celery\r\n                \r\n\r\n[tasks]\r\n  . experiments2.negative\r\n  . experiments2.times2\r\n  . experiments2.times3\r\n  . experiments2.times5\r\n\r\n[2017-08-31 17:27:04,097: INFO/MainProcess] Connected to redis://localhost:6379//\r\n[2017-08-31 17:27:04,106: INFO/MainProcess] mingle: searching for neighbors\r\n[2017-08-31 17:27:05,111: INFO/MainProcess] mingle: all alone\r\n[2017-08-31 17:27:05,124: WARNING/MainProcess] celery@sam-VirtualBox ready.\r\n[2017-08-31 17:27:11,152: INFO/MainProcess] Received task: experiments2.times2[3c33edfb-2390-4b2a-bce7-947139ef5bec]\r\n[2017-08-31 17:27:11,155: WARNING/Worker-1] run times2(0)\r\n[2017-08-31 17:27:11,163: INFO/MainProcess] Received task: experiments2.times3[60d62e08-3e36-46f2-a96b-d0b81f1de194]\r\n[2017-08-31 17:27:11,169: WARNING/Worker-3] run times3(0)\r\n[2017-08-31 17:27:11,198: INFO/MainProcess] Received task: experiments2.negative[3249987f-ce58-42f0-8bc0-374c74e083b1]\r\n[2017-08-31 17:27:11,201: WARNING/Worker-2] run negative(0)\r\n[2017-08-31 17:27:11,205: INFO/MainProcess] Task experiments2.negative[3249987f-ce58-42f0-8bc0-374c74e083b1] succeeded in 0.00359962000221s: 0\r\n[2017-08-31 17:27:11,210: INFO/MainProcess] Task experiments2.times2[3c33edfb-2390-4b2a-bce7-947139ef5bec] succeeded in 0.0542540519964s: 0\r\n[2017-08-31 17:27:11,212: INFO/MainProcess] Received task: experiments2.times5[08a46c29-689e-4960-97b6-28cd2ea4fddc]\r\n[2017-08-31 17:27:11,215: INFO/MainProcess] Task experiments2.times3[60d62e08-3e36-46f2-a96b-d0b81f1de194] succeeded in 0.0460929540131s: 0\r\n[2017-08-31 17:27:11,216: WARNING/Worker-4] run times5(0)\r\n[2017-08-31 17:27:11,220: INFO/MainProcess] Task experiments2.times5[08a46c29-689e-4960-97b6-28cd2ea4fddc] succeeded in 0.00382349599386s: 0\r\n[2017-08-31 17:27:12,184: INFO/MainProcess] Received task: experiments2.times2[602070e6-ec3d-41f3-a46c-b97341039d32]\r\n[2017-08-31 17:27:12,197: INFO/MainProcess] Received task: experiments2.times3[f723051a-44e4-429f-9d53-9880c7566aeb]\r\n[2017-08-31 17:27:12,202: ERROR/MainProcess] Task experiments2.times2[602070e6-ec3d-41f3-a46c-b97341039d32] raised unexpected: TypeError('times2() takes exactly 1 argument (2 given)',)\r\nTraceback (most recent call last):\r\n  File \"/home/sam/Documents/learning_projects/celery_canvas/venv/local/lib/python2.7/site-packages/celery/app/trace.py\", line 240, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n  File \"/home/sam/Documents/learning_projects/celery_canvas/venv/local/lib/python2.7/site-packages/celery/app/trace.py\", line 438, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\nTypeError: times2() takes exactly 1 argument (2 given)\r\n[2017-08-31 17:27:12,207: ERROR/MainProcess] Task experiments2.times3[f723051a-44e4-429f-9d53-9880c7566aeb] raised unexpected: TypeError('times3() takes exactly 1 argument (2 given)',)\r\nTraceback (most recent call last):\r\n  File \"/home/sam/Documents/learning_projects/celery_canvas/venv/local/lib/python2.7/site-packages/celery/app/trace.py\", line 240, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n  File \"/home/sam/Documents/learning_projects/celery_canvas/venv/local/lib/python2.7/site-packages/celery/app/trace.py\", line 438, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\nTypeError: times3() takes exactly 1 argument (2 given)\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4244/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "itayB": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4226", "title": "Celery upgrade (3.1->4.1) - Connection reset by peer", "body": "## Checklist\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [ ] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n\r\n## Expected behavior\r\n\r\n## Actual behavior\r\nWe are working with celery at the last year, with ~15 workers, each one defined with concurrency between 1-4.\r\n\r\nRecently we upgraded our celery from v3.1 to v4.1\r\n\r\nNow we are having the following errors in each one of the workers logs, any ideas what can cause to such error?\r\n\r\n    2017-08-21 18:33:19,780 94794  ERROR   Control command error: error(104, 'Connection reset by peer') [file: pidbox.py, line: 46]\r\n    Traceback (most recent call last):\r\n      File \"/srv/dy/venv/lib/python2.7/site-packages/celery/worker/pidbox.py\", line 42, in on_message\r\n        self.node.handle_message(body, message)\r\n      File \"/srv/dy/venv/lib/python2.7/site-packages/kombu/pidbox.py\", line 129, in handle_message\r\n        return self.dispatch(**body)\r\n      File \"/srv/dy/venv/lib/python2.7/site-packages/kombu/pidbox.py\", line 112, in dispatch\r\n        ticket=ticket)\r\n      File \"/srv/dy/venv/lib/python2.7/site-packages/kombu/pidbox.py\", line 135, in reply\r\n        serializer=self.mailbox.serializer)\r\n      File \"/srv/dy/venv/lib/python2.7/site-packages/kombu/pidbox.py\", line 265, in _publish_reply\r\n        **opts\r\n      File \"/srv/dy/venv/lib/python2.7/site-packages/kombu/messaging.py\", line 181, in publish\r\n        exchange_name, declare,\r\n      File \"/srv/dy/venv/lib/python2.7/site-packages/kombu/messaging.py\", line 203, in _publish\r\n        mandatory=mandatory, immediate=immediate,\r\n      File \"/srv/dy/venv/lib/python2.7/site-packages/amqp/channel.py\", line 1748, in _basic_publish\r\n        (0, exchange, routing_key, mandatory, immediate), msg\r\n      File \"/srv/dy/venv/lib/python2.7/site-packages/amqp/abstract_channel.py\", line 64, in send_method\r\n        conn.frame_writer(1, self.channel_id, sig, args, content)\r\n      File \"/srv/dy/venv/lib/python2.7/site-packages/amqp/method_framing.py\", line 178, in write_frame\r\n        write(view[:offset])\r\n      File \"/srv/dy/venv/lib/python2.7/site-packages/amqp/transport.py\", line 272, in write\r\n        self._write(s)\r\n      File \"/usr/lib64/python2.7/socket.py\", line 224, in meth\r\n        return getattr(self._sock,name)(*args)\r\n    error: [Errno 104] Connection reset by peer\r\n\r\nBTW: our tasks in the form:\r\n\r\n    @app.task(name='EXAMPLE_TASK'],\r\n              bind=True,\r\n              base=ConnectionHolderTask)\r\n    def example_task(self, arg1, arg2, **kwargs):\r\n        # task code\r\n\r\n\r\nref to stackoverflow question: https://stackoverflow.com/questions/45803728/celery-upgrade-3-1-4-1-connection-reset-by-peer", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4226/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/4048", "title": "link_error support for complex canvas", "body": "## Checklist\r\n\r\n- [\u221a] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [ ] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Setup\r\nCelery 4.0.2, broker=RabbitMQ, backend=Redis\r\n**UPDATE**: same for Celery 4.1\r\n\r\n## Steps to reproduce\r\nTry to add `link_error` in an apply_async of chain (with chord within). Till now I was using celery 3.1 and it didn't work at all. Now after upgrade to celery 4.0.2 I'm getting some unexpected behavior.\r\nHaving 13 different tasks working on a shared resource (products feed), I want to \"lock\" that resource on start and \"unlock\" it at the end. whenever task fails (and other concurrent tasks finish as well) I want to \"unlock\" the resource again.  \r\n\r\n`celery worker -A worker.celeryapp:app -l info -Q default -c 2 -n defaultworker@%h -Ofair`\r\n\r\n```\r\n@app.task(name='task_1',\r\n          bind=True,\r\n          base=MyConnectionHolderTask)\r\ndef task_1(self, feed_id, flow_id, **kwargs):\r\n    do_something()\r\n    # raise Exception('test')\r\n```\r\n\r\n```\r\nlock = lock_flow_task.si(feed_id, flow_id)\r\nunlock = unlock_flow_task.si(feed_id, flow_id)\r\ntask_1 = t_1.si(feed_id, flow_id)\r\n  .\r\n  .\r\ntask_13 = t_13.si(feed_id, flow_id)\r\n\r\n\r\n           (lock |\r\n            task_1 |\r\n            group((task_2 | task_3 | task_4),\r\n                  task_5,\r\n                  task_6,\r\n                  task_7,\r\n                  task_8) |\r\n            task_9 |\r\n            task_10 |\r\n            task_11 |\r\n            task_12 |\r\n            task_13 |\r\n            unlock).apply_async(link_error=unlock)\r\n````\r\n\r\n## Expected behavior\r\nEach one of the tasks (task_1..task_13) failure should trigger finally the `unlock` task. If one of the concurrent `group` tasks is running - it should wait until all finished and run the `unlock` as well.\r\n\r\n## Actual behavior\r\nTrying to test the `link_error` for each task failure (by adding `raise Exception(\"test\")` in each task.\r\nException in `task_1` works fine -> `unlock` run.\r\nException in `task_2` didn't work. Ends at the end of the grouped tasks.\r\nException in `task_4` caused the `unlock` to run twice!\r\nException in `task_5` caused the `unlock` to run twice!\r\nException in `task_6` caused the `unlock` to run twice!\r\nException in `task_9` works fine\r\n\r\nAny idea or workaround to achieve the desired state?\r\n\r\nlog from the twice unlock:\r\n```\r\n2017-05-24 07:48:52,155 14     ERROR   Chord callback for '9d8005cb-8335-4738-b3ae-138717193e3d' raised: ChordError(u\"Dependency a6c98e9f-87ac-40f2-9f91-51ad271d5a33 raised Exception(u'',)\",) [file: redis.py, line: 284]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/redis.py\", line 281, in on_chord_part_return\r\n    callback.delay([unpack(tup, decode) for tup in resl])\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/redis.py\", line 234, in _unpack_chord_result\r\n    raise ChordError('Dependency {0} raised {1!r}'.format(tid, retval))\r\nChordError: Dependency a6c98e9f-87ac-40f2-9f91-51ad271d5a33 raised Exception(u'test',)\r\n2017-05-24 07:48:52,157 14     INFO    MyCeleryRouter: Inserted To Queue: lock, task: unlock_task, args:(727, 1495612112858) [file: celery_router.py, line: 64]\r\n2017-05-24 07:48:52,165 14     ERROR   Chord '9d8005cb-8335-4738-b3ae-138717193e3d' raised: TypeError('sequence item 1: expected string, NoneType found',) [file: redis.py, line: 293]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/redis.py\", line 287, in on_chord_part_return\r\n    ChordError('Callback error: {0!r}'.format(exc)),\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 223, in chord_error_from_stack\r\n    return backend.fail_from_current_stack(callback.id, exc=exc)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 230, in fail_from_current_stack\r\n    self.mark_as_failure(task_id, exc, ei.traceback)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 163, in mark_as_failure\r\n    traceback=traceback, request=request)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 309, in store_result\r\n    request=request, **kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 652, in _store_result\r\n    self.set(self.get_key_for_task(task_id), self.encode(meta))\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 558, in get_key_for_task\r\n    self.task_keyprefix, key_t(task_id), key_t(key),\r\nTypeError: sequence item 1: expected string, NoneType found\r\n2017-05-24 07:48:52,167 14     INFO    Inserted To Queue: lock, task: unlock_task, args:(727, 1495612112858) [file: celery_router.py, line: 64]\r\n2017-05-24 07:48:52,167 1      INFO    Received task: unlock_task[74cf0ef9-4933-4ba4-b0fc-a9a4f50c3929]   [file: strategy.py, line: 109]\r\n2017-05-24 07:48:52,171 1      INFO    Received task: unlock_task[2784cca9-1139-4690-9b8b-60fef2644881]   [file: strategy.py, line: 109]\r\n2017-05-24 07:48:52,172 14     WARNING /usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/app/trace.py:542: RuntimeWarning: Exception raised outside body: TypeError('sequence item 1: expected string, NoneType found',):\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/app/trace.py\", line 381, in trace_task\r\n    I, R, state, retval = on_error(task_request, exc, uuid)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/app/trace.py\", line 323, in on_error\r\n    task, request, eager=eager, call_errbacks=call_errbacks,\r\n   File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/app/trace.py\", line 157, in handle_error_state\r\n    call_errbacks=call_errbacks)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/app/trace.py\", line 202, in handle_failure\r\n    call_errbacks=call_errbacks,\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 166, in mark_as_failure\r\n    self.on_chord_part_return(request, state, exc)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/redis.py\", line 296, in on_chord_part_return\r\n    ChordError('Join error: {0!r}'.format(exc)),\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 223, in chord_error_from_stack\r\n    return backend.fail_from_current_stack(callback.id, exc=exc)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 230, in fail_from_current_stack\r\n    self.mark_as_failure(task_id, exc, ei.traceback)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 163, in mark_as_failure\r\n    traceback=traceback, request=request)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 309, in store_result\r\n    request=request, **kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 652, in _store_result\r\n    self.set(self.get_key_for_task(task_id), self.encode(meta))\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 558, in get_key_for_task\r\n    self.task_keyprefix, key_t(task_id), key_t(key),\r\nTypeError: sequence item 1: expected string, NoneType found\r\n\r\n  exc, exc_info.traceback))) [file: log.py, line: 235]\r\n2017-05-24 07:48:52,177 14     ERROR   Task SIMILARITY_TASK[a6c98e9f-87ac-40f2-9f91-51ad271d5a33] raised unexpected: TypeError('sequence item 1: expected string, NoneType found',) [file: trace.py, line: 241]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/app/trace.py\", line 381, in trace_task\r\n    I, R, state, retval = on_error(task_request, exc, uuid)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/app/trace.py\", line 323, in on_error\r\n    task, request, eager=eager, call_errbacks=call_errbacks,\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/app/trace.py\", line 157, in handle_error_state\r\n    call_errbacks=call_errbacks)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/app/trace.py\", line 202, in handle_failure\r\n    call_errbacks=call_errbacks,\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 166, in mark_as_failure\r\n    self.on_chord_part_return(request, state, exc)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/redis.py\", line 296, in on_chord_part_return\r\n    ChordError('Join error: {0!r}'.format(exc)),\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 223, in chord_error_from_stack\r\n    return backend.fail_from_current_stack(callback.id, exc=exc)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 230, in fail_from_current_stack\r\n    self.mark_as_failure(task_id, exc, ei.traceback)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 163, in mark_as_failure\r\n    traceback=traceback, request=request)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 309, in store_result\r\n    request=request, **kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 652, in _store_result\r\n    self.set(self.get_key_for_task(task_id), self.encode(meta))\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/base.py\", line 558, in get_key_for_task\r\n    self.task_keyprefix, key_t(task_id), key_t(key),\r\nTypeError: sequence item 1: expected string, NoneType found\r\n2017-05-24 07:48:52,181 14     INFO    unlock_task    [2784cca9-1139-4690-9b8b-60fef2644881] unlock for feed 727, flow 1495612112858 [file: lock_flow_task.py, line: 50]\r\n2017-05-24 07:48:52,183 14     INFO    Task unlock_task[2784cca9-1139-4690-9b8b-60fef2644881] succeeded in 0.00978600300732s: None [file: trace.py, line: 441]\r\n2017-05-24 07:48:52,186 13     WARNING unlock_task    [74cf0ef9-4933-4ba4-b0fc-a9a4f50c3929] failed to unlock feed 727: expected flow 1495612112858, actual flow None [file: section_flow_lock.py, line: 67]\r\n2017-05-24 07:48:52,188 13     INFO    Task unlock_task[74cf0ef9-4933-4ba4-b0fc-a9a4f50c3929] succeeded in 0.0141102350026s: None [file: trace.py, line: 441]\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4048/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pitervergara": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4222", "title": "Reject exception doesn't cause the task to fail", "body": "Hi!\r\n\r\nI'm seeing the behaviour described in #2944.\r\nIf I reject a task, it is kept as __STARTED__. I'm not sure if did not understand the solution to that issue - and this is the expected behaviuor - or if this is a _regression_.\r\n\r\nI have verified that the behaviour is the same in the `master` branch of Celery (installed from commit bda678b9cd3a4ea47bfdd4c33aaabffb678de883).\r\n\r\n## Steps to reproduce\r\n```python\r\nfrom celery import task, Task\r\nfrom celery.exceptions import Reject\r\n\r\n@task(name='test_reject')\r\ndef test_reject():\r\n    raise Reject(reason=\"Some business logic...\", requeue=False)\r\n```\r\n\r\n```bash\r\n>>> from mymodule.tasks import *\r\n>>> test_reject.delay()\r\n<AsyncResult: 712bed3e-39e1-4455-b1f6-445aa37ef236>\r\n```\r\n\r\n## Expected behavior\r\nI would expect the task to go into FAILURE state\r\n\r\n\r\n## Actual behavior\r\nTask still _pending_\r\n\r\n```bash\r\n>>> from celery.result import AsyncResult\r\n>>> res = AsyncResult('712bed3e-39e1-4455-b1f6-445aa37ef236')\r\n>>> res.status\r\n'PENDING'\r\n>>> res.state\r\n'PENDING'\r\n>>> res.failed()\r\nFalse\r\n>>> res.successful()\r\nFalse\r\n```\r\n\r\n![captura de tela de 2017-08-22 11-06-21](https://user-images.githubusercontent.com/4817545/29583133-72c9b016-8755-11e7-9074-23e101fa90dc.png)\r\n\r\n\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4222/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lcd1232": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4198", "title": "Celery doesn't fetch settings properly", "body": "## Checklist\r\n\r\n- Celery 4.1.0\r\n- python 3.6\r\n- I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n\r\nInstall project - https://github.com/lcd1232/celery_django_test\r\n```\r\n$ git clone https://github.com/lcd1232/celery_django_test.git\r\n$ cd celery_django_test\r\n$ virtualenv -p python3 .env\r\n$ source .env/bin/activate\r\n$ pip install -r requirements.txt\r\n$ ./manage.py runserver\r\n```\r\n\r\nThe problem is that celery doesn't get settings **CELERY_TASK_ROUTES** https://github.com/lcd1232/celery_django_test/blob/752d45f43ba0a141ca23a36bb3a3ef76e9886391/celery_test/settings.py#L123 but if uncomment [celery_test/celery.py:21-25](https://github.com/lcd1232/celery_django_test/blob/master/celery_test/celery.py#L21) all works as expected.\r\n\r\n## Update\r\nI localized the problem. https://github.com/celery/celery/blob/master/celery/app/utils.py#L217\r\nFor example if we have in settings.py `TASK_ROUTES`, `is_in_new` and `is_in_old` will be empty set because `'TASK_ROUTES' != 'task_routes'`.\r\nIf we have in settings.py 'task_routes', variable `source` will not contain `task_routes`", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4198/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "legraina": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4191", "title": "Forcing celery workers to not sleep or improving ETA", "body": "Hi,\r\n\r\n## Checklist\r\n\r\nI'm using the stable celery 4.0.2 (installed with pip) and here my report:\r\n\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:3.4.2\r\n            billiard:3.5.0.2 py-amqp:2.1.4\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:disabled\r\n\r\ntask_queues: {\r\n    'queue1': {   'binding_key': '********',\r\n                               'exchange': 'queue1'},\r\n    'queue2': {'binding_key': '********', 'exchange': 'queue2'}}\r\ncfg: <module 'celery_workers.workersconfig' from '/usr/src/workers/celery_workers/workersconfig.py'>\r\nkey: '********'\r\nbeat_max_loop_interval: 60\r\nenable_utc: True\r\nos: <module 'os' from '/usr/lib/python3.4/os.py'>\r\nvalue: 'sequential_backend_north'\r\naccept_content: ['json']\r\ntask_ignore_result: True\r\ntask_serializer: 'json'\r\nresult_serializer: 'json'\r\ntask_routes: {\r\n    'celery_workers.workers.queue1.*': {   'queue': 'queue1'},\r\n    'celery_workers.workers.queue2.*': {'queue': 'queue2'}}\r\nbroker_url: 'amqp://guest:********@rabbitmq:5672//'\r\ntimezone: 'US/Eastern'\r\nworker_timer_precision: 0.01\r\ncrontab: <class 'celery.schedules.crontab'>\r\ninclude: ['celery_workers.workers.queue1',\r\n 'celery_workers.workers.queue2']\r\n\r\n## Steps to reproduce\r\n\r\nI'm trying to simulate the system before deployment. To do so, I'm simulating the time with a task simulate_time that updates the time second by second: it updates the db with the time and apply_async an another simulate_time with time+1. Then, I play with the speed of the system: I set the ETA of the next simulate_time with datetime.utcnow()+timedelta(microseconds=round(1e6/speed)).\r\n\r\n## Expected behavior\r\n\r\nI was wondering if there is a way of forcing workers to not sleep or waking up faster such as to run the task on the expected ETA (at least in a development mode setting).\r\n\r\n## Actual behavior\r\n\r\nCelery cannot follow a speed of 16. It seems that the workers go to sleep and take some time before waking up when receiving a new task, thus slowing the overall simulation. I've attached below a log for a speed of 16. If the simulation was working correctly, we should see around 16 events within a minute instead of only 4. And if we continue of increasing the speed, celery cannot follow.\r\nFor example, we can see (in the log below) that the task is sent at 14:01:38,308, received at 14:01:38,314 with an ETA of 10:01:38.366330, but only processed at 14:01:38,495 with a processing time of 0.012 (a waste of 0.12 s.).\r\n\r\n\r\nA log:\r\ndisp-simulator-workers | [2017-08-10 14:01:38,056: INFO/PoolWorker-2] Task celery_workers.workers.simulator.simulate_time[a17f220d-2ee6-46db-9e46-87a91b5ebe0c] succeeded in 0.006356447993312031s: 40702\r\ndisp-simulator-workers | [2017-08-10 14:01:38,070: INFO/MainProcess] Received task: celery_workers.workers.simulator.simulate_time[3411b0f1-cce8-4634-ad40-122177e3ecc1]  ETA:[2017-08-10 10:01:38.114537-04:00] \r\ndisp-simulator-workers | [2017-08-10 14:01:38,308: INFO/PoolWorker-2] Task celery_workers.workers.simulator.simulate_time[3411b0f1-cce8-4634-ad40-122177e3ecc1] succeeded in 0.006479467963799834s: 40703\r\ndisp-simulator-workers | [2017-08-10 14:01:38,312: INFO/MainProcess] Received task: celery_workers.workers.simulator.simulate_time[81924d54-2e96-4636-807f-eee48f444062]  ETA:[2017-08-10 10:01:38.366330-04:00] \r\ndisp-simulator-workers | [2017-08-10 14:01:38,495: INFO/PoolWorker-1] Task celery_workers.workers.simulator.simulate_time[b1691278-90c0-4dcf-8f23-b64cfd40f936] succeeded in 0.011639489035587758s: 40704\r\ndisp-simulator-workers | [2017-08-10 14:01:38,500: INFO/MainProcess] Received task: celery_workers.workers.simulator.simulate_time[c15fdd13-5824-428d-9dc1-b67cce61f6f4]  ETA:[2017-08-10 10:01:38.551918-04:00] \r\ndisp-simulator-workers | [2017-08-10 14:01:38,561: INFO/PoolWorker-1] Task celery_workers.workers.simulator.simulate_time[c15fdd13-5824-428d-9dc1-b67cce61f6f4] succeeded in 0.005412194004748017s: 40705\r\ndisp-simulator-workers | [2017-08-10 14:01:38,566: INFO/MainProcess] Received task: celery_workers.workers.simulator.simulate_time[bd693495-6117-4010-8508-f9e11e7a23ba]  ETA:[2017-08-10 10:01:38.620754-04:00] ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chrismeyersfsu": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4185", "title": "celery workers randomly hang on IPC (causing all jobs to queue, but no longer run)", "body": "**Fails in the same way on multiple celery versions:**\r\ncelery version 3.1.7 billiard version 3.3.0.23\r\ncelery version 3.1.25 billiard version 3.3.0.23\r\n\r\n`celery worker -l debug --autoscale=50,4 -Ofair -Q tower_scheduler,tower_broadcast_all,tower,localhost -n celery@localhost`\r\n\r\n## Steps to reproduce\r\nHappens sporadically when running our nightly 8 hour integration tests on Ubuntu 14.04 and 16.04. Does not happen on RHEL 7 nor Centos 7. We are working on a set of smaller recreation steps.\r\n\r\n## Expected behavior\r\n* For the workers to continue processing work.\r\n\r\n## Actual behavior\r\nThe celery master process and a particular worker both block on the same file descriptor, performing a `read()`. The worker has finished a job and is ready for more work. The parent is \"locked up\", blocking on the `read()` system call shared by the worker.\r\n\r\nRestarting celery \"fixes\" the issue. More surgically, sending a SIGUSR1 to the master process \"fixes\" the issue by breaking it out of the `read()` system call. The child then returns from the read and seems to process a/the pending message. The master process does NOT try to `read()` from the PIPE again.\r\n\r\n* Master process strace right before the SIGUSR1 https://gist.github.com/ryanpetrello/3d2ee5556ac5d9bb778a46e5165c3f14\r\n* Worker process strace right before the SIGUSR1 https://gist.github.com/ryanpetrello/442e31102a4671a95b4ef2ef41d27bf1\r\n* lsof before the SIGUSR1 https://gist.github.com/chrismeyersfsu/17c87a314e8fe8931c9994d8fd7ca95e\r\n\r\nhttps://github.com/celery/celery/blob/3.1/celery/concurrency/asynpool.py#L220 is where we are hanging. This is a call to `__read__` gotten from https://github.com/celery/billiard/blob/3.3/Modules/_billiard/multiprocessing.c#L202\r\nThis is non-blocking and non-asynchronous. I don't really understand how this code isn't susceptible to a deadlock/infinite blocking scenario. The `read` is non-blocking and is called in such a way that it can block forever if the child dies.\r\n\r\nWe are now trying to recreate the problem with `CELERY_RDBSIG=\"1\"` set so that we can jump into a remote debug session when the deadlock occurs.\r\n\r\nAny advise would be helpful. \r\n\r\nFrom an OS perspective, I can't reason how this could occur. \r\n* No `write()` being observed after the SIGUSR1. \r\n* 2 processes blocked on the same PIPE via a read(). This means that there is either no data in the pipe or not enough data.\r\n* One process \"gives up\" (because we sent the SIGUSR1)\r\n* Then the other processes read() proceeds with data ... without a write() observed.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4185/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "x00x70": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4182", "title": "Gevent pool not removing completed messages", "body": "billiard==3.5.0.3\r\nboto3==1.4.5\r\ncelery==4.1.0\r\nkombu==4.1.0\r\ngevent==1.2.2\r\ngreenlet==0.4.12\r\n\r\nBroker: AWS SQS\r\nOS: Centos\r\n\r\nI'm having a weird issue that when using `--pool=gevent`, completed tasks are not removed from the message queue.  I don't have this issue using prefork/solo pools. When the message visibility from SQS is hit, the tasks get re-run indefinitely, even though they are stored as successful in the backend.\r\n\r\nAnyone know why the gevent pool isnt able to update my SQS queue?\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4182/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tyarimi": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4167", "title": "apply_async hangs sporadically with high CPU usage when connecting to RabbitMQ", "body": "We are using celery 4.1.0, kombu 4.1.0, vine 1.1.4, amqp 2.2.1, billiard 3.5.0.3.\r\n\r\nWe have some long running processes that enqueue tasks on RabbitMQ. After running for a few hours, some of them start hanging forever with high CPU usage. Here is a stacktrace of one of them (last call first):\r\n\r\n```\r\nFile \"/usr/local/lib/python2.7/dist-packages/vine/five.py\", line 203 in array\r\n  File \"/usr/local/lib/python2.7/dist-packages/amqp/connection.py\", line 252 in __init__\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/transport/pyamqp.py\", line 128 in establish_connection\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/connection.py\", line 757 in _establish_connection\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/connection.py\", line 802 in connection\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/connection.py\", line 261 in connect\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/utils/functional.py\", line 333 in retry_over_time\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/connection.py\", line 405 in ensure_connection\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/connection.py\", line 515 in _ensured\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/common.py\", line 147 in _imaybe_declare\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/common.py\", line 128 in maybe_declare\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/backends/rpc.py\", line 168 in on_task_call\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/base.py\", line 736 in send_task\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/task.py\", line 536 in apply_async\r\n...\r\n```\r\n\r\nSeems like it gets stuck when trying to connect to Rabbit, maybe in an infinite loop inside the `ensure_connection` function.\r\n\r\nHere is our celery config:\r\n\r\n```python\r\nCELERY_BACKEND = 'rpc'\r\nBROKER_HEARTBEAT = 60\r\nCELERYD_HIJACK_ROOT_LOGGER = False\r\nCELERY_RESULT_BACKEND = CELERY_BACKEND\r\nCELERY_ACCEPT_CONTENT = {'pickle'}\r\nCELERY_TRACK_STARTED = True\r\nCELERY_TIMEZONE = 'UTC'\r\nCELERYD_MAX_TASKS_PER_CHILD = 1000\r\nCELERY_SEND_EVENTS = False\r\nCELERY_EVENT_QUEUE_EXPIRES = 60\r\nCELERYD_TASK_SOFT_TIME_LIMIT = 12 * 60 * 60  # 12 hours\r\nCELERY_RESULT_SERIALIZER = CELERY_TASK_SERIALIZER = 'pickle'\r\nCELERYD_CONCURRENCY = 5\r\nCELERYD_PREFETCH_MULTIPLIER = 1\r\n```\r\n\r\nAny ideas?", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4167/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "meets7": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4162", "title": "Redis connection refused error but redis-cli connects and shows values which I am expecting in db", "body": "## Checklist\r\n\r\n- [x] Celery : 4.0.2\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n-\r\n## Expected behavior\r\n-\r\n## Actual behavior\r\nI am not able to figure this out. I have flask app with celery and using Redis as a broker on Ubuntu 16.04. It was working fine for the last many days and today all of a sudden it is giving the following exception.\r\n\r\n```\r\n[2017-07-26 20:12:25,512: WARNING/MainProcess] consumer: Connection to broker lost. Trying to re-establish the connection...\r\nTraceback (most recent call last):\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 129, in _read_from_socket\r\n    raise socket.error(SERVER_CLOSED_CONNECTION_ERROR)\r\nOSError: Connection closed by server.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/client.py\", line 2165, in _execute\r\n    return command(*args)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 577, in read_response\r\n    response = self._parser.read_response()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 238, in read_response\r\n    response = self._buffer.readline()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 168, in readline\r\n    self._read_from_socket()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 143, in _read_from_socket\r\n    (e.args,))\r\nredis.exceptions.ConnectionError: Error while reading from socket: ('Connection closed by server.',)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 439, in connect\r\n    sock = self._connect()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 494, in _connect\r\n    raise err\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 482, in _connect\r\n    sock.connect(socket_address)\r\nConnectionRefusedError: [Errno 111] Connection refused\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/celery/worker/consumer/consumer.py\", line 318, in start\r\n    blueprint.start(self)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/celery/worker/consumer/consumer.py\", line 594, in start\r\n    c.loop(*c.loop_args())\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/celery/worker/loops.py\", line 88, in asynloop\r\n    next(loop)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/kombu/async/hub.py\", line 345, in create_loop\r\n    cb(*cbargs)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/kombu/transport/redis.py\", line 1039, in on_readable\r\n    self.cycle.on_readable(fileno)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/kombu/transport/redis.py\", line 337, in on_readable\r\n    chan.handlers[type]()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/kombu/transport/redis.py\", line 667, in _receive\r\n    ret.append(self._receive_one(c))\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/kombu/transport/redis.py\", line 678, in _receive_one\r\n    response = c.parse_response()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/client.py\", line 2183, in parse_response\r\n    return self._execute(connection, connection.read_response)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/client.py\", line 2172, in _execute\r\n    connection.connect()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 442, in connect\r\n    raise ConnectionError(self._error_message(e))\r\nredis.exceptions.ConnectionError: Error 111 connecting to localhost:6379. Connection refused.\r\n[2017-07-26 20:12:25,804: ERROR/MainProcess] consumer: Cannot connect to redis://localhost:6379//: Error 111 connecting to localhost:6379. Connection refused..\r\nTrying again in 2.00 seconds...\r\n\r\n[2017-07-26 20:12:26,725: ERROR/PoolWorker-3] Something went wrong in async processes.\r\nTraceback (most recent call last):\r\n  File \"/home/dsangvikar/apps/msbot/tasks.py\", line 66, in send_event_notification_async\r\n    sleep(nextEventDelayInSeconds)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/billiard/common.py\", line 125, in _shutdown_cleanup\r\n    sys.exit(-(256 - signum))\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/billiard/pool.py\", line 281, in exit\r\n    return _exit()\r\nSystemExit\r\n[2017-07-26 20:12:26,726: ERROR/PoolWorker-4] Something went wrong in async processes.\r\nTraceback (most recent call last):\r\n  File \"/home/dsangvikar/apps/msbot/tasks.py\", line 66, in send_event_notification_async\r\n    sleep(nextEventDelayInSeconds)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/billiard/common.py\", line 125, in _shutdown_cleanup\r\n    sys.exit(-(256 - signum))\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/billiard/pool.py\", line 281, in exit\r\n    return _exit()\r\nSystemExit\r\n[2017-07-26 20:12:26,734: ERROR/PoolWorker-8] Something went wrong in async processes.\r\nTraceback (most recent call last):\r\n  File \"/home/dsangvikar/apps/msbot/tasks.py\", line 66, in send_event_notification_async\r\n    sleep(nextEventDelayInSeconds)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/billiard/common.py\", line 125, in _shutdown_cleanup\r\n    sys.exit(-(256 - signum))\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/billiard/pool.py\", line 281, in exit\r\n    return _exit()\r\nSystemExit\r\n[2017-07-26 20:12:27,729: ERROR/MainProcess] Process 'PoolWorker-6' pid:2057 exited with 'signal 15 (SIGTERM)'\r\n[2017-07-26 20:12:27,740: ERROR/MainProcess] Process 'PoolWorker-5' pid:2055 exited with 'exitcode 15'\r\n[2017-07-26 20:12:27,750: ERROR/MainProcess] Process 'PoolWorker-2' pid:2052 exited with 'exitcode 15'\r\n[2017-07-26 20:12:27,761: ERROR/MainProcess] Process 'PoolWorker-1' pid:2051 exited with 'exitcode 15'\r\n[2017-07-26 20:12:27,812: ERROR/MainProcess] Process 'PoolWorker-7' pid:2059 exited with 'exitcode 15'\r\n[2017-07-26 20:13:27,799: ERROR/PoolWorker-3] Something went wrong in async processes.\r\nTraceback (most recent call last):\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 129, in _read_from_socket\r\n    raise socket.error(SERVER_CLOSED_CONNECTION_ERROR)\r\nOSError: Connection closed by server.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/client.py\", line 573, in execute_command\r\n    return self.parse_response(connection, command_name, **options)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/client.py\", line 585, in parse_response\r\n    response = connection.read_response()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 577, in read_response\r\n    response = self._parser.read_response()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 238, in read_response\r\n    response = self._buffer.readline()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 168, in readline\r\n    self._read_from_socket()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 143, in _read_from_socket\r\n    (e.args,))\r\nredis.exceptions.ConnectionError: Error while reading from socket: ('Connection closed by server.',)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 439, in connect\r\n    sock = self._connect()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 494, in _connect\r\n    raise err\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 482, in _connect\r\n    sock.connect(socket_address)\r\nConnectionRefusedError: [Errno 111] Connection refused\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dsangvikar/apps/msbot/tasks.py\", line 45, in send_event_notification_async\r\n    r.set(priority, True)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/client.py\", line 1072, in set\r\n    return self.execute_command('SET', *pieces)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/client.py\", line 578, in execute_command\r\n    connection.send_command(*args)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 563, in send_command\r\n    self.send_packed_command(self.pack_command(*args))\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 538, in send_packed_command\r\n    self.connect()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 442, in connect\r\n    raise ConnectionError(self._error_message(e))\r\nredis.exceptions.ConnectionError: Error 111 connecting to localhost:6379. Connection refused.\r\n[2017-07-26 20:13:27,800: ERROR/PoolWorker-4] Something went wrong in async processes.\r\nTraceback (most recent call last):\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 129, in _read_from_socket\r\n    raise socket.error(SERVER_CLOSED_CONNECTION_ERROR)\r\nOSError: Connection closed by server.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/client.py\", line 573, in execute_command\r\n    return self.parse_response(connection, command_name, **options)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/client.py\", line 585, in parse_response\r\n    response = connection.read_response()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 577, in read_response\r\n    response = self._parser.read_response()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 238, in read_response\r\n    response = self._buffer.readline()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 168, in readline\r\n    self._read_from_socket()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 143, in _read_from_socket\r\n    (e.args,))\r\nredis.exceptions.ConnectionError: Error while reading from socket: ('Connection closed by server.',)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 439, in connect\r\n    sock = self._connect()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 494, in _connect\r\n    raise err\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 482, in _connect\r\n    sock.connect(socket_address)\r\nConnectionRefusedError: [Errno 111] Connection refused\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dsangvikar/apps/msbot/tasks.py\", line 45, in send_event_notification_async\r\n    r.set(priority, True)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/client.py\", line 1072, in set\r\n    return self.execute_command('SET', *pieces)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/client.py\", line 578, in execute_command\r\n    connection.send_command(*args)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 563, in send_command\r\n    self.send_packed_command(self.pack_command(*args))\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 538, in send_packed_command\r\n    self.connect()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 442, in connect\r\n    raise ConnectionError(self._error_message(e))\r\nredis.exceptions.ConnectionError: Error 111 connecting to localhost:6379. Connection refused.\r\n[2017-07-26 20:13:27,802: ERROR/PoolWorker-8] Something went wrong in async processes.\r\nTraceback (most recent call last):\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 129, in _read_from_socket\r\n    raise socket.error(SERVER_CLOSED_CONNECTION_ERROR)\r\nOSError: Connection closed by server.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/client.py\", line 573, in execute_command\r\n    return self.parse_response(connection, command_name, **options)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/client.py\", line 585, in parse_response\r\n    response = connection.read_response()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 577, in read_response\r\n    response = self._parser.read_response()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 238, in read_response\r\n    response = self._buffer.readline()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 168, in readline\r\n    self._read_from_socket()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 143, in _read_from_socket\r\n    (e.args,))\r\nredis.exceptions.ConnectionError: Error while reading from socket: ('Connection closed by server.',)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 439, in connect\r\n    sock = self._connect()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 494, in _connect\r\n    raise err\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 482, in _connect\r\n    sock.connect(socket_address)\r\nConnectionRefusedError: [Errno 111] Connection refused\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dsangvikar/apps/msbot/tasks.py\", line 45, in send_event_notification_async\r\n    r.set(priority, True)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/client.py\", line 1072, in set\r\n    return self.execute_command('SET', *pieces)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/client.py\", line 578, in execute_command\r\n    connection.send_command(*args)\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 563, in send_command\r\n    self.send_packed_command(self.pack_command(*args))\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 538, in send_packed_command\r\n    self.connect()\r\n  File \"/home/dsangvikar/apps/msbot/msbotenv/lib/python3.5/site-packages/redis/connection.py\", line 442, in connect\r\n    raise ConnectionError(self._error_message(e))\r\nredis.exceptions.ConnectionError: Error 111 connecting to localhost:6379. Connection refused.\r\n\r\n```\r\nI guess it retries after 2 seconds and again fails and the same exceptions. What is more confusing is redis-server is up and running properly. redis-cli ping works. Further more, I opened up redis-cli and tried to get the values which the celery worker is supposed to add. Those are also gettable. Btw, I flush all the values on server restart. So when I restarted the server, it updated the values accordingly. How do I figure out what's wrong?\r\n\r\nI didn't make any changes on the redis-server configuration files. All ways working well till yesterday. Kindly let me know what more details are required. Thanks.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4162/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "adamcharnock": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4154", "title": "Endless recursion when registering task twice", "body": "## Steps to reproduce\r\n\r\nOpening a new issue as requested by @thedrow in #1051.\r\n\r\n```python\r\n@shared_task()\r\ndef foo():\r\n    pass\r\n\r\n# sometime later\r\n\r\ncurrent_app.tasks.register(foo)  # Infinite recursion error\r\n```\r\n\r\n## Expected behavior\r\n\r\nNot infinite recursion. Perhaps an error or warning.\r\n\r\n## Actual behavior\r\n\r\nInfinite recursion error.\r\n\r\nI've fixed this bug since it occurred a couple of weeks ago and am therefore not able to recreate it in it's original context. However, I trust past me was correct in saying that the above code will recreate the problem.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4154/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ChillarAnand": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4149", "title": "Is it possible to get results from broadcast tasks?", "body": "[Celery docs](https://celery.readthedocs.io/en/latest/userguide/routing.html#broadcast) suggests it\u2019s a good idea to set the task.ignore_result attribute for broadcast.\r\n\r\nWe have 10 servers with celery workers listening to a broadcast queue. There is a `foo`  task which returns some system stats. This task returns different results on different machines.\r\n\r\nI am trying to run that task(foo.apply_async) and get results from all the machines. Unfortunately, tasks sent have same id for all workers and `foo.result` has only 1 result.\r\n\r\nIs there a way to broadcast a task to all workers and get results from all of them?", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4149/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4069", "title": "List comprehension fails in Celery rdb ", "body": "This works in Python 2 but not in Python 3.\r\n\r\n## Steps to reproduce\r\nImport rdb and telnet from the specified port.\r\nIn rdb session\r\n```py\r\n(Pdb) y=1; [y==i for i in [1,2]]\r\n*** NameError: name 'y' is not defined\r\n```\r\n## Expected behavior\r\n```\r\nIn [17]: y=1; [y==i for i in [1,2]]\r\nOut[17]: [True, False]\r\n```\r\n## Actual behavior\r\n```py\r\n(Pdb) y=1; [y==i for i in [1,2]]\r\n*** NameError: name 'y' is not defined\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4069/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4013", "title": "Updated commands to kill celery workers", "body": "```\r\nchillar+  1696 26093  1 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:MainProcess] -active- (worker -l info -A t)\r\nchillar+  1715  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-1]\r\nchillar+  1716  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-2]\r\nchillar+  1717  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-3]\r\nchillar+  1718  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-4]\r\n```\r\nWith latest version, celery worker process names seems changed. So, the commands used to kill those process needs to be updated.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alukach": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4147", "title": "Feature Idea: Pass resolution of task to subtasks", "body": "_NOTE:_ This Issue started as a Question and then turned into a Feature Idea. Read the first two comments to fully grasp the feature.\r\n\r\nI'm imagining a workflow where I have a Task Producer that wants to schedule an operation and then specify a success-handler (e.g. `chain(BigSystemSummary.s(), EmailResult.s())()` or `BigSystemSummary.apply_async(link=EmailSuccess.s(), link_error=EmailFailure.s())`).\r\n\r\nHowever, consider that the `BigSystemSummary` task is written encompassing [granularity](http://celery.readthedocs.io/en/latest/userguide/tasks.html#granularity):\r\n\r\n```python\r\n@app.task()\r\ndef BigSystemSummary():\r\n    return chord(\r\n        get_user_stats.s(),\r\n        get_db_stats.s(),\r\n        get_aws_stats.s()\r\n    )(merge_and_store_data.s())\r\n```\r\n\r\nWe obviously [don't want `BigSystemSummary` waiting synchronously for the results of its subtasks](http://celery.readthedocs.io/en/latest/userguide/tasks.html#avoid-launching-synchronous-subtasks).  However, herein-lies the problem. `BigSystemSummary` will successfully complete quickly and run `EmailSuccess` before `merge_and_store_data` has had a chance to handle all the data.  We could write all of the subtask logic on the Task Producer, but I feel like this is a bit of anti-pattern as ideally we don't want the Task Producer to be concerned with _how_ `BigSystemSummary` works. Conversely, we could put the success/failure handlers into the `BigSystemSummary` task, however again we don't want to couple that logic and instead would rather have the Task Producer have the flexibility of dictating what to do as a followup.  The ideal solution (in my mind) would be to be able to pass something from `BigSystemSummary` to `merge_and_store_data` that would signify that the subtask's results is to be used as the result for `BigSystemSummary`.   It seems like [the docs on Semipredicates](http://celery.readthedocs.io/en/latest/userguide/tasks.html#semipredicates) kind of approach this idea, however I don't think it's completely achievable today.\r\n\r\nCan anyone advise a) if this is possible today; b) if this is a bad idea; c) advise on best course of actions to implement such a feature?", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4147/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4091", "title": "Restoring task with an exchange places task back into the exchange rather than queue", "body": "I have a situation where I have a Topic Exchange with two queues bound to the exchange. Given the nature of Topic Exchanges, it's possible for a single messager to be routed to multiple queues (this is desirable for my usage). If I have a worker processing messages on one of the queue and have to stop that worker, it will restore any unacknowledged messages. However, instead of putting the message back into the queue that it retrieved the message from, it puts the message back into the exchange. This is a problem in that it means duplicate messages in other queues.  \r\n\r\nFor example, say Exchange Foo has Queue A and Queue B accepting all messages off of the exchange (`routing_key='#'`). If we submit Message 123 to the exchange, it will be routed to both queues.  If a worker is reading off of Queue A and has to restore the unacknowledged Message 123 (for whatever reason), it will place that message back into Exchange Foo. This will place the Messager 123 back into Queue A (good!) and Queue B (bad!). Queue B now has two instances of Mesage 123.\r\n\r\nI believe the correct behaviour should be to simply place the message back into the queue that it came from.  Is there a technical limitation that requires us to do this?  If not, I'd be happy to make a PR to resolve this.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4091/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4022", "title": "link_error fails if errback task is written in external codebase, raises NotRegistered error", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n\r\n### Setup\r\n\r\nI have three systems:\r\n\r\n- TaskProducer, simply schedules tasks. This happens to be a Django app, however I don't believe that is related to this issue. This system manages the routing for the tasks and queues.\r\n- ExportWorker, handles `export` queue. This system is unaware of the routing for any tasks or queues.\r\n- MessageWorker, handles `msg` queue. This system is unaware of the routing for any tasks or queues.\r\n\r\nI'm attempting to keep these systems decoupled.  They share no code.\r\n\r\nExportWorker has a single task:\r\n```python\r\n@app.task(name='export.hello', bind=True)\r\ndef hello(self, name='world'):\r\n    if name == 'homer':\r\n        raise Exception(\"NO HOMERS ALLOWED!\")\r\n    return 'hello {}'.format(name)\r\n```\r\n\r\nMessageWorker has two tasks:\r\n```python\r\n@app.task(name='msg.success', bind=True)\r\ndef email_success(self, msg, email_address):\r\n    return 'Sending email: {}'.format(msg)\r\n\r\n\r\n@app.task(name='msg.err', bind=True)\r\ndef email_err(self, context, exception, traceback):\r\n    print(\"Handled error: {}\".format(exception))\r\n    return 'Something went wrong!'\r\n```\r\n\r\n#### Settings\r\n\r\n<details>\r\n<summary><code>taskProducer$ celery -A tasks report</code> (brief, only Celery pertinent settings)</summary>\r\n\r\n```\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:3.5.2\r\n            billiard:3.5.0.2 sqs:N/A\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:sqs results:disabled\r\n\r\nCELERY_BROKER_TRANSPORT = 'sqs'\r\nCELERY_BROKER_TRANSPORT_OPTIONS = {\r\n    'region': 'us-west-2',\r\n    'queue_name_prefix': 'platform-staging-',\r\n}\r\n\r\nCELERY_TASK_ROUTES = {\r\n    'export.*': {'queue': 'export'},\r\n    'import.*': {'queue': 'import'},\r\n    'msg.*': {'queue': 'msg'},\r\n}\r\nCELERY_RESULT_QUEUE = 'result.fifo'\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary><code>taskProducer$ celery -A tasks report</code> (long)</summary>\r\n\r\n```\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:3.5.2\r\n            billiard:3.5.0.2 sqs:N/A\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:sqs results:disabled\r\n\r\nCACHES: {\r\n    'default': {   'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\r\n                   'LOCATION': 'default'},\r\n    'jsonattrs': {   'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\r\n                     'LOCATION': 'jsonattrs'}}\r\nSETTINGS_MODULE: 'config.settings.dev_debug'\r\nPASSWORD_HASHERS: '********'\r\nOSM_ATTRIBUTION: <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e052e8>\r\nUSE_TZ: True\r\nMEDIA_ROOT: '/vagrant/cadasta/core/media'\r\nFORMAT_MODULE_PATH: None\r\nSIGNING_BACKEND: 'django.core.signing.TimestampSigner'\r\nICON_URL: 'https://s3-us-west-2.amazonaws.com/cadasta-resources/icons/{}.png'\r\nCSRF_COOKIE_HTTPONLY: False\r\nDATETIME_INPUT_FORMATS: ['%Y-%m-%d %H:%M:%S',\r\n '%Y-%m-%d %H:%M:%S.%f',\r\n '%Y-%m-%d %H:%M',\r\n '%Y-%m-%d',\r\n '%m/%d/%Y %H:%M:%S',\r\n '%m/%d/%Y %H:%M:%S.%f',\r\n '%m/%d/%Y %H:%M',\r\n '%m/%d/%Y',\r\n '%m/%d/%y %H:%M:%S',\r\n '%m/%d/%y %H:%M:%S.%f',\r\n '%m/%d/%y %H:%M',\r\n '%m/%d/%y']\r\nICON_LOOKUPS: {\r\n    'application/gpx+xml': 'gpx',\r\n    'application/msexcel': 'xls',\r\n    'application/msword': 'doc',\r\n    'application/pdf': 'pdf',\r\n    'application/vnd.ms-excel': 'xls',\r\n    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet': 'xlsx',\r\n    'application/vnd.openxmlformats-officedocument.wordprocessingml.document': 'docx',\r\n    'application/xml': 'xml',\r\n    'audio/1d-interleaved-parityfec': 'audio',\r\n    'audio/32kadpcm': 'audio',\r\n    'audio/3gpp': 'audio',\r\n    'audio/3gpp2': 'audio',\r\n    'audio/ATRAC-ADVANCED-LOSSESS': 'audio',\r\n    'audio/ATRAC-X': 'audio',\r\n    'audio/ATRAC3': 'audio',\r\n    'audio/BV16': 'audio',\r\n    'audio/BV32': 'audio',\r\n    'audio/CN': 'audio',\r\n    'audio/DAT12': 'audio',\r\n    'audio/DV': 'audio',\r\n    'audio/DV14': 'audio',\r\n    'audio/EVRC': 'audio',\r\n    'audio/EVRC-QCP': 'audio',\r\n    'audio/EVRC0': 'audio',\r\n    'audio/EVRC1': 'audio',\r\n    'audio/EVRCB': 'audio',\r\n    'audio/EVRCB0': 'audio',\r\n    'audio/EVRCB1': 'audio',\r\n    'audio/EVRCNW': 'audio',\r\n    'audio/EVRCNW0': 'audio',\r\n    'audio/EVRCNW1': 'audio',\r\n    'audio/EVRCWB': 'audio',\r\n    'audio/EVRCWB0': 'audio',\r\n    'audio/EVRCWB1': 'audio',\r\n    'audio/EVS': 'audio',\r\n    'audio/G711-0': 'audio',\r\n    'audio/G719': 'audio',\r\n    'audio/G722': 'audio',\r\n    'audio/G7221': 'audio',\r\n    'audio/G723': 'audio',\r\n    'audio/G726-16': 'audio',\r\n    'audio/G726-24': 'audio',\r\n    'audio/G726-32': 'audio',\r\n    'audio/G726-40': 'audio',\r\n    'audio/G728': 'audio',\r\n    'audio/G729': 'audio',\r\n    'audio/G7291': 'audio',\r\n    'audio/G729D': 'audio',\r\n    'audio/G729E': 'audio',\r\n    'audio/GSM': 'audio',\r\n    'audio/GSM-EFR': 'audio',\r\n    'audio/GSM-HR-08': 'audio',\r\n    'audio/L16': 'audio',\r\n    'audio/L20': 'audio',\r\n    'audio/L24': 'audio',\r\n    'audio/L8': 'audio',\r\n    'audio/LPC': 'audio',\r\n    'audio/MP4A-LATM': 'audio',\r\n    'audio/MPA': 'audio',\r\n    'audio/MPA2': 'audio',\r\n    'audio/PCMA': 'audio',\r\n    'audio/PCMA-WB': 'audio',\r\n    'audio/PCMU': 'audio',\r\n    'audio/PCMU-WB': 'audio',\r\n    'audio/QCELP': 'audio',\r\n    'audio/RED': 'audio',\r\n    'audio/SMV': 'audio',\r\n    'audio/SMV-QCP': 'audio',\r\n    'audio/SMV0': 'audio',\r\n    'audio/UEMCLIP': 'audio',\r\n    'audio/VDVI': 'audio',\r\n    'audio/VMR-WB': 'audio',\r\n    'audio/aac': 'audio',\r\n    'audio/aacp': 'audio',\r\n    'audio/ac3': 'audio',\r\n    'audio/amr': 'audio',\r\n    'audio/amr-wb': 'audio',\r\n    'audio/amr-wb+': 'audio',\r\n    'audio/aptx': 'audio',\r\n    'audio/asc': 'audio',\r\n    'audio/basic': 'audio',\r\n    'audio/clearmode': 'audio',\r\n    'audio/dls': 'dls',\r\n    'audio/dsr-es201108': 'audio',\r\n    'audio/dsr-es202050': 'audio',\r\n    'audio/dsr-es202211': 'audio',\r\n    'audio/dsr-es202212': 'audio',\r\n    'audio/eac3': 'audio',\r\n    'audio/encaprtp': 'audio',\r\n    'audio/example': 'audio',\r\n    'audio/fwdred': 'audio',\r\n    'audio/iLBC': 'audio',\r\n    'audio/ip-mr_v2.5': 'audio',\r\n    'audio/m4a': 'audio',\r\n    'audio/midi': 'audio',\r\n    'audio/mobile-xmf': 'audio',\r\n    'audio/mp3': 'mp3',\r\n    'audio/mp4': 'mp4',\r\n    'audio/mpa-robust': 'audio',\r\n    'audio/mpa-robust3': 'audio',\r\n    'audio/mpeg': 'mp3',\r\n    'audio/mpeg1': 'audio',\r\n    'audio/mpeg3': 'mp3',\r\n    'audio/mpeg4-generic': 'mp4',\r\n    'audio/ogg': 'audio',\r\n    'audio/opus': 'audio',\r\n    'audio/parityfec': 'audio',\r\n    'audio/raptorfec': 'audio',\r\n    'audio/rtp-enc-aescm128': 'audio',\r\n    'audio/rtp-midi': 'audio',\r\n    'audio/rtploopback': 'audio',\r\n    'audio/rtx': 'audio',\r\n    'audio/sp-midi': 'audio',\r\n    'audio/speex': 'audio',\r\n    'audio/t140c': 'audio',\r\n    'audio/t38': 'audio',\r\n    'audio/telephone-event': 'audio',\r\n    'audio/tone': 'audio',\r\n    'audio/ulpfec': 'audio',\r\n    'audio/vorbis': 'audio',\r\n    'audio/vorbis-config': 'audio',\r\n    'audio/wav': 'audio',\r\n    'audio/wave': 'audio',\r\n    'audio/x-flac': 'audio',\r\n    'audio/x-midi': 'audio',\r\n    'audio/x-mpeg-3': 'mp3',\r\n    'audio/x-wav': 'audio',\r\n    'image/gif': 'gif',\r\n    'image/jpeg': 'jpg',\r\n    'image/png': 'png',\r\n    'image/tif': 'tiff',\r\n    'image/tiff': 'tiff',\r\n    'text/csv': 'csv',\r\n    'text/plain': 'csv',\r\n    'text/xml': 'xml',\r\n    'video/mp4': 'mp4',\r\n    'video/mpeg': 'mp3',\r\n    'video/x-mpeg': 'mp3'}\r\nAUTH_PASSWORD_VALIDATORS: '********'\r\nCACHE_MIDDLEWARE_SECONDS: 600\r\nACCOUNT_LOGOUT_REDIRECT_URL: '/account/login/'\r\nSTATICFILES_STORAGE: 'django.contrib.staticfiles.storage.StaticFilesStorage'\r\nDEVSERVER_TRUNCATE_SQL: True\r\nSESSION_FILE_PATH: None\r\nDEBUG: True\r\nLANGUAGE_COOKIE_NAME: 'django_language'\r\nPREPEND_WWW: False\r\nDEFAULT_INDEX_TABLESPACE: ''\r\nES_HOST: 'localhost'\r\nDEBUG_PROPAGATE_EXCEPTIONS: False\r\nLANGUAGES_BIDI: ['he', 'ar', 'fa', 'ur']\r\nFILE_UPLOAD_HANDLERS: ['django.core.files.uploadhandler.TemporaryFileUploadHandler']\r\nCSRF_COOKIE_DOMAIN: None\r\nSESSION_COOKIE_PATH: '/'\r\nCSRF_FAILURE_VIEW: 'django.views.csrf.csrf_failure'\r\nCSRF_COOKIE_AGE: 31449600\r\nES_SCHEME: 'http'\r\nSTATICFILES_DIRS: []\r\nFILE_UPLOAD_TEMP_DIR: None\r\nSESSION_COOKIE_HTTPONLY: True\r\nDIGITALGLOBE_TILESET_URL_FORMAT: 'https://{{s}}.tiles.mapbox.com/v4/digitalglobe.{}/{{z}}/{{x}}/{{y}}.png?access_toke'\r\nSITE_ID: 1\r\nX_FRAME_OPTIONS: 'SAMEORIGIN'\r\nNUMBER_GROUPING: 0\r\nCELERY_BROKER_TRANSPORT: 'sqs'\r\nEMAIL_TIMEOUT: None\r\nACCOUNT_EMAIL_CONFIRMATION_ANONYMOUS_REDIRECT_URL: '/account/login/'\r\nSESSION_COOKIE_DOMAIN: None\r\nEMAIL_SUBJECT_PREFIX: '[Django] '\r\nEMAIL_HOST: 'localhost'\r\nES_MAX_RESULTS: 10000\r\nBASE_TEMPLATE_DIR: '/vagrant/cadasta/templates'\r\nLEAFLET_CONFIG: {\r\n    'PLUGINS': {   'draw': {'js': '/static/leaflet/draw/leaflet.draw.js'},\r\n                   'groupedlayercontrol': {   'css': '/static/css/leaflet.groupedlayercontrol.min.css',\r\n                                              'js': '/static/js/leaflet.groupedlayercontrol.min.js'}},\r\n    'RESET_VIEW': False,\r\n    'TILES': [   (   <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e44748>,\r\n                     'http://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png',\r\n                     {   'attribution': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e052e8>,\r\n                         'maxZoom': 19}),\r\n                 (   <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f940>,\r\n                     'https://{s}.tiles.mapbox.com/v4/digitalglobe.n6ngnadl/{z}/{x}/{y}.png?access_token=pk.eyJ1IjoiZ,\r\n                     {   'attribution': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e05358>,\r\n                         'maxZoom': 22}),\r\n                 (   <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f978>,\r\n                     'https://{s}.tiles.mapbox.com/v4/digitalglobe.nal0g75k/{z}/{x}/{y}.png?access_token=pk.eyJ1IjoiZ,\r\n                     {   'attribution': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e05358>,\r\n                         'maxZoom': 22}),\r\n                 (   <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f9e8>,\r\n                     'https://{s}.tiles.mapbox.com/v4/digitalglobe.n6nhclo2/{z}/{x}/{y}.png?access_token=pk.eyJ1IjoiZ,\r\n                     {   'attribution': (   <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e052e8,\r\n                                            <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e05358,\r\n                         'maxZoom': 22}),\r\n                 (   <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1fa58>,\r\n                     'https://{s}.tiles.mapbox.com/v4/digitalglobe.nal0mpda/{z}/{x}/{y}.png?access_token=pk.eyJ1IjoiZ,\r\n                     {   'attribution': (   <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e052e8,\r\n                                            <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e05358,\r\n                         'maxZoom': 22})]}\r\nis_overridden: <bound method Settings.is_overridden of <Settings \"config.settings.dev_debug\">>\r\nMIDDLEWARE_CLASSES:\r\n    ('debug_toolbar.middleware.DebugToolbarMiddleware',\r\n 'corsheaders.middleware.CorsMiddleware',\r\n 'django.contrib.sessions.middleware.SessionMiddleware',\r\n 'django.middleware.locale.LocaleMiddleware',\r\n 'django.middleware.common.CommonMiddleware',\r\n 'django.middleware.csrf.CsrfViewMiddleware',\r\n 'django.contrib.auth.middleware.AuthenticationMiddleware',\r\n 'django.contrib.auth.middleware.SessionAuthenticationMiddleware',\r\n 'django.contrib.messages.middleware.MessageMiddleware',\r\n 'django.middleware.clickjacking.XFrameOptionsMiddleware',\r\n 'django.middleware.security.SecurityMiddleware',\r\n 'audit_log.middleware.UserLoggingMiddleware',\r\n 'simple_history.middleware.HistoryRequestMiddleware')\r\nUSE_THOUSAND_SEPARATOR: False\r\nEMAIL_USE_TLS: False\r\nLOGGING: {\r\n    'disable_existing_loggers': False,\r\n    'formatters': {   'simple': {   'format': '%(asctime)s %(levelname)s '\r\n                                              '%(message)s'}},\r\n    'handlers': {   'file': {   'class': 'logging.FileHandler',\r\n                                'filename': '/var/log/django/debug.log',\r\n                                'formatter': 'simple',\r\n                                'level': 'DEBUG'}},\r\n    'loggers': {   'django': {   'handlers': ['file'],\r\n                                 'level': 'DEBUG',\r\n                                 'propagate': True},\r\n                   'xform.submissions': {   'handlers': ['file'],\r\n                                            'level': 'DEBUG'}},\r\n    'version': 1}\r\nDATA_UPLOAD_MAX_MEMORY_SIZE: 2621440\r\nAUTH_USER_MODEL: 'accounts.User'\r\nSESSION_SAVE_EVERY_REQUEST: False\r\nIGNORABLE_404_URLS: []\r\nSTATICFILES_FINDERS:\r\n    ('django.contrib.staticfiles.finders.FileSystemFinder',\r\n 'django.contrib.staticfiles.finders.AppDirectoriesFinder',\r\n 'sass_processor.finders.CssFinder')\r\nDATABASES: {\r\n    'default': {   'ENGINE': 'django.contrib.gis.db.backends.postgis',\r\n                   'HOST': 'localhost',\r\n                   'NAME': 'cadasta',\r\n                   'PASSWORD': '********',\r\n                   'USER': 'cadasta'}}\r\nCELERY_BROKER_TRANSPORT_OPTIONS: {\r\n 'queue_name_prefix': 'platform-staging-', 'region': 'us-west-2'}\r\nDEVSERVER_MODULES:\r\n    ('devserver.modules.sql.SQLSummaryModule',\r\n 'devserver.modules.profile.ProfileSummaryModule')\r\nDECIMAL_SEPARATOR: '.'\r\nSESSION_ENGINE: 'django.contrib.sessions.backends.db'\r\nALLOWED_HOSTS: ['*']\r\nFILE_UPLOAD_PERMISSIONS: None\r\nSESSION_EXPIRE_AT_BROWSER_CLOSE: False\r\nFIXTURE_DIRS: []\r\nTIME_FORMAT: 'P'\r\nSASS_PROCESSOR_INCLUDE_DIRS:\r\n    ('/vagrant/cadasta/core/node_modules',)\r\nDEBUG_TOOLBAR_CONFIG: {\r\n 'SHOW_TOOLBAR_CALLBACK': <function always at 0x7f8467e45400>}\r\nREST_FRAMEWORK: {\r\n    'DEFAULT_AUTHENTICATION_CLASSES': (   'rest_framework.authentication.TokenAuthentication',\r\n                                          'rest_framework.authentication.BasicAuthentication',\r\n                                          'rest_framework.authentication.SessionAuthentication'),\r\n    'DEFAULT_PERMISSION_CLASSES': (   'rest_framework.permissions.IsAuthenticated',),\r\n    'DEFAULT_VERSION': 'v1',\r\n    'DEFAULT_VERSIONING_CLASS': 'rest_framework.versioning.NamespaceVersioning',\r\n    'EXCEPTION_HANDLER': 'core.views.api.exception_handler'}\r\nCADASTA_INVALID_ENTITY_NAMES: ['add', 'new']\r\nTHOUSAND_SEPARATOR: ','\r\nACCOUNT_EMAIL_CONFIRMATION_EXPIRE_DAYS: 2\r\nDATE_INPUT_FORMATS: ['%Y-%m-%d',\r\n '%m/%d/%Y',\r\n '%m/%d/%y',\r\n '%b %d %Y',\r\n '%b %d, %Y',\r\n '%d %b %Y',\r\n '%d %b, %Y',\r\n '%B %d %Y',\r\n '%B %d, %Y',\r\n '%d %B %Y',\r\n '%d %B, %Y']\r\nSECURE_PROXY_SSL_HEADER: None\r\nFILE_CHARSET: 'utf-8'\r\nSECURE_HSTS_INCLUDE_SUBDOMAINS: False\r\nDEFAULT_CHARSET: 'utf-8'\r\nMESSAGE_STORAGE: 'django.contrib.messages.storage.fallback.FallbackStorage'\r\nFIRST_DAY_OF_WEEK: 0\r\nCSRF_COOKIE_PATH: '/'\r\nFILE_UPLOAD_DIRECTORY_PERMISSIONS: None\r\nCELERY_RESULT_QUEUE: 'result.fifo'\r\nFILE_UPLOAD_MAX_MEMORY_SIZE: 2621440\r\nDEFAULT_FROM_EMAIL: 'testing@example.com'\r\nSESSION_SERIALIZER: 'django.contrib.sessions.serializers.JSONSerializer'\r\nLOGGING_CONFIG: 'logging.config.dictConfig'\r\nUSE_L10N: True\r\nLANGUAGE_COOKIE_DOMAIN: None\r\nCSRF_HEADER_NAME: 'HTTP_X_CSRFTOKEN'\r\nEMAIL_USE_SSL: False\r\nTEMPLATES: [{'BACKEND': 'django.template.backends.django.DjangoTemplates',\r\n  'DIRS': ['/vagrant/cadasta/templates',\r\n           '/vagrant/cadasta/templates/allauth'],\r\n  'OPTIONS': {'context_processors': ['django.template.context_processors.debug',\r\n                                     'django.template.context_processors.request',\r\n                                     'django.contrib.auth.context_processors.auth',\r\n                                     'django.contrib.messages.context_processors.messages'],\r\n              'loaders': ['django.template.loaders.filesystem.Loader',\r\n                          'django.template.loaders.app_directories.Loader']}}]\r\nADMINS: []\r\nSECURE_SSL_REDIRECT: False\r\nLANGUAGE_CODE: 'en-us'\r\nSECURE_REDIRECT_EXEMPT: []\r\nEMAIL_SSL_CERTFILE: None\r\nWSGI_APPLICATION: 'config.wsgi.application'\r\nLANGUAGE_COOKIE_PATH: '/'\r\nDEFAULT_TABLESPACE: ''\r\nCORS_ORIGIN_ALLOW_ALL: False\r\nEMAIL_HOST_PASSWORD: '********'\r\nSHORT_DATE_FORMAT: 'm/d/Y'\r\nLOGIN_REDIRECT_URL: '/dashboard/'\r\nDEFAULT_CONTENT_TYPE: 'text/html'\r\nDATE_FORMAT: 'N j, Y'\r\nEMAIL_HOST_USER: ''\r\nCSRF_COOKIE_NAME: 'csrftoken'\r\nEMAIL_BACKEND: 'django.core.mail.backends.console.EmailBackend'\r\nPASSWORD_RESET_TIMEOUT_DAYS: '********'\r\nCSRF_TRUSTED_ORIGINS: []\r\nBASE_DIR: '/vagrant/cadasta/config'\r\nCACHE_MIDDLEWARE_KEY_PREFIX: '********'\r\nFORM_LANGS: {\r\n    'af': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e16908>,\r\n    'ar': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e16898>,\r\n    'az': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e169e8>,\r\n    'be': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e16a20>,\r\n    'bg': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e16a58>,\r\n    'bn': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e16a90>,\r\n    'br': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e16b00>,\r\n    'bs': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e16b70>,\r\n    'ca': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e16be0>,\r\n    'cs': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e16c50>,\r\n    'cy': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e16cc0>,\r\n    'da': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e16d30>,\r\n    'de': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e16da0>,\r\n    'el': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e16e10>,\r\n    'en': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e16e80>,\r\n    'eo': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e16ef0>,\r\n    'es': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e16f60>,\r\n    'et': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e16fd0>,\r\n    'eu': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c080>,\r\n    'fa': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c0f0>,\r\n    'fi': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c160>,\r\n    'fr': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c1d0>,\r\n    'fy': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c240>,\r\n    'ga': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c2b0>,\r\n    'gd': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c320>,\r\n    'gl': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c390>,\r\n    'he': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c400>,\r\n    'hi': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c470>,\r\n    'hr': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c4e0>,\r\n    'hu': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c550>,\r\n    'ia': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c5c0>,\r\n    'id': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c630>,\r\n    'io': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c6a0>,\r\n    'is': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c710>,\r\n    'it': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c780>,\r\n    'ja': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c7f0>,\r\n    'ka': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c860>,\r\n    'kar': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f8d0>,\r\n    'kk': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c8d0>,\r\n    'km': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c940>,\r\n    'kn': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1c9b0>,\r\n    'ko': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1ca20>,\r\n    'lb': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1ca90>,\r\n    'lt': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1cb00>,\r\n    'lv': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1cb70>,\r\n    'mk': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1cbe0>,\r\n    'ml': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1cc50>,\r\n    'mn': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1ccc0>,\r\n    'mr': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1cd30>,\r\n    'my': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1cda0>,\r\n    'nb': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1ce10>,\r\n    'ne': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1ce80>,\r\n    'nl': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1cef0>,\r\n    'nn': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1cf60>,\r\n    'os': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1cfd0>,\r\n    'pa': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f080>,\r\n    'pl': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f0f0>,\r\n    'pt': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f160>,\r\n    'ro': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f1d0>,\r\n    'ru': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f240>,\r\n    'sk': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f2b0>,\r\n    'sl': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f320>,\r\n    'sq': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f390>,\r\n    'sr': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f400>,\r\n    'sv': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f470>,\r\n    'sw': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f4e0>,\r\n    'ta': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f550>,\r\n    'te': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f5c0>,\r\n    'th': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f630>,\r\n    'tr': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f6a0>,\r\n    'tt': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f710>,\r\n    'uk': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f780>,\r\n    'ur': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f7f0>,\r\n    'vi': <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1f860>}\r\nDATETIME_FORMAT: 'N j, Y, P'\r\nTIME_INPUT_FORMATS: ['%H:%M:%S', '%H:%M:%S.%f', '%H:%M']\r\nLANGUAGE_COOKIE_AGE: None\r\nABSOLUTE_URL_OVERRIDES: {\r\n }\r\nINTERNAL_IPS:\r\n    ('0.0.0.0',)\r\nSERVER_EMAIL: 'root@localhost'\r\nMIME_LOOKUPS: {\r\n 'gpx': 'application/gpx+xml'}\r\nSITE_NAME: 'Cadasta'\r\nDATA_UPLOAD_MAX_NUMBER_FIELDS: 1000\r\nACCOUNT_ADAPTER: 'accounts.adapter.DefaultAccountAdapter'\r\nSILENCED_SYSTEM_CHECKS: []\r\nDATABASE_ROUTERS: '********'\r\nLOGOUT_URL: '/account/logout/'\r\nCSRF_COOKIE_SECURE: False\r\nSECURE_BROWSER_XSS_FILTER: False\r\nAUTHENTICATION_BACKENDS: ['core.backends.Auth',\r\n 'django.contrib.auth.backends.ModelBackend',\r\n 'accounts.backends.AuthenticationBackend']\r\nSECURE_SSL_HOST: None\r\nDEFAULT_FILE_STORAGE: 'buckets.test.storage.FakeS3Storage'\r\nEMAIL_PORT: 1025\r\nUSE_X_FORWARDED_PORT: False\r\nINSTALLED_APPS:\r\n    ('debug_toolbar',\r\n 'django_extensions',\r\n 'django.contrib.auth',\r\n 'django.contrib.contenttypes',\r\n 'django.contrib.sessions',\r\n 'django.contrib.messages',\r\n 'django.contrib.staticfiles',\r\n 'django.contrib.sites',\r\n 'django.contrib.gis',\r\n 'corsheaders',\r\n 'core',\r\n 'geography',\r\n 'accounts',\r\n 'organization',\r\n 'spatial',\r\n 'questionnaires',\r\n 'resources',\r\n 'buckets',\r\n 'party',\r\n 'xforms',\r\n 'search',\r\n 'tasks',\r\n 'crispy_forms',\r\n 'parsley',\r\n 'widget_tweaks',\r\n 'django_countries',\r\n 'leaflet',\r\n 'rest_framework',\r\n 'rest_framework_gis',\r\n 'rest_framework.authtoken',\r\n 'rest_framework_docs',\r\n 'djoser',\r\n 'tutelary',\r\n 'allauth',\r\n 'allauth.account',\r\n 'allauth.socialaccount',\r\n 'sass_processor',\r\n 'simple_history',\r\n 'jsonattrs')\r\nLOGIN_URL: '/account/login/'\r\nSECURE_CONTENT_TYPE_NOSNIFF: False\r\nSHORT_DATETIME_FORMAT: 'm/d/Y P'\r\nUSE_I18N: True\r\nSECURE_HSTS_SECONDS: 0\r\nACCOUNT_LOGIN_ATTEMPTS_TIMEOUT: 86400\r\nYEAR_MONTH_FORMAT: 'F Y'\r\nAPPEND_SLASH: True\r\nMIGRATION_MODULES: {\r\n }\r\nES_PORT: '8000'\r\nTEST_RUNNER: 'django.test.runner.DiscoverRunner'\r\nLOCALE_PATHS: ['/vagrant/cadasta/config/locale']\r\nMANAGERS: []\r\nTIME_ZONE: 'UTC'\r\nDEBUG_TOOLBAR_PANELS:\r\n    ('debug_toolbar.panels.version.VersionDebugPanel',\r\n 'debug_toolbar.panels.timer.TimerDebugPanel',\r\n 'debug_toolbar.panels.headers.HeaderDebugPanel',\r\n 'debug_toolbar.panels.request_vars.RequestVarsDebugPanel',\r\n 'debug_toolbar.panels.template.TemplateDebugPanel',\r\n 'debug_toolbar.panels.sql.SQLDebugPanel',\r\n 'debug_toolbar.panels.signals.SignalDebugPanel')\r\nACCOUNT_FORMS: {\r\n    'profile': 'accounts.forms.ProfileForm',\r\n    'signup': 'accounts.forms.RegisterForm'}\r\nSESSION_COOKIE_NAME: 'sessionid'\r\nMONTH_DAY_FORMAT: 'F j'\r\nSESSION_COOKIE_AGE: 1209600\r\nDIGITALGLOBE_ATTRIBUTION: <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e05358>\r\nDJOSER: {\r\n    'ACTIVATION_URL': 'account/activate/{uid}/{token}',\r\n    'DOMAIN': 'localhost:8000',\r\n    'PASSWORD_RESET_CONFIRM_RETYPE': '********',\r\n    'PASSWORD_RESET_CONFIRM_URL': '********',\r\n    'SERIALIZERS': {'set_password_retype': '********'},\r\n    'SET_PASSWORD_RETYPE': '********',\r\n    'SITE_NAME': 'Cadasta'}\r\nDISALLOWED_USER_AGENTS: []\r\nLANGUAGES: [('en',\r\n  <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1fac8>),\r\n ('fr',\r\n  <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1fb38>),\r\n ('es',\r\n  <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1fba8>),\r\n ('id',\r\n  <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1fc18>),\r\n ('pt',\r\n  <django.utils.functional.lazy.<locals>.__proxy__ object at 0x7f8467e1fc88>)]\r\nSTATIC_URL: '/static/'\r\nATTRIBUTE_GROUPS: {\r\n    'location_attributes': {   'app_label': 'spatial',\r\n                               'label': 'Location',\r\n                               'model': 'spatialunit'},\r\n    'location_relationship_attributes': {   'app_label': 'spatial',\r\n                                            'label': 'Spatial relationship',\r\n                                            'model': 'spatialrelationship'},\r\n    'party_attributes': {   'app_label': 'party',\r\n                            'label': 'Party',\r\n                            'model': 'party'},\r\n    'party_relationship_attributes': {   'app_label': 'party',\r\n                                         'label': 'Party relationship',\r\n                                         'model': 'partyrelationship'},\r\n    'tenure_relationship_attributes': {   'app_label': 'party',\r\n                                          'label': 'Tenure Relationship',\r\n                                          'model': 'tenurerelationship'}}\r\nSESSION_COOKIE_SECURE: False\r\nROOT_URLCONF: 'config.urls.dev'\r\nTEST_NON_SERIALIZED_APPS: []\r\nJSONATTRS_SCHEMA_SELECTORS: {\r\n    'party.party': (   'project.organization.pk',\r\n                       'project.pk',\r\n                       'project.current_questionnaire',\r\n                       'type'),\r\n    'party.partyrelationship': (   'project.organization.pk',\r\n                                   'project.pk',\r\n                                   'project.current_questionnaire'),\r\n    'party.tenurerelationship': (   'project.organization.pk',\r\n                                    'project.pk',\r\n                                    'project.current_questionnaire'),\r\n    'spatial.spatialrelationship': (   'project.organization.pk',\r\n                                       'project.pk',\r\n                                       'project.current_questionnaire'),\r\n    'spatial.spatialunit': (   'project.organization.pk',\r\n                               'project.pk',\r\n                               'project.current_questionnaire')}\r\nSASS_PROCESSOR_ROOT: '/vagrant/cadasta/core/static'\r\nSESSION_CACHE_ALIAS: 'default'\r\nSECRET_KEY: '********'\r\nFORCE_SCRIPT_NAME: None\r\nCACHE_MIDDLEWARE_ALIAS: 'default'\r\nACCOUNT_LOGOUT_ON_GET: True\r\nMIDDLEWARE: None\r\nACCOUNT_CONFIRM_EMAIL_ON_GET: True\r\nUSE_ETAGS: False\r\nCELERY_TASK_ROUTES: {\r\n    'export.*': {'queue': 'export'},\r\n    'import.*': {'queue': 'import'},\r\n    'msg.*': {'queue': 'msg'}}\r\nMEDIA_URL: '/media/'\r\nIMPORTERS: {\r\n    'csv': 'organization.importers.csv.CSVImporter',\r\n    'xls': 'organization.importers.xls.XLSImporter'}\r\nDEVSERVER_AUTO_PROFILE: False\r\nDEFAULT_EXCEPTION_REPORTER_FILTER: 'django.views.debug.SafeExceptionReporterFilter'\r\nSTATIC_ROOT: None\r\nLOGOUT_REDIRECT_URL: None\r\nUSE_X_FORWARDED_HOST: False\r\nACCOUNT_AUTHENTICATION_METHOD: 'username_email'\r\nEMAIL_SSL_KEYFILE: '********'\r\n```\r\n</details>\r\n\r\n\r\n<details>\r\n<summary><code>exportWorker$ celery -A app report</code></summary>\r\n\r\n```\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:3.5.2\r\n            billiard:3.5.0.2 sqs:N/A\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:sqs results:rpc:///\r\n\r\nbroker_transport: 'sqs'\r\nworker_prefetch_multiplier: 0\r\nbroker_transport_options: {\r\n 'queue_name_prefix': 'platform-staging-', 'region': 'us-west-2'}\r\ntask_track_started: True\r\nresult_backend: 'rpc:///'\r\nos: <module 'os' from '/vagrant/test_worker/env/lib/python3.5/os.py'>\r\nimports:\r\n    ('app.tasks',)\r\n```\r\n</details>\r\n\r\n\r\n<details>\r\n<summary><code>messageWorker$ celery -A app report</code></summary>\r\n\r\n```\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:3.5.2\r\n            billiard:3.5.0.2 sqs:N/A\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:sqs results:rpc:///\r\n\r\nimports:\r\n    ('app.tasks',)\r\nbroker_transport: 'sqs'\r\ntask_track_started: True\r\nresult_backend: 'rpc:///'\r\nos: <module 'os' from '/vagrant/test_worker/env/lib/python3.5/os.py'>\r\nworker_prefetch_multiplier: 0\r\nbroker_transport_options: {\r\n 'queue_name_prefix': 'platform-staging-', 'region': 'us-west-2'}\r\n```\r\n</details>\r\n\r\n### Causing the error\r\n\r\nI spin up ExportWorker with `celery -A app worker -Q export -l INFO` and MessageWorker with `celery -A app worker -Q msg -l INFO` and then schedule the following task from the TaskProduer:\r\n\r\n```python\r\n# TaskProducer:\r\nfrom celery import Signature\r\nSignature(\r\n    'export.hello', args=['homer'], \r\n    link_error=Signature('msg.err', queue='msg')\r\n).apply_async()\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe `export.hello` task fails on ExportWorker, scheduling the `msg.err` task that is then executed on MessageWorker.\r\n\r\n## Actual behavior\r\n\r\nThe ExportWorker is unable to schedule to followup errback task defined in `link_error`, raising a `NotRegistered` exception:\r\n\r\n```python\r\n# TaskProducer:\r\nfrom celery import Signature\r\nSignature(\r\n    'export.hello', args=['homer'], \r\n    link_error=Signature('msg.err', queue='msg')\r\n).apply_async()\r\n\r\n# ExportWorker:\r\n[2017-05-09 00:14:53,458: INFO/MainProcess] Received task: export.hello[ad4ef3ea-06e8-4980-8d9c-91ae68c2305a]\r\n[2017-05-09 00:14:53,506: INFO/PoolWorker-1] Resetting dropped connection: us-west-2.queue.amazonaws.com\r\n[2017-05-09 00:14:53,517: INFO/PoolWorker-1] Starting new HTTPS connection (9): us-west-2.queue.amazonaws.com\r\n[2017-05-09 00:14:53,918: WARNING/PoolWorker-1] /vagrant/test_worker/env/lib/python3.5/site-packages/celery/app/trace.py:542: RuntimeWarning: Exception raised outside body: Task of kind 'msg.err' never registered, please make sure it's imported.:\r\nTraceback (most recent call last):\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/celery/app/trace.py\", line 367, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/celery/app/trace.py\", line 622, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n  File \"/vagrant/test_worker/app/tasks.py\", line 9, in hello\r\n    raise Exception(\"NO HOMERS ALLOWED!\")\r\nException: NO HOMERS ALLOWED!\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/vagrant/test_worker/env/src/kombu/kombu/utils/objects.py\", line 42, in __get__\r\n    return obj.__dict__[self.__name__]\r\nKeyError: 'type'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/celery/app/trace.py\", line 381, in trace_task\r\n    I, R, state, retval = on_error(task_request, exc, uuid)\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/celery/app/trace.py\", line 323, in on_error\r\n    task, request, eager=eager, call_errbacks=call_errbacks,\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/celery/app/trace.py\", line 157, in handle_error_state\r\n    call_errbacks=call_errbacks)\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/celery/app/trace.py\", line 202, in handle_failure\r\n    call_errbacks=call_errbacks,\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/celery/backends/base.py\", line 168, in mark_as_failure\r\n    self._call_task_errbacks(request, exc, traceback)\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/celery/backends/base.py\", line 174, in _call_task_errbacks\r\n    if arity_greater(errback.type.__header__, 1):\r\n  File \"/vagrant/test_worker/env/src/kombu/kombu/utils/objects.py\", line 44, in __get__\r\n    value = obj.__dict__[self.__name__] = self.__get(obj)\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/celery/canvas.py\", line 490, in type\r\n    return self._type or self.app.tasks[self['task']]\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/celery/app/registry.py\", line 19, in __missing__\r\n    raise self.NotRegistered(key)\r\ncelery.exceptions.NotRegistered: 'msg.err'\r\n\r\n  exc, exc_info.traceback)))\r\n[2017-05-09 00:14:53,996: INFO/MainProcess] Resetting dropped connection: us-west-2.queue.amazonaws.com\r\n[2017-05-09 00:14:53,999: INFO/MainProcess] Starting new HTTPS connection (3): us-west-2.queue.amazonaws.com\r\n[2017-05-09 00:14:54,237: ERROR/MainProcess] Pool callback raised exception: Task of kind 'msg.err' never registered, please make sure it's imported.\r\nTraceback (most recent call last):\r\n  File \"/vagrant/test_worker/env/src/kombu/kombu/utils/objects.py\", line 42, in __get__\r\n    return obj.__dict__[self.__name__]\r\nKeyError: 'type'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/billiard/pool.py\", line 1748, in safe_apply_callback\r\n    fun(*args, **kwargs)\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/celery/worker/request.py\", line 366, in on_failure\r\n    self.id, exc, request=self, store_result=self.store_errors,\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/celery/backends/base.py\", line 168, in mark_as_failure\r\n    self._call_task_errbacks(request, exc, traceback)\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/celery/backends/base.py\", line 174, in _call_task_errbacks\r\n    if arity_greater(errback.type.__header__, 1):\r\n  File \"/vagrant/test_worker/env/src/kombu/kombu/utils/objects.py\", line 44, in __get__\r\n    value = obj.__dict__[self.__name__] = self.__get(obj)\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/celery/canvas.py\", line 490, in type\r\n    return self._type or self.app.tasks[self['task']]\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/celery/app/registry.py\", line 19, in __missing__\r\n    raise self.NotRegistered(key)\r\ncelery.exceptions.NotRegistered: 'msg.err'\r\n\r\n# (MessageWorker has no output)\r\n```\r\n\r\n\r\nBeing that neither of the workers have any awareness of task routing, I manually set the `queue` for any task that is scheduled on the worker. This technique works for standard `link` operations:\r\n\r\n```python\r\n# TaskProducer:\r\nfrom celery import Signature\r\nSignature(\r\n    'export.hello', args=['world'], \r\n    link=Signature(\r\n        'msg.success', kwargs={'email_address': 'world@company.com'}, queue='msg'\r\n    )\r\n).apply_async()\r\n\r\n# ExportWorker:\r\n[2017-05-09 00:08:29,290: INFO/MainProcess] Received task: export.hello[a08db60b-7c59-478e-9293-c0a716629b11]\r\n[2017-05-09 00:08:30,747: INFO/PoolWorker-1] Resetting dropped connection: us-west-2.queue.amazonaws.com\r\n[2017-05-09 00:08:30,751: INFO/PoolWorker-1] Starting new HTTPS connection (7): us-west-2.queue.amazonaws.com\r\n[2017-05-09 00:08:32,469: INFO/PoolWorker-1] Task export.hello[a08db60b-7c59-478e-9293-c0a716629b11] succeeded in 1.723272997973254s: 'hello world'\r\n\r\n# MessageWorker:\r\n[2017-05-09 00:08:32,448: INFO/MainProcess] Received task: msg.success[b728d3c7-34cf-49ea-9cfe-5d374c3c1d0e]\r\n[2017-05-09 00:08:32,497: INFO/PoolWorker-1] Resetting dropped connection: us-west-2.queue.amazonaws.com\r\n[2017-05-09 00:08:32,504: INFO/PoolWorker-1] Starting new HTTPS connection (5): us-west-2.queue.amazonaws.com\r\n[2017-05-09 00:08:32,738: INFO/PoolWorker-1] Task msg.success[b728d3c7-34cf-49ea-9cfe-5d374c3c1d0e] succeeded in 0.24422147497534752s: 'Sending email: hello world'\r\n```\r\n\r\nThe above code successfully runs `export.hello` on ExportWorker and then passes the results to the `msg.success` task which is run on MessageWorker. For the record, `chain()` works as well. \r\n\r\nFinally, if the errback is included with the callback as a list in the `link` argument, it appears that the errback isn't event attempted:\r\n\r\n```python\r\n# On TaskProducer\r\nSignature(\r\n    'export.hello', args=['homer'], \r\n    link=[\r\n        Signature(\r\n            'msg.success', kwargs={'email_address': 'world@company.com'}, queue='msg'),\r\n        Signature(\r\n            'msg.err', queue='msg')\r\n    ]\r\n).apply_async()\r\n\r\n\r\n# On ExportWorker\r\n[2017-05-09 00:02:26,406: ERROR/PoolWorker-1] Task export.hello[571fd654-909c-4aa4-b6df-50967d172d9f] raised unexpected: Exception('NO HOMERS ALLOWED!',)\r\nTraceback (most recent call last):\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/celery/app/trace.py\", line 367, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/celery/app/trace.py\", line 622, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n  File \"/vagrant/test_worker/app/tasks.py\", line 9, in hello\r\n    raise Exception(\"NO HOMERS ALLOWED!\")\r\nException: NO HOMERS ALLOWED!\r\n\r\n# (MessageWorker has no output)\r\n```\r\n\r\n### Ideas\r\n\r\nIf I move the `email.err` task to ExportWorker, it actually catches and handles the error:\r\n\r\n```python\r\n@app.task(name='export.hello', bind=True)\r\ndef hello(self, name='world'):\r\n    if name == 'homer':\r\n        raise Exception(\"NO HOMERS ALLOWED!\")\r\n    return 'hello {}'.format(name)\r\n\r\n\r\n@app.task(name='msg.err', bind=True)\r\ndef email_err(self, context, exception, traceback):\r\n    print(\"Handled error: {}\".format(exception))\r\n    return 'Something went wrong!'\r\n```\r\n\r\n```python\r\n# TaskProducer:\r\nfrom celery import Signature\r\nSignature(\r\n    'export.hello', args=['homer'], \r\n    link_error=Signature('msg.err', queue='msg')\r\n).apply_async()\r\n\r\n# ExportWorker:\r\n[2017-05-09 00:31:49,666: INFO/MainProcess] Received task: export.hello[14ca7a6e-6e7a-4b96-b0aa-42f4e4919f53]\r\n[2017-05-09 00:31:49,738: INFO/PoolWorker-1] Found credentials in environment variables.\r\n[2017-05-09 00:31:50,031: INFO/PoolWorker-1] Starting new HTTPS connection (1): us-west-2.queue.amazonaws.com\r\n[2017-05-09 00:31:50,429: WARNING/PoolWorker-1] Handled error: NO HOMERS ALLOWED!\r\n[2017-05-09 00:31:50,430: ERROR/PoolWorker-1] Task export.hello[14ca7a6e-6e7a-4b96-b0aa-42f4e4919f53] raised unexpected: Exception('NO HOMERS ALLOWED!',)\r\nTraceback (most recent call last):\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/celery/app/trace.py\", line 367, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n  File \"/vagrant/test_worker/env/lib/python3.5/site-packages/celery/app/trace.py\", line 622, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n  File \"/vagrant/test_worker/app/tasks.py\", line 9, in hello\r\n    raise Exception(\"NO HOMERS ALLOWED!\")\r\nException: NO HOMERS ALLOWED!\r\n\r\n# (MessageWorker has no output)\r\n```\r\nThis is despite the fact that ExportWorker should only be reading off of the `export` queue.  **This leads me to believe that the issue may be that the `link_error` logic does not respect the `queue` argument.**  However, I haven't dug into the code enough to verify this.\r\n\r\nThis is possibly related to #3350.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4022/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/1cf59f91343ce7305a867067102a63f644b99105", "message": "Fix CELERY_TASK_ settings (#4094)"}, {"url": "https://api.github.com/repos/celery/celery/commits/cef9781d4de95938edabfc20061e48a392c6390c", "message": "Add note about RPC backend not support chords (#4120)"}, {"url": "https://api.github.com/repos/celery/celery/commits/a30616add45a0a2ab197485e193e46d62a810530", "message": "Add link to Version 2"}, {"url": "https://api.github.com/repos/celery/celery/commits/bb80558fbe8c671b3973d93ed40c3226936e9af4", "message": "Fix docs example (small) (#4009)\n\n* Fix docs example\r\n\r\n* Freeze pydocstyle to 1.1.1\r\n\r\n* Add self to contributors"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "auvipy": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4142", "title": "Fund raising campaign for full time fellow maintainer position", "body": "http://www.celeryproject.org/news/fund-raising-celery-fellowship-program/\r\n\r\nor direct https://igg.me/at/dFLL0FmznfI/x/16384863", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4142/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/028dbe4a4d6786d56ed30ea49971cc5415fffb4b", "message": "update version to 4.2.0"}, {"url": "https://api.github.com/repos/celery/celery/commits/ce8ea16b8df10ea323ff2933d050578fe609b61a", "message": "added python3.6 on tox (#3749)"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4459", "title": "[wip] #3021 bug fix ", "body": "Fixes #3021", "author_association": "OWNER"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/18105603", "body": "How about moving it into a separate package? @ask \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18105603/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/18105632", "body": "If so then some voluntier could dedicate their time to maintain the package and the core remain lighter.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18105632/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/18105854", "body": "I will be there to help. and I believe there would be more interested from redislab\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18105854/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/19069619", "body": "+1 for 1.8+ only \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/19069619/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/48143307", "body": "isn't it manage.py? and if you write ./manage.py shell, no need to put python infront\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/48143307/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/73799260", "body": "there should be a newline at the end of code\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/73799260/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/119898264", "body": "what si() refers to? mul means multiply?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/119898264/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/119986188", "body": "i was asking about si(1, 2) :D ", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/119986188/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890603", "body": "this can be chopped", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890603/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890616", "body": "is this actually worth a mention?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890616/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890624", "body": "this doc fix?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890624/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890633", "body": "i guess it's not needed too", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890633/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890640", "body": "this type of doc fix can all be incorporated in a single note i believe", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890640/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890685", "body": "this can be dropped or modified", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128890685/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128892820", "body": "seems like yes :p ", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128892820/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128923843", "body": "changelog-4.1.rst ?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128923843/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "Shir0kamii": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4129", "title": "Suggested feature: task recorder", "body": "Hi!\r\n\r\nI recently implemented email notifications in a Flask application and for that, I used `Flask-Mail` and `celery`. Testing the code for mails was easy with their `record_messages` context manager, and I found that feature lacking for `celery`.\r\n\r\nMy idea is a `record_tasks` context manager yielding a list that will contain all tasks published during the context.\r\n\r\nHaving this feature would enable this sort of tests:\r\n\r\n```python\r\ndef send_mail_delayed():\r\n    send_mail.apply_async(eta=some_time)\r\n\r\ndef test_task_eta():\r\n    with record_tasks() as tasks:\r\n        send_mail_delayed()\r\n        assert len(tasks) == 1\r\n        assert tasks[0].headers[\"eta\"] == some_time\r\n```\r\n\r\nI didn't looked but it could also probably be used to make existing tests easier to read.\r\n\r\nI'm willing to implement it but would prefer approval of the concept before doing so. What do you think of it ?", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4129/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jiangrzh": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4127", "title": "Any idea to use direct reply-to futrue in the rpc backend?", "body": "It's any idea to using the direct reply-to futrue of the rabbitmq to improve performance  for rpc backend ? ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4127/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "listingmirror": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4125", "title": "Does WORKER_MAX_MEMORY_PER_CHILD work?", "body": "## Steps to reproduce\r\n\r\n`celery==4.0.2`\r\n`python 3.6.1`\r\n`WORKER_MAX_MEMORY_PER_CHILD=200000`\r\n\r\n## Expected behavior\r\n\r\n- Worker initial starts with about 90MB of used memory used, as measured by `mem_rss()`\r\n- As time goes on, it slowly approaches 200MB of used memory\r\n- After worker passes 200MB, worker is killed and replaced. \r\n- New worker starts at 90MB of used memory\r\n- Repeat\r\n\r\n\r\n## Actual behavior\r\n\r\n- Worker initial starts with about 90MB of used memory used, as measured by `mem_rss()`\r\n- As time goes on, it slowly approaches 200MB of used memory\r\n- After worker passes 200MB, worker is killed and replaced\r\n- New worker starts at 175-210MB of used memory, and is immediately killed most of the time\r\n- Stuck in a cycle where each worker does 1 task, and gets killed\r\n\r\n\r\n\r\nIs something in the fork for a worker retaining memory between replacements?\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4125/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rhymes": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4110", "title": "Question: any chance to have a on_started handler in the Task class?", "body": "It would be very handy to have an on_started call back in the Task class. In addition to after_return, on_failure, on_success and on_retry.\r\n\r\nThoughts?", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4110/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "oortega": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4107", "title": "Celery Beat run many times Task with Crontab X * * * * minutes", "body": "Hello, When I add a Periodic task with crontab every 3 minutes, this run many times does not respect the 3 minutes.\r\n\r\n## Steps to reproduce\r\n\r\nAttach an image with the problem:\r\n\r\nhttp://g.recordit.co/F14hwQ1rll.gif\r\n\r\nI tried  this but is the same result:\r\n\r\n```\r\nfrom celery.schedules import crontab\r\napp.conf.beat_schedule = {\r\n     'Suma': {\r\n         'task': 'test1.tasks.add',\r\n         'schedule': crontab(minute=4),\r\n         'args': (15, 15),\r\n     },\r\n }\r\n```\r\n\r\nBut if task run crontab  every X hours not happened.\r\n\r\n## Expected behavior\r\nRun every 3 minutes\r\n\r\n## Actual behavior\r\nRun many times every second, \r\n\r\n## Setup\r\nI Installed:\r\n(I installed this version because I nedd this change https://github.com/celery/celery/pull/3890)\r\n\r\npip install https://github.com/celery/celery/zipball/master#egg=celery\r\n pip install https://github.com/celery/billiard/zipball/master#egg=billiard\r\n pip install https://github.com/celery/py-amqp/zipball/master#egg=amqp\r\n pip install https://github.com/celery/kombu/zipball/master#egg=kombu\r\n pip install https://github.com/celery/vine/zipball/master#egg=vine\r\n pip install https://github.com/celery/django-celery-beat/zipball/master#egg=django-celery-beat\r\n\r\nI run celery:\r\n\r\n/var/waps/entornos/dcelery2/bin/celery -A dcelery beat -l info -S django \r\n/var/waps/entornos/dcelery2/bin/celery worker -A dcelery --loglevel=INFO\r\n\r\n#Pip Freeze\r\namqp==2.1.4\r\nbilliard==3.5.0.2\r\ncelery==4.0.2\r\nDjango==1.11.2\r\ndjango-celery-beat==1.0.1\r\ndjango-celery-results==1.0.1\r\nkombu==4.0.2\r\nMySQL-python==1.2.5\r\npytz==2017.2\r\nvine==1.1.3\r\n\r\nCELERY_BROKER_URL = 'redis://localhost:6379'\r\n#CELERY_BROKER_URL = 'amqp://guest:guest@localhost//'\r\n\r\n#: Only add pickle to this list if your broker is secured\r\n#: from unwanted access (see userguide/security.html)\r\nCELERY_ACCEPT_CONTENT = ['json']\r\nCELERY_RESULT_BACKEND = 'django-db'\r\nCELERY_TASK_SERIALIZER = 'json'\r\nCELERY_BEAT_SCHEDULER = 'django_celery_beat.schedulers:DatabaseScheduler'\r\nCELERY_ENABLE_UTC=True\r\nCELERY_TIMEZONE = TIME_ZONE\r\n\r\n\r\nThis is my project test: https://bitbucket.org/ortega/dcelery\r\n\r\n\r\nSomeone else happens?\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4107/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "psrok1": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4105", "title": "ResultConsumer greenlet race condition", "body": "When I tried to get result from task inside greenlet, I've got an exception:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/greenlet.py\", line 534, in run\r\n    result = self._run(*self.args, **self.kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/backends/async.py\", line 83, in run\r\n    self.result_consumer.drain_events(timeout=1)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/backends/redis.py\", line 69, in drain_events\r\n    m = self._pubsub.get_message(timeout=timeout)\r\nAttributeError: 'NoneType' object has no attribute 'get_message'\r\n<Greenlet at 0x7efd9d8ba550: <bound method geventDrainer.run of <celery.backends.async.geventDrainer object at 0x7efd9d99f550>>> failed with AttributeError\r\n```\r\n\r\nCelery version:\r\n\r\n```\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:2.7.12\r\n                   billiard:3.5.0.2 redis:2.10.5\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis\r\n```\r\n\r\nAfter some recon, I've found that error occurs, when `threading` module is monkey-patched by `gevent.monkey`. It seems that attribute `_pubsub` from `celery.backends.async.ResultConsumer` is initialized a bit too late. Bug is triggered by racing `greenletDrainer.start()` and `ResultConsumer.start()` initializers.\r\n\r\n```python\r\n    def add_pending_result(self, result, weak=False, start_drainer=True):\r\n        if start_drainer:\r\n            # Spawns greenlet and starts to drain events\r\n            self.result_consumer.drainer.start()\r\n        try:\r\n            self._maybe_resolve_from_buffer(result)\r\n        except Empty:\r\n            # Initializes pubsub needed by drainer\r\n            self._add_pending_result(result.id, result, weak=weak)\r\n        return result\r\n```\r\n\r\n```python\r\nclass greenletDrainer(Drainer):\r\n    #...\r\n\r\n    def run(self):\r\n        self._started.set()\r\n        while not self._stopped.is_set():\r\n            try:\r\n                self.result_consumer.drain_events(timeout=1)\r\n            except socket.timeout:\r\n                pass\r\n        self._shutdown.set()\r\n\r\n    # self.result_consumer.drainer.start()\r\n    def start(self):\r\n        if not self._started.is_set():\r\n            self._g = self.spawn(self.run)\r\n            # Switches immediately to self.run\r\n            self._started.wait()\r\n```\r\n\r\nIssue related with #3452.\r\n\r\nFiltered Python trace and way to reproduce a bug can be found [in this Gist](https://gist.github.com/psrok1/8dc27d3cdf367573183fc3f1e5524293)\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4105/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rishabh18": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4098", "title": "RuntimeErrorkombu.connection in acquire ; Acquire on closed pool", "body": "## Checklist\r\nUsing following versions of services\r\nkombu==4.0.2\r\ncelery==3.1.18\r\ndjango-celery==3.1.16\r\namqp==2.1.4\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [ ] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n\r\n## Expected behavior\r\n\r\n## Actual behavior\r\nConnection is getting closed while serving the tasks , and throwing error as Acquire on closed pool\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4098/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mehdigmira": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4089", "title": "Attributes are shared across multiple workers when using gevent pool", "body": "My version of celery is 4.0.2 (latentcall)\r\n\r\n## Example that works fine (using a subprocess pool for the worker):\r\n\r\n```python\r\n# tasks/tasks.py\r\n\r\n#from gevent import monkey\r\n#monkey.patch_all()\r\nfrom celery import Celery\r\nfrom time import sleep\r\nfrom celery import Task\r\n\r\n\r\napp = Celery('tasks', broker='redis://localhost:6379/0')\r\n\r\n\r\nclass TestTask(Task):\r\n    def run(self):\r\n        self.x = []\r\n        for t in range(10):\r\n                self.x.append(t)\r\n                sleep(1)\r\n                print self.x\r\n\r\napp.register_task(TestTask())\r\n```\r\n```python\r\n# run.py\r\n\r\nfrom celery import Celery\r\nfrom tasks import TestTask\r\napp = Celery('tasks', broker='redis://localhost:6379/0')\r\nfor x in range(5):\r\n        app.send_task(\"tasks.tasks.TestTask\")\r\n```\r\n\r\nThen running `celery -A tasks worker -c 4 -l DEBUG` outputs (as expected):\r\n\r\n> [2017-06-15 09:48:14,552: WARNING/PoolWorker-2] [0]\r\n> [2017-06-15 09:48:14,552: WARNING/PoolWorker-1] [0]\r\n> [2017-06-15 09:48:14,558: WARNING/PoolWorker-4] [0]\r\n> [2017-06-15 09:48:14,558: WARNING/PoolWorker-3] [0]\r\n> [2017-06-15 09:48:15,554: WARNING/PoolWorker-2] [0, 1]\r\n> [2017-06-15 09:48:15,554: WARNING/PoolWorker-1] [0, 1]\r\n> [2017-06-15 09:48:15,559: WARNING/PoolWorker-3] [0, 1]\r\n> [2017-06-15 09:48:15,559: WARNING/PoolWorker-4] [0, 1]\r\n> [2017-06-15 09:48:16,556: WARNING/PoolWorker-1] [0, 1, 2]\r\n> [2017-06-15 09:48:16,556: WARNING/PoolWorker-2] [0, 1, 2]\r\n> [2017-06-15 09:48:16,560: WARNING/PoolWorker-4] [0, 1, 2]\r\n> [2017-06-15 09:48:16,560: WARNING/PoolWorker-3] [0, 1, 2]\r\n> [2017-06-15 09:48:17,558: WARNING/PoolWorker-1] [0, 1, 2, 3]\r\n> [2017-06-15 09:48:17,558: WARNING/PoolWorker-2] [0, 1, 2, 3]\r\n> [2017-06-15 09:48:17,561: WARNING/PoolWorker-4] [0, 1, 2, 3]\r\n> [2017-06-15 09:48:17,562: WARNING/PoolWorker-3] [0, 1, 2, 3]\r\n> [2017-06-15 09:48:18,559: WARNING/PoolWorker-1] [0, 1, 2, 3, 4]\r\n> [2017-06-15 09:48:18,559: WARNING/PoolWorker-2] [0, 1, 2, 3, 4]\r\n> [2017-06-15 09:48:18,563: WARNING/PoolWorker-4] [0, 1, 2, 3, 4]\r\n> [2017-06-15 09:48:18,563: WARNING/PoolWorker-3] [0, 1, 2, 3, 4]\r\n> [2017-06-15 09:48:19,561: WARNING/PoolWorker-1] [0, 1, 2, 3, 4, 5]\r\n> [2017-06-15 09:48:19,561: WARNING/PoolWorker-2] [0, 1, 2, 3, 4, 5]\r\n> [2017-06-15 09:48:19,565: WARNING/PoolWorker-3] [0, 1, 2, 3, 4, 5]\r\n> [2017-06-15 09:48:19,565: WARNING/PoolWorker-4] [0, 1, 2, 3, 4, 5]\r\n> [2017-06-15 09:48:20,562: WARNING/PoolWorker-1] [0, 1, 2, 3, 4, 5, 6]\r\n> [2017-06-15 09:48:20,562: WARNING/PoolWorker-2] [0, 1, 2, 3, 4, 5, 6]\r\n> [2017-06-15 09:48:20,566: WARNING/PoolWorker-4] [0, 1, 2, 3, 4, 5, 6]\r\n> [2017-06-15 09:48:20,566: WARNING/PoolWorker-3] [0, 1, 2, 3, 4, 5, 6]\r\n> [2017-06-15 09:48:21,564: WARNING/PoolWorker-2] [0, 1, 2, 3, 4, 5, 6, 7]\r\n> [2017-06-15 09:48:21,564: WARNING/PoolWorker-1] [0, 1, 2, 3, 4, 5, 6, 7]\r\n> [2017-06-15 09:48:21,567: WARNING/PoolWorker-3] [0, 1, 2, 3, 4, 5, 6, 7]\r\n> [2017-06-15 09:48:21,567: WARNING/PoolWorker-4] [0, 1, 2, 3, 4, 5, 6, 7]\r\n> [2017-06-15 09:48:22,566: WARNING/PoolWorker-2] [0, 1, 2, 3, 4, 5, 6, 7, 8]\r\n> [2017-06-15 09:48:22,566: WARNING/PoolWorker-1] [0, 1, 2, 3, 4, 5, 6, 7, 8]\r\n> [2017-06-15 09:48:22,569: WARNING/PoolWorker-3] [0, 1, 2, 3, 4, 5, 6, 7, 8]\r\n> [2017-06-15 09:48:22,569: WARNING/PoolWorker-4] [0, 1, 2, 3, 4, 5, 6, 7, 8]\r\n> [2017-06-15 09:48:23,567: WARNING/PoolWorker-2] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n> [2017-06-15 09:48:23,568: WARNING/PoolWorker-1] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n\r\n## Example that doesn't work fine (using a gevent pool):\r\n\r\nJust changing tasks.py to make it gevent friendly\r\n```python\r\n# tasks/tasks.py\r\n\r\nfrom gevent import monkey\r\nmonkey.patch_all()\r\nfrom celery import Celery\r\nfrom time import sleep\r\nfrom celery import Task\r\n\r\n\r\napp = Celery('tasks', broker='redis://localhost:6379/0')\r\n\r\n\r\nclass TestTask(Task):\r\n    def run(self):\r\n        self.x = []\r\n        for t in range(10):\r\n                self.x.append(t)\r\n                sleep(1)\r\n                print self.x\r\n\r\napp.register_task(TestTask())\r\n```\r\nand then running `celery -A tasks worker -c 4 -l DEBUG -P gevent` outputs:\r\n\r\n> [2017-06-15 09:49:35,825: WARNING/MainProcess] [0]\r\n> [2017-06-15 09:49:35,826: WARNING/MainProcess] [0, 1]\r\n> [2017-06-15 09:49:35,826: WARNING/MainProcess] [0, 1, 1]\r\n> [2017-06-15 09:49:35,830: WARNING/MainProcess] [0, 1, 1, 1]\r\n> [2017-06-15 09:49:36,826: WARNING/MainProcess] [0, 1, 1, 1, 1]\r\n> [2017-06-15 09:49:36,826: WARNING/MainProcess] [0, 1, 1, 1, 1, 2]\r\n> [2017-06-15 09:49:36,827: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2]\r\n> [2017-06-15 09:49:36,832: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2]\r\n> [2017-06-15 09:49:37,830: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2]\r\n> [2017-06-15 09:49:37,831: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3]\r\n> [2017-06-15 09:49:37,831: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3]\r\n> [2017-06-15 09:49:37,833: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3]\r\n> [2017-06-15 09:49:38,834: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]\r\n> [2017-06-15 09:49:38,834: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4]\r\n> [2017-06-15 09:49:38,835: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4]\r\n> [2017-06-15 09:49:38,835: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4]\r\n> [2017-06-15 09:49:39,840: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4]\r\n> [2017-06-15 09:49:39,840: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5]\r\n> [2017-06-15 09:49:39,841: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5]\r\n> [2017-06-15 09:49:39,841: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5]\r\n> [2017-06-15 09:49:40,841: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5]\r\n> [2017-06-15 09:49:40,841: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6]\r\n> [2017-06-15 09:49:40,842: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6]\r\n> [2017-06-15 09:49:40,842: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6]\r\n> [2017-06-15 09:49:41,843: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6]\r\n> [2017-06-15 09:49:41,843: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7]\r\n> [2017-06-15 09:49:41,844: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7]\r\n> [2017-06-15 09:49:41,844: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7]\r\n> [2017-06-15 09:49:42,849: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7]\r\n> [2017-06-15 09:49:42,849: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8]\r\n> [2017-06-15 09:49:42,850: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8]\r\n> [2017-06-15 09:49:42,850: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8]\r\n> [2017-06-15 09:49:43,854: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8]\r\n> [2017-06-15 09:49:43,854: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9]\r\n> [2017-06-15 09:49:43,855: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9]\r\n> [2017-06-15 09:49:43,855: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9]\r\n> [2017-06-15 09:49:44,859: WARNING/MainProcess] [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9]\r\n\r\nAs you can see the self.x attribute gets shared across all the gevent workers running at the same time, which is absolutely not what i want !", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4089/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pumazi": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4088", "title": "Hanging celery_worker pytest fixture", "body": "## Checklist\r\n\r\n- [X] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n```\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:2.7.12\r\n            billiard:3.5.0.2 py-amqp:2.1.4\r\nplatform -> system:Darwin arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:pyamqp results:db+postgresql://*******@localhost/*************\r\n\r\nresult_backend: u'db+postgresql://**********@localhost/****************'\r\nbroker_url: u'amqp://guest:********@localhost:5672//'\r\nresult_persistent: True\r\n```\r\n- [X] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n\r\nThere are two tests in a class that use the pytest `celery.contrib.pytest.celery_worker` fixture. Individually the tests run and pass without issue. Together the first test passes and the second hangs with the following log output:\r\n\r\n```pytb\r\n[2017-06-14 16:10:45,617: CRITICAL/MainProcess] Unrecoverable error: AttributeError(\"'NoneType' object has no attribute 'register'\",)\r\nTraceback (most recent call last):\r\n  File \".../lib/python2.7/site-packages/celery/worker/worker.py\", line 203, in start\r\n    self.blueprint.start(self)\r\n  File \".../lib/python2.7/site-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \".../lib/python2.7/site-packages/celery/bootsteps.py\", line 370, in start\r\n    return self.obj.start()\r\n  File \".../lib/python2.7/site-packages/celery/worker/consumer/consumer.py\", line 318, in start\r\n    blueprint.start(self)\r\n  File \".../lib/python2.7/site-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \".../lib/python2.7/site-packages/celery/worker/consumer/connection.py\", line 21, in start\r\n    c.connection = c.connect()\r\n  File \".../lib/python2.7/site-packages/celery/worker/consumer/consumer.py\", line 407, in connect\r\n    conn.transport.register_with_event_loop(conn.connection, self.hub)\r\n  File \".../lib/python2.7/site-packages/kombu/transport/pyamqp.py\", line 146, in register_with_event_loop\r\n    loop.add_reader(connection.sock, self.on_readable, connection, loop)\r\n  File \".../lib/python2.7/site-packages/kombu/async/hub.py\", line 208, in add_reader\r\n    return self.add(fds, callback, READ | ERR, args)\r\n  File \".../lib/python2.7/site-packages/kombu/async/hub.py\", line 159, in add\r\n    self.poller.register(fd, flags)\r\nAttributeError: 'NoneType' object has no attribute 'register'\r\n```\r\n\r\n## Expected behavior\r\n\r\nI'd expect both tests to pass in sequence.\r\n\r\n## Actual behavior\r\n\r\nThe pytest runner hangs.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4088/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mke21": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4084", "title": "rpc backend doesn't seem to report message status correctly.", "body": "This is version 4.0.2\r\nIf I use rabbitmq, the rpc result backend, and a custom queue the message's status never seems to change and stays 'PENDING', even if the logging in the worker reports that it has been successfully executed until I do a get() of some sorts when the status changes to 'SUCCESS'. When I change the backend to amqp this the system works as expected, giving SUCCESS before doing the get(). Also the redis backend doesn't give this problem, so it seems to be rpc specific.\r\n\r\nNote that not setting a custom queue, so using the default also works as expected just like the other backends!\r\n\r\nI've got in my tasks.py:\r\n```python\r\n# tasks.py\r\nfrom celery import Celery\r\n\r\napp = Celery('tasks', \r\nbroker='amqp://',\r\nbackend='rpc://' # here I use the rpc backend\r\n)\r\n@app.task\r\ndef test_task(s):\r\n    return s\r\n```\r\nI start this with `celery -A tasks worker --loglevel=info -Q myqueue\r\n\r\nOn the other side I do:\r\n```python\r\n>>> import tasks\r\n>>> result = tasks.test_task.apply_async(queue='myqueue', args=['blaat',]) # set custom queue\r\n\r\n>>> result.ready() # worker already executed, so should give True\r\nFalse\r\n>>> result.status # with RPC backend it returned False while it should have been True in previous statement\r\n'PENDING'\r\n>>> print(result.get())\r\n'This is a string'\r\n>>> result.ready()\r\nTrue\r\n>>> result.status\r\n'SUCCESS'\r\n```\r\n\r\nIf I start this qithout the -Q option: 'celery -A tasks worker --loglevel=info' I get:\r\n```python\r\n>>> import tasks\r\n>>> result = tasks.test_task.apply_async(args=['blaat',]) # set no custom queue\r\n\r\n>>> result.ready() # worker already executed, so should give True\r\nTrue\r\n>>> result.status # previous seemed to work\r\n'SUCCESS'\r\n>>> print(result.get()) # SUCCESS was also reported\r\n'This is a string'\r\n>>> result.ready()\r\nTrue\r\n>>> result.status\r\n'SUCCESS'\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4084/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "philpep": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4076", "title": "Unexpected side effect of task.freeze()", "body": "Celery version:\r\n\r\n```\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:2.7.9\r\n            billiard:3.5.0.2 redis:2.10.5\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis:///\r\n\r\nresult_backend: u'redis:///'\r\nbroker_url: u'redis://localhost:6379//'\r\n```\r\n\r\nConsider this tasks in `app.py`\r\n\r\n```python\r\nimport celery\r\n\r\napp = celery.Celery('app', broker='redis://')\r\napp.conf.result_backend = 'redis://'\r\n\r\n@app.task\r\ndef identity(i)\r\n    return i\r\n\r\n@app.task\r\ndef tsum(args):\r\n    return sum(args)\r\n\r\n```\r\n\r\nThen run a worker with `PYTHONPATH=. celery worker -A app` and open a python shell:\r\n\r\n```\r\n>>> from app import identity, tsum\r\n>>> from celery import chord\r\n>>> task = chord([identity.s(i) for i in range(10)], tsum.s()) \r\n>>> task.delay().get()\r\n45\r\n```\r\nOk fine, this was expected, but let's freeze the task before sending it (We need it to introspect and collect task ids before sending them to the broker).\r\n\r\n```\r\n>>> task = chord([identity.s(i) for i in range(10)], tsum.s())\r\n>>> task.freeze()\r\n>>> task.delay().get()\r\n21\r\n```\r\n\r\nThis is unexpected, only some 'identity' tasks are received by 'tsum', also subsequent calls to `task.delay()` result in different values each time....\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4076/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mjtamlyn": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4071", "title": "add_to_chord fails when in \"eager\" mode", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n```\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:2.7.10\r\n            billiard:3.5.0.2 redis:2.10.5\r\nplatform -> system:Darwin arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis://localhost/\r\n\r\nbroker_url: u'redis://localhost:6379//'\r\nresult_backend: u'redis://localhost/'\r\n```\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n\r\nWith `app.conf.task_always_eager = True`, try any `self.add_to_chord(signature)` in a task\r\n\r\n## Expected behavior\r\n\r\nTask is added to the chord and executed later\r\n\r\n## Actual behavior\r\n\r\n`ValueError: Current task is not member of any chord`\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4071/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vitenbergd": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4063", "title": "Celery eventlet CalledProcessError exception issue", "body": "**Celery/deps version:**\r\n\r\namqp==2.1.4\r\nappdirs==1.4.3\r\nBabel==2.4.0\r\nbilliard==3.5.0.2\r\ncelery==4.0.2\r\nenum-compat==0.0.2\r\neventlet==0.21.0\r\nflower==0.9.1\r\ngreenlet==0.4.12\r\nkombu==4.0.2\r\npackaging==16.8\r\npyparsing==2.2.0\r\npytz==2017.2\r\nredis==2.10.5\r\nsix==1.10.0\r\ntornado==4.2\r\nvine==1.1.3\r\n\r\npython version is 3.6\r\n\r\n**Problem description:**\r\n\r\nIf i run celery with **'-P eventlet'** option i cannot catch **CalledProcessError** raised by **subprocess.check_output** inside the task \r\nfunction. I only can catch general Exception class exception instead.\r\n\r\nPS. maybe it's somehow related with exceptions serialization?\r\nPSS. works with **'-P gevent'** \r\n\r\n**Task code:**\r\nhttps://gist.github.com/vitenbergd/8fc8d3bb87219c871b854def667929c4#file-task-py\r\n\r\n## Steps to reproduce\r\n**1st case:**\r\n\r\nf i run:\r\n\r\n```\r\ncelery  -A task worker --loglevel=info\r\n\r\n>>> from task import test\r\n>>> test.delay()\r\n<AsyncResult: eca78f41-8242-465d-b996-dc8b39144f3b>\r\n\r\n```\r\nthe output is:\r\n\r\n```\r\n\r\n[2017-06-01 09:23:37,379: INFO/MainProcess] Connected to redis://localhost:6379/0\r\n[2017-06-01 09:23:37,385: INFO/MainProcess] mingle: searching for neighbors\r\n[2017-06-01 09:23:38,400: INFO/MainProcess] mingle: all alone\r\n[2017-06-01 09:23:38,405: INFO/MainProcess] celery@localhost ready.\r\n[2017-06-01 09:24:33,734: INFO/MainProcess] Received task: task.test[eca78f41-8242-465d-b996-dc8b39144f3b]  \r\n[2017-06-01 09:24:33,738: WARNING/PoolWorker-4] caller process exception\r\n[2017-06-01 09:24:33,738: WARNING/PoolWorker-4] <class 'subprocess.CalledProcessError'>\r\n[2017-06-01 09:24:33,738: INFO/PoolWorker-4] Task task.test[eca78f41-8242-465d-b996-dc8b39144f3b] succeeded in 0.0037605260004056618s: None\r\n\r\n```\r\n**2nd case**\r\n\r\n\r\nif i run:\r\n```\r\n\r\n celery -P eventlet -A task worker --loglevel=info\r\n\r\n>>> from task import test\r\n>>> test.delay()\r\n<AsyncResult: 871fc6b7-1a2a-4b02-94a4-c0f3fe3f5eac>\r\n\r\n```\r\n\r\nthe output is:\r\n\r\n```\r\n[2017-06-01 09:25:11,538: INFO/MainProcess] Connected to redis://localhost:6379/0\r\n[2017-06-01 09:25:11,543: INFO/MainProcess] mingle: searching for neighbors\r\n[2017-06-01 09:25:12,556: INFO/MainProcess] mingle: all alone\r\n[2017-06-01 09:25:12,560: INFO/MainProcess] celery@localhost ready.\r\n[2017-06-01 09:25:12,566: INFO/MainProcess] pidbox: Connected to redis://localhost:6379/0.\r\n[2017-06-01 09:25:15,325: INFO/MainProcess] Received task: task.test[871fc6b7-1a2a-4b02-94a4-c0f3fe3f5eac]  \r\n[2017-06-01 09:25:15,328: WARNING/MainProcess] general exception\r\n[2017-06-01 09:25:15,328: WARNING/MainProcess] <class 'subprocess.CalledProcessError'>\r\n[2017-06-01 09:25:15,329: INFO/MainProcess] Task task.test[871fc6b7-1a2a-4b02-94a4-c0f3fe3f5eac] succeeded in 0.0034008210022875573s: None\r\n\r\n```\r\n**3d case: if raise CalledProcessError manually**\r\n```\r\n\r\nfrom celery import Celery\r\nfrom celery import Task\r\n\r\nimport subprocess\r\nfrom subprocess import CalledProcessError\r\n\r\napp = Celery('task', broker='redis://localhost:6379/0')\r\n\r\n@app.task()                                                                                                          \r\ndef test():\r\n    try:\r\n        raise CalledProcessError(1, 'test')\r\n    except CalledProcessError as cpe:\r\n        print('caller process exception')\r\n        print(type(cpe))\r\n        print(cpe)\r\n    except Exception as e:\r\n        print('general exception')\r\n        print(type(e))\r\n        print(e)\r\n```\r\n\r\nif i run:\r\n\r\n```\r\n\r\ncelery -P eventlet -A task worker --loglevel=info\r\n\r\n>>> from task import test\r\n>>> test.delay()\r\n<AsyncResult: 4ef2d70e-e6f5-4fa0-87e6-8a996be84bde>\r\n```\r\nEverything works as i expected:\r\n\r\n```\r\n[2017-06-02 11:15:07,852: INFO/MainProcess] Connected to redis://localhost:6379/0\r\n[2017-06-02 11:15:07,858: INFO/MainProcess] mingle: searching for neighbors\r\n[2017-06-02 11:15:08,873: INFO/MainProcess] mingle: all alone\r\n[2017-06-02 11:15:08,883: INFO/MainProcess] celery@localhost ready.\r\n[2017-06-02 11:15:08,884: INFO/MainProcess] pidbox: Connected to redis://localhost:6379/0.\r\n[2017-06-02 11:15:11,874: INFO/MainProcess] Received task: task.test[4ef2d70e-e6f5-4fa0-87e6-8a996be84bde]  \r\n[2017-06-02 11:15:11,875: WARNING/MainProcess] caller process exception\r\n[2017-06-02 11:15:11,875: WARNING/MainProcess] <class 'subprocess.CalledProcessError'>\r\n[2017-06-02 11:15:11,875: WARNING/MainProcess] Command 'test' returned non-zero exit status 1.\r\n[2017-06-02 11:15:11,875: INFO/MainProcess] Task task.test[4ef2d70e-e6f5-4fa0-87e6-8a996be84bde] succeeded in 0.000474446002044715s: None\r\n\r\n```\r\n\r\n## Expected behavior\r\nsubprocess.CalledProcessError - caugth in both cases\r\n## Actual behavior\r\nsubprocess.CalledProcessError - caugth  in 1st case\r\nsubprocess.CalledProcessError - no exception trails in 2nd case\r\nException class - caugth in 2nd case, but the printed class is <class 'subprocess.CalledProcessError'>", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4063/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mittagessen": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4060", "title": "celery.accumulate inconsistent behavior", "body": "My code replaces a head task with a group containing a variable number of subtasks. This works fine but unfortunately ``celery.accumulate`` does not accumulate the output of a group containing a single task. Sample code:\r\n\r\n```\r\nfrom celery import Celery\r\nfrom celery import signature, group, chain\r\n\r\napp = Celery('tasks', broker='redis://', backend='redis://')\r\n\r\n@app.task\r\ndef task_1():\r\n    return [1, 2]\r\n\r\n@app.task\r\ndef task_2():\r\n    return [3, 4]\r\n\r\n@app.task(bind=True)\r\ndef repl_1(self):\r\n    raise self.replace(group([task_1.si()]))\r\n\r\n@app.task(bind=True)\r\ndef repl_2(self):\r\n    raise self.replace(group([task_1.si(), task_2.si()]))\r\n```\r\n\r\nOutput:\r\n\r\n```\r\n>>> test.repl_1.apply_async().get()\r\n[1, 2]\r\n>>> res = test.repl_2.apply_async().get()\r\n[[1, 2], [3, 4]]\r\n```\r\n\r\nThis is somewhat annoying as 1. it is inconsistent and 2. I'm chaining the repl_* tasks requiring me to somehow find out the number of tasks run in a previous step of the chain to know what input to expect (some tasks return a nested list and there is no way to find out if it is the result of ```celery.accumulate``` or a legitimate task.\r\n\r\nExpected output is:\r\n\r\n```\r\n>>> test.repl_1.apply_async().get()\r\n[[1, 2]]\r\n>>> res = test.repl_2.apply_async().get()\r\n[[1, 2], [3, 4]]\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "joker-ace": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4042", "title": "Celery: worker receives first task after `broker_heartbeat` seconds", "body": "Sorry, this issue is not formatted according to your requirements, It can't be. Here is my issue:\r\n\r\n   Environment:\r\n     - Vagrant (1.9.1) Ubuntu 14.04:\r\n         - Docker (version 1.12.5):\r\n             - RabbitMQ v3.6.5\r\n             - Client (Celery app v4.0.2, Python 2.7)\r\n             - Server (Celery app v4.0.2, Python 3.5)\r\n\r\nUse case: functional tests.\r\n\r\n\r\n    Workflow #1:\r\n     1. py.test (TestApp) starts in docker RabbitMQ and Server application.\r\n     2. TestApp registers \"new\" Client by sending task to Server.\r\n     3. TestApp starts in docker Client application.\r\n     4. Client and Server do a \"handshake\" by own protocol.\r\n     5. TestApp sends task to Client for test purposes (TestTask).\r\n     6. Client receives task immediately and executes it.\r\n\r\n\r\n    Workflow #2:\r\n     1. py.test (TestApp) starts in docker RabbitMQ and Server application.\r\n     2. TestApp registers \"active\" Client by sending task to Server.\r\n     3. TestApp starts in docker Client application.\r\n     4. TestApp sends task to Client for test purposes (TestTask).\r\n     6. Client receives task after 60 seconds (THIS IS A PROBLEM).\r\n\r\n\r\n    Workflow #2:\r\n     1. py.test (TestApp) starts in docker RabbitMQ and Server application.\r\n     2. TestApp registers \"active\" Client by sending task to Server.\r\n     3. TestApp starts in docker Client application.\r\n     4. TestApp sleeps for 20 seconds.\r\n     5. TestApp sends task to Client for test purposes (TestTask).\r\n     6. Client receives task immediately and executes it.\r\n\r\n\r\nWhat I've researched:\r\n\r\nWhen Celery app establishing connection with RabbitMQ, they negotiate a Heartbeat Timeout Interval,\r\nwhich is 60 seconds and this value set in Celery configuration for Client.\r\n\r\nDebugging \"internals\" of Celery in Workflow #2, I've found, that \"epoll\" returns RabbitMQ connection socket in \"ready for read\" state\r\nonly after RabbiMQ will send his Heartbeat to this connection (60 seconds for my configuration).\r\n\r\nFrom Workflows #1 & #3 we can see, that if there is a little delay between after Client started and TestTask sent\r\n(handshake and synthetic delay), everything is working perfectly.\r\n\r\nI have no ideas about this behavior. I need tasks be executed as soon as they will be sent/retrieved, not after this big delay.\r\n\r\nI can fix tests by decreasing Heartbeat Timeout Interval, but this is not an option for production.\r\n\r\nWhat can you suggest?", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4042/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/3856", "title": "Signal \"before_task_publish\" received wrong dictionary of message properties", "body": "## Checklist\r\n\r\n- Celery version: 4.0.2\r\n- I have verified that the issue exists against the `master` branch of Celery.\r\n\r\nDeclare signal like this\r\n\r\n```\r\n@signals.before_task_publish.connect\r\ndef _do_something_with_properties(properties=None, **kwds):\r\n    properties['my_key'] = 'my_value'\r\n```\r\n\r\nand you will never get this this 'my_key' published in task properties.\r\nTake a look:\r\n\r\nCelery 3:\r\n\r\nhttps://github.com/celery/celery/blob/3.1/celery/app/amqp.py#L296\r\nhttps://github.com/celery/celery/blob/3.1/celery/app/amqp.py#L310\r\n\r\nCelery 4:\r\n\r\nhttps://github.com/celery/celery/blob/4.0/celery/app/amqp.py#L547\r\nhttps://github.com/celery/celery/blob/4.0/celery/app/amqp.py#L558\r\n\r\nAs you can see, `properties` was updated before signal code invoked, signal code receives `kwargs` value as `properties` param, where I need to add some keys to `properties`, but  task is publishing with old `properties` dict from line 547.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3856/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hp685": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4039", "title": "Operation timed out when using redis backend ", "body": "I'm using Celery version 4.0.2\r\nWhen using Redis as the backend, I've been observing TimeoutError exception waiting on a .get() of a celery task. \r\nThe issue is intermittent. Roughly, 1/30 cases exhibit the issue\r\nFrom the logs, I observe that the task has finished successfully and therefore, I expect to see the result stored in the backend and available for the client. However, the client never receives that result, and instead, raises a TimeoutError. \r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/result.py\", line 189, in get\r\n    on_message=on_message,\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/async.py\", line 189, in wait_for_pending\r\n    for _ in self._wait_for_pending(result, **kwargs):\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/async.py\", line 256, in _wait_for_pending\r\n    on_interval=on_interval):\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/async.py\", line 57, in drain_events_until\r\n    yield self.wait_for(p, wait, timeout=1)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/async.py\", line 66, in wait_for\r\n    wait(timeout=timeout)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery-4.0.2-py2.7.egg/celery/backends/redis.py\", line 69, in drain_events\r\n    m = self._pubsub.get_message(timeout=timeout)\r\n  File \"build/bdist.linux-x86_64/egg/redis/client.py\", line 2260, in get_message\r\n    response = self.parse_response(block=False, timeout=timeout)\r\n  File \"build/bdist.linux-x86_64/egg/redis/client.py\", line 2183, in parse_response\r\n    return self._execute(connection, connection.read_response)\r\n  File \"build/bdist.linux-x86_64/egg/redis/client.py\", line 2165, in _execute\r\n    return command(*args)\r\n  File \"build/bdist.linux-x86_64/egg/redis/connection.py\", line 577, in read_response\r\n    response = self._parser.read_response()\r\n  File \"build/bdist.linux-x86_64/egg/redis/connection.py\", line 238, in read_response\r\n    response = self._buffer.readline()\r\n  File \"build/bdist.linux-x86_64/egg/redis/connection.py\", line 168, in readline\r\n    self._read_from_socket()\r\n  File \"build/bdist.linux-x86_64/egg/redis/connection.py\", line 139, in _read_from_socket\r\n    raise TimeoutError(\"Timeout reading from socket\")\r\nTimeoutError: Timeout reading from socket\r\n```\r\n\r\n ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4039/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cadupont": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4032", "title": "result_backend cannot be configured in pytest ", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n----------------------------------------------\r\ncelery -A firehose.tasks.orchestration report\r\n\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:3.5.2\r\n            billiard:3.5.0.2 py-amqp:2.1.4\r\nplatform -> system:Darwin arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:pyamqp results:redis://localhost:6379/0\r\n\r\nresult_backend: 'redis://localhost:6379/0'\r\nbroker_url: 'amqp://guest:********@localhost:5672//'\r\n----------------------------------------------\r\n\r\n\r\n## Steps to reproduce\r\nCreate one celery task in a separate file, import the celery task and try to get its result from within a pytest test. Try specifying the result backend via marks or fixtures.\r\ne.g.\r\n```\r\nfrom ... import fetch\r\nimport pytest\r\n\r\n@pytest.mark.celery(result_backend='redis://localhost:6379/0')\r\ndef test_successful_orchestration(celery_worker): \r\n    f = fetch.delay()\r\n    assert f.get() == 'result'\r\n```\r\nI also have the same result if I try with the config fixture\r\n```\r\n@pytest.fixture(scope='session')\r\ndef celery_config():\r\n    return {\r\n        \"result_backend\": \"redis://localhost/0\",\r\n        \"broker_url\": \"amqp://guest:guest@localhost:5672//\"\r\n    }\r\n```\r\n\r\n\r\n## Expected behavior\r\nI would expect the result_backend to be available to the test.\r\nNote that when I launch a worker separately, when I am not testing, with the same backend settings, it works.\r\n\r\n\r\n## Actual behavior\r\nRunning pytest mytest.py returns:\r\n\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n../../.pyenv/versions/3.5.2/envs/celery3.5.2/lib/python3.5/site-packages/celery/result.py:194: in get\r\n    on_message=on_message,\r\n../../.pyenv/versions/3.5.2/envs/celery3.5.2/lib/python3.5/site-packages/celery/backends/base.py:466: in wait_for_pending\r\n    no_ack=no_ack,\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nself = <celery.backends.base.DisabledBackend object at 0x1103d5518>, args = ('985a0f2e-ff3a-4ada-a045-11137f358694',), kwargs = {'interval': 0.5, 'no_ack': True, 'on_interval': <promise@0x1105f0048>, 'timeout': None}\r\n\r\n    def _is_disabled(self, *args, **kwargs):\r\n>       raise NotImplementedError(E_NO_BACKEND.strip())\r\nE       NotImplementedError: No result backend is configured.\r\nE       Please see the documentation for more information.\r\n\r\n../../.pyenv/versions/3.5.2/envs/celery3.5.2/lib/python3.5/site-packages/celery/backends/base.py:772: NotImplementedError\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4032/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stefanmh": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4030", "title": "Celery busy-waits on the result under gevent/eventlet", "body": "## Steps to reproduce\r\n\r\nPoC:\r\n\r\n```python\r\nimport gevent.monkey\r\ngevent.monkey.patch_all()\r\n\r\nimport traceback\r\nimport greenlet\r\nimport time\r\nimport sys\r\nimport os\r\n\r\nfrom celery import Celery\r\n\r\napp = Celery('busylooping', broker='redis://', backend='redis://')\r\n\r\nnum_switches = 0\r\n\r\ndef trace_tracker(event, args):\r\n\tglobal num_switches\r\n\tif event in ('switch', 'throw'):\r\n\t\t# Uncomment to get the busy waiting traceback\r\n\t\t# origin, target = args\r\n\t\t# print 'Switching from\\n%s\\n\\n\\nto\\n\\n\\n%s' % (\r\n\t\t# \t''.join(traceback.format_list(traceback.extract_stack(origin.gr_frame))),\r\n\t\t# \t''.join(traceback.format_list(traceback.extract_stack(target.gr_frame)))\r\n\t\t# )\r\n\t\tnum_switches += 1\r\n\r\n\r\n@app.task\r\ndef add(x, y):\r\n\ttime.sleep(3.0)\r\n\treturn x + y\r\n\r\ndef main1():\r\n\tapp.worker_main()\r\n\r\ndef main2():\r\n\tgreenlet.settrace(trace_tracker)\r\n\tadd.s(3, 3).apply_async().get()\r\n\tprint 'context switches:', num_switches\r\n\r\ndef main():\r\n\tif os.getenv('X') == '1':\r\n\t\tmain1()\r\n\telse:\r\n\t\tmain2()\r\n\r\nif __name__ == '__main__':\r\n\tmain()\r\n\r\n\r\n```\r\n\r\nIn the first terminal, run `X=1 python celery_busylooping.py`.\r\nIn the second terminal, run `python celery_busylooping.py`\r\n\r\n## Expected behavior\r\n\r\nWait on an event instead of spinning.\r\n\r\n## Actual behavior\r\n\r\n```\r\n\u279c  ~/tmp python celery_busylooping.py\r\ncontext switches: 1148048\r\n```\r\n\r\n## Cause\r\n\r\nSee https://github.com/celery/celery/blob/master/celery/backends/async.py#L97\r\n\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"celery_busylooping.py\", line 47, in <module>\r\n    main()\r\n  File \"celery_busylooping.py\", line 44, in main\r\n    main2()\r\n  File \"celery_busylooping.py\", line 37, in main2\r\n    print add.s(3, 3).apply_async().get()\r\n  File \"***/local/lib/python2.7/site-packages/celery/result.py\", line 189, in get\r\n    on_message=on_message,\r\n  File \"***/local/lib/python2.7/site-packages/celery/backends/async.py\", line 189, in wait_for_pending\r\n    for _ in self._wait_for_pending(result, **kwargs):\r\n  File \"***/local/lib/python2.7/site-packages/celery/backends/async.py\", line 258, in _wait_for_pending\r\n    sleep(0)\r\n  File \"/usr/lib/python2.7/dist-packages/gevent/hub.py\", line 192, in sleep\r\n    waiter.get()\r\n  File \"/usr/lib/python2.7/dist-packages/gevent/hub.py\", line 875, in get\r\n    return self.hub.switch()\r\n  File \"/usr/lib/python2.7/dist-packages/gevent/hub.py\", line 606, in switch\r\n    return greenlet.switch(self)\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4030/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "neutralino1": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4028", "title": "Clarify support for task priority with redis backend", "body": "The official documentation is not clear as to how to get task priorities to work with a redis backend.\r\n\r\nI have tried the following and it does not work:\r\n```python\r\n@app.task(queue = 'priority_test', priority = 9)\r\ndef high_pri():\r\n  print(\"HIGH\")\r\n\r\n@app.task(queue = 'priority_test', priority = 0)\r\ndef low_pri():\r\n  sleep(1)\r\n  print(\"LOW\")\r\n```\r\n\r\nI started a worker with single concurrency, queued 100 low priority tasks, and then one high priority task and it got run last. I tried inverting the priorities, same result.\r\n\r\nSearching through the code for `priority` doesn't show anything obvious.\r\n\r\nAre tasks priorities supported with a redis back-end, and if so, what is the right way to set it up?\r\n\r\nThank you", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4028/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "klahnakoski": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4026", "title": "Add `add_context_manager()` to workers", "body": "Redash appears to do something that is neccesary, but confusing:\r\n\r\n    class ContextTask(TaskBase):\r\n        abstract = True\r\n    \r\n        def __call__(self, *args, **kwargs):\r\n            with current_app.app_context():\r\n                return TaskBase.__call__(self, *args, **kwargs)\r\n\r\n    celery.Task = ContextTask\r\n\r\nhttps://github.com/getredash/redash/blob/e6fc73f4440a69351e56e6db4b9c1f07f3970c6e/redash/worker.py#L63\r\n\r\nThis is confusing because the parameter passing is, at best unclear, and at worst hours to debug.  For example, which `self` is being used?  How to you pass bound objects through this call pattern?  Good luck passing `kwargs` with a `self` parameter!\r\n\r\nPlease let Celery accept a context manager (a class with `__enter__` and __exit__` methods) to all workers.  This will allow Redash, and maybe other Celery users, to \r\n\r\n    class ContextTask(object):\r\n    \r\n        def __int__(self):\r\n            self.ctx=None\r\n    \r\n        def __enter__(self):\r\n            self.ctx = current_app.app_context()\r\n            self.ctx.__enter__()\r\n    \r\n        def __exit__(self, exc_type, exc_val, exc_tb):\r\n            self.ctx.__exit()\r\n\r\n    celery.Task.add_context_manager(ContextTask)\r\n\r\nThe clear interface may also help Redash be less dependent on Celery internal method call patterns.\r\n\r\nMy apologies if Celery has this already\r\n\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4026/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/4014", "title": "__main__.py has relative import", "body": "In the `__main__.py` file\r\n\r\nhttps://github.com/celery/celery/blob/master/celery/__main__.py\r\n\r\nThe `from __future__ import absolute_import` line and the `from . import maybe_patch_concurrency` line conflict\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4014/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "HaddyYang": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4020", "title": "The celery document has a mistake about crontab", "body": "## about crontab\r\nthe document in this link about crontab simple.\r\nhttp://docs.celeryproject.org/en/master/userguide/periodic-tasks.html\r\n\r\n**crontab(0, 0, day_of_month='2-30/3')**\r\nExecute on every even numbered day.\r\n\r\nshuld be \r\n**crontab(0, 0, day_of_month='2-30/2')**", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4020/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alternativshik": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4019", "title": "Termination tasks with eventlet/gevent worker pool.", "body": "This is a feature request about ability to terminate task execution when using eventlet or gevent TaskPool.\r\n\r\nNow this feature presents only on Prefork TaskPool, but, I think, this is a useful feature for other Pools. ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4019/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "InvalidInterrupt": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4010", "title": "Configuration docs: redis_socket_connect_timeout marked as added in 5.0.1", "body": "See here: http://docs.celeryproject.org/en/latest/userguide/configuration.html#redis-socket-connect-timeout\r\n\r\nIt appears to me this was actually added in 4.0.1.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4010/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Korijn": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4008", "title": "Eager mode hides serialization side-effects", "body": "Running a task in eager mode doesn't pass the arguments through serialization, and therefore hides some of its side-effects. For example, calling a celery task in eager mode with UUID arguments, will leave them as UUIDs, whereas passing them through serialization (with eager mode switched off) converts them to strings. This hid a bunch of bugs in our application code.\r\n\r\nDo you agree it would be more reasonable to pass the arguments through the serialization backends, even in eager mode, to have a more consistently behaving API?", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4008/reactions", "total_count": 12, "+1": 12, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yoichi": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4004", "title": "exception in chain upstream doesn't signal downstream GroupResult", "body": "Suppose that we have a chain with the last element is a group:\r\n\r\n```\r\nchain(task_a.si(), group(task_b.si(), task_c.si(),...))\r\n```\r\n\r\nWhen exception occur in task_a, chain(...).delay().get() hangs while the error is detected in the worker side.\r\n\r\n## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n\r\n```\r\nsoftware -> celery:3.1.25 (Cipater) kombu:3.0.37 py:3.5.2\r\n            billiard:3.3.0.23 redis:2.10.5\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis://localhost:6379/\r\n\r\nCELERY_RESULT_BACKEND: 'redis://localhost:6379/'\r\nBROKER_URL: 'redis://localhost:6379//'\r\n```\r\n\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n```\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:3.5.2\r\n            billiard:3.5.0.2 redis:2.10.5\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis://localhost:6379/\r\n\r\nbroker_url: 'redis://localhost:6379//'\r\nresult_backend: 'redis://localhost:6379/'\r\n```\r\n( e812c5780b4006516116f059ab498e1f043bdd50 in master)\r\n\r\n## Steps to reproduce\r\n\r\nrun worker by \"celery -A tasks worker -i info\" with following code (tasks.py)\r\n\r\n```\r\nfrom celery import Celery\r\n\r\napp = Celery('tasks', backend='redis://localhost:6379', broker='redis://localhost:6379')\r\n\r\n@app.task\r\ndef work():\r\n    pass\r\n\r\n@app.task\r\ndef nop():\r\n    pass\r\n\r\n@app.task\r\ndef error():\r\n    raise Exception(\"error!!!\")\r\n```\r\n\r\nrun chain by following code (tasks_run.py)\r\n\r\n```\r\nimport celery\r\nimport tasks\r\n\r\nc = celery.chain(tasks.error.si(), \r\n                 celery.group(tasks.work.si(), tasks.work.si()))\r\nr = c.delay()\r\nr.get()\r\n```\r\n\r\n## Expected behavior\r\n\r\nWhen the worker detects the error as:\r\n```\r\n[2017-05-01 15:17:06,725: INFO/MainProcess] Received task: tasks.error[b74e2759-4caf-41c1-9694-37b8a58ce6c6]\r\n[2017-05-01 15:17:06,727: ERROR/MainProcess] Task tasks.error[b74e2759-4caf-41c1-9694-37b8a58ce6c6] raised unexpected: Exception('error!!!',)\r\nTraceback (most recent call last):\r\n  File \"/home/developer/python3-dev/lib/python3.5/site-packages/celery/app/trace.py\", line 240, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n  File \"/home/developer/python3-dev/lib/python3.5/site-packages/celery/app/trace.py\", line 438, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n  File \"/home/developer/TECH-51/tasks.py\", line 15, in error\r\n    raise Exception(\"error!!!\")\r\nException: error!!!\r\n```\r\n\r\n\"r.get()\" reraises the Exception on the client side.\r\n\r\n## Actual behavior\r\n\r\n\"r.get()\" hangs. Ctrl+c shows following stacktraces:\r\n\r\nwith Celery v3.1.25\r\n\r\n```\r\n$ python tasks_run.py \r\n^CTraceback (most recent call last):\r\n  File \"tasks_run.py\", line 7, in <module>\r\n    r.get()\r\n  File \"/home/developer/python3-dev/lib/python3.5/site-packages/celery/result.py\", line 578, in get\r\n    interval=interval, callback=callback, no_ack=no_ack)\r\n  File \"/home/developer/python3-dev/lib/python3.5/site-packages/celery/result.py\", line 688, in join_native\r\n    for task_id, meta in self.iter_native(timeout, interval, no_ack):\r\n  File \"/home/developer/python3-dev/lib/python3.5/site-packages/celery/backends/base.py\", line 495, in get_many\r\n    time.sleep(interval)  # don't busy loop.\r\nKeyboardInterrupt\r\n```\r\n\r\nwith Celery (master)\r\n\r\n```\r\n$ python tasks_run.py \r\n^CTraceback (most recent call last):\r\n  File \"tasks_run.py\", line 7, in <module>\r\n    r.get()\r\n  File \"/home/developer/python3-dev-celery-master/lib/python3.5/site-packages/celery/result.py\", line 635, in get\r\n    on_message=on_message,\r\n  File \"/home/developer/python3-dev-celery-master/lib/python3.5/site-packages/celery/result.py\", line 746, in join_native\r\n    on_message, on_interval):\r\n  File \"/home/developer/python3-dev-celery-master/lib/python3.5/site-packages/celery/backends/async.py\", line 143, in iter_native\r\n    for _ in self._wait_for_pending(result, no_ack=no_ack, **kwargs):\r\n  File \"/home/developer/python3-dev-celery-master/lib/python3.5/site-packages/celery/backends/async.py\", line 256, in _wait_for_pending\r\n    on_interval=on_interval):\r\n  File \"/home/developer/python3-dev-celery-master/lib/python3.5/site-packages/celery/backends/async.py\", line 57, in drain_events_until\r\n    yield self.wait_for(p, wait, timeout=1)\r\n  File \"/home/developer/python3-dev-celery-master/lib/python3.5/site-packages/celery/backends/async.py\", line 66, in wait_for\r\n    wait(timeout=timeout)\r\n  File \"/home/developer/python3-dev-celery-master/lib/python3.5/site-packages/celery/backends/redis.py\", line 69, in drain_events\r\n    m = self._pubsub.get_message(timeout=timeout)\r\n  File \"/home/developer/python3-dev-celery-master/lib/python3.5/site-packages/redis/client.py\", line 2260, in get_message\r\n    response = self.parse_response(block=False, timeout=timeout)\r\n  File \"/home/developer/python3-dev-celery-master/lib/python3.5/site-packages/redis/client.py\", line 2181, in parse_response\r\n    if not block and not connection.can_read(timeout=timeout):\r\n  File \"/home/developer/python3-dev-celery-master/lib/python3.5/site-packages/redis/connection.py\", line 572, in can_read\r\n    bool(select([sock], [], [], timeout)[0])\r\nKeyboardInterrupt\r\n```\r\n\r\n## Workaround?\r\n\r\nWith celery v3.1.25, put single task at the end of the chain seems to resolve the issue (exception will be reraised).\r\n\r\n```\r\nimport celery\r\nimport tasks\r\n\r\nc = celery.chain(tasks.error.si(),\r\n                 celery.group(tasks.work.si(), tasks.work.si()),\r\n                 tasks.nop.si())\r\nr = c.delay()\r\nr.get()\r\n```\r\n\r\nOn the other hand, with celery v4.0.2, the modified code sometime reraise the exception, and sometime hangs. An interesting thing is that if we insert time.sleep(0.5) just before r.get() in addition to add the single task at the end of the chain resolve the problem in v4.0.2.\r\n\r\nSince we're going to use celery 3.1.25 with our code, I want to know if the workaround in v3.1.25 is correct.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4004/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/fd8b52e3c8ab25652f56198b52bdcb60d7de65b2", "message": "Make id, name always accessible from logging.Formatter via extra (#3994)\n\n* Make id, name always accessible from Formatter via extra\r\n\r\n* define info with docstring\r\n\r\n* use the same name to minimize the change\r\n* clarify the reason for passing 'extra'\r\n\r\n* Specific version for pydocstyle\r\n\r\n* Add myself to the contributors list"}, {"url": "https://api.github.com/repos/celery/celery/commits/8eecc886839469e2c824299993a515f7c5576a29", "message": "recover loglevel for ExecStart in systemd config (#4023)\n\n* it was lost in d54eb6d\r\n* \"systemctl start celery\" doesn't respect CELERYD_LOG_LEVEL as\r\n  \"systemctl restart celery\" does\r\n* give options in the same order as ExecReload for symmetry"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/114062421", "body": "Thank you for the suggestion. I've added a docstring including the reason for adding extra.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/114062421/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "pgeez": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4002", "title": "Worker shutdown creates duplicate messages in SQS broker", "body": "## Checklist\r\n\r\nI'm `pip` installing Celery from `master`.\r\n\r\n## Steps to reproduce\r\n\r\nSubmit a task to an SQS broker and verify that there are 1 messages available and 0 messages \"in flight\" (total messages: 1). Launch a worker connected to that broker and now verify that there are 0 messages available and 1 messages \"in flight\" (total messages: 1). Shutdown the worker before the task completes (via Ctrl + C). The task will then be resubmitted with the following message:\r\n\r\n```Restoring 1 unacknowledged message(s).```\r\n\r\nVerify that the total number of messages in SQS is now 2 (1 available and 1 \"in flight\").\r\n\r\nRepeat.\r\n\r\n## Expected behavior\r\n\r\nI expect the total number of messages in the SQS broker to remain constant, as is the behavior I observe when I perform the repro with a Redis broker.\r\n\r\n## Actual behavior\r\n\r\nMessages are duplicated, which can cause the queue to grow in an unbounded way.\r\n\r\n\r\nAdding to this, I have long-running tasks (~45 min) and notice that I quite frequently see this error message \r\n\r\n`[INFO] botocore.vendored.requests.packages.urllib3.connectionpool: Resetting dropped connection: sqs.us-west-2.amazonaws.com`\r\n\r\nin conjunction with `Restoring N unacknowledged messages.`\r\n\r\nIt seems that dying SQS connections are causing workers to restart, which is in turn causing my queue to grow. (FYI: overnight, it grew from ~10 jobs to 13,000 because of this behavior when running a cluster of ~20 machines)", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "denizs": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3990", "title": "Workers dis- and reconnect to SQS after SQSError 599 and don't take tasks anymore", "body": "## Checklist\r\n\r\n- [x ] I have included the output of ``celery -A proj report`` in the issue.\r\n\r\n- [x ] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nI am running a celery task queue on AWS using SQS as my transport and several amazon ec2 instances as my workers. \r\n\r\n## Expected behavior\r\nThe workers stay connected to the server\r\n\r\n## Actual behavior\r\nAfter a couple of hours, the workers somehow lose connection to the message broker, initiated by a promise rejection due to an empty reply from the server, which throws a `SQSError 599` exception.  I have investigated this topic and solely found one related [issue](https://github.com/celery/celery/issues/3701) where this could be solved by reinstalling pycurl with openssl rather than gnutls. I have tried this solution, which unfortunately does not work for me. \r\nI have included both, the worker log and the `celery report` in this issue. \r\n\r\n### Celery config\r\n```python\r\nCELERY_BROKER_URL = 'sqs://{0}:{1}@'.format(\r\n    urllib.quote(AWS_SQS_ACCESS_KEY, safe=''),\r\n    urllib.quote(AWS_SQS_SECRET_ACCESS_KEY, safe='')\r\n)\r\n\r\nCELERY_RESULT_BACKEND ='django-db'\r\nCELERY_ACCEPT_CONTENT = ['json']\r\nCELERY_TASK_SERIALIZER = 'json'\r\nCELERY_RESULT_SERIALIZER = 'json'\r\nCELERY_TIMEZONE = 'Europe/Berlin'\r\nCELERY_TRACK_STARTED=True\r\nCELERY_WORKER_PREFETCH_MULTIPLIER = 0\r\nCELERY_BROKER_TRANSPORT_OPTIONS['queue_name_prefix'] = 'berries-prod-'\r\nCELERY_SEND_TASK_ERROR_EMAILS = True\r\n\r\n\r\nCELERY_BROKER_TRANSPORT_OPTIONS = {\r\n    'region': 'eu-central-1',\r\n    'polling_interval': 30,\r\n    'wait_time_seconds': 20,\r\n    'visibility_timeout': 3600,\r\n}\r\n```\r\n### Worker logs\r\n```shell\r\n2017-04-23 10:12:48,431: INFO/MainProcess] Connected to sqs://AKIAIPAKNCSMWZUH7OOA:**@localhost//\r\n[2017-04-23 10:12:48,473: INFO/MainProcess] celery@ip-172-31-17-229 ready.\r\n[2017-04-23 10:29:54,042: ERROR/MainProcess] Empty body: SQSError: 599 Empty reply from server\r\n\r\n[2017-04-23 10:29:54,043: ERROR/MainProcess] Callback <promise@0x7f11afd98808> raised exception: SQSError: 599 Empty reply from server\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/kombu/async/http/curl.py\", line 178, in _process\r\n    buffer=buffer, effective_url=effective_url, error=error,\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/vine/promises.py\", line 146, in __call__\r\n    svpending(*ca, **ck)\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/vine/promises.py\", line 139, in __call__\r\n    return self.throw()\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/vine/promises.py\", line 136, in __call__\r\n    retval = fun(*final_args, **final_kwargs)\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/vine/funtools.py\", line 100, in _transback\r\n    return callback(ret)\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/vine/promises.py\", line 139, in __call__\r\n    return self.throw()\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/vine/promises.py\", line 136, in __call__\r\n    retval = fun(*final_args, **final_kwargs)\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/vine/funtools.py\", line 98, in _transback\r\n    callback.throw()\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/vine/funtools.py\", line 96, in _transback\r\n    ret = filter_(*args + (ret,), **kwargs)\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/kombu/async/aws/connection.py\", line 269, in _on_list_ready\r\n    raise self._for_status(response, body)\r\nSQSError: SQSError: 599 Empty reply from server\r\n\r\n[2017-04-23 10:29:54,046: WARNING/MainProcess] consumer: Connection to broker lost. Trying to re-establish the connection...\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/celery/worker/consumer/consumer.py\", line 318, in start\r\n    blueprint.start(self)\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/celery/worker/consumer/consumer.py\", line 594, in start\r\n    c.loop(*c.loop_args())\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/celery/worker/loops.py\", line 88, in asynloop\r\n    next(loop)\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/kombu/async/hub.py\", line 345, in create_loop\r\n    cb(*cbargs)\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/kombu/async/http/curl.py\", line 111, in on_readable\r\n    return self._on_event(fd, _pycurl.CSELECT_IN)\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/kombu/async/http/curl.py\", line 124, in _on_event\r\n    self._process_pending_requests()\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/kombu/async/http/curl.py\", line 132, in _process_pending_requests\r\n    self._process(curl, errno, reason)\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/kombu/async/http/curl.py\", line 178, in _process\r\n    buffer=buffer, effective_url=effective_url, error=error,\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/vine/promises.py\", line 146, in __call__\r\n    svpending(*ca, **ck)\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/vine/promises.py\", line 139, in __call__\r\n    return self.throw()\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/vine/promises.py\", line 136, in __call__\r\n    retval = fun(*final_args, **final_kwargs)\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/vine/funtools.py\", line 100, in _transback\r\n    return callback(ret)\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/vine/promises.py\", line 139, in __call__\r\n    return self.throw()\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/vine/promises.py\", line 136, in __call__\r\n    retval = fun(*final_args, **final_kwargs)\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/vine/funtools.py\", line 98, in _transback\r\n    callback.throw()\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/vine/funtools.py\", line 96, in _transback\r\n    ret = filter_(*args + (ret,), **kwargs)\r\n  File \"/home/ubuntu/.virtualenvs/berries/local/lib/python2.7/site-packages/kombu/async/aws/connection.py\", line 269, in _on_list_ready\r\n    raise self._for_status(response, body)\r\nSQSError: SQSError: 599 Empty reply from server\r\n\r\n[2017-04-23 10:29:54,083: INFO/MainProcess] Connected to sqs://AKIAIPAKNCSMWZUH7OOA:**@localhost//\r\n```\r\n## Output from `celery -A project report`\r\n```shell\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:2.7.12\r\n            billiard:3.5.0.2 sqs:N/A\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:sqs results:django-db\r\n\r\nSECURE_SSL_REDIRECT: False\r\nSECURE_HSTS_SECONDS: 0\r\nUSE_X_FORWARDED_PORT: False\r\nCSRF_COOKIE_SECURE: False\r\nCELERY_TRACK_STARTED: True\r\nLANGUAGE_CODE: 'de-de'\r\nROOT_URLCONF: 'project.urls'\r\nLOGIN_URL: u'/accounts/login/'\r\nBASE_DIR: '/home/ubuntu/webapps/project/project'\r\nTEST_NON_SERIALIZED_APPS: []\r\nDEFAULT_CHARSET: u'utf-8'\r\nAWS_S3_CUSTOM_DOMAIN: 'dxbsrkzoxdmkb.cloudfront.net'\r\nACCOUNT_EMAIL_REQUIRED: True\r\nCELERY_WORKER_PREFETCH_MULTIPLIER: 0\r\nACCOUNT_EMAIL_SUBJECT_PREFIX: ''\r\nALLOWED_HOSTS: ['*']\r\nMESSAGE_STORAGE: u'django.contrib.messages.storage.fallback.FallbackStorage'\r\nEMAIL_SUBJECT_PREFIX: u'[Django] '\r\nSERVER_EMAIL: u'root@localhost'\r\nCELERY_RESULT_SERIALIZER: 'json'\r\nSTATICFILES_FINDERS: [u'django.contrib.staticfiles.finders.FileSystemFinder',\r\n u'django.contrib.staticfiles.finders.AppDirectoriesFinder']\r\nSESSION_CACHE_ALIAS: u'default'\r\nSESSION_COOKIE_DOMAIN: None\r\nSESSION_COOKIE_NAME: u'sessionid'\r\nAWS_SES_RETURN_PATH: 'it@***.com'\r\nCELERY_BROKER_URL: u'sqs://****:********@localhost//'\r\nAWS_STORAGE_BUCKET_NAME: '***-static'\r\nTIME_INPUT_FORMATS: [u'%H:%M:%S', u'%H:%M:%S.%f', u'%H:%M']\r\nSECURE_REDIRECT_EXEMPT: []\r\nDATABASES: {\r\n    'default': {   'ENGINE': 'django.db.backends.postgresql_psycopg2',\r\n                   'HOST': '**.com',\r\n                   'NAME': '***_prod',\r\n                   'PASSWORD': u'********',\r\n                   'PORT': '****',\r\n                   'USER': '****'}}\r\nEMAIL_SSL_KEYFILE: u'********'\r\nFILE_UPLOAD_DIRECTORY_PERMISSIONS: None\r\nFILE_UPLOAD_PERMISSIONS: None\r\nFILE_UPLOAD_HANDLERS: [u'django.core.files.uploadhandler.MemoryFileUploadHandler',\r\n u'django.core.files.uploadhandler.TemporaryFileUploadHandler']\r\nDEFAULT_CONTENT_TYPE: u'text/html'\r\nSCARFACE_REGION_NAME: 'eu-central-1'\r\nACCOUNT_EMAIL_VERIFICATION: 'mandatory'\r\nAPPEND_SLASH: True\r\nFIRST_DAY_OF_WEEK: 0\r\nDATABASE_ROUTERS: u'********'\r\nDEFAULT_TABLESPACE: u''\r\nYEAR_MONTH_FORMAT: u'F Y'\r\nSTATICFILES_STORAGE: 'storages.backends.s3boto.S3BotoStorage'\r\nCACHES: {\r\n    u'default': {   u'BACKEND': u'django.core.cache.backends.locmem.LocMemCache'}}\r\nSESSION_COOKIE_PATH: u'/'\r\nSILENCED_SYSTEM_CHECKS: []\r\nMIDDLEWARE_CLASSES:\r\n    ('django.contrib.sessions.middleware.SessionMiddleware',\r\n 'django.middleware.common.CommonMiddleware',\r\n 'django.contrib.auth.middleware.AuthenticationMiddleware',\r\n 'django.contrib.auth.middleware.SessionAuthenticationMiddleware',\r\n 'django.contrib.messages.middleware.MessageMiddleware',\r\n 'django.middleware.clickjacking.XFrameOptionsMiddleware',\r\n 'django.middleware.security.SecurityMiddleware')\r\nUSE_I18N: True\r\n\r\nSECRET_KEY: u'********'\r\nAWS_SES_REGION_NAME: 'us-west-2'\r\nDEFAULT_INDEX_TABLESPACE: u''\r\nLOGGING_CONFIG: u'logging.config.dictConfig'\r\n\r\nWSGI_APPLICATION: 'project.wsgi.application'\r\nAUTHENTICATION_METHOD: 'email'\r\nX_FRAME_OPTIONS: u'SAMEORIGIN'\r\nAUTHENTICATION_BACKENDS:\r\n    ('django.contrib.auth.backends.ModelBackend',\r\n 'allauth.account.auth_backends.AuthenticationBackend')\r\nUSE_THOUSAND_SEPARATOR: False\r\nUSE_X_FORWARDED_HOST: False\r\nCSRF_HEADER_NAME: u'HTTP_X_CSRFTOKEN'\r\nEMAIL_TIMEOUT: None\r\nSECURE_SSL_HOST: None\r\nSIGNING_BACKEND: u'django.core.signing.TimestampSigner'\r\nSESSION_COOKIE_SECURE: False\r\nCSRF_COOKIE_DOMAIN: None\r\nFILE_CHARSET: u'utf-8'\r\nDEBUG: False\r\nDATA_UPLOAD_MAX_MEMORY_SIZE: 2621440\r\nSCARFACE_DEVICE_OWNER_MODEL: 'authentication.Profile'\r\nDEFAULT_FILE_STORAGE: u'django.core.files.storage.FileSystemStorage'\r\nINSTALLED_APPS:\r\n    ('django.contrib.admin',\r\n 'django.contrib.auth',\r\n 'django.contrib.contenttypes',\r\n 'django.contrib.sessions',\r\n 'django.contrib.messages',\r\n 'django.contrib.staticfiles',\r\n 'django.contrib.sites',\r\n 'django_celery_beat',\r\n 'django_celery_results',\r\n 'allauth',\r\n 'allauth.account',\r\n 'authentication',\r\n '**',\r\n 'rest_framework',\r\n 'rest_framework.authtoken',\r\n 'rest_auth.registration',\r\n 'scarface',\r\n 'setup',\r\n 'tasks',\r\n 'storages')\r\n\r\nUSE_L10N: True\r\nSTATIC_ROOT: 'https://project-static.s3.amazonaws.com/'\r\nSECURE_HSTS_INCLUDE_SUBDOMAINS: False\r\nMIDDLEWARE: None\r\nis_overridden: <bound method Settings.is_overridden of <Settings \"project.settings.prod\">>\r\nPREPEND_WWW: False\r\nSECURE_PROXY_SSL_HEADER: None\r\nLANGUAGE_COOKIE_AGE: None\r\nSESSION_COOKIE_HTTPONLY: True\r\nAWS_SECRET_ACCESS_KEY: u'********'\r\nDEBUG_PROPAGATE_EXCEPTIONS: False\r\nACCOUNT_USERNAME_REQUIRED: False\r\nMEDIA_URL: u''\r\nAWS_SES_SECRET_ACCESS_KEY: u'********'\r\nMONTH_DAY_FORMAT: u'F j'\r\nMANAGERS: []\r\nSESSION_EXPIRE_AT_BROWSER_CLOSE: False\r\nAUTH_PASSWORD_VALIDATORS: u'********'\r\nTIME_FORMAT: u'P'\r\nAWS_SQS_SECRET_ACCESS_KEY: u'********'\r\nAUTH_USER_MODEL: u'auth.User'\r\nDATE_INPUT_FORMATS: [u'%Y-%m-%d',\r\n u'%m/%d/%Y',\r\n u'%m/%d/%y',\r\n u'%b %d %Y',\r\n u'%b %d, %Y',\r\n u'%d %b %Y',\r\n u'%d %b, %Y',\r\n u'%B %d %Y',\r\n u'%B %d, %Y',\r\n u'%d %B %Y',\r\n u'%d %B, %Y']\r\nCELERY_BROKER_TRANSPORT_OPTIONS: {\r\n    'polling_interval': 30,\r\n    'queue_name_prefix': 'project-prod-',\r\n    'region': 'eu-central-1',\r\n    'visibility_timeout': 3600,\r\n    'wait_time_seconds': 20}\r\nCSRF_TRUSTED_ORIGINS: []\r\nCSRF_COOKIE_NAME: u'csrftoken'\r\nEMAIL_HOST_PASSWORD: u'********'\r\nAWS_SES_REGION_ENDPOINT: 'email.us-west-2.amazonaws.com'\r\nPASSWORD_RESET_TIMEOUT_DAYS: u'********'\r\nLANGUAGE_COOKIE_DOMAIN: None\r\nCACHE_MIDDLEWARE_ALIAS: u'default'\r\nSESSION_SAVE_EVERY_REQUEST: False\r\nNUMBER_GROUPING: 0\r\nSESSION_ENGINE: u'django.contrib.sessions.backends.db'\r\nAWS_SQS_ACCESS_KEY: u'********'\r\nCSRF_FAILURE_VIEW: u'django.views.csrf.csrf_failure'\r\nCSRF_COOKIE_PATH: u'/'\r\nLOGIN_REDIRECT_URL: u'/accounts/profile/'\r\nCELERY_TASK_SERIALIZER: 'json'\r\nDECIMAL_SEPARATOR: u'.'\r\nCACHE_MIDDLEWARE_KEY_PREFIX: u'********'\r\nLOCALE_PATHS: []\r\nAWS_ACCESS_KEY: u'********'\r\nSESSION_FILE_PATH: None\r\nSECURE_BROWSER_XSS_FILTER: False\r\nFIXTURE_DIRS: []\r\nEMAIL_HOST: u'localhost'\r\nDATE_FORMAT: u'N j, Y'\r\nMEDIA_ROOT: u''\r\nENCRYPTED_FIELDS_KEYDIR: u'********'\r\nDEFAULT_EXCEPTION_REPORTER_FILTER: u'django.views.debug.SafeExceptionReporterFilter'\r\nADMINS: []\r\nFORMAT_MODULE_PATH: None\r\nDEFAULT_FROM_EMAIL: 'some@project.com'\r\nTHOUSAND_SEPARATOR: u','\r\nREST_FRAMEWORK: {\r\n    'DEFAULT_AUTHENTICATION_CLASSES': (   'rest_framework.authentication.TokenAuthentication',)}\r\nSTATICFILES_DIRS:\r\n    ('/home/ubuntu/webapps/project/project/../static',)\r\nSECURE_CONTENT_TYPE_NOSNIFF: False\r\nDATETIME_FORMAT: u'N j, Y, P'\r\nCELERY_ACCEPT_CONTENT: ['json']\r\nAWS_SES_ACCESS_KEY_ID: u'********'\r\nSITE_ID: 1\r\nDISALLOWED_USER_AGENTS: []\r\nREST_AUTH_REGISTER_SERIALIZERS: {\r\n    'REGISTER_SERIALIZER': 'authentication.serializers.RegistrationSerializer'}\r\nCELERY_TIMEZONE: 'Europe/Berlin'\r\nLOGGING: {\r\n    'disable_existing_loggers': False,\r\n    'filters': {   'require_debug_false': {   '()': 'django.utils.log.RequireDebugFalse'}},\r\n    'handlers': {   'mail_admins': {   'class': 'django.utils.log.AdminEmailHandler',\r\n                                       'filters': ['require_debug_false'],\r\n                                       'level': 'ERROR'}},\r\n    'loggers': {   'django.request': {   'handlers': ['mail_admins'],\r\n                                         'level': 'ERROR',\r\n                                         'propagate': True}},\r\n    'version': 1}\r\nSHORT_DATE_FORMAT: u'm/d/Y'\r\nTEMPLATES: [{'APP_DIRS': True,\r\n  'BACKEND': 'django.template.backends.django.DjangoTemplates',\r\n  'DIRS': ['templates'],\r\n  'OPTIONS': {'context_processors': ['django.template.context_processors.debug',\r\n                                     'django.template.context_processors.request',\r\n                                     'django.contrib.auth.context_processors.auth',\r\n                                     'django.contrib.messages.context_processors.messages']}}]\r\nLOGOUT_REDIRECT_URL: None\r\nTEST_RUNNER: u'django.test.runner.DiscoverRunner'\r\nACCOUNT_LOGOUT_ON_PASSWORD_CHANGE: u'********'\r\nIGNORABLE_404_URLS: []\r\nEMAIL_USE_TLS: False\r\nTIME_ZONE: 'Europe/Berlin'\r\nFILE_UPLOAD_MAX_MEMORY_SIZE: 2621440\r\nEMAIL_BACKEND: 'django_ses.SESBackend'\r\nS3_URL: 'https://project-static.s3.amazonaws.com/'\r\nLANGUAGE_COOKIE_NAME: u'django_language'\r\nCSRF_COOKIE_AGE: 31449600\r\nEMAIL_USE_SSL: False\r\nCELERY_SEND_TASK_ERROR_EMAILS: True\r\nMIGRATION_MODULES: {\r\n    }\r\nREST_AUTH_SERIALIZERS: {\r\n    'LOGIN_SERIALIZER': 'authentication.serializers.CustomLoginSerializer',\r\n    'USER_DETAILS_SERIALIZER': 'authentication.serializers.UserSerializer'}\r\nDATA_UPLOAD_MAX_NUMBER_FIELDS: 1000\r\nSESSION_COOKIE_AGE: 1209600\r\nSESSION_SERIALIZER: u'django.contrib.sessions.serializers.JSONSerializer'\r\nSETTINGS_MODULE: u'project.settings.prod'\r\nUSE_ETAGS: False\r\n\r\nLANGUAGES_BIDI: [u'he', u'ar', u'fa', u'ur']\r\nFILE_UPLOAD_TEMP_DIR: None\r\nINTERNAL_IPS: []\r\nSTATIC_URL: 'https://project-static.s3.amazonaws.com/'\r\nEMAIL_PORT: 25\r\nUSE_TZ: True\r\nSHORT_DATETIME_FORMAT: u'm/d/Y P'\r\nCELERY_RESULT_BACKEND: 'django-db'\r\nPASSWORD_HASHERS: u'********'\r\nACCOUNT_UNIQUE_EMAIL: True\r\nABSOLUTE_URL_OVERRIDES: {\r\n    }\r\nLANGUAGE_COOKIE_PATH: u'/'\r\nCACHE_MIDDLEWARE_SECONDS: 600\r\nEMAIL_SSL_CERTFILE: None\r\nCSRF_COOKIE_HTTPONLY: False\r\nDATETIME_INPUT_FORMATS: [u'%Y-%m-%d %H:%M:%S',\r\n u'%Y-%m-%d %H:%M:%S.%f',\r\n u'%Y-%m-%d %H:%M',\r\n u'%Y-%m-%d',\r\n u'%m/%d/%Y %H:%M:%S',\r\n u'%m/%d/%Y %H:%M:%S.%f',\r\n u'%m/%d/%Y %H:%M',\r\n u'%m/%d/%Y',\r\n u'%m/%d/%y %H:%M:%S',\r\n u'%m/%d/%y %H:%M:%S.%f',\r\n u'%m/%d/%y %H:%M',\r\n u'%m/%d/%y']\r\nFORCE_SCRIPT_NAME: None\r\nEMAIL_HOST_USER: u''\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3990/reactions", "total_count": 5, "+1": 5, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pnovotnak": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3986", "title": "Errors in Django Apps are Silently Ignored", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n\r\nImport error in django app's tasks.py module causes the import to silently fail, but allows the celery process to otherwise start properly. No warnings or errors are emitted.\r\n\r\nDemo; https://github.com/pnovotnak/celery/tree/bugreport/silent-exception-django-loader/examples/django\r\n\r\n## Expected behavior\r\n\r\nBlow up at load time, or at least emit an error in the logs explaining why some tasks were not loaded.\r\n\r\n## Actual behavior\r\n\r\nNothing is emitted, making determining why tasks are not being loaded an extremely painful experience when it should be trivially easy.\r\n\r\nAgainst master;\r\n\r\n```\r\n$ celery -A proj worker -l info\r\n\r\n\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\r\n\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\r\n\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\r\n\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\r\n\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\r\n\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\r\n\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\r\n\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\u00cd\r\n\r\ncelery@Slug.local v4.0.2 (latentcall)\r\n\r\nDarwin-16.5.0-x86_64-i386-64bit 2017-04-18 20:07:55\r\n\r\n[config]\r\n.> app:         proj:0x105e10cc0\r\n.> transport:   amqp://guest:**@localhost:5672//\r\n.> results:     sqlite:///results.sqlite\r\n.> concurrency: 8 (prefork)\r\n.> task events: OFF (enable -E to monitor tasks in this worker)\r\n\r\n[queues]\r\n.> celery           exchange=celery(direct) key=celery\r\n\r\n\r\n[tasks]\r\n  . proj.celery.debug_task\r\n\r\n[2017-04-18 20:07:55,450: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672//\r\n[2017-04-18 20:07:55,464: INFO/MainProcess] mingle: searching for neighbors\r\n[2017-04-18 20:07:56,499: INFO/MainProcess] mingle: all alone\r\n[2017-04-18 20:07:56,514: WARNING/MainProcess] /Users/pnovotnak/Documents/Cyrus/celery/examples/django/env/lib/python3.6/site-packages/celery-4.0.2-py3.6.egg/celery/fixups/django.py:202: UserWarning: Using settings.DEBUG leads to a memory leak, never use this setting in production environments!\r\n  warnings.warn('Using settings.DEBUG leads to a memory leak, never '\r\n[2017-04-18 20:07:56,514: INFO/MainProcess] celery@Slug.local ready.\r\n```\r\n\r\n```\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:3.6.1\r\n            billiard:3.5.0.2 py-amqp:2.1.4\r\nplatform -> system:Darwin arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:db+sqlite:///results.sqlite\r\n\r\nABSOLUTE_URL_OVERRIDES: {\r\n }\r\nADMINS:\r\n    ()\r\nALLOWED_HOSTS: []\r\nAPPEND_SLASH: True\r\nAUTHENTICATION_BACKENDS: ['django.contrib.auth.backends.ModelBackend']\r\nAUTH_PASSWORD_VALIDATORS: '********'\r\nAUTH_USER_MODEL: 'auth.User'\r\nCACHES: {\r\n 'default': {'BACKEND': 'django.core.cache.backends.locmem.LocMemCache'}}\r\nCACHE_MIDDLEWARE_ALIAS: 'default'\r\nCACHE_MIDDLEWARE_KEY_PREFIX: '********'\r\nCACHE_MIDDLEWARE_SECONDS: 600\r\nCELERY_ACCEPT_CONTENT: ['json']\r\nCELERY_BROKER_URL: 'amqp://guest:********@localhost:5672//'\r\nCELERY_RESULT_BACKEND: 'db+sqlite:///results.sqlite'\r\nCELERY_TASK_SERIALIZER: 'json'\r\nCSRF_COOKIE_AGE: 31449600\r\nCSRF_COOKIE_DOMAIN: None\r\nCSRF_COOKIE_HTTPONLY: False\r\nCSRF_COOKIE_NAME: 'csrftoken'\r\nCSRF_COOKIE_PATH: '/'\r\nCSRF_COOKIE_SECURE: False\r\nCSRF_FAILURE_VIEW: 'django.views.csrf.csrf_failure'\r\nCSRF_HEADER_NAME: 'HTTP_X_CSRFTOKEN'\r\nCSRF_TRUSTED_ORIGINS: []\r\nCSRF_USE_SESSIONS: False\r\nDATABASES: {\r\n    'default': {   'ENGINE': 'django.db.backends.sqlite3',\r\n                   'HOST': '',\r\n                   'NAME': 'test.db',\r\n                   'PASSWORD': '********',\r\n                   'PORT': '',\r\n                   'USER': ''}}\r\nDATABASE_ROUTERS: '********'\r\nDATA_UPLOAD_MAX_MEMORY_SIZE: 2621440\r\nDATA_UPLOAD_MAX_NUMBER_FIELDS: 1000\r\nDATETIME_FORMAT: 'N j, Y, P'\r\nDATETIME_INPUT_FORMATS: ['%Y-%m-%d %H:%M:%S',\r\n '%Y-%m-%d %H:%M:%S.%f',\r\n '%Y-%m-%d %H:%M',\r\n '%Y-%m-%d',\r\n '%m/%d/%Y %H:%M:%S',\r\n '%m/%d/%Y %H:%M:%S.%f',\r\n '%m/%d/%Y %H:%M',\r\n '%m/%d/%Y',\r\n '%m/%d/%y %H:%M:%S',\r\n '%m/%d/%y %H:%M:%S.%f',\r\n '%m/%d/%y %H:%M',\r\n '%m/%d/%y']\r\nDATE_FORMAT: 'N j, Y'\r\nDATE_INPUT_FORMATS: ['%Y-%m-%d',\r\n '%m/%d/%Y',\r\n '%m/%d/%y',\r\n '%b %d %Y',\r\n '%b %d, %Y',\r\n '%d %b %Y',\r\n '%d %b, %Y',\r\n '%B %d %Y',\r\n '%B %d, %Y',\r\n '%d %B %Y',\r\n '%d %B, %Y']\r\nDEBUG: True\r\nDEBUG_PROPAGATE_EXCEPTIONS: False\r\nDECIMAL_SEPARATOR: '.'\r\nDEFAULT_CHARSET: 'utf-8'\r\nDEFAULT_CONTENT_TYPE: 'text/html'\r\nDEFAULT_EXCEPTION_REPORTER_FILTER: 'django.views.debug.SafeExceptionReporterFilter'\r\nDEFAULT_FILE_STORAGE: 'django.core.files.storage.FileSystemStorage'\r\nDEFAULT_FROM_EMAIL: 'webmaster@localhost'\r\nDEFAULT_INDEX_TABLESPACE: ''\r\nDEFAULT_TABLESPACE: ''\r\nDISALLOWED_USER_AGENTS: []\r\nEMAIL_BACKEND: 'django.core.mail.backends.smtp.EmailBackend'\r\nEMAIL_HOST: 'localhost'\r\nEMAIL_HOST_PASSWORD: '********'\r\nEMAIL_HOST_USER: ''\r\nEMAIL_PORT: 25\r\nEMAIL_SSL_CERTFILE: None\r\nEMAIL_SSL_KEYFILE: '********'\r\nEMAIL_SUBJECT_PREFIX: '[Django] '\r\nEMAIL_TIMEOUT: None\r\nEMAIL_USE_LOCALTIME: False\r\nEMAIL_USE_SSL: False\r\nEMAIL_USE_TLS: False\r\nFILE_CHARSET: 'utf-8'\r\nFILE_UPLOAD_DIRECTORY_PERMISSIONS: None\r\nFILE_UPLOAD_HANDLERS: ['django.core.files.uploadhandler.MemoryFileUploadHandler',\r\n 'django.core.files.uploadhandler.TemporaryFileUploadHandler']\r\nFILE_UPLOAD_MAX_MEMORY_SIZE: 2621440\r\nFILE_UPLOAD_PERMISSIONS: None\r\nFILE_UPLOAD_TEMP_DIR: None\r\nFIRST_DAY_OF_WEEK: 0\r\nFIXTURE_DIRS: []\r\nFORCE_SCRIPT_NAME: None\r\nFORMAT_MODULE_PATH: None\r\nFORM_RENDERER: 'django.forms.renderers.DjangoTemplates'\r\nIGNORABLE_404_URLS: []\r\nINSTALLED_APPS:\r\n    ('django.contrib.auth',\r\n 'django.contrib.contenttypes',\r\n 'django.contrib.sessions',\r\n 'django.contrib.sites',\r\n 'django.contrib.messages',\r\n 'django.contrib.staticfiles',\r\n 'django.contrib.admin',\r\n 'demoapp')\r\nINTERNAL_IPS: []\r\nLANGUAGES: [('af', 'Afrikaans'),\r\n ('ar', 'Arabic'),\r\n ('ast', 'Asturian'),\r\n ('az', 'Azerbaijani'),\r\n ('bg', 'Bulgarian'),\r\n ('be', 'Belarusian'),\r\n ('bn', 'Bengali'),\r\n ('br', 'Breton'),\r\n ('bs', 'Bosnian'),\r\n ('ca', 'Catalan'),\r\n ('cs', 'Czech'),\r\n ('cy', 'Welsh'),\r\n ('da', 'Danish'),\r\n ('de', 'German'),\r\n ('dsb', 'Lower Sorbian'),\r\n ('el', 'Greek'),\r\n ('en', 'English'),\r\n ('en-au', 'Australian English'),\r\n ('en-gb', 'British English'),\r\n ('eo', 'Esperanto'),\r\n ('es', 'Spanish'),\r\n ('es-ar', 'Argentinian Spanish'),\r\n ('es-co', 'Colombian Spanish'),\r\n ('es-mx', 'Mexican Spanish'),\r\n ('es-ni', 'Nicaraguan Spanish'),\r\n ('es-ve', 'Venezuelan Spanish'),\r\n ('et', 'Estonian'),\r\n ('eu', 'Basque'),\r\n ('fa', 'Persian'),\r\n ('fi', 'Finnish'),\r\n ('fr', 'French'),\r\n ('fy', 'Frisian'),\r\n ('ga', 'Irish'),\r\n ('gd', 'Scottish Gaelic'),\r\n ('gl', 'Galician'),\r\n ('he', 'Hebrew'),\r\n ('hi', 'Hindi'),\r\n ('hr', 'Croatian'),\r\n ('hsb', 'Upper Sorbian'),\r\n ('hu', 'Hungarian'),\r\n ('ia', 'Interlingua'),\r\n ('id', 'Indonesian'),\r\n ('io', 'Ido'),\r\n ('is', 'Icelandic'),\r\n ('it', 'Italian'),\r\n ('ja', 'Japanese'),\r\n ('ka', 'Georgian'),\r\n ('kk', 'Kazakh'),\r\n ('km', 'Khmer'),\r\n ('kn', 'Kannada'),\r\n ('ko', 'Korean'),\r\n ('lb', 'Luxembourgish'),\r\n ('lt', 'Lithuanian'),\r\n ('lv', 'Latvian'),\r\n ('mk', 'Macedonian'),\r\n ('ml', 'Malayalam'),\r\n ('mn', 'Mongolian'),\r\n ('mr', 'Marathi'),\r\n ('my', 'Burmese'),\r\n ('nb', 'Norwegian Bokm\u00e5l'),\r\n ('ne', 'Nepali'),\r\n ('nl', 'Dutch'),\r\n ('nn', 'Norwegian Nynorsk'),\r\n ('os', 'Ossetic'),\r\n ('pa', 'Punjabi'),\r\n ('pl', 'Polish'),\r\n ('pt', 'Portuguese'),\r\n ('pt-br', 'Brazilian Portuguese'),\r\n ('ro', 'Romanian'),\r\n ('ru', 'Russian'),\r\n ('sk', 'Slovak'),\r\n ('sl', 'Slovenian'),\r\n ('sq', 'Albanian'),\r\n ('sr', 'Serbian'),\r\n ('sr-latn', 'Serbian Latin'),\r\n ('sv', 'Swedish'),\r\n ('sw', 'Swahili'),\r\n ('ta', 'Tamil'),\r\n ('te', 'Telugu'),\r\n ('th', 'Thai'),\r\n ('tr', 'Turkish'),\r\n ('tt', 'Tatar'),\r\n ('udm', 'Udmurt'),\r\n ('uk', 'Ukrainian'),\r\n ('ur', 'Urdu'),\r\n ('vi', 'Vietnamese'),\r\n ('zh-hans', 'Simplified Chinese'),\r\n ('zh-hant', 'Traditional Chinese')]\r\nLANGUAGES_BIDI: ['he', 'ar', 'fa', 'ur']\r\nLANGUAGE_CODE: 'en-us'\r\nLANGUAGE_COOKIE_AGE: None\r\nLANGUAGE_COOKIE_DOMAIN: None\r\nLANGUAGE_COOKIE_NAME: 'django_language'\r\nLANGUAGE_COOKIE_PATH: '/'\r\nLOCALE_PATHS: []\r\nLOGGING: {\r\n    'disable_existing_loggers': False,\r\n    'filters': {   'require_debug_false': {   '()': 'django.utils.log.RequireDebugFalse'}},\r\n    'handlers': {   'mail_admins': {   'class': 'django.utils.log.AdminEmailHandler',\r\n                                       'filters': ['require_debug_false'],\r\n                                       'level': 'ERROR'}},\r\n    'loggers': {   'django.request': {   'handlers': ['mail_admins'],\r\n                                         'level': 'ERROR',\r\n                                         'propagate': True}},\r\n    'version': 1}\r\nLOGGING_CONFIG: 'logging.config.dictConfig'\r\nLOGIN_REDIRECT_URL: '/accounts/profile/'\r\nLOGIN_URL: '/accounts/login/'\r\nLOGOUT_REDIRECT_URL: None\r\nMANAGERS:\r\n    ()\r\nMEDIA_ROOT: ''\r\nMEDIA_URL: ''\r\nMESSAGE_STORAGE: 'django.contrib.messages.storage.fallback.FallbackStorage'\r\nMIDDLEWARE: None\r\nMIDDLEWARE_CLASSES:\r\n    ('django.middleware.common.CommonMiddleware',\r\n 'django.contrib.sessions.middleware.SessionMiddleware',\r\n 'django.middleware.csrf.CsrfViewMiddleware',\r\n 'django.contrib.auth.middleware.AuthenticationMiddleware',\r\n 'django.contrib.messages.middleware.MessageMiddleware')\r\nMIGRATION_MODULES: {\r\n }\r\nMONTH_DAY_FORMAT: 'F j'\r\nNUMBER_GROUPING: 0\r\nPASSWORD_HASHERS: '********'\r\nPASSWORD_RESET_TIMEOUT_DAYS: '********'\r\nPREPEND_WWW: False\r\nROOT_URLCONF: 'proj.urls'\r\nSECRET_KEY: '********'\r\nSECURE_BROWSER_XSS_FILTER: False\r\nSECURE_CONTENT_TYPE_NOSNIFF: False\r\nSECURE_HSTS_INCLUDE_SUBDOMAINS: False\r\nSECURE_HSTS_PRELOAD: False\r\nSECURE_HSTS_SECONDS: 0\r\nSECURE_PROXY_SSL_HEADER: None\r\nSECURE_REDIRECT_EXEMPT: []\r\nSECURE_SSL_HOST: None\r\nSECURE_SSL_REDIRECT: False\r\nSERVER_EMAIL: 'root@localhost'\r\nSESSION_CACHE_ALIAS: 'default'\r\nSESSION_COOKIE_AGE: 1209600\r\nSESSION_COOKIE_DOMAIN: None\r\nSESSION_COOKIE_HTTPONLY: True\r\nSESSION_COOKIE_NAME: 'sessionid'\r\nSESSION_COOKIE_PATH: '/'\r\nSESSION_COOKIE_SECURE: False\r\nSESSION_ENGINE: 'django.contrib.sessions.backends.db'\r\nSESSION_EXPIRE_AT_BROWSER_CLOSE: False\r\nSESSION_FILE_PATH: None\r\nSESSION_SAVE_EVERY_REQUEST: False\r\nSESSION_SERIALIZER: 'django.contrib.sessions.serializers.JSONSerializer'\r\nSETTINGS_MODULE: 'proj.settings'\r\nSHORT_DATETIME_FORMAT: 'm/d/Y P'\r\nSHORT_DATE_FORMAT: 'm/d/Y'\r\nSIGNING_BACKEND: 'django.core.signing.TimestampSigner'\r\nSILENCED_SYSTEM_CHECKS: []\r\nSITE_ID: 1\r\nSTATICFILES_DIRS:\r\n    ()\r\nSTATICFILES_FINDERS:\r\n    ('django.contrib.staticfiles.finders.FileSystemFinder',\r\n 'django.contrib.staticfiles.finders.AppDirectoriesFinder')\r\nSTATICFILES_STORAGE: 'django.contrib.staticfiles.storage.StaticFilesStorage'\r\nSTATIC_ROOT: ''\r\nSTATIC_URL: '/static/'\r\nTEMPLATES: []\r\nTEMPLATE_DEBUG: True\r\nTEMPLATE_DIRS:\r\n    ()\r\nTEMPLATE_LOADERS:\r\n    ('django.template.loaders.filesystem.Loader',\r\n 'django.template.loaders.app_directories.Loader')\r\nTEST_NON_SERIALIZED_APPS: []\r\nTEST_RUNNER: 'django.test.runner.DiscoverRunner'\r\nTHOUSAND_SEPARATOR: ','\r\nTIME_FORMAT: 'P'\r\nTIME_INPUT_FORMATS: ['%H:%M:%S', '%H:%M:%S.%f', '%H:%M']\r\nTIME_ZONE: 'America/Chicago'\r\nUSE_ETAGS: False\r\nUSE_I18N: True\r\nUSE_L10N: True\r\nUSE_THOUSAND_SEPARATOR: False\r\nUSE_TZ: True\r\nUSE_X_FORWARDED_HOST: False\r\nUSE_X_FORWARDED_PORT: False\r\nWSGI_APPLICATION: 'proj.wsgi.application'\r\nX_FRAME_OPTIONS: 'SAMEORIGIN'\r\nYEAR_MONTH_FORMAT: 'F Y'\r\nis_overridden: <bound method Settings.is_overridden of <Settings \"proj.settings\">>\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3986/reactions", "total_count": 4, "+1": 4, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mlissner": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3979", "title": "Documentation of ignored results", "body": "First, on the [canvas page][c] there's currently a warning in the section about chords that indicates that they won't work if you ignore their results. \r\n\r\nThere's no such warning about groups, but I believe the same is true for groups, since they need to join until all of the results are in. Can we add such a warning? \r\n\r\nSecond, on the [tasks page][t] there's documentation about `ignore_result`, but it makes no mention of which features will fail if you provide that argument to your task. Can we at least outline that `group` and `chord` won't work? \r\n\r\n[c]: http://docs.celeryproject.org/en/3.1/userguide/canvas.html?highlight=CELERY_IGNORE_RESULT\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3979/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/3949", "title": "Routing questions left unanswered by docs", "body": "I've been using Celery for many years now in a fairly basic way. Whenever I try to use it in more complex ways, I'm left a bit flummoxed by the docs. I'm filing this bug in hopes that it may be useful for improving at least one page of the docs \u2014 [the routing page][r] \u2014 and perhaps the API overall. I don't necessarily need answers (though I'd like them!) \u2014 my bigger goal is to provide constructive feedback, if I can.\r\n\r\n[r]: http://docs.celeryproject.org/en/3.1/userguide/routing.html\r\n\r\nFirst a couple suggestions:\r\n\r\n1. I think the AMQP primer should go first on the page. If I read that first, I'd have a better time understanding the stuff that's currently above it. I could be wrong, but I think we can assume that most users of Celery are unfamiliar with AMQP primitives.\r\n\r\n2. There's a section on changing the name of the default queue, and then the rest of the docs assume that's been done. Perhaps move this section to the end and explain earlier that the default queue is called 'celery' (and update everywhere that 'default' is used as the name). I'm actually not sure what value this section has to most users. I'm fine with the default being named 'celery' for historical reasons and have no intention of changing the name to default at the cost of adding a more complex config.\r\n\r\nAnd now some questions that I still don't understand. Perhaps others have similar questions:\r\n\r\n1. Why do we bother with exchanges at all? What do we get from the added complexity? I see two things that they accomplish:\r\n\r\n    1. You can use exchanges for routing tasks to queues. But why bother? The important thing is that a task be processed in a given queue, and we can route tasks to queues using `routing_key`s without bothering with exchanges. Do exchanges add something to routing that I'm not understanding? If so, should we document that?\r\n\r\n    1. Exchanges can be either direct or topic type. But why not drop this bit of complication and just make direct queues from any topic queue that doesn't use a `#` or an `*`? Isn't that the same thing? Is this a concept we can drop?\r\n\r\n    If exchanges aren't really useful for routing and they're not useful for having different types...can we drop them altogether? \r\n\r\n1. Why do we have class-based and dict-based routers? After providing numerous dict-based examples, the docs say that we should avoid hardcoding the router config and should use the class-based routers instead. Why? It looks like two sides of the same coin. Perhaps the classes are more powerful. If so, an example of how they're useful would be great, and perhaps the docs should be updated to use class-based routers instead of dict-based configs.\r\n\r\n1. Why are broadcast tasks useful? I guess they're for doing maintenance on the task servers? Should we spell that out? \r\n\r\nI hope this is all useful. I've read this page many times, and every time I do I'm left feeling confused about why there's so much complication, and stressed about how I should properly apply it in our environment. I don't know much about AMQP, but from a totally naive perspective, it seems like there are some opportunities to simplify the docs and the API?", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3949/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "davidhyman": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3971", "title": "parallel `self.replace` tasks cause deadlock", "body": "## Checklist\r\n```\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:3.6.0\r\n            billiard:3.5.0.2 redis:2.10.5\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis://localhost:6379/1\r\n\r\nBROKER_URL: 'redis://localhost:6379/0'\r\nCELERYD_POOL_RESTARTS: True\r\nCELERYD_TASK_SOFT_TIME_LIMIT: 300\r\nCELERYD_TASK_TIME_LIMIT: 360\r\nCELERY_ACCEPT_CONTENT: ['json']\r\nCELERY_RESULT_BACKEND: 'redis://localhost:6379/1'\r\nCELERY_RESULT_SERIALIZER: 'json'\r\nCELERY_ROUTES: \r\n    ([('dyna_map', {'queue': 'dyna'}), ('*', {'queue': 'celery'})],)\r\nCELERY_TASK_SERIALIZER: 'json'\r\nCELERY_TRACK_STARTED: True\r\n```\r\nmaster@522ef610ea17ae79695e995ddd82d034e8e8bc7a\r\n\r\n## Steps to reproduce\r\n```\r\n@apium.task(name='O2O')\r\ndef one_to_one(arg):\r\n    result = 2 * arg\r\n    logger.info('o2o %r -> %r', arg, result)\r\n    return result\r\n\r\n@apium.task(name='substitute', bind=True)\r\ndef substitute(self, arg):\r\n    logger.info('substitute %r', arg)\r\n    arg = ensure_list(arg)\r\n    raise self.replace(group(one_to_one.s(a) for a in arg))\r\n\r\n@apium.task(name='O2M')\r\ndef one_to_many(arg):\r\n    result = list(range(1, arg + 1))\r\n    logger.info('o2m %r -> %r', arg, result)\r\n    return result\r\n\r\nt = tasks.one_to_many.s(5) | tasks.substitute.s()\r\nr = t.delay().get()\r\n```\r\n## Expected behavior\r\nThe `substitute` task is ended after submitting the group to the broker. The implication is that a single worker could process all of these tasks without issue.\r\n\r\n## Actual behavior\r\nThe `substitute` task appears to run for the duration of the group tasks, with behaviour as if it were simply a nested chord waiting on `task.delay().get()`.\r\nThe implication is that you may only run as many `substitute` tasks as you have workers. Exceeding that limit results in deadlock (hence the Celery's existing protection from doing `.get()` when already in a task).\r\n\r\n## Workaround\r\nCurrently I'm running two celery workers listening on different queues where there is at least one more consumer for child tasks than there is for `substitute` ones. Seems to work for now.\r\n\r\n---\r\nIn my opinion, this behaviour is needed because it would allow us to write one-many-one tasks in complex workflows. It should be decoupled from the number of workers/consumers available. I really hope I'm doing it wrong, because to me, this is the primary functionality of `.replace()`!  In which case, suggestions welcome :)\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3971/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nachopro": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3957", "title": "Celery's send_task/delay lost the first message after Broker goes offline", "body": "I'm using a simple app to verify the problem that I have.\r\n\r\nMy task `add` append some string to a text file.\r\n\r\nWhen RabbitMQ is running, I achieve this appending from an ipython shell executing `send_task('add', ('my message',))`.\r\n\r\nThen I manually stops RabbitMQ and execute `send_task('add', ('a new message',))` in the same ipython session. This execution occurs without trwhoing any exception.\r\n\r\nThe __'a new messasage'__ never been enqueued and not noticed about this.\r\nIn the subsequent send_task calls, an `OperationalError: [Errno 104] Connection reset by peer` is raised, that I properly handling with `try/except`.\r\n\r\nHow can I avoid this \"first message\" lost when Rabbit goes offline?\r\n\r\nThanks in advance.\r\n\r\nHere a Video showing this issue in Action!\r\nhttps://www.youtube.com/watch?v=WmAYBkVJk7Q\r\n\r\n__tasks.py__\r\n\r\n<!-- language: lang-py -->\r\n\r\n    from celery import Celery\r\n\r\n    app = Celery('tasks', broker='amqp://dev:dev@127.0.0.1/')\r\n    \r\n    @app.task\r\n    def add(algo):\r\n        print('Writing \"{}\" to file'.format(algo))\r\n        open('/tmp/pepe.txt', 'a').write('{}\\n'.format(algo))\r\n\r\n__ipython shell__ (just after stopping rabbitmq)\r\n\r\n<!-- language: lang-py -->\r\n\r\n    In [1]: from celery import Celery\r\n    \r\n    In [2]: cel = Celery('tasks', broker='amqp://dev:dev@127.0.0.1/')\r\n    \r\n    In [3]: cel.send_task('tasks.add', ('my message 23',))\r\n    Out[3]: <AsyncResult: a2167d23-3190-480b-bdb0-96dd1c98b08c>\r\n\r\n\r\n  [1]: https://www.youtube.com/watch?v=WmAYBkVJk7Q", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3957/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dralley": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3955", "title": "Consumer bootsteps fail to work if they require the Timer component ", "body": "## Checklist\r\n\r\n- [x] Affected Celery versions:  3.1.z, 4.0.z\r\n- [x] Issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n\r\n```\r\nfrom celery import Celery\r\nfrom celery import bootsteps\r\n\r\nclass Reproducer(bootsteps.StartStopStep):\r\n    requires = ('celery.worker.components:Timer', )\r\n\r\n    def __init__(self, parent, **kwargs):\r\n        print('{0!r} is in init'.format(parent))\r\n\r\n    def start(self, worker):\r\n        print('{0!r} is starting up'.format(worker))\r\n\r\n        self.timer_ref = worker.timer.call_repeatedly(\r\n            5,\r\n            self.do_work,\r\n            (worker, ),\r\n            priority=10,\r\n        )\r\n\r\n    def do_work(self, worker):\r\n        print('{0!r} heartbeat'.format(worker))\r\n\r\n    def stop(self, parent):\r\n        print('{0!r} is stopping'.format(parent))\r\n\r\n    def shutdown(self, parent):\r\n        print('{0!r} is shutting down'.format(parent))\r\n\r\napp = Celery(broker='amqp://')\r\napp.steps['consumer'].add(Reproducer)\r\n```\r\n\r\n1. Start the worker \"celery worker -A reproducer.app\"\r\n2. Stop the broker \"sudo systemctl stop rabbitmq-server\"\r\n3.  Wait a few seconds\r\n4.  Restart the broker \"sudo systemctl start rabbitmq-server\"\r\n\r\n## Expected behavior\r\n\r\nCelery worker starts properly\r\n\r\n## Actual behavior\r\n\r\n```\r\n[dalley@dhcp129-27 devel]$ celery worker -A reproducer\r\n<Consumer: celery@dhcp129-27.rdu.redhat.com (initializing)> is in init\r\nTraceback (most recent call last):\r\n  File \"/usr/bin/celery\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/lib/python2.7/site-packages/celery/__main__.py\", line 14, in main\r\n    _main()\r\n  File \"/usr/lib/python2.7/site-packages/celery/bin/celery.py\", line 326, in main\r\n    cmd.execute_from_commandline(argv)\r\n  File \"/usr/lib/python2.7/site-packages/celery/bin/celery.py\", line 488, in execute_from_commandline\r\n    super(CeleryCommand, self).execute_from_commandline(argv)))\r\n  File \"/usr/lib/python2.7/site-packages/celery/bin/base.py\", line 281, in execute_from_commandline\r\n    return self.handle_argv(self.prog_name, argv[1:])\r\n  File \"/usr/lib/python2.7/site-packages/celery/bin/celery.py\", line 480, in handle_argv\r\n    return self.execute(command, argv)\r\n  File \"/usr/lib/python2.7/site-packages/celery/bin/celery.py\", line 412, in execute\r\n    ).run_from_argv(self.prog_name, argv[1:], command=argv[0])\r\n  File \"/usr/lib/python2.7/site-packages/celery/bin/worker.py\", line 221, in run_from_argv\r\n    return self(*args, **options)\r\n  File \"/usr/lib/python2.7/site-packages/celery/bin/base.py\", line 244, in __call__\r\n    ret = self.run(*args, **kwargs)\r\n  File \"/usr/lib/python2.7/site-packages/celery/bin/worker.py\", line 255, in run\r\n    **kwargs)\r\n  File \"/usr/lib/python2.7/site-packages/celery/worker/worker.py\", line 99, in __init__\r\n    self.setup_instance(**self.prepare_args(**kwargs))\r\n  File \"/usr/lib/python2.7/site-packages/celery/worker/worker.py\", line 139, in setup_instance\r\n    self.blueprint.apply(self, **kwargs)\r\n  File \"/usr/lib/python2.7/site-packages/celery/bootsteps.py\", line 214, in apply\r\n    step.include(parent)\r\n  File \"/usr/lib/python2.7/site-packages/celery/bootsteps.py\", line 384, in include\r\n    inc, ret = self._should_include(parent)\r\n  File \"/usr/lib/python2.7/site-packages/celery/bootsteps.py\", line 339, in _should_include\r\n    return True, self.create(parent)\r\n  File \"/usr/lib/python2.7/site-packages/celery/worker/components.py\", line 241, in create\r\n    prefetch_multiplier=w.prefetch_multiplier,\r\n  File \"/usr/lib/python2.7/site-packages/celery/bootsteps.py\", line 335, in instantiate\r\n    return instantiate(name, *args, **kwargs)\r\n  File \"/usr/lib/python2.7/site-packages/celery/utils/imports.py\", line 53, in instantiate\r\n    return symbol_by_name(name)(*args, **kwargs)\r\n  File \"/usr/lib/python2.7/site-packages/celery/worker/consumer/consumer.py\", line 220, in __init__\r\n    self.blueprint.apply(self, **dict(worker_options or {}, **kwargs))\r\n  File \"/usr/lib/python2.7/site-packages/celery/bootsteps.py\", line 214, in apply\r\n    step.include(parent)\r\n  File \"/usr/lib/python2.7/site-packages/celery/bootsteps.py\", line 343, in include\r\n    return self._should_include(parent)[0]\r\n  File \"/usr/lib/python2.7/site-packages/celery/bootsteps.py\", line 339, in _should_include\r\n    return True, self.create(parent)\r\n  File \"/usr/lib/python2.7/site-packages/celery/worker/components.py\", line 39, in create\r\n    if w.use_eventloop:\r\nAttributeError: 'Consumer' object has no attribute 'use_eventloop'\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3955/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xavierhardy": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3954", "title": "Need more documentation about the configuration of the `auth` serializer", "body": "http://docs.celeryproject.org/en/latest/userguide/security.html#message-signing\r\n\r\n\"To enable this you should configure the `task_serializer` setting to use the `auth` serializer.\"\r\n\r\nWhich serializer is used alongside the message signature mechanism? How to change it? Let's say I want to use `yaml` with message signing, is it possible?", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3954/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/3953", "title": "Redis command list", "body": "Could you please document the Redis commands each Celery service uses? This way people could partially secure their Redis instances by removing unused commands.\r\n\r\nThanks", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3953/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "exister": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3946", "title": "Chord body not called if nested tasks has chain with chord as last step", "body": "## Checklist\r\n\r\n- [X] I have included the output of ``celery -A proj report`` in the issue.\r\n```\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:3.5.2\r\n            billiard:3.5.0.2 py-amqp:2.1.4\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:redis://redis:6379/0\r\n\r\nCELERY_TASK_ACKS_LATE: True\r\nCELERY_WORKER_PREFETCH_MULTIPLIER: 1\r\nCELERY_TASK_DEFAULT_EXCHANGE: 'tasks_exchange'\r\nCELERY_BROKER_TRANSPORT_OPTIONS: {\r\n 'confirm_publish': True}\r\nCELERY_TASK_TRACK_STARTED: True\r\nCELERY_ACCEPT_CONTENT: ['json']\r\nCELERY_TASK_SEND_SENT_EVENT: True\r\nCELERY_TASK_ALWAYS_EAGER: False\r\nCELERY_WORKER_DIRECT: False\r\nCELERY_BROKER_URL: 'amqp://guest:********@rabbitmq:5672//'\r\nCELERY_BROKER_HEARTBEAT: 10\r\nCELERY_TASK_DEFAULT_QUEUE: 'tasks'\r\nCELERY_BROKER_HEARTBEAT_CHECKRATE: 2\r\nCELERY_RESULT_BACKEND: 'redis://redis:6379/0'\r\nCELERY_TASK_SERIALIZER: 'json'\r\nCELERY_TIMEZONE: 'UTC'\r\nCELERY_RESULT_SERIALIZER: 'json'\r\nCELERY_WORKER_MAX_TASKS_PER_CHILD: 300\r\nCELERY_WORKER_HIJACK_ROOT_LOGGER: False\r\n```\r\n\r\n- [X ] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n\r\n```\r\n@current_app.task(bind=True)\r\ndef get_parts(self, job_id):\r\n    return [1, 2, 3]\r\n\r\n\r\n@current_app.task(bind=True)\r\ndef process_part(self, part_id):\r\n    raise self.replace(\r\n        chain(\r\n            process_step1.si(part_id),\r\n            chord(\r\n                [\r\n                    process_step2.si(part_id),\r\n                    process_step3.si(part_id)\r\n                ],\r\n                cleanup_part.si(part_id)\r\n            )\r\n        )\r\n    )\r\n\r\n@current_app.task(bind=True)\r\ndef cleanup_all(self, job_id):\r\n    pass\r\n\r\n@current_app.task(bind=True)\r\ndef cleanup_part(self, part_id):\r\n    pass\r\n\r\n@current_app.task(bind=True)\r\ndef dmap(self, parts, callback, final_callback):\r\n    callback = signature(callback)    \r\n    final_callback = signature(final_callback)\r\n    raise self.replace(\r\n        chord(\r\n            [callback.clone([part]) for part in parts],\r\n            final_callback\r\n        )\r\n    )\r\n\r\ndef run_job(job_id):\r\n    chain(\r\n        get_parts.si(job_id),\r\n        dmap.s(process_part.s(), final_callback=cleanup_all.si(job_id)),\r\n    ).apply_async()\r\n\r\nrun_job(1)\r\n```\r\n\r\n## Expected behavior\r\n\r\n`get_parts` returns list of parts ids\r\n`dmap` accepts parts ids and dynamically creates `chord` that runs `process_part` for each `part_id`\r\nafter all `process_part` are done, `cleanup_all` should be called\r\n\r\n## Actual behavior\r\n\r\nAll `process_part` tasks are processed correctly, but `cleanup_all` is never called.\r\n\r\nIf `CELERY_TASK_ALWAYS_EAGER` is set to `True` then everything works as expected and `cleanup_all` is called as last step.\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3946/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sieben": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3938", "title": "Unrecoverable error: UnicodeDecodeError", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n\r\n$ python3 -m celery -A tasks report\r\n\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:3.5.2\r\n            billiard:3.5.0.2 py-amqp:2.1.4\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:pyamqp results:redis:///\r\n\r\nresult_backend: 'redis:///'\r\nbroker_url: 'amqp://guest:********@localhost:5672//'\r\naccept_content: ['json']\r\ntask_serializer: 'json'\r\nenable_utc: True\r\ntimezone: 'Europe/Paris'\r\nresult_serializer: 'json'\r\n\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n\r\nLaunching with:\r\n\r\n    python3 -m celery -A tasks  worker --loglevel=debug\r\n\r\n## Expected behavior\r\n\r\nWorker starting to run and perform tasks smoothly\r\n\r\n## Actual behavior\r\n\r\nAfter some seconds it fails and gives the following output:\r\n\r\n```\r\n[2017-03-27 10:42:51,530: CRITICAL/MainProcess] Unrecoverable error: UnicodeDecodeError('utf-8', b'N\\xfd\\x17=\\x00\\x00', 1, 2, 'invalid start byte')\r\nTraceback (most recent call last):\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/celery-4.0.2-py3.5.egg/celery/worker/worker.py\", line 203, in start\r\n    self.blueprint.start(self)\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/celery-4.0.2-py3.5.egg/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/celery-4.0.2-py3.5.egg/celery/bootsteps.py\", line 370, in start\r\n    return self.obj.start()\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/celery-4.0.2-py3.5.egg/celery/worker/consumer/consumer.py\", line 320, in start\r\n    blueprint.start(self)\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/celery-4.0.2-py3.5.egg/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/celery-4.0.2-py3.5.egg/celery/worker/consumer/consumer.py\", line 596, in start\r\n    c.loop(*c.loop_args())\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/celery-4.0.2-py3.5.egg/celery/worker/loops.py\", line 85, in asynloop\r\n    update_qos()\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/kombu/common.py\", line 413, in update\r\n    return self.set(self.value)\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/kombu/common.py\", line 406, in set\r\n    self.callback(prefetch_count=new_value)\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/celery-4.0.2-py3.5.egg/celery/worker/consumer/tasks.py\", line 43, in set_prefetch_count\r\n    apply_global=qos_global,\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/kombu/messaging.py\", line 557, in qos\r\n    apply_global)\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/amqp/channel.py\", line 1828, in basic_qos\r\n    wait=spec.Basic.QosOk,\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 73, in send_method\r\n    return self.wait(wait, returns_tuple=returns_tuple)\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 93, in wait\r\n    self.connection.drain_events(timeout=timeout)\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/amqp/connection.py\", line 464, in drain_events\r\n    return self.blocking_read(timeout)\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/amqp/connection.py\", line 469, in blocking_read\r\n    return self.on_inbound_frame(frame)\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/amqp/method_framing.py\", line 72, in on_frame\r\n    msg.inbound_header(buf)\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/amqp/serialization.py\", line 564, in inbound_header\r\n    self._load_properties(class_id, buf, offset)\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/amqp/serialization.py\", line 521, in _load_properties\r\n    props, offset = classes[class_id](buf, offset)\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/amqp/serialization.py\", line 423, in decode_properties_basic\r\n    _f, offset = loads('F', buf, offset)\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/amqp/serialization.py\", line 228, in loads\r\n    key = pstr_t(buf[offset:offset + keylen])\r\n  File \"/home/sieben/.local/lib/python3.5/site-packages/amqp/utils.py\", line 82, in bytes_to_str\r\n    return s.decode()\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xfd in position 1: invalid start byte\r\n```\r\n\r\nIf I run the same code but with python2.7 I get the following messages in the log: \r\n\r\n````\r\n[2017-03-27 10:44:16,219: WARNING/MainProcess] Received and deleted unknown message.  Wrong destination?!?\r\n\r\nThe full contents of the message body was: body: [[], {}, {u'errbacks': None, u'callbacks': None, u'chord': None, u'chain': None}] (77b)\r\n{content_type:'application/json' content_encoding:'utf-8'\r\n  delivery_info:{u'consumer_tag': u'None4', u'redelivered': True, u'routing_key': u'celery', u'delivery_tag': 1, u'exchange': u''} headers={'\\xe5\\xca.\\xdb\\x00\\x00\\x00\\x00\\x00': None, 'P&5\\x07\\x00': None, 'T\\nKB\\x00\\x00\\x00': '10daae72-fea9-494a-a6f7-741b3d81c31c', 'N\\xfd\\x17=\\x00\\x00': 'gen27770@sieben-inria', '\\xcfb\\xddR': 'py', '9*\\xa8': None, '\\xb7/b\\x84\\x00\\x00\\x00': 0, '\\xe0\\x0b\\xfa\\x89\\x00\\x00\\x00': None, '\\xdfR\\xc4x\\x00\\x00\\x00\\x00\\x00': [None, None], 'T3\\x1d ': '6tisch/sf0/01', '\\xae\\xbf': '10daae72-fea9-494a-a6f7-741b3d81c31c', '\\x11s\\x1f\\xd8\\x00\\x00\\x00\\x00': '()', 'UL\\xa1\\xfc\\x00\\x00\\x00\\x00\\x00\\x00': '{}'}}\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3938/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dharanpdarsana": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3932", "title": "Connection to broker lost. Trying to re-establish the connection...", "body": "We are using Django Rest Framework with MongoEngine, Redis, Celery and Kombu, and we are getting the following error in our logs:\r\n\r\n`[2017-03-22 06:26:01,702: WARNING/MainProcess] consumer: Connection to broker lost. Trying to re-establish the connection...\r\nTraceback (most recent call last):\r\nFile \"/usr/local/lib/python2.7/dist-packages/celery/worker/consumer/consumer.py\", line 318, in start\r\n    blueprint.start(self)\r\nFile \"/usr/local/lib/python2.7/dist-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\nFile \"/usr/local/lib/python2.7/dist-packages/celery/worker/consumer/consumer.py\", line 594, in start\r\n    c.loop(*c.loop_args())\r\nFile \"/usr/local/lib/python2.7/dist-packages/celery/worker/loops.py\", line 88, in asynloop\r\n    next(loop)\r\nFile \"/usr/local/lib/python2.7/dist-packages/kombu/async/hub.py\", line 345, in create_loop\r\n    cb(*cbargs)\r\nFile \"/usr/local/lib/python2.7/dist-packages/kombu/transport/redis.py\", line 1039, in on_readable\r\n    self.cycle.on_readable(fileno)\r\nFile \"/usr/local/lib/python2.7/dist-packages/kombu/transport/redis.py\", line 337, in on_readable\r\n    chan.handlers[type]()\r\nFile \"/usr/local/lib/python2.7/dist-packages/kombu/transport/redis.py\", line 667, in _receive\r\n    ret.append(self._receive_one(c))\r\nFile \"/usr/local/lib/python2.7/dist-packages/kombu/transport/redis.py\", line 678, in _receive_one\r\n    response = c.parse_response()\r\nFile \"/usr/local/lib/python2.7/dist-packages/redis/client.py\", line 2183, in parse_response\r\n    return self._execute(connection, connection.read_response)\r\nFile \"/usr/local/lib/python2.7/dist-packages/redis/client.py\", line 2176, in _execute\r\n    return command(*args)\r\nFile \"/usr/local/lib/python2.7/dist-packages/redis/connection.py\", line 577, in read_response\r\n    response = self._parser.read_response()\r\nFile \"/usr/local/lib/python2.7/dist-packages/redis/connection.py\", line 238, in read_response\r\n    response = self._buffer.readline()\r\nFile \"/usr/local/lib/python2.7/dist-packages/redis/connection.py\", line 168, in readline\r\n    self._read_from_socket()\r\nFile \"/usr/local/lib/python2.7/dist-packages/redis/connection.py\", line 143, in _read_from_socket\r\n    (e.args,))\r\nConnectionError: Error while reading from socket: ('Connection closed by server.',)\r\n\r\n[2017-03-22 06:26:01,868: INFO/MainProcess] Connected to redis://:******************************/1\r\n\r\n### Versions used\r\npython==2.7.12\r\nredis==3.2.3\r\nkombu==4.0.2\r\nDjango==1.10.3\r\ncelery==4.0.2\r\namqp==2.1.1\r\nbilliard==3.5.0.2\r\npytz==2016.7\r\nDjango==1.10.3o\r\n\r\n\r\nWe have HaProxy infront of Redis cluster and connections, everything else work without any issues. Could you please help us trouble shoot this error ?\r\nOr guide on what to look for and where to look for , etc ?\r\n\r\nWe really appreciate your help,\r\n\r\nThanks,\r\nDarsana\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3932/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "aatishnn": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3930", "title": "\"Ensuring a task is only executed one at a time\" snippet keeps deleting the lock key", "body": "I am referring to the following doc:\r\nhttp://docs.celeryproject.org/en/latest/tutorials/task-cookbook.html\r\n\r\nI was going to use this snippet and realized the lock wasn't working. `memcached -vv` displayed the following:\r\n\r\n```\r\n<36 add :1:mylockkey 2 600 1 \r\n>36 STORED\r\n<36 delete :1:mylockkey\r\n>36 DELETED\r\n```\r\n\r\nAs soon as the lock is created, it is being deleted. Maybe, this line is the culprit?\r\n\r\n```\r\n# memcache delete is very slow, but we have to use it to take\r\n        # advantage of using add() for atomic locking\r\n        if monotonic() < timeout_at:\r\n```\r\n\r\n\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3930/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "draskomikic": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3921", "title": "Celery 4 worker can't connect to RabbitMQ broker failover", "body": "I have 3 RabbitMQ nodes in cluster in HA mode. Each node is on separate Docker container.\r\n\r\nI have used this command to set HA policy: \r\n```rabbitmqctl set_policy ha-all \"\" '{\"ha-mode\":\"all\",\"ha-sync-mode\":\"automatic\"}'```\r\n\r\nCelery config looks like this:\r\n```\r\nCELERY = dict(\r\n    broker_url=[\r\n        'amqp://guest@rabbitmq1:5672',\r\n        'amqp://guest@rabbitmq2:5672',\r\n        'amqp://guest@rabbitmq3:5672',\r\n    ],\r\n    celery_queue_ha_policy='all',\r\n    accept_content=['json'],\r\n    task_serializer='json',\r\n    result_serializer='json',\r\n    task_default_queue='default',\r\n    task_queues=(\r\n        Queue('default', Exchange('default')),\r\n        Queue('preprocessor', Exchange('preprocessor')),\r\n        Queue('vcp', Exchange('vcp')),\r\n        Queue('alpha', Exchange('alpha')),\r\n    ),\r\n    task_routes={\r\n        'myapp.worker.tasks.preprocess': {'queue': 'preprocessor'},\r\n        'myapp.worker.tasks.vcp': {'queue': 'vcp'},\r\n         'myapp.worker.tasks.alpha': {'queue': 'alpha'},\r\n    },\r\n    imports=[\r\n        'myapp.worker.tasks',\r\n    ]\r\n)\r\n```\r\n\r\nEverything works fine until I stop master RabbitMQ application in order to test Celery failover feature using command:\r\n```rabbitmqctl stop_app```\r\n\r\nImmediately after RabbitMQ application is stopped I started seeing errors in log bellow. Frequency of log messages is very high and it doesn't slow down with number of attempts. \r\n\r\nAccording to logs Celery tries to reconnect using next failover but it get interrupted by another try to reconnect to node that was stopped. The same thing happens over and over like in infinite loop.\r\n\r\n```\r\n[2017-03-17 15:10:28,084: ERROR/MainProcess] consumer: Cannot connect to amqp://guest:**@rabbitmq1:5672//: [Errno 111] Connection refused.\r\nWill retry using next failover.\r\n\r\n[2017-03-17 15:10:28,300: DEBUG/MainProcess] Start from server, version: 0.9, properties: {'information': 'Licensed under the MPL.  See http://www.rabbitmq.com/', 'product': 'RabbitMQ', 'copyright': 'Copyright (C) 2007-2016 Pivotal Software, Inc.', 'capabilities': {'exchange_exchange_bindings': True, 'connection.blocked': True, 'authentication_failure_close': True, 'direct_reply_to': True, 'basic.nack': True, 'per_consumer_qos': True, 'consumer_priorities': True, 'consumer_cancel_notify': True, 'publisher_confirms': True}, 'cluster_name': 'rabbit@rabbitmq1', 'platform': 'Erlang/OTP', 'version': '3.6.6'}, mechanisms: [u'PLAIN', u'AMQPLAIN'], locales: [u'en_US']\r\n[2017-03-17 15:10:28,302: DEBUG/MainProcess] ^-- substep ok\r\n[2017-03-17 15:10:28,303: DEBUG/MainProcess] | Consumer: Starting Mingle\r\n[2017-03-17 15:10:28,303: INFO/MainProcess] mingle: searching for neighbors\r\n[2017-03-17 15:10:28,303: DEBUG/MainProcess] using channel_id: 1\r\n[2017-03-17 15:10:28,318: DEBUG/MainProcess] Channel open\r\n[2017-03-17 15:10:28,470: WARNING/MainProcess] consumer: Connection to broker lost. Trying to re-establish the connection...\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/site-packages/celery/worker/consumer/consumer.py\", line 318, in start\r\n    blueprint.start(self)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery/worker/consumer/mingle.py\", line 38, in start\r\n    self.sync(c)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery/worker/consumer/mingle.py\", line 42, in sync\r\n    replies = self.send_hello(c)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery/worker/consumer/mingle.py\", line 55, in send_hello\r\n    replies = inspect.hello(c.hostname, our_revoked._data) or {}\r\n  File \"/usr/local/lib/python2.7/site-packages/celery/app/control.py\", line 129, in hello\r\n    return self._request('hello', from_node=from_node, revoked=revoked)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery/app/control.py\", line 81, in _request\r\n    timeout=self.timeout, reply=True,\r\n  File \"/usr/local/lib/python2.7/site-packages/celery/app/control.py\", line 436, in broadcast\r\n    limit, callback, channel=channel,\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/pidbox.py\", line 315, in _broadcast\r\n    serializer=serializer)\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/pidbox.py\", line 290, in _publish\r\n    serializer=serializer,\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/messaging.py\", line 181, in publish\r\n    exchange_name, declare,\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/messaging.py\", line 187, in _publish\r\n    channel = self.channel\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/messaging.py\", line 209, in _get_channel\r\n    channel = self._channel = channel()\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/utils/functional.py\", line 38, in __call__\r\n    value = self.__value__ = self.__contract__()\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/messaging.py\", line 224, in <lambda>\r\n    channel = ChannelPromise(lambda: connection.default_channel)\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/connection.py\", line 819, in default_channel\r\n    self.connection\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/connection.py\", line 802, in connection\r\n    self._connection = self._establish_connection()\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/connection.py\", line 757, in _establish_connection\r\n    conn = self.transport.establish_connection()\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/transport/pyamqp.py\", line 130, in establish_connection\r\n    conn.connect()\r\n  File \"/usr/local/lib/python2.7/site-packages/amqp/connection.py\", line 294, in connect\r\n    self.transport.connect()\r\n  File \"/usr/local/lib/python2.7/site-packages/amqp/transport.py\", line 120, in connect\r\n    self._connect(self.host, self.port, self.connect_timeout)\r\n  File \"/usr/local/lib/python2.7/site-packages/amqp/transport.py\", line 161, in _connect\r\n    self.sock.connect(sa)\r\n  File \"/usr/local/lib/python2.7/socket.py\", line 228, in meth\r\n    return getattr(self._sock,name)(*args)\r\nerror: [Errno 111] Connection refused\r\n[2017-03-17 15:10:28,508: DEBUG/MainProcess] Closed channel #1\r\n[2017-03-17 15:10:28,570: DEBUG/MainProcess] | Consumer: Restarting event loop...\r\n[2017-03-17 15:10:28,572: DEBUG/MainProcess] | Consumer: Restarting Gossip...\r\n[2017-03-17 15:10:28,575: DEBUG/MainProcess] | Consumer: Restarting Heart...\r\n[2017-03-17 15:10:28,648: DEBUG/MainProcess] | Consumer: Restarting Control...\r\n[2017-03-17 15:10:28,655: DEBUG/MainProcess] | Consumer: Restarting Tasks...\r\n[2017-03-17 15:10:28,655: DEBUG/MainProcess] Canceling task consumer...\r\n[2017-03-17 15:10:28,655: DEBUG/MainProcess] | Consumer: Restarting Mingle...\r\n[2017-03-17 15:10:28,655: DEBUG/MainProcess] | Consumer: Restarting Events...\r\n[2017-03-17 15:10:28,672: DEBUG/MainProcess] | Consumer: Restarting Connection...\r\n[2017-03-17 15:10:28,673: DEBUG/MainProcess] | Consumer: Starting Connection\r\n[2017-03-17 15:10:28,947: ERROR/MainProcess] consumer: Cannot connect to amqp://guest:**@rabbitmq1:5672//: [Errno 111] Connection refused.\r\nWill retry using next failover.\r\n\r\n[2017-03-17 15:10:29,345: DEBUG/MainProcess] Start from server, version: 0.9, properties: {'information': 'Licensed under the MPL.  See http://www.rabbitmq.com/', 'product': 'RabbitMQ', 'copyright': 'Copyright (C) 2007-2016 Pivotal Software, Inc.', 'capabilities': {'exchange_exchange_bindings': True, 'connection.blocked': True, 'authentication_failure_close': True, 'direct_reply_to': True, 'basic.nack': True, 'per_consumer_qos': True, 'consumer_priorities': True, 'consumer_cancel_notify': True, 'publisher_confirms': True}, 'cluster_name': 'rabbit@rabbitmq1', 'platform': 'Erlang/OTP', 'version': '3.6.6'}, mechanisms: [u'PLAIN', u'AMQPLAIN'], locales: [u'en_US']\r\n[2017-03-17 15:10:29,506: INFO/MainProcess] Connected to amqp://guest:**@rabbitmq2:5672//\r\n[2017-03-17 15:10:29,535: DEBUG/MainProcess] ^-- substep ok\r\n[2017-03-17 15:10:29,569: DEBUG/MainProcess] | Consumer: Starting Events\r\n[2017-03-17 15:10:29,682: ERROR/MainProcess] consumer: Cannot connect to amqp://guest:**@rabbitmq1:5672//: [Errno 111] Connection refused.\r\nWill retry using next failover.\r\n\r\n[2017-03-17 15:10:29,740: DEBUG/MainProcess] Start from server, version: 0.9, properties: {'information': 'Licensed under the MPL.  See http://www.rabbitmq.com/', 'product': 'RabbitMQ', 'copyright': 'Copyright (C) 2007-2016 Pivotal Software, Inc.', 'capabilities': {'exchange_exchange_bindings': True, 'connection.blocked': True, 'authentication_failure_close': True, 'direct_reply_to': True, 'basic.nack': True, 'per_consumer_qos': True, 'consumer_priorities': True, 'consumer_cancel_notify': True, 'publisher_confirms': True}, 'cluster_name': 'rabbit@rabbitmq1', 'platform': 'Erlang/OTP', 'version': '3.6.6'}, mechanisms: [u'PLAIN', u'AMQPLAIN'], locales: [u'en_US']\r\n[2017-03-17 15:10:29,768: DEBUG/MainProcess] ^-- substep ok\r\n[2017-03-17 15:10:29,770: DEBUG/MainProcess] | Consumer: Starting Mingle\r\n[2017-03-17 15:10:29,770: INFO/MainProcess] mingle: searching for neighbors\r\n[2017-03-17 15:10:29,771: DEBUG/MainProcess] using channel_id: 1\r\n[2017-03-17 15:10:29,795: DEBUG/MainProcess] Channel open\r\n[2017-03-17 15:10:29,874: WARNING/MainProcess] consumer: Connection to broker lost. Trying to re-establish the connection...\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/site-packages/celery/worker/consumer/consumer.py\", line 318, in start\r\n    blueprint.start(self)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery/worker/consumer/mingle.py\", line 38, in start\r\n    self.sync(c)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery/worker/consumer/mingle.py\", line 42, in sync\r\n    replies = self.send_hello(c)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery/worker/consumer/mingle.py\", line 55, in send_hello\r\n    replies = inspect.hello(c.hostname, our_revoked._data) or {}\r\n  File \"/usr/local/lib/python2.7/site-packages/celery/app/control.py\", line 129, in hello\r\n    return self._request('hello', from_node=from_node, revoked=revoked)\r\n  File \"/usr/local/lib/python2.7/site-packages/celery/app/control.py\", line 81, in _request\r\n    timeout=self.timeout, reply=True,\r\n  File \"/usr/local/lib/python2.7/site-packages/celery/app/control.py\", line 436, in broadcast\r\n    limit, callback, channel=channel,\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/pidbox.py\", line 315, in _broadcast\r\n    serializer=serializer)\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/pidbox.py\", line 290, in _publish\r\n    serializer=serializer,\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/messaging.py\", line 181, in publish\r\n    exchange_name, declare,\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/messaging.py\", line 187, in _publish\r\n    channel = self.channel\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/messaging.py\", line 209, in _get_channel\r\n    channel = self._channel = channel()\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/utils/functional.py\", line 38, in __call__\r\n    value = self.__value__ = self.__contract__()\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/messaging.py\", line 224, in <lambda>\r\n    channel = ChannelPromise(lambda: connection.default_channel)\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/connection.py\", line 819, in default_channel\r\n    self.connection\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/connection.py\", line 802, in connection\r\n    self._connection = self._establish_connection()\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/connection.py\", line 757, in _establish_connection\r\n    conn = self.transport.establish_connection()\r\n  File \"/usr/local/lib/python2.7/site-packages/kombu/transport/pyamqp.py\", line 130, in establish_connection\r\n    conn.connect()\r\n  File \"/usr/local/lib/python2.7/site-packages/amqp/connection.py\", line 294, in connect\r\n    self.transport.connect()\r\n  File \"/usr/local/lib/python2.7/site-packages/amqp/transport.py\", line 120, in connect\r\n    self._connect(self.host, self.port, self.connect_timeout)\r\n  File \"/usr/local/lib/python2.7/site-packages/amqp/transport.py\", line 161, in _connect\r\n    self.sock.connect(sa)\r\n  File \"/usr/local/lib/python2.7/socket.py\", line 228, in meth\r\n    return getattr(self._sock,name)(*args)\r\nerror: [Errno 111] Connection refused\r\n[2017-03-17 15:10:29,887: DEBUG/MainProcess] Closed channel #1\r\n[2017-03-17 15:10:29,907: DEBUG/MainProcess] | Consumer: Restarting event loop...\r\n[2017-03-17 15:10:29,908: DEBUG/MainProcess] | Consumer: Restarting Gossip...\r\n[2017-03-17 15:10:29,908: DEBUG/MainProcess] | Consumer: Restarting Heart...\r\n[2017-03-17 15:10:29,908: DEBUG/MainProcess] | Consumer: Restarting Control...\r\n[2017-03-17 15:10:29,909: DEBUG/MainProcess] | Consumer: Restarting Tasks...\r\n[2017-03-17 15:10:29,910: DEBUG/MainProcess] Canceling task consumer...\r\n[2017-03-17 15:10:29,911: DEBUG/MainProcess] | Consumer: Restarting Mingle...\r\n[2017-03-17 15:10:29,912: DEBUG/MainProcess] | Consumer: Restarting Events...\r\n[2017-03-17 15:10:29,953: DEBUG/MainProcess] | Consumer: Restarting Connection...\r\n[2017-03-17 15:10:29,954: DEBUG/MainProcess] | Consumer: Starting Connection\r\n[2017-03-17 15:10:30,036: ERROR/MainProcess] consumer: Cannot connect to amqp://guest:**@rabbitmq1:5672//: [Errno 111] Connection refused.\r\nWill retry using next failover.\r\n```\r\n\r\npip list:\r\n```\r\n...\r\namqp (2.1.4)\r\ncelery (4.0.2)\r\nflower (0.9.1)\r\nkombu (4.0.2)\r\npep8 (1.7.0)\r\npip (9.0.1)\r\nsetuptools (34.3.0)\r\nsix (1.10.0)\r\n...\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3921/reactions", "total_count": 4, "+1": 4, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jayendra13": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3919", "title": "Accessing/passing worker information inside the task", "body": "My application requires to customise the Worker. I want to bind a jupyter kernel with the celery worker, so each celery worker will have it's own jupyter kernel.\r\nLater I want to use that jupyter application to execute some arbitrary python code inside the celery task.\r\n\r\nHere is the code where I am overriding the celery worker class. am I doing this correctly or is there any better way.\r\n\r\n```python\r\nfrom __future__ import absolute_import, unicode_literals\r\nfrom celery import Celery\r\nfrom jupyter_client import MultiKernelManager\r\n\r\n\r\napp = Celery('proj',\r\n broker='redis://',\r\n backend='redis://',\r\n include=['tasks'])\r\n\r\n\r\napp.conf.update(\r\n result_expires=3600\r\n)\r\n\r\nclass CustomWorker(app.Worker):\r\n    def __init__(self, *args, **kwargs):\r\n        self.km = MultiKernelManager()\r\n        self.kernel_id = self.km.start_kernel()\r\n        print(\"Custom initializing\")\r\n        self.kernel_client = km.get_kernel(kernel_id).client()\r\n        super(CustomWorker, self).__init__(*args, **kwargs)\r\n\r\n\r\n    def on_close(self):\r\n        self.km.shutdown_kernel(self.kernel_id)\r\n        super(CustomWorker, self).on_close()\r\n\r\napp.Worker = CustomWorker\r\n\r\nif __name__ == '__main__':\r\n app.start()\r\n```\r\n\r\nhere is the my task definition, where I want to use the kernel_client inside the my task, the problem is that how can I know that custom property set for the worker inside the task while execution?\r\n\r\n```python\r\nfrom __future__ import absolute_import, unicode_literals\r\nfrom celery import app\r\n\r\n\r\nfrom celery import Task\r\nfrom tornado import gen\r\nfrom jupyter_client import MultiKernelManager\r\nfrom zmq.eventloop import ioloop\r\nfrom zmq.eventloop.zmqstream import ZMQStream\r\nioloop.install()\r\n\r\n\r\nreply_futures = {}\r\n\r\n\r\n# This is my celery task where I pass the arbitary python code to execute on\r\n# some celery worker(actually to the corresponding kernel)\r\n@app.task\r\ndef pythontask(code):\r\n    # I don't know how to get the kernel_client for current celery worker !!?\r\n    mid = kernel_client.execute(code)\r\n\r\n\r\n    # defining the callback which will be executed when message arrives on\r\n    # zmq stream\r\n    def reply_callback(session, stream, msg_list):\r\n        idents, msg_parts = session.feed_identities(msg_list)\r\n        reply = session.deserialize(msg_parts)\r\n        parent_id = reply['parent_header'].get('msg_id')\r\n        reply_future = reply_futures.get(parent_id)\r\n        if reply_future:\r\n            reply_future.set_result(reply)\r\n\r\n\r\n    @gen.coroutine\r\n    def execute(kernel_client, code):\r\n        msg_id = kernel_client.execute(code)\r\n        f = reply_futures[msg_id] = Future()\r\n        yield f\r\n        raise gen.Return(msg_id)\r\n\r\n\r\n    # initializing the zmq streams and attaching the callback to receive message\r\n    # from the kernel\r\n    shell_stream = ZMQStream(kernel_client.shell_channel.socket)\r\n    iopub_stream = ZMQStream(kernel_client.iopub_channel.socket)\r\n    shell_stream.on_recv_stream(partial(reply_callback, kernel_client.session))\r\n    iopub_stream.on_recv_stream(partial(reply_callback, kernel_client.session))\r\n\r\n\r\n    # create a IOLoop\r\n    loop = ioloop.IOLoop.current()\r\n    # listen on the streams\r\n    msg_id = loop.run_sync(lambda: execute(kernel_client,code))\r\n    print(reply_msgs[msg_id])\r\n    reply_msgs[msg_id] = []\r\n\r\n\r\n    # Disable callback and automatic receiving.\r\n    shell_stream.on_recv_stream(None)\r\n    iopub_stream.on_recv_stream(None)\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3919/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "manneorama": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3916", "title": "Celery4 + Rabbit + Redis, workers stop executing tasks while RabbitMQ disk usage skyrockets", "body": "## Checklist\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n```\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:2.7.12\r\n            billiard:3.5.0.2 py-amqp:2.1.4\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:redis://******:6379/0\r\n\r\ntask_serializer: 'json'\r\ntheoracle: <module 'theoracle' from '/******/minion/local/lib/python2.7/site-packages/theoracle/__init__.pyc'>\r\nevent_queue_ttl: 60\r\nworker_send_task_events: False\r\nenable_utc: True\r\ntimezone: 'UTC'\r\ntask_default_queue: 'default-20170313-0'\r\nresource_intensive_task: <@task: minion_worker.celery_tasks.exception_handling_task.resource_intensive_task of minion:0x7f6706dd8ed0>\r\nworker_max_tasks_per_child: 1\r\nstandard_task: <@task: minion_worker.celery_tasks.exception_handling_task.standard_task of minion:0x7f6706dd8ed0>\r\naccept_content: ['json']\r\ninclude: ['minion_worker.celery_tasks.bootstrap',\r\n 'minion_worker.celery_tasks.exception_handling_task',\r\n 'minion_worker.celery_tasks.notify_task']\r\nresult_backend: u'redis://******:6379/0'\r\nresult_serializer: 'json'\r\ntimedelta: <type 'datetime.timedelta'>\r\ntask_acks_late: True\r\ntask_routes: {\r\n    u'minion_worker.celery_tasks.bootstrap.celery_bootstrap': {   'queue': 'bootstrap.linux-20170313-0',\r\n                                                                  'routing_key': u'********'},\r\n    u'minion_worker.celery_tasks.exception_handling_task.resource_intensive_task': {   'queue': 'heavy.linux-20170313-0',\r\n                                                                                       'routing_key': u'********'},\r\n    u'minion_worker.celery_tasks.exception_handling_task.standard_task': {   'queue': 'main.linux-20170313-0',\r\n                                                                             'routing_key': u'********'},\r\n    u'minion_worker.celery_tasks.notify_task.dummy_task': {   'queue': 'bootstrap.linux-20170313-0',\r\n                                                              'routing_key': u'********'},\r\n    u'minion_worker.celery_tasks.notify_task.notify_task': {   'queue': 'bootstrap.linux-20170313-0',\r\n                                                               'routing_key': u'********'}}\r\nQueue: <class 'kombu.entity.Queue'>\r\nresult_expires: datetime.timedelta(2)\r\nnotify_task: <@task: minion_worker.celery_tasks.notify_task.notify_task of minion:0x7f6706dd8ed0>\r\ntask_queues: \r\n    (<unbound Queue default-20170313-0 -> <unbound Exchange u''(direct)> -> default-20170313-0>,\r\n <unbound Queue bootstrap.linux-20170313-0 -> <unbound Exchange minion(topic)> -> bootstrap.linux-20170313-0.#>,\r\n <unbound Queue heavy.linux-20170313-0 -> <unbound Exchange minion(topic)> -> heavy.linux-20170313-0>,\r\n <unbound Queue main.linux-20170313-0 -> <unbound Exchange minion(topic)> -> main.linux-20170313-0>)\r\ndummy_task: <@task: minion_worker.celery_tasks.notify_task.dummy_task of minion:0x7f6706dd8ed0>\r\ntask_create_missing_queues: False\r\ncelery_bootstrap: <@task: minion_worker.celery_tasks.bootstrap.celery_bootstrap of minion:0x7f6706dd8ed0>\r\nworker_prefetch_multiplier: 1\r\nworker_disable_rate_limits: True\r\nbroker_url: u'amqp://minion:********@*******:5672/minion_host'\r\nos: <module 'os' from '/******/minion/lib/python2.7/os.pyc'>\r\n```\r\n\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nWe're not quite sure how to reproduce this outside our own environment, since we don't know if the combination Celery4 + our canvas is the thing causing the troubles to begin with. Anyway, we're using a canvas that looks something like this:\r\n```\r\nchain(\r\n    chain(\r\n        group(chain(a11, b11, c11), chain(a12, b12, c12), ...) | callback1.\r\n        group(chain(a21, b21, c21), chain(a22, b22, c22), ...) | callback2\r\n    ),\r\n    notify_slack\r\n)\r\n```\r\nTogether with custom error handling since we've noticed that everything just halts completely if anything raises an exception that isn't caught before reaching Celery (something to do with the chords, it seems).\r\n\r\nAnyway, we've recently upgraded to Celery 4.0.2 from 3.1.25, and that's where the problems started. \r\n    \r\n## Expected behavior\r\nRabbitMQ and Celery play nice for extended periods of time (as was the case with Celery 3.1.25)\r\n\r\n## Actual behavior\r\nUpon a restart of the rabbit instance and all of the workers, everything works fine. But after a while, Rabbit's disk usage starts climbing (specifically the folder mnesia/minion_broker/msg_store_persistent), reaching 400 GB in just a couple of hours), while the chains stop functioning after the first group has been successfully executed.\r\n\r\nSeeing this in worker logs:\r\n`[2017-03-16 09:38:24,533: ERROR/MainProcess] Control command error: error(104, 'Connection reset by peer')`\r\n\r\nTraceback (timestamp omitted for brevity):\r\n```\r\nerror: [Errno 104] Connection reset by peer\r\n    return getattr(self._sock,name)(*args)\r\n  File \"/usr/lib/python2.7/socket.py\", line 228, in meth\r\n    self._write(s)\r\n  File \"/env/local/lib/python2.7/site-packages/amqp/transport.py\", line 269, in write\r\n    write(view[:offset])\r\n  File \"/env/local/lib/python2.7/site-packages/amqp/method_framing.py\", line 174, in write_frame\r\n    conn.frame_writer(1, self.channel_id, sig, args, content)\r\n  File \"/env/local/lib/python2.7/site-packages/amqp/abstract_channel.py\", line 64, in send_method\r\n    (0, exchange, routing_key, mandatory, immediate), msg\r\n  File \"/env/local/lib/python2.7/site-packages/amqp/channel.py\", line 1748, in _basic_publish\r\n    mandatory=mandatory, immediate=immediate,\r\n  File \"/env/local/lib/python2.7/site-packages/kombu/messaging.py\", line 203, in _publish\r\n    exchange_name, declare,\r\n  File \"/env/local/lib/python2.7/site-packages/kombu/messaging.py\", line 181, in publish\r\n    **opts\r\n  File \"/env/local/lib/python2.7/site-packages/kombu/pidbox.py\", line 265, in _publish_reply\r\n    serializer=self.mailbox.serializer)\r\n  File \"/env/local/lib/python2.7/site-packages/kombu/pidbox.py\", line 135, in reply\r\n    ticket=ticket)\r\n  File \"/env/local/lib/python2.7/site-packages/kombu/pidbox.py\", line 112, in dispatch\r\n    return self.dispatch(**body)\r\n  File \"/env/local/lib/python2.7/site-packages/kombu/pidbox.py\", line 129, in handle_message\r\n    self.node.handle_message(body, message)\r\n  File \"/env/local/lib/python2.7/site-packages/celery/worker/pidbox.py\", line 42, in on_message\r\nTraceback (most recent call last):\r\n[2017-03-16 09:38:24,533: ERROR/MainProcess] Control command error: error(104, 'Connection reset by peer')\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3916/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sklif5": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3905", "title": "Protocol example in documentation is not working.", "body": "## Checklist\r\n\r\n- [ 4.0.2]  ( the Celery\r\n       version ).\r\n- [ X] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nRun the example from the documentation [page](http://docs.celeryproject.org/en/latest/internals/protocol.html)\r\n\r\n## Expected behavior\r\nThe example should send a task\r\nchain: add(add(add(2, 2), 4), 8) == 2 + 2 + 4 + 8\r\n## Actual behavior\r\nThe code is not working\r\n1. In Python 3.5.2 uuid module does not contain uuid()\r\n2. The serialization message=json.dumps.... is not working, hence not allowing to see actual example of the message.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3905/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nigma": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3898", "title": "Celery pool with Redis broker freeze silently or crash & freeze when hiredis package is installed", "body": "## Overview\r\n\r\nCelery workers freeze silently or crash and freeze after executing `worker_max_tasks_per_child` tasks when using Redis broker and the `hiredis` (https://github.com/redis/hiredis-py) package is installed.\r\n\r\n`redis-py` is a library used by Celery to connect to Redis broker and  ships with two parser classes, the PythonParser and the HiredisParser. By default, `redis-py` will attempt to use the HiredisParser if the `hiredis` module installed and will fallback to the PythonParser otherwise.\r\n\r\nCelery is not able to recover from this error and restart broker connection, which is causing permanent deadlock.\r\n\r\n**Note**: this as well may be a problem with `redis-py` and `hiredis` packages. I'm filling in this issue because it took me some time to reproduce and narrow down the problem (previous report in https://github.com/celery/celery/issues/2464#issuecomment-274998448) and I see a couple similar issues reported that may be caused by this conflict rather than contributed by AWS Elastic Beanstalk / AWS ElastiCache Redis setup.\r\n\r\n## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n\r\n```\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:3.6.0\r\n            billiard:3.5.0.2 redis:2.10.5\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis://redis:6379/2\r\n\r\naccept_content: ['json']\r\nbroker_connection_max_retries: 100\r\nbroker_connection_retry: True\r\nbroker_connection_timeout: 4\r\nbroker_heartbeat: 20\r\nbroker_pool_limit: 10\r\nbroker_transport_options: {\r\n 'socket_keepalive': True, 'socket_timeout': 90}\r\nbroker_url: 'redis://redis:6379/1'\r\nredis_max_connections: 10\r\nredis_socket_connect_timeout: None\r\nredis_socket_timeout: 10\r\nresult_backend: 'redis://redis:6379/2'\r\nresult_expires: datetime.timedelta(1)\r\ntask_acks_late: True\r\ntask_compression: 'gzip'\r\ntask_ignore_result: True\r\ntask_publish_retry: True\r\ntask_serializer: 'json'\r\ntask_time_limit: 600.0\r\ntask_track_started: True\r\nworker_concurrency: 5\r\nworker_disable_rate_limits: False\r\nworker_hijack_root_logger: True\r\nworker_lost_wait: 10.0\r\nworker_max_memory_per_child: None\r\nworker_max_tasks_per_child: 10\r\nworker_pool: 'prefork'\r\nworker_prefetch_multiplier: 1\r\nworker_send_task_events: True\r\n```\r\n\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Environment\r\n\r\n1. Celery 4.0.2 from release or master branch (see `requirements.txt`)\r\n2. Redis 3.2.4 or 3.2.8 or AWS ElastiCache Redis\r\n3. `redis` (redis-py) package 2.10.5 or `8474249` (current master) \r\n4. `hiredis` package 0.2.0 or `5cfdf41` (current master) \r\n5. Configuration - https://github.com/nigma/celery-worker-deadlock-debug/blob/master/main.py\r\n6. Command line:\r\n\r\n```\r\ncelery worker --app main --hostname default@%h --events --loglevel info -O fair\r\n```\r\n\r\n7. requirements.txt\r\n\r\n```\r\n#celery[redis,tblib]==4.0.2\r\n\r\ngit+https://github.com/celery/celery.git@144f88b4e1be21780e737d4be5a734b19f1cf511\r\ngit+https://github.com/mher/flower.git@3c5f1f15dc6db018c36cf80df443a4c11f4cd149\r\n\r\nkombu==4.0.2\r\nbilliard==3.5.0.2\r\nanyjson==0.3.3\r\namqp==2.1.4\r\nvine==1.1.3\r\nsetproctitle==1.1.10\r\ntblib==1.3.0\r\n\r\nredis==2.10.5\r\n#git+https://github.com/andymccurdy/redis-py.git@84742495fd952f878fa6d0725d03d867ec39cd5\r\n\r\nhiredis==0.2.0\r\n#git+https://github.com/redis/hiredis-py.git\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1. Open the sample project from https://github.com/nigma/celery-worker-deadlock-debug\r\n2. Build and run the project using `docker-compose` to reproduce the exact environment and behaviour. Execute `make clean && make run`.\r\n3. Repeat several times to observe several kinds of freezes, crashes and stack tracebacks.\r\n\r\n## Expected behavior\r\n\r\nCelery pool workers do not freeze or crash when `redis-py` is using `hiredis` parser when connecting to Redis broker.\r\n\r\n## Actual behavior\r\n\r\n1. Celery worker pool freezes and gets offline without any error message after reaching `worker_max_tasks_per_child` processed tasks.\r\n2. Celery freezes with 100% CPU usage when workers try to exit after reaching `worker_max_tasks_per_child` processed tasks.\r\n    - `redis.exceptions.ConnectionError: Error 32 while writing to socket. Broken pipe.`\r\n3. Worker pool gets offline after workers reach `worker_max_tasks_per_child` processed tasks, resumes after broker connection timeout and then crashes after next batch of tasks:\r\n    - Connection to broker lost / `BrokenPipeError`\r\n    - `billiard.exceptions.WorkerLostError: Worker exited prematurely: exitcode 155.`\r\n    - `redis.exceptions.ConnectionError: Error while reading from socket: ('Connection closed by server.',)`\r\n    - `redis.exceptions.TimeoutError: Timeout reading from socket`\r\n    - `redis.exceptions.ConnectionError: Error 32 while writing to socket. Broken pipe.`\r\n    - Kombu: `Unrecoverable error: AttributeError(\"'NoneType' object has no attribute 'fileno'\",)`\r\n\r\nAbove problems are not observed when `hiredis` package is not present in the system.\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3898/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cryptogun": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3893", "title": "Die of RABBITMQ address would cause error to celery, celerybeat and CPU 100% usage", "body": "I found that my laptop fan is loud and CPU reached 100% at 2017-03-06 04:09. And there are 2 errors in my celery/celerybeat logs: `ERROR: Pidfile (celerybeat.pid) already exists.` and `ERROR/MainProcess] consumer: Cannot connect to amqp:`.\r\nI think `supervisor` starting `celerybeat` repeatedly is the cause of high CPU usage.\r\nOkay, now I know what happened:\r\n- Broker: amqp:// 192.168.1.2 (is localhost)\r\n- Power cut off, router's shutdown. So no 192.168.1.x assigned.\r\n- Laptop has battery.\r\n- Next task scheduling.\r\n\r\nThen celery `cannot connect to amqp:`. But why celerybeat keeps `celerybeat.pid` file after unable to connect to broker and exited?\r\nHere's the detail:\r\n\r\n```log\r\n[2017-03-05 17:17:01 +0800] [977] [INFO] Starting gunicorn 19.6.0\r\n[2017-03-05 17:17:22,607: INFO/MainProcess] beat: Starting...\r\n[2017-03-05 17:17:22,841: INFO/MainProcess] Connected to amqp://site:**@192.168.1.2:5672/site_vhost\r\n[2017-03-05 17:17:22,934: INFO/MainProcess] mingle: searching for neighbors\r\n\r\n# 3 crontab tasks (one 0,10,20,30,40,50 * * * *, two 1,11,21,31,41,51 * * * *)\r\n[2017-03-05 17:30:00,002: INFO/MainProcess] Scheduler: Sending due task TASK_A (homepage.tasks.task_a)\r\n[2017-03-05 17:30:00,006: INFO/MainProcess] Received task: homepage.tasks.task_a[86a4f8fe-7e8d-4526-9ae8-0016bd7a2348]  \r\n[2017-03-05 17:30:00,178: INFO/PoolWorker-4] Task homepage.tasks.task_a[86a4f8fe-7e8d-4526-9ae8-0016bd7a2348] succeeded in 0.0022326859999566295s: 'Already done pickday.'\r\n[2017-03-05 17:31:00,002: INFO/MainProcess] Scheduler: Sending due task TASK_B (homepage.tasks.task_b)\r\n[2017-03-05 17:31:00,005: INFO/MainProcess] Received task: homepage.tasks.task_b[fd3f72c4-8c49-46ec-949a-9b0a39aa4c95]  \r\n[2017-03-05 17:31:00,008: INFO/MainProcess] Scheduler: Sending due task TASK_C (homepage.tasks.task_c)\r\n[2017-03-05 17:31:00,012: INFO/MainProcess] Received task: homepage.tasks.task_c[4dcce5bd-40b2-466e-a1df-983f2d3e672f]  \r\n\r\n[2017-03-05 17:31:00,119: INFO/PoolWorker-4] Task homepage.tasks.task_b[fd3f72c4-8c49-46ec-949a-9b0a39aa4c95] succeeded in 0.001223967999976594s: 'Already done cart.'\r\n[2017-03-05 17:31:00,122: INFO/PoolWorker-4] Task homepage.tasks.task_c[4dcce5bd-40b2-466e-a1df-983f2d3e672f] succeeded in 0.0005572640000082174s: 'Already done goodtopic (create threads).'\r\n\r\n--------------------------------------------------------------\r\n[2017-03-05 17:40:00,004: INFO/MainProcess] Scheduler: Sending due task TASK_A (homepage.tasks.task_a)\r\nERROR: Pidfile (celerybeat.pid) already exists.\r\nSeems we're already running? (pid: 1376)\r\ncelery beat v4.0.2 (latentcall) is starting.\r\n\r\n# Above three outputs loop forever... and no other outputs.\r\n--------------------------------------------------------------\r\n[2017-03-05 17:40:00,010: INFO/MainProcess] Received task: homepage.tasks.task_a[70f1e3bb-3975-4c04-a523-817cafde5718]  \r\n[2017-03-05 17:40:00,473: INFO/PoolWorker-5] Task homepage.tasks.task_a[70f1e3bb-3975-4c04-a523-817cafde5718] succeeded in 0.003183575000093697s: 'Already done pickday.'\r\n[2017-03-05 17:40:09,530: INFO/MainProcess] missed heartbeat from celery_default@laptop\r\n[2017-03-05 17:40:09,614: INFO/MainProcess] missed heartbeat from celery_pdf@laptop\r\n=============================\r\n[2017-03-05 17:41:07,421: WARNING/MainProcess] consumer: Connection to broker lost. Trying to re-establish the connection...\r\nTraceback (most recent call last):\r\n  File \"/home/a/Desktop/sitesite/siteenv/lib/python3.5/site-packages/celery/worker/consumer/consumer.py\", line 318, in start\r\n    blueprint.start(self)\r\n  File \"/home/a/Desktop/sitesite/siteenv/lib/python3.5/site-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/home/a/Desktop/sitesite/siteenv/lib/python3.5/site-packages/celery/worker/consumer/consumer.py\", line 594, in start\r\n    c.loop(*c.loop_args())\r\n  File \"/home/a/Desktop/sitesite/siteenv/lib/python3.5/site-packages/celery/worker/loops.py\", line 88, in asynloop\r\n    next(loop)\r\n  File \"/home/a/Desktop/sitesite/siteenv/lib/python3.5/site-packages/kombu/async/hub.py\", line 345, in create_loop\r\n    cb(*cbargs)\r\n  File \"/home/a/Desktop/sitesite/siteenv/lib/python3.5/site-packages/kombu/transport/base.py\", line 236, in on_readable\r\n    reader(loop)\r\n  File \"/home/a/Desktop/sitesite/siteenv/lib/python3.5/site-packages/kombu/transport/base.py\", line 218, in _read\r\n    drain_events(timeout=0)\r\n  File \"/home/a/Desktop/sitesite/siteenv/lib/python3.5/site-packages/amqp/connection.py\", line 464, in drain_events\r\n    return self.blocking_read(timeout)\r\n  File \"/home/a/Desktop/sitesite/siteenv/lib/python3.5/site-packages/amqp/connection.py\", line 468, in blocking_read\r\n    frame = self.transport.read_frame()\r\n  File \"/home/a/Desktop/sitesite/siteenv/lib/python3.5/site-packages/amqp/transport.py\", line 237, in read_frame\r\n    frame_header = read(7, True)\r\n  File \"/home/a/Desktop/sitesite/siteenv/lib/python3.5/site-packages/amqp/transport.py\", line 377, in _read\r\n    s = recv(n - len(rbuf))\r\nTimeoutError: [Errno 110] Connection timed out\r\n[2017-03-05 17:41:07,448: ERROR/MainProcess] consumer: Cannot connect to amqp://site:**@192.168.1.2:5672/site_vhost: [Errno 101] Network is unreachable.\r\nTrying again in 2.00 seconds...\r\n=============================\r\n[2017-03-05 17:41:14,640: WARNING/MainProcess] consumer: Connection to broker lost. Trying to re-establish the connection...\r\nTimeoutError: [Errno 110] Connection timed out\r\nTrying again in 32.00 seconds...\r\n\r\n\r\n\r\n[2017-03-05 17:45:15,297: ERROR/MainProcess] consumer: Cannot connect to amqp://site:**@192.168.1.2:5672/site_vhost: [Errno 101] Network is unreachable.\r\n[2017-03-05 17:45:15 +0800] [977] [INFO] Handling signal: term\r\nworker: Warm shutdown (MainProcess)\r\nworker: Warm shutdown (MainProcess)\r\n\r\n\r\n[2017-03-05 18:32:26 +0800] [868] [INFO] Starting gunicorn 19.6.0\r\n[2017-03-05 18:32:38,359: ERROR/MainProcess] consumer: Cannot connect to amqp://site:**@192.168.1.2:5672/site_vhost: [Errno 101] Network is unreachable.\r\nTrying again in 2.00 seconds...\r\n...\r\n[2017-03-05 18:32:58,462: INFO/MainProcess] Connected to amqp://site:**@192.168.1.2:5672/site_vhost\r\n[2017-03-05 18:32:58,488: INFO/MainProcess] mingle: searching for neighbors\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3893/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "fingon": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3862", "title": "plain celery.beat.Scheduler is not functional", "body": "## Checklist\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n 4.0.2, also master\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n ^ based on code review, code hasn't changed\r\n\r\n## Steps to reproduce\r\n\r\ncreate a project that has scheduled tasks, do --scheduler celery.beat.Scheduler -B instead of -B when starting a worker\r\n\r\n## Expected behavior\r\n\r\nget running periodic tasks\r\n\r\n## Actual behavior\r\n\r\nperiodic tasks are not run at all.\r\n\r\nThe reason for this is relatively obvious: https://github.com/celery/celery/blob/master/celery/beat.py#L458\r\n\r\nThe beat_schedule is merged in only in PersistentScheduler and not the superclass.\r\n\r\nin my own workaround 'FixedScheduler', I simply call         self.merge_inplace(self.app.conf.beat_schedule) and then Scheduler's own setup_schedule.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3862/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Taywee": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3851", "title": "celery.beat.Scheduler._when wrong result for timezone-aware datetimes", "body": "[celery.beat.Scheduler._when](https://github.com/celery/celery/blob/master/celery/beat.py#L235) reduces into a timetuple ignoring tzinfo.  If you have multiple schedules with different tzinfos, the ones in later-shifted timezones will possibly never run, because they will almost always be at the bottom of the priority queue.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3851/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/462d4e57f3825b2c9180f24660bc87115b658ed1", "message": "celery.schedule.crontab: fix reduce (#3826) (#3827)\n\ncelery.schedule.crontab wasn't storing its kwargs with __reduce__ and therefore\r\nwasn't unpickling correctly.  Noteably, nowfun wasn't being wrapped into it."}, {"url": "https://api.github.com/repos/celery/celery/commits/97597f443a0ae92fd6886ea3ebfebeb1bd3ae357", "message": "fix maybe_make_aware (#3850)\n\n* celery.utils.time: fix maybe_make_aware (#3849)\r\n\r\n* t.unit.utils.test_time: add asserts for maybe_make_aware change (#3849)"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3852", "title": "celery.beat.Scheduler: fix _when to make tz-aware (#3851)", "body": "Fixes #3851 ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/100966725", "body": "I figured an equality check was better in case you pass in a callable (such as a \"timezone now\" class which stores a timezone attribute and returns the current time with `__call__` based on its stored timezone).  `is` will fail after such an object is pickled and unpickled, even if it evaluates as equal.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/100966725/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/100970417", "body": "I'm not too worried about code equality for functions, more so that the nowfun will evaluate equivalent and operate the same after being pickled and unpickled.  It wouldn't work for lambdas.  General functions will pickle and unpickle in a way that preserves equality.\r\n\r\nNote that celery beat, on startup, pickles and unpickles all schedules, so before this, nowfun is entirely nonfunctional in general.  I couldn't find a way that a passed in nowfun could be preserved and used after celery beat's startup.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/100970417/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/101415120", "body": "What I meant is that before this pull request, in celery beat, nowfun does not work at all.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/101415120/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "radiac": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3843", "title": "Loader still references djcelery", "body": "This is causing problems under certain circumstances in the latest version of Celery 4. The offending code exists in the `master` branch of Celery.\r\n\r\nThe loader still references `djcelery`:\r\n\r\n* https://github.com/celery/celery/blob/master/celery/loaders/__init__.py#L15\r\n\r\nThis has caused several obscure errors for me, but this seems to be a reliable way to recreate the problem when added to the relevant part of a django project's `celery.py`:\r\n\r\n```\r\napp.autodiscover_tasks()\r\nfrom celery.task.control import inspect\r\ninspect()```\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3843/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "linar-jether": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3832", "title": "Manually acknowledge tasks", "body": "Since setting the **acks_late** or **task_acks_late** for a specific task signature ( i.e. `task_name.s(arg1,arg2).set(queue='test', acks_late=False, task_acks_late=False)`)  \r\ndoes not seem to work...\r\n\r\nUsing the redis backend, is it possible to manually acknowledge a task? (Without raising Ignore() as i still want the task to continue running)\r\n\r\nI'm asking because I want to disable the acks_late for certain long running tasks so they won't be retried when the visibility_timeout occurs \r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3832/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "br1anjtuck": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3810", "title": "Group of Chains Exception/Error Propagation", "body": "## Checklist\r\n-  I'm running Celery 4.0.2\r\n\r\n## Steps to reproduce\r\ntasks.py\r\n```\r\n@shared_task\r\ndef add(x,y):\r\n    return x+y\r\n\r\n@shared_task\r\ndef raising_exception():\r\n    raise RuntimeError(\"BLAH\")\r\n```\r\ntest.py\r\n```\r\nfrom celery import chord, group\r\nfrom my_app.tasks import xsum, raising_exception\r\n\r\ntasks = group(add.si(1,2), raising_exception.si() | add.si(1,1) )\r\nresult = tasks.apply_async()\r\n\r\ntry:\r\n    result.get()\r\nexcept Exception as ex:\r\n    raise ex\r\n```\r\nRun the following:\r\ncelery worker -A grizzly_project -c 8 &\r\npython manage.py runserver\r\n\r\nthen in another terminal, run:\r\npython test.py\r\n## Expected behavior\r\nWhen the \"raising_exception\" task fails, result.get() should propagate one task back in the chain from the last task (for the second part of the group) to receive the exception: \" RuntimeError: BLAH\" thus allowing me to catch the Exception similar to below:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 15, in <module>\r\n    raise ex\r\ncelery.backends.base.RuntimeError: BLAH\r\n```\r\nThis is the expected behaviour, according to the back propagation of errors in a chain that was implemented in the closing of Issue [1014](https://github.com/celery/celery/issues/1014).\r\n\r\n## Actual behavior\r\nRather, when the result.get() statement executes, the program hangs. \r\n\r\nWhen I change the group statement from the above code to this such that the failing task is the last to execute:\r\n```\r\ntasks = group(add.si(1,2), add.si(1,1) | raising_exception.si() )\r\n```\r\n...the result.get() doesn't hang but rather the Exception is caught, as expected \r\n\r\nAlso, if i simply make it just a chain (rather than a group of chains) with the failing task first:\r\n```\r\ntasks = raising_exception.si() | add.si(1,1)\r\n```\r\n...result.get() doesn't hang but accurately catches the Exception (granted a wait a second or two before executing result.get())\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sww": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3802", "title": "Successful tasks are not acked if acks_late=True on warm shutdown.", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n\r\n```\r\n$ celery -A app report\r\n\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:3.4.3\r\n            billiard:3.5.0.2 redis:2.10.5\r\nplatform -> system:Darwin arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:disabled\r\n\r\nbroker_url: 'redis://127.0.0.1:6379/3'\r\n```\r\n\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n\r\nTasks that finish and succeed after the warm shutdown signal are not acked if `acks_late=True`. To reproduce:\r\n\r\n1. Create `app.py`:\r\n```python\r\nfrom time import sleep\r\n\r\nfrom celery import Celery\r\n\r\napp = Celery('foo', broker='redis://127.0.0.1:6379/3')\r\n\r\n@app.task(acks_late=True)\r\ndef foo():\r\n    sleep(5)\r\n    return 'foo'\r\n```\r\n2. Queue up a task:\r\n```bash\r\n$ python -c \"from app import foo; foo.apply_async()\"\r\n```\r\n3. Start the worker and issue a warm shutdown (CTRL+C) before the task finishes.\r\n```\r\n$ celery -A app worker --loglevel=debug\r\n[2017-01-30 16:23:46,092: DEBUG/MainProcess] | Worker: Preparing bootsteps.\r\n[2017-01-30 16:23:46,094: DEBUG/MainProcess] | Worker: Building graph...\r\n[2017-01-30 16:23:46,094: DEBUG/MainProcess] | Worker: New boot order: {Timer, Hub, Pool, Autoscaler, StateDB, Beat, Consumer}\r\n[2017-01-30 16:23:46,108: DEBUG/MainProcess] | Consumer: Preparing bootsteps.\r\n[2017-01-30 16:23:46,109: DEBUG/MainProcess] | Consumer: Building graph...\r\n[2017-01-30 16:23:46,124: DEBUG/MainProcess] | Consumer: New boot order: {Connection, Events, Mingle, Gossip, Agent, Tasks, Control, Heart, event loop}\r\n\r\n\r\n\r\ncelery@foobar v4.0.2 (latentcall)\r\n\r\nDarwin-16.3.0-x86_64-i386-64bit 2017-01-30 16:23:46\r\n\r\n[config]\r\n.> app:         foo:0x1034ddc88\r\n.> transport:   redis://127.0.0.1:6379/3\r\n.> results:     disabled://\r\n.> concurrency: 4 (prefork)\r\n.> task events: OFF (enable -E to monitor tasks in this worker)\r\n\r\n[queues]\r\n.> celery           exchange=celery(direct) key=celery\r\n\r\n\r\n[tasks]\r\n  . app.foo\r\n  . celery.accumulate\r\n  . celery.backend_cleanup\r\n  . celery.chain\r\n  . celery.chord\r\n  . celery.chord_unlock\r\n  . celery.chunks\r\n  . celery.group\r\n  . celery.map\r\n  . celery.starmap\r\n\r\n[2017-01-30 16:23:46,139: DEBUG/MainProcess] | Worker: Starting Hub\r\n[2017-01-30 16:23:46,139: DEBUG/MainProcess] ^-- substep ok\r\n[2017-01-30 16:23:46,140: DEBUG/MainProcess] | Worker: Starting Pool\r\n[2017-01-30 16:23:46,248: DEBUG/MainProcess] ^-- substep ok\r\n[2017-01-30 16:23:46,249: DEBUG/MainProcess] | Worker: Starting Consumer\r\n[2017-01-30 16:23:46,250: DEBUG/MainProcess] | Consumer: Starting Connection\r\n[2017-01-30 16:23:46,268: INFO/MainProcess] Connected to redis://127.0.0.1:6379/3\r\n[2017-01-30 16:23:46,268: DEBUG/MainProcess] ^-- substep ok\r\n[2017-01-30 16:23:46,269: DEBUG/MainProcess] | Consumer: Starting Events\r\n[2017-01-30 16:23:46,278: DEBUG/MainProcess] ^-- substep ok\r\n[2017-01-30 16:23:46,278: DEBUG/MainProcess] | Consumer: Starting Mingle\r\n[2017-01-30 16:23:46,278: INFO/MainProcess] mingle: searching for neighbors\r\n[2017-01-30 16:23:47,309: INFO/MainProcess] mingle: all alone\r\n[2017-01-30 16:23:47,309: DEBUG/MainProcess] ^-- substep ok\r\n[2017-01-30 16:23:47,309: DEBUG/MainProcess] | Consumer: Starting Gossip\r\n[2017-01-30 16:23:47,316: DEBUG/MainProcess] ^-- substep ok\r\n[2017-01-30 16:23:47,316: DEBUG/MainProcess] | Consumer: Starting Tasks\r\n[2017-01-30 16:23:47,320: DEBUG/MainProcess] ^-- substep ok\r\n[2017-01-30 16:23:47,320: DEBUG/MainProcess] | Consumer: Starting Control\r\n[2017-01-30 16:23:47,325: DEBUG/MainProcess] ^-- substep ok\r\n[2017-01-30 16:23:47,325: DEBUG/MainProcess] | Consumer: Starting Heart\r\n[2017-01-30 16:23:47,329: DEBUG/MainProcess] ^-- substep ok\r\n[2017-01-30 16:23:47,329: DEBUG/MainProcess] | Consumer: Starting event loop\r\n[2017-01-30 16:23:47,329: INFO/MainProcess] celery@foobar ready.\r\n[2017-01-30 16:23:47,330: DEBUG/MainProcess] | Worker: Hub.register Pool...\r\n[2017-01-30 16:23:47,330: DEBUG/MainProcess] basic.qos: prefetch_count->16\r\n[2017-01-30 16:23:48,093: INFO/MainProcess] Received task: app.foo[a1d83951-0e24-488b-b400-0ddbd4161254]\r\n[2017-01-30 16:23:48,093: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x10357c598> (args:('app.foo', 'a1d83951-0e24-488b-b400-0ddbd4161254', {'kwargsrepr': '{}', 'id': 'a1d83951-0e24-488b-b400-0ddbd4161254', 'task': 'app.foo', 'expires': None, 'reply_to': '6de20f21-2460-3189-b534-29ae63c15d1c', 'lang': 'py', 'group': None, 'argsrepr': '()', 'eta': None, 'retries': 0, 'delivery_info': {'exchange': '', 'redelivered': None, 'routing_key': 'celery', 'priority': 0}, 'root_id': 'a1d83951-0e24-488b-b400-0ddbd4161254', 'correlation_id': 'a1d83951-0e24-488b-b400-0ddbd4161254', 'origin': 'gen52076@foobar', 'timelimit': [None, None], 'parent_id': None}, b'[[], {}, {\"chord\": null, \"callbacks\": null, \"chain\": null, \"errbacks\": null}]', 'application/json', 'utf-8') kwargs:{})\r\n[2017-01-30 16:23:48,095: DEBUG/MainProcess] Task accepted: app.foo[a1d83951-0e24-488b-b400-0ddbd4161254] pid:52112\r\n^C\r\nworker: Hitting Ctrl+C again will terminate all running tasks!\r\n\r\nworker: Warm shutdown (MainProcess)\r\n[2017-01-30 16:23:49,613: DEBUG/MainProcess] | Worker: Closing Hub...\r\n[2017-01-30 16:23:49,613: DEBUG/MainProcess] | Worker: Closing Pool...\r\n[2017-01-30 16:23:49,614: DEBUG/MainProcess] | Worker: Closing Consumer...\r\n[2017-01-30 16:23:49,614: DEBUG/MainProcess] | Worker: Stopping Consumer...\r\n[2017-01-30 16:23:49,614: DEBUG/MainProcess] | Consumer: Closing Connection...\r\n[2017-01-30 16:23:49,614: DEBUG/MainProcess] | Consumer: Closing Events...\r\n[2017-01-30 16:23:49,614: DEBUG/MainProcess] | Consumer: Closing Mingle...\r\n[2017-01-30 16:23:49,614: DEBUG/MainProcess] | Consumer: Closing Gossip...\r\n[2017-01-30 16:23:49,614: DEBUG/MainProcess] | Consumer: Closing Tasks...\r\n[2017-01-30 16:23:49,615: DEBUG/MainProcess] | Consumer: Closing Control...\r\n[2017-01-30 16:23:49,615: DEBUG/MainProcess] | Consumer: Closing Heart...\r\n[2017-01-30 16:23:49,615: DEBUG/MainProcess] | Consumer: Closing event loop...\r\n[2017-01-30 16:23:49,615: DEBUG/MainProcess] | Consumer: Stopping event loop...\r\n[2017-01-30 16:23:49,615: DEBUG/MainProcess] | Consumer: Stopping Heart...\r\n[2017-01-30 16:23:49,617: DEBUG/MainProcess] | Consumer: Stopping Control...\r\n[2017-01-30 16:23:49,620: DEBUG/MainProcess] | Consumer: Stopping Tasks...\r\n[2017-01-30 16:23:49,620: DEBUG/MainProcess] Canceling task consumer...\r\n[2017-01-30 16:23:49,621: DEBUG/MainProcess] | Consumer: Stopping Gossip...\r\n[2017-01-30 16:23:49,624: DEBUG/MainProcess] | Consumer: Stopping Mingle...\r\n[2017-01-30 16:23:49,624: DEBUG/MainProcess] | Consumer: Stopping Events...\r\n[2017-01-30 16:23:49,624: DEBUG/MainProcess] | Consumer: Stopping Connection...\r\n[2017-01-30 16:23:49,625: DEBUG/MainProcess] | Worker: Stopping Pool...\r\n[2017-01-30 16:23:53,099: INFO/PoolWorker-2] Task app.foo[a1d83951-0e24-488b-b400-0ddbd4161254] succeeded in 5.002987815998495s: 'foo'\r\n[2017-01-30 16:23:54,103: DEBUG/MainProcess] | Worker: Stopping Hub...\r\n[2017-01-30 16:23:54,103: DEBUG/MainProcess] | Consumer: Shutdown Heart...\r\n[2017-01-30 16:23:54,104: DEBUG/MainProcess] | Consumer: Shutdown Control...\r\n[2017-01-30 16:23:54,104: DEBUG/MainProcess] | Consumer: Shutdown Tasks...\r\n[2017-01-30 16:23:54,104: DEBUG/MainProcess] Canceling task consumer...\r\n[2017-01-30 16:23:54,104: DEBUG/MainProcess] Closing consumer channel...\r\n[2017-01-30 16:23:54,104: DEBUG/MainProcess] | Consumer: Shutdown Gossip...\r\n[2017-01-30 16:23:54,104: DEBUG/MainProcess] | Consumer: Shutdown Events...\r\n[2017-01-30 16:23:54,105: DEBUG/MainProcess] | Consumer: Shutdown Connection...\r\n[2017-01-30 16:23:54,106: WARNING/MainProcess] Restoring 1 unacknowledged message(s)\r\n[2017-01-30 16:23:54,111: DEBUG/MainProcess] removing tasks from inqueue until task handler finished\r\n```\r\n(Note the second to last line even after the log says the task succeded.)\r\n\r\n## Expected behavior\r\n\r\nThe task is acked before exit.\r\n\r\n## Actual behavior\r\n\r\nThe task is not acked and restored. The next time the worker starts up, the same task will be executed again.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3802/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "markine": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3796", "title": "Double execution of tasks with warm shutdown & AMQP", "body": "## Checklist\r\n\r\n- [X] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n\r\n```\r\nsoftware -> celery:4.0.0rc7 (0today8) kombu:4.0.2 py:2.7.12+\r\n            billiard:3.5.0.2 py-amqp:2.1.4\r\nplatform -> system:Linux arch:64bit imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\nsettings -> transport:amqp results:disabled\r\n```\r\n\r\nWorker info:\r\n\r\n```\r\n -------------- celery@Mikhails-MacBook-Pro.local v4.0.0rc7 (0today8)\r\n---- **** ----- \r\n--- * ***  * -- Darwin-16.3.0-x86_64-i386-64bit 2017-01-26 17:56:45\r\n-- * - **** --- \r\n- ** ---------- [config]\r\n- ** ---------- .> app:         XXXXXXXSNIPXXXXXXX\r\n- ** ---------- .> transport:   amqp://XXXXXXXSNIPXXXXXXX\r\n- ** ---------- .> results:     disabled://\r\n- *** --- * --- .> concurrency: 4 (gevent)\r\n-- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\r\n--- ***** ----- \r\n -------------- [queues]\r\n                .> celery           exchange=celery(direct) key=celery\r\n```\r\n\r\nWe're launching Celery workers via Django. Late acks are disabled.\r\n\r\n\r\n- [Not yet] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n\r\nWrite a test Celery task that logs, sleeps for 5 seconds, logs again and returns.\r\nLoad up the AMQP with, say 10 test tasks.\r\nLaunch the worker, let a few tasks finish, send a SIGTERM to trigger a warm shutdown.\r\nLaunch the worker again and let it finish all remaining tasks.\r\nStudy logs for task IDs.\r\n\r\n## Expected behavior\r\n\r\nAny particular task ID should only complete once in either the first or the second execution of the worker.\r\n\r\n## Actual behavior\r\n\r\nSome task IDs complete during warm shutdown of the first execution and show up again in the second execution of the worker.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3796/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alanhamlett": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3773", "title": "Couldn't ack, reason: BrokenPipeError(32, 'Broken pipe')", "body": "## Checklist\r\n\r\n- [X] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n```\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:3.4.3\r\n            billiard:3.5.0.2 py-amqp:2.1.4\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:disabled\r\n```\r\n\r\n- [X] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nUpgrade to Celery 4.0.2.\r\n\r\n## Expected behavior\r\nNo error.\r\n\r\n## Actual behavior\r\nGetting this error from my workers:\r\n\r\n```\r\n[2017-01-19 04:07:57,411: CRITICAL/MainProcess] Couldn't ack 461, reason:BrokenPipeError(32, 'Broken pipe')\r\nTraceback (most recent call last):\r\n  File \"/opt/app/current/venv/lib/python3.4/site-packages/kombu/message.py\", line 130, in ack_log_error\r\n    self.ack(multiple=multiple)\r\n  File \"/opt/app/current/venv/lib/python3.4/site-packages/kombu/message.py\", line 125, in ack\r\n    self.channel.basic_ack(self.delivery_tag, multiple=multiple)\r\n  File \"/opt/app/current/venv/lib/python3.4/site-packages/amqp/channel.py\", line 1408, in basic_ack\r\n    spec.Basic.Ack, argsig, (delivery_tag, multiple),\r\n  File \"/opt/app/current/venv/lib/python3.4/site-packages/amqp/abstract_channel.py\", line 64, in send_method\r\n    conn.frame_writer(1, self.channel_id, sig, args, content)\r\n  File \"/opt/app/current/venv/lib/python3.4/site-packages/amqp/method_framing.py\", line 174, in write_frame\r\n    write(view[:offset])\r\n  File \"/opt/app/current/venv/lib/python3.4/site-packages/amqp/transport.py\", line 269, in write\r\n    self._write(s)\r\nBrokenPipeError: [Errno 32] Broken pipe\r\n```\r\n\r\nThis causes the worker to reset it's connection to RabbitMQ every few seconds, and sometimes leads to hitting the max connection limit allowed on the RabbitMQ server.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3773/reactions", "total_count": 11, "+1": 11, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/4d63867c8281e94c74dcdf84fe2a441eaed6671b", "message": "prevent consuming queue before ready on startup (#3752)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ryanhiebert": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3767", "title": "task delivery_info not matching any active_queues", "body": "## Checklist\r\n\r\n\u2713 I have included the output of ``celery -A proj report`` in the issue.\r\n\u2713 I have verified that the issue exists against the `master` branch of Celery.\r\n\r\nOutput of ``celery -A tasks report``:\r\n\r\n```\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:3.6.0\r\n            billiard:3.5.0.2 py-amqp:2.1.4\r\nplatform -> system:Darwin arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:disabled\r\n```\r\n\r\n## Steps to reproduce\r\n\r\nPut this into `tasks.py`:\r\n\r\n```python\r\nimport time, celery\r\napp = celery.Celery('tasks')\r\n@app.task\r\ndef wait(secs):\r\n    time.sleep(secs)\r\n```\r\n\r\nRun `celery -A tasks worker --loglevel=info`.\r\n\r\nIn another terminal window, see the following Python Console session:\r\n\r\n```pycon\r\nPython 3.6.0 (default, Jan  5 2017, 23:49:18)\r\n[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from pprint import pprint\r\n>>> import tasks\r\n>>> tasks.wait.delay(60)\r\n<AsyncResult: bfebb40f-a2b6-4b83-b0b1-905e2f867b85>\r\n>>> from pprint import pprint\r\n>>> pprint(tasks.app.control.inspect().active_queues())\r\n{'celery@Ryans-MacBook-Air.local': [{'alias': None,\r\n                                     'auto_delete': False,\r\n                                     'binding_arguments': None,\r\n                                     'bindings': [],\r\n                                     'consumer_arguments': None,\r\n                                     'durable': True,\r\n                                     'exchange': {'arguments': None,\r\n                                                  'auto_delete': False,\r\n                                                  'delivery_mode': None,\r\n                                                  'durable': True,\r\n                                                  'name': 'celery',\r\n                                                  'no_declare': False,\r\n                                                  'passive': False,\r\n                                                  'type': 'direct'},\r\n                                     'exclusive': False,\r\n                                     'expires': None,\r\n                                     'max_length': None,\r\n                                     'max_length_bytes': None,\r\n                                     'max_priority': None,\r\n                                     'message_ttl': None,\r\n                                     'name': 'celery',\r\n                                     'no_ack': False,\r\n                                     'no_declare': None,\r\n                                     'queue_arguments': None,\r\n                                     'routing_key': 'celery'}]}\r\n>>> pprint(tasks.app.control.inspect().active())\r\n{'celery@Ryans-MacBook-Air.local': [{'acknowledged': True,\r\n                                     'args': '(60,)',\r\n                                     'delivery_info': {'exchange': '',\r\n                                                       'priority': 0,\r\n                                                       'redelivered': False,\r\n                                                       'routing_key': 'celery'},\r\n                                     'hostname': 'celery@Ryans-MacBook-Air.local',\r\n                                     'id': 'bfebb40f-a2b6-4b83-b0b1-905e2f867b85',\r\n                                     'kwargs': '{}',\r\n                                     'name': 'tasks.wait',\r\n                                     'time_start': 1147215.628198587,\r\n                                     'type': 'tasks.wait',\r\n                                     'worker_pid': 87791}]}\r\n>>>\r\n```\r\n\r\nNote that the `active_queues` call says that the `celery` queue has an `exchange` of `'celery'`, as well as a `routing_key` of `'celery'`, which is expected. However, the task's delivery info has a `exchange` of `''`, even though the `routing_key` is `'celery'`.\r\n\r\nIn HireFire we use the `active_queues` inspect call to match the tasks to the queues that they are in. This is broken for us with Celery 4. See jezdez/hirefire#21.\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3767/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/eb08d0ca382df06079d54f0221673ac7555b6376", "message": "Avoid duplicating chains in chords (#3779)\n\n* Avoid duplicating chains in chords\r\n\r\n* Add Ryan Hiebert to contributors"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sjagoe": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3761", "title": "Redis pubsub timeout when waiting for multiple results in threads", "body": "Submitting tasks and waiting for results in threads often (but not always) appears to hang, and finally terminate with a timeout on the Redis result PUBSUB socket.\r\n\r\nThis scenario appears in a project I have recently become involved in that uses grpc.io effectively as a \"frontend\" to celery, allowing a Java component to submit tasks to celery workers and wait for the results. gRPC appears to require a `concurrent.futures.ThreadPoolExecutor` in order to handle requests, hence the celery requests being submitted from threads.\r\n\r\n- `celery report` output included at the bottom of the report.\r\n- The issue is present at least in Celery 4.0.2 and `master`.\r\n- The described scenario worked in Celery 3.1.x\r\n\r\n## Steps to reproduce\r\n\r\nIn this example scenario, I have a single worker with prefetch multiplier of `1`, and I'm submitting two tasks (submitted from separate threads) that take up to 500ms each to complete, so it is certain that one of the tasks will have to wait in the queue until the worker is free.\r\n\r\nWatcging the celery worker logs, it is clear that both tasks _have_ successfully executed.\r\n\r\nIn looking into the issue, it seems that the result for both submitted tasks _has_ been read from redis by the client (by inserting a `print` in `celery.backends.redis.ResultConsumer.drain_events`), but one thread seems to go back to `drain_events()` and block until a timeout from redis.\r\n\r\nI named this snippet below `redis_pubsub_timeout.py` and just dumped it in a virtualenv `site-packages` (or otherwise on `sys.path`) for simplicity.\r\n\r\nRun the worker with `celery worker --app redis_pubsub_timeout.app -l info` and the client with `python -m redis_pubsub_timeout`.\r\n\r\n```python\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nimport random\r\nimport time\r\n\r\nfrom celery import Celery\r\n\r\nCONFIG = {\r\n    'broker_url': 'redis://localhost:6379/4',\r\n    'result_backend': 'redis://localhost:6379/4',\r\n    'task_serializer': 'json',\r\n    'result_serializer': 'json',\r\n    'accept_content': ['json'],\r\n    'worker_prefetch_multiplier': 1,\r\n    'worker_concurrency': 1,\r\n    'worker_pool': 'prefork',\r\n}\r\n\r\napp = Celery(__name__)\r\napp.config_from_object(CONFIG)\r\n\r\n\r\n@app.task(name='redis_pubsub_timeout.mytask')\r\ndef mytask():\r\n    sleeptime = random.randint(10, 50) / float(100)\r\n    time.sleep(sleeptime)\r\n    return sleeptime\r\n\r\n\r\nif __name__ == '__main__':\r\n    def execute_task():\r\n        result = mytask.delay()\r\n        return result.get()\r\n\r\n    WORKERS = 2\r\n    with ThreadPoolExecutor(max_workers=WORKERS) as executor:\r\n        futures = [executor.submit(execute_task) for i in range(WORKERS)]\r\n    values = [future.result() for future in futures]\r\n    print(values)\r\n```\r\n\r\n\r\n## Expected behavior\r\n\r\nThe tasks complete and return the result to the caller.\r\n\r\n## Actual behavior\r\n\r\nThe following error is seen the the client when waiting for the results from the worker.\r\n\r\n```\r\n$ python -m redis_pubsub_timeout\r\nTraceback (most recent call last):\r\n  File \"/nix/store/7zqg0icnd3ymbzpgwsh4qdv064ccgwc4-python-2.7.13/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/nix/store/7zqg0icnd3ymbzpgwsh4qdv064ccgwc4-python-2.7.13/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/home/sjagoe/celery4/lib/python2.7/site-packages/redis_pubsub_timeout.py\", line 37, in <module>\r\n    values = [result.result() for result in results]\r\n  File \"/home/sjagoe/celery4/lib/python2.7/site-packages/concurrent/futures/_base.py\", line 398, in result\r\n    return self.__get_result()\r\n  File \"/home/sjagoe/celery4/lib/python2.7/site-packages/concurrent/futures/thread.py\", line 55, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/home/sjagoe/celery4/lib/python2.7/site-packages/redis_pubsub_timeout.py\", line 32, in execute_task\r\n    return result.get()\r\n  File \"/home/sjagoe/celery4/lib/python2.7/site-packages/celery/result.py\", line 189, in get\r\n    on_message=on_message,\r\n  File \"/home/sjagoe/celery4/lib/python2.7/site-packages/celery/backends/async.py\", line 190, in wait_for_pending\r\n    for _ in self._wait_for_pending(result, **kwargs):\r\n  File \"/home/sjagoe/celery4/lib/python2.7/site-packages/celery/backends/async.py\", line 259, in _wait_for_pending\r\n    on_interval=on_interval):\r\n  File \"/home/sjagoe/celery4/lib/python2.7/site-packages/celery/backends/async.py\", line 57, in drain_events_until\r\n    yield self.wait_for(p, wait, timeout=1)\r\n  File \"/home/sjagoe/celery4/lib/python2.7/site-packages/celery/backends/async.py\", line 67, in wait_for\r\n    wait(timeout=timeout)\r\n  File \"/home/sjagoe/celery4/lib/python2.7/site-packages/celery/backends/redis.py\", line 69, in drain_events\r\n    m = self._pubsub.get_message(timeout=timeout)\r\n  File \"/home/sjagoe/celery4/lib/python2.7/site-packages/redis/client.py\", line 2260, in get_message\r\n    response = self.parse_response(block=False, timeout=timeout)\r\n  File \"/home/sjagoe/celery4/lib/python2.7/site-packages/redis/client.py\", line 2183, in parse_response\r\n    return self._execute(connection, connection.read_response)\r\n  File \"/home/sjagoe/celery4/lib/python2.7/site-packages/redis/client.py\", line 2165, in _execute\r\n    return command(*args)\r\n  File \"/home/sjagoe/celery4/lib/python2.7/site-packages/redis/connection.py\", line 577, in read_response\r\n    response = self._parser.read_response()\r\n  File \"/home/sjagoe/celery4/lib/python2.7/site-packages/redis/connection.py\", line 238, in read_response\r\n    response = self._buffer.readline()\r\n  File \"/home/sjagoe/celery4/lib/python2.7/site-packages/redis/connection.py\", line 168, in readline\r\n    self._read_from_socket()\r\n  File \"/home/sjagoe/celery4/lib/python2.7/site-packages/redis/connection.py\", line 139, in _read_from_socket\r\n    raise TimeoutError(\"Timeout reading from socket\")\r\nredis.exceptions.TimeoutError: Timeout reading from socket\r\n```\r\n\r\n## Celery report\r\n\r\n```\r\n$ celery report --app redis_pubsub_timeout.app  \r\n\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:2.7.13\r\n            billiard:3.5.0.2 redis:2.10.5\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis://localhost:6379/4\r\n\r\nworker_pool: 'prefork'\r\ntask_serializer: 'json'\r\nresult_serializer: 'json'\r\nworker_concurrency: 3\r\nworker_prefetch_multiplier: 1\r\naccept_content: ['json']\r\nbroker_url: u'redis://localhost:6379/4'\r\nresult_backend: u'redis://localhost:6379/4'\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3761/reactions", "total_count": 4, "+1": 4, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "grantmcconnaughey": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3759", "title": "Tasks received but not executing", "body": "I have tasks that are received but will not execute. This the output of `celery worker`:\r\n\r\n```\r\n$ celery worker -A myapp.celeryapp --loglevel=INFO\r\n[tasks]\r\n  . myapp.tasks.trigger_build\r\n\r\n[2017-01-12 23:34:25,206: INFO/MainProcess] Connected to sqs://13245:**@localhost//\r\n[2017-01-12 23:34:25,391: INFO/MainProcess] celery@ip-111-11-11-11 ready.\r\n[2017-01-12 23:34:27,700: INFO/MainProcess] Received task: myapp.tasks.trigger_build[b248771c-6dd5-469d-bc53-eaf63c4f6b60]\r\n```\r\n\r\nI have tried adding `-Ofair` when running `celery worker` but that did not help. Some other info that might be helpful:\r\n\r\n* Celery always receives 8 tasks, although there are about 100 messages waiting to be picked up.\r\n* About once in every 4 or 5 times a task actually _will_ run and complete, but then it gets stuck again.\r\n* This is the result of `ps aux`. Notice that it is running celery in 3 different processes (not sure why) and one of them has 99.6% CPU utilization, even though it's not completing any tasks or anything.\r\n\r\n```\r\n$ ps aux | grep celery\r\nnobody    7034 99.6  1.8 382688 74048 ?        R    05:22  18:19 python2.7 celery worker -A myapp.celeryapp --loglevel=INFO\r\nnobody    7039  0.0  1.3 246672 55664 ?        S    05:22   0:00 python2.7 celery worker -A myapp.celeryapp --loglevel=INFO\r\nnobody    7040  0.0  1.3 246672 55632 ?        S    05:22   0:00 python2.7 celery worker -A myapp.celeryapp --loglevel=INFO\r\n```\r\n\r\n## Steps to reproduce\r\n\r\nBelow are my settings and report.\r\n\r\n```\r\nCELERY_BROKER_URL = 'sqs://%s:%s@' % (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY.replace('/', '%2F'))\r\nCELERY_BROKER_TRANSPORT = 'sqs'\r\nCELERY_BROKER_TRANSPORT_OPTIONS = {\r\n    'region': 'us-east-1',\r\n    'visibility_timeout': 60 * 30,\r\n    'polling_interval': 0.3,\r\n    'queue_name_prefix': 'myapp-',\r\n}\r\nCELERY_BROKER_HEARTBEAT = 0\r\nCELERY_BROKER_POOL_LIMIT = 1\r\nCELERY_BROKER_CONNECTION_TIMEOUT = 10\r\n\r\nCELERY_DEFAULT_QUEUE = 'myapp'\r\nCELERY_QUEUES = (\r\n    Queue('myapp', Exchange('default'), routing_key='default'),\r\n)\r\n\r\nCELERY_ALWAYS_EAGER = False\r\nCELERY_ACKS_LATE = True\r\nCELERY_TASK_PUBLISH_RETRY = True\r\nCELERY_DISABLE_RATE_LIMITS = False\r\n\r\nCELERY_IGNORE_RESULT = True\r\nCELERY_SEND_TASK_ERROR_EMAILS = False\r\nCELERY_TASK_RESULT_EXPIRES = 600\r\n\r\nCELERY_RESULT_BACKEND = 'django-db'\r\nCELERY_TIMEZONE = TIME_ZONE\r\n\r\nCELERY_TASK_SERIALIZER = 'json'\r\nCELERY_ACCEPT_CONTENT = ['application/json']\r\n\r\nCELERYD_PID_FILE = \"/var/celery_%N.pid\"\r\nCELERYD_HIJACK_ROOT_LOGGER = False\r\nCELERYD_PREFETCH_MULTIPLIER = 1\r\nCELERYD_MAX_TASKS_PER_CHILD = 1000\r\n```\r\n\r\n```\r\n$ celery report -A myapp.celeryapp\r\n\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:2.7.12\r\n            billiard:3.5.0.2 sqs:N/A\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:sqs results:django-db\r\n```\r\n\r\n## Expected behavior\r\n\r\nI expect the jobs to be picked up and executed.\r\n\r\n## Actual behavior\r\n\r\nThe tasks are received but nothing happens. My celery log file contains nothing except that the task was received.\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3759/reactions", "total_count": 7, "+1": 7, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jclark360": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3751", "title": "Celery error calling add_consumer randomly ocurrs", "body": "## Checklist\r\n\r\n- [X] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n\r\n  \r\n- [X] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nCall add_consumer in celery control randomly from a worker that's been up for awhile.  It seems to happen after a worker has been up for a day or so.  For us it's happening every day.\r\n\r\nCode:\r\n\r\n```\r\ncelery_app.control.add_consumer(\r\n                queue.name,\r\n                exchange='foo',\r\n                destination=['foo-worker@foo.com']\r\n            )\r\n```\r\n\r\n## Expected behavior\r\nThat consumers are always added after the call..\r\n\r\n## Actual behavior\r\nThe exception below is thrown:\r\n\r\n```\r\nFile \"/usr/lib/python3.5/site-packages/celery/app/control.py\", line 293, in add_consumer\r\n    **kwargs\r\n  File \"/usr/lib/python3.5/site-packages/celery/app/control.py\", line 436, in broadcast\r\n    limit, callback, channel=channel,\r\n  File \"/usr/lib/python3.5/site-packages/kombu/pidbox.py\", line 315, in _broadcast\r\n    serializer=serializer)\r\n  File \"/usr/lib/python3.5/site-packages/kombu/pidbox.py\", line 285, in _publish\r\n    with self.producer_or_acquire(producer, chan) as producer:\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 59, in __enter__\r\n    return next(self.gen)\r\n  File \"/usr/lib/python3.5/site-packages/kombu/pidbox.py\", line 247, in producer_or_acquire\r\n    with self.producer_pool.acquire() as producer:\r\n  File \"/usr/lib/python3.5/site-packages/kombu/resource.py\", line 74, in acquire\r\n    raise RuntimeError('Acquire on closed pool')\r\nRuntimeError: Acquire on closed pool\r\n```\r\n\r\n\r\nCelery report:\r\n\r\n```\r\nsoftware -> celery:4.0.1 (latentcall) kombu:4.0.1 py:3.5.2\r\n            billiard:3.5.0.2 py-amqp:2.1.3\r\nplatform -> system:Linux arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:disabled\r\n\r\nevent_queue_ttl: 60\r\ntask_acks_late: True\r\ntask_always_eager: False\r\ntask_create_missing_queues: True\r\ntask_default_queue: 'default'\r\ninclude: ['crawler_service.crawlers.reviews',\r\n 'crawler_service.crawlers.product',\r\n 'crawler_service.crawlers.facet_discovery',\r\n 'crawler_service.crawlers.discovery',\r\n 'crawler_service.crawlers.zone_discovery',\r\n 'crawler_service.crawlers.crawler_stats',\r\n 'crawler_service.crawl_scheduler.schedule_task']\r\ntask_soft_time_limit: 300\r\nbeat_schedule: {\r\n    'every-30s': {   'args': '',\r\n                     'schedule': datetime.timedelta(0, 30),\r\n                     'task': 'crawler_service.crawlers.crawler_stats.crawler_stats_rollup'},\r\n    'every-60s': {   'args': '',\r\n                     'schedule': datetime.timedelta(0, 60),\r\n                     'task': 'crawler_service.crawlers.crawler_stats.crawler_health_check'},\r\n    'every-day': {   'args': '',\r\n                     'schedule': datetime.timedelta(1),\r\n                     'task': 'crawler_service.crawlers.crawler_stats.crawler_stats_cleanup'}}\r\nbroker_transport_options: {\r\n 'confirm_publish': True}\r\nresult_serializer: 'json'\r\ntask_serializer: 'json'\r\nworker_max_tasks_per_child: 30000\r\nbroker_heartbeat: 30\r\ntask_routes: <crawler_service.celery.router.CrawlerRouter object at 0x7fc43f8b56a0>\r\nworker_redirect_stdouts_level: 'DEBUG'\r\nbroker_url: 'amqp://*******:********@foo.com:5672//'\r\ntask_queues:\r\n    (<unbound Queue scheduled_tasks -> <unbound Exchange scheduled_tasks(direct)> -> scheduled_tasks>,\r\n <unbound Queue crawl_scheduler -> <unbound Exchange crawl_scheduler(direct)> -> crawl_scheduler>)\r\naccept_content: ['json']\r\ntask_time_limit: 500\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3751/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "max8899": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3747", "title": "time limit not working when using eventlet", "body": "## Checklist\r\n\r\n- [x]  ``celery report``\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:2.7.12\r\n            billiard:3.5.0.2 py-amqp:2.1.4\r\nplatform -> system:Darwin arch:64bit imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\nsettings -> transport:amqp results:disabled\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nproject : https://github.com/celery/celery/tree/master/examples/eventlet\r\nstart worker: celery worker -l info --concurrency=1 --pool=eventlet\r\n\r\nDo some change in celeryconfig.py:\r\n```\r\nbroker_url = 'amqp://guest:guest@localhost:5672//'\r\nworker_disable_rate_limits = True\r\nworker_prefetch_multiplier = 1\r\ntask_time_limit = 2\r\ntask_soft_time_limit = 1\r\nworker_max_tasks_per_child = 100\r\nresult_backend = 'redis://localhost:6379/'\r\nresult_expires = 30 * 60\r\n```\r\n\r\n\r\nTry some url use more than 3 seconds to fetch, since in China, I use google\r\n```\r\n$ cd examples/eventlet\r\n$ python\r\n>>> from tasks import urlopen\r\n>>> res = urlopen.delay('http://www.google.com/')\r\n>>> res.state\r\nu'PENDING'\r\n```\r\n\r\nthe PENDING state takes more than 3 seconds to FAILURE and the worker doesn't reload\r\n\r\n\r\n## Expected behavior\r\n\r\ntask fail quickly, and release the worker process\r\n\r\n\r\n## Actual behavior\r\n\r\ntakes more time until the socket connection time out reached", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3747/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zaro": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3743", "title": "chain/chord not propagating exceptions as expected", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [ ] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n\r\nTasks:\r\n```python\r\n@app.task\r\ndef a1(arg):\r\n    print('Task A1, starring with arg:' + str(arg))\r\n    if arg == 3:\r\n        raise Exception('A1 Error')\r\n    time.sleep(5)\r\n    print('A1 done')\r\n    return {'a1': True}\r\n\r\n@app.task\r\ndef a2(arg):\r\n    print('Task A2, starring with arg:' + str(arg))\r\n    time.sleep(5)\r\n    print('A2 done')\r\n    return {'a2': True}\r\n\r\n@app.task\r\ndef b1(arg):\r\n    print('Task B1, starring with arg:' + str(arg))\r\n    time.sleep(5)\r\n    print('B1 done')\r\n    return {'b1': True}\r\n\r\n```\r\n\r\nIf we start a chrod() with 3 a1 task fails as expected:\r\n```python\r\n>>> chord([ a1.s(1)  , a1.s(2) , a1.s(3)  ])(b1.s()).get()\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\n  File \"/Users/zaro/dev/proj-dev/proj/.direnv/python-3.5.2/lib/python3.5/site-packages/celery/result.py\", line 189, in get\r\n    on_message=on_message,\r\n  File \"/Users/zaro/dev/proj-dev/proj/.direnv/python-3.5.2/lib/python3.5/site-packages/celery/backends/async.py\", line 191, in wait_for_pending\r\n    return result.maybe_throw(callback=callback, propagate=propagate)\r\n  File \"/Users/zaro/dev/proj-dev/proj/.direnv/python-3.5.2/lib/python3.5/site-packages/celery/result.py\", line 294, in maybe_throw\r\n    self.throw(value, self._to_remote_traceback(tb))\r\n  File \"/Users/zaro/dev/proj-dev/proj/.direnv/python-3.5.2/lib/python3.5/site-packages/celery/result.py\", line 287, in throw\r\n    self.on_ready.throw(*args, **kwargs)\r\n  File \"/Users/zaro/dev/proj-dev/proj/.direnv/python-3.5.2/lib/python3.5/site-packages/vine/promises.py\", line 213, in throw\r\n    reraise(type(exc), exc, tb)\r\n  File \"/Users/zaro/dev/proj-dev/proj/.direnv/python-3.5.2/lib/python3.5/site-packages/vine/five.py\", line 176, in reraise\r\n    raise value\r\ncelery.backends.base.ChordError: Callback error: ChordError(\"Dependency e90cc09c-47bc-4fac-9a3d-08f1260c2d5c raised Exception('A1 Error',)\",)\r\n```\r\n\r\nIf the we use chains instead of a single task in the the chord like this:\r\n```python\r\n>>> chord([ ( a1.s(1) | a2.s()) , ( a1.s(2) | a2.s()), ( a1.s(3) | a2.s())  ])(b1.s()).get()\r\n\r\n.... Hangs forever, and never throws ....\r\n```\r\n\r\nSo first I though maybe chain() doesn't support error propagation, so I tried:\r\n\r\n```python\r\n>>> ( a1.s(3) | a2.s()).apply_async().get() \r\n.... Hangs forever, and never throws ....\r\n```\r\nAnd indeed it seems the behaviour is the same on a single chain.\r\nBut the I don't know why I tried it also with a variable:\r\n\r\n```python\r\n>>> r = ( a1.s(3) | a2.s()).apply_async()\r\n>>> r.get()\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\n  File \"/Users/zaro/dev/proj-dev/proj/.direnv/python-3.5.2/lib/python3.5/site-packages/celery/result.py\", line 172, in get\r\n    self._maybe_reraise_parent_error()\r\n  File \"/Users/zaro/dev/proj-dev/proj/.direnv/python-3.5.2/lib/python3.5/site-packages/celery/result.py\", line 195, in _maybe_reraise_parent_error\r\n    node.maybe_throw()\r\n  File \"/Users/zaro/dev/proj-dev/proj/.direnv/python-3.5.2/lib/python3.5/site-packages/celery/result.py\", line 294, in maybe_throw\r\n    self.throw(value, self._to_remote_traceback(tb))\r\n  File \"/Users/zaro/dev/proj-dev/proj/.direnv/python-3.5.2/lib/python3.5/site-packages/celery/result.py\", line 287, in throw\r\n    self.on_ready.throw(*args, **kwargs)\r\n  File \"/Users/zaro/dev/proj-dev/proj/.direnv/python-3.5.2/lib/python3.5/site-packages/vine/promises.py\", line 213, in throw\r\n    reraise(type(exc), exc, tb)\r\n  File \"/Users/zaro/dev/proj-dev/proj/.direnv/python-3.5.2/lib/python3.5/site-packages/vine/five.py\", line 176, in reraise\r\n    raise value\r\ncelery.backends.base.Exception: A1 Error\r\n```\r\n\r\nAnd this chain works as expected.\r\n\r\nSo I tried the same thing with the chord():\r\n```python\r\n>>> r = chord([ ( a1.s(1) | a2.s()) , ( a1.s(2) | a2.s()), ( a1.s(3) | a2.s())  ])(b1.s())\r\n>>> r.get()\r\n.... Hangs forever, and never throws ....\r\n```\r\nbut it didn't help.\r\n\r\n## Expected behavior\r\n\r\nWhile not being a python expert I would expect that:\r\n\r\n```python\r\n>>> ( a1.s(3) | a2.s()).apply_async().get() \r\n```\r\n```python\r\n>>> r = ( a1.s(3) | a2.s()).apply_async()\r\n>>> r.get()\r\n```\r\nproduce the same result\r\n\r\nI would also expect that chord() will propagate exceptions regardless of whether it's header is list of tasks or list of chains().\r\n\r\n## Actual behavior\r\n\r\nchain() w/o intermediate variable doesn't propagate exceptions\r\nchord() doesn't propagate exceptions occurring in the header chains.\r\n\r\n## celery report\r\n\r\n```\r\n\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:3.5.2\r\n            billiard:3.5.0.2 redis:2.10.5\r\nplatform -> system:Darwin arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis:///\r\n\r\n.......\r\nCELERY_BROKER_URL: 'redis://localhost:6379//'\r\nCELERY_RESULT_BACKEND: 'redis:///'\r\n......\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3743/reactions", "total_count": 5, "+1": 5, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "NickStefan": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3709", "title": "chord on_error callback does not work as documented", "body": "## Checklist\r\n\r\n- version 4.0.2 of celery\r\n- im just copying the examples from the docs from here: http://docs.celeryproject.org/en/latest/userguide/canvas.html#error-handling\r\n\r\n## Steps to reproduce\r\n\r\nfile1.py\r\n```\r\nfrom celery import chord, group\r\nfrom app.graph.tasks import on_chord_error, add, xsum\r\n\r\nc = (group(add.s(i, i) for i in range(1)) | xsum.s().on_error(on_chord_error.s())).delay()\r\n```\r\n\r\ntasks.py\r\n```\r\n@shared_task\r\ndef add(x, y):\r\n    raise Exception(\"bob\")\r\n\r\n@shared_task\r\ndef xsum(numbers):\r\n    return sum(numbers)\r\n\r\n@shared_task\r\ndef on_chord_error(request, exc, traceback):\r\n    print('calling this random error method!!!!')\r\n    print('Task {0!r} raised error: {1!r}'.format(request.id, exc))\r\n```\r\n\r\n## Expected behavior\r\nThat the on_chord_error callback would be invoked and would print out \"calling this random error method\"\r\n\r\n## Actual behavior\r\nWhether I define an on_error callback or not (e.g. `c = (group(add.s(i, i) for i in range(1)) | xsum.s().on_error(on_chord_error.s())).delay()` versus `c = (group(add.s(i, i) for i in range(1)) | xsum.s()).delay()` , i get this same output:\r\n\r\noutput\r\n```\r\ndjango      | [2016-12-20 23:32:14,688: INFO/MainProcess] Received task: app.graph.tasks.add[c5eeb13a-21f6-4f15-ac22-eab0784d2806]\r\ndjango      | [2016-12-20 23:32:16,068: ERROR/PoolWorker-2] Task app.graph.tasks.add[c5eeb13a-21f6-4f15-ac22-eab0784d2806] raised unexpected: Exception('bob',)\r\ndjango      | Traceback (most recent call last):\r\ndjango      |   File \"/usr/local/lib/python3.5/site-packages/celery/app/trace.py\", line 367, in trace_task\r\ndjango      |     R = retval = fun(*args, **kwargs)\r\ndjango      |   File \"/usr/local/lib/python3.5/site-packages/celery/app/trace.py\", line 622, in __protected_call__\r\ndjango      |     return self.run(*args, **kwargs)\r\ndjango      |   File \"/usr/src/app/graph/tasks.py\", line 44, in add\r\ndjango      |     raise Exception(\"bob\")\r\ndjango      | Exception: bob\r\n```\r\nIt never calls the on_error handler.\r\n\r\n# normal circumstances work:\r\nIf I rewrite add from\r\n```\r\n@shared_task\r\ndef add(x, y):\r\n    raise Exception(\"bob\")\r\n```\r\nto this:\r\n```\r\n@shared_task\r\ndef add(x, y):\r\n    return x + y\r\n```\r\n\r\nthen everything works. So I believe one of three things is going on:\r\n- the documentation regarding group / chord error handling is incorrect\r\n- chord / group error handling does not work\r\n- i'm misinterpreting how error handling is supposed to work in celery (i have scoured the internet for good examples, but i cannot find any good examples of people propagating errors from a group of subprocesses. I would think this would be a very common thing!)\r\n\r\n### What is the celery equivalent of something like this from Node.js land? \r\n```\r\nvar async = require('async');\r\nasync.auto({\r\n     thing1: function(){},\r\n     thing2: function(){}\r\n}, function(err, results) {\r\n    if (err) {\r\n         // do stuff!\r\n   }\r\n   // do normal stuff post batch of stuff\r\n})\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3709/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dagostinelli": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3698", "title": "Manual Acknowledgement Reboot", "body": "Related to: https://github.com/celery/celery/issues/3035\r\n\r\nI think this project really needs a manual acknowledgement capability.  While I do appreciate the vision of the project team, this project is the go-to for RabbitMQ integration on Python and as such, it ought to support a few other development conventions.  The auto-ack-everything convention doesn't work for everyone (as evidenced by all the confusion about this).  So let's give the community what it's asking for.\r\n\r\nThe other issue died on the vine because the project team is kicking this to the community.  That's OK.  I hereby volunteer to do this.  But I need a little guidance.\r\n\r\nMy proposal:\r\n- In the task decorator, add a method called \"ack()\" that does the acking.  It would be implemented next to retry()\r\n\r\n```\r\n@celery.current_app.task(name='dosomething', bind=True, auto_ack=False)\r\ndef dosomething(self):\r\n\ttry:\r\n\t\tself.ack()\r\n\texcept MaxRetriesExceededError:\r\n\t\t# still not acking\r\n\texcept Exception as exc:\r\n\t\t# not going to ack, log error, but also not going to call retry()\r\n```\r\n\r\n- In the celeryconfig.py, support a setting called \"auto_ack = False\" which would apply the behavior globally\r\n\r\nThat one will be hardest to implement.  I noticed that the auto-acking logic is all around the code base.  I think I need some advice on how to approach that one.\r\n\r\n- Does it make sense to add some warnings around that tell folks not to call retry if they are not using auto_ack'ing?  (to avoid forever loops)\r\n\r\nThoughts?\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3698/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "matthewatabet": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3673", "title": "Disable propagate for a portion of a canvas.", "body": "I have a `group` of tasks, some of which may fail unexpectedly. These tasks make use of shared resources which are allocated just before they run and need to be cleaned up afterwards.\r\n\r\nI attempted to solve this like so:\r\n\r\n```python\r\n@task\r\ndef do_work():\r\n    return generate_some_results()\r\n\r\n\r\n@task\r\ndef cleanup(results):\r\n    clean_stuff_up()\r\n    return cull_exceptions_from_results(results)\r\n\r\n\r\ncanvas = group(do_work.s(propagate=False) for i in range(0, 10)) | cleanup.s()\r\n```\r\n\r\nBut, if an exception is raised in one of the work tasks, the chord still fails without running `cleanup`. I found that running `canvas(propagate=False)` did work, but disables propagation for the entire canvas which is not what I want.\r\n\r\nEssentially, I'd like to implement a canvas which achieves a try-except-finally semantic, where the finally clause is always run despite upstream errors. Any pointers? Thanks a bunch.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3673/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "KBoehme": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3653", "title": "Unacked messages return to queue shortly after soft shutdown (RabbitMQ)", "body": "## Checklist\r\n```\r\npip list | grep celery\r\ncelery (4.0.0) #Tested on master and seems its affected by this too.\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1. Start a worker\r\n2. Submit long running task where `acks_late=True` to worker.\r\n3. Soft kill worker, the task continues to be processed as expected.\r\n4. Wait ~35 seconds and see the same currently running task message resubmitted to queue.\r\n\r\n```python\r\n#Define a task where acks_late=True\r\n@app.task(acks_late=True)\r\ndef sleep_task_1(time_to_sleep=60):\r\n    logger.info('Running sleep_task_1! sleeping for {} seconds'.format(\r\n        time_to_sleep))\r\n    for i in range(time_to_sleep):\r\n        print(i)\r\n        sleep(1)\r\n\r\n#Submit task\r\nsleep.apply_async(kwargs={'time_to_sleep': 5000}) # Send long-running, acks_late=True task to running worker.\r\n```\r\n\r\n## Expected behavior\r\nWhen a worker running a long running task configured with `acks_late=True` is soft killed it should continue to process the current task and SHOULD NOT resubmit the task message to the queue.\r\n\r\n## Actual behavior\r\nWhen a worker running a long running task configured with `acks_late=True` is soft killed it DOES continue to process the current task BUT resubmits the task message back to the queue shortly after getting soft killed.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3653/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mbattifarano": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3642", "title": "pytest plugin example fails with `NotRegistered`", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected). -- This failure happens using only the pytest plugin fixtures\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n\r\nVersions: Celery 4.0.0 (and master), pytest 3.0.4 with the plugin from `celery/contrib/pytest.py`\r\n\r\n+ Copy and paste the example test from the [docs](http://docs.celeryproject.org/en/master/userguide/testing.html#celery-app-celery-app-used-for-testing) into a file. \r\n+ Run `pytest test_tasks.py`\r\n\r\n`test_tasks.py`:\r\n```python\r\ndef test_create_task(celery_app, celery_worker):\r\n    @celery_app.task\r\n    def mul(x, y):\r\n        return x * y\r\n\r\n    assert mul.delay(4, 4).get(timeout=10) == 16\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe test passes\r\n\r\n## Actual behavior\r\n\r\nA `NotRegistered` exception is raised on `assert mul.delay(4, 4).get(timeout=10) == 16`:\r\n\r\n```\r\n_______________________________________________ test_create_task ________________________________________________\r\n\r\ncelery_app = <Celery celery.tests:0x10369ff50>, celery_worker = <Worker: gen71974@mbhome.local (running)>\r\n\r\n    def test_create_task(celery_app, celery_worker):\r\n        @celery_app.task\r\n        def mul(x, y):\r\n            return x * y\r\n    \r\n>       assert mul.delay(4, 4).get(timeout=10) == 16\r\n\r\ntest_tasks.py:6: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n[...]/celery/celery/result.py:189: in get\r\n    on_message=on_message,\r\n[...]/celery/celery/backends/base.py:470: in wait_for_pending\r\n    return result.maybe_throw(propagate=propagate, callback=callback)\r\n[...]/celery/celery/result.py:294: in maybe_throw\r\n    self.throw(value, self._to_remote_traceback(tb))\r\n[...]/celery/celery/result.py:287: in throw\r\n    self.on_ready.throw(*args, **kwargs)\r\n../.virtualenvs/celery-dev/lib/python2.7/site-packages/vine-1.1.3-py2.7.egg/vine/promises.py:213: in throw\r\n    reraise(type(exc), exc, tb)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\ntp = <class 'celery.backends.base.NotRegistered'>, value = NotRegistered(u\"u'test_tasks.mul'\",), tb = None\r\n\r\n>   ???\r\nE   NotRegistered: u'test_tasks.mul'\r\n\r\n<string>:1: NotRegistered\r\n--------------------------------------------- Captured stderr call ----------------------------------------------\r\n[2016-12-01 16:49:43,763: ERROR/MainProcess] Received unregistered task of type u'test_tasks.mul'.\r\nThe message has been ignored and discarded.\r\n\r\nDid you remember to import the module containing this task?\r\nOr maybe you're using relative imports?\r\n\r\nPlease see\r\nhttp://docs.celeryq.org/en/latest/internals/protocol.html\r\nfor more information.\r\n\r\nThe full contents of the message body was:\r\n'[[4, 4], {}, {\"chord\": null, \"callbacks\": null, \"errbacks\": null, \"chain\": null}]' (81b)\r\nTraceback (most recent call last):\r\n  File \"[...]/celery/celery/worker/consumer/consumer.py\", line 558, in on_task_received\r\n    strategy = strategies[type_]\r\nKeyError: u'test_tasks.mul'\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3642/reactions", "total_count": 9, "+1": 9, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Nekorooni": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3639", "title": "Running \"celery multi\" on windows conflicts with path", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n```\r\nsoftware -> celery:4.0.0 (latentcall) kombu:4.0.0 py:3.5.2\r\n            billiard:3.5.0.2 py-amqp:2.1.1\r\nplatform -> system:Windows arch:32bit, WindowsPE imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:amqp:///\r\n\r\nresult_backend: 'amqp:///'\r\n```\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n1. Open cmd in project folder\r\n2. Run `celery multi start 4 -A xxx -l info -c4 --pidfile=\"C:\\%n.pid\"`\r\n\r\n## Expected behavior\r\nCelery starts 4 workers.\r\n\r\n## Actual behavior\r\nCelery fails starting a worker 4 times.\r\n\r\n```CMD\r\nC:\\Users\\xxx\\xxx>celery multi start 4 -A xxx-c4 --pidfile=\"C:\\%n.pid\"\r\ncelery multi v4.0.0 (latentcall)\r\n> Starting nodes...\r\nusage: c:\\program files (x86)\\python35-32\\lib\\site-packages\\celery\\bin\\celery.py worker [options]\r\nc:\\program files (x86)\\python35-32\\lib\\site-packages\\celery\\bin\\celery.py: error: unrecognized arguments: files (x86)\\python35-32\\python.exe\r\n        > celery1@DESKTOP-2738VCR: * Child terminated with exit code 2\r\nFAILED\r\nusage: c:\\program files (x86)\\python35-32\\lib\\site-packages\\celery\\bin\\celery.py worker [options]\r\nc:\\program files (x86)\\python35-32\\lib\\site-packages\\celery\\bin\\celery.py: error: unrecognized arguments: files (x86)\\python35-32\\python.exe\r\n        > celery2@DESKTOP-2738VCR: * Child terminated with exit code 2\r\nFAILED\r\nusage: c:\\program files (x86)\\python35-32\\lib\\site-packages\\celery\\bin\\celery.py worker [options]\r\nc:\\program files (x86)\\python35-32\\lib\\site-packages\\celery\\bin\\celery.py: error: unrecognized arguments: files (x86)\\python35-32\\python.exe\r\n        > celery3@DESKTOP-2738VCR: * Child terminated with exit code 2\r\nFAILED\r\nusage: c:\\program files (x86)\\python35-32\\lib\\site-packages\\celery\\bin\\celery.py worker [options]\r\nc:\\program files (x86)\\python35-32\\lib\\site-packages\\celery\\bin\\celery.py: error: unrecognized arguments: files (x86)\\python35-32\\python.exe\r\n        > celery4@DESKTOP-2738VCR: * Child terminated with exit code 2\r\nFAILED\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3639/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "april": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3633", "title": "UnicodeEncodeError in celery logging (\\xf6)", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n\r\n```\r\nsoftware -> celery:4.0.0 (latentcall) kombu:4.0.0 py:3.4.3\r\n            billiard:3.5.0.2 redis:2.10.5\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:disabled\r\n\r\nCELERY_REDIRECT_STDOUTS_LEVEL: 'WARNING'\r\nBROKER_URL: 'redis://localhost:6379/0'\r\nCELERY_RESULT_SERIALIZER: 'json'\r\nCELERYD_TASK_SOFT_TIME_LIMIT: 751\r\nCELERY_IGNORE_RESULTS: True\r\nCELERYD_TASK_TIME_LIMIT: 1129\r\nCELERY_ACCEPT_CONTENT: ['json']\r\nCELERY_TASK_SERIALIZER: 'json'\r\n```\r\n\r\n- [ ] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nI have celery running a task using requests, setup with loglevel `WARNING`.  If I requests.get('https://self-storage.virtualways.de') in my code, I get the following traceback:\r\n\r\n```\r\n[2016-11-30 15:37:45,932: WARNING/PoolWorker-312] --- Logging error ---\r\n[2016-11-30 15:37:45,933: WARNING/PoolWorker-312] Traceback (most recent call last):\r\n[2016-11-30 15:37:45,933: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/requests/packages/urllib3/connection.py\", line 313, in _match_hostname\r\n    match_hostname(cert, asserted_hostname)\r\n[2016-11-30 15:37:45,934: WARNING/PoolWorker-312] File \"/usr/lib/python3.4/ssl.py\", line 285, in match_hostname\r\n    % (hostname, ', '.join(map(repr, dnsnames))))\r\n[2016-11-30 15:37:45,934: WARNING/PoolWorker-312] ssl.CertificateError: hostname 'self-storage.virtualways.de' doesn't match either of 'www.handwerkstraum.de', 'handwerkstraum.de'\r\n[2016-11-30 15:37:45,934: WARNING/PoolWorker-312] During handling of the above exception, another exception occurred:\r\n[2016-11-30 15:37:45,934: WARNING/PoolWorker-312] Traceback (most recent call last):\r\n[2016-11-30 15:37:45,934: WARNING/PoolWorker-312] File \"/usr/lib/python3.4/logging/__init__.py\", line 980, in emit\r\n    stream.write(msg)\r\n[2016-11-30 15:37:45,935: WARNING/PoolWorker-312] UnicodeEncodeError: 'ascii' codec can't encode character '\\xf6' in position 516: ordinal not in range(128)\r\n[2016-11-30 15:37:45,935: WARNING/PoolWorker-312] Call stack:\r\n[2016-11-30 15:37:45,940: WARNING/PoolWorker-312] File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n[2016-11-30 15:37:45,941: WARNING/PoolWorker-312] File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n[2016-11-30 15:37:45,941: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/__main__.py\", line 18, in <module>\r\n    main()\r\n[2016-11-30 15:37:45,941: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/__main__.py\", line 14, in main\r\n    _main()\r\n[2016-11-30 15:37:45,941: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/bin/celery.py\", line 326, in main\r\n    cmd.execute_from_commandline(argv)\r\n[2016-11-30 15:37:45,941: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/bin/celery.py\", line 488, in execute_from_commandline\r\n    super(CeleryCommand, self).execute_from_commandline(argv)))\r\n[2016-11-30 15:37:45,941: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/bin/base.py\", line 278, in execute_from_commandline\r\n    return self.handle_argv(self.prog_name, argv[1:])\r\n[2016-11-30 15:37:45,941: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/bin/celery.py\", line 480, in handle_argv\r\n    return self.execute(command, argv)\r\n[2016-11-30 15:37:45,942: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/bin/celery.py\", line 412, in execute\r\n    ).run_from_argv(self.prog_name, argv[1:], command=argv[0])\r\n[2016-11-30 15:37:45,942: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/bin/worker.py\", line 221, in run_from_argv\r\n    return self(*args, **options)\r\n[2016-11-30 15:37:45,942: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/bin/base.py\", line 241, in __call__\r\n    ret = self.run(*args, **kwargs)\r\n[2016-11-30 15:37:45,942: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/bin/worker.py\", line 256, in run\r\n    worker.start()\r\n[2016-11-30 15:37:45,942: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/worker/worker.py\", line 203, in start\r\n    self.blueprint.start(self)\r\n[2016-11-30 15:37:45,942: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n[2016-11-30 15:37:45,942: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/bootsteps.py\", line 370, in start\r\n    return self.obj.start()\r\n[2016-11-30 15:37:45,942: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/worker/consumer/consumer.py\", line 318, in start\r\n    blueprint.start(self)\r\n[2016-11-30 15:37:45,943: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n[2016-11-30 15:37:45,943: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/worker/consumer/consumer.py\", line 584, in start\r\n    c.loop(*c.loop_args())\r\n[2016-11-30 15:37:45,943: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/worker/loops.py\", line 88, in asynloop\r\n    next(loop)\r\n[2016-11-30 15:37:45,943: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/kombu/async/hub.py\", line 345, in create_loop\r\n    cb(*cbargs)\r\n[2016-11-30 15:37:45,943: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/concurrency/asynpool.py\", line 444, in _event_process_exit\r\n    self.maintain_pool()\r\n[2016-11-30 15:37:45,943: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/billiard/pool.py\", line 1308, in maintain_pool\r\n    self._maintain_pool()\r\n[2016-11-30 15:37:45,943: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/billiard/pool.py\", line 1300, in _maintain_pool\r\n    self._repopulate_pool(joined)\r\n[2016-11-30 15:37:45,943: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/billiard/pool.py\", line 1285, in _repopulate_pool\r\n    self._create_worker_process(self._avail_index())\r\n[2016-11-30 15:37:45,944: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/concurrency/asynpool.py\", line 439, in _create_worker_process\r\n    return super(AsynPool, self)._create_worker_process(i)\r\n[2016-11-30 15:37:45,944: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/billiard/pool.py\", line 1117, in _create_worker_process\r\n    w.start()\r\n[2016-11-30 15:37:45,944: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/billiard/process.py\", line 122, in start\r\n    self._popen = self._Popen(self)\r\n[2016-11-30 15:37:45,944: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/billiard/context.py\", line 333, in _Popen\r\n    return Popen(process_obj)\r\n[2016-11-30 15:37:45,944: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/billiard/popen_fork.py\", line 24, in __init__\r\n    self._launch(process_obj)\r\n[2016-11-30 15:37:45,944: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/billiard/popen_fork.py\", line 79, in _launch\r\n    code = process_obj._bootstrap()\r\n[2016-11-30 15:37:45,944: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/billiard/process.py\", line 306, in _bootstrap\r\n    self.run()\r\n[2016-11-30 15:37:45,944: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/billiard/process.py\", line 112, in run\r\n    self._target(*self._args, **self._kwargs)\r\n[2016-11-30 15:37:45,945: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/billiard/pool.py\", line 290, in __call__\r\n    sys.exit(self.workloop(pid=pid))\r\n[2016-11-30 15:37:45,945: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/billiard/pool.py\", line 359, in workloop\r\n    result = (True, prepare_result(fun(*args, **kwargs)))\r\n[2016-11-30 15:37:45,945: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/app/trace.py\", line 531, in _fast_trace_task\r\n    uuid, args, kwargs, request,\r\n[2016-11-30 15:37:45,945: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/app/trace.py\", line 368, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n[2016-11-30 15:37:45,945: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/celery/app/trace.py\", line 623, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n[2016-11-30 15:37:45,945: WARNING/PoolWorker-312] File \"/home/httpobs/httpobs/httpobs/scanner/tasks.py\", line 31, in scan\r\n    reqs = retrieve_all(hostname, cookies=headers['cookies'], headers=headers['headers'])\r\n[2016-11-30 15:37:45,945: WARNING/PoolWorker-312] File \"/home/httpobs/httpobs/httpobs/scanner/retriever/retriever.py\", line 147, in retrieve_all\r\n    https_session = __create_session('https://' + hostname + kwargs['https_port'] + kwargs['path'], **kwargs)\r\n[2016-11-30 15:37:45,945: WARNING/PoolWorker-312] File \"/home/httpobs/httpobs/httpobs/scanner/retriever/retriever.py\", line 52, in __create_session\r\n    r = s.get(url, timeout=TIMEOUT)\r\n[2016-11-30 15:37:45,946: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/requests/sessions.py\", line 488, in get\r\n    return self.request('GET', url, **kwargs)\r\n[2016-11-30 15:37:45,946: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/requests/sessions.py\", line 475, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n[2016-11-30 15:37:45,946: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/requests/sessions.py\", line 596, in send\r\n    r = adapter.send(request, **kwargs)\r\n[2016-11-30 15:37:45,946: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/requests/adapters.py\", line 423, in send\r\n    timeout=timeout\r\n[2016-11-30 15:37:45,946: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/requests/packages/urllib3/connectionpool.py\", line 595, in urlopen\r\n    chunked=chunked)\r\n[2016-11-30 15:37:45,946: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/requests/packages/urllib3/connectionpool.py\", line 352, in _make_request\r\n    self._validate_conn(conn)\r\n[2016-11-30 15:37:45,946: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/requests/packages/urllib3/connectionpool.py\", line 831, in _validate_conn\r\n    conn.connect()\r\n[2016-11-30 15:37:45,946: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/requests/packages/urllib3/connection.py\", line 305, in connect\r\n    _match_hostname(cert, self.assert_hostname or hostname)\r\n[2016-11-30 15:37:45,947: WARNING/PoolWorker-312] File \"/usr/local/lib/python3.4/dist-packages/requests/packages/urllib3/connection.py\", line 317, in _match_hostname\r\n    'Certificate: %s', asserted_hostname, cert\r\n```\r\n\r\nThis seems to crash the master process, and thereafter celery inspect simply returns `Error: No nodes replied within time constraint.`\r\n\r\n`sys.getdefaultencoding()` is utf-8.\r\n\r\n## Expected behavior\r\n\r\nMost certificate errors get handled properly by my code (and celery's logging), simply logging information about the certificate at `ERROR` level.\r\n\r\n## Actual behavior\r\n\r\nThis particular certificate causes a stacktrace in celery's logging.  Note that a simple logger setup has no problem with this, just celery's handling of it:\r\n\r\n```\r\n>>> requests.get('https://self-storage.virtualways.de')\r\nINFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): self-storage.virtualways.de\r\nERROR:requests.packages.urllib3.connection:Certificate did not match expected hostname: self-storage.virtualways.de. Certificate: {'caIssuers': ('http://crt.comodoca.com/COMODORSAOrganizationValidationSecureServerCA.crt',), 'subject': ((('countryName', 'DE'),), (('postalCode', '89077'),), (('stateOrProvinceName', 'BW'),), (('localityName', 'Ulm'),), (('streetAddress', 'S\u00f6flinger Stra\u00dfe 207'),), (('organizationName', 'EG-Shop GmbH'),), (('organizationalUnitName', 'Shop'),), (('organizationalUnitName', 'Authorized by United SSL'),), (('organizationalUnitName', 'InstantSSL'),), (('commonName', 'www.handwerkstraum.de'),)), 'serialNumber': '540A934851941CD6C2485635EA0BB93C', 'issuer': ((('countryName', 'GB'),), (('stateOrProvinceName', 'Greater Manchester'),), (('localityName', 'Salford'),), (('organizationName', 'COMODO CA Limited'),), (('commonName', 'COMODO RSA Organization Validation Secure Server CA'),)), 'notBefore': 'Sep  8 00:00:00 2016 GMT', 'OCSP': ('http://ocsp.comodoca.com',), 'subjectAltName': (('DNS', 'www.handwerkstraum.de'), ('DNS', 'handwerkstraum.de')), 'notAfter': 'Sep  8 23:59:59 2018 GMT', 'crlDistributionPoints': ('http://crl.comodoca.com/COMODORSAOrganizationValidationSecureServerCA.crl',), 'version': 3}\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3633/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tabascoterrier": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3631", "title": "WorkerLostError: Worker exited prematurely: exitcode 0 when shutting down worker", "body": "I'm new to Celery, so do correct me if I'm missing the obvious!\r\n\r\nWhen Ctrl+Cing (or sending SIGTERM) to the Celery process, Im getting WorkerLostErrors a lot of (but not all) the time:\r\n```\r\n[2016-11-29 16:18:07,837: ERROR/MainProcess] Task handler raised error: WorkerLostError('Worker exited prematurely: exitcode 0.',)\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/venv/lib/python3.5/site-packages/celery/worker/worker.py\", line 203, in start\r\n    self.blueprint.start(self)\r\n  File \"/home/ubuntu/venv/lib/python3.5/site-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/home/ubuntu/venv/lib/python3.5/site-packages/celery/bootsteps.py\", line 370, in start\r\n    return self.obj.start()\r\n  File \"/home/ubuntu/venv/lib/python3.5/site-packages/celery/worker/consumer/consumer.py\", line 318, in start\r\n    blueprint.start(self)\r\n  File \"/home/ubuntu/venv/lib/python3.5/site-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/home/ubuntu/venv/lib/python3.5/site-packages/celery/worker/consumer/consumer.py\", line 593, in start\r\n    c.loop(*c.loop_args())\r\n  File \"/home/ubuntu/venv/lib/python3.5/site-packages/celery/worker/loops.py\", line 88, in asynloop\r\n    next(loop)\r\n  File \"/home/ubuntu/venv/lib/python3.5/site-packages/kombu/async/hub.py\", line 289, in create_loop\r\n    events = poll(poll_timeout)\r\n  File \"/home/ubuntu/venv/lib/python3.5/site-packages/kombu/utils/eventio.py\", line 84, in poll\r\n    return self._epoll.poll(timeout if timeout is not None else -1)\r\n  File \"/home/ubuntu/venv/lib/python3.5/site-packages/celery/apps/worker.py\", line 282, in _handle_request\r\n    raise exc(exitcode)\r\ncelery.exceptions.WorkerShutdown: 1\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/venv/lib/python3.5/site-packages/billiard/pool.py\", line 1224, in mark_as_worker_lost\r\n    human_status(exitcode)),\r\nbilliard.exceptions.WorkerLostError: Worker exited prematurely: exitcode 0.\r\n```\r\n\r\nI'm seeing this on both macOS 10.12.1 and Ubuntu 16.04 \r\n\r\nTo reproduce:\r\nLoad the queue:\r\npython task_test.py\r\n\r\nRun the worker:\r\ncelery worker -A task_test --loglevel=INFO --concurrency=10\r\n\r\nGive it a few seconds for some tasks to run and complete, and Ctrl+C the worker.\r\n\r\nExpected behaviour: Tasks exit cleanly\r\n\r\nObserved behaviour: WorkerLostErrors\r\n\r\n\r\nMinimal test case task_test.py:\r\n```python\r\nimport time\r\nimport random\r\nfrom celery import Celery\r\n\r\napp = Celery('tasks', broker='redis://127.0.0.1')\r\n\r\n@app.task(name='test_task')\r\ndef test_task(wait_for):\r\n    time.sleep(wait_for)\r\n\r\nif __name__ == '__main__':\r\n    for x in range(1000):\r\n        test_task.delay(random.randrange(1, 10))\r\n```\r\n\r\n\r\ncelery -A task_test report\r\n```\r\nsoftware -> celery:4.0.0 (latentcall) kombu:4.0.0 py:3.5.2\r\n            billiard:3.5.0.2 redis:2.10.5\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:disabled\r\n\r\nbroker_url: 'redis://127.0.0.1:6379//'\r\n```\r\n\r\nI'm seeing this on both macOS 10.12.1 and Ubuntu 16.04 \r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3631/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "darklow": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3628", "title": "How to get task name in TaskResult / django-db result backend?", "body": "Is there a way to get task name by TaskResult instance when using `django-db` as `CELERY_RESULT_BACKEND`? Or some way to extend result backend handling and save extra data like task name and queue along with TaskResult result and other info which already exists.\r\n\r\nI understand that the point of result backend is to check status of task by ID, however if there is already task `result` and `traceback` for failed tasks, why there isn't such a core thing as task name? \r\n\r\nThank you.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3628/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ssudake21": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3625", "title": "On_message not working with rabbitmq backend", "body": "## Checklist\r\n\r\n- [X] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [X] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\nCelery report output.\r\n```\r\n[root@zm03 script]# celery -A celery_test report\r\n\r\nsoftware -> celery:4.0.0 (latentcall) kombu:4.0.0 py:2.7.9\r\n            billiard:3.5.0.2 py-amqp:2.1.1\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:amqp://guest@192.168.1.201//\r\n```\r\n## Steps to reproduce\r\n\r\nhttp://stackoverflow.com/questions/40784593/celery-raise-improperlyconfigured-exception\r\n\r\n## Expected behavior\r\nExpected to work\r\n## Actual behavior\r\n\r\n```\r\nTraceback (most recent call last):   \r\nFile \"test.py\", line 8, in <module> print(r.get(on_message=on_raw_message, propagate=False))   \r\nFile \"/usr/local/python2.7/lib/python2.7/site-packages/celery/result.py\", line 189, in get on_message=on_message,   \r\nFile \"/usr/local/python2.7/lib/python2.7/site-packages/celery/backends/base.py\", line 460, in wait_for_pending 'Backend does not support on_message callback')  \r\ncelery.exceptions.ImproperlyConfigured: Backend does not support on_message callback\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3625/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/89519572", "body": "Wonder if this condition can cause following issue? Mq backed is not supported for on_message ? @xBeAsTx @auvipy \r\nhttp://stackoverflow.com/questions/40784593/celery-raise-improperlyconfigured-exception", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/89519572/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "pymonger": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3617", "title": "running 5000 celery workers in AWS EC2", "body": "Hello,\r\n\r\nI'm running my celery workers in AWS spot market using an AutoScaling group. I'm trying to reach 5000 EC2 instances where each instance runs a single celery worker. All is well until we scale up to about 4300 instances. At that point, the new instances that come up fail to connect to RabbitMQ with the following error:\r\n```\r\n[2016-11-22 17:20:40,144: ERROR/MainProcess] consumer: Cannot connect to amqp://guest:**@172.31.0.173:5672//: timed out.\r\nTrying again in 32.00 seconds...\r\n```\r\n\r\nThis may be more of a question on how to scale up RabbitMQ but if you have any insights or suggestions it would be great.\r\n\r\nMy settings are:\r\n```\r\nsoftware -> celery:3.1.25 (Cipater) kombu:3.0.37 py:2.7.5\r\n            billiard:3.3.0.23 py-amqp:1.4.9\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:redis://127.0.0.1/\r\n\r\nCELERY_ACCEPT_CONTENT: ['msgpack']\r\nBROKER_HEARTBEAT: 300\r\nCELERY_TRACK_STARTED: True\r\nCELERY_TIMEZONE: 'US/Pacific-New'\r\nCELERY_SEND_TASK_SENT_EVENT: True\r\nBROKER_URL: 'amqp://guest:********@172.31.0.173:5672//'\r\nCELERY_ENABLE_UTC: True\r\nCELERY_RESULT_BACKEND: 'redis://127.0.0.1/'\r\nCELERY_TASK_SERIALIZER: 'msgpack'\r\nUSER_RULES_DATASET_INDEX: 'user_rules'\r\nCELERY_SEND_TASK_ERROR_EMAILS: False\r\nCELERY_ACKS_LATE: True\r\nCELERY_QUEUE_MAX_PRIORITY: 10\r\nCELERY_SEND_EVENTS: True\r\nCELERY_TASK_RESULT_EXPIRES: 86400\r\nCELERY_IMPORTS: ['proj.job_worker']\r\nCELERY_EVENT_SERIALIZER: 'msgpack'\r\nCELERYD_PREFETCH_MULTIPLIER: 1\r\nCELERY_RESULT_SERIALIZER: 'msgpack'\r\nBROKER_HEARTBEAT_CHECKRATE: 5\r\n```\r\n\r\nEach worker runs in supervisord with the following command:\r\n```\r\ncelery worker --app=proj --concurrency=1 --loglevel=INFO -Q job_worker-small -n job_worker-small -O fair --without-mingle --without-gossip --heartbeat-interval=60\r\n```\r\n\r\nThe RabbitMQ server (v3.6.2) has an increase ulimit:\r\n```\r\nulimit -n 102400\r\n```\r\n\r\nAnd the configuration is:\r\n```\r\n[\r\n  { rabbit,\r\n    [\r\n      { loopback_users, [] },\r\n      { heartbeat, 300 },\r\n      { vm_memory_high_watermark, 100 },\r\n      { tcp_listen_options, [ binary,\r\n                              { packet, raw },\r\n                              { reuseaddr, true },\r\n                              { backlog, 4096 },\r\n                              { nodelay, true },\r\n                              { exit_on_close, false },\r\n                              { keepalive, true }\r\n                           ]\r\n      }\r\n    ]\r\n  }\r\n].\r\n```\r\n\r\nThanks,\r\n\r\nGerald", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3617/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "imanhodjaev": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3596", "title": "AsyncResult task existence check with arbitrary task id", "body": "## Checklist\r\n\r\n- [x] I have verified that the issue exists against the latest stable version of Celery 4.0.0.\r\n- [x] I updated my Django project to use the latest stable Celery.\r\n\r\n## Steps to reproduce\r\n\r\n1. Open Celery shell (command might look like `celery shell -A proj.celery_app:app`) and repeat the following instructions\r\n\r\n```python\r\nIn [21]: from celery.result import AsyncResult\r\nIn [22]: res = AsyncResult('ANYTHING')\r\nIn [23]: res.status\r\nOut[23]: u'PENDING'\r\nIn [24]: res.state\r\nOut[24]: u'PENDING'\r\nIn [25]: res.ready()\r\nOut[25]: False\r\n```\r\n\r\n## Expected behavior\r\nIt should at least return error or raise exception when you try to call methods or provide some method like `.exists()` to check if task exists in the queue.\r\n\r\n\r\n\r\n\r\nAlso there are some workarounds on this like http://stackoverflow.com/questions/9824172/find-out-whether-celery-task-exists but it is about hacks as well as people use custom solutions to track tasks.\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3596/reactions", "total_count": 5, "+1": 5, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "The-Fonz": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3590", "title": "Chain within group does not get passed arg", "body": "## Checklist\r\n\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n\r\n```\r\nsoftware -> celery:4.0.0 (latentcall) kombu:4.0.0 py:3.5.1+\r\n            billiard:3.5.0.2 redis:2.10.5\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis://localhost:6379/0\r\n\r\naccept_content: {'pickle'}\r\ntask_serializer: 'pickle'\r\nbroker_url: 'redis://localhost:6379/0'\r\nresult_backend: 'redis://localhost:6379/0'\r\nresult_serializer: 'pickle'\r\n```\r\n\r\n## Steps to reproduce\r\nTake any task, e.g.:\r\n\r\n```\r\n@app.task()\r\ndef printargs(*args, **kwargs):\r\n    logger.info(args)\r\n    logger.info(kwargs)\r\n    return args\r\n```\r\n\r\nThen run a chain within a group:\r\n`group((printargs.s(n='ca') | group(printargs.s(n='cga'),printargs.s(n='cgb'))),printargs.s(n='f')).delay('there')`\r\n\r\nAnd observe that tasks 'ca', 'cga', and 'cgb' do not get passed the 'there' argument, only task 'f' does. This happens for amqp or redis, with json or pickle serialization.\r\n\r\n## Expected behavior\r\nThe chain receives the argument given to the group just like any other task in the group.\r\n\r\n## Actual behavior\r\nThe nested chain does not get passed the argument given to the group.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3590/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jcsaaddupuy": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3586", "title": "Celery does not respect exceptions types when using a serializer different than pickle.", "body": "## Checklist\r\n```\r\n~ : celery -A analystick report\r\nsoftware -> celery:4.0.0 (latentcall) kombu:4.0.0 py:3.5.2\r\n            billiard:3.5.0.2 redis:2.10.5\r\nplatform -> system:Darwin arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis://127.0.0.1:6379/1\r\n```\r\n\r\n## Steps to reproduce\r\n(See example code below)\r\n\r\n## Expected behavior\r\n\r\n__When using a result serializer different than pickle__, exceptions types should be the same as the raised exception.\r\n\r\n## Actual behavior\r\nCelery does not respect the exception type but create _a new type_ instead.\r\n\r\nThe main problem is that instead of using the actual type of the exception, celery will [reconstruct a type](https://github.com/celery/celery/blob/0f87321df385c5f3dca717ec2a4a9c0d25f88054/celery/utils/serialization.py#L43-L45) on the fly, but without respecting the original exception module.\r\n\r\nFor example, using the `yaml` result serializer (I believe it will be the same for `json`): \r\n- if a task raises a `ValueError`, the caller will receive a `celery.backends.base.ValueError`\r\n- if a task raises a `custom.module.CustomError`, the caller will receive a `celery.backends.base.CustomError`\r\n\r\n\r\nThis ends with wrongs behaviour when raising a exception from a task and trying to catch it from the caller.\r\n\r\n### Minimal reproductible test\r\nAs an example, I've setup a minimal reproductible test, using a redis backend :\r\n\r\ncelery config (I can provide a full config if needed):\r\n```python\r\nCELERY_TASK_SERIALIZER = 'yaml'\r\nCELERY_RESULT_SERIALIZER='yaml'\r\n```\r\n\r\nTasks : \r\n\r\n```python\r\n# module myapp.tasks\r\nfrom myapp import celery_app\r\n@celery_app.task\r\ndef raises_valueerror():\r\n    raise ValueError('Builtin exception')\r\n\r\nclass CustomError(Exception):\r\n    pass\r\n\r\n@celery_app.task\r\ndef raises_customerror():\r\n    raise CustomError('Custom exception', {'a':1})\r\n```\r\n\r\nUnittest : \r\n```python\r\nfrom myapp import tasks\r\nfrom myapp.tasks import CustomError\r\n\r\ndef test_builtin_exception():\r\n    t = tasks.raises_valueerror.s()\r\n    r = t.apply_async()\r\n\r\n    exc = None\r\n    try:\r\n        r.get(propagate=True)\r\n    except Exception as e:\r\n        exc = e\r\n\r\n    # with celery bug, the actual class of exc will be  `celery.backends.base.ValueError` instead of builtin ValueError\r\n    assert isinstance(exc, ValueError), \"Actual class %s}\" % (exc.__class__)\r\n\r\ndef test_custom_exception():\r\n    t = tasks.raises_customerror.s()\r\n    r = t.apply_async()\r\n\r\n    exc = None\r\n    try:\r\n        r.get(propagate=True)\r\n    except Exception as e:\r\n        exc = e\r\n\r\n    # with celery bug, the actual class of exc will be  `celery.backends.base.CustomError` instead of builtin CustomError\r\n    assert isinstance(exc, CustomError), \"1/2 Actual class is %s\" % (exc.__class__)\r\n    assert isinstance(exc, tasks.CustomError), \"2/2 Actual class is %s\" % (exc.__class__)\r\n\r\n```\r\n\r\nTheses tests will fail with the following errors : \r\n\r\n```\r\n# ...\r\nAssertionError: Actual class <class 'celery.backends.base.ValueError'>\r\n# ...\r\nAssertionError: 1/2 Actual class is <class 'celery.backends.base.CustomError'>\r\n```\r\n\r\nAnother side effect for this problem will be that a code like the one below won't work if a subtask raise a `ValueError`, as the propagated exception won't be of the builtin type `ValueError` but `celery.backends.base.ValueError`:\r\n\r\n```python\r\ntry:\r\n    r.get(propagate=True)\r\nexcept ValueError as e:\r\n    # do something        \r\n```\r\n\r\nThis problem will be the same also for any custom exceptions.\r\n\r\nWhile I'm not sure about the possible side-effects, [I have a fix for this](https://github.com/jcsaaddupuy/celery/commit/8d4e613e24f6561fdaafd4e6ede582ceac882804) and I will gladly create a PR for this problem as it seems pretty critical.\r\n\r\nWhat do you think ?\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3586/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3592", "title": "fixes exception deserialization when not using pickle", "body": "fix for #3586 \r\n\r\nThis PR try to fix how exceptions are deserialized when using a serializer different than pickle.\r\n\r\nThis avoid to [create new types](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/utils/serialization.py#L46) for exceptions, by doing 2 things : \r\n\r\n- store the exception module in [celery/celery/backends/base.py::Backend::prepare_exception](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L243-L245) and reuse it in [celery/celery/backends/base.py::Backend::prepare_exception](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L249-L258) instead of using raw `__name__` as module name\r\n\r\n- in [ celery/celery/utils/serialization.py::create_exception_cls](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/utils/serialization.py#L86-L98), try to find the exception class either from  `__builtins__`, or from the excetion module. Fallback on current behavior (which may still be wrong)\r\n\r\nAlso, it uses `exc.args` in [celery/celery/backends/base.py::Backend::prepare_exception](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L247) and pass it to the exception constructor in [celery/celery/backends/base.py::Backend::exception_to_python](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L255), instead of using `str(exc.message)` which could lead to unwanted behavior\r\n\r\nThis won't work in every cases. If a class is defined locally in a function, this code won't be able to import the exception class using `import_module` and the old (wrong) behavior will still be used.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alekibango": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3576", "title": "make REVOKES_MAX and REVOKE_EXPIRES configurable", "body": "Values of REVOKE_EXPIRES and REVOKES_MAX in worker/state.py  are hardcoded.\r\n\r\nThis should be configurable. Some of us really needed to change this.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3576/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/16665987", "body": "this line is not really needed. _data always contains correct keys and timestamps. The only problem with _data is ordering.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/16665987/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/55784318", "body": "well, heap is actually not ordered as you might think...\nhttps://docs.python.org/2/library/heapq.html#priority-queue-implementation-notes says:\nThe interesting property of a heap is that its smallest element is always the root, heap[0].\nTry it yourself: \n\n``` py\nh=[x for x in range(10,1, -1)]\nheapq.heapify(h)\nprint(h)\n[2, 3, 4, 7, 6, 5, 8, 9, 10]\n```\n\nYou might use heapq.nsmallest(2*len(h), h), which you think might be fast. But then you need to filter out removed items.   Still, sorted(self._data.values()) is as fast as heapq.nsmallest(2*len(s._heap), s._heap).  I tried to measure few things before i settled on that line :)\n\nAll the time i was coding this, while being angry at heapq, the only possible method i see to improve this code is to just use fifo queue of fixed limited size, and forget extra expiration ideas. It might make the code simpler and faster. But adding revoked items from other workers should not go to the front of the queue... it should merge in correct time -- which was not yet done correctly, by the way. \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55784318/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55784544", "body": "this is prolly mistake on my side... i didnt notice any code using that.\nI will reorder it, along with arguments of init()\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55784544/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55785711", "body": "I am trying to call _refresh_heap()  only if size of _heap is bigger than acceptable (see variable _MAX_HEAP_PERCENTS_OVERLOAD , which is hardcoded to be  15% over the size of _data).\nReason: _refresh_heap is taking way too much time. It really makes the speed difference to make this refresh a bit lazy.  \n\nUntil we clean up the heap, REMOVED is needed to mark removed entries in the _heap.\nPrevious code relied on update to make (sometimes) refresh.  But by the way the code was used in the wild (_passing _data directly_), it failed to do update; instead it did many adds, without correct insertion timestamp. And the refresh (purge) method was never actually used there. \n\nNow we are rehashing, just when ammount of garbage in _heap requires it, on every add or update operation, along with purging all items which should be gone.. \nMy first attempts to fix this code was to run purge everytime... but now it is really thousands times faster.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55785711/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55786655", "body": "to see REMOVED items in _heap, try this:\n\n``` py\ns = LimitedSet(maxlen=20)\nfor i in range(8): \n    s.add(i)\ns.discard(2)\nprint(s._heap)\n```\n\nNote you must add at least 8 items, as 15 % of 8 is 1.2, which means we can have 1 discarded item marked as REMOVED. Discarding 2 of 8 items will initiate garbage collection (_refresh_heap()).\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55786655/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55789758", "body": "On the other though, \nmaybe it is possible to effectively remove items from heap and avoid using marks.\nI will play with it a bit and report results to you here.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55789758/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55820630", "body": "Good reading about [deleting from heapq  at stackoverflow](http://stackoverflow.com/questions/5484929/removing-an-item-from-a-priority-queue)\n\nThere is a post showing how to remove item effectively if we know its position. But finding that position is also up to O(n) slow. So i guess my solution with lazy removing is best we can do with heapq.  Maybe we rather should take look at [treap](http://stromberg.dnsalias.org/~dstromberg/treap/) if we need something faster. Table comparing speed of different methods is very nice on that page.  \n\nSpeed of my code:\n\n``` py\ndef speedtest():\n    s = LimitedSet(maxlen=50000, minlen=20000, expires=1000)\n    start = time.time()\n    for i in range(1000000, 0, -1):\n        s.add(i)\n    middle = time.time()\n    for i in s:\n        s.discard(i)\n    end = time.time()\n    print('adding 1M items, keeping only 50k last:', middle - start)\n    print('deleting 50k items', end - middle)\n\nadding 1M items, keeping only 50k last: 8.85692286491\ndeleting 50k items 0.271380186081\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55820630/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55932184", "body": "What the check is doing: it makes sure we remove from data oldest item, ordering provided by _heap. Exactly one existing item from data.    \n\nItems are only deleted from _heap when we do refresh_heap() or pop() (which removes exactly one).\n\nThey are marked 'REMOVED' inside the _heap, when we discard(item) or  add(item) we already have in the set.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55932184/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "not-raspberry": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3571", "title": "Nested chords take exponential time (both in caller and in worker)", "body": "```\r\n$ celery -A app report\r\n\r\nsoftware -> celery:4.0.0 (latentcall) kombu:4.0.0 py:3.5.2\r\n            billiard:3.5.0.2 redis:2.10.5\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis://localhost:6379/11\r\n\r\nbroker_url: 'redis://localhost:6379/10'\r\nresult_backend: 'redis://localhost:6379/11'\r\n```\r\n\r\nHappens on master (b8fa241) and 4.0.0.\r\n\r\n## Steps to reproduce\r\napp.py (those chained groups could as well be chords):\r\n```py\r\nimport gc\r\nfrom timeit import timeit\r\nfrom celery import Celery, chain, group\r\nfrom redis import StrictRedis\r\n\r\n\r\nBROKER = 'redis://localhost:6379/10'\r\nBACKEND = 'redis://localhost:6379/11'\r\napp = Celery('abc', broker=BROKER, backend=BACKEND)\r\nbroker_redis = StrictRedis.from_url(BROKER)\r\nresult_redis = StrictRedis.from_url(BACKEND)\r\nbroker_redis.flushdb()\r\nresult_redis.flushdb()\r\n\r\n\r\n@app.task\r\ndef a_task(*a, **kw):\r\n    print('invoking', a, kw)\r\n\r\n\r\ndef make_group(base_name):\r\n    \"\"\"Return a group of 2 similarly named tasks.\"\"\"\r\n    return group([a_task.si('%s-A' % base_name), a_task.si('%s-B' % base_name)])\r\n\r\n\r\ndef build_canvas(depth):\r\n    callback = a_task.si(depth)\r\n    if depth == 0:\r\n        return callback\r\n    else:\r\n        return chain(\r\n            make_group(depth),\r\n            build_canvas(depth=depth - 1)\r\n        )\r\n\r\n\r\ndef build_canvas_unrolled():\r\n    \"\"\"\r\n    In case you suspect the recursive function above is botched.\r\n\r\n    Uncomment the lines below and invoke the task to observe the time increasing.\r\n    \"\"\"\r\n    from itertools import count\r\n    c = count()\r\n    return chain(\r\n        #make_group(next(c)),\r\n        #chain(\r\n        #  make_group(next(c)),\r\n        #  chain(\r\n                make_group(next(c)),\r\n                chain(\r\n                    make_group(next(c)),\r\n                    chain(\r\n                        make_group(next(c)),\r\n                        chain(\r\n                            make_group(next(c)),\r\n                            chain(\r\n                                make_group(next(c)),\r\n                                chain(\r\n                                    make_group(next(c)),\r\n                                    chain(\r\n                                        make_group(next(c)),\r\n                                        chain(\r\n                                            make_group(next(c)),\r\n                                            a_task.si('fin')\r\n                                        )\r\n                                    )\r\n                                )\r\n                            )\r\n                        )\r\n                    )\r\n                )\r\n        #   )\r\n        #)\r\n    )\r\n\r\n\r\ndef probe(depth):\r\n    canvas = build_canvas(depth)\r\n    print('=' * 10)\r\n    print('Canvas depth:', depth)\r\n    print('Canvas:', canvas)\r\n    print('Took:', timeit('canvas.delay()',\r\n                          globals={'canvas': canvas},\r\n                          number=1))\r\n\r\n    print('Broker redis size:', broker_redis.info()['used_memory_human'],\r\n          'Peak:', broker_redis.info()['used_memory_human'])\r\n    print('Result redis size:', result_redis.info()['used_memory_human'],\r\n          'Peak:', result_redis.info()['used_memory_human'])\r\n\r\n    # Who knows...\r\n    del canvas\r\n    gc.collect()\r\n    broker_redis.flushdb()\r\n    result_redis.flushdb()\r\n\r\n\r\nif __name__ == '__main__':\r\n    print('Warm up in case of lazy imports')\r\n    probe(3)\r\n    print('Warm-up finished')\r\n\r\n    print('============================= Actual measurements start here.')\r\n\r\n    for depth in range(1, 11):\r\n        probe(depth)\r\n```\r\n\r\nRun: `$ python app.py`\r\n\r\n## Expected behavior\r\nChords are scheduled in reasonable time - probably linear.\r\n\r\n## Actual behavior\r\nChords schedule in exponential time:\r\n\r\n### Measurements\r\nLines \"bend\" when depth=8 because I made the measurements with depth<8 repeat trice, but failed to clean the redis between them. It's not super important, just don't assume that artifact is relevant. The code above is fixed.\r\n\r\nChart of times: http://www.wolframalpha.com/input/?i=0.009336177026852965+0.012298419023863971+0.013047932996414602+0.021236155065707862+0.055250592064112425+0.19522034004330635+0.7236925609176978+2.9150424430845305+4.2594310390995815+20.646082949009724\r\n\r\nChart of Redis memory (both DBs are actually the same Redis): http://www.wolframalpha.com/input/?i=722880+738380+792820+1050000+2120000+6370000+21650000+108670000+144670000+552680000\r\n\r\n```\r\n$ python app.py  # Run without active workers.\r\nWarm up in case of lazy imports\r\n==========\r\nCanvas depth: 3\r\nCanvas: %celery.chord([abc.a_task('3-A'), abc.a_task('3-B')], header=[abc.a_task('2-A'), abc.a_task('2-B')], body=%abc.a_task([a_task('1-A'), a_task('1-B')], 0), kwargs={})\r\nTook: 0.05945688800420612\r\nBroker redis size: 823.21K Peak: 823.21K\r\nResult redis size: 715.26K Peak: 715.26K\r\nWarm-up finished\r\n============================= Actual measurements start here.\r\n==========\r\nCanvas depth: 1\r\nCanvas: %abc.a_task([a_task('1-A'), a_task('1-B')], 0)\r\nTook: 0.009336177026852965\r\nBroker redis size: 722.88K Peak: 722.88K\r\nResult redis size: 722.88K Peak: 722.88K\r\n==========\r\nCanvas depth: 2\r\nCanvas: %celery.chord([abc.a_task('2-A'), abc.a_task('2-B')], header=[abc.a_task('1-A'), abc.a_task('1-B')], body=abc.a_task(0), kwargs={})\r\nTook: 0.012298419023863971\r\nBroker redis size: 738.38K Peak: 738.38K\r\nResult redis size: 738.38K Peak: 738.38K\r\n==========\r\nCanvas depth: 3\r\nCanvas: %celery.chord([abc.a_task('3-A'), abc.a_task('3-B')], header=[abc.a_task('2-A'), abc.a_task('2-B')], body=%abc.a_task([a_task('1-A'), a_task('1-B')], 0), kwargs={})\r\nTook: 0.013047932996414602\r\nBroker redis size: 756.84K Peak: 756.84K\r\nResult redis size: 792.82K Peak: 792.82K\r\n==========\r\nCanvas depth: 4\r\nCanvas: %celery.chord([abc.a_task('4-A'), abc.a_task('4-B')], header=[abc.a_task('3-A'), abc.a_task('3-B')], body=%chord([abc.a_task('2-A'), abc.a_task('2-B')], header=[abc.a_task('1-A'), abc.a_task('1-B')], body=abc.a_task(0), kwargs={}), kwargs={})\r\nTook: 0.021236155065707862\r\nBroker redis size: 1.05M Peak: 1.05M\r\nResult redis size: 1.05M Peak: 1.05M\r\n==========\r\nCanvas depth: 5\r\nCanvas: %celery.chord([abc.a_task('5-A'), abc.a_task('5-B')], header=[abc.a_task('4-A'), abc.a_task('4-B')], body=%chord([abc.a_task('3-A'), abc.a_task('3-B')], header=[abc.a_task('2-A'), abc.a_task('2-B')], body=%abc.a_task([a_task('1-A'), a_task('1-B')], 0), kwargs={}), kwargs={})\r\nTook: 0.055250592064112425\r\nBroker redis size: 2.08M Peak: 2.08M\r\nResult redis size: 2.12M Peak: 2.12M\r\n==========\r\nCanvas depth: 6\r\nCanvas: %celery.chord([abc.a_task('6-A'), abc.a_task('6-B')], header=[abc.a_task('5-A'), abc.a_task('5-B')], body=%chord([abc.a_task('4-A'), abc.a_task('4-B')], header=[abc.a_task('3-A'), abc.a_task('3-B')], body=%chord([abc.a_task('2-A'), abc.a_task('2-B')], header=[abc.a_task('1-A'), abc.a_task('1-B')], body=abc.a_task(0), kwargs={}), kwargs={}), kwargs={})\r\nTook: 0.19522034004330635\r\nBroker redis size: 6.33M Peak: 6.33M\r\nResult redis size: 6.37M Peak: 6.37M\r\n==========\r\nCanvas depth: 7\r\nCanvas: %celery.chord([abc.a_task('7-A'), abc.a_task('7-B')], header=[abc.a_task('6-A'), abc.a_task('6-B')], body=%chord([abc.a_task('5-A'), abc.a_task('5-B')], header=[abc.a_task('4-A'), abc.a_task('4-B')], body=%chord([abc.a_task('3-A'), abc.a_task('3-B')], header=[abc.a_task('2-A'), abc.a_task('2-B')], body=%abc.a_task([a_task('1-A'), a_task('1-B')], 0), kwargs={}), kwargs={}), kwargs={})\r\nTook: 0.7236925609176978\r\nBroker redis size: 21.61M Peak: 21.61M\r\nResult redis size: 21.65M Peak: 21.65M\r\n==========\r\nCanvas depth: 8\r\nCanvas: %celery.chord([abc.a_task('8-A'), abc.a_task('8-B')], header=[abc.a_task('7-A'), abc.a_task('7-B')], body=%chord([abc.a_task('6-A'), abc.a_task('6-B')], header=[abc.a_task('5-A'), abc.a_task('5-B')], body=%chord([abc.a_task('4-A'), abc.a_task('4-B')], header=[abc.a_task('3-A'), abc.a_task('3-B')], body=%chord([abc.a_task('2-A'), abc.a_task('2-B')], header=[abc.a_task('1-A'), abc.a_task('1-B')], body=abc.a_task(0), kwargs={}), kwargs={}), kwargs={}), kwargs={})\r\nTook: 2.9150424430845305\r\nBroker redis size: 108.63M Peak: 108.63M\r\nResult redis size: 108.67M Peak: 108.67M\r\n==========\r\nCanvas depth: 9\r\nCanvas: %celery.chord([abc.a_task('9-A'), abc.a_task('9-B')], header=[abc.a_task('8-A'), abc.a_task('8-B')], body=%chord([abc.a_task('7-A'), abc.a_task('7-B')], header=[abc.a_task('6-A'), abc.a_task('6-B')], body=%chord([abc.a_task('5-A'), abc.a_task('5-B')], header=[abc.a_task('4-A'), abc.a_task('4-B')], body=%chord([abc.a_task('3-A'), abc.a_task('3-B')], header=[abc.a_task('2-A'), abc.a_task('2-B')], body=%abc.a_task([a_task('1-A'), a_task('1-B')], 0), kwargs={}), kwargs={}), kwargs={}), kwargs={})\r\nTook: 4.2594310390995815\r\nBroker redis size: 144.64M Peak: 144.64M\r\nResult redis size: 144.67M Peak: 144.67M\r\n==========\r\nCanvas depth: 10\r\nCanvas: %celery.chord([abc.a_task('10-A'), abc.a_task('10-B')], header=[abc.a_task('9-A'), abc.a_task('9-B')], body=%chord([abc.a_task('8-A'), abc.a_task('8-B')], header=[abc.a_task('7-A'), abc.a_task('7-B')], body=%chord([abc.a_task('6-A'), abc.a_task('6-B')], header=[abc.a_task('5-A'), abc.a_task('5-B')], body=%chord([abc.a_task('4-A'), abc.a_task('4-B')], header=[abc.a_task('3-A'), abc.a_task('3-B')], body=%chord([abc.a_task('2-A'), abc.a_task('2-B')], header=[abc.a_task('1-A'), abc.a_task('1-B')], body=abc.a_task(0), kwargs={}), kwargs={}), kwargs={}), kwargs={}), kwargs={})\r\nTook: 20.646082949009724\r\nBroker redis size: 552.64M Peak: 552.64M\r\nResult redis size: 552.68M Peak: 552.68M\r\n```\r\n\r\n\r\nThe `b'celery\\x06\\x169'` list items in the broker redis seem to be heavily duplicated.\r\nThe list after executing a canvas with `depth=4` (deeply deserialised by hand and pretty-printed as json): https://gist.github.com/not-raspberry/ee60565b584d0342c96345d744baaa3d", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3571/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sahilkataria": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3561", "title": "Celery + SQLAlchemy with RabbitMQ broker hosted on compose.io resulting in (psycopg2.OperationalError) SSL SYSCALL error: EOF detected", "body": "We are using celery to send some emails in the background. The celery task connects to the database to lookup some order related information and basically sends the email through mandrill. Our app stack is as follows:\r\n```\r\n- flask (0.10.1)\r\n- SQLAlchemy (1.0.15)\r\n- PostgreSQL 9.4.9\r\n- Celery (3.1.23)\r\n- RabbitMQ broker hosted on compose.io (3.6.5) - SSL enabled\r\n```\r\n\r\nHere are the celery config parameters we're using:\r\n```python\r\nCELERY_ACKS_LATE=True\r\nBROKER_USE_SSL = True\r\nCELERYD_MAX_TASKS_PER_CHILD=20\r\n```\r\n\r\nHere's how we create the SQL engine, session and the task:\r\n```python \r\ndb_engine = create_engine(\r\n        'postgresql+psycopg2://{0}:{1}@{2}:{3}/{4}'\r\n        .format(sql_username, sql_password, db_ip, db_port, db_name)\r\n    )\r\ndb_session = scoped_session(sessionmaker(autocommit=False, autoflush=False))\r\ndb_session.bind = db_engine\r\nBase.metadata.bind = db_engine\r\n\r\nclass DBTask(Task):\r\n  _session = None\r\n\r\n  def after_return(self, *args, **kwargs):\r\n    if self._session is not None:\r\n      db_session.remove()\r\n      self._session = None\r\n\r\n  def on_failure(self, *args, **kwargs):\r\n    if self._session is not None:\r\n      db_session.remove()\r\n      self._session = None\r\n\r\n  @property\r\n  def session(self):\r\n    if self._session is None:\r\n      self._session = db_session\r\n\r\n    return self._session\r\n\r\n# the task sits in another file - only listed here as reference\r\n@celery.task(base=DBTask, bind=True, default_retry_delay=15 * 60, max_retries=4, ignore_result=True)\r\ndef some_email_task(self, order_id, other_args):\r\n   ...\r\n```\r\n\r\nAnd here's the output for ``celery -A proj report``\r\n```\r\nsoftware -> celery:3.1.23 (Cipater) kombu:3.0.37 py:3.4.3\r\n            billiard:3.3.0.23 py-amqp:1.4.9\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:disabled\r\nCELERY_ACKS_LATE: True\r\nRMQ_PASSWORD: '********'\r\nRMQ_PORT: '1234'\r\nAPP_ENV: 'DEV'\r\nCELERYD_MAX_TASKS_PER_CHILD: 20\r\nBROKER_URL: 'amqp://some_user:********@gcp-us-east1-cpu.0.dblayer.com:1234/some_app_name'\r\nCELERY_ACCEPT_CONTENT: ['pickle', 'json']\r\nRMQ_HOSTNAME: 'gcp-us-east1-cpu.0.dblayer.com'\r\nRMQ_NAME: 'some_app_name'\r\nBROKER_USE_SSL: True\r\nRMQ_USERNAME: 'some_user' \r\n```\r\n\r\n## The Issue\r\nThe celery workers start fine initially and seem to process tasks normally. But after they have been left to run for a few days, the workers seem to be going into an apparent stale connection state with the following error:\r\n\r\n> (psycopg2.OperationalError) SSL SYSCALL error: EOF detected\r\n\r\nThe error is non-recoverable and also results in the following subsequent error:\r\n\r\n> exception: (sqlalchemy.exc.InvalidRequestError) Can't reconnect until invalid transaction is rolled back\r\n\r\nThe workers need to be restarted to return them to a normal state. I came across a similar issue on your github (#1564) and was wondering if that might be what's wrong here? Does anyone have an idea if a \"register_after_fork\" would help resolve this issue here? Any help is greatly appreciated. ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3561/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jonathanstrong": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3545", "title": "subtask argument order compared to typical partial function application", "body": "For a task:\r\n\r\n```python\r\n@app.task\r\ndef double_identity(x, y):\r\n    return x, y\r\n```\r\n\r\nPartially applying this with `double_identity.s(1)` is the equivalent of `curry(flip(double_identity))(1)` in functional pseudo-code. In other words, why is the second argument \"prepended\" when the function is called with both arguments? \r\n\r\n```python\r\n>>> double_identity(1, 2)\r\n(1, 2)\r\n>>> double_identity.s(1)(2)\r\n(2, 1)\r\n```\r\nThis goes against every implementation of currying I've seen. Normally `f(x, y)` curried with one argument returns a function that takes `y` and applies `f` with `x` from the partial application and `y` from the new call. \r\n\r\nI found this to be a fairly annoying part of the API because it requires moving function arguments so that the variable argument is on the inside, where it won't be as useful in other applications. \r\n\r\nHow I normally use currying/partial application, for example with `toolz.curry`\r\n\r\n```python\r\nfrom toolz import curry\r\n\r\n@curry\r\ndef f(stable, variable):\r\n    # whatever....\r\n    return True \r\n\r\nresults = map(f(1), range(10))\r\n```\r\n\r\nClearly, at this point, you can't go changing this and breaking everyone's code. However, I did want to inquire about the feasibility of a possible solution. \r\n\r\nWould it be a) easy, b) hard, or c) impossible to allow the use of a special placeholder object to subtask that lets the user control the argument order? I'm not familiar with celery's internals at all - where would one start on something like that? \r\n\r\nSomething like: \r\n\r\n```python\r\nfrom celery.utils.functional import _ # make believe\r\n\r\nf = double_identity.s(1, _)(2) # -> (1, 2)\r\n```\r\n\r\nThanks for any insights you can provide. As a new user I was quite pleased to see the embrace of functional concepts throughout the code, so I figured this fairly esoteric concern might find some interest here. ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3545/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ask": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3531", "title": "Investigate support for using Serf for events/gossip.", "body": "", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3531/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/b5b64945daaba87a7d975b1b4fca08b0948b91bb", "message": "Fixes rst typo"}, {"url": "https://api.github.com/repos/celery/celery/commits/0dde9df9d8dd5dbbb97ef75a81757bc2d9a4b33e", "message": "Bump version: 4.0.1 \u2192 4.0.2"}, {"url": "https://api.github.com/repos/celery/celery/commits/f1626033d6d3efcd8d0337952d025a9767c4681e", "message": "Merge branch 'master' into 4.0"}, {"url": "https://api.github.com/repos/celery/celery/commits/66b3e80d832ac0039dc1fde01f9bbaacedb15603", "message": "Updates Changelog"}, {"url": "https://api.github.com/repos/celery/celery/commits/b2c90fd7df29719558666a0b57479ccf09186946", "message": "Now depends on kombu 4.0.2"}, {"url": "https://api.github.com/repos/celery/celery/commits/8c7ac5d84d245028f02b5d7f6a8684a1ce84dc9e", "message": "group cannot override dict.__iter__: now returns list of keys, not list of tasks.  Closes #3688"}, {"url": "https://api.github.com/repos/celery/celery/commits/199cf69f98f3aa655fd9ccd59a09d22de2716b2d", "message": "Merge branch 'master' of github.com:celery/celery"}, {"url": "https://api.github.com/repos/celery/celery/commits/64a02078faaf12e1a8c6310814000292420362e6", "message": "Worker: Request.info should not contain message body, makes sure JSON serializable.  Closes #3667"}, {"url": "https://api.github.com/repos/celery/celery/commits/a2849a5b06fee3f10d1296712a3920534a151855", "message": "flakes"}, {"url": "https://api.github.com/repos/celery/celery/commits/2c0c449515b7304c561ce96e9c522f867213dbfe", "message": "Fixes test failure"}, {"url": "https://api.github.com/repos/celery/celery/commits/1e6910353a80c21a9a48430d3078fa81b3da7c38", "message": "Signal: Disable caching for all signals.  Closes #3670"}, {"url": "https://api.github.com/repos/celery/celery/commits/0204d00584eca2a1e549900bac7a1f4296da8026", "message": "Fixes pack(str) -> pack(bytes).  Closes #3674"}, {"url": "https://api.github.com/repos/celery/celery/commits/a22331857354e809b28d3c831467445bda8a13d8", "message": "saferepr: Support unicode bytes on Python 2.  Closes #3676"}, {"url": "https://api.github.com/repos/celery/celery/commits/a260de094b97ac2d9da3208a6a527de446f9db72", "message": "Tasks: Fixes type checking for function with kwargs and kwonlyargs.  Closes #3678"}, {"url": "https://api.github.com/repos/celery/celery/commits/56ff9caaf39151c7a9a930fc2bf245e2def141a8", "message": "Bump version: 4.0.0 \u2192 4.0.1"}, {"url": "https://api.github.com/repos/celery/celery/commits/d2c61b2467fbd4166dba273e06251b5514a6d5b1", "message": "Updates Changelog"}, {"url": "https://api.github.com/repos/celery/celery/commits/bf7935dafe1d05abcde44900268e4fe1eb5be05a", "message": "Merge branch '4.0'"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/15545", "body": "README: Fix typo \"task.register\" -> \"tasks.register\". Closed by 8b5685e5b11f8987ba56c28ccb47f6c139541384. Thanks gregoirecachet)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15545/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15546", "body": "What version is this? I thought I had fixed this already.\n\nThe examples in README should be updated. I think the best way of defining tasks now is using a Task class:\n\n```\n from celery.task import Task\n\n class MyTask(Task):\n     name = \"myapp.mytask\"\n\n     def run(self, x, y):\n         return x * y\n```\n\nand then\n\n```\n>>> from myapp.tasks import MyTask\n>>> MyTask.delay(2, 2)\n```\n\nas this makes it easier later to define a default routing_key for your task etc.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15546/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17205", "body": "This works now. I remember it didn't at some point, and I remember I fixed it, so unless it still doesn't work for you I'm closing this issue.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17205/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15554", "body": "oh, that's bad. Got to get rid of yadayada dependency anyway, it's been an old trashbag for utilities, and all that is used from it now is the PickledObjectField.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15554/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15555", "body": "Remove yadayada dependency. that means we've copy+pasted the\nPickledObjectField, when will djangosnippets ever die? :( Closed by fb582312905c5a1e001b6713be78ee2154b13204. Thansk\ngregoirecachet!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15555/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17182", "body": "I'm not sure if that's so bad. Test requirements is not the same as install requirements. Sad the Django test runner is so broken. Maybe I can get it in somewhere else. \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17182/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18353", "body": "Add the test-runner from yadayada into the repo so we don't depend on yadayada\nanymore. Closed by af9ba75e195fc740493c9af6dbe84105b369d640.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18353/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18428", "body": "Seems to be fine now. We'll re-open the issue if anyone says otherwise.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18428/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/39255", "body": "Implemented by 048d67f4bfb37c75f0a5d3dd4d0b4e05da400185 +  89626c59ee3a4da1e36612449f43362799ac0305\nAnd it really _is fast_ compared to the database/key-value store backends which uses polling to wait for the result.\n\nTo enable this back-end add the following setting to your settings.py/celeryconf.py:\n\n```\nCELERY_BACKEND=\"amqp\"\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/39255/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/38017", "body": "A sample implementation has been commited (794582ebb278b2f96080a4cf4a68f1e77c3b003b + 93f6c1810c1051f8bdea6a7eae21d111997388d00 + fe62c47cb04723af738192087b40caefd27cab6a ) but not tested yet. Currently seeking anyone willing to test this feature.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/38017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/38115", "body": "Task retries seems to be working with tests passing. Closed by 41a38bb25fcacb48ee925e4319d35af9ab89d2bf\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/38115/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17721", "body": "Use basic.consume instead of basic.get to receive messages. Closed by 72fa505c7dfcf52c3215c276de67e10728898e70. This\nmeans the CELERY_QUEUE_WAKEUP_AFTER and CELERY_EMPTY_MSG_EMIT_EVERY settings,\nand the -w|--wakeup-after command line option to celeryd has been removed.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17721/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17902", "body": "If delay() is hanging, I'm guessing it's not because of the database, but because it can't get a connection to the AMQP server. (amqplib's default timeout must be very high). Is the broker running? Are you running RabbitMQ?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18071", "body": "I set the default connection timeout to be 4 seconds. Closing this issue.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18071/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/28627", "body": "Re-opening the issue as setting the amqplib connection timeout didn't resolve it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/28627/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/43160", "body": "This is actually an issue with RabbitMQ and will be fixed in the 1.7 release. I added the following to the celery FAQ:\n\n RabbitMQ hangs if it isn't able to authenticate the current user,\nthe password doesn't match or the user does not have access to the vhost\nspecified.  Be sure to check your RabbitMQ logs\n(`/var/log/rabbitmq/rabbit.log` on most systems), it usually contains a\nmessage describing the reason.\n\nThis will be fixed in the RabbitMQ 1.7 release.\n\nFor more information see the relevant thread on the rabbitmq-discuss mailing\nlist: http://bit.ly/iTTbD\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/43160/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17924", "body": "AsyncResult.ready() was always True. Closed by 4775a4c279179c17784bb72dc329f9a9d442ff0a. Thanks talentless.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17924/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18110", "body": "Oh. That's indeed a problem with the installation. Could you try to install it using pip?\n\n```\n$ easy_install pip\n$ pip install celery\n```\n\nI will fix it as soon as possible, but in the mean time you could use pip.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18110/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18111", "body": "Only use README as long_description if the file exists so easy_install don't\nbreak. Closed by e8845afc1a53aeab5b30d82dea29de32eb46b1d6. Thanks tobycatlin\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18111/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/35169", "body": "The consumerset branch was merged into master in c01a9885bbb8c83846b3770364fe208977a093fd (original contribution: screeley/celery@e2d0a56c913c66f69bf0040c9b76f74f0bb7dbd8). Big thanks to Sean Creeley.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/35169/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18472", "body": "Fixed in  faaa58ca717f230fe8b65e4804ad709265b18d5a\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18472/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18432", "body": "By the way; This worked before we started using auto_ack=True, and basic.get.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18432/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18448", "body": "Wait with message acknowledgement until the task has actually been executed.\nClosed by ef3f82bcf3b4b3de6584dcfc4b189ddadb4f50e6.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18448/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/19017", "body": "This now merged into master. On second thought Munin plugins for these stats doesn't make sense, at least not generally. You could use this to make munin-plugins though, it's more like the groundwork for something more interesting later. (and it's useful for profiling right away)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/19017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18966", "body": "Make TaskSet.run() respect message options from the Task class. Closed by 03d30a32de3502b73bca370cb0e70863c0ad3dd2.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/78179", "body": "this isn't an issue anymore, the statistics code has been removed and replaced by celerymon.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/78179/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/38261", "body": "nodjango branch has been merged with master, for hints on how to use this see: http://ask.github.com/celery/faq.html#can-i-use-celery-without-django\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/38261/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/19940", "body": "I don't know if this is easily done, but there could be two variables:\n\n```\nCELERY_TASK_LIFETIME_SOFT_LIMIT\nCELERY_TASK_LIFETIME_HARD_LIMIT\n```\n\nWhen the soft limit has passed the worker raises an exception, \n    TaskLifetimeSoftLimitExceeded()\n\nwhich the task can catch to clean up any work.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/19940/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/39809", "body": "I'm closing this for now, it's an interesting feature but I can't see how the soft limit would be implemented.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/39809/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/20334", "body": "Fixed by  99a23ddb27e357d59b433\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/20334/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/21228", "body": "You should update to carrot >= 0.5.0!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/21228/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/21240", "body": "Sorry, too fast on the close trigger there. Tasks sent using an older version is actually incompatible with carrot 0.3.20 because of the new version of carrot (0.5.0) which uses the content-type attribute on the AMQP message to define the serialization scheme used. If the old tasks are important, you'd have to contact me for information on how to convert them, if not you can start celeryd with the --discard option to flush out the old messages.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/21240/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/26647", "body": "I think this can be achieved using the TaskSet? Have you looked at that? I would really like support for progress bars, like time estimates. etc.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/26647/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/39812", "body": "I'm setting this as a wishlist feature, if anyone is working on it, please add your name to it as a label.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/39812/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/230375", "body": "Closing this issue as no feedback has been received. If anyone still wants this feature, please outline a requirements list so it can be implemented.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/230375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/28458", "body": "Task.name is now automatically generated out of class module+name, e.g.\ndjangotwitter.tasks.UpdateStatusesTask. Very convenient, no idea why I didn't\ndo this before :) Closed by 2e38279ad3e4da0c6f570bacea8374602d3f670b\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/28458/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/34746", "body": "Seems to be something with multiprocessing. What version of multiprocessing are you running?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/34746/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/35172", "body": "Example on my machine:\n\n```\n>>> import multiprocessing\n>>> multiprocessing.__version__\n'2.6.1.1'\n```\n\nPlease try to do the following:\n\n```\n>>> from multiprocessing import get_logger, log_to_stderr\n>>> import logging\n>>> log_to_stderr()\n>>> logger = get_logger()\n>>> logger.setLevel(logging.DEBUG)\n>>> logger.handlers\n>>> logger.info(\"The quick brown fox...\")\n```\n\nof course It could be something with the Python version as well.\nAlso, you should upgrade to celery version 0.6.0  which was just released: http://pypi.python.org/pypi/celery/0.6.0\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/35172/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/39808", "body": "See http://code.google.com/p/python-multiprocessing/issues/detail?id=18\nand http://bugs.python.org/issue6720\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/39808/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/39948", "body": "This bug was introduced in multiprocessing 2.6.1.2.\nSolution: Install the previous version or apply the patch in the link above.\n\n```\n$ pip install -U multiprocessing==2.6.1.1\n```\n\nor\n\n```\neasy_install -U multiprocessing==2.6.1.1\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/39948/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/36040", "body": "Fixed by commit 584204ced6de7d3edf6fbadb87da66654af1a83a\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/36040/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/39807", "body": "see 637b4d887bcc44014d0250839dbd60a53c46746a\n\nit works as long as celeryd is not detached, I have no idea why it won't work when it's in the background\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/39807/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/109111", "body": "Added SIGHUP handler that restarts the worker. (This works now that we no longer have our own --detach). Closed by 4eed0eed29ecff76921c3d065f39a3551940e12a\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/109111/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/4575955", "body": "@nitzanm That sounds supervisor specific, isn't there a setting for supervisord to wait before respawning?\nIt's always going to take some time for a process to shutdown, and especially in celeryd's case which have to finish current tasks.  Also, maybe supervisord's own pidfile implementation (if any) has some workaround for this?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/4575955/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/5150020", "body": "Thanks for the pull request!\n\nI wonder if the other fix I did recently does not already solve the problem:\nf4fe9258b199dc4889056c77b41d12f8303c2b1e\n\nCould you try this?  (it's already part of celery 2.5.2)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/5150020/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/33953013", "body": "You should not use HUP. it's not reliable since the worker will be responsible for restarting itself.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/33953013/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/38730", "body": "Implement AMQP basic.qos (Quality of Service) to set the message prefetch\nwindow. Closed by c5b31df8501f4b786c941fab55bdfa97ca2d9bc4\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/38730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/39678", "body": "Add traceback to result value on failure. Closed by 929c842541e8e1a9894e6e9391a040738abd714c. **NOTE** If you use the\ndatabase backend you have to re-create the database table celery_taskmeta\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/39678/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/43159", "body": "We're now redirecting stdout+stderr to the celery log file when detached, so catching these errors will be a lot easier. (commit 16af178569c9ac3e27d3ef9923642622d319c319) \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/43159/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/41617", "body": "what OS are you running?\n\nWhy are you using postgres to run the tests? Please use SQLite, as this is what is used to run the tests, I wouldn't guarantee the tests to run with any other database platform, and that wouldn't necessarily mean anything.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/41617/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/41618", "body": "Oh, so you're running tests from your own application, and celerys tests are failing in that context. That Django is running tests for all applications in INSTALLED_APPS is wrong imho, and a pet peeve of mine. You should use a test runner that either 1) explicitly lists the apps you want to run tests for, or 2) make a test runner that skips tests for apps you don't want to run.\n\nFor example this test runner that celery is using:\nhttp://github.com/ask/celery/blob/f90491fe0194aa472b5aecdefe5cc83289e65e69/celery/tests/runners.py\n\nTo use this add the following to your settings.py:\n\n```\nTEST_RUNNER = \"celery.tests.runners.run_tests\"\nTEST_APPS = (\n    \"app1\",\n    \"app2\",\n    \"app3\",\n    \"app4\",\n )\n```\n\nIf you just want to skip celery you could use:\n\n```\nINSTALLED_APPS = (.....)\nTEST_RUNNER = \"celery.tests.runners.run_tests\"\nTEST_APPS = filter(lambda k: k != \"celery\", INSTALLED_APPS)\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/41618/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/43158", "body": "Added FAQ: The celery test-suite is failing. Closed by 66ce9ad61e22468ddac17a497a6bc0d10cecb4e6\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/43158/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/42684", "body": "TaskSetResult.join causes TypeError when timeout=None. Thanks Jerzy Kozera.  Closed by 7cb2855e93cf131c3c159778e0251e899f1639ab\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/42684/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/42683", "body": "views.apply should return HttpResponse instance. Thanks to Jerzy Kozera. Closed by bc4d31ac353607c3c0eebfa348da628ee985abe2\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/42683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/42913", "body": "Document default keyword arguments passed to Task.run. Closed by dc339d3dd3384f28deb3f1224c455d3394602465\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/42913/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/43202", "body": "Great catch! Thanks.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/43202/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/45282", "body": "Fixed by abecciu  ( 8cecd9f62d1f4faa5ac071011194a1aa0af19220 )\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/45282/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/69861", "body": "This is actually not needed anymore as we're removing the periodic task backends completely: http://wiki.github.com/ask/celery/rewriting-the-periodic-task-service\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/69861/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/50210", "body": "I made a patch that should make it work better, that is, it should at least be acknowledged: e19189ebda7d614b7b42bbde91018803a25ab036\nCould you try to see if it fixes it? That is acknowledges the message.\n\n> I don't think a bad signal should allow a task to fail, but even if it could that should be reflected in calling task.successful().\n\nI don't agree. I mean, it should definitely be acknowledged, but the signal should not be able to affect the status of the task at all. It's more like a callback. Especially the task_postrun signal is not even run until after the \"successful\" status is set. Does this make sense? The real problem here is that the message is not acknowledged (and that the error message is not printed, am I right?) This is a problem with the exception not being supported by mutliprocessing.Pool's pickling of it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/50210/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/50531", "body": "Ok, second try, I tested this locally and it works: 58b688e4f1815139d7036099bc7c0801b82880c4\n\nDoes it work for you?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/50531/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/50948", "body": "Great! Closing it then.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/50948/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/55441", "body": "Added to nocallables branch, although with some drastic changes.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/55441/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/55929", "body": "Mention AMQP result backend in docs/configuration. Closed by 684296c0cceded6e31324aa7e4f767f4493ae208\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/55929/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/64342", "body": "Hah. This is a mess. You're right, it should be AMQP_SERVER. But it will be renamed to AMQP_HOST in carrot v0.8.0. (by deprecation, server will still work but will issue a warning until carrot version 1.0 where it will be removed).\n\nI've changed all references to AMQP_SERVER, and will start supporting AMQP_HOST soon.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/64342/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/64337", "body": "Seems this is because of a broken pidfile, or the existence of a lockfile, try to see if the following files exists, and delete them: celery.pid, celeryd.pid.lock\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/64337/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/69857", "body": "Hey! Ok. I knew this code wasn't very well tested. What is left to support \"project.settings\"? Do you have a patch?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/69857/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/69904", "body": "Do you know if windows has something like os.environ? And if so, is that environ passed on to the launched process?\n\nIf we could use that we could set something like os.environ[\"CELERYD_SETTINGS_FILE\"] and os.environ[\"CELERYD_USE_DJANGO\"]\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/69904/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/109202", "body": "I think I may have found a solution for this now. Could you please try and see if 906a5c3917a9aec80067fa62038f5c45cc0bc3b1 fixes this bug?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/109202/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/114957", "body": "Great thinking there!\n\nBut if we need the full path, then we can't use DJANGO_SETTINGS_MODULE, right?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/114957/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/422483", "body": "This is not an issue any more afaik, as `detect_loader` isn't used anymore (django-celery is always setting the `CELERY_LOADER` environment variable).\n\nThe `project.settings` thing is still a problem though, but all that is needed is to\nlaunch `celeryd` with the following arguments:\n\n```\npython manage.py celeryd --settings=settings -l info\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/422483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/74875", "body": "Better still, is to use another backend, I think! The database backend is not very good for this kind of work. The most elegant and effective solution is the amqp backend. The amqp backend does not allow you to retrieve a result twice, but you could wrap it so it saves the result to the database the first time it's retrieved. The result would be very fast and with no polling.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/74875/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/77033", "body": "What memory leak is this btw, are you talking about when settings.DEBUG is on?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/77033/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/77118", "body": "oh. then that's something that ought to be a problem elsewhere as well. One of our tasks using django models is leaking some, very little though, it can take months to go out of memory. Haven't tracked it down yet, but it may be related.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/77118/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/420291", "body": "`Task.get_or_create` isn't used anymore, does this fix the issue?\n\nMaybe we could implement something like `backend.get_many`, to select the results\nfor more than one task at once (using sql `id IN ...` for example).\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/420291/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/141493", "body": "As Redis has a very unstable API, we have to wait for a while until it can be part of celery by default.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/141493/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/230372", "body": "the `backends/` package name will probably not change in the foreseeable future . The `CELERY_BACKEND` setting has been renamed to `CELERY_RESULT_BACKEND`.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/230372/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/74314", "body": "Mention CELERY_ALWAYS_EAGER in the documentation for apply_async/delay. Closed by 4e2b2c327fc57d4d276423db9fc8686dbc6df12e\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/74314/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/74297", "body": "Fixed in execute-refactor branch.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/74297/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/74874", "body": "This would be useful indeed, but when would have the on_success to be called?\n\nRight now the only reasonable place to execute it is in result.join(), because it's impossible to know when it's finished without waiting for all the tasks to finish. And then you could just as well do:\n\n```\n>>> res = TaskSet(..)\n>>> res.join()\n>>> # what you would do on_success\n```\n\nOr, how would you want such a thing to work?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/74874/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/74903", "body": "How do you think map-reduce does this? I can't see how it could be done without blocking somewhere. I think the only solution is to rethink your algorithms so you can do it asynchronously.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/74903/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/76213", "body": "Note that you can launch it from a Task. But remember that it blocks a worker process as long as it's waiting.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/76213/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/76162", "body": "thanks! Does it work now? Sorry for the inconvenience.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/76162/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/76209", "body": "Ok. Confimed fixed\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/76209/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/78181", "body": "Moved to billiard.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/78181/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/88054", "body": "Hey! Could you open up a new bug with the schedule filename request? I merged your code on this one, so I can close it. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/88054/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/88795", "body": "I have never seen this `HOSTNAME.MainThread-PID` file, what operating system are you running?\n\nBtw, I fixed your request for CELERYBEAT_SCHEDULE_FILENAME in 1762e216eabb6aebbd1c1fb8f3e8a193ca7c608b\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/88795/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/89305", "body": "Usually, the programs is started by root, they open up their privileged network ports, and write pidfiles, opens logs, etc, then it sets it's effective uid to the specified unprivileged user (www-data). So, if you start it as root, is that what happens? If you can't start with root, use another directory for the --pidfile= argument.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/89305/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/89329", "body": "I changed the process a little bit, so all privileged actions are done before the seteuid/setgid happens, also I allowed using usernames/groupnames instead of uid/gid.\n\nStill, now it works creating the pidfile/logfile, but it can't remove it at shutdown. I think the usual way to do this is to fork another process -> set the uid/gid in there and start the application -> the parent process waits for the fork to stop and cleans up, still being the privileged user. Sadly, forking and multiprocessing doesn't work well, as seen with the --supervised fiasco.\n\nWhat I can do is ignore any errors when trying to remove the pidfile, and then the initscript's stop function could do it instead. It removes stale pidfiles at startup anyway.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/89329/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/89330", "body": "Btw, the related commit is here: 1a7ebcc1c87a5160cd1645a3eb8054a36f1ab11e\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/89330/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/109106", "body": "Celery no longer does its own detaching as of 01a8a0ecbbf128ea6ad6ab7fa0344cafe8ce8bf2. There has been far too many problems with it, and since it works when detaching using start-stop-daemon, supervisord, launchd and so on, you are encouraged to use those tools instead.\n\n--uid and --gid flags has been removed, use the features available for this in your daemonization tool (start-stop-daemon, launchd, supervisord, etc)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/109106/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/83929", "body": "But it's not a given, you can use celery without using django.\n\nIt doesn't have a specific version requirement, so there's nothing stopping you from installing the svn version of django first (or after), is it?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/83929/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/84017", "body": "Why wouldn't the svn version be managed by setuptools? I'd say that is a problem with Django, not with listing it as a dependency?\n\nYou can run \"python setup.py develop\" in the django checkout, and it will be easily be registered as installed by setuptools.\n\nWhile you don't need to have a django project to use celery, celery still depends on Django (this might change in the future)\n\nI might accept this, but I need better reasons. To me it seems this is a problem with the old django distribution model, where you used trunk and shipped with all the \"reusable\" apps you needed.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/84017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/84041", "body": "What if we only add django to `install_requires` if it isn't already installed?\nThen you could use `PYTHONPATH` to manipulate it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/84041/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/84076", "body": "But this is just a problem with developers of django itself, or bleeding-edge development installs of django. This would be equally annoying if you're working on your own copy of e.g. amqplib installed via symlink or `PYTHONPATH`, and celery installed the stable version. If we only install django if the django package isn't importable, it would work for everyone, no?\n\n```\ntry:\n    import django\n except ImportError:\n    install_requires.append(\"django\")\n```\n\n(btw, Just removed django-unittest-depth from install_requires as it shouldn't be there, I can see why that would annoy you, just haven't fixed it yet :)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/84076/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/84078", "body": "one could even say this is a problem with setuptools notion of \"installed\". If a package is importable, isn't it then installed?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/84078/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/84097", "body": "Yes, but wouldn't it help in your use case? (having a development version of django)\n\nIt's crucial that it's possible to install celery in the standard way, either \"easy_install celery\" or \"pip install celery\", and it should work with sensible defaults out of the box, users shouldn't have to read any installation instructions to get that.\n\nI hope that we can remove django as a dependency at some point, but as of now this isn't the case, and I don't think sacrificing easy, familiar installation is a good idea.\n\nWith the import test, it should be possible to use a trunk checkout of django with buildout, pip requirement files, and easy_install/pip (as long as django is installed before celery), or wouldn't it?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/84097/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/247860", "body": "Fixed in djangofree branch. This branch will become celery 1.1.0. Django integration has been moved to http://ask.github.com/ask/django-celery\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/247860/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/88056", "body": "It's not important, you could use something like start-stop-daemon, or similar, but you probably want it detached. I've had a lot of problems with python-daemon+multiprocessing so far, so getting pretty tired of it.\n\nHave you seen anything about this bug anywhere else? (multiprocessing + MemoryError)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/88056/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/88057", "body": "That is, you probably want it running in the background, not necessarily using --detach\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/88057/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/109105", "body": "Celery no longer does its own detaching as of 01a8a0ecbbf128ea6ad6ab7fa0344cafe8ce8bf2. There has been far too many problems with it, and since it works when detaching using start-stop-daemon, supervisord, launchd and so on, you are encouraged to use those tools instead.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/109105/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/88055", "body": "This is a bug actually, I fixed it in d663c1b6154e597db9a6dac2ec633526c3e1bef9\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/88055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/89846", "body": "For the ClockService not stopping, you're right. I totally forgot that sleeping in a thread will stop it from shutting down until the sleep period has passed. I guess we have to set CELERYBEAT_MAX_LOOP_INTERVAL to a low value when embedded inside celeryd.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/89846/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/89853", "body": "Added two new commits: 35e3e69cf5eef09fb67eadbf1482d1f1ca78dbaa +  0a09648fe37f44fe46b865073eb350ae6dd7c91f\n\nPlease try and see if these fixes your problems.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/89853/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/90680", "body": "I fixed these now: http://github.com/ask/celery/blob/master/celery/platform.py#L23-84\n\nThe pidlockfile in python-daemon seems kinda silly, it uses lockfile which locks with the granularity of threads, where we only need to lock on the main process. So I now only use the single pidfile (which is the normal way of doing things anyway)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/90680/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/90847", "body": "Great. I'll close this issue then.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/90847/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/90184", "body": "Fix typo os.unlink(filename) -> os.unlink(path). Thanks dmishe. Closed by 15ebb814dabefa7ba3e4c5e7287f353e729a9c84\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/90184/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/90219", "body": "Is this using the master branch or 0.8.2?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/90219/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/90224", "body": "I think I made a fix: 2dbc041560a2c81693eefd1baf010f32411a074c\nCould you please test if this is working for you?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/90224/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/109112", "body": "Probably resolved.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/109112/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/169133", "body": "I tested this by moving the settings file in `examples/django/demoproject` to `examples/django`, and adding `demoproject.demoapp` to `INSTALLED_APPS`,\nthen running `celeryd` with:\n\n```\nPYTHONPATH=. DJANGO_SETTINGS_MODULE=settings django-admin.py    celeryd -l info\n```\n\nthe task was properly registered:\n\n```\n. tasks ->\n    . demoproject.demoapp.tasks.add\n```\n\nIf someone still has problems with this, they have to show how to reproduce this,\nas I'm not able to.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/169133/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/262423", "body": "Tests now passing without a running broker. Using the in-memory backend.\n\nClosed by 347bc2b80856a5ef36bbbc451823d476e9d47f99\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/262423/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/92725", "body": "TaskSet.run renamed to TaskSet.apply_async and is pending deprecation for v1.2.0. Closed by 819920bc8edea8cc155603c5cdb7f022057f6ff7.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/92725/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/95075", "body": "Fixes views.is_task_successful after removal of task.is_successful (thanks\nboardman) Closed by e5fe661b9586a17c0062d8a9caf9863ad619844d\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/95075/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/95201", "body": "Forgot to add new module: celery.utils.info. Closed by 6aaf99933a0cc1208fa5d86e8535301eefbabe23\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/95201/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/95581", "body": "Nice. I applied the patch to master. Thanks for your help!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/95581/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/96263", "body": "The loads error was because of differences in JSON implementations, fixed in ask/ghettoq@25c1f968838ba68f9bf43a96a33404ecb7252c77\n\nThe `redis.ResponseError` when it crashes is because of some strange effect with atexit handlers and the redis-py library. Haven't had the time to look into it yet. \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/96263/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/107593", "body": "Where do you get IMAPFetcher from? Could I get that code as well?\n\nBtw, I recently deployed some changes related to detaching in celery. The master branch contains the to become 1.0 code, and has a long list of backward incompatible changes, so if you're not yet using that branch you would probably be better off using the `stable` branch: http://github.com/ask/celery/tree/stable\n\n```\n$ git clone git://github.com/ask/celery.git\n$ cd celery\n$ git checkout stable\n$ python setup.py develop\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/107593/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/107595", "body": "Sorry the correct way to install latest changes to the stable branch is:\n\n```\n$ git clone git://github.com/ask/celery.git\n$ cd celery\n$ git checkout -b stable remotes/origin/stable\n$ python setup.py develop\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/107595/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/107910", "body": "You could paste it to pastie.org, or send it to askh@opera.com\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/107910/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/109107", "body": "Celery no longer does its own detaching as of 01a8a0ecbbf128ea6ad6ab7fa0344cafe8ce8bf2. There has been far too many problems with it, and since it works when detaching using start-stop-daemon, supervisord, launchd and so on, you are encouraged to use those tools instead.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/109107/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/108693", "body": "Thanks! Fixed in 28e52eeefc31beff9254591374488ea463389cd0\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/108693/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/109540", "body": "Fix outdated examples in cookbook/unit-testing. Thanks kmike. Closed by 31ace90f7f8fad015583e5a70c92ed2408742469\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/109540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/117661", "body": "I fixed some things in your code, but I couldn't get \"paster celeryd test.ini\" to work, it gives the error:\n\n```\nLookupError: No section 'main' (prefixed by 'app' or 'application' or 'composite' or 'composit' or 'pipeline' or 'filter-app') found in config /opt/devel/celery-paste/test.ini\n```\n\nAny idea? I don't have any experience using paste.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/117661/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/117666", "body": "I managed to get it to work: http://bitbucket.org/asksol/celery-paste/\n\nPlease try it out:\n\n```\n$ paster celeryd test.ini\n```\n\nthen in another session:\n\n```\npython tests/test_run.py\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/117666/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/254184", "body": "Interesting optimization but not crucial. Closing for now.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/254184/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/138512", "body": "This seems to have been fixed now, please verify so the issue can be closed.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/138512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/140603", "body": "Great news!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/140603/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/164225", "body": "Added Task.delivery_mode / CELERY_DEFAULT_DELIVERY_MODE, with this you can configure tasks to be non-persistent. Closed by 012ca1239ff1189f47211986835a8bd42c0b8f1c.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/164225/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/138515", "body": "I'm not sure what you mean by the above? Isn't that just adding the default queue?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/138515/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/133522", "body": "Fixed typo in redis's default port: 6739 -> 6379. Thanks to brosner! Closed by af27b67a7003efaa9519b2e21b3559998ae62ab5\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/133522/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/137286", "body": "Added --schedule|-s option to celeryd, so it is possible to specify a schedule filename when using the -B option, also debian init scripts is now using the --chdir option of start-stop-daemon. Thanks to David De La Harpe Golden.  Closed by e65184933a115badde1d7030c2e09ff829b0bbc0\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/137286/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/137289", "body": "Added DAEMON_OPTS as an env variable for extra daemon options, also CELERYD/CELERYBEAT_USER + _GROUP as variables for the --chuid and --group options. Thanks to David De La Harpe Golden. Closed by 8c3df7cd65530879e20b5f85271f2ddefb8756bb\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/137289/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/138253", "body": "Ah, that's right. I knew the throw argument was confusing, just forgot to open up an issue about it, so thanks for that.\n\nBut the throw argument is actually about raising the RetryTaskError exception, which celeryd catches to mark the task as RETRY instead of SUCCESS. So if you don't throw it, the task would be marked as successful. So, this is used internally only. Not logging retries would be another feature request, we could add an option for that.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/138253/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/138282", "body": "Give better description of confusing throw argument to Task.retry. Closed by 85cbd63b1afd86c6e31e2b13215e2713d82258a1\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/138282/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/138528", "body": "On second thought, only the AMQP backend should cache results.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/138528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/141498", "body": "The AMQP backend now uses the LocalCache dictionary as cache, to limit the number of results kept in memory. Closed by 1600df8024c25fb465a74529c63094aca13463c2\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/141498/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/141494", "body": "Why shouldn't it be executable? Usually this is the way initscripts work. If the script is not executable, it means the service is disabled.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/141494/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/142471", "body": "initscripts: Die if the provided celeryd/celerybeat is not executable. Closed by 1cba6d685c207aab62a6d1b1b8b73bb56764e3db\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/142471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/142472", "body": "You're right! I'll print an error if $CELERYD is a file and not executable\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/142472/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/141483", "body": "Mention CELERY_DEFAULT_RATE_LIMIT in configuration docs. Thanks to kmike!  Closed by eaf019587a571cd76779cc08345d6d6b500b2e17\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/141483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/145628", "body": "This is a little bit \"inside-out\" than what I had in mind when you asked on the mailing list :)\n\nThe individual worker pool processes doesn't bind to any queues actually. The main process consumes the tasks, and then send them to the subprocesses. So it would be hard to get this working.\n\nHowever, an easier way to launch multiple celeryd's would be great! For example a command line option to specify which of the CELERY_QUEUES it should bind to:\n\n```\n# Worker 1 (5 concurrent processes)\n$ celeryd -Q laptop -c 5\n\n# Worker 2 (5 concurrent processes)\n$ celeryd -Q laptop,server -c 5\n\n# Worker 3 (10 concurrent processes)\n$ celeryd -Q server -c 10\n```\n\nSo the actual configuration would be up to the init script or whatever, or maybe we could create a celeryd_multi utility (like mysql and rabbitmq has)\n\n@jkozera How do you know if the task is IO or CPU bound? Add another Task attribute?\n\nI think you should use multiple queues for this, one for \"real-time\" and one for low-priority tasks, when real-time tasks can't be processed fast enough, you add another worker to consume from the \"real-time\" queue.\n\nYou could probably implement this, but I don't think it would be that easy, the biggest problem is how to track how many tasks are actually currently being executed in the pool (a limitation in multiprocessing, that my accept-callback patch might help solve)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/145628/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/146260", "body": "@winhamwr I think we should be able to make it simple enough.\n\nIf we're going to change the behavior, so the mainprocess has a mapping of queues, and what processes to send them to, we would have to rewrite multiprocessing.Pool, and we would need one multiprocessing.queues.SimpleQueue for each process, instead of one Queue for all of the processes, which means there's some overhead.\n\nThe other approach is to have each pool worker be their own consumer, and this is not as easy as it sounds, because amqplib doesn't work when it's not the main process, and with the GIL we might not get very good performance anyway. And then we still need to manage multiple main processes.\n\nManaging multiple celeryd's on one machine, is not much harder than multiple celeryd's on multiple machines. And a wrapper script could make it seamless.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/146260/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/254301", "body": "celeryd -Q option: Ability to specifiy list of queues to use, disabling other configured queues. Closed by 8df76d0a5dc30411f0dfaaedaa071cfbbbaa8419\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/254301/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/256660", "body": "Added support for router classes (like the django multidb routers)\n\nNew setting: CELERY_ROUTES\n\nThis is a single, or a list of routers to traverse when\nsending tasks. Dicts in this list converts to a \"celery.routes.MapRoute\"\ninstance.\n\nExample:\n\n```\n>>> CELERY_ROUTES = {\"celery.ping\": \"default\",\n                      \"mytasks.add\": \"cpu-bound\",\n                      \"video.encode\": {\n                        \"queue\": \"video\",\n                        \"exchange\": \"media\"\n                        \"routing_key\": \"media.video.encode\"}}\n\n>>> CELERY_ROUTES = (\"myapp.tasks.Router\",\n                     {\"celery.ping\": \"default})\n```\n\nWhere myapp.tasks.Router could be:\n\n```\nclass Router(object):\n\n    def route_for_task(self, task, task_id=None, args=None, kwargs=None):\n        if task == \"celery.ping\":\n            return \"default\"\n```\n\nroute_for_task may return a string or a dict. A string then means\nit's a queue name in CELERY_QUEUES, a dict means it's a custom route.\n\nWhen sending tasks, the routers are consulted in order. The first\nrouter that doesn't return None is the route to use. The message options\nis then merged with the found route settings, where the routers settings\nhave priority.\n\nExample if apply_async has these arguments::\n\n```\nTask.apply_async(immediate=False, exchange=\"video\",\n```\n\nrouting_key=\"video.compress\")\n\nand a router returns::\n\n```\n{\"immediate\": True,\n \"exchange\": \"urgent\"}\n```\n\nthe final message options will be::\n\n```\nimmediate=True, exchange=\"urgent\", routing_key=\"video.compress\"\n```\n\n(and any default message options defined in the Task class)\n\nClosed by 6d701af9e622ef779d84e83885d3ff6b0c31d40a\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/256660/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/156244", "body": "hehe. point taken.\n\nIt's kind of hard though, as some backends doesn't even require a hostname. So I added a change to carrot: http://github.com/ask/carrot/commit/7dcbdf37765c347b2ac304d847546c3c99ab263c\n\nSo at least it's made clear what's missing, even if it doesn't point back to the BROKER_HOST setting.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/156244/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/161171", "body": "closing now\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/161171/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/161170", "body": "Come to think of it, this makes sense as locmem, means it keeps it in memory and since celeryd and the client you fetch result from is in different processes, they won't have the same memory. Tripped me up for a second there, but this would be expected behavior. \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/161170/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/168048", "body": "Unittests: Don't noop the django test database teardown, instead fix the underlying issue, which was caused by modifications to the DATABASE_NAME setting. Thanks to JannKleen. Closed by 13b7a1fd64dc38661e6a495b083d6c19cac9e842.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/168048/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/164765", "body": "management.commands.camqadm: Fix typo camqpadm -> camqadm. Thanks to jpwatts.  Closed by bf8defa2d679d68720ea1668c2841d60fb085d7b\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/164765/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/168035", "body": "Debian init scripts: Use -a not \"&&\". Thanks to jcassee. Closed by 98e5cad9741d9958339537bee5cf3dc5ab94779d.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/168035/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/230370", "body": "I think a tips and best practices section in the documentation would be great!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/230370/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/241327", "body": "just found out we already have this in the userguide: \nhttp://ask.github.com/celery/userguide/executing.html#connections-and-connection-timeouts\nbut I also added a reference to it in the FAQ.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/241327/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/230368", "body": "The example task doesn't use keyword arguments? It would be really nice if you have an example. I'm notoriously bad at examples :( \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/230368/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/230369", "body": "userguide/tasks: Use of positional argument after keyword argument in example.\n\nClosed by 5c9aaf10e43591abe70d6d05ba52795dbfc83fb9. Thanks to mitechie and winhamwr.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/230369/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/213443", "body": "The fix has been merged. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/213443/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/213444", "body": "Fixed by kmike\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/213444/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/213445", "body": "Fixed missing '@task' decorator in Tasks Userguide documentation. Thanks to\nosantana. Closed by 57bd9dce7031ad09209fcdbcbcf85c96ac220d79\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/213445/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/213447", "body": "Fixed by kmike\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/213447/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/213448", "body": "Fixed by kmike.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/213448/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/213446", "body": "Fixes merged! Thanks a lot, I think this has been a problem for some.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/213446/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/230326", "body": "I think I have managed to track down this issue.\n\nIt seems as if `PickleField.get_db_prep_value` is not called when saving, because if I disable the picklefield, and do manual pickle/depickle in the database backends `_store_result` and `_get_task_meta_for` everything works as expected.\n\nMore investigations to follow...\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/230326/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/420333", "body": "Is this still an issue for anyone?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/420333/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/422521", "body": "I justed tested `sandbox.tgz` with celery master branch, and I am not able to reproduce this anymore (the result was always correct even after 1000 runs, with one and 10 concurrent processes.)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/422521/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/213402", "body": "Nice work! The patch has been merged.\n\nI think we could do this even better, by always using the same connection, and using\nthe same \"re-establish connection if it fails\" functionality celeryd is using (celery.utils.retry_over_time)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/213402/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/230366", "body": "apply_async: Raise ValueError if task args is not a list, or kwargs is not a tuple. Closed by ad8443eb2c44c9cc5661580dabb31550c39df60d. Thanks to TheAtilla.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/230366/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/218178", "body": "Hey!\n\nmultiprocessing is not an option in Jython, so we need to use threads instead.\nI recently added the `CELERYD_POOL` setting which can make celeryd use a custom pool class.\n\nSo we could implement a thread pool version of `celery.worker.pool.TaskPool`, and simply set\n\n```\nCELERYD_POOL = \"celery.worker.pool.ThreadPool\"\n```\n\nThere's a PEP being written that abstracts this, but it looks kind of funky to me:\nhttp://www.python.org/dev/peps/pep-3148/\n\nBut there's a reference implementation that already has a thread pool that behaves like multiprocessing.Pool, could be of interest.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/218178/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/348310", "body": "@pordonez Cool! Do you know how we could recognize that we're running on jython in `setup.py`, so we can disable the multiprocessing requirement?\n\nI've experimented with thread pool support, which can be found in `celery.concurrency.threads`.  The problem is that amqplib may not run threaded, so maybe we have to restrict this for use with Pika.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/348310/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/525908", "body": "@FlaPer87\n\nNice!  Thanks for the patch.\nI started a thread pool implementation celery.concurrency.threads, but it's not functional mainly due to amqplib being blocking and that you can't reuse the connection across threads.  The task needs to be acked as soon as it's started which requires the connection, there are several ways to accomplish this though.\n\nI think the main goal is getting it to work with Pika, I am not sure about the thread safety of that library though.\n\nMy main idea was to use the MainThread exclusively for receiving and acking messages, the thread pool then sends ack requests back to the main thread using a `Queue`.  If Pika supports threads, then maybe we can ack directly.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/525908/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/700387", "body": "I've got it working with Jython!  When running under jython the thread pool is default.\n\nShutdown by ctrl+c is not working properly, but hopefully that is not too hard to fix.\n\nPlease test!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/700387/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/764448", "body": "Closed, please open individual issues for any bugs you may find.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/764448/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/230356", "body": "Typo in CELERYD_SEND_TASK_ERROR_EMAILS configuration documentation. Closed by 7a92d16f4e1c13c9aedb8ae643e5fca39f8229c2. Thanks to Kami.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/230356/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/227962", "body": "Hey!\n\nI'm not sure what you're trying to do. task.control.rate_limit sends a broadcast command to the workers to change the rate limit of a task. But this is only executed in the MainProcess, pool workers will not get updated, as the `registry` is local.\n\nBut rate limits are enforced in the main process anyway, so this shouldn't be a problem (except for the values in the pool workers being out of sync)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/227962/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/230355", "body": "Right. Thanks for noticing. I pushed a fix, could you please verify it's working for you? 3049f338ebc43dc36f9d6a43f8a25b03cc35c1e0\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/230355/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/231539", "body": "Great news! I'll close this issue then. Thanks a lot for your help.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/231539/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/227931", "body": "celery has not been optimized for memory usage. There is probably something that could be done in this area. Creating processes on request is an option, but note that creating processes is slow.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/227931/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/293590", "body": "The worker pool processes only take about 3MB for me, are you sure you're not also counting shared memory?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/293590/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/348368", "body": "@jonozzz\n\nI'm experimenting with alternative methods of concurrency.\nUsing stackless/evenlet/etc is a good alternative for IO-bound jobs, but it also\nrequires everything to be written async, or else it will block.\n\nSo multiprocessing is still the most generally useful solution, but we will have support\nfor other models soon.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/348368/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/581885", "body": "Autoscale support has been added, and eventlet support.  Will be released for 2.2.0, I'd say this pretty much closes this issue for now.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/581885/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/230381", "body": "The easiest way to get your patch applied is to fork the project here on github: http://help.github.com/forking/\n\nIf you can't get this to work, tell me and I'll apply the patch manually.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/230381/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/230420", "body": "On second thought. I think this may be solved in a cleaner way. We need to find out what `execute_manager` does to setup the environment properly.\n\nDoes replacing this with:\n\n```\nfrom django.core.management import setup_environ\nsettings_mod = os.environ.get(\"DJANGO_SETTINGS_MODULE\", \"settings\")\nproject_settings = __import__(settings_mod, {}, {}, [''])\nsetup_environ(project_settings)\n```\n\nwork?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/230420/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/230475", "body": "What if you add this to the mix (after `setup_environ`):\n\n```\n from StringIO import StringIO\n from django.core.management import validation\n errorlog = StringIO()\n validation.get_validation_errors(errorlog)\n if errorlog:\n     errorlog.seek(0)\n     raise Exception(\"One or more models did not validate:\\n%s\" % (\n         errorlog.read()))\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/230475/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/231540", "body": "Any news on this? Would be great to have this fixed.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/231540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/422499", "body": "This seems to work now.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/422499/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/246328", "body": "@haridsv Awesome!\n\nHaven't looked at it yet, but this is a step forward.\nI've created a new djangofree branch in celery:\nhttp://github.com/ask/celery/tree/djangofree \n\nand a new project, django-celery, for the django integration:\nhttp://github.com/ask/django-celery\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/246328/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/247861", "body": "Fixed in djangofree branch. This branch will become celery 1.1.0. Django integration has been moved to http://ask.github.com/ask/django-celery\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/247861/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/252015", "body": "Yeah, that needs to be done for ghettoq. Ghettoq will be merged with carrot for 1.2.0 I think, so will probably add these as individual packages. carrot-sqlalchemy, carrot-django, carrot-redis, carrot-mongodb and so on.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/252015/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/234455", "body": "Processes spawned when receiving the `KILL` signal is weird indeed. I don't see that behavior when used outside of `supervisord`, so maybe this is something caused by it?\n\nIf you install the `setproctitle` module, celery should report the kind of process in `ps` listings, could you do that to investigate what kind of process is created?\n\n(`easy_install setproctitle`)\n\nSetting the timeout to `600` is probably good. Is there any setting for infinity (maybe with a warning if it takes too long)? When `celeryd` is killed via `TERM` (which is the preferred shutdown signal) it stops receiving messages and waits for the currently executing tasks to finish. And I guess for most applications, termination mid-execution is not acceptable.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/234455/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/241295", "body": "I can't reproduce this locally. Btw, are you running the master branch? I just fixed a bug that could hang shutdown. If you could test with this it would be nice.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/241295/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/242332", "body": "@kmike I still can't reproduce with the commands above. Maybe because I'm on OS X, or maybe you're running Python 2.5? (I'm running 2.6.1)\n\nCould run it with `--loglevel=DEBUG?` It could provide some info about where it stops.\n\nThe celerybeat process is started by the main process, so I'm assuming the main process is waiting\nfor the celerybeat to exit before it kills the remaining pool processes.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/242332/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/242340", "body": "hmm, you're right of course. It's almost like the beat process takes ownership of the pool processes.\n\nI just tried to reproduce on Debian Lenny with python 2.5, and it works just right there.\nTried killing with both TERM and INT.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/242340/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/242924", "body": "Ah! I'm so sorry. I didn't read the issue carefully enough. Yes! That's exactly what happens when you kill it with SIGKILL. The 9 signal can't be catched, so there's nothing we can do about this AFAIK.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/242924/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/253063", "body": "Revoked tasks now marked with state REVOKED, and result.get() will now raise TaskRevokedError.\n\nClosed by aa389721b141da27b33c211c7a35f58d3677c145\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/253063/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/253060", "body": "Updated docs/configuration with new settings. Closed by db85833c3fb2834ae06b3a8e8699530cc0670ba4. Closed by db85833c3fb2834ae06b3a8e8699530cc0670ba4.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/253060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/241258", "body": "Events: Added setting CELERY_EVENT_SERIALIZER with default \"pickle\".\n\nUsed JSON previously.\n\nClosed by 41bffe660d8433c1c3f1cd09b411b00dc859e140. Thanks to dfdeshom!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/241258/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/241262", "body": "returning the string representation of the dict works? remember it needs to be reassembled on the other side.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/241262/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/241622", "body": "Come to think of it, using pickle may be a bad idea, as the objects needs to be importable on the other side as well, so a string representation of the args/kwargs/result is probably better.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/241622/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/242306", "body": "Ok. I now send the repr() of args, kwargs, results and exceptions instead. It's intended for information after all, not deep introspection.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/242306/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/242310", "body": "Events: repr complex objects sent as event fields.\n\nIt's a bad idea to sent pickled objects with the events, as the receiving\npart may even not have the classes installed.\n\nAnd anyway, the events are meant for information not deep introspection.\n\nClosed by f91ade43674751b6f27fa020d5fc576a9f47c9ed. Thanks to dfdeshom. Please verify.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/242310/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/253062", "body": "Updated docs/configuration with new settings. Closed by db85833c3fb2834ae06b3a8e8699530cc0670ba4. Closed by db85833c3fb2834ae06b3a8e8699530cc0670ba4.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/253062/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/253059", "body": "apply(throw=True) / CELERY_EAGER_PROPAGATES_EXCEPTIONS: Makes eager execution re-raise task errors. Thanks to proppy. Closed by 760e94ec1b276e7f296fd27754bb2144eeafb953.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/253059/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/247023", "body": "Some of these signals may be harder to implement, but what you describe here should\ndefinitely be easy to do.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/247023/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/252764", "body": "I've added celery.signals.worker_process_init, the rest of the proposed signals will have to wait, open up new issues for them when someone needs them.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/252764/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/252765", "body": "New signal: celery.signals.worker_process_init: Sent inside the pool worker process at init. Thanks to jonozzz. Closed by d073ff0e9c613713009220061599a835dd52eba8\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/252765/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/248421", "body": "Could you please dump the contents of `**kwargs` in `Task2`?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/248421/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/251934", "body": "celery.execute.apply: Pass logfile/loglevel arguments as task kwargs. Closed by 5613a57184f2e6d883bf0a7daf3eac8219dc2b8b Thanks to jonozzz.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/251934/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/248345", "body": "What result backend is this?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/248345/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/251827", "body": "Ah, ok. That was the context I was missing.\nThanks for finding this! `apply()` shouldn't return the `ExceptionInfo`, it should've returned the exception itself.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/251827/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/29867", "body": "Thanks for pointing that out. It's a bug, indeed.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/29867/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/38526", "body": "I added the importlib backport module as a dependency, is that >= 2.6 only? I guess I could add django==1.1 as a dependency, but I think there's someone using 1.0.x still.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38526/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/38984", "body": "2.7 you mean? Yeah, but I added http://pypi.python.org/pypi/importlib as a dependency. Does it work with Python >= 2.4?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38984/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/38985", "body": "Ah, I just saw the trove classifiers, all the way down to 2.3.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38985/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/51698", "body": "This commit was actually authored by me, just forgot to reset my `GIT_AUTHOR_NAME`, and `GIT_AUTHOR_EMAIL` env variables.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/51698/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/52587", "body": "damn. Seems I forgot to reset my GIT_AUTHOR_\\* settings :(\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/52587/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/82477", "body": "Ah, ok. That's good to know! I wasn't sure about this. Also, I'm wondering what holds the connection, is it the engine or the session?\nCreating a new connection for every operation is probably a bad idea, but not sure if it does some connection pooling by default.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/82477/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/91900", "body": "lol, yeah. I guess this is borderline.\n\nYou have the ability to supply your own connection, if you do we can't close the connection for you.\n\nAbove it says:\n\n```\nconn = connection or establish_connection(connect_timeout=connect_timeout)\n```\n\nso `connection or conn.close`, is \"if user didn't supply a connection, close the connection we established\".\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/91900/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/91937", "body": "Forgot that it's using the `@with_connection decorator` which takes care of this automatically :/\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/91937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/352668", "body": "Thanks!  Fixed in 154431f2c4ff04515000462ede70e205672e1751\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/352668/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/605179", "body": "On 17 Sep 2011, at 17:49, Mher Movsisyan wrote:\n\n> https://bitbucket.org/jezdez/sphinx-pypi-upload/issue/1/minor-patch-for-namespace-modules-etc patch should be applied before launching  paver upload_pypi_docs. upload_pypi_docs fails if .build contains empty subdirectories.\n> \n> Is there a way to automate this?\n> ## \n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/ask/celery/commit/66137154ed5bf7a07a8a4f67d1419be2444c7eac#commitcomment-595269\n\nAh, that's true!\n\nI've bumped into this problem as well, and I have just changed\nthe sphinx-pypi-upload code manually actually :(\n\nI don't know how, but it should definitely be fixed somehow.\nTo me it seems like the problem is in sphinx-pypi-upload\n## \n\nAsk Solem\ntwitter.com/asksol | +44 (0)7713357179\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/605179/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/1006973", "body": "@gtaylor,\n\n1 commit makes you famous,\n10 commits and telling 10 of your friends gives you eternal life\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/1006973/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/1433801", "body": "So this means that this isn't a real CLA (it probably wasn't to begin with anyway), and that I may bother you to sign a real document at some point (which of course is up to you)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/1433801/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/1793769", "body": "done!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/1793769/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/1793772", "body": "also done!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/1793772/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/1793856", "body": "It will first be removed in 3.1, it's only deprecated in 3.0\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/1793856/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/1793869", "body": "Should maybe add instructions to the contributing guide!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/1793869/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/2165281", "body": "It fixes the issue of sending events quickly in a tight loop.  The other problem would require a new issue\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/2165281/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/2613983", "body": "Are you sure you're running 3.0.15?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/2613983/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/2613999", "body": "Ah, you mean in the `master` branch.  Seems I broke the merge, sorry.  Fixed now!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/2613999/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/3328772", "body": "I don't know.  Docs must be updated and more unit tests must be written.  It must also be tested, and the more people help out with this the sooner it can be released\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/3328772/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/4934419", "body": "On Dec 24, 2013, at 8:06 AM, aspyatkin notifications@github.com wrote:\n\n> stat -Lc%u\n\nUrg, thanks!\n\nIs it the -r option that is not supported?  As BSD/OSX does not support the -c argument (they have -f though).\n\nWhat error code does using -Lr give? (e.g. `stat -Lr /etc || echo $1`)\n## \n\nAsk Solem\ntwitter.com/asksol | +44 07454281208\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/4934419/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/5117343", "body": "It should work on Linux, but perhaps not all distros.  I had a poll beforehand to see what arguments worked.\n\nSent from my iPhone\n\n> On Jan 18, 2014, at 11:04 AM, Taha Jahangir notifications@github.com wrote:\n> \n> This means that current script only works on BSD/OSX (not linux)?!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/5117343/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/5118803", "body": "Just pass a constructor that returns a redis connection pool object, e.g connection_pool=redis.ConnectionPool\n\nSent from my iPhone\n\n> On Jan 20, 2014, at 2:07 PM, Pepijn de Vos notifications@github.com wrote:\n> \n> Nice! How do I use this?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/5118803/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/5129132", "body": "Celery(backend=RedisBackend())\n\nOr app.backend = RedisBackend()\nThe latter must happen before the tasks are finalized and the tasks inherit the backend.\n\nSent from my iPhone\n\n> On Jan 20, 2014, at 3:59 PM, Pepijn de Vos notifications@github.com wrote:\n> \n> I get that, but what do you do with the resulting backend instance?\n> \n> I ended up with\n> \n> class MyCelery(Celery):\n>     def _get_backend(self):\n>         return celery.backends.redis.RedisBackend(\n>                 connection_pool=redis.BlockingConnectionPool,\n>                 max_connections=50,\n>                 url=backend,\n>                 app=self)\n> \u2014\n> Reply to this email directly or view it on GitHub.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/5129132/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/5139331", "body": "That's a good point! You could do something like `backend=\"proj.backends:create_backend\"` where:\n\n```\ndef create_backend(app, **kwargs):\n     return RedisBackend(...)\n```\n\nBut definitely awkward compared to 1\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/5139331/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/6601973", "body": "You cannot edit this file directly, you have to edit `docs/templates/readme.txt` and then do `$ make readme`\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/6601973/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/7205435", "body": "No, seems left over after moving from method to property. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/7205435/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/8225932", "body": "Well, since you cannot support it without central coordination, add_to_chord is a special case currently only supported by the redis result backend when new_join is enabled.\n\nA group is disconnected, so I cannot really see a reason for having a add_to_group.\nThe group can be joined by others (.e.g. the caller doing `group(..)().get()`, but they wouldn't be able\nto see the additional tasks, since you cannot modify the callers information about the group.\n.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/8225932/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/12143142", "body": "Thanks! Fixed\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/12143142/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/12165226", "body": "Shouldn't worry too much about keeping .apply() compatibility.  This is used for ALWAYS_EAGER and is pretty much useless for anything but as a safeguard from executing tasks\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/12165226/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/13970342", "body": "Yeah, you're right, but this was for the Python version check above.\n\nIf you try to install celery with Python 2.6 you would get a SyntaxError instead of the \"Celery 4.0 requires Python 2.7 or higher\" error message.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/13970342/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/14970164", "body": "Thanks!  Fixed by 50ce76bfa73ca369c276739c2bece0616e53dfba\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/14970164/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/15201538", "body": "Thanks, should have some way to run flakeplus pre-push :/\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/15201538/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/16691949", "body": "Which line? The one that is filtering out the removed items? Thea idea of filtering early means we never have to deal with removed items in the heap (like in `pop`).  If we are iterating over _data we can just as well do some work ;)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/16691949/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/17886249", "body": "The cache will probably make it faster, but caching in pip often cause trouble.   Sadly I don't remember exactly why, maybe it refused to upgrade some version or similar?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/17886249/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/17906788", "body": "This was the main reason for the memory leak, as Python's gc cannot collect objects with cyclic references if `__del__` is implemented.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/17906788/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/18034569", "body": "We have no resources to maintain it.  I downgraded all transports other than RabbitMQ and Redis to experimental status in 3.1 as a hope to get the situation changed.  It improved somewhat for MongoDB but there's still so many issues open, which leads to general dismay towards the project, and burnout for me :(\n\nI think the MongoDB transport will still exist for a while, but be undocumented so that new users don't use i.t\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18034569/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/18035609", "body": "@thedrow is right, it's not just about a list of specific issues. If all the current issues were fixed I guess that would lessen the burden considerably, but we also need to support users and probably the biggest challenge is the need for rewriting it into fully using async I/O.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18035609/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/18035669", "body": "I'm guessing fixing the transport would be a few weeks of dedicated work, but still insurmountable with all the other things that needs done.  There's also many things that could be improved, like implementing proper acknowledgments.  It's was always lagging behind, but now it's at a standstill after I stopped spending personal time on the project.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18035669/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/18105818", "body": "We have maintained multiple things in separate packages over the years:  django-kombu, django-sqlalchemy, ghettoq.  The problem with this is that it's still associated with the main project, so any criticism will be on us, and people usually creates issues on the core issue tracker anyway.  If there's a maintainer then that would solve most of our problems :)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18105818/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/18546807", "body": "I don't remember this line, it's not there anymore in 4.0 (master) so not sure, probably something to do with upgrading 'group() | sig' to chord.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18546807/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/18751243", "body": "No, disabling rate limits is not required.  It used to give better performance, but that is no longer the case after 3.1\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18751243/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/19068150", "body": "hmm, that's a very good point.  The new django-celery-beat and django-celery-results packages are actually 1.8+ already!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/19068150/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/112280", "body": "Is this deliberate?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/112280/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/205588", "body": "Why did you remove the `Content-Type`?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/205588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251074", "body": "Think there's a race here if it enters `time.sleep()`, and the putlock is released when the pools state != RUN.  The task will be sent to the queue in this case.  I'll fix it before I merge.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251074/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251078", "body": "Also changed the interval to 1.0 as we discussed on IRC.  The shutdown process is already almost always delayed by at least one second because of other thread sleeps, so it doesn't make a difference (apart from less CPU usage in online mode).\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251078/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374181", "body": "Actually, the Pool implementation shouldn't depend on Celery, it's more of a patch for fixes and features we need for multiprocessing.\nSo this option should be added to `Pool.__init__`, then passed on from `celery.worker.WorkController`, then `celery.concurrency.processes.TaskPool`\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374181/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374183", "body": "and, oh yeah, `celery.conf` is deprecated to be removed in 3.0, so you don't have to add anything to it.\n\nWhen added to `WorkController`, it works in the case where you instantiate the worker without configuration too,\ne.g: `WorkController(worker_lost_wait=20)`.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008375", "body": "Ah, this was just a typo, I copied the document...\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008380", "body": "It's on my TODO, I have to write it\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576043", "body": "You can't terminate a reserved task, as it hasn't been executed yet.\n\nIt adds the id of the task to the list of revoked tasks, this is then checked again when the worker is about to execute the task (moves from reserved -> active).  If you only search the reserved requests then the terminate functionality would be useless (it wouldn't have any process to kill)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576052", "body": "the other changes look great\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576052/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597034", "body": "Revoked check for reserved/ETA tasks happens here:\nhttps://github.com/celery/celery/blob/3.0/celery/worker/job.py#L185-186\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597034/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597089", "body": "> Also, as the set of reserved tasks includes active tasks as well, terminate works as expected (e.g. terminates active tasks)\n\nStill, you shouldn't terminate a reserved task.  If a reserved task is revoked it should be dropped _before_ any pool worker starts to execute it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597089/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/3297191", "body": "Hmm, maybe we could remove the `set` here, so that it preserves the original order.\n\nIt's not terribly bad if it imports the same module twice after all\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/3297191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4962552", "body": "For Python 2.6 you have to include positions: {0} {1} {2}\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4962552/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7127484", "body": "Celery related code should not be called by this method as it should be decoupled from Celery.  You need to find a way to support this by exposing it in the API.\nE.g. `Worker(on_shutdown=signals.worker_process_shutdown.send)`\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7127484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/8476390", "body": "Since created is True, it would also call 'on_node_join' in this instance, even if it just left.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/8476390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304331", "body": "Should this be logged?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304331/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9305633", "body": "Right, I suspected that. It cannot be considered critical at least if it's automatically handled.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9305633/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9949455", "body": "This should be `getattr(obj, '__qualname__', None) or obj.__name__` as that ensures that `obj.__name__` is not called unless qualname is missing.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9949455/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9949543", "body": "I'm pretty sure both of these are deliberate as the difference\nwith your version is that it'll always create an empty list even if the getattr succeeds.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9949543/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/12848123", "body": "We don't use `func` around here, we always use `fun` :)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/12848123/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/13031596", "body": "You don't need the `__getitem__` here, because dict will already call `__missing__` if the key is not found!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/13031596/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/13075747", "body": "We cannot just include new fields here, they have to be added to all backends simultaneously, so it must be a coordinated effort.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/13075747/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/32344300", "body": "The celery source code actually never uses more than one level of relative imports (so no more than one dot).\nIt used to, but it ended up being a mess\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/32344300/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/32344507", "body": "Converting a module to a package (or the other way around) can cause problems if the user installs the new version on top of the old one (since the old module will not be deleted, and .pyc files may also exist).\n\nSo something should be added to https://github.com/celery/celery/blob/master/setup.py#L17-L46, but upgrading instead of downgrade_packages.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/32344507/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/41548467", "body": "please use `self.reject(requeue=True)` to make it more obvious what's happening\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/41548467/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/42553002", "body": "Seems I made a mistake in the virtual kombu transports here, as they set this in `delivery_info`.\nSince this is for the 4.0 release we can just as well fix it properly, and read it from properties directly.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/42553002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/38785129", "body": "Terminology here is a bit confusing, we usually reserve worker for 'celery worker instance', and `child process` for pool worker processes.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/38785129/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/38785229", "body": "Maybe be consistent with `max_tasks_per_child` here, and have `max_mem_per_child`.  I inherited that naming from multiprocessing, but now that we use it's useful to be consistent.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/38785229/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/38785264", "body": "Same here, `worker` -> `child process`\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/38785264/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/44823058", "body": "`+e` is the default, but you are setting it to -e later?\n\nIf the set -e is propagated from something else calling the script then it would be better to\nwrap it in a function:\n\n```\norigin_is_runlevel_dir () {\n    set +e\n    dirname $0 | grep -q \"/etc/rc.\\.d\"\n    echo $?\n}\n\nif [ $(origin_is_runlevel_dir) -eq 0 ]; then\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/44823058/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/45934230", "body": "Did you revert this? It should be blank now since Django 1.8 apps may be virtual (AppConfigs)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/45934230/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/45934266", "body": "Namespace here should be CELERY_ to add a prefix to all Celery settings (new in 4.0)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/45934266/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/45934275", "body": "Prefix!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/45934275/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/48575604", "body": "This version always works even if the manage.py file has no `+x` permission\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/48575604/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/54190188", "body": "Should be `disabled://` ?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/54190188/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/54190358", "body": "Yeah, I think you can technically using SASL but the Celery cache backend doesn't let you do so via the URL\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/54190358/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/54317367", "body": "Please use `{0}` here, as position-less format is not supported by all 2.7 versions\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/54317367/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55773310", "body": "The heap is ordered, no? Surely sorting the data must be much slower than iterating over the heap?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55773310/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55773573", "body": "This is actually backwards incompatible with data serialized with older versions.\n\nAll new keyword arguments must be added to the end, so `minlen` must be positioned after `data`\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55773573/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55774908", "body": "I cannot see any case here where item should be `REMOVED`?\n\nUnless you count _refresh_heap, and I think we can just as well filter REMOVED values there\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55774908/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55907792", "body": "Oh, yeah, I definitely think lazily purging the heap is a huge improvement.\nThe treap module looks interesting, hope I get the chance to use it some time.\n\nAFAICS the only place where items in _heap can end up being `REMOVED` is in `refresh_heap`:\n\n```\nself._heap[:] = [entry for entry in self._data.values()]\n```\n\nBut then this is also doing the wrong check?:\n\n```\n _, item = heappop(self._heap)\n +            if item is not self.REMOVED:\n```\n\nWon't `heappop(self._heap)`[1] be `REMOVED[1]`\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55907792/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55913978", "body": "Oh yeah, this actually bit me before, but I guess not painful enough for me to remember ;)\n\nNothing is actually using `__iter__` as far as I can tell, so I guess it doesn't matter much anyway.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/55913978/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/58115407", "body": "I think this part should be in the test, not in the original class\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/58115407/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/43160950", "body": "Should we then also remove PickleType, if it's no longer used?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/43160950/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/64279832", "body": "there _should_ be space between methods in a class, that includes the first method :)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/64279832/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/69800425", "body": "Why is this necessary? I feel if the couchdb backend needs to inject anything into the generated structure it should do so before the structure is serialized, e.g. by overriding `encode_result()` or `encode()`.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/69800425/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/69800832", "body": "You should actually be able to use `if c.connection.supports_exchange_type('fanout'):` now!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/69800832/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "alexvicegrab": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3505", "title": "Setting a particular $PATH environment for the celery daemon", "body": "Hi,\n\nI'm running Celery on a Django app on an Ubuntu AWS instance (with Nginx & Gunicorn)\n\nI need my celery instance to have a specific $PATH environmental variable, beyond the typical:\n\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\"\n\nHaving gone through the basic tutorials on the website and extensively searched on Stack Overflow I've not found a solution:\n\n1) changing /etc/default/celeryd to include an \"export PATH\" does not solve the problem.\n2) Indeed, echoing the $PATH in the /etc/init.d/celeryd script shows the $PATH I want, but it is apparently reset within the celery worker.\n\nHow can I solve this?\n\nThank you!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3505/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "RyanKung": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3479", "title": "Any plan to support redis cluster", "body": "Any plan to support redis-cluster?\n\nI think redis-cluster can be simply implement by inheriting `celery.backend.redis.RedisBackend`, Is there any plan to do that?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3479/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gdubicki": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3459", "title": "celery multi is working strange with systemd", "body": "Hi,\n\nAs I had problems with running celery using systemd on Centos 7 I did some testing to find out what exactly is happening. I used the \"tasks.py\" app from the tutorial (http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html#application).\n\nHere are my findings.\n\n**1. You HAVE TO use PIDFile systemd service option for running a single worker**\n\nExample of a working service file with a single worker, but with 4 concurrent processes:\n\n```\n[Unit]\nDescription=Celery Service\nAfter=network.target\n\n[Service]\nType=forking\nUser=vagrant\nGroup=vagrant\n\n# directory with tasks.py\nWorkingDirectory=/home/vagrant/celery_example\n\n# this is REQUIRED!\n# you will still get a warning \"PID file /var/run/celery/single.pid not readable (yet?) after start.\" from systemd but service will in fact be starting, stopping and restarting properly\nPIDFile=/var/run/celery/single.pid\n\nExecStart=/home/vagrant/celery_example/venv/bin/celery multi start single-worker -A tasks --pidfile=/var/run/celery/single.pid --logfile=/var/log/celery/single.log \"-c 4 -Q celery -l INFO\"\n\nExecStop=/home/vagrant/celery_example/venv/bin/celery multi stopwait single-worker --pidfile=/var/run/celery/single.pid --logfile=/var/log/celery/single.log\n\nExecReload=/home/vagrant/celery_example/venv/bin/celery multi restart single-worker --pidfile=/var/run/celery/single.pid --logfile=/var/log/celery/single.log\n\n[Install]\nWantedBy=multi-user.target\n```\n\n**2. You CAN'T use `celery multi` --pidfile or --logfile options when running multiple workers**\n\nExample of a working service file with 3 workers, 4 concurrent processes each:\n\n```\n[Unit]\nDescription=Celery Service\nAfter=network.target\n\n[Service]\nType=forking\nUser=vagrant\nGroup=vagrant\n\n# directory with tasks.py\nWorkingDirectory=/home/vagrant/celery_example\n\n# don't set --pidfile or --logfile or it won't work!\nExecStart=/home/vagrant/celery_example/venv/bin/celery multi start 3 -A tasks \"-c 4 -Q celery -l INFO\"\n\nExecStop=/home/vagrant/celery_example/venv/bin/celery multi stopwait 3\n\nExecReload=/home/vagrant/celery_example/venv/bin/celery multi restart 3\n\n[Install]\nWantedBy=multi-user.target\n```\n\nWhen using pidfile or logfile only one worker is actually starting.\n\nCan you explain some of this?\n\nAlso I think that you should fix the example celery.service as it is not working - my celery was starting and stopping immediately when using it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3459/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dtheodor": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3456", "title": "Task cleanup on worker failures", "body": "Hi. My requirement is simple: I want to be able to run task-specific cleanup code no matter what the failure was. That includes\n- exceptions happening within the task execution\n  - `Task.on_failure` callback \n  - `signals.task_failure`\n- termination\n  - `signals.task_revoked`\n- worker failures: either killed externally, or cold shutdown\n  - not possible.\n\nI think this is because the signals and callbacks are executed in the worker itself, and when the worker fails there's no worker to execute anything. However, there is a process that picks up these worker failures, logs them, and sets the task status to FAILED, so it would be possible to have that process run cleanup code.\n\nCan we do this?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3456/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "colinmcintosh": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3434", "title": "Celery not using all available worker processes due to unreleased LaxBoundedSemaphore", "body": "I'm seeing an issue on Celery 3.1.23 where I have a single worker node with 20 allocated worker processes. After some amount of time (~8 hrs for me) the worker will eventually start running less and less tasks concurrently, using fewer and fewer worker processes. This doesn't affect task performance but will lead to such as Tasks being revoked as the queue up and hit time limits. Eventually the celery node will become inactive and stop consuming tasks.\n\nI looked deeper and found that this appears to be an issue with the LaxBoundedSemaphore that is used by celery.worker.WorkController._process_task_sem to call req.execute_using_pool. Watching this semaphore object it looks like over time release isn't called enough times. The leads to the semaphore value bottoming-out at 0 and not allowing anymore tasks to execute\n\n```\n# with a logging statement added to celery.worker.WorkController._process_task_sem\n12704/MainThread      2016-09-08 04:13:01,008               celery.worker:_process_task_sem        INFO: Worker semaphore: <LaxBoundedSemaphore at 0x7f95c1406690 value:3 waiting:0>\n12704/MainThread      2016-09-08 04:13:02,015               celery.worker:_process_task_sem        INFO: Worker semaphore: <LaxBoundedSemaphore at 0x7f95c1406690 value:2 waiting:0>\n12704/MainThread      2016-09-08 04:13:02,042               celery.worker:_process_task_sem        INFO: Worker semaphore: <LaxBoundedSemaphore at 0x7f95c1406690 value:1 waiting:0>\n12704/MainThread      2016-09-08 04:13:03,008               celery.worker:_process_task_sem        INFO: Worker semaphore: <LaxBoundedSemaphore at 0x7f95c1406690 value:1 waiting:0>\n12704/MainThread      2016-09-08 04:13:03,022               celery.worker:_process_task_sem        INFO: Worker semaphore: <LaxBoundedSemaphore at 0x7f95c1406690 value:0 waiting:0>\n12704/MainThread      2016-09-08 04:13:04,005               celery.worker:_process_task_sem        INFO: Worker semaphore: <LaxBoundedSemaphore at 0x7f95c1406690 value:0 waiting:1>\n12704/MainThread      2016-09-08 04:13:04,012               celery.worker:_process_task_sem        INFO: Worker semaphore: <LaxBoundedSemaphore at 0x7f95c1406690 value:0 waiting:2>\n12704/MainThread      2016-09-08 04:13:04,021               celery.worker:_process_task_sem        INFO: Worker semaphore: <LaxBoundedSemaphore at 0x7f95c1406690 value:0 waiting:3>\n12704/MainThread      2016-09-08 04:13:07,036               celery.worker:_process_task_sem        INFO: Worker semaphore: <LaxBoundedSemaphore at 0x7f95c1406690 value:0 waiting:4>\n12704/MainThread      2016-09-08 04:13:07,040               celery.worker:_process_task_sem        INFO: Worker semaphore: <LaxBoundedSemaphore at 0x7f95c1406690 value:0 waiting:5>\n12704/MainThread      2016-09-08 04:13:07,051               celery.worker:_process_task_sem        INFO: Worker semaphore: <LaxBoundedSemaphore at 0x7f95c1406690 value:0 waiting:6>\n12704/MainThread      2016-09-08 04:13:07,058               celery.worker:_process_task_sem        INFO: Worker semaphore: <LaxBoundedSemaphore at 0x7f95c1406690 value:0 waiting:7>\n```\n\nI'm not yet sure what's causing this behavior in 3.1.23. I'm going to give this same thing a try with 4.0.0rc3 and see if the issue reoccurs.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "MarcSalvat": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3430", "title": "Long tasks are executed multiple times", "body": "Hi,\n\nWe are using celery with tasks that may take up a lot of time (hours or even days). The problem is that after a couple of hours task is sent back to the queue and run from another worker (or the same one after it finishes).\n\nLooks like the task is not acknowledged and after that 2h limit it is readded (by rabbit?).  If that is the problem, is there a way to acknowledge them in the begining of the task or disable this behaviour? \n\nWe are using celery 4.0.0 and RabbitMQ 3.6.3 with the following celery config:\n\nCELERY_ACCEPT_CONTENT = ['pickle', 'json']\nCELERY_TASK_SERIALIZER = 'json'\nCELERY_ENABLE_UTC = True\nCELERY_TIMEZONE = 'Etc/UTC'\nCELERY_DISABLE_RATE_LIMITS = True\nCELERY_IGNORE_RESULT = True\nCELERYD_PREFETCH_MULTIPLIER = 1\nCELERY_SEND_EVENTS = False\n\nThanks\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3430/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "elyezer": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3422", "title": "Coverage data is not captured for running celery processes when using prefork POOL_CLS", "body": "I have a functional/integration test suite which I run against a running Celery + Django project and I wanted to capture coverage data. I've followed all the information needed to capture coverage data from subprocess but since Celery uses a fork from multiprocessing the coverage data is not being captured.\n\nThe above is not true when using `solo` or `threads` as `POOL_CLS` (`-P` command line option). When running using a different pool class the coverage data is being captured without any problem.\n\nTo clarify, by capturing coverage data I mean coverage information for the body of the Celery tasks. With that said when running in `prefork` mode, all but Celery tasks coverage data is captured.\n\nThis can be easily reproduced by following these steps:\n\nCreate a directory and create a `myapp.py` Python module with the same contents as https://github.com/celery/celery/blob/3.1/examples/app/myapp.py.\n\nCreate a virtual environment and install celery and coverage (I am using virtualenv-wrapper):\n\n``` console\nmkvirtualenv -p python3 celery\npip install celery==3.1 coverage threadpool\n```\n\nThen configure create a `sitecustomize.py` with the following contents on the corresponding Python site-packages:\n\n``` python\nimport coverage\ncoverage.process_startup()\n```\n\nIn my case I did:\n\n``` console\ncat > ~/.virtualenvs/celery/lib/python3.4/site-packages/sitecustomize.py << EOF\nimport coverage\ncoverage.process_startup()\nEOF\n```\n\nNext create a `.coveragerc` file:\n\n``` console\ncat > .coveragerc << EOF\n[run]\ninclude=myapp\nsource=.\nconcurrency=\n    thread\n    multiprocessing\nEOF\n```\n\nAnd export the `COVERAGE_PROCESS_START` with the path of the `.coveragerc` file, in my case it was located on the same directory:\n\n``` console\nexport COVERAGE_PROCESS_START=.coveragerc\n```\n\nWith all the above steps we are now set to check the the behavior using the different `POOL_CLS` options. First let's start with the default `prefork`:\n\nRun celery on one terminal window:\n\n``` console\ncelery -A myapp worker -l info\n```\n\nThen on a second window, open a python shell session and execute the `add` task:\n\n``` console\n$ python\n>>> from myapp import add\n>>> add.delay(2, 3)\n```\n\nAfter that, go back to the first window and press Ctrl+c to finish the celery process. If everything were setup properly some `.coverage.*` files should have been created. Here is the contents I got for all the coverage data files:\n\n```\n!coverage.py: This is a private format, don't read it directly!{\"lines\": {\"/home/elyezer/code/celery/examples/app/myapp.py\": [35, 39, 24, 25, 27, 28, 29]}}\n```\n\nAs you can see the line 37 is not listed there, this is the line for the add task function body. As we can see the coverage data was not captured even though the task was executed.\n\nNext step is to check how coverage data capturing works with `solo` or `threads`, on my local tests they behaved the same. For this we will use just `solo` since it does not require any additional package to be installed.\n\nMake sure to clean up the coverage data files before proceeding:\n\n``` console\nrm .coverage.*\n```\n\nNow we can run celery using `solo` as the `POOL_CLS`, on the first terminal window:\n\n``` console\ncelery -A myapp worker -l info -P solo\n```\n\nRepeat the task call on the second terminal window and exit the celery process on the first window. Then check for the generated coverage data files, on my local test I got:\n\n```\n!coverage.py: This is a private format, don't read it directly!{\"lines\": {\"/home/elyezer/code/celery/examples/app/myapp.py\": [35, 37, 39, 24, 25, 27, 28, 29]}}\n```\n\nNow the line 37 is included as expected.\n\nThe documentation about how to capture coverage data from subprocess can be found at [1]. Also coverage.py allows us to specify a concurrency library, multiprocessing is one of the valid values and coverage.py apply some patches [2] to the multiprocessing library when it is specified as one of the concurrency libraries.\n\nI saw that Billiard is setting some information on the Python built-in multiprocessing library but it seem that it is missing something.\n\nI wish I could capture the coverage information on a running celery process no matter the `POOL_CLS` used. \n\n[1] http://coverage.readthedocs.io/en/coverage-4.2/subprocess.html\n\n[2] https://bitbucket.org/ned/coveragepy/src/257e52793fb0f28853ddca679f67b158107262bf/coverage/multiproc.py\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3422/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xBeAsTx": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3412", "title": "Fix autoscaler issues", "body": "We use autoscaler and it is greate featuer. Please return autoscaler code in 4.0\n\nWhy you has droped support this feature?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3412/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jcyrss": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3377", "title": "Celery worker connection with Rabittmq met broken pipe error IN Gevent Mode", "body": "Celery version 3.1.22\n\nIn our web performance testing, Celery worker connection with Rabbitmq met broken pipe error IN Gevent Mode, while no problem when Celery worker work in Process pool mode.  \nAfter that, Celery workers will not get task messages from Rabbitmq anymore until they are restarted.\n\nThat issue always happen when the speed of Celery workers consuming task messages slower than Django applications producing messages, and about 3 thounds of messages piled in Rabbitmq.\n\nIs that due to Gevent Monkey Patch?\n\n======  Celery log ======\n\n[2016-08-08 13:52:06,913: CRITICAL/MainProcess] Couldn't ack 293, reason:error(32, 'Broken pipe')\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/site-packages/kombu/message.py\", line 93, in ack_log_error\n    self.ack()\n  File \"/usr/local/lib/python2.7/site-packages/kombu/message.py\", line 88, in ack\n    self.channel.basic_ack(self.delivery_tag)\n  File \"/usr/local/lib/python2.7/site-packages/amqp/channel.py\", line 1584, in basic_ack\n    self._send_method((60, 80), args)\n  File \"/usr/local/lib/python2.7/site-packages/amqp/abstract_channel.py\", line 56, in _send_method\n    self.channel_id, method_sig, args, content,\n  File \"/usr/local/lib/python2.7/site-packages/amqp/method_framing.py\", line 221, in write_method\n    write_frame(1, channel, payload)\n  File \"/usr/local/lib/python2.7/site-packages/amqp/transport.py\", line 182, in write_frame\n    frame_type, channel, size, payload, 0xce,\n  File \"/usr/local/lib/python2.7/site-packages/gevent/_socket2.py\", line 412, in sendall\n    timeleft = self.__send_chunk(chunk, flags, timeleft, end)\n  File \"/usr/local/lib/python2.7/site-packages/gevent/_socket2.py\", line 351, in __send_chunk\n    data_sent += self.send(chunk, flags)\n  File \"/usr/local/lib/python2.7/site-packages/gevent/_socket2.py\", line 320, in send\n    return sock.send(data, flags)\nerror: [Errno 32] Broken pipe\n\n======= Rabbitmq log ==================\n\n=ERROR REPORT==== 8-Aug-2016::14:28:33 ===\nclosing AMQP connection <0.15928.4> (10.26.39.183:60732 -> 10.26.39.183:5672):\n{writer,send_failed,{error,enotconn}}\n\n=ERROR REPORT==== 8-Aug-2016::14:29:03 ===\nclosing AMQP connection <0.15981.4> (10.26.39.183:60736 -> 10.26.39.183:5672):\n{writer,send_failed,{error,enotconn}}\n\n=ERROR REPORT==== 8-Aug-2016::14:29:03 ===\nclosing AMQP connection <0.15955.4> (10.26.39.183:60734 -> 10.26.39.183:5672):\n{writer,send_failed,{error,enotconn}}\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3377/reactions", "total_count": 2, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 2, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xidui": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3358", "title": "Route task to multiple queues?", "body": "Is there a mechanism in celery that can satisfy this? \n**two queues in the cluster:**\n- queue1: old server listen on this queue which can process tasks\n- queue2: new server listen on this queue which can also process tasks\n\nTasks can be processed either by server with queue1 or queue2, but queue2 is better.\nIf no server listen on queue2, then server listen on queue1 is also ok to process the tasks.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3358/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "larmoreg": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3350", "title": "Chain doesn't work between separately configured apps", "body": "I am trying to chain two separate workers (app1.py and app2.py) together using celery.\n\napp1.py\n\n```\nfrom celery import Celery\nfrom kombu import Exchange, Queue\n\napp1 = Celery('app1')\napp1.conf.update(\n    BROKER_URL='amqp://',\n    CELERY_RESULT_BACKEND='db+sqlite:///results.db',\n    CELERY_QUEUES=(\n        Queue('app1', Exchange('default', type='direct'), routing_key='app1'),\n        Queue('app2', Exchange('default', type='direct'), routing_key='app2'),\n    ),\n)\n\n\n@app1.task(exchange='default', routing_key='app1')\ndef add(x, y):\n    return x + y\n```\n\napp2.py\n\n```\nfrom celery import Celery\nfrom kombu import Exchange, Queue\n\napp2 = Celery('app2')\napp2.conf.update(\n    BROKER_URL='amqp://',\n    CELERY_RESULT_BACKEND='db+sqlite:///results.db',\n    CELERY_QUEUES=(\n        Queue('app1', Exchange('default', type='direct'), routing_key='app1'),\n        Queue('app2', Exchange('default', type='direct'), routing_key='app2'),\n    ),\n)\n\n\n@app2.task(exchange='default', routing_key='app2')\ndef mul(x, y):\n    return x * y\n```\n\nThe two workers are run using celery multi with two separate queues as follows:\n\n```\ncelery multi start app1 -A app1 -Q app1\ncelery multi start app2 -A app2 -Q app2\n```\n\nAnd below is the test I am running (called test.py). The first two lines work as expected but the third hangs.\n\ntest.py\n\n```\n#!/usr/bin/env python2\n\nfrom app1 import add\nfrom app2 import mul\n\nif __name__ == '__main__':\n    print(add.s(1, 2).apply_async().get())\n    print(mul.s(1, 2).apply_async().get())\n    print((add.s(1, 2) | mul.s(3)).apply_async().get())\n```\n\nLooking at rabbitmqadmin I see one message sitting in the celery queue with the payload:\n\n`[[3, 3], {}, {\"chord\": null, \"callbacks\": null, \"errbacks\": null, \"chain\": []}]`\n\nIs this a bug or a feature not yet implemented? It seems to me that having the ability to chain between multiple (potentially remote) workers would be an extremely useful feature.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3350/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bruno-rino": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3329", "title": "Task.replace() does not return results when CELERY_ALWAYS_EAGER = True", "body": "When CELERY_ALWAYS_EAGER = True, tasks are executed as expected, but the return value is missing. As a test, see this sample code adapted from #817:\n\n``` python\nfrom celery import Celery\n\napp = Celery('hello')\napp.conf.CELERY_ALWAYS_EAGER = True\n\n@app.task(bind=True)\ndef hello(self):\n    print('hello...')\n    raise self.replace(goodbye.s())\n    print('world')\n\n@app.task\ndef goodbye():\n    print('later...')\n    return 'good bye!'\n\nif __name__ == '__main__':\n    mytasks = hello.s()\n    result = mytasks.apply_async()\n    print(result.get())\n```\n\nWhen run, 'hello...' and 'later...' are printed, but result.get() returns None\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3329/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "outshow": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3316", "title": "celery events displays wrong time.", "body": "celery events displays wrong time with UTC zero timezone, not local timezone.\n\n337f0643-06a7-4d10-867e-baf704fcc63b celery@Kepler-186f    add    08:37:52 SUCCESS\n4662a488-4d04-4752-8fa4-ab9e58e637a2 celery@Kepler-186f   add    08:37:42 SUCCESS \n5e618b92-b2aa-43bc-8019-49cca426c64e celery@Kepler-186f    add    08:37:33 SUCCESS \n\ncelery flower displays the correct time.\n\nadd 337f0643-06a7-4d10-867e-baf704fcc63b    SUCCESS [16, 16]    {}  '32'    2016-07-14 16:37:52.698 2016-07-14 16:37:52.729 celery@Kepler-186f\nadd 4662a488-4d04-4752-8fa4-ab9e58e637a2    SUCCESS [16, 16]    {}  '32'    2016-07-14 16:37:42.643 2016-07-14 16:37:42.674 celery@Kepler-186f\n\nMy local timezone is Asia/Shanghai, and my configuration as below:\n\nCELERY_TIMEZONE = 'Asia/Shanghai'\n\ncelery version is 3.1.23 (Cipater)\nSorry for my pool English, is there anything wrong with my config?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3316/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alecklandgraf": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3273", "title": "Added documentation on interval kwarg", "body": "In the docs in a couple places `interval` is used though not documented. \n\ne.g. http://docs.celeryproject.org/en/latest/userguide/canvas.html#important-notes\n\n``` py\nfrom celery import maybe_signature\n\n@app.task(bind=True)\ndef unlock_chord(self, group, callback, interval=1, max_retries=None):\n    if group.ready():\n        return maybe_signature(callback).delay(group.join())\n    raise self.retry(countdown=interval, max_retries=max_retries)\n```\n\n_[EDIT 6-24-2016] rereading the example, I see `interval` isn't passed to anything in the celery lib but used as the countdown value for retry._\n\nI've also seen in used as an kwarg to a chord as follows, and I'm not sure what it accomplishes?\n\n``` py\nchord((import_contact.s(c) for c in contacts), notify_complete.si(import_id), interval=15).apply_async()\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3273/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "anan-lee": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3270", "title": "duplicate task in each work.", "body": "celery: 3.1.23 (Cipater)\nrabbitmq-server\uff1a3.2.4-1\n\ni found some task send to rabbitmq and then celery worker get duplicate task .\n\nfunction:\ndef do_backup(name):\n....\n\nfor example:\nserver: do_backup.delay(\"/tmp/__36\")\n\nworker02: \n[2016-06-23 20:38:27,182: INFO/MainProcess] Received task: tasks.do_backup[40159186-1b88-4d98-aabf-839bb4b7a852]\n[2016-06-23 20:38:27,183: INFO/MainProcess] tasks.do_backup[40159186-1b88-4d98-aabf-839bb4b7a852]: get file name: /tmp/__36\nworker12:\n[2016-06-23 20:36:21,647: INFO/MainProcess] Received task: tasks.do_backup[d265ffba-fa21-48b6-baad-2c0185a3168f]\n........\n[2016-06-23 20:37:01,492: INFO/MainProcess] tasks.do_backup[d265ffba-fa21-48b6-baad-2c0185a3168f]: get file name: /tmp/__36\n[2016-06-23 20:37:01,554: INFO/MainProcess] tasks.do_backup[d265ffba-fa21-48b6-baad-2c0185a3168f]: do task\n\ni make sure the server only send a \"/tmp/__36\" \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3270/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tisdall": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3241", "title": "task failure not handled by celery when broker connection error", "body": "In my case I'm using RabbitMQ as a broker.  If I try calling `.delay` on a task when RabbitMQ is down, I get a socket exception because a connection is attempted and it fails.  It seems like in other failure cases celery has machinery for handling exceptions (such as a task running and throwing an exception), but in this case an exception bubbles up out of celery.  So, it seems like I need to wrap every call in a try-except if I want to handle connection errors.  Shouldn't this be something celery does?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3241/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jainankit": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3238", "title": "celery, flask sqlalchemy: DatabaseError: (DatabaseError) SSL error: decryption failed or bad record mac", "body": "Related issues:\nhttps://github.com/celery/celery/issues/1564 and\nhttps://github.com/celery/celery/issues/634\n\nI'm using celery 3.1.20 and having similar issue to the one mentioned above even though I'm closing the connection after each task:\n\n```\n@task_postrun.connect\ndef close_session(*args, **kwargs):\n    # Flask SQLAlchemy will automatically create new sessions for you from \n    # a scoped session factory, given that we are maintaining the same app\n    # context, this ensures tasks have a fresh session (e.g. session errors \n    # won't propagate across tasks)\n    d.session.remove()\n\n@task_prerun.connect\ndef on_task_init(*args, **kwargs):\n    d.engine.dispose()\n```\n\nI'm still seeing this error. I believe this is related to celery forking and sharing a database connection, but then it should be fixed with above approach? Is there a way to force celery to create new database connection during fork?\n\nNote that I'm running this on AWS (with two servers accessing same database). Database itself is hosted on it's own server (not RDS). The total celery background tasks running are 6 (2+4). Flask frontend is running using gunicorn. Also I see this behavior intermittently and not consistently.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3238/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ztsv": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3237", "title": "RuntimeError: Second simultaneous read on fileno 23 detected", "body": "Sometimes i get this error:\n\nThe contents of the full traceback was: \n\n```\nTraceback (most recent call last): \nFile \"/home/sm/env/local/lib/python2.7/site-packages/celery/app/trace.py\", line 240, in trace_task \nR = retval = fun(*args, **kwargs) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/celery/app/trace.py\", line 438, in __protected_call__ \nreturn self.run(*args, **kwargs) \nFile \"imapp/tasks.py\", line 163, in save_users_with_meta \nupdate_user_info.apply_async((user['id'], ), ) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/celery/app/task.py\", line 565, in apply_async \n**dict(self._get_exec_options(), **options) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/celery/app/base.py\", line 354, in send_task \nreply_to=reply_to or self.oid, **options \nFile \"/home/sm/env/local/lib/python2.7/site-packages/celery/app/amqp.py\", line 310, in publish_task \n**kwargs \nFile \"/home/sm/env/local/lib/python2.7/site-packages/kombu/messaging.py\", line 172, in publish \nrouting_key, mandatory, immediate, exchange, declare) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/kombu/connection.py\", line 436, in _ensured \nreturn fun(*args, **kwargs) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/kombu/messaging.py\", line 184, in _publish \n[maybe_declare(entity) for entity in declare] \nFile \"/home/sm/env/local/lib/python2.7/site-packages/kombu/messaging.py\", line 111, in maybe_declare \nreturn maybe_declare(entity, self.channel, retry, **retry_policy) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/kombu/common.py\", line 113, in maybe_declare \nreturn _maybe_declare(entity, declared, ident, channel) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/kombu/common.py\", line 120, in _maybe_declare \nentity.declare() \nFile \"/home/sm/env/local/lib/python2.7/site-packages/kombu/entity.py\", line 522, in declare \nself.queue_declare(nowait, passive=False) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/kombu/entity.py\", line 548, in queue_declare \nnowait=nowait) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/kombu/transport/virtual/__init__.py\", line 447, in queue_declare \nreturn queue_declare_ok_t(queue, self._size(queue), 0) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/kombu/transport/redis.py\", line 690, in _size \nsizes = pipe.execute() \nFile \"/home/sm/env/local/lib/python2.7/site-packages/redis/client.py\", line 2578, in execute \nreturn execute(conn, stack, raise_on_error) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/redis/client.py\", line 2447, in _execute_transaction \nconnection.send_packed_command(all_cmds) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/redis/connection.py\", line 532, in send_packed_command \nself.connect() \nFile \"/home/sm/env/local/lib/python2.7/site-packages/redis/connection.py\", line 440, in connect \nself.on_connect() \nFile \"/home/sm/env/local/lib/python2.7/site-packages/redis/connection.py\", line 508, in on_connect \nif nativestr(self.read_response()) != 'OK': \nFile \"/home/sm/env/local/lib/python2.7/site-packages/redis/connection.py\", line 569, in read_response \nresponse = self._parser.read_response() \nFile \"/home/sm/env/local/lib/python2.7/site-packages/redis/connection.py\", line 224, in read_response \nresponse = self._buffer.readline() \nFile \"/home/sm/env/local/lib/python2.7/site-packages/redis/connection.py\", line 162, in readline \nself._read_from_socket() \nFile \"/home/sm/env/local/lib/python2.7/site-packages/redis/connection.py\", line 120, in _read_from_socket \ndata = self._sock.recv(socket_read_size) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/eventlet/greenio/base.py\", line 326, in recv \ntimeout_exc=socket.timeout(\"timed out\")) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/eventlet/greenio/base.py\", line 201, in _trampoline \nmark_as_closed=self._mark_as_closed) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/eventlet/hubs/__init__.py\", line 158, in trampoline \nlistener = hub.add(hub.READ, fileno, current.switch, current.throw, mark_as_closed) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/eventlet/hubs/epolls.py\", line 49, in add \nlistener = BaseHub.add(self, evtype, fileno, cb, tb, mac) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/eventlet/hubs/hub.py\", line 177, in add \nevtype, fileno, evtype, cb, bucket[fileno])) \nRuntimeError: Second simultaneous read on fileno 23 detected. Unless you really know what you're doing, make sure that only one greenthread can read any particular socket. Consider using a pools.Pool. If you do know what you're doing and want to disable this error, call eventlet.debug.hub_prevent_multiple_readers(False) - MY THREAD=<built-in method switch of greenlet.greenlet object at 0x7f59b15ac870>; THAT THREAD=FdListener('read', 23, <function on_read at 0x7f59ae3e89b0>, <function on_error at 0x7f59ae32c140>)\n```\n\nLimits:\n\n```\ncat /proc/3491/limits\nLimit                     Soft Limit           Hard Limit           Units\nMax cpu time              unlimited            unlimited            seconds\nMax file size             25600000             25600000             bytes\nMax data size             unlimited            unlimited            bytes\nMax stack size            8388608              unlimited            bytes\nMax core file size        0                    unlimited            bytes\nMax resident set          unlimited            unlimited            bytes\nMax processes             3950                 3950                 processes\nMax open files            100000               100000               files\nMax locked memory         65536                65536                bytes\nMax address space         unlimited            unlimited            bytes\nMax file locks            unlimited            unlimited            locks\nMax pending signals       3950                 3950                 signals\nMax msgqueue size         819200               819200               bytes\nMax nice priority         0                    0\nMax realtime priority     0                    0\nMax realtime timeout      unlimited            unlimited            us\n```\n\nFD Usage:\n\n```\nls -l /proc/3491/fd\ntotal 0\nlrwx------ 1 sm sm 64 Jun  5 12:34 0 -> /dev/null\nlrwx------ 1 sm sm 64 Jun  5 12:34 1 -> /dev/null\nl-wx------ 1 sm sm 64 Jun  5 12:34 10 -> /var/log/celery/celery.log\nl-wx------ 1 sm sm 64 Jun  5 12:34 11 -> /var/log/celery/celery.log\nlrwx------ 1 sm sm 64 Jun  5 12:34 12 -> socket:[93605326]\nlrwx------ 1 sm sm 64 Jun  5 12:34 13 -> anon_inode:[eventpoll]\nlrwx------ 1 sm sm 64 Jun  5 12:34 14 -> socket:[93605327]\nlrwx------ 1 sm sm 64 Jun  5 12:34 15 -> socket:[93605328]\nlrwx------ 1 sm sm 64 Jun  5 12:34 16 -> socket:[93605329]\nlrwx------ 1 sm sm 64 Jun  5 12:34 17 -> socket:[93605474]\nlrwx------ 1 sm sm 64 Jun  5 12:34 18 -> socket:[93605475]\nlrwx------ 1 sm sm 64 Jun  5 12:34 19 -> socket:[93605476]\nlrwx------ 1 sm sm 64 Jun  5 12:34 2 -> /dev/null\nlrwx------ 1 sm sm 64 Jun  5 12:34 20 -> socket:[93605478]\nlrwx------ 1 sm sm 64 Jun  5 12:34 21 -> socket:[93605479]\nlrwx------ 1 sm sm 64 Jun  5 12:34 22 -> socket:[93605482]\nlrwx------ 1 sm sm 64 Jun  5 12:34 23 -> socket:[93605483]\nlrwx------ 1 sm sm 64 Jun  5 12:34 25 -> socket:[93614751]\nlrwx------ 1 sm sm 64 Jun  5 12:34 26 -> socket:[93614580]\nlrwx------ 1 sm sm 64 Jun  5 12:34 28 -> socket:[93614488]\nlrwx------ 1 sm sm 64 Jun  5 12:34 3 -> /dev/null\nlrwx------ 1 sm sm 64 Jun  5 12:34 30 -> socket:[93625627]\nlrwx------ 1 sm sm 64 Jun  5 12:34 38 -> socket:[93610841]\nlrwx------ 1 sm sm 64 Jun  5 12:34 4 -> /dev/null\nlrwx------ 1 sm sm 64 Jun  5 12:34 5 -> /dev/null\nl-wx------ 1 sm sm 64 Jun  5 12:34 6 -> /home/sm/log/accounts.log\nl-wx------ 1 sm sm 64 Jun  5 12:34 7 -> /home/sm/log/sm.log\n```\n\nPackages:\n\n```\ncelery==3.1.23\ngreenlet==0.4.9\nkombu==3.0.35\nredis==2.10.3\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3237/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/3227", "title": "Redis: Eventlet: Worker \"Too many connections\" error while executing chords", "body": "Sometimes i have an issue. It happens only on workers that uses chords.\n\nThe contents of the full traceback was: \n\n<pre>\nTraceback (most recent call last): \nFile \"/home/sm/env/local/lib/python2.7/site-packages/celery/app/trace.py\", line 283, in trace_task \nuuid, retval, SUCCESS, request=task_request, \nFile \"/home/sm/env/local/lib/python2.7/site-packages/celery/backends/base.py\", line 271, in store_result \nrequest=request, **kwargs) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/celery/backends/base.py\", line 505, in _store_result \nself.set(self.get_key_for_task(task_id), self.encode(meta)) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/celery/backends/redis.py\", line 161, in set \nreturn self.ensure(self._set, (key, value), **retry_policy) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/celery/backends/redis.py\", line 150, in ensure \n**retry_policy \nFile \"/home/sm/env/local/lib/python2.7/site-packages/kombu/utils/__init__.py\", line 246, in retry_over_time \nreturn fun(*args, **kwargs) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/celery/backends/redis.py\", line 170, in _set \npipe.execute() \nFile \"/home/sm/env/local/lib/python2.7/site-packages/redis/client.py\", line 2572, in execute \nself.shard_hint) \nFile \"/home/sm/env/local/lib/python2.7/site-packages/redis/connection.py\", line 874, in get_connection \nconnection = self.make_connection() \nFile \"/home/sm/env/local/lib/python2.7/site-packages/redis/connection.py\", line 881, in make_connection \nraise ConnectionError(\"Too many connections\") \nConnectionError: Too many connections \n</pre>\n\n\nI set additional settings bit it doesn't help:\n\n<pre>\nBROKER_TRANSPORT_OPTIONS = {\n    'max_connections': 2000,\n}\n</pre>\n\n\nOther workers works fine.\n\n<pre>\nredis==2.10.3\ncelery==3.1.23\nkombu==3.0.34\n</pre>\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3227/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "RyanBalfanz": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3213", "title": "relation \"task_id_sequence\" does not exist when SQLAlchemy is configured as the result backend", "body": "The Exception is thrown when using postgresql, but not when using sqlite.\n\n```\nCELERY_BROKER_URL=redis://redis\nCELERY_RESULT_BACKEND=db+postgresql://postgres:postgres@db/postgres\n```\n\n```\nworker_1  | [2016-05-17 09:32:07,886: CRITICAL/MainProcess] Task composeexample.celery.debug_task[49b2046c-2619-4ad7-9b3b-ad67ea4f0c03] INTERNAL ERROR: ProgrammingError('(psycopg2.ProgrammingError) relation \"task_id_sequence\" does not exist\\nLINE 1: ...us, result, date_done, traceback) VALUES (nextval(\\'task_id_s...\\n                                                             ^\\n',)\nworker_1  | Traceback (most recent call last):\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/celery/app/trace.py\", line 283, in trace_task\nworker_1  |     uuid, retval, SUCCESS, request=task_request,\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/celery/backends/base.py\", line 271, in store_result\nworker_1  |     request=request, **kwargs)\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/celery/backends/database/__init__.py\", line 63, in _inner\nworker_1  |     return fun(*args, **kwargs)\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/celery/backends/database/__init__.py\", line 125, in _store_result\nworker_1  |     session.flush()\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/sqlalchemy/orm/session.py\", line 2019, in flush\nworker_1  |     self._flush(objects)\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/sqlalchemy/orm/session.py\", line 2137, in _flush\nworker_1  |     transaction.rollback(_capture_exception=True)\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/sqlalchemy/util/langhelpers.py\", line 60, in __exit__\nworker_1  |     compat.reraise(exc_type, exc_value, exc_tb)\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/sqlalchemy/util/compat.py\", line 186, in reraise\nworker_1  |     raise value\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/sqlalchemy/orm/session.py\", line 2101, in _flush\nworker_1  |     flush_context.execute()\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/sqlalchemy/orm/unitofwork.py\", line 373, in execute\nworker_1  |     rec.execute(self)\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/sqlalchemy/orm/unitofwork.py\", line 532, in execute\nworker_1  |     uow\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/sqlalchemy/orm/persistence.py\", line 174, in save_obj\nworker_1  |     mapper, table, insert)\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/sqlalchemy/orm/persistence.py\", line 800, in _emit_insert_statements\nworker_1  |     execute(statement, params)\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/sqlalchemy/engine/base.py\", line 914, in execute\nworker_1  |     return meth(self, multiparams, params)\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/sqlalchemy/sql/elements.py\", line 323, in _execute_on_connection\nworker_1  |     return connection._execute_clauseelement(self, multiparams, params)\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/sqlalchemy/engine/base.py\", line 1010, in _execute_clauseelement\nworker_1  |     compiled_sql, distilled_params\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/sqlalchemy/engine/base.py\", line 1146, in _execute_context\nworker_1  |     context)\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/sqlalchemy/engine/base.py\", line 1341, in _handle_dbapi_exception\nworker_1  |     exc_info\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/sqlalchemy/util/compat.py\", line 202, in raise_from_cause\nworker_1  |     reraise(type(exception), exception, tb=exc_tb, cause=cause)\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/sqlalchemy/util/compat.py\", line 185, in reraise\nworker_1  |     raise value.with_traceback(tb)\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/sqlalchemy/engine/base.py\", line 1139, in _execute_context\nworker_1  |     context)\nworker_1  |   File \"/usr/local/lib/python3.5/site-packages/sqlalchemy/engine/default.py\", line 450, in do_execute\nworker_1  |     cursor.execute(statement, parameters)\nworker_1  | sqlalchemy.exc.ProgrammingError: (psycopg2.ProgrammingError) relation \"task_id_sequence\" does not exist\nworker_1  | LINE 1: ...us, result, date_done, traceback) VALUES (nextval('task_id_s...\nworker_1  |                                                              ^\nworker_1  |  [SQL: \"INSERT INTO celery_taskmeta (id, task_id, status, result, date_done, traceback) VALUES (nextval('task_id_sequence'), %(task_id)s, %(status)s, %(result)s, %(date_done)s, %(traceback)s) RETURNING celery_taskmeta.id\"] [parameters: {'result': None, 'status': 'PENDING', 'date_done': datetime.datetime(2016, 5, 17, 9, 32, 7, 831590), 'task_id': '49b2046c-2619-4ad7-9b3b-ad67ea4f0c03', 'traceback': None}]\n```\n\n``` shell\n(.venv) \u279c  pip freeze\namqp==1.4.9\nanyjson==0.3.3\nBabel==2.3.4\nbackports.shutil-get-terminal-size==1.0.0\nbilliard==3.3.0.23\ncelery==3.1.23\ndecorator==4.0.9\ndj-database-url==0.4.1\nDjango==1.9.6\ndjango-celery==3.1.17\nflower==0.9.1\nipython==4.2.0\nipython-genutils==0.1.0\nkombu==3.0.35\npexpect==4.0.1\npickleshare==0.7.2\npsycopg2==2.6.1\nptyprocess==0.5.1\npytz==2016.4\nredis==2.10.5\nrequests==2.10.0\nsimplegeneric==0.8.1\nSQLAlchemy==1.0.13\ntornado==4.2\ntraitlets==4.2.1\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3213/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "juanriaza": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3198", "title": "Custom signature", "body": "I've been trying to make the following [custom signature](http://celery-workflows.herokuapp.com/#/12/2) to work on `master`.\n\n``` python\nimport itertools\n\nfrom celery import Celery\nfrom celery import chord\nfrom celery.canvas import Signature\nfrom celery.canvas import maybe_signature\nfrom celery.utils.functional import chunks\nfrom celery.utils.log import get_task_logger\n\nlogger = get_task_logger(__name__)\n\n\napp = Celery(\n    'celery-demo',\n    backend='redis://localhost',\n    broker='redis://localhost')\n\napp.conf.update(\n    CELERY_TASK_SERIALIZER='json',\n    CELERY_ACCEPT_CONTENT=['json'],  # Ignore other content\n    CELERY_RESULT_SERIALIZER='json',\n    CELERY_TIMEZONE='Europe/Madrid',\n    CELERY_ENABLE_UTC=True,\n)\n\n\n@app.task\ndef jointask(it):\n    return list(itertools.chain(*it))\n\n\n@Signature.register_type\nclass weave(Signature):\n    def __init__(self, my_task, n, task='weave',\n                 args=(), kwargs={}, app=None, **options):\n        Signature.__init__(\n            self, task, args, dict(kwargs, my_task=my_task, n=n),\n            app=app, **options)\n        self.subtask_type = 'weave'\n\n    @classmethod\n    def from_dict(cls, d, app=None):\n        args, d['kwargs'] = cls._unpack_args(**d['kwargs'])\n        return cls(*args, app=app, **d)\n\n    @staticmethod\n    def _unpack_args(my_task=None, n=None, **kwargs):\n        # Python signatures are better at extracting keys from dicts\n        # than manually popping things off.\n        return (my_task, n), kwargs\n\n    def apply_async(self, args=None, kwargs=None, **options):\n        args, kwargs, _ = self._merge(args=args, kwargs=kwargs)\n        it, = args\n        n = kwargs['n']\n        task = kwargs['my_task']\n        task = maybe_signature(task)\n        body = jointask.s()\n        body.freeze(self.freeze().task_id)\n        tasks = [task.clone(args=[p]) for p in chunks(iter(it), n)]\n        return chord(tasks, body).apply_async()\n\n\n@app.task\ndef rangetask(_min, _max):\n        return list(range(_min, _max))\n\n\n@app.task\ndef multiplytask(it, num):\n    it = map(lambda i: i * num, it)\n    return it\n```\n\nSo far this works,\n\n``` python\n>>> from samplecel import weave, rangetask, multiplytask\n>>> demo_task = (rangetask.s(1, 7) | weave(multiplytask.s(2), 2)).apply_async()\n>>> demo_task\n<AsyncResult: 5b2e27e6-8ba7-4049-8147-8d869026959a>\n>>> demo_task.get()\n[2, 4, 6, 8, 10, 12]\n```\n\nbut hangs if I try to chain multiple tasks of the custom signature\n\n``` python\n>>> from samplecel import weave, rangetask, multiplytask\n>>> demo_task = (rangetask.s(1, 7) | weave(multiplytask.s(2), 2) | weave(multiplytask.s(3), 2)).apply_async()\n>>> demo_task\n<AsyncResult: 15733369-bef1-460d-b0be-e61129c694a9>\n>>> demo_task.get()\n\n```\n\nI'm probably missing something while trying to recreate the custom signature. Any help would be much appreciated.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3198/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "obforfair": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3193", "title": "Eventlet: app.control.inspect().active_queues() error", "body": "HI all:\nI use celery 3.1.23\nsometimes when app.control.inspect().active_queues()  cause this error:\n\nload_queues \u6570\u636e\u51fa\u9519Traceback (most recent call last):\nFile \"pycron/util/tool.py\", line 16, in task\nreturn f(_args, *_kwargs)\nFile \"pycron/pycron.py\", line 403, in load_queues\nload_queues(retry)\nFile \"pycron/util/tool.py\", line 24, in task\nraise e\nRuntimeError: Second simultaneous read on fileno 26 detected. Unless you really know what you're doing, make sure that only one greenthread can read any particular socket. Consider using a pools.Pool. If you do know what you're doing and want to disable this error, call eventlet.debug.hub_prevent_multiple_readers(False) - MY THREAD=; THAT THREAD=FdListener('read', 26, , )\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3193/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "amol-": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3189", "title": "Using Worker in same process of the main app makes tasks think they are called from another task", "body": "I wanted to manage the Worker that runs the tasks in a separate thread of the same application that offloads them. The target is to replace previous `Thread().start()` calls to offload some work maintaining the same convenience with added guarantee of tracking and recovering tasks execution.\n\nThe issue is that `task_join_will_block`\u00a0just reads a global module variable, so tasks think that they got called from inside another task and issue a warning.\n\nI have been able to replicate the problem with this snippet:\n\n```\nimport celery\ncapp = celery.Celery('testapp',\n                     broker='mongodb://localhost/celery_test_broker',\n                     backend='mongodb://localhost/celery_test_results')\n\nimport celery.app\ncelery.app.set_default_app(capp)\n\nimport threading\nimport celery.worker\nworker = celery.worker.WorkController(app=capp, pool_cls='solo')\nthreading.Thread(target=worker.start).start()\n\n# Wait for Worker to be fully up and running\nimport time\ntime.sleep(1)\n\n@capp.task(name='testapp.sometaskwithresult')\ndef sometask_withresult(a, b):\n   return a+b\n\n# Update list of available tasks as task has been created after worker has started.\nworker.reload()\n\nprint '\\n\\nRESULT:', sometask_withresult.apply_async((1, 2)).get(timeout=2)\n\nworker.stop()\n```\n\nRunning the snippet will lead to\n\n```\n.../lib/python2.7/site-packages/celery/result.py:45: RuntimeWarning: Never call result.get() within a task!\nSee http://docs.celeryq.org/en/latest/userguide/tasks.html#task-synchronous-subtasks\n\nIn Celery 3.2 this will result in an exception being\nraised instead of just being a warning.\n\n  warnings.warn(RuntimeWarning(E_WOULDBLOCK))\n\n\nRESULT: 3\n```\n\nAs this is currently just a warning I can probably ignore it as everything seems to be working as expected (as there isn't any task actually waiting for another task) but I'm concerned about the fact that it will be switched to an exception actually breaking this. \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3189/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chris-hydra": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3187", "title": "Documentation issue: BROKER_URL !== CELERY_BROKER_URL", "body": "Thanks for all the great work on Celery - cross-posting this in case anyone else has this issue using Celery in Docker.\n\nThe documentation doesn't make it clear that `BROKER_URL` is deprecated in favour of `CELERY_BROKER_URL`.\n\nThe [configuration](http://docs.celeryproject.org/en/latest/configuration.html) documentation makes no mention of `CELERY_BROKER_URL` - all examples use `BROKER_URL`.\n\nIf you use the [official Celery image](https://github.com/docker-library/celery) in a docker-compose setup and rename your RabbitMQ service to anything but \"rabbit\" in your `docker-compose.yml`, and try (according to the  [configuration docs](http://docs.celeryproject.org/en/latest/configuration.html)) to set `BROKER_URL` accordingly, this image won't work.\n\nFor example, this docker-compose file won't work:\n\n```\nfoo:\n  image: rabbitmq:3\n  expose:\n    - \"5672\"\n\ncelery:\n  image: celery:3.1\n  environment:\n    - BROKER_URL=amqp://guest:guest@foo//\n  links:\n    - foo\n```\n\n`[ERROR/MainProcess] consumer: Cannot connect to amqp://guest:**@rabbit:5672//: [Errno -2] Name or service not known.`\n\nThis works:\n\n```\nfoo:\n  image: rabbitmq:3\n  expose:\n    - \"5672\"\n\ncelery:\n  image: celery:3.1\n  environment:\n    - CELERY_BROKER_URL=amqp://guest:guest@foo//\n  links:\n    - foo\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3187/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "edanayal": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3183", "title": "Documentation issue: relevancy of Visibility Timeout for periodic tasks", "body": "Hi,\nIn http://docs.celeryproject.org/en/latest/getting-started/brokers/redis.html, I found: _\"Periodic tasks will not be affected by the visibility timeout, as this is a concept separate from ETA/countdown\"_\nHowever, in my celery-redis system, I have a long periodic task (issued by beat) that does get retried after 1 hour because it did not yet complete. After extending the Visibility Timeout I can see that there's no retry after 1 hour.\nSo, I think this line in the documentation might not be accurate, or maybe it refers to some other case?\nThanks!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wavenator": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3179", "title": "Workers goes offline after completing random number of tasks", "body": "``` shell\n[2016-04-24 03:30:06,066: ERROR/Process-1] Unrecoverable error: AttributeError(\"'NoneType' object has no attribute 'fileno'\",)\nTraceback (most recent call last):\n  File \"/usr/lib/python3.5/site-packages/celery/worker/__init__.py\", line 206, in start\n    self.blueprint.start(self)\n  File \"/usr/lib/python3.5/site-packages/celery/bootsteps.py\", line 123, in start\n    step.start(parent)\n  File \"/usr/lib/python3.5/site-packages/celery/bootsteps.py\", line 374, in start\n    return self.obj.start()\n  File \"/usr/lib/python3.5/site-packages/celery/worker/consumer.py\", line 279, in start\n    blueprint.start(self)\n  File \"/usr/lib/python3.5/site-packages/celery/bootsteps.py\", line 123, in start\n    step.start(parent)\n  File \"/usr/lib/python3.5/site-packages/celery/worker/consumer.py\", line 838, in start\n    c.loop(*c.loop_args())\n  File \"/usr/lib/python3.5/site-packages/celery/worker/loops.py\", line 41, in asynloop\n    obj.controller.register_with_event_loop(hub)\n  File \"/usr/lib/python3.5/site-packages/celery/worker/__init__.py\", line 218, in register_with_event_loop\n    description='hub.register',\n  File \"/usr/lib/python3.5/site-packages/celery/bootsteps.py\", line 155, in send_all\n    fun(parent, *args)\n  File \"/usr/lib/python3.5/site-packages/celery/worker/components.py\", line 185, in register_with_event_loop\n    w.pool.register_with_event_loop(hub)\n  File \"/usr/lib/python3.5/site-packages/celery/concurrency/prefork.py\", line 141, in register_with_event_loop\n    return reg(loop)\n  File \"/usr/lib/python3.5/site-packages/celery/concurrency/asynpool.py\", line 448, in register_with_event_loop\n    [self._track_child_process(w, hub) for w in self._pool]\n  File \"/usr/lib/python3.5/site-packages/celery/concurrency/asynpool.py\", line 448, in <listcomp>\n    [self._track_child_process(w, hub) for w in self._pool]\n  File \"/usr/lib/python3.5/site-packages/celery/concurrency/asynpool.py\", line 431, in _track_child_process\n    hub.add_reader(fd, self._event_process_exit, hub, proc)\n  File \"/usr/lib/python3.5/site-packages/kombu/async/hub.py\", line 210, in add_reader\n    return self.add(fds, callback, READ | ERR, args)\n  File \"/usr/lib/python3.5/site-packages/kombu/async/hub.py\", line 160, in add\n    fd = fileno(fd)\n  File \"/usr/lib/python3.5/site-packages/kombu/utils/__init__.py\", line 445, in fileno\n    return f.fileno()\nAttributeError: 'NoneType' object has no attribute 'fileno'\n```\n\nWe're using redis as a backend and broker.\nThe have hundreds of workers, some of them turned offline and the majority of them were not.\nThose who turned offline has logged this error, and there are some that hasn't logged any kind of error.\nWe're using python 3.5, and the most recent stable versions of celery/kombu....\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3179/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Gwill": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3177", "title": "[feature request] a method return chain task state", "body": "related with: #3140 \n\n``` python\n@task\ndef t1(x):\n    raise Exception\n\n@task\ndef t2(x):\n    return x + 1\n\nr = chain(t1.s(1), t2.s(2))()\n```\n\nI think, there should be a method like `r.chain_state()` return the r state. no need user to check all states of parent tasks.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3177/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sumitgoelpw": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3176", "title": "Documentation: Celery calling tasks error - OSError: [Errno 61] Connection refused", "body": "Hi there,\n\nI have Django==1.9.5, celery==3.1.23, kombu==3.0.35 and amqp==1.4.9 with redis broker and redis==2.10.5 running on Python 3.5.1. I am running redis locally, installed using brew.\n\nI followed the documentionation http://docs.celeryproject.org/en/latest/django/first-steps-with-django.html but keep getting below error,\n\n```\nPython 3.5.1 (default, Mar  5 2016, 15:28:46)\n[GCC 4.2.1 Compatible Apple LLVM 7.0.2 (clang-700.1.81)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from focuss.tasks import add\n>>> add.delay(3,2)\nTraceback (most recent call last):\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/kombu/utils/__init__.py\", line 423, in __call__\n    return self.__value__\nAttributeError: 'ChannelPromise' object has no attribute '__value__'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/kombu/connection.py\", line 436, in _ensured\n    return fun(*args, **kwargs)\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/kombu/messaging.py\", line 177, in _publish\n    channel = self.channel\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/kombu/messaging.py\", line 194, in _get_channel\n    channel = self._channel = channel()\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/kombu/utils/__init__.py\", line 425, in __call__\n    value = self.__value__ = self.__contract__()\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/kombu/messaging.py\", line 209, in <lambda>\n    channel = ChannelPromise(lambda: connection.default_channel)\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/kombu/connection.py\", line 757, in default_channel\n    self.connection\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/kombu/connection.py\", line 742, in connection\n    self._connection = self._establish_connection()\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/kombu/connection.py\", line 697, in _establish_connection\n    conn = self.transport.establish_connection()\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/kombu/transport/pyamqp.py\", line 116, in establish_connection\n    conn = self.Connection(**opts)\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/amqp/connection.py\", line 165, in __init__\n    self.transport = self.Transport(host, connect_timeout, ssl)\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/amqp/connection.py\", line 186, in Transport\n    return create_transport(host, connect_timeout, ssl)\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/amqp/transport.py\", line 299, in create_transport\n    return TCPTransport(host, connect_timeout)\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/amqp/transport.py\", line 95, in __init__\n    raise socket.error(last_err)\nOSError: [Errno 61] Connection refused\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/celery/app/task.py\", line 453, in delay\n    return self.apply_async(args, kwargs)\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/celery/app/task.py\", line 565, in apply_async\n    **dict(self._get_exec_options(), **options)\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/celery/app/base.py\", line 354, in send_task\n    reply_to=reply_to or self.oid, **options\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/celery/app/amqp.py\", line 310, in publish_task\n    **kwargs\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/kombu/messaging.py\", line 172, in publish\n    routing_key, mandatory, immediate, exchange, declare)\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/kombu/connection.py\", line 457, in _ensured\n    interval_max)\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/kombu/connection.py\", line 369, in ensure_connection\n    interval_start, interval_step, interval_max, callback)\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/kombu/utils/__init__.py\", line 246, in retry_over_time\n    return fun(*args, **kwargs)\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/kombu/connection.py\", line 237, in connect\n    return self.connection\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/kombu/connection.py\", line 742, in connection\n    self._connection = self._establish_connection()\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/kombu/connection.py\", line 697, in _establish_connection\n    conn = self.transport.establish_connection()\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/kombu/transport/pyamqp.py\", line 116, in establish_connection\n    conn = self.Connection(**opts)\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/amqp/connection.py\", line 165, in __init__\n    self.transport = self.Transport(host, connect_timeout, ssl)\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/amqp/connection.py\", line 186, in Transport\n    return create_transport(host, connect_timeout, ssl)\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/amqp/transport.py\", line 299, in create_transport\n    return TCPTransport(host, connect_timeout)\n  File \"/Users/sumit.goel/.virtualenvs/myfocus/lib/python3.5/site-packages/amqp/transport.py\", line 95, in __init__\n    raise socket.error(last_err)\nOSError: [Errno 61] Connection refused\n>>>\n```\n\nI have following config for celery in settings.py,\n\n```\nBROKER_URL = 'redis://127.0.0.1:6379/'\nCELERY_RESULT_BACKEND = 'redis://127.0.0.1:6379/'\nCELERY_ACCEPT_CONTENT = ['json']\nCELERY_TASK_SERIALIZER = 'json'\nCELERY_RESULT_SERIALIZER = 'json'\n```\n\nGoogle search shows similar issue had been reported under https://github.com/celery/kombu/issues/369 but the issue is closed with a comment that Python 3 issue back in 2014. Please help to resolve the issue.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3176/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 1, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "macnibblet": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3163", "title": "Worker spamming sync with other workers", "body": "log output and this just goes on and on\n\n```\n[2016-04-12 20:42:20,471: INFO/MainProcess] sync with celery@transfer.bs12 \n[2016-04-12 20:42:20,652: INFO/MainProcess] sync with celery@transfer.bs5 \n[2016-04-12 20:42:20,693: INFO/MainProcess] sync with celery@transfer.bs22 \n[2016-04-12 20:42:20,736: INFO/MainProcess] sync with celery@transfer.bs13 \n[2016-04-12 20:42:20,887: INFO/MainProcess] sync with celery@transfer.bs9 \n[2016-04-12 20:42:20,913: INFO/MainProcess] sync with celery@transfer.bs26 \n[2016-04-12 20:42:20,983: INFO/MainProcess] sync with celery@transfer.bs21 \n[2016-04-12 20:42:21,266: INFO/MainProcess] sync with celery@transfer.bs30 \n[2016-04-12 20:42:21,424: INFO/MainProcess] sync with celery@transfer.bs11 \n[2016-04-12 20:42:21,456: INFO/MainProcess] sync with celery@transfer.bs24 \n[2016-04-12 20:42:21,462: INFO/MainProcess] sync with celery@transfer.bs15 \n[2016-04-12 20:42:21,514: INFO/MainProcess] sync with celery@transfer.bs10 \n[2016-04-12 20:42:21,579: INFO/MainProcess] sync with celery@transfer.bs27 \n[2016-04-12 20:42:21,609: INFO/MainProcess] sync with celery@transfer.bs17 \n[2016-04-12 20:42:21,611: INFO/MainProcess] sync with celery@transfer.bs12 \n[2016-04-12 20:42:21,860: INFO/MainProcess] sync with celery@transfer.bs22 \n[2016-04-12 20:42:21,899: INFO/MainProcess] sync with celery@transfer.bs13 \n[2016-04-12 20:42:21,901: INFO/MainProcess] sync with celery@transfer.bs5 \n[2016-04-12 20:42:22,038: INFO/MainProcess] sync with celery@transfer.bs9 \n[2016-04-12 20:42:22,060: INFO/MainProcess] sync with celery@transfer.bs26 \n[2016-04-12 20:42:22,124: INFO/MainProcess] sync with celery@transfer.bs21 \n[2016-04-12 20:42:22,409: INFO/MainProcess] sync with celery@transfer.bs30 \n[2016-04-12 20:42:22,589: INFO/MainProcess] sync with celery@transfer.bs11 \n[2016-04-12 20:42:22,606: INFO/MainProcess] sync with celery@transfer.bs24 \n[2016-04-12 20:42:22,612: INFO/MainProcess] sync with celery@transfer.bs15 \n[2016-04-12 20:42:22,655: INFO/MainProcess] sync with celery@transfer.bs10 \n[2016-04-12 20:42:22,727: INFO/MainProcess] sync with celery@transfer.bs27 \n[2016-04-12 20:42:22,753: INFO/MainProcess] sync with celery@transfer.bs12 \n[2016-04-12 20:42:22,779: INFO/MainProcess] sync with celery@transfer.bs17 \n[2016-04-12 20:42:23,022: INFO/MainProcess] sync with celery@transfer.bs22 \n[2016-04-12 20:42:23,078: INFO/MainProcess] sync with celery@transfer.bs13 \n[2016-04-12 20:42:23,166: INFO/MainProcess] sync with celery@transfer.bs5 \n[2016-04-12 20:42:23,181: INFO/MainProcess] sync with celery@transfer.bs9 \n[2016-04-12 20:42:23,202: INFO/MainProcess] sync with celery@transfer.bs26 \n[2016-04-12 20:42:23,275: INFO/MainProcess] sync with celery@transfer.bs21 \n[2016-04-12 20:42:23,585: INFO/MainProcess] sync with celery@transfer.bs30 \n[2016-04-12 20:42:23,735: INFO/MainProcess] sync with celery@transfer.bs11 \n[2016-04-12 20:42:23,755: INFO/MainProcess] sync with celery@transfer.bs24 \n[2016-04-12 20:42:23,762: INFO/MainProcess] sync with celery@transfer.bs15 \n[2016-04-12 20:42:23,796: INFO/MainProcess] sync with celery@transfer.bs10 \n[2016-04-12 20:42:23,880: INFO/MainProcess] sync with celery@transfer.bs27 \n[2016-04-12 20:42:23,900: INFO/MainProcess] sync with celery@transfer.bs12 \n[2016-04-12 20:42:23,924: INFO/MainProcess] sync with celery@transfer.bs17 \n[2016-04-12 20:42:24,158: INFO/MainProcess] sync with celery@transfer.bs22 \n[2016-04-12 20:42:24,216: INFO/MainProcess] sync with celery@transfer.bs13 \n[2016-04-12 20:42:24,315: INFO/MainProcess] sync with celery@transfer.bs9 \n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3163/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "csaftoiu": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3150", "title": "Current working directory disappears from sys.path during worker bootstep", "body": "To reproduce:\n\n```\n# bootstep_path_bug.py\nfrom celery import Celery, bootsteps\n\napp = Celery()\n\n\nimport sys\nprint(sys.path)\n\nclass BootstepSomething(bootsteps.Step):\n    def __init__(self, worker, **options):\n        import sys\n        print(sys.path)\n\napp.steps['worker'].add(BootstepSomething)\n```\n\nThen:\n\n```\ncelery -A bootstep_path_bug worker\n```\n\nOutput:\n\n```\n['/Users/csaftoiu', '/usr/local/bin', ... snip other system module paths ... ]\n['/usr/local/bin', ... snip other system module paths ... ]\n```\n\nThis is surprising behavior, as if I import something in the root of the module, from my app, that same import won't work from within the bootstep.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3150/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "plaes": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3090", "title": "Docs on how to test celery tasks?", "body": "Currently Celery is missing testing section in its documentation.\n\nThere once was a Django-specific testing section, but this is now gone.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3090/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "fillest": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3085", "title": "WorkerLostError: Worker exited prematurely: exitcode 155", "body": "I have a host with a local-only (everything on the same host) Celery setup for some continuous data processing. Recently the load got much higher, all the CPU cores often are under 100% load and I've started to get `WorkerLostError: Worker exited prematurely: exitcode 155` out of the blue multiple times per day. A worker works and then suddenly\n\n```\nERROR/MainProcess] Task some_task[some_id] raised unexpected: WorkerLostError('Worker exited prematurely: exitcode 155.',)\nTraceback (most recent call last):\n  File \"...venv/local/lib/python2.7/site-packages/billiard/pool.py\", line 1175, in mark_as_worker_lost\n    human_status(exitcode)),\nWorkerLostError: Worker exited prematurely: exitcode 155.\n```\n\nI had Celery 3.1.19 but upgrading to the latest version changed nothing. Right now the versions are:\n\n```\ncelery==3.1.20\namqp==1.4.9\nbilliard==3.3.0.22\nkombu==3.0.33\n```\n\nI use Redis as the broker (tuned to save each message on the disk), and maybe these Celery settings will tell you something:\n\n```\nCELERY_ACKS_LATE = True\nCELERYD_MAX_TASKS_PER_CHILD = 1\nCELERYD_PREFETCH_MULTIPLIER = 1\n#-Ofair\n#CELERYD_CONCURRENCY is > 1\nBROKER_CONNECTION_TIMEOUT = 4\nBROKER_CONNECTION_MAX_RETRIES = 3\nBROKER_TRANSPORT_OPTIONS = {\n    'fanout_prefix': True,\n    'fanout_patterns': True,\n    'visibility_timeout': CELERYD_TASK_TIME_LIMIT,\n}\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3085/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "asmodehn": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3076", "title": "Revoking/Aborting tasks on worker shutdown", "body": "I am using 3.1.20 (Redis broker and backend) and I would like a way to Abort/Revoke the currently running tasks when the worker is being shutdown.\nThe point is to mark the tasks as FAILED if possible, and not rerun them next time the worker starts again.\n\nI am running one task at a time and since the task has side-effect ( and I cannot change that ), killing the worker would be the expected user behavior when something goes wrong, and I don't want the task to be rerun next time I start the worker (default sighandler behavior I believe...)\n\nI have tried http://stackoverflow.com/a/8230470 without success.\nAnd I also tried a few things using the control interface or the worker from a bootstep : \n\n``` python\n\nfrom celery import Celery, bootsteps\nfrom celery.task.control import revoke\n\n# TODO : configuration for tests...\nclass BootPyrosNode(bootsteps.StartStopStep):\n\n    def __init__(self, worker, **kwargs):\n        logging.warn('{0!r} is starting from {1}'.format(worker, __file__))\n\n        [...]\n\n    def create(self, worker):\n        return self\n\n    def start(self, worker):\n        # our step is started together with all other Worker/Consumer\n        # bootsteps.\n        pass  # not sure in which process this is run.\n\n    def stop(self, worker):\n        # the Consumer calls stop every time the consumer is restarted\n        # (i.e. connection is lost) and also at shutdown.  The Worker\n        # will call stop at shutdown only.\n        logging.warn('{0!r} is stopping. Attempting abort of current tasks...'.format(worker))\n        for req in worker.state.active_requests:\n            # worker.app.control.revoke(req.id, terminate=True) # not working\n            # revoke(req.id, terminate=True) # not working\n        self.node_proc.shutdown()\n\n```\n\ninstalled this way : \n\n``` python\n\nceleros_app = Celery()\n\n# setting up custom bootstep to start ROS node and pass ROS arguments to it\nceleros_app.steps['worker'].add(BootPyrosNode)\nceleros_app.user_options['worker'].add(Option('-R', '--ros-arg', action=\"append\", help='Arguments for ros initialisation'))\n```\n\nHowever it seems that my task cannot be revoked/aborted, ( maybe due to the worker not processing control messages after stopping ? ) and I am running out of ideas.\n\nIf you want to see more, the code comes from : https://github.com/asmodehn/celeros.\n\nIs there a way, or is this a customization that is not possible yet ?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3076/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/3050", "title": "celery worker no command line option for setting result backend", "body": "With celery 3.1.20 I get : \n\n```\n(celery)alexv@asmodehn:~/Projects$ celery worker --help\nUsage: celery worker [options] \n\nStart worker instance.\n\nExamples::\n\n    celery worker --app=proj -l info\n    celery worker -A proj -l info -Q hipri,lopri\n\n    celery worker -A proj --concurrency=4\n    celery worker -A proj --concurrency=1000 -P eventlet\n\n    celery worker --autoscale=10,0\n\nOptions:\n  -A APP, --app=APP     app instance to use (e.g. module.attr_name)\n  -b BROKER, --broker=BROKER\n                        url to broker.  default is 'amqp://guest@localhost//'\n  --loader=LOADER       name of custom loader class to use.\n  --config=CONFIG       Name of the configuration module\n  --workdir=WORKING_DIRECTORY\n                        Optional directory to change to after detaching.\n  -C, --no-color        \n  -q, --quiet           \n  -c CONCURRENCY, --concurrency=CONCURRENCY\n                        Number of child processes processing the queue. The\n                        default is the number of CPUs available on your\n                        system.\n  -P POOL_CLS, --pool=POOL_CLS\n                        Pool implementation: prefork (default), eventlet,\n                        gevent, solo or threads.\n  --purge, --discard    Purges all waiting tasks before the daemon is started.\n                        **WARNING**: This is unrecoverable, and the tasks will\n                        be deleted from the messaging server.\n  -l LOGLEVEL, --loglevel=LOGLEVEL\n                        Logging level, choose between DEBUG, INFO, WARNING,\n                        ERROR, CRITICAL, or FATAL.\n  -n HOSTNAME, --hostname=HOSTNAME\n                        Set custom hostname, e.g. 'w1.%h'. Expands: %h\n                        (hostname), %n (name) and %d, (domain).\n  -B, --beat            Also run the celery beat periodic task scheduler.\n                        Please note that there must only be one instance of\n                        this service.\n  -s SCHEDULE_FILENAME, --schedule=SCHEDULE_FILENAME\n                        Path to the schedule database if running with the -B\n                        option. Defaults to celerybeat-schedule. The extension\n                        \".db\" may be appended to the filename. Apply\n                        optimization profile.  Supported: default, fair\n  --scheduler=SCHEDULER_CLS\n                        Scheduler class to use. Default is\n                        celery.beat.PersistentScheduler\n  -S STATE_DB, --statedb=STATE_DB\n                        Path to the state database. The extension '.db' may be\n                        appended to the filename. Default: None\n  -E, --events          Send events that can be captured by monitors like\n                        celery events, celerymon, and others.\n  --time-limit=TASK_TIME_LIMIT\n                        Enables a hard time limit (in seconds int/float) for\n                        tasks.\n  --soft-time-limit=TASK_SOFT_TIME_LIMIT\n                        Enables a soft time limit (in seconds int/float) for\n                        tasks.\n  --maxtasksperchild=MAX_TASKS_PER_CHILD\n                        Maximum number of tasks a pool worker can execute\n                        before it's terminated and replaced by a new worker.\n  -Q QUEUES, --queues=QUEUES\n                        List of queues to enable for this worker, separated by\n                        comma. By default all configured queues are enabled.\n                        Example: -Q video,image\n  -X EXCLUDE_QUEUES, --exclude-queues=EXCLUDE_QUEUES\n  -I INCLUDE, --include=INCLUDE\n                        Comma separated list of additional modules to import.\n                        Example: -I foo.tasks,bar.tasks\n  --autoscale=AUTOSCALE\n                        Enable autoscaling by providing max_concurrency,\n                        min_concurrency. Example:: --autoscale=10,3 (always\n                        keep 3 processes, but grow to 10 if necessary)\n  --autoreload          Enable autoreloading.\n  --no-execv            Don't do execv after multiprocessing child fork.\n  --without-gossip      Do not subscribe to other workers events.\n  --without-mingle      Do not synchronize with other workers at startup.\n  --without-heartbeat   Do not send event heartbeats.\n  --heartbeat-interval=HEARTBEAT_INTERVAL\n                        Interval in seconds at which to send worker heartbeat\n  -O OPTIMIZATION       \n  -D, --detach          \n  -f LOGFILE, --logfile=LOGFILE\n                        Path to log file. If no logfile is specified, stderr\n                        is used.\n  --pidfile=PIDFILE     Optional file used to store the process pid. The\n                        program will not start if this file already exists and\n                        the pid is still alive.\n  --uid=UID             User id, or user name of the user to run as after\n                        detaching.\n  --gid=GID             Group id, or group name of the main group to change to\n                        after detaching.\n  --umask=UMASK         Effective umask (in octal) of the process after\n                        detaching.  Inherits the umask of the parent process\n                        by default.\n  --executable=EXECUTABLE\n                        Executable to use for the detached process.\n  --version             show program's version number and exit\n  -h, --help            show this help message and exit\n```\n\nI do not see how I can set the result database.\n\nIn config file I have : \n\n```\nBROKER_URL = 'redis://localhost:6379'\nCELERY_RESULT_BACKEND = BROKER_URL\n```\n\nThen starting the worker I can pass `--broker=redis://another-host:6379`\nBut I cannot find anyway to change the result backend to override the configuration setting...\n\nAm I missing something ?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3050/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/2843", "title": "Document Task State graphically in relation with settings", "body": "I was confused for a while with ACKS_LATE and how this would affect the behavior I was seeing related to prefetch_count value https://github.com/celery/celery/issues/2788 . And It seems I am not alone : http://stackoverflow.com/search?q=celery+prefetch\n\nSo I think a way to improve people understanding is via  a diagram showing the \"data flow\" of a task, relative to different settings.\n\nWhile getting familiar with celery at the beginning, I tend to think in terms of task and \"follow\" that task in very basic steps : \n- send from host1\n- received by host2\n- processed by host2\n- result returned to host1\n\nIt could be good to educate people reading the documentation by showing the different steps involved, relative to the settings. Something like : \n- send from host1 into Q1\n- if host2 listening to Q1 host2 receive it\n- if not ACKS_LATE, host2 acknowledge it and keep receiving <prefetch_count> other tasks.\n- processed by host2\n- if exception raised => return fail task\n- if return => return SUCCESS task.\n- if ACKS_LATE is true, host2 acknowledge and keep receiving <prefetch_count> other_tasks.\n  etc.\n\nA visual diagram (grafcet/dataflow style) would help a lot understanding how celery processes task, and how to get the settings right for a specific setup.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/2843/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rrauenza": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3065", "title": "docs: How does .../bin/celery find app's celery.py?", "body": "I'm a bit mystified at the magic that allows virtualenv/bin/celery find my proj/proj/celery.py file for bootstrapping my app.  Tracing through the code ... aha.  It looks like celery assumes there is an directory underneath the the current directory named after the app, and then it loads a celery module from there.   This means I always have to run celery from the top of my django directory.\n\nIs it worth explicitly documenting this (maybe in First Steps)?  I found it confusing, and now that I understand it, a lot of things are now clearer to me.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3065/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/2301", "title": "Enhancement request: Adding jitter to recurring tasks/schedule", "body": "I'd like to de-cluster some of my recurring tasks.  I'm hoping maybe this would be useful to others, too.\n\nIn the current implementation it seems like I can only do this by making my periods all slightly different or prime ...\n\nIt would be nice if something like 'relative' could also take an integer that offsets them from eachother.  Or just a flag that takes the given period and starts the task at a random offset into the period.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/2301/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stefanfoulis": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3027", "title": "Document best practices for using celery in other Addons", "body": "", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3027/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "fxfitz": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3025", "title": "Celery results are NOT obeying results serializer config in databases", "body": "So, we're working on getting all of our tasks results stored in a database backend (for now just postgres, but we're not tied to any particular backend).\n\nAnyway, we've noticed that no matter what we try to do, the results are **always** stored as a pickle object, even though we have the other configurations set properly:\n\n``` python\napp.conf.update(CELERY_ACCEPT_CONTENT=[\"json\"],\n                CELERY_TASK_SERIALIZER=\"json\",\n                CELERY_RESULT_SERIALIZER=\"json\")\n```\n\nWhile not being terribly familiar with SQL Alchemy, I do believe we have found the [offending line](https://github.com/celery/celery/blob/master/celery/backends/database/models.py#L33).\n\nAm I understanding this right? To me, it looks like this has been a bug for _years_... is it because we might be the only ones using a database backend? :-P\n\nThanks!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3025/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bowlofeggs": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3022", "title": "RFE: man pages for celery, celeryd, celerybeat, and celeryd-multi", "body": "It would be nice if celery, celeryd, celerybeat, and celeryd-multi had man pages. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3022/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chadrik": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/3021", "title": "group and chord: subtasks are not executed until entire generator is consumed", "body": "When passing a generator to `group` or `chord`, sub-tasks are not submitted to the broker for execution until the entire generator is consumed.  The expected result is that subtasks are submitted during iteration, as soon as they are visited.\n\nHere is the test code I'm using:\n\n``` python\nfrom celery import Celery, group\nfrom celery.utils.log import get_task_logger\n\napp = Celery(__name__, broker='redis://localhost:6379/0',\n             backend='redis://localhost:6379/0')\n\nlogger = get_task_logger(__name__)\n\ndef longyield():\n    import time\n    for i in xrange(5):\n        time.sleep(2)\n        print \"yielding\", i\n        yield add.s(i, i)\n\n@app.task\ndef add(x, y):\n    logger.info(\"adding {} + {}\".format(x, y))\n    return x + y\n\ndef itertest():\n    it = longyield()\n    g = group(it)\n    result = g.delay()\n    print result.get()\n```\n\nI'm testing with 4.0rc1.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/3021/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3043", "title": "group and chord: subtasks are not executed until entire generator is consumed", "body": "When passing a generator to `group` or `chord`, sub-tasks are not submitted to the backend for execution until the entire generator is consumed.  The expected result is that subtasks are submitted during iteration, as soon as they are yielded.  This can have a big impact on performance if generators yield subtasks over a long period of time.\n\nI also opened issue #3021 on the subject.  [This explanation](https://github.com/celery/celery/issues/3021#issuecomment-176729202) from @eli-green I think is pretty on point.\n\nSo far I've only added support for redis.  I started looking at the other backends but I ran out of time.  It would be great to get some feedback on what I have so far and to get some thoughts on how difficult it will be to add for the other backends.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shiraiyuki": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/2991", "title": "Celery: worker connection timed out", "body": "I'm using amqp (1.4.6) , celery (3.1.14) and kombu (3.0.22). Recently, I create a worker in the internet connecting with rabbitmq-server(broker). Sometimes, worker get a [errno110] connection timed out error. The following is the worker's log:\n\n```\n[2016-01-07 15:44:02,001: WARNING/MainProcess] Traceback (most recent call last):\n[2016-01-07 15:44:02,001: WARNING/MainProcess] File \"../lib/python2.7/site-packages/eventlet/hubs/poll.py\", line 115, in wait\n[2016-01-07 15:44:02,036: WARNING/MainProcess] listener.cb(fileno)\n[2016-01-07 15:44:02,036: WARNING/MainProcess] File \"../lib/python2.7/site-packages/celery/worker/pidbox.py\", line 112, in loop\n[2016-01-07 15:44:02,063: WARNING/MainProcess] connection.drain_events(timeout=1.0)\n[2016-01-07 15:44:02,063: WARNING/MainProcess] File \"../lib/python2.7/site-packages/kombu/connection.py\", line 275, in drain_events\n[2016-01-07 15:44:02,074: WARNING/MainProcess] return self.transport.drain_events(self.connection, **kwargs)\n[2016-01-07 15:44:02,075: WARNING/MainProcess] File \"../lib/python2.7/site-packages/kombu/transport/pyamqp.py\", line 91, in drain_events\n[2016-01-07 15:44:02,092: WARNING/MainProcess] return connection.drain_events(**kwargs)\n[2016-01-07 15:44:02,093: WARNING/MainProcess] File \"../lib/python2.7/site-packages/amqp/connection.py\", line 302, in drain_events\n[2016-01-07 15:44:02,107: WARNING/MainProcess] chanmap, None, timeout=timeout,\n[2016-01-07 15:44:02,107: WARNING/MainProcess] File \"../lib/python2.7/site-packages/amqp/connection.py\", line 365, in _wait_multiple\n[2016-01-07 15:44:02,107: WARNING/MainProcess] channel, method_sig, args, content = read_timeout(timeout)\n[2016-01-07 15:44:02,107: WARNING/MainProcess] File \"../lib/python2.7/site-packages/amqp/connection.py\", line 336, in read_timeout\n[2016-01-07 15:44:02,107: WARNING/MainProcess] return self.method_reader.read_method()\n[2016-01-07 15:44:02,107: WARNING/MainProcess] File \"../lib/python2.7/site-packages/amqp/method_framing.py\", line 189, in read_method\n[2016-01-07 15:44:02,108: WARNING/MainProcess] raise m\n[2016-01-07 15:44:02,108: WARNING/MainProcess] error: [Errno 110] Connection timed out\n[2016-01-07 15:44:02,108: WARNING/MainProcess] Removing descriptor: 6\n[2016-01-07 15:44:17,609: WARNING/MainProcess] consumer: Connection to broker lost. Trying to re-establish the connection...\napp.conf.update(\n    CELERY_IGNORE_RESULT=True,\n    CELERY_RESULT_BACKEND='rpc://',\n    CELERY_DISABLE_RATE_LIMITS=True,\n    # CELERY_DEFAULT_DELIVERY_MODE = True,\n    CELERY_TASK_SERIALIZER='json',\n    CELERY_RESULT_SERIALIZER='json',\n    CELERY_TIMEZONE='Asia/Taipei',\n    # if the heartbeat is 10.0 and the rate is the default 2.0, the check will be performed every 5 seconds\n    # BROKER_HEARTBEAT=10.0,\n    # BROKER_HEARTBEAT_CHECKRATE=2.0,\n    BROKER_CONNECTION_MAX_RETRIES=None,\n)\n```\n\nAfter this error message, when worker reconnect to broker, we get another error message:\nconsumer: `Cannot connect to amqp://guest:**@***:5672//: [Errno 104] Connection reset by peer`.\n\nThen after a long time, sometimes the worker can reconnect to broker. But when I assign some jobs to the worker, the worker can't run normally. The worker can get the jobs but doesn't return success to\nbroker.\n\nThere is my celery setting: \n\n```\napp.conf.update(\n    CELERY_IGNORE_RESULT=True,\n    CELERY_RESULT_BACKEND='rpc://',\n    CELERY_DISABLE_RATE_LIMITS=True,\n    # CELERY_DEFAULT_DELIVERY_MODE = True,\n    CELERY_TASK_SERIALIZER='json',\n    CELERY_RESULT_SERIALIZER='json',\n    CELERY_TIMEZONE='Asia/Taipei',\n    # if the heartbeat is 10.0 and the rate is the default 2.0, the check will be performed every 5 seconds\n    # BROKER_HEARTBEAT=10.0,\n    # BROKER_HEARTBEAT_CHECKRATE=2.0,\n    BROKER_CONNECTION_MAX_RETRIES=None,\n)\n```\n\nAre there any setting that I need to change?\nAnd if the timeout occur, there is a method that we can know the worker is abnormal and restart worker automaticly?\n\nThanks\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/2991/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hakanw": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/2912", "title": "crontab(hour=3) in CELERYBEAT_SCHEDULE is not Pythonic", "body": "I woke up to an avalanche of errors in my production environment after switching from\n\n`timedelta(hour=24)` for something running once every day, to: `crontab(hour=3)`\n\nIt turned out, of course, that it started a job _every minute_ at 3 am. I would argue that this is unexpected behavior from a Python standpoint, where you are used to default parameters set to zero, for example:\n\n`datetime.timedelta(hours=3)` is the same as `datetime.timedelta(hours=3, minutes=0, seconds=0)`. It is the same in other situations where you specify time in other parts of the stdlib.\n\nI know the reason for this is probably that it is inherited from Linux crontab behaviour, and that the default value there is \"*\", but I don't think it is very pythonic in this situation (given the argument and example above) and thus that the default value for the `minute` argument in `crontab` should be set to `0`.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/2912/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "djsmith42": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/2873", "title": "If a malformed message gets into Rabbit, Celery worker fails to start", "body": "If a publisher manages to publish a corrupted, non-UTF8 decodable message into Rabbit, the Celery worker will fail to start. I propose that it should instead leave the message unacknowledged, log the error, and continue working on other messages.\n\nHere's an example stack trace when this happens:\n\n```\n[2015-10-17 23:34:57,419: ERROR/MainProcess] Unrecoverable error: UnicodeDecodeError('utf8', 'qr=-1&qf=[date:between:2015-09-01,2015-09-Traceback (most recent call last):\n  File \"venv/local/lib/python2.7/site-packages/celery/worker/__init__.py\", line 206, in start\n    self.blueprint.start(self)\n  File \"venv/local/lib/python2.7/site-packages/celery/bootsteps.py\", line 123, in start\n    step.start(parent)\n  File \"venv/local/lib/python2.7/site-packages/celery/bootsteps.py\", line 374, in start\n    return self.obj.start()\n  File \"venv/local/lib/python2.7/site-packages/celery/worker/consumer.py\", line 278, in start\n    blueprint.start(self)\n  File \"venv/local/lib/python2.7/site-packages/celery/bootsteps.py\", line 123, in start\n    step.start(parent)\n  File \"venv/local/lib/python2.7/site-packages/celery/worker/consumer.py\", line 821, in start\n    c.loop(*c.loop_args())\n  File \"venv/local/lib/python2.7/site-packages/celery/worker/loops.py\", line 70, in asynloop\n    next(loop)\n  File \"venv/local/lib/python2.7/site-packages/kombu/async/hub.py\", line 272, in create_loop\n    item()\n  File \"venv/local/lib/python2.7/site-packages/amqp/utils.py\", line 42, in __call__\n    self.set_error_state(exc)\n  File \"venv/local/lib/python2.7/site-packages/amqp/utils.py\", line 39, in __call__\n    **dict(self.kwargs, **kwargs) if self.kwargs else kwargs\n  File \"venv/local/lib/python2.7/site-packages/kombu/transport/base.py\", line 144, in _read\n    drain_events(timeout=0)\n  File \"venv/local/lib/python2.7/site-packages/amqp/connection.py\", line 302, in drain_events\n    chanmap, None, timeout=timeout,\n  File \"venv/local/lib/python2.7/site-packages/amqp/connection.py\", line 365, in _wait_multiple\n    channel, method_sig, args, content = read_timeout(timeout)\n  File \"venv/local/lib/python2.7/site-packages/amqp/connection.py\", line 336, in read_timeout\n    return self.method_reader.read_method()\n  File \"venv/local/lib/python2.7/site-packages/amqp/method_framing.py\", line 186, in read_method\n    self._next_method()\n  File \"venv/local/lib/python2.7/site-packages/amqp/method_framing.py\", line 126, in _next_method\n    self._process_content_header(channel, payload)\n  File \"venv/local/lib/python2.7/site-packages/amqp/method_framing.py\", line 154, in _process_content_header\n    partial.add_header(payload)\n  File \"venv/local/lib/python2.7/site-packages/amqp/method_framing.py\", line 54, in add_header\n    self.msg._load_properties(payload[12:])\n  File \"venv/local/lib/python2.7/site-packages/amqp/serialization.py\", line 476, in _load_properties\n    d[key] = getattr(r, 'read_' + proptype)()\n  File \"venv/local/lib/python2.7/site-packages/amqp/serialization.py\", line 141, in read_table\n    val = table_data.read_item()\n  File \"venv/local/lib/python2.7/site-packages/amqp/serialization.py\", line 191, in read_item\n    val = self.read_table()  # recurse\n  File \"venv/local/lib/python2.7/site-packages/amqp/serialization.py\", line 141, in read_table\n    val = table_data.read_item()\n  File \"venv/local/lib/python2.7/site-packages/amqp/serialization.py\", line 150, in read_item\n    val = self.read_longstr()\n  File \"venv/local/lib/python2.7/site-packages/amqp/serialization.py\", line 131, in read_longstr\n    return self.input.read(slen).decode('utf-8')\n  File \"venv/lib/python2.7/encodings/utf_8.py\", line 16, in decode\n    return codecs.utf_8_decode(input, errors, True)\nUnicodeDecodeError: 'utf8' codec can't decode byte 0x96 in position 80: invalid start byte\n```\n\nVersions:\n\nCelery: 3.1.16\nKombu: 3.0.23\n\nUsing `pyamqp://`\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/2873/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "drabaioli": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/2852", "title": "FR: Integrate Redis Cluster as broker", "body": "The python Redis Cluster lib **Grokzen/redis-py-cluster** is considered production ready, therefore suitable for integrating Redis Cluster into Celery (in the future _redis-py-cluster_ will be integrated in _redis-py_, currently used by Celery).\n\nAs far as I can understand, Redis Cluster should be integrated in Kombu first (celery/kombu#526)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/2852/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "malinoff": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/2849", "title": "Remove Legacy Python (python2) support", "body": "Note that this will also remove `Jython` and `Pypy` support until they do not support python3.3+\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/2849/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/2122", "title": "Implement pattern and matcher arguments for remote control", "body": "Hi Ask,\n\nAs you said in IRC, this feature may be useful.\nExample:\n\n``` python\napp.control.ping(pattern=r'.*(frontend|db).*', matcher='pcre')\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/2122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/2125", "title": "Remote control functions as tasks", "body": "Refs #2121 \n\nThis is not ready for merge.\n", "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/2123", "title": "Added pattern/matcher arguments to control", "body": "Refs #2122\n", "author_association": "OWNER"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/12082291", "body": "@PMickael could you please explain what this commit does? You broke current master tests.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/12082291/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/12121596", "body": "@PMickael no, it fails because there is no `final_options` variable.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/12121596/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/13077594", "body": "I have added this new field to all backends. You can check this in the files below\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/13077594/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/22430124", "body": "No, you can't do that - because at this point you don't have any task _object_, only the string representation. And you may send a string representation of a task to a remote worker that has that specific task implementation, while the caller side may has no such task implementation at all.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/22430124/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/37781003", "body": "Please, avoid mutable class-level objects. IMO we don't need all these connection-related parameters on class level.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/37781003/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/37781252", "body": "Specify default value for `getattr` instead of catching `AttributeError`, same for the lines below\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/37781252/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/37781487", "body": "This check should be moved right after all needed parameters are evaluated to reduce unnecessary function calls.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/37781487/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/37781619", "body": "Consider using a template string and `format` method instead of concatenation.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/37781619/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/37781696", "body": "Move to the module level, also use `celery.five` instead of `six`\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/37781696/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/34226393", "body": "It is better to explicitly check `is None` when you want to check if it is None.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/34226393/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/34226401", "body": "Same as previous comment.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/34226401/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "dstufft": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/2833", "title": "Support Redis + TLS URLs", "body": "redis-py supports redis instances which are secured by TLS by using `rediss://`URLs instead of `redis://` URLs. Celery should support this too.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/2833/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "averwind": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/2748", "title": "Proposal to eliminate polling in chord_unlock with RabbitMQ", "body": "Right now the chord unlock task checks each task in group/header and than retries itself. What we thought about is that every member of the group will know the total number of tasks (N) in the group and a separate queue will be created holding indices from 1 to N in order. As each task finished it will pull an item from this queue if it is N it will call the callback, if it is not it will just remove the message it consumed from the queue. \n\nWanted to see what the community thinks about this approach before making a pull request.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/2748/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/2541", "title": "Countdown/ETA task behavior", "body": "Assume the following scenario where there is only on worker:\n- The prefetch is set to 1\n- There are some number of long running tasks queued with countdown set.\n- Since all of them are tasks with countdown the worker gets all the tasks as it is the only one.\n- Eventually worker starts working on one task\n- The other tasks cannot proceed as prefetch count is 1 even if countdown timer has expired for them\n- A new worker is started\n- Even though there is an available worker the tasks are all kept in the original worker.\n\nDoesn't it make more sense to re-queue these tasks if the local worker is not able to process them so that another can at least fetch the task?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/2541/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ghost": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/2727", "title": "raise Terminated(15, ) when terminating task on 3.1.18", "body": "[2015-07-27 12:52:38,386: ERROR/MainProcess] Task myapp.tasks.test [150a75dc-592e-4c0c-b686-1cc5aaf5a168] raised unexpected: Terminated(15,)\nTraceback (most recent call last):\n  File \"/home/blake/projects/venv/myapp/lib/python2.7/site-packages/billiard/pool.py\", line 1674, in _set_terminated\n    raise Terminated(-(signum or 0))\nTerminated: 15\n\npackages installed:\n\namqp==1.4.6\nanyjson==0.3.3\nbeautifulsoup4==4.4.0\nbilliard==3.3.0.20\ncelery==3.1.18\nDjango==1.7.7\ndjango-celery==3.1.16\ndjango-pipeline==1.5.2\nfutures==3.0.3\nkombu==3.0.25\npytz==2015.4\nredis==2.10.3\nrequests==2.7.0\nwheel==0.24.0\n\nI'm starting worker with:\n\ncelery -A myapp worker --loglevel=INFO --concurrency=10 \n\ni'm revoking with:\n\nfrom celery.task.control import revoke\nrevoke(task_id, terminate=True, signal='SIGKILL')\n\nis this the wrong approach?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/2727/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/214970", "body": "I've traced  this down to celery.worker.listener.receive_message, but  I'm still not into the celery code enough to figure how to register the failure properly. \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/214970/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "rudeb0t": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/2689", "title": "apply_async() hangs indefinitely when RabbitMQ server hits vm_memory_high_watermark and begins flow control", "body": "When RabbitMQ server hits the `vm_memory_high_watermark` setting (defaults at 0.4 or 40% of RAM) it begins throttling publishers as documented here: https://www.rabbitmq.com/memory.html When a task.apply_async() is called against this RabbitMQ server, it will hang indefinitely and if called in an WSGI application it will cause 504 gateway timeout errors if the WSGI application is proxied. The WSGI process **still** runs however and **will continue to run in the background** using up DB connections, RAM and CPU.\n\n**How to reproduce:**\n1. Set up RabbitMQ. Set `vm_memory_high_watermark` to 0 in the config file before starting the server or `rabbitmqctl set_vm_memory_high_watermark 0` from the command line.\n2. Create a small project with a simple async task. Call `apply_async()`.\n\nThe `apply_async()` call will hang. It is not unusual for the first call of `apply_async()` to hang, usually it submits. But then a second call will hang.\n\n**Expected Result:**\n\n`apply_async()` should honor `BROKER_CONNECTION_TIMEOUT` and raise an exception if the broker takes too long to accept and acknowledge the message.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/2689/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "andreif": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/2638", "title": "Broadcasting with Redis broker", "body": "Hi,\nI am looking into possibility to broadcast with Redis and wonder if anything has changed since Ask's answer [two years ago](https://groups.google.com/forum/#!searchin/celery-users/broadcast/celery-users/s4v_0ndiLT8/rJSME59TQ4YJ):\n\n> On Tuesday, May 28, 2013 at 5:03:19 PM UTC+2, Ask Solem wrote:\n> \n> > On May 27, 2013, at 5:04 PM, Maxime Verger wrote: \n> > \n> > Hi everybody, \n> > \n> > I'm trying to broadcast a task on every worker. Here is my configuration: \n> > \n> > celery==3.0.12 \n> > gevent==0.13.8 \n> > redis==2.7.2 \n> > \n> > CELERY_CREATE_MISSING_QUEUES = True \n> > CELERY_QUEUES = (Broadcast('broadcast_tasks'), ) \n> > CELERY_ROUTES = { \n> >     'tasks.reload_cache': {'queue': 'broadcast_tasks'}, \n> > } \n> > \n> > I'm launching celery workers using this command: \n> > \n> > ./manage.py celeryd_multi start celery1 celery2 celery3 priority export -Q:celery1 celery -Q:celery2 celery ..snip.. \n> > \n> > Moreover I saw that redis supports PUB / SUB mode for fanout. Maybe it's not working because of that, but I don't know how to configure it, if someone has a clue, I'd be happy ! \n> \n> I don't think the fanout exchange type works with Redis and Celery at this time. \n> The fanout exchanges must be consumed on from a different connection (to use LISTEN), and the worker does not currently do this (it only does so for the remote control command, \n> I guess fanout task queues could share the same connection as remote control). \n\nIs broadcasting with Redis theoretically possible? If yes, what needs to be done? If no, why?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/2638/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bmbouter": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/2492", "title": "worker_direct: Support for configurable auto_delete of queues.", "body": "Normal, default use of Celery dispatches tasks as durable. They survive broker restarts, and celery component restarts (workers, celerybeat). I expect the [CELERY_WORKER_DIRECT](http://celery.readthedocs.org/en/latest/configuration.html#celery-worker-direct) queues to carry a similar durability, but instead any messages that are present on a worker direct queue are lost when either the worker dies/restarts or the broker dies/restarts.\n\nTo reproduce:\n1) Dispatch a task onto a dedicated queue for a worker, and have it halt when it runs so the task never completes.\n2) Dispatch a second task onto that same dedicated queue. Have it stay on the queue and not be received by the worker by starting it with `-c 1`.\n3) Use a broker specific tool to list the number of messages in each queue.\n4) Send a SIGKILL to the celery worker so that all celery processes are stopped\n5) Verify the worker was stopped, and then start the worker again\n6) List the number of messages in the queues and observe that the message that was in the dedicated queue is now gone\n\nWhat type of workarounds can be done to avoid this behavior? What options are there to fix this in Celery?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/2492/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/2491", "title": "Channel errors recover through reconnection which is unecessary", "body": "Celery 3.1 uses Kombu 3.0. Kombu 3.0 defines two error classes for errors raised by the transport.\n\n`recoverable_connection_errors`\n`recoverable_channel_errors`\n\nThe Kombu 3.0 interface is not very well documented, but there is [this post](http://comments.gmane.org/gmane.comp.python.amqp.celery.user/4854) which describes it some. There is also the [attribute docs here](http://kombu.readthedocs.org/en/3.0/reference/kombu.connection.html?highlight=recoverable_channel_errors#kombu.connection.Connection.recoverable_channel_errors).\n\nFrom my testing it seems that Celery does not use the newer 3.0 style error classes. It should be safe to start using them immediately because Kombu falls back to the 2.0 style if a transport doesn't support the new style. Celery seems to use the old style `connection_error` and `channel_error`. Celery not using the new style causes channel errors to completely interrupt the connection which is not very efficient considering the Kombu 3.0 style is available as a requirement of Celery 3.1.\n\nAnother alternative would be to have Celery rely on Kombu for retry behaviors entirely since it does seem to support automatic retry through the use of decorators. I'm not as sure of how to do this though.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/2491/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/2216", "title": "Messages dispatched to a dedicated queue may be rejected by the broker", "body": "I am using the Qpid broker, but this should be reproducible with RabbitMQ.\n\n1) Start a single celeryd worker with a single worker process, and configure that worker to use `CELERY_WORKER_DIRECT = True`\n2) Start an interactive python shell in a terminal and import a task that the worker knows about\n3) Use apply async to dispatch specifically into that queue (ie: 'foo').  `my_task.apply_async((param_one,), queue=foo)`\n4) Observe the task runs (as expected)\n5) Restart the worker\n6) Dispatch another task (redo step 3)\n7) Observe that the task from step 6 is never run\n\nThe Celery router that is being used in the terminal performs a bind of the queue 'foo' to the exchange 'foo' with the bind key 'foo'. This appears to be the responsibility of the sender, and is only done once. The binding is deleted when the worker restarts because the default behavior of dedicated queue is to auto-delete the queue when the worker stops, and create it again when the worker starts. Step 5 above causes the queue to be recreated, and the expectation of the dispatcher that the queue 'foo' is still bound to exchange 'foo' with bind key 'foo'.\n\nThere are several possible ways to resolve this:\n1. I expected apply_async with queue=foo specified to not need to dispatch on an exchange at all, and instead dispatch onto a queue by name without specifying an exchange. This should cause kombu to do the right thing, and it would end up in the queue!\n2. We could use the [`CELERY_ROUTES`](http://celery.readthedocs.org/en/latest/configuration.html#celery-worker-direct) configuration to specify that our tasks need to be dispatched on to the C.dq exchange. I would really prefer not to do this because we have a lot of tasks, and we write new ones, and delete old ones.\n3. We could use apply_async with the `exchange='C.dq'` parameter and queue, but [the docs](http://celery.readthedocs.org/en/latest/reference/celery.app.task.html#celery.app.task.Task.apply_async) say the exchange parameter is typically not used with queue specified. Also our different tasks use dedicated queue, but many don't so this would be difficult for similar reasons as option 2.\n4. The dedicated queue could gratutiously create the bind of queue 'foo' to exchange 'foo' with binding key 'foo' upon startup. If the clients additionally do this, it should have no effect on the broker.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/2216/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/132745726", "body": "@daviddavis Can a note be added here that identifies that `weak` is set to False whenever `retry == True` regardless of the value passed in to `connect()`.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132745726/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "kerwin": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/1495", "title": "Failover for celerybeat", "body": "I want to make schedule process run in multiple nodes to improve availability, for reducing single point issue.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/1495/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "diranged": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/1452", "title": "Feature: Add a 'suspend' and 'resume' command to Celery?", "body": "It would be really useful to be able to issue a 'suspend' command to all of the celery workers in a given queue (or globally) via the broker, and then a 'resume' command. This would help out quite a bit with maintenances or other work that requires that celery jobs cease.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/1452/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "davemt": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/893", "title": "Feature: Canvas: Reuse evaluated signatures", "body": "Consider the following (where A, B, C, D, E are tasks):\n\n```\nsub_A = A.s()\nchord_D = chord([sub_A, B.s()], D.s())\nchord_E = chord([sub_A, C.s()], E.s())\nres = group(chord_D, chord_E)()\n```\n\nThis creates two separate invocations for task A.  For many cases, we would want the subtask for A to be invoked just once, and the work shared.\n\nIt seems that manually giving the subtask a uuid leads to the desired outcome, but it would be nice to have support for this in the API.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/893/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "regisb": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/fd6e4758260612cbd03dc007010be5699208ca2f", "message": "Refer worker request info to absolute time (#3684)\n\n* Add R\u00e9gis Behmo to the list of contributors\r\n\r\n* Report absolute time on inspect"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Chris7": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/ba2dec7956782c84068ef779e554fb07de524beb", "message": "Propagate arguments to chains inside groups (#4481)\n\n* Remove self._frozen from _chain run method\r\n\r\n* Add in explicit test for group results in chain"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4482", "title": "Add docker-compose and base dockerfile for development", "body": "This adds a docker based development environment. This removes the need for users to install their own rabbit/redis/virtual environment/etc. to begin development of celery. I use this myself (since after moving to docker I have essentially nothing installed on my computer anymore) and saw interest in it from issue #4334 so thought a PR may be appropriate to share my setup.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zpl": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/442f42b7084ff03cb730ca4f452c3a47d9b8d701", "message": "task_replace chord inside chord fix (fixes #4368) (#4369)\n\n* task_replace chord inside chord fix\r\n\r\n* Complete fix for replace inside chords with tests\r\n\r\n* Add integration tests for add_to_chord\r\n\r\n* Fix JSON serialisation in tests\r\n\r\n* Raise exception when replacing signature has a chord"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4302", "title": "Ignore celery.exception.Ignore on autoretry", "body": "Autoretry for task should ignore celery.exception.Ignore, which is generated by self.replace()\r\n\r\notherwise, it goes to infinite loop.\r\n\r\n```python\r\n@app.task(autoretry_for=(Exception,), default_retry_delay=1,\r\n          max_retries=None,\r\n          bind=True,acks_late=True)\r\ndef TaskA(self):\r\n    raise self.replace(TaskB.s()) # Always retrying because of replace\r\n\r\n```\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jdufresne": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/5eba340aae2e994091afb7a0ed7839e7d944ee13", "message": "Pass python_requires argument to setuptools (#4479)\n\nHelps pip decide what version of the library to install.\r\n\r\nhttps://packaging.python.org/tutorials/distributing-packages/#python-requires\r\n\r\n> If your project only runs on certain Python versions, setting the\r\n> python_requires argument to the appropriate PEP 440 version specifier\r\n> string will prevent pip from installing the project on other Python\r\n> versions.\r\n\r\nhttps://setuptools.readthedocs.io/en/latest/setuptools.html#new-and-changed-setup-keywords\r\n\r\n> python_requires\r\n>\r\n> A string corresponding to a version specifier (as defined in PEP 440)\r\n> for the Python version, used to specify the Requires-Python defined in\r\n> PEP 345."}, {"url": "https://api.github.com/repos/celery/celery/commits/e962a7250761ca8bb8191c8c16c73be5683fb578", "message": "Include license file in the generated wheel package (#4404)\n\nThe wheel package format supports including the license file. This is\r\ndone using the [metadata] section in the setup.cfg file. For additional\r\ninformation on this feature, see:\r\n\r\nhttps://wheel.readthedocs.io/en/stable/index.html#including-the-license-in-the-generated-wheel-file\r\n\r\nDistributing a wheel now complies with license:\r\n\r\n> Redistributions in binary form must reproduce the above copyright\r\n> notice, this list of conditions and the following disclaimer in the\r\n> documentation and/or other materials provided with the distribution."}, {"url": "https://api.github.com/repos/celery/celery/commits/897f2a7270308e0d60f13d895ba49158dae8105c", "message": "Rename section to [bdist_wheel] as [wheel] is considered legacy (#4033)\n\nSee:\r\n\r\nhttps://bitbucket.org/pypa/wheel/src/54ddbcc9cec25e1f4d111a142b8bfaa163130a61/wheel/bdist_wheel.py?fileviewer=file-view-default#bdist_wheel.py-119:125\r\n\r\nhttp://pythonwheels.com/"}, {"url": "https://api.github.com/repos/celery/celery/commits/d90caee6d91a0fcc91756329503a35bf8fef720a", "message": "Prefer https over http when available (#3966)"}, {"url": "https://api.github.com/repos/celery/celery/commits/ac88523956469e365e99e8a06398d40fa08d4e88", "message": "Document support for Python 3.6 (#3904)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "freakboy3742": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/a4abe149aa00b0f85024a6cac64fd984cb2d0a6b", "message": "Refs #4356: Handle \"hybrid\" messages that have moved between Celery versions (#4358)\n\n* handle \"hybrid\" messages which have passed through a protocol 1 and protocol 2 consumer in its life.\r\n\r\nwe detected an edgecase which is proofed out in https://gist.github.com/ewdurbin/ddf4b0f0c0a4b190251a4a23859dd13c#file-readme-md which mishandles messages which have been retried by a 3.1.25, then a 4.1.0, then again by a 3.1.25 consumer. as an extension, this patch handles the \"next\" iteration of these mutant payloads.\r\n\r\n* explicitly construct proto2 from \"hybrid\" messages\r\n\r\n* remove unused kwarg\r\n\r\n* fix pydocstyle check\r\n\r\n* flake8 fixes\r\n\r\n* correct fix for misread pydocstyle error"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "djw": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/9ce3df9962830a6f9e0e68005bdeec1092e314e4", "message": "Corrected the default visibility timeout (#4476)\n\nAccording to kombu, the default visibility timeout is 30 minutes.\r\n\r\nhttps://github.com/celery/kombu/blob/3a7cdb07c9bf75b54282274d711af15ca6ad5d9f/kombu/transport/SQS.py#L85"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alexgarel": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/0ffd36fbf9343fe2f6ef7744a14ebfbec5ac86b6", "message": "request on_timeout now ignores soft time limit exception (fixes #4412) (#4473)\n\n* request on_timeout now ignores soft time limit exception (closes #4412)\r\n\r\n* fix quality"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pachewise": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/c8f9b7fbab3fe8a8de5cbae388fca4edf54bf503", "message": "Fixes #4452 - Clearer Django settings documentation (#4467)\n\n* reword django settings section in first steps\r\n\r\n* anchor link for django admonition\r\n\r\n* mention django-specific settings config"}, {"url": "https://api.github.com/repos/celery/celery/commits/48de89df0c76e2961c3c27341b650dcd3a074be9", "message": "Fixes #4106 - change GroupResult:as_tuple() to include parent (#4205)\n\n* include parent in GroupResult:to_tuple()\r\n\r\n* unit test for #4106\r\n\r\n* check for list/tuple before unpacking id, parent\r\n\r\n* flake8 issues\r\n\r\n* wrong method call in unit test\r\n\r\n* assert results equal in test_GroupResult_with_parent\r\n\r\n* GroupResult.__eq__ checks parents, del redundant parent=None\r\n\r\n* added test for GroupResult _eq_\r\n\r\n* GroupResult_as_tuple unit test"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/133633137", "body": "this is causing the code-quality check to fail... basically, it's asking you to pass in the % arguments instead of calling `str.format`.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133633137/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133524053", "body": "taskID of the parent? or the second_result?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133524053/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133524681", "body": "will double-check and get back to you later.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133524681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133610185", "body": "GroupResult's == is implemented as:\r\n\r\n````python\r\n    def __eq__(self, other):\r\n        if isinstance(other, GroupResult):\r\n            return other.id == self.id and other.results == self.results\r\n        return NotImplemented\r\n````\r\n\r\nWondering if we should include the parents in the comparisons, too?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133610185/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133611490", "body": "<img width=\"908\" alt=\"screen shot 2017-08-16 at 7 31 50 pm\" src=\"https://user-images.githubusercontent.com/2257119/29393615-a6879e90-82b9-11e7-8e6f-5afbddf3ee06.png\">\r\n\r\nfirst one and last four might need to be changed.\r\n\r\nEDIT: looked deeper at the uses:\r\n\r\n(1) _unlock_chord\r\n(2) BaseKeyValueStoreBackend:_apply_chord_incr\r\n(3) group:apply\r\n(4) group:apply_async\r\n(5) group:freeze\r\n\r\ndoesn't really look like these would have a parent value set there, as those are usually set by the chaining logic.  ", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133611490/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133625441", "body": "ResultBase already defines `parent = None`. Wondering if we should get rid of L818/L828.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133625441/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "hclihn": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/3ca1a54e65762ccd61ce728b3e3dfcb622fc0c90", "message": "Allow the shadow kwarg and the shadow_name method to set shadow properly (#4381)\n\n* Allow the shadow kwarg and the shadow_name method to set shadow properly \r\n\r\nThe shadow_name option in the @app.task() decorator (which overrides the shadow_name method in the Task class) and the shadow keyword argument of Task.apply_async() don't work as advertised.\r\nThis moves the shadow=... out of the 'if self.__self__ is not None:' block and allows shadow to be set by the shadow keyword argument of Task.apply_async() or the shadow_name method in the Task class (via, say, the shadow_name option in the @app.task() decorator).\r\n\r\n* Added a test to cover calling shadow_name().\r\n\r\n* Sort imports.\r\n\r\n* Fix missing import."}, {"url": "https://api.github.com/repos/celery/celery/commits/f51204f13a6efafd746ad4f61d0ec8ce4229b355", "message": "Add test coverage for get_task_name() in PR #4379 (#4397)\n\n* Add test coverage for get_task_name() in PR #4379 \r\n\r\nAdd test coverage for get_task_name() in PR #4379\r\n\r\n* Added import for the function to be tested."}, {"url": "https://api.github.com/repos/celery/celery/commits/11a287ceb37663f90630cdcd467d23d8b9d9e215", "message": "Allow shadow to override task name in trace and logging messages. (#4379)\n\n* Allow shadow to override task name in trace and logging messages.\r\n\r\nThe single-line commit in celery/app/amqp.py from PR#4350 did not fix the issue entirely as it was pointed out in the comment of the PR. To fix this issue in the \"received\" log entries, you need to fix a few places in celery/celery/app/trace.py which will fix the \"received\" log/trace entries.\r\n\r\n* Use a common function to simplify task name usage.\r\n\r\nUse a common function to simplify task name usage for both `TraceInfo` and  `trace_task`. The function will use `shadow` in request for task name if applicable.\r\n\r\n* Modified get_task_name()\r\n\r\nModified get_task_name() to use default (instead of name) per suggestion.\r\n\r\n* Updated get_task_name()\r\n\r\nUpdated get_task_name() to make sure default is returned if request.shadow is None or an empty string."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "AlexHill": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/fde58ad677a1e28effd1ac13f1f08f7132392463", "message": "Run chord_unlock on same queue as chord body - fixes #4337 (#4448)"}, {"url": "https://api.github.com/repos/celery/celery/commits/25f5e29610b2224122cf10d5252de92b4efe3e81", "message": "Support chords with empty headers (#4443)"}, {"url": "https://api.github.com/repos/celery/celery/commits/7ef809f41c1e0db2f6813c9c3a66553ca83c0c69", "message": "Add bandit baseline file with contents this time"}, {"url": "https://api.github.com/repos/celery/celery/commits/fdf0928b9b5698622c3b8806e2bca2d134df7fa3", "message": "Add bandit baseline file"}, {"url": "https://api.github.com/repos/celery/celery/commits/10f06ea1df75f109bf08fb8d42f9977cabcd7e0e", "message": "Fix length-1 and nested chords (#4393 #4055 #3885 #3597 #3574 #3323) (#4437)\n\n* Don't convert single-task chord to chain\r\n\r\n* Fix evaluation of nested chords"}, {"url": "https://api.github.com/repos/celery/celery/commits/973bb1875f24e33d08ecbc0abd07b477ab7e7693", "message": "Fix current_app fallback in GroupResult.restore() (#4431)"}, {"url": "https://api.github.com/repos/celery/celery/commits/b8550a050ae165e4909540207e4efac34f830d17", "message": "Correctly restore an empty GroupResult (#2202) (#4427)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pokoli": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/63c747889640bdea7753e83373a3a3e0dffc4bd9", "message": "Add celery_tryton integration on framework list (#4446)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "azaitsev": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/83872030b00a1ac75597ed3fc0ed34d9f664c6c1", "message": "Fixed wrong value in example of celery chain (#4444)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "matteius": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/976515108a4357397a3821332e944bb85550dfa2", "message": "make astimezone call in localize more safe (#4324)\n\nmake astimezone call in localize more safe; with tests"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "myw": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/4dc8c001d063a448d598f4bbc94056812cf15fc8", "message": "Add Mikhail Wolfson to CONTRIBUTORS.txt (#4439)"}, {"url": "https://api.github.com/repos/celery/celery/commits/dd2cdd9c4f8688f965d7b5658fa4956d083a7b8b", "message": "Resolve TypeError on `.get` from nested groups (#4432)\n\n* Accept and pass along the `on_interval` in ResultSet.get\r\n\r\nOtherwise, calls to .get or .join on ResultSets fail on nested groups.\r\nFixes #4274\r\n\r\n* Add a unit test that verifies the fixed behavior\r\n\r\nVerified that the unit test fails on master, but passes on the patched version. The\r\nnested structure of results was borrowed from #4274\r\n\r\n* Wrap long lines\r\n\r\n* Add integration test for #4274 use case\r\n\r\n* Switch to a simpler, group-only-based integration test\r\n\r\n* Flatten expected integration test result\r\n\r\n* Added back testcase from #4274 and skip it if the backend under test does not support native joins.\r\n\r\n* Fix lint.\r\n\r\n* Enable only if chords are allowed.\r\n\r\n* Fix access to message."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wido": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/ebb99a183872e60531e338584411b01c4787e8e0", "message": "[backends]: Consul: Do not encode, but decode a byte() object (#4416)\n\nWe want a string and it's already a byte() code array, so we should\r\ndecode it instead of encoding it.\r\n\r\nWhile doing so fix another case where PY3 was used on the same file\r\n\r\nTo do so use kombu.utils.encoding.bytes_to_str and add a test for this\r\n\r\nSigned-off-by: Wido den Hollander <wido@widodh.nl>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lpsinger": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/04f29ff7250b557ec6974adf828f151b41fb9609", "message": "Allow automatically documenting task decorated objects using Sphinx (e.g. with automodule directive) (#4422)\n\n* Allow automatically documenting task decorated objects using Sphinx (e.g. with automodule directive).\r\n\r\n* Fix docstyle error\r\n\r\nFix this error message:\r\n        D204: 1 blank line required after class docstring (found 0)\r\n\r\n* Fix docstyle error\r\n\r\nThis docstring is more of a comment anyway."}, {"url": "https://api.github.com/repos/celery/celery/commits/f132d3b03b856ca3cc93631d98bdd5d91bff7724", "message": "Strip `self` from sphinx documentation of bound tasks (#4421)\n\nIn Sphinx-generated documentation, the `self` argument should be\r\nhidden from the signature of a task that was created with\r\n`bind=True`.\r\n\r\n(Sphinx does normally strip the `self` or `cls` argument from\r\nmethod signatures.)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "haos616": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/d02d260a192f8923dfa331ae2b503af57534654a", "message": "Add check task-synchronous-subtasks for task_always_eager mode (#4322)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vbarbaresi": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/89b0b46632656a4fbd0fb0772cc1a358b8f50cc6", "message": "Replace start timezone on remaining() estimation if timezone changed (#1604) (#4403)\n\nWhen computing the remaining time after DST change, last_run_at time\r\nwas localized in the previous time zone (before DST)\r\nThe resulting end date was then also localized in the previous time zone\r\nIn case UTC offets are different, Replace tzinfo with\r\nthe current one"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jmartinm": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/d7d4c2a40ee4f482ae043f92e2a5391d395027a7", "message": "Fix getfullargspec Python 2.x compatibility in contrib/sphinx.py (#4399)\n\n* In Python 2.x, when documenting tasks with autotask an error was\r\n  being thrown since the named tuple passed to formatargspec was\r\n  was meant for Python 3 (fixes #3993)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mariia-zelenova": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/1f7b2aecc9da7c073accb282c98f463c68f14a39", "message": "Fix equal schedules (#4312)\n\n* Fix equal schedules\r\n\r\n* review fixes\r\n\r\n* Fix tests\r\n\r\n* Use tuple instead of a list."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mikolevy": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/e5cf4ba25f2b76000f8372cedf9bd0e66f80fa6c", "message": "Update task-cookbook lock code example (#4344)\n\nAdd checking did current worker/thread acquire the lock before releasing it, to avoid the situation when one worker release lock owned by someone else"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "anentropic": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/1bb747a627ca443ce8ec6a0dfea584de5baa1a9f", "message": "comments applied to opposite code block (#4364)\n\nfrom my reading of the code, these comments were the wrong way round"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jairojair": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/1d8b64ef0d094287a0ed7934f77aa93574913760", "message": "fix(typo): Fix typo in documentation (#4365)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "palewire": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/243cc454503a26d82c611bccdd2e5204b7443922", "message": "Typo in the example systemd service"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jmdacruz": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/2d2a1450955b955d174cac6ce520586c226eb664", "message": "Adding 'shadow' property to as_task_v2"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "michaeljpeake": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/c57077f03fcc57039fe55338e3e55f6d7861bcd8", "message": "Update next-steps.rst (#4332)\n\nTypo fixed."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rixx": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/3130a00ee52ac37df2f972a63475c10731b70c47", "message": "Try to import directly, do not use deprecated imp method (#4216)\n\nDetails are discussed in #2523 \u2013 `imp.find_module` is deprecated and\r\nleads to issues in some places where `importlib.import_module` works\r\nperfectly."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "geoffreybauduin": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/7d52b0bddfcac369acc750c532c2bb08012721a5", "message": "Adding redis sentinel backend (#4144)\n\n* Adding redis sentinel backend\r\n\r\nSigned-off-by: Geoffrey Bauduin <geoffrey.bauduin@corp.ovh.com>\r\n\r\n* Addressed flake8 issues\r\n\r\nSigned-off-by: Geoffrey Bauduin <geoffrey.bauduin@corp.ovh.com>\r\n\r\n* Updating pydoc style to match requirements\r\n\r\nSigned-off-by: Geoffrey Bauduin <geoffrey.bauduin@corp.ovh.com>\r\n\r\n* Fixing unit tests on Appveyor by setting up a fake sentinel module inside the SentinelBackend class\r\n\r\nSigned-off-by: Geoffrey Bauduin <geoffrey.bauduin@corp.ovh.com>\r\n\r\n* Redefining the fake Sentinel class to use correct parameters when calling 'master_for'\r\n\r\nSigned-off-by: Geoffrey Bauduin <geoffrey.bauduin@corp.ovh.com>\r\n\r\n* Updated configuration name, 'result_backend_transpoirt_options'\r\n\r\nSigned-off-by: Geoffrey Bauduin <geoffrey.bauduin@corp.ovh.com>\r\n\r\n* Added documentation for the feature\r\n\r\nSigned-off-by: Geoffrey Bauduin <geoffrey.bauduin@corp.ovh.com>\r\n\r\n* Make sure the database/password parameters are forwarded to the StrictRedis class while instantiating a Sentinel\r\n\r\nSigned-off-by: Geoffrey Bauduin <geoffrey.bauduin@corp.ovh.com>\r\n\r\n* Database/password parameters should be sent as connection_kwargs to the Sentinel class, to be used when connection to redis is created\r\n\r\nSigned-off-by: Geoffrey Bauduin <geoffrey.bauduin@corp.ovh.com>\r\n\r\n* Fixing flake8 issues\r\n\r\nSigned-off-by: Geoffrey Bauduin <geoffrey.bauduin@corp.ovh.com>\r\n\r\n* Addressing configcheck issues\r\n\r\nSigned-off-by: Geoffrey Bauduin <geoffrey.bauduin@corp.ovh.com>\r\n\r\n* Add docstring to SentinelBackend.\r\n\r\n* Fix flake8 error."}, {"url": "https://api.github.com/repos/celery/celery/commits/afebe7a6e0e4320b87d6a73e8514d206d7ccf564", "message": "doc: adding examples to connect to sentinel broker (#4143)\n\nSigned-off-by: Geoffrey Bauduin <geoffrey.bauduin@corp.ovh.com>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "caronc": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/2f3c5aa5e8903eb8a7382953be8233db3575d31d", "message": "LimitedSets support on faster systems refs #3879 and #3891 (#3892)\n\n* LimitedSets support on faster systems refs #3879 and #3891\r\n\r\n* corrected reference (ported from #3879)\r\n\r\n* handle update() calls where time is specified\r\n\r\n* added monotonic() replacement to time() in LimitSets; refs #3891\r\n\r\n* using monotonic() from celery.vine; refs #3891\r\n\r\n* oops forgot to remove some old references to code; sorry\r\n\r\n* references to time in LimitSet tests updated\r\n\r\n* removed unnessisary inline comments"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/132817955", "body": "Couldn't agree more! \ud83d\ude04 - standby", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132817955/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132845530", "body": "Almost seems like this test isn't necessary at all any more at all.  You can't get any closer to the real time other than monotonic().  I could sub in the **s.add(i, now=next(clock))** to just **s.add(i, now=monotonic())** ([reference](https://github.com/celery/celery/blob/2394e738a7d0c0c736f5d689de4d32325ba54f48/t/unit/utils/test_collections.py#L298-L300)), but that would just be re-iterating what is under the hood anyway.\r\n\r\nOr are you suggesting that if we detect two (specified) times that are exactly the same, we add **1e9** to it (instead of **1e6**)? It would be a bit like merging the original pull request with this one.  This test function you're referencing here could then pass in **time()**? Might be overkill at this point though; thoughts?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132845530/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/143339490", "body": "done!", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/143339490/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "rachiebytes": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/70d3e9692bdd812a8aaf778dea76644b23a77fd4", "message": "Remove extra word 'or'\n\nThis removes the extra 'or' on line 187."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Schweigi": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/c086cfd27b9ec5d45e74dbcc6fd0d70926c3410a", "message": "Use Django DB max age connection setting (fixes #4116) (#4292)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chenfengyuan": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/be55de622381816d087993f1c7f9afcf7f44ab33", "message": "fix wrong configruation name CELERY_TASK_ACKS_LATE (#4291)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "atombrella": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/2c4f5149cbdddf0107089cc10a9e1ce692a31ad3", "message": "Merge content of contributing.rst using include-directive. (#4272)\n\nThis fixes Github issue #4218"}, {"url": "https://api.github.com/repos/celery/celery/commits/fcec01f6e041a70e5ddd061beba5fccb32d74e24", "message": "Deleted trailing whitespace in different files (#4219)"}, {"url": "https://api.github.com/repos/celery/celery/commits/39d86e7dbe7f3a0a43e973910be880146de96fb7", "message": "Prefer dict literal syntax over dict() (#4217)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "scttcper": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/06c6cfefb5948286e5c634cfc5b575dafe9dc98d", "message": "add cassandra_options (#4224)\n\n* add cassandra_options\r\n\r\n* fix misspelled `cassandra_options`\r\n\r\n* add cassandra_options to defaults\r\n\r\n* cassandra options match auth_kwargs"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "arpanshah29": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/a20e7922b3641d03110df2b770b7480c3f32b42c", "message": "Changing eta documentation (#4264)"}, {"url": "https://api.github.com/repos/celery/celery/commits/fd427f2f5183ad4659e0cc45cfa11c8cc034ffff", "message": "Add to contributers and authors (#4263)"}, {"url": "https://api.github.com/repos/celery/celery/commits/ca962fa72f943c706f626db2465eeeca66f48ea3", "message": "Add new function to handle etas and limits together (#4251)\n\n* Add new function to handle etas and limits together\r\n\r\n* Adding unit test\r\n\r\n* Fixing indentation"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "argsno": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/9ca61fa13c3587e79cad1f6784b5263b70b0faae", "message": "Add myself to the AUTHORS (#4254)"}, {"url": "https://api.github.com/repos/celery/celery/commits/3da5eb95e564bbf4e49805aae14d7f85791359b0", "message": "fix typo in --pidfile examples. (#4253)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gjedeer": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/2f422c52bdd3dece65f05c1e7015b335e8bd0175", "message": "Fix link in documentation\n\n`~@AsyncResult` results in `mailto:~@AsyncResult` link in the doc instead of a proper link to AsyncResult class\r\n\r\nhttp://docs.celeryproject.org/en/master/reference/celery.app.task.html?highlight=retry#celery.app.task.Task.apply_async"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jackieleng": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/6b6117faa2c733e400f68debd87d06dc73a3d47b", "message": "Fix typo in retry docstring."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "morenoh149": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/10adb99538b23621418a69d674c1c01de267e045", "message": "Update installation.txt\n\nfix grammar and link to Contributing section"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mkai": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/e3dd0ff0766ce07a680cdaaf12becc50cfcc2b5d", "message": "Added _default_now backwards compatibility alias"}, {"url": "https://api.github.com/repos/celery/celery/commits/4f77d42f813cdfe3de7efd9b47d467764405064a", "message": "Made ScheduleEntry._default_now method public"}, {"url": "https://api.github.com/repos/celery/celery/commits/c816114e3b7d7731a860f6e03bc5e4d6c0d77fe9", "message": "Update CONTRIBUTORS.txt"}, {"url": "https://api.github.com/repos/celery/celery/commits/27a686fd3c2ded4f8d0eeefb3c725b8d775eb70c", "message": "Beat: fixed entry._default_now() not used everywhere\n\nI'm using custom scheduler entries with an overridden ``is_due()`` method that do not require a schedule (``entry.schedule``) to be set. Judging from the ``entry._default_now()`` method, this seems to be supported by the scheduler in Celery 4.1.0, however that method is not used everywhere, leading to an AttributeError in my use case.\r\n\r\nThis change just uses ``_default_now()`` everywhere which fixes the issue. It shouldn't have any impact on the default implementation."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/136165750", "body": "Agreed. But a public method or getter would just duplicate (or call) ``_default_now``. So how about just making that public, i.e. merely removing the underscore and slightly refactoring?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/136165750/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "knaperek": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/5f2141af2edfb70333763476c78893fbfb8890cf", "message": "Fix grammar typo in docstring"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alej0varas": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/9b2a1720781930f8eed87bce2c3396e40a99529e", "message": "Updated SQS requirments file (#4231)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rpkilby": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/cbbf481801079f0e2cfbfe464c9ecfe3ccc7a067", "message": "Remove 'is_not_contained' check from _create_app"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4212", "title": "Fix celery_worker test fixture", "body": "This is an attempt to fix #4088. Thanks to @karenc for providing a [breakdown](https://github.com/celery/celery/issues/4088#issuecomment-321287239) of what's happening.\r\n\r\nSide note:\r\nUsing `celery_worker` over `celery_session_worker` in the integration tests is a bit slower, given the worker startup/teardown for each test. This could be faster if the worker was scoped at a module level, but it's not possible to change the scope a fixture. The [recommendation](https://github.com/pytest-dev/pytest/issues/2300) is to simply create a fixture per scope. \r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mozillazg": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/aa4332b49c9d993e131b5e9d459fb50d723f2b80", "message": "Fix example about Default values (#4230)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "PatDuJour": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/bda678b9cd3a4ea47bfdd4c33aaabffb678de883", "message": "Update periodic-tasks.rst to fix import issue (#4194)\n\nThe docs were not updated for celery 4, which does not need django-celery library.\r\nMost issues arose were because of the doc does not give clear guidance for celery 4 and above.\r\nFix issues:\r\nhttps://github.com/celery/django-celery/issues/496\r\nhttps://github.com/celery/django-celery/issues/523\r\nhttps://github.com/celery/django-celery/issues/491 \r\nhttps://github.com/celery/celery/issues/3637"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Kxrr": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/d3ce72d0fff068e3f3696bb37b44d47ff8ae21f4", "message": "Inspect scheduler class settings in celery beat command. (#4189)\n\n* Inspect scheduler class settings in celery beat command.\r\n\r\n* Add myself to contributors list."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "daviddavis": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/01ba38507fa1b57557fc1c8cc329d94fd71cee55", "message": "Adding ability to retry signal receiver after raised exception (#4192)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/132740319", "body": "In python 3 we could use `nonlocal` but for python 2 we have to use a dict to mutate the values for dispatch_uid and weak.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132740319/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "raphael-riel": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/f3f67917f311c69ef14d10c1c2b3e2747021edcd", "message": "Django fixup should close all cache backends (#4187)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "diemuzi": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/d8ac5fdecdeff33b12c4506c0f59ccf5ee3820a3", "message": "Update CONTRIBUTORS.txt (#4210)\n\n* Update CONTRIBUTORS.txt\r\n\r\nAdded name as requested\r\n\r\n* Update CONTRIBUTORS.txt\r\n\r\nAdded missing comma"}, {"url": "https://api.github.com/repos/celery/celery/commits/1b9678915bfad166fbf4671ad49b3cfaec5bb633", "message": "Update platforms.py (#4203)\n\nRewords root user message to include the correct syntax on running as a different user"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kanemra": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/f89592efc0afe06e9ec19004d76ca4ff8ca801b4", "message": "Update routing.rst (#4206)\n\nTypo fix."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "martialp": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/f9a9337e851944cc782482c3253c3bade7908010", "message": "Adds stopasgroup to the supervisor scripts (#4200)\n\n* Adds stopasgroup to the supervisor scripts\r\n\r\n* Add details on the stopasgroup parameter in supervisor configs"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/133449036", "body": "It is not in the documentation, but if we take a look at the `test_stopasgroup` function in the supervisor [sources](https://github.com/Supervisor/supervisor/blob/8569e3f4e8e15210b10adcaddea1daf2756db220/supervisor/tests/test_process.py#L880-L899), we can see that SIGTERM signal is used.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/133449036/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 1, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "frostoov": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/bd347f7565f3a72c8cfb686ea0bfe38cfa76e09b", "message": "Using Exception.args to serialize/deserialize exceptions instead of str(Exception) (#4085)\n\n* Using Exception.args for serializetion\r\n\r\n* backwards compat\r\n\r\n* Fixed exception creating\r\n\r\n* Fixed typo\r\n\r\n* Test fix\r\n\r\n* backwards fixes\r\n\r\n* Formatting\r\n\r\n* Formatting and exception_to_python fix\r\n\r\n* Tests fix\r\n\r\n* Fixed exception deserialization\r\n\r\n* formatting"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tbodt": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/9d345f583631709cfdd38b77d2947ec74d83a562", "message": "Set the SO_REUSEADDR option on the socket (#3969)\n\nIf the server exits before the client does, the port will appear to be in use for 90 seconds unless you set this option. TCP is annoying."}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/18546051", "body": "Why, exactly, is this necessary?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18546051/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/18547029", "body": "It was causing bugs for me, so I took it out in my fork...just want to make sure I didn't break anything\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18547029/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/18821688", "body": "Does `Task.replace` work if you pass a chord? Or a chain?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18821688/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/129886624", "body": "`SO_REUSEADDR` has been around on Linux and Mac since time immemorial. You're probably looking at `SO_REUSEPORT`, which was added in 3.9.\r\n\r\nNot sure about windows, though.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/129886624/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/129887237", "body": "Actually, according to MSDN, it's available since Windows 95. So there's no problem there. https://msdn.microsoft.com/en-us/library/windows/desktop/ms740621(v=vs.85).aspx", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/129887237/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "kevingu1003": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/13916dee1bb647a9660c8fff36b1377220f9fe64", "message": "Fix celery ignores exceptions raised during `django.setup()` (#4146)\n\n* Check `import_modules.send`\u2019s return value in `import_default_modules`, make sure no exception is silenced.\r\n\r\n* Fix codestyle issues."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "michael-k": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/c258ea0a958ba796edda88064cce2084493b022a", "message": "Fixed rst syntax in readme (#4165)"}, {"url": "https://api.github.com/repos/celery/celery/commits/fb9ad6c9d4e856ec3797e1f042d705f408320bb9", "message": "Fixed TypeError due to change of task protocol (#3714)\n\nIf the body is a tuple, protocol version 2 is used.  Therefore collect\r\nthe parameters according to:\r\nhttps://github.com/celery/celery/blob/v4.0.2/docs/internals/protocol.rst#definition\r\n\r\nOtherwise protocol version 1 is still in use.\r\n\r\nFixes celery/celery#3707"}, {"url": "https://api.github.com/repos/celery/celery/commits/6cbc03847f935d96892bdc04239976a777e38296", "message": "[docs] Fixed link to pytest (#3720)\n\nPointed to https://pypi.python.org/pypi/py.test%20%3Cpytest/\r\ninstead of https://pypi.python.org/pypi/pytest/"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mperice": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/22adf673e9f6e920ddf559b57bd18baf186b692e", "message": "Broker connection uses the heartbeat setting from app config unless set otherwise (#4148)\n\n* broker connection uses the heartbeat setting from app config unless set otherwise\r\n\r\n* test for broker heartbeat settings\r\n\r\n* split long lines of code"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "DDevine": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/b47d12dd2dcf7c16875eaa44eaa63b140353024c", "message": "Fixed exception caused by next_transit receiving an unexpected argument. (#4103)\n\n* Fixed exception caused by next_transit receiving an unexpected argument.\r\n\r\n* Fixing inconsistent style.\r\n\r\n* Adding tests for solar methods to determine if they accept the use_center argument or not.\r\n\r\n* Fixing flake8 errors."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/124166985", "body": "I don't see a problem with using self.method - am I missing something?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/124166985/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/124167052", "body": "Good catch. Oops!", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/124167052/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ryanguest": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/f83b072fba7831f60106c81472e3477608baf289", "message": "Fix a couple typos (#4156)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "singingwolfboy": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/0d5b840af1890a9a499a339aa3256445b43837dc", "message": "Add options for exponential backoff with task autoretry (#4101)\n\n* Add options for exponential backoff with task autoretry\r\n\r\n* Add test for exponential backoff\r\n\r\n* closer to a fixed test\r\n\r\n* Move autoretry backoff functionality inside run wrapper\r\n\r\n* Add a test for jitter\r\n\r\n* Correct for semantics of `random.randrange()`\r\n\r\n`random.randrange()` treats the argument it receives as just *outside* the\r\nbound of possible return values. For example, if you call\r\n`random.randrange(2)`, you might get 0 or 1, but you'll never get 2.\r\nSince we want to allow the `retry_jitter` parameter to occasionally apply no\r\njitter at all, we need to add one to the value we pass to `randrange()`,\r\nso that there's a chance that we receive that original value back.\r\n\r\n* Put side_effect on patch lines\r\n\r\n* Fix flake8\r\n\r\n* Add celery.utils.time.get_exponential_backoff_interval\r\n\r\n* Use exponential backoff calculation from utils in task\r\n\r\n* Update docs around retry_jitter\r\n\r\n* Remove unnecessary random.choice patching\r\n\r\n* Update task auto-retry documentation\r\n\r\n* PEP8: remove unused import\r\n\r\n* PEP8: remove trailing whitespace\r\n\r\n* PEP8: Fix E123 warning"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/128903114", "body": "Is there a reason why you're mocking `randrange` as `lambda n: n - 2` here, but mocking it as `lambda n: n - 1` in the `test_negative_values` test? It's not _wrong_, but it's confusing.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128903114/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128903159", "body": "Looks like you removed the feature I described in the documentation, where you can pass `retry_jitter` an integer and it will treat it as a maximum deviation from the calculated countdown value. Is that intentional? I imagine that some applications will only want a small amount of jitter applied to their calculated value, which is why I included that feature.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128903159/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128930587", "body": "No, I don't have an actual use case. I included that version because, when I first started thinking about the concept of jitter and making a pull request to Celery, that was the algorithm that I thought of. But if research shows that the \"full jitter\" algorithm is superior, then I'm fine with dropping my naive algorithm.\r\n\r\nHowever, we'll need to update the documentation, so that it no longer refers to a feature that doesn't exist!", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128930587/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128887215", "body": "Added!", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128887215/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128887251", "body": "> `random.randrange` excludes the endpoint that is why I added `1.` This was also excluding defining `1` as a possible jitter value.\r\n\r\nGood catch! Thanks, I fixed that.\r\n\r\nAs for making the code more compact, I'd like to do that, but I'm bumping up against the \"max characters per line\" style restriction. So, I'll keep things the way they currently are in the pull request.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128887251/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128895984", "body": "Oops! Fixed.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128895984/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128896049", "body": "You're certainly welcome to check out my code from this branch and manually test it, if that's what you mean. If you're asking to change the code in this branch, I'd prefer if you made a new branch off of this one, make whatever changes you're suggesting there, and make a pull request back to the `exponential-backoff` branch of my fork. That way, I can see exactly what you're suggesting, without you modifying my work unilaterally.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/128896049/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "samueldg": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/6bb75aca13ba877a9f968fc4134b87e1257ead17", "message": "Update Celery/Python/Pypy versions in the README (#4157)\n\n- Celery bumped 4.1;\r\n- Python 3.6 now supported;\r\n- Pypy bumped to 5.8, which is used for CI."}, {"url": "https://api.github.com/repos/celery/celery/commits/9f6ea762a7d7bad41d7b081ae57341c38936a862", "message": "Fix pytest config file name"}, {"url": "https://api.github.com/repos/celery/celery/commits/7c3b057d1a26f65e636b8edc2b1023667c64d05a", "message": "Update contributors list"}, {"url": "https://api.github.com/repos/celery/celery/commits/34097b58aeaa015eaa97adc690911964def166e9", "message": "Fix multiline formatting issue"}, {"url": "https://api.github.com/repos/celery/celery/commits/ff317ef326b2b6ccda4898fe7682d966475d7005", "message": "Fix link reference in the FAQ\n\n- Use exact header title;\n- Add `_` to have a link."}, {"url": "https://api.github.com/repos/celery/celery/commits/70c6f7f0acabadd6166d946d995acb70f9270193", "message": "Fix minor typos in the FAQ and user guide"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/118824124", "body": "This is a reference to another question, which is not aligned with the actual question.\r\n\r\nIf you look two questions below this answer, you'll see it is indeed:\r\n\r\n> Why is Task.delay/apply*/the worker just hanging?\r\n\r\nand not \r\n\r\n> Why is Task.delay/apply* just hanging?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/118824124/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "pkmoore": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/325041f123e3eadd8cfa54996e800a904e4b0604", "message": "Updating contributors list (#4141)"}, {"url": "https://api.github.com/repos/celery/celery/commits/bccea8de7dc083d390509f686aaa79c5e30fa5bd", "message": "Make appstr use standard format (#4134) (#4139)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vivekanand1101": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/d9fcca32274341a08d8938699d2d3af08e22e0d9", "message": "Fix typos (#4140)\n\n* examples/django: fix typo. iself => itself\r\n\r\n* examples/django: fix grammer."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "anilaratna2": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/41848e29cc9aa4332c33fcc0937de61229045467", "message": "CELERY_SEND_EVENTS instead of CELERYD_SEND_EVENTS for 3.1.x compatibility (#3997)\n\nUse 3.1.x option CELERY_SEND_EVENTS in celery 4.0 instead of CELERYD_SEND_EVENTS for compatibility.\r\ncelery 3.1.x : https://github.com/celery/celery/blob/v3.1.18/celery/app/defaults.py#L150"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "clokep": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/767e6b99437ed1bb0e137d804853ba76b6baf0a7", "message": "Restore behavior so Broadcast queues work. (#3934)\n\n* Restore behavior so Broadcast queues work.\r\n\r\n* Add unit test.\r\n\r\n* Add additional unit tests."}, {"url": "https://api.github.com/repos/celery/celery/commits/3c98e6216167d7e5f554790ecdc00ff671ef3f06", "message": "Use the Sphinx add_directive_to_domain API. (#4037)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/108145563", "body": "I'll give it a try, any pointers of where to add this / look for a similar test? (Thanks for reviewing!)", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108145563/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108167871", "body": "I see a bunch of `test_send_task_message__*` methods in [t/unit/app/test_amqp.py](https://github.com/celery/celery/blob/522ef610ea17ae79695e995ddd82d034e8e8bc7a/t/unit/app/test_amqp.py), so I'll start there...", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108167871/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "dhuang": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/19a7d8f925bbe7f5e9237e693e3a15130202912d", "message": "Add worker_shutting_down signal (#3998)"}, {"url": "https://api.github.com/repos/celery/celery/commits/e812c5780b4006516116f059ab498e1f043bdd50", "message": "Update changelog for v4.0.3 (#3999)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "h4ck3rm1k3": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/dbf1a821035600a55ab57050afb620d791b6225d", "message": "Update rdb.py (#4135)\n\nhttp://snippets.dzone.com/posts/show/7248 not available or on archive.org"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rh0dium": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/8dc5d254620152ed8b3435b2763b0e93a7c899f4", "message": "Updated documentation settings for DatabaseScheduler. #4057 (#4058)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vinodc": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/e84aabad8e73fc3763fcd24ab739052fda00885a", "message": "Updating CONTRIBUTORS."}, {"url": "https://api.github.com/repos/celery/celery/commits/ceb36e1a161d1a468c5cf7d5732514c185dd04c4", "message": "Ensuring pending results are removed from the ResultSet when complete."}, {"url": "https://api.github.com/repos/celery/celery/commits/8728c606f90477a820f80ec7d757d2b7700e6461", "message": "Removing weak-references to bound methods since they are dead-on-arrival."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "staticfox": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/8fba154e870a5293f43cc6500923b463afbfdf38", "message": "elasticsearch: Fix serializing keys (#3924)\n\n* elasticsearch: Fix serializing keys\r\n\r\nelasticsearch requires keys to be a string, not bytes,\r\nso make sure we convert the key to a string\r\n\r\n* Address code review.\r\n\r\n* Added a testcase that ensures byte keys are converted to strings.\r\n\r\n* Fix.\r\n\r\n* Use bytes_to_str instead of string for 2/3 compatibility\r\n\r\n* Update unit test for bytes_to_str\r\n\r\n* Remove five.string reference\r\n\r\n* Correctly convert sentinel.task_id\r\n\r\n* Convert sentinel object to bytes/str"}, {"url": "https://api.github.com/repos/celery/celery/commits/32e9fd1cab02681fa9ef7e02a80b46738dae0095", "message": "AUTHORS: Add staticfox (#3908)"}, {"url": "https://api.github.com/repos/celery/celery/commits/c7b163a5601140fe1e1bd58d8868d11b903bdeab", "message": "Fix grammar, update .gitignore (#3887)\n\n* gitignore: Ignore .eggs directory\r\n\r\n* docs: Fix typos/grammar"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/107589565", "body": "Sure, that's fine", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/107589565/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "aydin": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/52fa10248e8a53f1cc96da5c3cd5aa0ed6e37f3b", "message": "Update CONTRIBUTORS.txt\n\nContributors list update according to below PR.\r\nhttps://github.com/celery/celery/pull/4086"}, {"url": "https://api.github.com/repos/celery/celery/commits/08d8da96a5bf8bee9a76622e36562b656cffdbf5", "message": "Create states.py\n\nRemoved duplicate REJECTED variable"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jaymcgrath": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/de2d075fb07850eee247766ebeb11d5530a43734", "message": "fix minor grammatical errors and rewrite to be more clear and concise, (#3801)\n\nadd jaymcgrath to AUTHORS.txt\r\n\r\nRevert to punctuation outside parenthesis on line 435, revise text on\r\nline 461."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/108066725", "body": "I can revert it, no problem. \r\nThe difference is stylistic, American convention is punctuation inside quotes, British is outside the quotes. \r\nhttp://www.lupinworks.com/roche/pages/quotations.php", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/108066725/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "Djaler": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/a68b3d013c3115f9374b0a24ed99960aabf25d32", "message": "Fix wrong type in docstring (#4067)\n\n* Fix wrong type in docstring\r\n\r\n* Remove trailing whitespace"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rpatterson": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/397aa019342d276687fb9756662f3ce621f2ea9b", "message": "Fix task eager Django setting names (#4061)\n\nUnder celery 4.0.2, I've tried the settings in this document before this change, variations with and without the `*_TASK_*` element, and a few other variations, and the two names in this change are the only two that work for me."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "primoz-k": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/655e25fe2526cebf3968f19a8c83abee6546fa2b", "message": "Update whatsnew-4.0 (#3909)\n\nIssue: https://github.com/celery/celery/issues/3874"}, {"url": "https://api.github.com/repos/celery/celery/commits/d4763d09a8fd58e0eb7879ffeeb29ef0ce651b1b", "message": "Fix typo in periodic-tasks docs (#3869)"}, {"url": "https://api.github.com/repos/celery/celery/commits/144f88b4e1be21780e737d4be5a734b19f1cf511", "message": "Add myself to the AUTHORS file (#3881)\n\nReference: #3869"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tamers": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/1e5e24e409482e514fea19f899b7cb6519561f90", "message": "Fix CELERYBEAT_LOG_FILE replacement option typo (#3788)\n\nThe CELERYBEAT_LOG_FILE was set to be replaced with `celery beat --loglevel`. It should be `celery beat --logfile`"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "epoelke": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/3761066032ed41acc8409feb18cfe2bed996e3d1", "message": "documentation fix. (#3732)\n\nAdd missing parentheses to example on line 176."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "fcoelho": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/1735336dadfdb0d4939459a3576d46dbb23c732c", "message": "Deserialize all tasks in a chain (#4015)\n\nWhen loading a chain that had been fully serialized to json, deserialization\r\nhappened only in part of the original task. Specifically, take a\r\njson-serialized task that looks like the following:\r\n\r\n    {\r\n        ...,\r\n        \"kwargs\": {\r\n            \"tasks\": [\r\n                {...}, {...}, {...}\r\n            ]\r\n        }\r\n    }\r\n\r\nAfter calling `celery.signature(that_thing_above)`, we get an object that\r\nactually looks like this:\r\n\r\n    {\r\n        ...,\r\n        \"kwargs\": {\r\n            \"tasks\": [\r\n                task_1, # an instance of celery.Signature\r\n                {...}, {...} # same as before deserialization\r\n            ]\r\n        }\r\n    }\r\n\r\nThe culprit was `chain.from_dict`, which was converting only the first subtask\r\nof the chain to actual `Signature` instances. This commit changes that\r\nbehaviour and converts all subtasks to signatures instead. Without this, some\r\noperations on the chain object, such as calling `chain.on_error(...)` would\r\ncause errors of the form `'dict' object has no attribute 'xyz'`."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vnavkal": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/4ca12c3bf053809fc03504c8ca2e8dab0156c655", "message": "fix small documentation typos"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "spookylukey": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/818ef79858dce139e4e2935c14ca6018d97b32cd", "message": "Correct broker URL in RabbitMQ getting started info. (#4043)\n\nThe broker URL should correspond to the actual setup instructions."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rmoorman": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/e37588fa663f7cfcdd74655ab8f389f6b4d8f9cb", "message": "Update testing.rst (#4046)\n\nWithout mocking the retry method of the task and actually setting Retry as side effect, raising an operational error within the task will not lead to a Retry exception but the original Exception to be thrown as the task is called directly."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "simonschmidt": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/c6ce9557adefb50d0694062482188d32960399df", "message": "Clarify AsyncResult.ready docstring (#4038)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "txomon": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/069ed5434ed86b9af7485c8632673a2bfbc2d771", "message": "Signal before_task_publish not doing what documented (#4035)\n\n* Signal before_task_publish not doing what documented\r\n\r\n* Freeze Sphinx version\r\n\r\n* Freeze pydocstyle version"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chriskuehl": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/8c8354f77eb5f1639bd4c314131ab123c1e5ad53", "message": "Add SSL option for redis backends (#3831)\n\n* Add redis_backend_use_ssl option\r\n\r\n* Correct documentation for broker_use_ssl with redis\r\n\r\nThe previous documentation was only correct for pyamqp but not redis,\r\nwhich doesn't allow `True` and requires a slightly different set of keys\r\nwhen passing a dictionary.\r\n\r\n* Document redis_backend_use_ssl\r\n\r\n* Add a test for redis_backend_use_ssl\r\n\r\n* Fixed typo.\r\n\r\n* Add redis_backend_use_ssl to celery.app.defaults"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tarkatronic": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/c81ab5d72d745375e577e1f22ba221c326457b38", "message": "Updates, fixes, and improvements to documentation (#3976)\n\n* Make the Django module installation steps consistent\r\n\r\n* Fix typos and omissions in contribution docs\r\n\r\n* Add a max_line_length, in accordance with contribution docs\r\n\r\n* Add myself to the contributors"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "qingyunha": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/1434cadef35c24f6a1d62b7f9c81383089ebf02c", "message": "Remove duplicate entry in __all__"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bavaria95": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/cb17ab296458c5af40f0f15224f3df1a2d3e6791", "message": "Added myself to the contributors list"}, {"url": "https://api.github.com/repos/celery/celery/commits/bbb580b76e706cbf3fb9532568d3711bbffe6f4b", "message": "Typo of variable name"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "brianmay": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/2552c5957cbdbd96f117f69fc38d9b9556b3541a", "message": "Add self to CONTRIBUTORS.txt\n\nSee #3958."}, {"url": "https://api.github.com/repos/celery/celery/commits/b27c0f143b86989a5f655bcc9592221bbbba0f5f", "message": "Add tests for ensuring schedule changes are detected"}, {"url": "https://api.github.com/repos/celery/celery/commits/53531f5c02de6d289c226f41604761d5ae14cd5c", "message": "Make shallow copy of schedules dictionary\n\nOtherwise, if we mutate the dictionary (e.g. using provided methods),\nwe won't see any changes."}, {"url": "https://api.github.com/repos/celery/celery/commits/02381357a291bedbeea232fa39b2c4d898385bcc", "message": "Simplify if condition using or"}, {"url": "https://api.github.com/repos/celery/celery/commits/1bd0576dd3333bfbb3a95e2f7fe29d4122d2baa3", "message": "Rename b to new_entry"}, {"url": "https://api.github.com/repos/celery/celery/commits/2f44682aac10977e05c97de56b83a4a8ea71e176", "message": "Rename model to old_entry"}, {"url": "https://api.github.com/repos/celery/celery/commits/7f0910cd1b7d015609b44e86d16d444cc220e32b", "message": "Rename b to new_schedules"}, {"url": "https://api.github.com/repos/celery/celery/commits/88e52d88b343e83f08b28292bffd057068ab2034", "message": "Rename a to old_schedules"}, {"url": "https://api.github.com/repos/celery/celery/commits/0a00ad98e6216d2845a5dfb9e144e903b2f4f245", "message": "Address feedback from previous commit"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/109829822", "body": "See https://github.com/celery/django-celery-beat/issues/7#issuecomment-267451710 for why this apparently ugly code is required.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109829822/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109830034", "body": "Yes, that does appear wrong. Will try to understand how this code works.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109830034/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109830752", "body": "``set(a.keys()) != set(b.keys())`` works for me.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109830752/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109831388", "body": "I believe this if comparison can be simplified to:\r\n\r\n```\r\nif model.schedule != b_model.schedule:\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109831388/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109833355", "body": "`model` and `b_model` are of type `django_celery_beat.schedulers.ModelEntry`.\r\n\r\nSo this code is django_celery_beat specific. Not convinced this is appropriate code for celery :-(", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109833355/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109833470", "body": "These `self.old_schedulers` and `self.schedule` are of type `dict` containing instances of `django_celery_beat.schedulers.ModelEntry`, which is derived from `celery.beat ScheduleEntry`, so the `==` or `!=` is only a shallow comparison.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109833470/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109835082", "body": "Oh, wait, my objection here is unfounded.\r\n\r\n`django_celery_beat.schedulers.ModelEntry` is actually inherited from `celery.beat ScheduleEntry` - so I am guessing is actually a standard celery object with some extra bits added.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109835082/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109833853", "body": "This code looks like it is specific the Django DB models used in django_celery_beat. May not be appropriate to include in celery???", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109833853/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109835256", "body": "Oh, wait, my objection to my own pull request might be unfounded.\r\n\r\n`django_celery_beat.schedulers.ModelEntry` is actually inherited from `celery.beat ScheduleEntry` - so I am guessing is actually a standard celery object with some extra bits added.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109835256/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109835608", "body": "I believe the above code is fine.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109835608/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109835787", "body": "For the record, on this line we are comparing two ``celery.schedules.schedule`` objects (for interval times) or two ``celery.schedules.crontab`` objects (for crontab entries). Or maybe one of each (if the user has just changed from one to the other).", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/109835787/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110555324", "body": "Yes, agreed. Probably partly responsible for my confusion above.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110555324/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "Acey9": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/455a11972aab69a1ffc6b08e82466016dd1e6760", "message": "fix test_config_params build err"}, {"url": "https://api.github.com/repos/celery/celery/commits/d3ec0ee5284dd9ee01d2a8781a7dd4c57d8ca054", "message": "fix unit test error"}, {"url": "https://api.github.com/repos/celery/celery/commits/a52cfad2f0806ad44fca37e58b115b552147a09d", "message": "fix uint test error"}, {"url": "https://api.github.com/repos/celery/celery/commits/26588b47da1792c33ddaa18a2cf247430366d492", "message": "add elasticsearch unit test"}, {"url": "https://api.github.com/repos/celery/celery/commits/35357167cf7dce05446b6bc1ea484370dd725ba6", "message": "fix code friendly"}, {"url": "https://api.github.com/repos/celery/celery/commits/b456c616f1fdea9dc55d2dc29cd0f4c9cb0158a1", "message": "fix build document error"}, {"url": "https://api.github.com/repos/celery/celery/commits/21bce2b8e649d37da4e58dc5c8be51a50bdd60e7", "message": "fix error about PEP8"}, {"url": "https://api.github.com/repos/celery/celery/commits/b95514af5d3f88077914ebc26badc545a6f68f67", "message": "fix es default value and document error,fix code friendly"}, {"url": "https://api.github.com/repos/celery/celery/commits/a08c1cb8183bde2b202837723f02d7843373cff8", "message": "revert code"}, {"url": "https://api.github.com/repos/celery/celery/commits/3a905302b6c5e5a6348d40ba67cf338f2618f8fc", "message": "elasticsearch: Fix serializing document id."}, {"url": "https://api.github.com/repos/celery/celery/commits/3f2ce9488ae8fb79f63d28175e1fa9bd900e08f0", "message": "elasticsearch: Fix serializing document id."}, {"url": "https://api.github.com/repos/celery/celery/commits/a935689ca49fa11eab70b465062be5cd0bb174ce", "message": "update document of elasticsearch backend settings"}, {"url": "https://api.github.com/repos/celery/celery/commits/23005b22f6e51ff02ed3853ac2e85e63172623c3", "message": "supports elasticsearch backend options settings"}, {"url": "https://api.github.com/repos/celery/celery/commits/cb3e58c527be121c8fa08abd3db7e49cebf9bef6", "message": "bugfix"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wzywno": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/6a24e00b7e6ae9cbb03bf52e14b67c8d31d7d43c", "message": "Populate heap when periodic tasks are changed"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "akayj": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/a7ada0d30226bc43e45934061df3e31b1b13d50c", "message": "Add myself to the AUTHORS\n\nrefer to https://github.com/celery/celery/pull/3962"}, {"url": "https://api.github.com/repos/celery/celery/commits/4f6c3b5d184455b0fdacd7968bbba53583dddb03", "message": "return boolean value according to the doc\n\nreturn boolean value according to the doc"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "marcgibbons": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/522ef610ea17ae79695e995ddd82d034e8e8bc7a", "message": "Lookup task only if list has items"}, {"url": "https://api.github.com/repos/celery/celery/commits/0ad41a09ae7d4d48cd3ba402296ee7d3022b7fa2", "message": "Write failing test"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "triat": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/b6ca6b6579f118a9d7b3ebe46da7861df04f1c12", "message": "Update slightly the doc (#3929)\n\nJust a small update to make the exemple more clear"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mans0954": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/9399a16e7189900619abf667eaa2deb4f3f29eb0", "message": "Remove non-free ICC profiles from docs/images (#3936)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "GeyseR": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/7cf709d63e517c4223ae6df508db8789184a57eb", "message": "Changed default redis_socket_timeout setting value (#3920)\n\nAccording to changes in https://github.com/celery/celery/commit/6d4ff8689b2128a2e95e52857d9e346b1ad9827a"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "beezz": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/d3c00e17c860f4c9b26ed0a964f83ba972ea3b51", "message": "Indent statement in docs function (#3910)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "3lnc": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/0cd1e3c67d8c95b7b64fb17165ac0db4ee6f260b", "message": "-Ofair reference, rst formatting fixes (#3913)"}, {"url": "https://api.github.com/repos/celery/celery/commits/c015bfdb895672408fcdbf085efdd8974f5f75fc", "message": "* Fixes items format route in docs (#3875)\n\n* Minor fix to contributing.rst"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "browniebroke": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/389c844ff7c1155606b796969175e69b2c445b8b", "message": "Document rate limit and ETA incompatibilities (#3899)\n\nFixes #3888"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bittner": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/df98aff258ffa06c01512e48701b94a6c7e9ab0e", "message": "Add myself to the AUTHORS file (#3889)\n\nAs suggested in #3842 by @thedrow"}, {"url": "https://api.github.com/repos/celery/celery/commits/faaa949c4d88d8b271b39afb45b9553fc2abbe67", "message": "Use a wording a bit closer to English (#3842)\n\nThe sentence to explain the first code sample is a bit bumpy. This change should make it a bit easier to digest, hopefully."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "michaelhelmick": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/791ea70312798e6586ce7985306ae0ca24db48ef", "message": "Fix setting name for task_eager_propagates (#3846)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bremac": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/6c0abdbc0ecfcd40c3d7bc5b190166d4df1d2ac0", "message": "Fix retried tasks with expirations (#3790)\n\n* Fix retried tasks with expirations\r\n\r\n* Use string_t instead of basestring for python 3"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "srinchiera": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/f68b63c03d39bfe1d475f2411bad3b2aa6b03c63", "message": "Fix typo from \"restart limit\" to \"retry limit\" (#3807)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jalessio": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/fd21084fecf85599f991af6be8a3314f4b6ac911", "message": "Typo in README (#3836)\n\n\"Please the Contributing section\" to \"Please see the Contributing section\""}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "baixuexue123": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/64d0a09a4435482bc2a731129bd83fcc737e1468", "message": "confirm error (#3840)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "simon04": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/e42f07301c2647cca7f94556ac743692db85638c", "message": "Fix URLs to userguide/daemonizing docs (#3871)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "orf": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/393d3220707df01ee947caf459b3015f66b433af", "message": "Add a link to django-transaction-hooks (#3799)"}, {"url": "https://api.github.com/repos/celery/celery/commits/9f7adf4329b3ec668d0c1babc9a80ef9897d76f8", "message": "Cache pip installs (#3785)\n\n* Cache pip installs\r\n\r\n* Update .travis.yml\r\n\r\n* Update .travis.yml"}, {"url": "https://api.github.com/repos/celery/celery/commits/c5793740685fa1376b2d06f3678c872ca175ed6f", "message": "Add myself to the authors file (#3784)\n\nRef #3777"}, {"url": "https://api.github.com/repos/celery/celery/commits/fc00d0dd8b271396896c97426d4260bad179a298", "message": "Update tasks.rst (#3777)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "edmorley": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/2a31a987179c6b4a371d8486197b5d37e75efc46", "message": "Docs: Fix typo in whatsnew-4.0.rst (#3793)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jalanb": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/2918d0d8c45474cc41a3dd00434ba06a73f5ce17", "message": "Add myself to the authors file (#3800)\n\nJust adding myself to the authors file for #3787.\r\nOops: should have done it as part of that MR rather than a separate one."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "arcivanov": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/7d1588e004875b35a8aa3e994f148413db52c845", "message": "Add myself to contributors (#3769)\n\nrelated to #3745"}, {"url": "https://api.github.com/repos/celery/celery/commits/c1fa9af097971256f44c3e4b7d77810166a20693", "message": "Consumer does not shutdown properly when embedded in gevent application (#3746)\n\nfixes #3745"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/95082723", "body": "Done", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/95082723/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "morganiq": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/615da5bdf051e9a5aa511cf876079392041db86c", "message": "Add myself to contributors (#3766)\n\nFirst contribution was PR #3730, merged on 2 January 2017 UTC."}, {"url": "https://api.github.com/repos/celery/celery/commits/96177c6e5d6dfdeb5723f505a12bf8117d87fc84", "message": "Fix #3725: Task replaced with group does not complete (#3731)\n\n* Add add_to_all test task\r\n\r\n* Add new failing test test_chord.test_group_chain\r\n\r\n- Exercises a task replaced with a group\r\n\r\n* Fix calculation of __length_hint__ on chord.tasks when it's a group\r\n\r\nBefore group.__iter__() was removed in 8c7ac5d, passing either a list or\r\na group through list() would yield the same thing: the list of tasks.\r\nAfter the removal of group.__iter__(), list(g) yields the same thing as\r\ng.keys(), so we need to handle the group case explicitly now.\r\n\r\nUsing len(g.keys()) for __length_hint__ will only happen to be the\r\ncorrect chord size sometimes; the rest of the time it will result in\r\nchords that never get unlocked due to a mismatch between the computed\r\nchord size and the size of the complete set of results returned.\r\n\r\nMakes test_chord.test_group_chain pass.\r\n\r\nFixes celery/celery#3725"}, {"url": "https://api.github.com/repos/celery/celery/commits/9d2566e9c0764ab7467db47610ccb3ee5f4303ff", "message": "Fix #3726 - Chaining of replaced tasks (#3730)\n\n* Add add_replaced test task\r\n\r\n* Make test_complex_chain fail by adding a replaced task\r\n\r\n- Update expected output\r\n\r\n* Copy replaced task's request chain in reverse\r\n\r\n- Make t/integration/test_canvas.py::test_chain::test_complex_chain pass"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/94337812", "body": "I don't know; the `@flaky` decorator looked to be indiscriminately applied to all tests in `t/integration`, so I included it for consistency. Someone else would have to answer about whether or where it is still needed.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/94337812/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "alanjds": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/75b624b8226bada6cb8211b7b9346ae75aa362ca", "message": "Added Alan Justino da Silva to CONTRIBUTORS.txt (#3739)\n\nAs asked at https://github.com/celery/celery/pull/3480#issuecomment-253951353, added myself. Used the date of the request. Please modify if needed."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ahvigil": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/0496fd9cf8553bec4a35684ba3450301c96a9cd7", "message": "fix sqs broker documentation typos (#3742)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mback2k": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/892623997886dbe184ff102b0d7857eb0f298459", "message": "Update first-steps-with-django.rst (#3724)\n\nAlign project layout with [Django documentation](https://docs.djangoproject.com/en/1.10/intro/reusable-apps/). The previous project layout was not clear about wether the proj/ is double-nested and on which level manage.py exists."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shalev67": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/b9a84ca49dc45874829cd32764db731f3a0d6262", "message": "Sync subtask option (#3696)\n\n* Added disable_sync_subtasks option\r\n\r\n* Added some documentation for  *disable_sync_subtasks*  option\r\n\r\n* Added myself to contributors\r\n\r\n* Added unit test for sync subtask option"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "decaz": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/43058644b88a62c92aa2258d336abb660f52cac6", "message": "Fix request context for blocking task apply (added hostname) (#3716)\n\n* Added hostname to the request context for blocking task apply\r\n\r\n* Added test"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "blueyed": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/7ba12982dc081f044533cd4adfad3903ef033c00", "message": "doc: add 'self' to test_send_order.test_failure (#3718)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rsichny": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/1992cb07f1d4de44ab80ff7ee8ab93614517d7ae", "message": "fix #3678 (#3693)"}, {"url": "https://api.github.com/repos/celery/celery/commits/5a87ede8b2f206d32b9adf94dada18bd9e455533", "message": "make missing args/kwargs in protocol v1 message to return empty value in protocol v2 message (#3695)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/92824289", "body": "args are always a tuple, i don't think that returning different type in case of empty args will be correct.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/92824289/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "aleperno": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/cfcf3f8e77dc15d5bbc2768dd2f6ba9e81a2dc85", "message": "Provide a transparent method to update the Scheduler heap (#3721)\n\n* Provide a transparent method to update the heap\r\n\r\nIn previous versions, we were able to modify dynamically the `schedule`\r\ndict and it will be seen reflected in the scheduled tasks since `tick`\r\nmethod directly iterated over it.\r\n\r\nIn 4.0 version, a heap is used and set after the init (when `_heap` is\r\nNone), this breaks our ability to dynamically change the `schedule` and\r\nsee it reflected without directly modifying the `_heap` attribute thus\r\nbreaking the abstraction.\r\n\r\nSince there are already methods than allows to modify the `schedule`\r\nwithout going straight into inner implementation, therefore providing a\r\nmethod to modify the `_heap` without knowing of its internal makes\r\nsense.\r\n\r\n* Remove unnecessary passing of arguments\r\n\r\nThe arguments removed are not for user change but performance\r\n\r\n* Pep 8\r\n\r\n* Fix function docstring\r\n\r\n* Remove unused argument\r\n\r\n* Add myself to contributors"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chensjlv": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/98222863674c7be625d9091d0301fd02a1244b07", "message": "add myself to contributor list (#3710)\n\nadd myself to contributor list"}, {"url": "https://api.github.com/repos/celery/celery/commits/ad78f7317c502436e00a1d1922117d5983f007c2", "message": "Stop generate a new field every time when a new result is being put (#3708)\n\n* stop generate a new field every time when a new result is being put\r\n\r\nThe current elasticsearch backend generate a new field every time\r\na new result is being put. This is really inefficient for elasticsearch.\r\n\r\nSwitch to use the same field every time since the key is already stored in\r\nelasticsearch's _id field.\r\n\r\n* fix test cases for elasticsearch backend"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "imZack": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/0697b5ec4879c9bea03d94512dfa069fa2b69e86", "message": "fix typo (#3717)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bronsen": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/3d5f96c7de8363acfe27f4d6354576b5738e8f83", "message": "Grammar: pluralize verb (#3713)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cailloumajor": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/fd9ea7b50ff22cf34425156c800cd246b45a4fad", "message": "Specify default value for pidfile option of celery beat. (#3722)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "adq": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/f04f64a6b767de3286432ff25a9b358e2b404236", "message": "Fix celery mongodb integration so that binary encodings like pickle/m\u2026 (#3705)\n\n* Fix celery mongodb integration so that binary encodings like pickle/msgpack work in v4 again\r\n\r\n* add missing space"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pelme": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/0b6b5da92df2932e896f725ba9f5fcd68dd10282", "message": "Added myself to CONTRIBUTORS.txt (#3680)"}, {"url": "https://api.github.com/repos/celery/celery/commits/a35e58cf92c8f838115623a4a294fb873bd3cd34", "message": "Fixed #3651 - add `app` argument to `GroupResult.restore`. (#3669)\n\nThis makes the restore method behave the same way as the `GroupResult`\r\nconstructor."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "icemac": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/595273e435929380c99e489a721e06bb3dead598", "message": "Fix code examples in routing documentation (#3664)\n\n* Fix duplicate argument names.\r\n\r\n`kwargs` can only used once in the declaration.\r\n\r\n* Fix duplicate argument names."}, {"url": "https://api.github.com/repos/celery/celery/commits/1528400abb8c5b609ec289e09583c27046dec621", "message": "Fix code examples in routing documentation (#3664)\n\n* Fix duplicate argument names.\r\n\r\n`kwargs` can only used once in the declaration.\r\n\r\n* Fix duplicate argument names."}, {"url": "https://api.github.com/repos/celery/celery/commits/b59e911a06b2a319d954da7195746f86f8ecbc1d", "message": "Add `celery_worker_parameters` test fixture. (#3668)\n\nThis enables customized worker init parameters."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "PauloPeres": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4484", "title": "Adding the CMDS for Celery and Celery Beat to Run on Azure WebJob", "body": "This is what worked for us!\r\n\r\n## Description\r\nCreated the .cmd files to be used on Azure.\r\nJust going into Web Jobs on Azure servers and creating a Continuous Web Job, and uploading the zip files of the celery should work.\r\nEach folder should have a separeted Web Job.\r\nAlso whoever use should take a look where their celery package is\r\n\r\n\r\nThis pull request don't fix any bugs, it's just a new \"Helper\" for the ones who want to use Celery into Azure servers.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zengdahuaqusong": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4475", "title": "separate backend database from login database", "body": "by setting 'backend_database' in MONGODB_BACKEND_SETTINGS, users can separate backend database from login database. Which means they can authenticate mongodb with 'database', while data is actually writing to 'backend_database'\r\n\r\n*Note*: Before submitting this pull request, please review our [contributing\r\nguidelines](https://docs.celeryproject.org/en/master/contributing.html).\r\n\r\n## Description\r\n\r\nPlease describe your pull request.\r\n\r\nNOTE: All patches should be made against master, not a maintenance branch like\r\n3.1, 2.5, etc.  That is unless the bug is already fixed in master, but not in\r\nthat version series.\r\n\r\nIf it fixes a bug or resolves a feature request,\r\nbe sure to link to that issue via (Fixes #4412) for example.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "neaket360pi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4472", "title": "Cleanup the mailbox's producer pool after forking", "body": "Fixes https://github.com/celery/celery/issues/3751\r\n\r\nIf the mailbox is used before forking the workers will\r\nnot be able to broadcast messages.  This is because the producer pool\r\ninstance on the mail box will be `closed.`  This fix will cause the\r\nmailbox to load the producer pool again after forking.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "charettes": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4456", "title": "Perform a serialization roundtrip on eager apply_async.", "body": "Fixes #4008 \r\n\r\n/cc @AlexHill ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jurrian": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4442", "title": "Fix for get() follow_parents exception handling (#1014)", "body": "## Description\r\n\r\nWhen working with chains, the situation might be that the first chain task has failed. Calling `x.get(propagate=True, follow_parents=True)` should cope with that since it checks if the parent tasks have raised exceptions.\r\n\r\nHowever, in my experience, when `x.get(propagate=True, follow_parents=True)` is called the failed task is still pending at that moment and will become failed seconds later. This creates a bug in the behaviour of `follow_parents`.\r\n\r\nThis fix calls `get(propagate=True, follow_parents=False)` on each parent instead, which will cause it to raise directly when the parent task becomes failed. It looks like `maybe_throw()` can be safely replaced since it is called by `get()` at some other place.\r\n\r\nIn order to be more comprehensive, I extended the documentation for `follow_parents`.\r\n\r\nI am not experienced enough in this project to see the whole picture, so please advise on possible problems that might arise with this fix for #1014 .\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bluestann": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4241", "title": "Use Cherami as a broker", "body": "Hi, \r\n\r\nI have added support for celery to use [Cherami](https://eng.uber.com/cherami/) as a broker. \r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "harukaeru": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4239", "title": "Changed raise RuntimeError to RuntimeWarning when task_always_eager is True", "body": "## Description\r\nThis PR changes RuntimeError to RuntimeWarning when `task_always_eager` is True.\r\n\r\nSometimes I tested what is related to celery task using `task_always_eager` in celery v3.\r\n(The tests are not only celery task but codes using celery task and other codes)\r\n\r\nI know the config cannot be completely tested async task but it's useful when doing rush works.\r\nI never use them in the production code but in the test, I use.\r\n\r\nI read the issue (https://github.com/celery/celery/issues/2275) was produced and the commit (https://github.com/celery/celery/commit/c71cd08fc72742efbfc846a81020939aa3692501) resolved the above.\r\n\r\n\r\nI almost agree with them but people who want to test perfectly only don't turn on `task_always_eager`.\r\nOthers who want to test synchronously also want to use `task_always_eager`.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bbgwilbur": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4077", "title": "Catch NotRegistered exception for errback", "body": "If the errback task is not registered in the currently running worker, the arity_greater check will fail. We can just assume its going to work as an old-style signature and deal with the possible error if it doesn't later.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "JackDanger": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3838", "title": "Removing last references to task_method", "body": "AFAICT the code removed in this PR only served to support the\n(now-removed) celery.contrib.methods.task_method() function.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gugu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3834", "title": "Handle ignore_result", "body": "## Description\r\n\r\nCurrently (according to my checks and code grep) ignore_result option is a stub, it does nothing. This patch skips backend calls for tasks with `ignore_result=True`\r\n\r\nThis is needed to minimize effect of broken redis backend support", "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/pulls/3815", "title": "Chord does not clean up subscribed channels in redis. Fixes #3812", "body": "This pull requests fixes issue, when chord gets result for subtasks, but does not do `UNSUBSCRIBE` command for them.\r\n\r\n**What does this patch fix**:\r\n\r\nWithout this patch `chord(task() for i in range(50))` creates 51 subscriptions. After patch it will create 1\r\n\r\n**What this patch does not fix**:\r\n\r\nEvery task, which result is not consumed, creates redis subscription. Even if `ignore_result` is specified. I plan to submit separate patch for this issue\r\n\r\nI think, that changing old slow redis behavior to new fast and broken was not a good idea, but as soon as choice made, we need to make it usable", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "justdoit0823": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3757", "title": "Add supporting for task execution working with tornado coroutine", "body": "With this future, we can use tornado coroutine in celery app task when pool implementation is gevent. The main idea here is using a standalone tornado ioloop thread to execute coroutine task. If a task is a coroutine, the executor will register a callback on the tornado ioloop and switch to the related gevent hub. Here using a callback means that the coroutine will be totally executed inside the tornado ioloop thread. When the coroutine is finished, the executor will spawn a new greenlet which will switch to the previous task greenlet with the coroutine's result. Then task greenlet returns the result to the outer function inside the same greenlet. \r\n\r\nWe can write code in celery task as following:\r\n\r\n\r\n```python\r\nfrom celery import app\r\nfrom tornado import gen\r\n\r\n@app.task\r\n@gen.coroutine\r\ndef task_A():\r\n\r\n    process_1()\r\n    res = yield get_something()\r\n    do_something_with_res()\r\n    return res\r\n\r\n@gen.coroutine\r\ndef get_something():\r\n\r\n    res = {}\r\n    return res\r\n```\r\n\r\nThis is interesting when we use the tornado framework to write a project, we can easily divide partial logic into a celery task without any modifications. And in tornado 5.0, which will support Python 3 native coroutine, this future can connect more things together. I think people will like this.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/132917610", "body": "Yeah, recent works give me a idea about how to implement a more elegant interface between greenlet and tornado coroutine. I will start a daemon tornado ioloop thread and receive tornado coroutine in greenlet. When coroutine finishs, tornado ioloop can send result to related greenlet.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/132917610/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/134079158", "body": "In this case, the default future executor will do the right job as before. If the task is a coroutine function, which will be partially executed in the tornado ioloop. But the result of coroutine function can't immediately return.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/134079158/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "astewart-twist": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3293", "title": "Deserialize json string prior to inclusion in CouchDB doc", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/69990078", "body": "I'm not familiar with the anatomy of a results backend class, so I'm not at all surprised if there's a better place to put that code.\n\nThe result of this code (wherever it goes) should be necessary, however, unless the intention is that the value of 'value' is supposed to be serialized within the serialized json string.  Like this:\n\n```\n{\n  \"_id\": \"celery-task-meta-2c040203-a07a-4a92-9fa1-3269839e2c54\",\n  \"_rev\": \"1-6b9803d5b1a4ed51a5274ef7c567a332\",\n  \"value\": \"{'status': 'SUCCESS', 'traceback': null, 'result': {}, 'task_id': '2c040203-a07a-4a92-9fa1-3269839e2c54', 'children': []}\"\n}\n```\n\nIf that's not the intention, my proposed change would instead result in the CouchDB document looking like this:\n\n```\n{\n   \"_id\": \"celery-task-meta-2c040203-a07a-4a92-9fa1-3269839e2c54\",\n   \"_rev\": \"1-6b9803d5b1a4ed51a5274ef7c567a332\",\n   \"status\": \"SUCCESS\",\n   \"result\": { },\n   \"task_id\": \"2c040203-a07a-4a92-9fa1-3269839e2c54\",\n   \"traceback\": null,\n   \"children\": [\n   ]\n}\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/69990078/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "m4ddav3": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/2881", "title": "Database Backend: Use configured serializer, instead of always pickling.", "body": "Use the BLOB as an sa.BLOB\nSerialise the result an add to the db as bytes\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/43448452", "body": "Sure, I've removed the line\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/43448452/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "slapec": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/2629", "title": "Fix: Django fixup throws error when the app uses custom loader class", "body": "This small bug appeared when I instantiated Celery with a custom loader class in my Django project. The problem was that the `celery.fixups.django.fixup` function expected the loader class to be a symbol string, but it can be also the class directly.\nI've added this small function `celery.utils.imports.dot` which returns the module and class name as a string too so the condition haven't had to change drastically in the fixup.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "javrasya": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/2414", "title": "High available celery beat", "body": "This is related with the issue #1495 and PR #1888 . In PR #1888 requested by @magmax is still opened. \n\nI guess It is not accepted because it does not support other than `AMQP`. Actually the solution can not be implemented for other types of broker easily. \n\nIn my solution, there is a method used like in broker backends. There will be many implementation as they are developed. There is implementation only for AMQP at the moment. I added Redis strategy to be implemented as soon.  The solution in @magmax 's PR is helped me a lot to implement it for AMQP. \n\nI saw, tasks are applying in schedulers. So I added a scheduler which is `HAScheduler`(High Available Scheduler). This one is checking whether current node is master or not according to chosen strategy. If current node is master defined by the chosen failover strategy, normal way is used. There is no change for that case. But, if the node is slave defined by the chosen failover strategy, beat will think, task is applied, but it won't actually be applied. This scheduler should be used to have high available celery beat instead of default `Scheduler` class.\n\nTo decide what strategy is used, broker backend is used by parsing `BROKER_URL` as default. This can also be configured with a celery config which is `CELERYBEAT_FAILOVER_BROKER_URL`. So you can use different broker for your celery cluster and beat failover strategy. If you don't give, it'll be using BROKER_URL.\n\nNow, you can run multiple celery beat as you want. One of them will be master. If master goes down, one among still up an running slave beats, will be new master by taking the throne with the way defined in the chosen failover strategy. \n\nI thought, beat code is not structured well. Because of failover strategy and its factory class, I just moved **celery/beat.py** into **celery/beat/**init**.py** and I implemented all these codes under **celery/beat** module. It can still be accessed by `from celery import beat`or `from celery.beat import *`. I hope that doesn't  cause another problem. If it does, I can change beat module name which I created.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "buckensl": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/2238", "title": " auto retry methods, fixes #1260 ", "body": "fixes #1260\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/48606776", "body": "Just for the mongo backend indeed. I merged in latest changes from master to resolve the conflicts\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/48606776/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "aristeinberg": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/2016", "title": "honor hijack setting for root loglevel", "body": "if CELERYD_HIJACK_ROOT_LOGGER is False, then, in addition to avoiding changing the handler, celery should also not change the root logger's loglevel setting.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ionelmc": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/1928", "title": "Add workhorse TaskPool.", "body": "Fixes #1919\n", "author_association": "OWNER"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/15475020", "body": "@jerrypy Hey, can you make a PR that fixes the docs?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/15475020/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304835", "body": "I added this to help with debugging #1785. I still think we should have it (it could indicate other problems).\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304835/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "cce": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/1913", "title": "Configuration to disable auto-declare queues in worker", "body": "We found that in our use case \u2014 binding to hundreds or thousands of queues, all of which have already been declared by the publisher or by other setup code \u2014 skipping the auto-declaration of queues can speed up worker startup time considerably by avoiding unnecessary round trips to call the AMQP `queue.declare` method for each queue.  This helps us get new capacity up and running faster when demand increases.\n\nDoes this look good to you?  Any advice or suggestions are welcome, this has only been lightly tested so far.  Usage is to set `CELERYD_DISABLE_AUTO_DECLARE` to True in your celery config.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kalefranz": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/1897", "title": "ClassTask: a metaclass that turns any generic class into a celery task", "body": "I would like to submit this pull request for _feedback only_ right now, with the possibility of merging into `celery:master` after further work. It is not currently ready to integrate into the celery code base (although as you'll see, all tests pass and I don't think it would create any issues).\n\nMy objective here was to create some type of class modifier (mixin, decorator, metaclass, etc) that turned the _entire_ class into a task.  A small example might best illustrate what I want.\n\n```\nclass WelcomeEmail(ClassTaskMixin, EmailBase):\n\n    def __init__(self, recipient_address, template_name, template_context):\n        self.recipient_address = recipient_address\n        self.template_name = template_name\n        self.template_context = template_context\n\n   def send(self):\n       self.render_templates()\n       self.construct_mime()\n       self.archive_to_db()\n       return self.send_smtp_email()\n\n   def run(self):\n       return self.send()\n```\n\nThe `WelcomeEmail` class would then be used as\n\n```\nwe = WelcomeEmail(recipient_address, template_name, template_context)\nwe.enqueue()\n```\n\nwhere the `enqueue` method is provided by `ClassTaskMixin` and is analogous to the celery `delay` or `apply_async`.  Here, since the `run` method redirects to `send`, `enqueue` would simply send the email in the asynchronous process.\n\nIt's important to note that one of my goals was/is **delayed initialization of the instance object**.  That is, the python standard call to `__init__` should be \"lazy\" and **not** called when the instance is created.  This is important.  With a _lazy_ `__init__`, the user need not worry about \"heavy\" and duplicitous work being done in `__init__` (duplicitous because without lazy `__init__` the work would be done in both the synchronous and worker processes).  In the `WelcomeEmail` example, the coder may have for whatever reason put the calls to `render_templates` and `construct_mime` within `__init__`; and here, calling `render_templates` and `construct_mime` in the synchronous process is wasteful.\n\nNow, I am aware of `celery.contrib.methods`, and after the above explanation, you can see that what I'm after is a bit more extensive.\n\nI believe the pull request I am submitting here accomplishes my goals.  Rather than a class mixin, it was necessary to create a metaclass to accomplish the lazy initialization.  The two files in this pull request contain the `ClassTask` metaclass and a retrofitting of the first 12 of 32 tests for `celery.Task` (in `celery/tests/tasks/test_tasks.py`)\n\nI am _very_ much interested in community input.  This code will likely be used in production for my employer.  And if the community thinks this might be more broadly useful, I am happy to contribute it to the celery library.\n\nSome questions to start with...\n1) Are there better/more direct ways to test the `ClassTask` code than be retrofitting/duplicating what's in `celery/tests/tasks/test_tasks.py`?\n2) For the methods in `ClassTaskBase` that are almost nearly just a call to `super` (_i.e._ `apply_async`, `apply`, `retry`, etc.), is there a better way to avoid the wrote code?  My guess is there will be even more of these methods once I work through all of the applicable unit tests.\n3) Any general comments/critiques/criticisms/questions/insights to help me make this code better?\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gcachet": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/15549", "body": "It's master. I figured out this way to define tasks.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15549/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15557", "body": "Thanks! Removing yadayada dependency was my first though also, but I wasn't sure about the implications.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "talentless": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/18501", "body": "No problem!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18501/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "tobycatlin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/18135", "body": "pip installed ok. I haven't tried the source code out of git. \n\nThanks for the quick response\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18135/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "brosner": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/18434", "body": "My best guess is we've called terminate more than once? Will need some investigation.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "jonatkinson": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/34760", "body": "How do I find that out? If it helps, I'm using Debian Lenny Python packages, which reports: Python 2.5.2 (r252:60911, Jan  4 2009, 21:59:32) \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/34760/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "e98cuenc": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/33939353", "body": "This issue is still present in Celery 3.0.22. If I send a HUP signal to celery it dies with the error message:\n\n`[2014-02-03 10:27:28,870: WARNING/MainProcess] ERROR: Pidfile (/opt/var/pid/thumbrit/celery-thumbrit.pid) already exists.\nSeems we're already running? (pid: 22313)`\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/33939353/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "nitzanm": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/4427308", "body": "This doesn't seem to work when Celeryd is run with a pidfile (`python manage.py celeryd --pidfile=/var/run/celeryd.pid [...]`).  I send SIGHUP to the supervisor, it does indeed do a warm shutdown and re-exec itself, but apparently the new supervisor starts up before the old one dies, so instead of going about its business, it complains `ERROR: Pidfile (/var/run/celeryd.pid) already exists. Seems we're already running? (PID: 13214)` and dies.  Or maybe I'm missing something?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/4427308/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ivirabyan": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/5065078", "body": "This isn't specific to supervisord. It can be reproduced by running celeryd detached, with pidfile enabled, and the sending SIGHUP to the main process. I've made a fix for that: https://github.com/ask/celery/pull/662\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/5065078/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "gonzalodiaz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/72083732", "body": "Hi, @ask how did you solve this? is there a way to set basic.qos from celery? \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/72083732/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "abecciu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/45248", "body": "This is a great idea! Also, it would be nice to have a _init_tasks_ method (just like init_periodic_tasks) to setup indexes and other data store settings for the regular tasks.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/45248/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "bradjasper": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/50885", "body": "This one fixes it.\n\nThanks.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/50885/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "miracle2k": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/69876", "body": "My patch for now is hard-wiring the code to load \"settings\", and a roadmap to switch to Ubuntu ;)\n\nI don't really know how to best approach this. Of the top of my head:\n- When importing DJANGO_SETTINGS_MODULE fails, simply try a hardcoded \"import settings\", as manage.py would do (this is not full-proof either though, because a user might have modified manage.py to import a different module).\n- Pass along the actual path to the settings module via an environment option. E.g. the parent process would need to set os.environ['TEMP_CELERY_SETTINGS_PATH'], and the child process could pick it up and could import the file through path hackery (add containing directory to path, import the module, remove containing directory again).\n- I'm not familiar with multiprocess - can one pass arguments to the child process via command line? It'd be a Windows-only thing, so I guess not.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/69876/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/70617", "body": "Windows certainly has os.environ, and it is passed along (that how the current hack around the lack of fork seems to work - asking the child process to set it self up again using the Django settings module environment variable.\n\nMaybe I can wip something up.\n\nWhat should the semantics of CELERYD_USE_DJANGO be, precisely?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/70617/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/112753", "body": "Sorry, I am actually on Ubuntu now.\n\nJust from reading the code in this changeset I'm not sure if the core issue doesn't still exist: When manage.py is used, DJANGO_SETTINGS_MODULE will end up as \"project.settings\". manage.py, through the setup_environ() call will modify Python's sys.path to make the \"project\" namespace importible in the first place.\n\nIf child processes are forked, as on *nix systems, then this sys.path modification will persist to the children; on Windows, were new processes are spawned, the child start off with a fresh sys.path, and still try to import \"project.settings\". This won't work, since the parent directory of \"project\" is not on the path, i.e. importing \"settings\" works.\n\nMaybe the best way to solve this that I can think of right now is to pass along, through os.environ, the actual path to the settings file. The child process can then reconstruct what manage.py did in the parent process: import that module, and call django.core.management.setup_environ() with the module object.\n\nPossibly do this ONLY when the celeryd Django command is used, since in other cases, the path manipulation that Django's manage.py is doing internally isn't going on and the user has to deal with making sure that the proper  DJANGO_SETTINGS_MODULE and Python paths are configured.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/112753/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/88553", "body": "Yeah, I guess I was rambling a bit in this one.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/88553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/88976", "body": "Thanks; I was planning on adding a ticket, I promose. Had the tabs open for the whole days.\n\nIt's a debian lenny server with Python 2.5. I was assuming \nThe file is definitely created everytime I  start celeryd (using the init.d script), but neither a process or a thread with the given PID seems to be active, afterwards.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/88976/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/88982", "body": "Hm. Managed to extract a stacktrace via strace. The source seems to be the lockfile external package:\n\nhttp://www.google.com/codesearch/p?hl=en#Q0_z8mLy5P4/trunk/mailerdev/mailer/lockfile.py&sa=N&cd=1&ct=rc&l=171\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/88982/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/88985", "body": "In case the link above doesn't work, it's lockfile.LockBase.**init**.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/88985/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/88998", "body": "Ok, trying hard not to spam this ticket too much, but that indeed seems to be the reason:\n\ndaemon.py first detaches, i.e. is switching to the configured user id first, then tries to create the lockfile (which consists of a dummy HOSTNAME.MainThread-PID, and then creating a link to it). \n\nThe problem then is not the file itself, but generally that it attempts to write to /var/run as a normal user. Or maybe it isn't a problem? I'm not familiar enough with unix to tell.\n\nIt's just that /var/run by default isn't writable for non-root users; other apps like say lighttpd seem to run fine as www-data, and still have a pidfile written as root.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/88998/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/89315", "body": "I am using the debian init script that comes with celeryd, which as far as I can tell merely starts up the celeryd process; that will be done as root, and it works; it's just that I'd _like_ to run as an unprivileged user, so I am passing the --uid/--gid options.\n\nPer the above, that doesn't currently seem possible, at least unless I specify a pidfile location the unprivileged user can write to, as you suggest. I can do that, but clearly, it would be nicer if it would just work with /var/run, without requiring any setup.\n\nSo from what you said I understand the correct solution for celeryd would be to first create the pidfile, then switch to the unprivileged user. That doesn't seem to be the way the python-daemon package works though...\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/89315/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/89405", "body": "Thanks, I'll give that a try.\n\nI just had a look at lighttpd, which runs as www-data with no parent process, and the init script's stop command indeed does a \"if start-stop-daemon then rm -f $PIDFILE\".\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/89405/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/89748", "body": "Running as an unprivileged user works fine now; I have had some issues when trying to shutdown the service again, for which I opened #54.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/89748/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/90581", "body": "First, there was a name error, which I fixed in miracle2k@ 9afb521f59295d98727863fc5233ba8857026a9d.\n\nSecond, a permission denied error occurred on stop when calling into \"LinkFileLock.release\" from inside the custom SafeRemovePIDLockFile wrapper. LinkFileLock still tries to remove the lock file and the unique_name file by itself, and fails. I tried to fix this in miracle2k@2ef768222b344cc4e9f0f28d81704706ae35a7ff.\n\nFinally, at this point the pid files remain after shutdown, so I updated the init script to delete them on stop: miracle2k@b38e15d3a0c46fa37242350ccb93903d48414533-\n\nPotentially the only problem left now is the fact that the \"unique_name\" files (host.MainThread-PID) remain. They are too created as root, but their name is hard to guess from the init script. They are also not deleted on startup, so multiple start/stop cycles cause them to pile on. Note a big deal really, since /var/run seems to be a tempfs on most modern systems anyway I suppose, but I thought I would mention it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/90581/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/90825", "body": "Yep, seems to work fine for me now. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/90825/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "mdomans": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/77036", "body": "This is related to how python memory managment interacts with django. When python allocates large amount of memory for django based models, even after all references to those objects are removed, memory won't get released. Have little time to investigate, but this may be problem related to the AppCache facility.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/77036/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/77132", "body": "That's usually the problem whe you iterate over large datasets and would like to process django model instances with finite amount of memory. For example when parsing data to xml for some external service.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/77132/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ducky427": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/74887", "body": "The reason, I didn't want to use the join method of TaskSet is that it is a blocking operation and making the system very much like a Remote Procedure Call type of system. I mean, you send the jobs and then the program sending the jobs waits for them to finish. \n\nMy other option is to store the TaskSet id and poll at regular intervals to check if that TaskSet is done.\n\nbtw, Carrot and Celery are awesome!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/74887/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/74909", "body": "True. I'll go back to my drawing board.\n\ncheers.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/74909/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "jezdez": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/84043", "body": "@ask Actually you can't run python setup.py develop in a Django checkout since it uses distutils in the setup.py and the develop command is a setuptools feature.\n\nAlso, IMO, you should only have Django in the install_requires parameter if Celery _really_ needs Django to work properly. If not it's just an optional requirement you should mention in the installation documentation. You could also provide a pip requirements file (think \"Do pip install -r http://celeryproject.org/1.0/reqs.txt\") to help users to satisfy the Celery requirements quickly.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/84043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/84047", "body": "Ah, one more thing: I, too, was annoyed when I typed \"pip install celery\" and got Django installed automatically -- not to speak of the \"django-unittest-depth\" package which uses the obscure \"djangox\" namespace.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/84047/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/84075", "body": "Well, of course is a symlink not as good as using an installer. But shouldn't the user of celery be able to decide how a requirement is installed? Whether by using buildout, a pip requirements file (in a virtualenv or not) or even with system package management? It's a good idea to recommend a way to fulfill the requirements, but forcing it by using the setup.py based solution seems like a dead end to me (even with setuptools' extras).\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/84075/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/84091", "body": "Thanks for fixing the django-unittest-depth issue :)\n\nsetuptools defines a package as \"installed\" if it finds the egg info (aka metadata) of a \"Distribution\" of that package/module. That's why running \"python setup.py develop\" will always create the egg-info data _and_ create a (pseudo) symlink. (Btw, more details on setuptools' concepts: http://blog.ianbicking.org/2008/12/14/a-few-corrections-to-on-packaging/)\n\nThat's why I think adding Django to install_requires only if it can't import it, won't really help. Tools like pip and buildout solve that part of automation much better than using the plain setup.py, IMHO.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/84091/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/84392", "body": "Look, I'm just saying that while celery might need an \"easy\" way to make sure all dependencies are installed, I made the experience that tracking dependencies in the setup.py of the package is orthogonal to what you really want: making sure the current environment of the user is prepared for celery. Unless you want to make wild guesses about that environment by manually looking for Django I'd like to encourage you to simply tell your users what to do instead of relying on magic. I'm sure they'll appreciate your openess and wouldn't mind the small extra step of using a buildout.cfg or requirements.txt, something which they probably already use for their own project. :)\n\nAlternatively, this should make the check for Django on install time bullet-proof:\n    try:\n        pkg_resources.get_distribution('Django')\n    except pkg_resources.DistributionNotFound:\n        install_requires.append('Django')\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/84392/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "d0ugal": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/84034", "body": "I think it should be in as a dependency. After all, that's exactly what it is.\n\nThis seems to be a problem with the setuptools/django or server setup. Setting up django via a symlink seems like a backwards way to do things... if your going to have a special case setup like that you must be prepared to handle bugs in svn versions and digging into internals so its presumably more realistic to imagine those users would modify the requirements?\n\nHowever, I do think its almost 'a given' that django will be setup already as most people will come to celery because of Django and not the other way around. Having said that, it means we should make it as easy as possible for users coming to celery with no django experience.\n\nAs another thought is there a way we can add something that asks if they want to install Django? or add an argument that skips Django.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/84034/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "kmike": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/83933", "body": "If you can use celery without django then there is no need to install django for celery? :)\n\nThe issue with django SVN version is that it is not installed and managed by setuptools so setup.py install_requires installs django (last release) even if django SVN is already installed via symlink. \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/83933/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/109531", "body": "In the same page it will be good if 'Writing Tasks' will be a link.\n\nBig thanks for celery!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/109531/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/234539", "body": "As for process spawning: setproctitle and watching for process ids was helpful. It is not process spawning. Worker processes remain alive when parent process is killed. \nThis is a simulation of supervisord restart with manual killing and zero timeout:\n\n```\n 4976 ?        Ss     0:00 /usr/bin/python /usr/bin/supervisord --pidfile /var/run/supervisord.pid\n 5422 ?        S      0:01  \\_ [celerybeat] --schedule=/var/lib/celery/celerybeat-schedule-nadovmeste --loglevel=INFO                                                             \n 6101 ?        Sl     0:00  \\_ [celeryd.MainProcess] Running... (--loglevel=INFO)                                                           \n 6108 ?        S      0:00      \\_ [celeryd.PoolWorker-1]                                                                                       \n nadovmeste:~# kill 6101 & kill -9 6101 &\n```\n\nps -afx:\n\n```\n 4976 ?        Ss     0:00 /usr/bin/python /usr/bin/supervisord --pidfile /var/run/supervisord.pid\n 5422 ?        S      0:01  \\_ [celerybeat] --schedule=/var/lib/celery/celerybeat-schedule-nadovmeste --loglevel=INFO                                                             \n 6867 ?        Sl     0:00  \\_ [celeryd.MainProcess] Running... (--loglevel=INFO)                                                           \n 6875 ?        S      0:00      \\_ [celeryd.PoolWorker-1]                                                                                       \n 6108 ?        S      0:00 [celeryd.PoolWorker-1]       \n```\n\nI was able reproduce this only with such artifical race between `kill` and `kill -9`. Sometimes worker gets killed properly. The issue seems to be supervisord-specific because when I start celeryd from console I have no luck reproducing it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/234539/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/234553", "body": "I was able to reproduce this with console-started scripts after several attempts:\n\n```\n/home/nadovmeste/envs/nadovmeste/bin/python /home/nadovmeste/src/nadovmeste/manage.py celeryd -B --loglevel=INFO&\n```\n\nand then in another terminal session:\n\n```\nnadovmeste:~# ps -afx\n\n 6450 ?        Ss     0:00  \\_ sshd: root@pts/2 \n 6452 pts/2    Ss+    0:00      \\_ -bash\n 9343 pts/2    Sl     0:00          \\_ [celeryd.MainProcess] Running... (-B --loglevel=INFO)                                                           \n 9350 pts/2    S      0:00              \\_ [celeryd.PoolWorker-2]                                                                                          \n 9355 pts/2    S      0:00              \\_ [celerybeat]     \n\nnadovmeste:~# kill 9343 & kill -9 9343\n\nnadovmeste:~# ps -afx\n\n 4526 ?        Ss     0:00  \\_ sshd: root@pts/1 \n 4529 pts/1    Ss     0:00  |   \\_ -bash\n 9366 pts/1    R+     0:00  |       \\_ ps -afx\n 6450 ?        Ss     0:00  \\_ sshd: root@pts/2 \n 6452 pts/2    Ss+    0:00      \\_ -bash    \n ...\n 9350 pts/2    S      0:00 [celeryd.PoolWorker-2]                                                                                          \n 9355 pts/2    S      0:00 [celerybeat]\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/234553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/234554", "body": "Haven't found any special option for infinite timeout with warning in supervisord docs. Probably very large number will suffice if it is what we want.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/234554/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/234557", "body": "Maybe it's something related to celerybeat because the I was able to reproduce the issue for console-started celeryd only after using the `-B` option.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/234557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/241450", "body": "Yes, I'm running the latest master branch. I saw your bug-fixing commit and hoped that it will help but it seems that it doesn't help in my case: the latest celery seems to behave the same. But it is possible that the initial problem is solved - I check this only with an immediate kill. Can't wrap my hand around it now :) The ctrl-c issue is not reproducible with my setup.\n\nSo the bug report, simplified: http://gist.github.com/401028 . The results are always the same (not sometimes). I have some periodic tasks and some non-periodic. Tasks are simple and don't take much time to finish. Is it a bug that children processes stay alive after killing the main process? If so and you can't reproduce it then I'll try to provide the minimal project.\n\nThe celerybeat killing behaviour is interesting: when I kill hanging(?) celerybeat process the hanging(?) worker process also shutdowns.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/241450/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/242338", "body": "I thought that the main process was killed: it is not visible in process list. Don't have much experience with process management though.\n\nMy setup was Debian Lenny + python 2.5. \n\nI'll try to run celeryd with --loglevel=DEBUG and to reproduce it on my macbook. \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/242338/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/242353", "body": "Ask, thank you for help.\n\nI think that initial problem was solved with increased supervisord timeout and your bug-fixing commit. The simulation was incorrect because I use `kill -9` commands and they send KILL signal instead of TERM. With TERM signal processes are getting killed properly.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/242353/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/242354", "body": "Supervisord use TERM signal so all should be fine.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/242354/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/242357", "body": "But the thing that scares me a bit is that the initial bug wasn't investigated. I'll try to reproduce it and let you know.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/242357/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "jasonbaker": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/84055", "body": "Django is required to run celery.  I would recommend _installing_ django in a virtualenv or using buildout.  Having something as fragile as a symlink in your global site-packages directory is just asking to have something clobber it.\n\nI suppose one alternative is to have an extra that installs django:  http://peak.telecommunity.com/DevCenter/setuptools#declaring-extras-optional-features-with-their-own-dependencies.  This way, you can do easy_install celery[django].\n\nPersonally, I think that django should be installed by default, but that would be an acceptable alternative.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/84055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/84106", "body": "I agree with ask. It's a hacky workaround to be sure, but it looks to be the best solution for us right now. Perhaps when distribute becomes more popular, we can fix some of the deficiencies that setuptools has.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/84106/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "dmishe": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/87723", "body": "is it important to use --detach on production? Maybe i can just use interactive...\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/87723/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/89338", "body": "```\nHave you seen anything about this bug anywhere else? (multiprocessing + MemoryError)\n```\n\ni did some search on internets, but nothing concrete, it really could be internal python on sunos bug :/\n\n```\nThat is, you probably want it running in the background, not necessarily using --detach\n```\n\nthanks, i'll try that\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/89338/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "thruflo": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/88064", "body": "tx ask, one of the few occaisions when it's not just me going mad ;)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/88064/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "evanbeard": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/107812", "body": "I tried on both the stable branch and the most recent code and both generated the error. I found a work-around, I am deploying / daemonizing celery with nohup: \"nohup python manage.py celeryd &\" and everything works perfectly.\n\nDo you have an email address to which I can send my IMAPFetcher class? Hopefully with that class you can replicate the error.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/107812/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "twillis": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/117270", "body": "I'm throwing in the towel on this. I feel like I am fighting too many conventions here. So, I've thrown what I have up on my account on bitbucket if someone with more python-fu thinks this is a worthy task to pursue. http://bitbucket.org/twillis/celery-paste/src/\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/117270/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/117683", "body": "ask: thanks so much. after briefly looking at the code it doesn't look like I was that far off. I'll look at it more closely over the next couple of days and get back to you,\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/117683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "aaronelliotross": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/138781", "body": "Verified! And I haven't forgetten, I'm just still working on the test.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/138781/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "matclayton": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/132283", "body": "this was being caused by having a default queue on a machine which didn't have that queue in the CELERY_QUEUES list. Although not as critical, we would still like to be able to have a machines default tasks run on a queue not being serviced by that machine, which as I understand it isn't currently possible?\n\nMat\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/132283/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/132284", "body": "closing the issue\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/132284/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/138669", "body": "basically if a machine sets a default queue which isn't in the queues list for that machine, the error message was confusing me.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/138669/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dgoldenichec": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/136243", "body": "Ah, this is because of \"celerybeat-schedule\" I think.  celeryd -B wants to make it, and so it needs to be somewhere it has write permissions (django_project_dir might be suboptimal, it happened to work for my test case because the user I was testing the new celery as was the owner of the project dir).   The independent celerybeat initscript can presumably be amended to use `--schedule`, but `celeryd -B`, if you use it, doesn't seem to allow specification of path to the file.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/136243/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "reverie": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/142063", "body": "If that's how init scripts always work, then it isn't a bug. My manage.py wasn't executable for some reason. I just tried django-admin.py startproject, and the file is created with +x by default. Maybe I lost it through some VCS mishap. I suspect that many people using celery won't be that familiar with init configuration, so it may be worth adding a note to the docs.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/142063/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "winhamwr": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/145051", "body": "In the case where there are more settings than cores, do you think celery should ignore the default --concurrency and open up that many works anyway?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/145051/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/145934", "body": "Running multiple celeryd processes would be kind of a pain as far as operations overhead, but I can see how it would be difficult to change the implementation for one master to feed different queues to workers.\n\nWould it be easier to change the celery behavior in this regard or write a cross-platform utility that does the same job as mysql_multi? Just adding the command line options isn't bad, but from a practical standpoint, it's not much better than having two settings.py files when it comes to run this setup in production, at least from my perspective. Maybe there are other people who are much better sysadmins who would know how to manage that kind of setup, though. Am I missing something there?\n\nI am of course sensitive to things that are difficult to implement, I'm just wondering which approach is going to  be more consistent.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/145934/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/213677", "body": "It would also be nice when fixing this bug to change the example to a Task that accepted keyword arguments itself. Took me a little bit to figure out what I needed to do, at least: \nhttp://github.com/winhamwr/mixpanel-celery/commit/2454ee51c8f7adf811fb0e637dde82faae78904c\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/213677/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "jkozera": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/145588", "body": "+1\n\nWe need something like this for our application, at least I think so.\n\nMy reasons for needing it:\n\nWe have many kinds of tasks here, some of them are very time-consuming and create lots of smaller tasks which fill up the processes pool and don't allow new tasks to execute. And there is a kind of task which should be executed as soon as possible, so doing that bunch of tasks and locking whole pool is bad here.\n\nAnother thing is a huge kind of task consisting of two big subsets of tasks - one subset is IO-bound, second one is mostly CPU-bound, and the first one is sent to queue as first one. Because it doesn't consume CPU, all these IO-bound tasks just waste whole pool for waiting for IO while not allowing CPU-bound tasks to execute.\n\nAll these problems could be solved by running multiple celeryd instances with different queues, but I think it would be neat to have it solvable by configuring one celeryd. I see two possible solutions:\n1. The one outlined here - allocating specific queues to specific workers\n2. Another idea - allowing to specify a limit on a number of tasks running from specific queue in one process pool. Then we could spefify something like CELERYD_CONCURRENCY = num_cores + 2 and make IO bound tasks limited to 1 core, and all CPU bound batch tasks to num_cores which would always leave one process left for 'real-time' tasks.\n\nLet me know what you think about it, I would like to write a patch for Celery to implement one of these solutions.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/145588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/145652", "body": "I actually thought you can mark IO-bound (or even network-bound/HD-bound) and CPU-bound tasks as being such by putting them in separate queues.\n\nBut we still need to have some way to separate queues between pools, don't we?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/145652/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "timbull": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/145073", "body": "It does open some areas for considerations regards default behaviour.  \n\nJust dealing with the instances where CELERY_WORKER is specified (otherwise use existing defaults), I can see a few scenarios you'd need to make some decisions (ignoring the ones where the two options are equal which is obvious).\n1. CELERY_WORKER > Default\n2. CELERY_WORKER < Default\n3. CELERY_WORKER > CELERYD_CONCURRENCY\n4. CELERY_WORKER < CELERYD_CONCURRENCY\n\nAfter thinking about what the easiest approach here would actually be (what makes most sense for the user), I think that the easiest use case to explain would be to simply say CELERY_WORKER takes precedence over CELERYD_CONCURRENCY/\n\nThis would keep it very simple - if you specify 3 CELERY_WORKER you get 3 and CELERD_CONCURRENCY (or it's default) could just be ignored.  Perhaps if they both existed and didn't match, you might get a warning.\n\nIt makes sense to me, because if your going to the effort of splitting up the queues to workers, you are making some very deliberate decisions.\n\nIf you don't use CELERY_WORKER, then you can use CELERYD_CONCURRENCY to just get the normal behaviour.\n\nWhat do you think?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/145073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/145904", "body": "@ask Thanks for that clarification, makes a lot more sense to me know.\n\nI don't see a problem with having the celeryd process specify the queues and the workers just taking tasks from their related \"master\".\n\nOn the assumption I could run multiple celeryd on the one server, this would do exactly what I had in mind, and without needing a new setting which is always good :-)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/145904/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "tjulien": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/214280", "body": "could this be the same cause (celery 1.0.2):\n\nhttp://dpaste.com/185555/\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/214280/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "flaper87": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/525385", "body": "@ask \n\nimport sys\nsys.platform.startswith(\"java\") could be a good way to recognize if we're running celery under jython. This is the method used by pip.\n\nI'll maybe start working on a jython threadpool implementation for celery. Does any of you have done something ? If not I'll have to to start it from scratch.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/525385/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "pordonez": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/346817", "body": "Not sure if this helps, but I was able to get Celery working sufficiently enough on Jython to send requests (e.g., send_task(\"tasks.add\", [1, 2])) to a celeryd hosted on Python.\n\nAfter experiencing the multiprocessing error when installing Celery on Jython, I proceeded to manually install the other Celery dependencies, which I discovered via trail and error.  The following is the list of dependencies that I needed to manually install:\n\nimportlib-1.0.2-py2.5.egg\ncarrot-0.10.5-py2.5.egg\namqplib-0.6.1-py2.5.egg\nanyjson-0.2.4-py2.5.egg\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/346817/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/348434", "body": "@ask, Excellent Pika use suggestion.  There's a reasonable discussion about detecting jython here: http://stackoverflow.com/questions/1103487/can-i-detect-if-my-code-is-running-on-cpython-or-jython.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/348434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "jbalogh": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/228181", "body": "Ok, the root issue: when I call `control.rate_limit`, nothing changes.  My task keeps churning at whatever rate I gave it in the `@task` decorator.\n\nThe rate_limit in registry gets updated, but it seems like the TaskBuckets would have to get updated for the change to have an effect.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/228181/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/230546", "body": "Thanks for taking a look.  I got a traceback when I tried to change the rate limit: `AttributeError: 'NoneType' object has no attribute 'ready_queue'`.  The full traceback is at http://paste.pocoo.org/show/210184/.\n\nI sent pull requests for carrot and celery that fix the issues.  With those patches applied, remote rate limiting works for me.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/230546/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "digi604": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/227985", "body": "If i reduce concurrency to 1 and start with -B i get 3 processes.\nthe beat process uses 16MB\nthe master uses: 18 MB\nand the task uses: 19 MB\n\nmaybe there gets too much imported? Probably the whole celery stack. Maybe even django? Though its probably not used for the beat and task process....\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/227985/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/240349", "body": "If i am testing some celery tasks locally and i use the -B option sometimes the process is not killed when i used ctrl-c.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/240349/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "jonozzz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/345565", "body": "BTW each of my workers uses 110MB and I have 8 of them but I guess that's just python. I can't do much about it :( That's why I've suggested a while ago, to use stackless python (http://www.stackless.com) for multithreading, maybe that way the workers could share some of that memory.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/345565/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/893766", "body": "I just upgraded to 2.2.4, switched to eventlet workers pool and I'm impressed! I like how I can balance between memory and CPU usage. Obviously this is a bit more CPU intensive vs. the multiprocess approach which can toss the workload over multiple cores, but for me the memory usage was more important.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/893766/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/248735", "body": "When Task2 is executed non-async (apply()):\n{'task_name': 'test123.revtest.tasks.Task2', 'task_id': '5c1e3a06-64b9-4184-aefe-7e7531437018', 'loglevel': 0, 'delivery_info': {'is_eager': True}, 'task_is_eager': True, 'logfile': None, 'task_retries': 0}\n\nWhen Task2 is executed async (apply_async()) -- logging works as expected:\n{'task_is_eager': False, 'task_id': 'e9702ae1-18b5-4569-b799-ecf879d6273f', 'loglevel': 20, 'delivery_info': {'consumer_tag': None, 'routing_key': None, 'exchange': None}, 'task_name': 'test123.revtest.tasks.Task2', 'logfile': None, 'task_retries': 0}\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/248735/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/250905", "body": "I got a fix. In celery.execute.apply(), instead of:\n    \"logfile\": None,\n    \"delivery_info\": {\"is_eager\": True},\n    \"loglevel\": 0}\n\nuse:\n    \"logfile\": options.get('logfile'),\n    \"delivery_info\": {\"is_eager\": True},\n    \"loglevel\": options.get('loglevel', 0)}\n\nTell me what you think.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/250905/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/91703", "body": "I don't quite understand this one... why \"connection\" and why \"or\" ?\nI think it should be:\n    conn and conn.close()\n\nBecause when connection is None, conn.close() will be executed anyway.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/91703/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "washeck": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/229299", "body": "I don't know how to attach a path to github, so please see http://dpaste.com/190670/\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/229299/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/230401", "body": "OK, I forked it, committed the change and sent pull request to you.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/230401/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/230433", "body": "No, we tried already before. It does not perform django validation, so the modules are notloaded.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/230433/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/231956", "body": "I added the above code after `setup_environ` in `worker/__init__.py` and I always get validation error:\n\n```\nTraceback (most recent call last):\n  File \"C:\\Python26\\lib\\multiprocessing\\process.py\", line 232, in _bootstrap\n    self.run()\n  File \"C:\\Python26\\lib\\multiprocessing\\process.py\", line 88, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Python26\\lib\\multiprocessing\\pool.py\", line 53, in worker\n    initializer(*initargs)\n  File \"C:\\Python26\\lib\\site-packages\\celery\\worker\\__init__.py\", line 48, in process_initializer\n    raise Exception(\"One or more models did not validate:\\n%s\" % ( errorlog.read ()))\nException: One or more models did not validate:\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/231956/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/231961", "body": "If you have access to a Windows box, you can easily reproduce the django loading bug.\n\nI took the sample project http://bitbucket.org/richleland/celery-test-project and modified it:\n\n1) add `cars` application with the following models:\n    from django.db import models\n    from people.models import Person\n\n```\nclass Car(models.Model):\n    owner = models.ForeignKey(Person) \n```\n\n2) change line `people/tasks.py` to\n    person = Person.objects.get(pk=person_id,car__isnull=True)\n\nceleryd 1.0.2 will throw an exception about `car` field missing. It works OK after applying my hack.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/231961/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "haridsv": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/245797", "body": "Here is a rough backend implementation using SQLAlchemy, in case it helps to speed the implementation of this issue.\n\nhttp://github.com/haridsv/celery-alchemy-poc\n\nI was able to use this code to execute tasks with celery running over SQL Server using pyodbc/FreeTDS on linux. To use this code, after pulling this repo, change alchemy_backend.py for the right connection string. The model doesn't currently work with sqlite.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/245797/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/247952", "body": "Nice to see this moving along. What is the plan for queue backend? Will that change be done directly to ghettoq?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/247952/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/82469", "body": "Is this creating a new engine every time a new session is needed? As per sqlalchemy documentation, engine should be created only one time, unless of course the connection URL itself is changing.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/82469/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/82478", "body": "Yes, the engine by default has a connection pool enabled: http://www.sqlalchemy.org/docs/05/reference/sqlalchemy/pooling.html#connection-pool-configuration\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/82478/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "rafaelpivato": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/77150136", "body": "In case you are still having issues terminating your Celery workers, you might try setting `stopasgroup=true` before increasing your `stopwaitsecs`.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/77150136/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dfdeshom": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/241206", "body": "A quick fix for this bug is here: http://gist.github.com/400869\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/241206/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/241565", "body": "Thanks for your quick response! You're right, returning the string representation of the dict will crash the celerymon side. \n\nBut now I'm having the same problem with celerymon. When you execute GET /api/task/ and the task information includes a python datetime object, you get a 500 error. The reason is the same: the API spits out json so it needs to convert whatever it's passed to to json.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/241565/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/241580", "body": "opening a new issue since this one is closed\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/241580/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/241593", "body": "Right after creating this ticket I realized that it belongs to celerymon. Sorry! \n\nMy quick fix for this was to encode the values of the dict passed to the celerymon into a string. See a diff here: http://gist.github.com/401223 . I'm sure something more sophisticated can be thought of.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/241593/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/242315", "body": "Excellent, works perfectly.Thanks!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/242315/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "nikitka": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/29856", "body": "why after **import** you use import celeryconfig ? this is work?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/29856/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "brettcannon": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/38473", "body": "Django as of (I believe) 1.1 has importlib included w/ it under django.util.importlib, so you don't need to rely on Python 2.6 to get import_module.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38473/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/38959", "body": "importlib was added in Python 2.6.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38959/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "paltman": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/79488", "body": "This is why the test now fails -- now that is no longer 29 minutes past the hour, but 30, it is now due to run, while the assertion was to make sure it handled the case when it wasn't due, which when the mocked value was 29 minutes past the hour it properly returned False.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/79488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/204916", "body": "Was there a test for this failure before the fix?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/204916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "nvie": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/81557", "body": "Nice and tidy.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/81557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "simonz05": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/103991", "body": "Nice, the previous one was aweful. Good to have the side-bar back.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/103991/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "Kami": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/104086", "body": "I agree, great work.\n\nThe blue (first?) version wasn't bad either, but the previous version was kinda step in the wrong direction.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/104086/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "adamn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/123743", "body": "$VIRTUALENV was removed - does it still work with a virtualenv?  It doesn't seem to for me on Ubuntu 10.04\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/123743/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/18510604", "body": "It seems like this whole section should be removed since django-celery is [no longer required](https://pypi.python.org/pypi/django-celery)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18510604/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "zen4ever": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/131915", "body": "Thanks for the fix. That was fast.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/131915/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "joshdrake": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/136184", "body": "Was just about to post an issue on this. I was surprised by this too, but even subclasses of accepted types must have adapters registered for database backends. Here's documentation on the process for pyscopg2:\n\nhttp://initd.org/psycopg/docs/advanced.html#adapting-new-types\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/136184/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "zzzeek": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/136191", "body": "you need to look at TypeDecorator:\n\nhttp://www.sqlalchemy.org/docs/reference/sqlalchemy/types.html?highlight=typedecorator#sqlalchemy.types.TypeDecorator\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/136191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "passy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/147340", "body": "The dot should be inside the comment. It's a syntax error otherwise.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/147340/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "dcramer": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/175143", "body": "I should note I have no idea what this does, I stole it form somewhere on the internet and it fixed the problems :)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/175143/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "shimon": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/270710", "body": "Why did the date_done entry get removed? This is useful information that other backends seem to provide.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/270710/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "enlavin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/352650", "body": "this should be _kill(...)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/352650/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "mher": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/595269", "body": "https://bitbucket.org/jezdez/sphinx-pypi-upload/issue/1/minor-patch-for-namespace-modules-etc patch should be applied before launching  paver upload_pypi_docs. upload_pypi_docs fails if .build contains empty subdirectories.\n\nIs there a way to automate this?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/595269/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/6615257", "body": "Fixed 2473f5d503d0829b878ba3828d05478d95b128a3\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/6615257/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/3502810", "body": "I think it would be better to move registry._set_default_serializer('auth') to setup_security\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/3502810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "gtaylor": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/1004140", "body": "Does this mean I'm famous now?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/1004140/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "tomprimozic": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/1166404", "body": "Which version of `kombu` do you assume?\n\nI can't get the line `from kombu.utils.url import _parse_url` to work, there is no `kombu.utils.url` module in `kombu`, or it has no `_parse_url` function.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/1166404/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "fernandogrd": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/1409682", "body": "Celery is getting even more simpler. Great work!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/1409682/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "huntrax11": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/1552456", "body": "Added argument for postrun signal, I mean [\"state\"](https://github.com/celery/celery/commit/e869e8a871b7bedde94a1db4d59c2a1fe2115f94#L1R229) above, should be reflected to [documentation](http://docs.celeryproject.org/en/latest/userguide/signals.html?highlight=postrun#task-postrun).\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/1552456/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "kkonrad": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/1573027", "body": "Did you meant _article_ instead of _model_object_?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/1573027/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "aidan": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/1652391", "body": "Sadly not :(\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/1652391/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "zllak": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/1776587", "body": "This was not part of my PR, but I see you fixed it after :)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/1776587/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "steeve": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/1782719", "body": "Forgot to update that, my bad :(\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/1782719/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/1782821", "body": "Is this normal that this is not in 3.0.7 ?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/1782821/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/1786775", "body": "@ask, shouldn't we pull this one in `3.0` ?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/1786775/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/1786778", "body": "As well at this one? @ask \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/1786778/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/112600", "body": "yes, this allows for the result backend to sub on this, allowing it not to poll\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/112600/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "marcinn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/2165204", "body": "But one thing - it seems that celery (billiard?) cannot reconnect somehow. Do you think that inserting sleep will fix that?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/2165204/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "glinscott": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/2611849", "body": "Starting with this commit, I get an exception when starting a worker.  If I sync back to 785a28fa9b1bcd842fc331a0f163573a292bed66 it works fine.\n\nTraceback (most recent call last):\n  File \"/home/user/celery/celery/worker/**init**.py\", line 200, in start\n    self.namespace.start(self)\n  File \"/home/user/celery/celery/bootsteps.py\", line 119, in start\n    step.start(parent)\n  File \"/home/user/celery/celery/bootsteps.py\", line 352, in start\n    return self.obj.start()\n  File \"/home/user/celery/celery/worker/consumer.py\", line 195, in start\n    ns.start(self)\n  File \"/home/user/celery/celery/bootsteps.py\", line 119, in start\n    step.start(parent)\n  File \"/home/user/celery/celery/worker/consumer.py\", line 572, in start\n    super(Gossip, self).start(c)\n  File \"/home/user/celery/celery/bootsteps.py\", line 380, in start\n    self.consumers = self.get_consumers(c.connection)\n  File \"/home/user/celery/celery/worker/consumer.py\", line 625, in get_consumers\n    ev = self.Receiver(channel, routing_key='worker.#')\n  File \"/home/user/celery/celery/events/**init**.py\", line 223, in **init**\n    exchange=self.exchange,\nAttributeError: 'EventReceiver' object has no attribute 'exchange'\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/2611849/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/2615287", "body": "Great, thanks!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/2615287/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "jbeard565": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/2744809", "body": "Yep, that seems to fix it!  \nThanks\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/2744809/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "guettli": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/2758787", "body": "Thumbs up for this fast update. Thank you.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/2758787/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "declanshanaghy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/2809368", "body": ":+1:\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/2809368/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "udoprog": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/3096030", "body": "pretty sure this is a mistake :)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/3096030/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "extesy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/3328644", "body": "@ask When do you expect the final release, if you don't mind me asking?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/3328644/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "mikej165": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/4253172", "body": "That did the trick. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/4253172/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "aspyatkin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/4933604", "body": "Starting celeryd and celerybeat with new startup script fails on CentOS 6.\nSeems that command 'stat -Lr' isn't right\nI used 'stat -Lc%u' and 'stat -Lc%a' (without awk) to retrieve /etc/default/celeryd or /etc/default/celerybeat config file owner and permissions. \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/4933604/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/4934523", "body": "Yes, -r option is not supported.\n\n`stat -Lr /etc || echo $1`\nstat: invalid option -- 'r'\nTry `stat --help' for more information.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/4934523/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "tahajahangir": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/5108126", "body": "This means that current script only works on BSD/OSX (not linux)?!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/5108126/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "pepijndevos": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/5117693", "body": "Nice! How do I use this?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/5117693/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/5118895", "body": "I get that, but what do you do with the resulting backend instance? \n\nI ended up with\n\n```\nclass MyCelery(Celery):\n    def _get_backend(self):\n        return celery.backends.redis.RedisBackend(\n                connection_pool=redis.BlockingConnectionPool,\n                max_connections=50,\n                url=backend,\n                app=self)\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/5118895/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/5138663", "body": "The former doesnt work becouse the backend needs the app as a parameter, but the second one works. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/5138663/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "OEP": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/5157471", "body": "There's a forgotten dash on the `-ne` in celeryd:33.\n\nI'd say if you want to leave `celerybeat` alone, its shebang line is using bash so I bet what you're doing is OK, (except for that troublesome stat command).\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/5157471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "decibyte": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/5160453", "body": "I can confirm the `-r` problem on CentOS. `stat` doesn't even have that option on my local Ubuntu. But the fix/workaround by aspyatkin fixed it for me.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/5160453/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "rogerhu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/5316049", "body": "Will there be another release soon?  We're waiting anxiously to upgrade because of this commit.  Thanks!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/5316049/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "sibblegp": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/5718532", "body": "Awesome!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/5718532/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "carlmjohnson": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/7194268", "body": "Is it possible for these `kwargs` to ever be set to anything? I believe that it is not.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/7194268/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/7205476", "body": "Cool. We were fixing a similar bug in our code for The Atlantic and I noticed that it was also in celery, so I suspected the same thing had happened to you.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/7205476/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "fortran01": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/7792966", "body": "line 130: export $CELERY_LOADER\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/7792966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "domibel": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/8085541", "body": "The new requirement is missing in the changelogs. http://celery.readthedocs.org/en/latest/changelog.html\nThere is also a misleading typo in the commit message.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/8085541/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "dbrgn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/8225612", "body": "Shouldn't this be `raise self.replace(...)`?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/8225612/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/8225650", "body": "Awesome commit, thanks!\n\nWill `add_to_chord` stay? Will there be an `add_to_group`? (I don't actually need the group thing, but it might make sense for consistency reasons.)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/8225650/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/8226129", "body": "Ah, alright. Maybe it'd be possible to add a check regarding the backend and its configuration, that raises an exception if the backend is not supported?\n\nAlso, small unrelated detail: I'm not sure if a ValueError is the right thing to raise if a task is not a member of a chord. Maybe an AssertionError would be better, as being member of a chord is a precondition?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/8226129/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/49070125", "body": "It should still be `manage.py` instead of `manage.sh` though :) And the leading `./` can be dropped if called by python directly.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/49070125/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "westurner": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/8378933", "body": "Thanks!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/8378933/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "benctamas": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/10318809", "body": "ah, next time I will check that, thanks\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/10318809/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "paveltyavin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/10545558", "body": "This is a very useful fix for using celery with django 1.8\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/10545558/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "PMickael": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/11256094", "body": "Generate a bug, when calling like this a task : \n\n``` python\ntasks.elasticsearch_bulk_stat.apply_async(kwargs={\"actions\": actions},\n        ignore_result=True,\n        queue='elasticsearch_bulk_actions')\n\n\nFile \"/var/www/xxx/lib/stats.py\", line 75, in engine\n    queue='elasticsearch_bulk_actions')\nFile \"/usr/local/lib/python2.7/dist-packages/celery/app/task.py\", line 480, in apply_async\n    shadow=shadow or self.shadow_name(args, kwargs, final_options),\n\nTypeError: unbound method shadow_name() must be called with elasticsearch_bulk_stat instance as first argument (got NoneType instance instead)\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/11256094/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/comments/12118615", "body": "Hi @malinoff sorry i only saw you're message now, it's a problem with batches tasks where `self.__self__` is equal to None, so get the shadow name throw exception. Maybe test not work because of the new function shadow name implement by @ask ?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/12118615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/comments/12121825", "body": "Oh I see, nice catch. That a mistake because I roll back my first attemp to fix this problem and it was `final_options` the variable name ! I will fix this when I'm at home.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/12121825/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/comments/12124547", "body": "Test do not fail because of this commit, erase by this commit : https://github.com/celery/celery/commit/72b16ac7c43c9cbd56d2cc9d87ba5552d159ef1e\n\nPING @ask please pay attention to this line when you merge.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/12124547/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "aarcro": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/12133088", "body": "Looks like there's an error here: \nhttps://travis-ci.org/celery/celery/jobs/70524126#L293\n\nself._frozen can be None, and thus not tuple expanded.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/12133088/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/12135867", "body": "+1 (bump)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/12135867/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/12163739", "body": "s/result/results\n\nFixed in https://github.com/celery/celery/pull/2707\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/12163739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/12164917", "body": "I'm not sure if this was a good idea after all. It makes .delay() return a result with one less list wrapper than .apply()\n\nI've changed to new_join in my redis connection and that's helped some of the hanging tasks.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/12164917/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "nhooey": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/12158458", "body": "Shouldn't this invocation of `main()` be `main(sys.argv)`, since `celery.bin.celery.main()` takes `argv` as a parameter that's defaulted to `None`?\n\nI'm experiencing a bug where Celery 3.1.18 quits quietly after invoking the init script because `sys.argv` isn't passed in. ~~When I change it to `main(sys.argv)` as stated above, Celery starts without issue.~~ Looks like I have some other problem that's preventing the Celery worker from starting, but I'm still not sure why `sys.argv` isn't passed in here.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/12158458/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "Smirl": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/12516183", "body": "Hi @ask \n\nJust wondering if this change ever made it to PyPi? I have 3.1.18 but I can still see the TaskType and CeleryABC which is causing some strange behaviour. I see here (or in another commit somewhere) you have deleted TaskType.\n\nThanks\nAlex\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/12516183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "jmickle": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/13605059", "body": "This causes unpredictable behavior with the restart function as it loses the exit too quickly and results in the restart failing to restart celerybeat. If you are going to move away from bash rewrite the check_path check_dev_null and wait_stop function to reflect pure shell and not bash functions\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/13605059/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "berkerpeksag": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/13970333", "body": "If I'm not missing something, 2.7, 3.4 and 3.5 are all support set and dictionary comprehensions.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/13970333/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/comments/19068011", "body": "Django 1.7 and older versions are EOL. Perhaps we should support Django 1.8+ in Celery 4.0?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/19068011/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/celery/celery/comments/19071530", "body": "I've opened https://github.com/celery/celery/pull/3449. Please let me know if you have suggestions for wording.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/19071530/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "cjh1": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/14360913", "body": "@attiasr Ok, I figured out the issue is the [[ causing the problem. One ubuntu 14.0.4 /bin/sh is linked to /bin/dash which doesn't seem the support [[ I am seeing \n\n```\n/etc/init.d/celeryd: 42: /etc/init.d/celeryd: [[: not found\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/14360913/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/44823358", "body": "It set to -e at the top ... I will make the change you suggest. Thanks\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/44823358/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "kevcampb": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/14955221", "body": "The parameter at issue here is socket_connect_timeout, not client_connect_timeout. Appears to be a typo here. Doing a s/client_connect_timeout/socket_connect_timeout/ on this file fixes it and repairs support for redis < 2.10 or at least that passes testing here (haven't checked the test suite)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/14955221/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "frol": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/15200092", "body": "FYI, the debug print statement has been committed.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/15200092/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/16842598", "body": "`ref` seems to be not used here.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/16842598/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "jerrypy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/15474207", "body": "In your doc(http://docs.celeryproject.org/en/latest/userguide/workers.html#time-limits), says gevent pool supports revoke(with and without terminate), think you guys should add a note that it doesn't.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/15474207/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "serhatbolsu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/17020292", "body": "I confirm that currently restart does not work with this implementation, also when I reboot machine, celerybeat fails to start\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/17020292/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/17020320", "body": "also reverting this change fixed the restart\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/17020320/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "SpaxFiz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/17674554", "body": "If the application would raise it's own exception and celery backend is set, meta['result'] will be raised as dict. And the old-style classes problem still there #2458 \n\n```\n# sample code\ng = group([t.s(self.order_id) for t in tasks])\nres = g()\nfor task, result in zip(tasks, res.get()):\n    update_result(task, result)\n```\n\nsome solution here?\n\n**Python 2.7, celery 3.1.19, broker/backend MongoDB**\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/17674554/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "Riprock": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/17963331", "body": "@ask I think this is still in 3.1 and causing problems. When cloning a chain the first signature's args are modified, so when you clone the chain again the arguments accumulate. Or if using replace\u2014arguments are whatever the last ones used in the thread are.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/17963331/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/17963370", "body": "@ask Similar to chains in https://github.com/celery/celery/commit/a97e66067aaf16391e25c6347616fd6fd28de736#commitcomment-17963331 I think this fixes the issue for groups.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/17963370/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "matt-snider": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/18022819", "body": "Sad to see support dropped for Mongo :(. May I ask why?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18022819/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/18037709", "body": "I understand, thanks for the response :)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18037709/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "dangayle": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/18035533", "body": "Do you have a list of tickets that haven't been addressed that would reverse this?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18035533/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "dvirsky": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/18037781", "body": "In this context I'd like to add that there is a new feature in redis called Streams in the works (design stage, would probably go live in a few months). It makes redis more kafka like and can make redis work properly as a scalable and durable queue. It might be worth to just rewrite the redis support to use this new feature once this is ready. \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18037781/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/18037943", "body": "In any case, I work for Redis Labs, we'll see if we can allocate some resources to helping you guys. I'll keep you posted.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18037943/reactions", "total_count": 7, "+1": 7, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "robsonpeixoto": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/18750676", "body": "Is it really required?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18750676/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/18756505", "body": "Thanks \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/18756505/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "astronouth7303": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/19380365", "body": "`reduce` is no longer a builtin in Python 3:\n\n> - Removed `reduce()`. Use [`functools.reduce()`](https://docs.python.org/3/library/functools.html#functools.reduce) if you really need it; however, 99 percent of the time an explicit for loop is more readable.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/19380365/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "suryamanikar": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/20947919", "body": "I am using celery 3.1.18 (Cipater) and when I set new values of time_limit/soft_time_limit while constructing a task object or while calling .s or .si(**kwargs) kwargs = {'time_limit': 20, 'soft_time_limit': 25}\r\nI have posted details to \r\nhttp://stackoverflow.com/questions/42322288/setting-celery-task-attributes-i-e-time-limit-and-soft-time-limit-does-not-wor/42323579#42323579\r\n\r\nI was able to make it work by changing execute_using_pool. Can you check if the values can be overridden.\r\npre-requisite:\r\nThe task has default values (time_limit/soft_time_limit)assigned and I want to change during task creation/runtime.", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/20947919/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "mitar": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/205921", "body": "As I explained, the content is urlencoded combination of parameters, not JSON. Why it would be `application/json`? We do not serialize to JSON (at this stage) anywhere. But urllib does urlencode it. If we remove manual override, then urllib does the right thing and sets it to `application/x-www-form-urlencoded` (it also sets `Content-Length` properly). This can Django (or any other receiver) then properly decode. So, urllib sends content as `application/x-www-form-urlencoded` and header should match that.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/205921/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "brendoncrawford": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/374182", "body": "Ok, I'll submit a new patch within the next week or so.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374182/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374188", "body": "Ok, ill take a stab at it. Might take a few tries, but I think I can get it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374188/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ztlpn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/1591747", "body": "Well I have not been able to check it with the latest version yet, but at least on version 3.0.6 this does not happen! If reserved but not yet active task is revoked, no check against revoked list is made when task becomes active. Instead it executes normally.\n\nAlso, as the set of reserved tasks includes active tasks as well, terminate works as expected (e.g. terminates active tasks)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1591747/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ambv": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659809", "body": "SIGTERM is not 9.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659809/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659810", "body": "Ditto.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "mlavin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/4940553", "body": "This style of string formatting is only available to Python 2.7+. I believe Celery is still supporting 2.6.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4940553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "VRGhost": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/4960911", "body": "Well, it should be\n\n```\nwarnings.warn(\"%s consumed store_result call with args %s, %s\" % (self.__class__.__name__, args, kwargs))\n```\n\nthan. :-)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4960911/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "dmtaub": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/7128779", "body": "Ok, I will work on an api-based version soon. Depending on whether I need\nto write zmq interprocess communication this week, it might end up being\nless important to merge our branches at this particular moment :)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7128779/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "andrewkittredge": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/7473195", "body": "this is wrong.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7473195/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "loic": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/13004793", "body": "Removed a bunch of unused variable here, that's a cleanup, it's not strictly needed for the patch.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/13004793/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "lukmdo": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/18638334", "body": "looks great ... one I can see is that `context.expires` is being lost \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/18638334/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "oeuftete": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/24985873", "body": "Realizing I gutted too much here.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/24985873/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "kevinharvey": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/41785245", "body": "'they' in this sentence is referring to 'Tasks', yes? i.e., 'Tasks can read default values from configuration?'\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/41785245/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "m-vdb": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/54182304", "body": "I wasn't sure about that, LMK if I'm completely off\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/54182304/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/54190912", "body": "I kept the [previous behaviour](https://github.com/celery/celery/pull/3079/files#diff-5057f62807f26f9a701a0d9bf9a2a2b3L209) but makes more sense yes\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/54190912/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/54191030", "body": "that's what I figured :white_check_mark: \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/54191030/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "MichaelAquilina": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/57740556", "body": "As @adamchainz has already pointed out, this should be the preferred mechanism from Django 1.7 onwards. Django 1.9 just removed the `syncdb` command.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/57740556/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/57740689", "body": "I think this is probably the better solution\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/57740689/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "adamchainz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/57740596", "body": "I would put this first, and leave the `syncdb` for Django < 1.7 (is this even supported any more?). `migrate` was added in 1.7 and `syncdb` became just a wrapper for it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/57740596/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "tayfun": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/68267544", "body": "This fixes one regression added in https://github.com/celery/celery/pull/3244/\n\nI thought these parameters were not being used and removed them, when in fact they were being removed from `options` which is later handed over.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/68267544/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "gabrielpjordao": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/92823783", "body": "Minor (probably will have the same effect), but the default is an empty list, rather than a tuple.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/92823783/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "brechin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/96426421", "body": "@georgepsarakis Yes, I think you're right. I'll swap the nesting. Thanks!", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/96426421/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "AlmightyOatmeal": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/110037760", "body": "It is also PEP 8 friendly.", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/110037760/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "invenis-paris": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/116752715", "body": "Hi, is it possible to validate this PR ?", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/116752715/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}}}}